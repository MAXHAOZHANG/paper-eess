# ArXiv eess --Fri, 28 Aug 2020
### 1.Listener-Position and Orientation Dependency of Auditory Perception in an Enclosed Space: Elicitation of Salient Attributes  [ :arrow_down: ](https://arxiv.org/pdf/2008.12255.pdf)
>  This paper presents a subjective study conducted on the perception of salient auditory attributes depending on the listener's position and head orientations in an enclosed space. Two elicitation experiments were carried out using the Repertory Grid Technique; in-situ and laboratory experiments aimed to identify perceptual attributes among ten different combinations of the listener's positions and head orientations in a concert hall. Results revealed that, between the in-situ and laboratory experiments, the listening positions and head orientations were clustered identically. Ten salient perceptual attributes were identified from the data obtained from the laboratory experiment. Whilst these included conventional attributes such as ASW (apparent source width) and LEV (listener envelopment), new attributes such as PRL (perceived reverb loudness), ARW (apparent reverb width) and Reverb Direction were identified, and they are hypothesised to be sub-attributes of LEV (listener envelopment). Timbral characteristics such as Reverb Brightness and Echo Brightness were also identified as salient attributes, which are considered to potentially contribute to the overall sound clarity.      
### 2.Length- and Noise-aware Training Techniques for Short-utterance Speaker Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.12218.pdf)
>  Speaker recognition performance has been greatly improved with the emergence of deep learning. Deep neural networks show the capacity to effectively deal with impacts of noise and reverberation, making them attractive to far-field speaker recognition systems. The x-vector framework is a popular choice for generating speaker embeddings in recent literature due to its robust training mechanism and excellent performance in various test sets. In this paper, we start with early work on including invariant representation learning (IRL) to the loss function and modify the approach with centroid alignment (CA) and length variability cost (LVC) techniques to further improve robustness in noisy, far-field applications. This work mainly focuses on improvements for short-duration test utterances (1-8s). We also present improved results on long-duration tasks. In addition, this work discusses a novel self-attention mechanism. On the VOiCES far-field corpus, the combination of the proposed techniques achieves relative improvements of 7.0% for extremely short and 8.2% for full-duration test utterances on equal error rate (EER) over our baseline system.      
### 3.HoloGen: An open source toolbox for high-speed hologram generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.12214.pdf)
>  The rise of mixed reality systems such as Microsoft HoloLens has prompted an increase in interest in the fields of 2D and 3D holography. Already applied in fields including telecommunications, imaging, projection, lithography, beam shaping and optical tweezing, Computer Generated Holography (CGH) offers an exciting approach to a wide range of light shaping problems. The numerical processing required to generate a hologram is high and requires significant domain expertise. This has historically slowed the adoption of holographic techniques in emerging fields. In this paper we present HoloGen, an open-source Cuda C and C ++ framework for computer generated holography. HoloGen unites, for the first time, a wide array of existing hologram generation algorithms with state of the art performance while attempting to remain intuitive and easy to use. This is enabled by a C # and Windows Presentation Framework (WPF) graphical user interface (GUI). A novel reflection based parameter hierarchy is used to ensure ease of modification. Extensive use of C ++ templates based on the Standard Template Library (STL), compile time flexibility is preserved while maintaining runtime performance. The current release of HoloGen unites implementations of well known generation algorithms including Gerchberg-Saxton (GS), Liu-Taghizadeh (LT), Direct Search (DS), Simulated Annealing (SA) and One-Step Phase-Retrieval (OSPR) with less known specialist variants including Weighted GS and Adaptive OSPR. Benchmarking results are presented for several key algorithms. The software is freely available under an MIT license.      
### 4.Improving Holographic Search Algorithms using Sorted Pixel Selection  [ :arrow_down: ](https://arxiv.org/pdf/2008.12213.pdf)
>  Traditional search algorithms for computer hologram generation such as Direct Search and Simulated Annealing offer some of the best hologram qualities at convergence when compared to rival approaches. Their slow generation times and high processing power requirements mean, however, that they see little use in performance critical applications. This paper presents the novel Sorted Pixel Selection (SPS) modification for Holographic Search Algorithms (HSAs) that offers Mean Square Error (MSE) reductions in the range of 14.7 - 19.2% for the test images used. SPS operates by substituting a weighted search selection procedure for traditional random pixel selection processes. While small, the improvements seen are observed consistently across a wide range of test cases and require limited overhead for implementation.      
### 5.On Compatibility and Region of Attraction for Safe, Stabilizing Control Laws  [ :arrow_down: ](https://arxiv.org/pdf/2008.12179.pdf)
>  In this paper, a novel control method is proposed to ensure compatibility of safe, stabilizing control laws, i.e. simultaneous satisfaction of asymptotic stability and constraint satisfaction for nonlinear affine systems. A region of attraction is defined for which the proposed control safely stabilizes the system. The results presented here are dependent on the existence of an asymptotically stabilizing control law and a zeroing control barrier function (ZCBF). Our approach does not require the existence of a control Lyapunov function (CLF). Instead, the proposed method allows for the use of LaSalle's invariance principle in the stability design for applicability to more general systems. The results are then extended to design a compatible CLF-ZCBF type control law, ensure robustness of the proposed control, and show how the proposed method facilitates the computation of the region of attraction for a special class of nonlinear systems (which includes Euler-Lagrange systems). Numerical examples are used to demonstrate the proposed technique.      
### 6.Koopman Mode Decomposition of Oscillatory Temperature Field inside a Room  [ :arrow_down: ](https://arxiv.org/pdf/2008.12149.pdf)
>  Koopman mode decomposition (KMD) is a technique of nonlinear time-series analysis capable of decomposing data on complex spatio temporal dynamics into multiple modes oscillating with single frequencies, called the Koopman modes (KMs). We apply KMD to measurement data on oscillatory dynamics of a temperature field inside a room that is a complex phenomenon ubiquitous in our daily lives and has a clear technological motivation in energy-efficient air conditioning. To characterize not only the oscillatory field (scalar field) but also associated heat flux (vector field), we introduce the notion of a temperature gradient using the spatial gradient of a KM. By estimating the temperature gradient directly from data, we show that KMD is capable of extracting a distinct structure of the heat flux embedded in the oscillatory temperature field, relevant in terms of air conditioning.      
### 7.Defect Prediction of Railway Wheel Flats based on Hilbert Transform and Wavelet Packet Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2008.12111.pdf)
>  For efficient railway operation and maintenance, the demand for onboard monitoring systems is increasing with technological advances in high-speed trains. Wheel flats, one of the common defects, can be monitored in real-time through accelerometers mounted on each axle box so that the criteria of relevant standards are not exceeded. This study aims to identify the location and height of a single wheel flat based on non-stationary axle box acceleration (ABA) signals, which are generated through a train dynamics model with flexible wheelsets. The proposed feature extraction method is applied to extract the root mean square distribution of decomposed ABA signals on a balanced binary tree as orthogonal energy features using the Hilbert transform and wavelet packet decomposition. The neural network-based defect prediction model is created to define the relationship between input features and output labels. For insufficient input features, data augmentation is performed by the linear interpolation of existing features. The performance of defect prediction is evaluated in terms of the accuracy of detection and localization and improved by augmented input features and highly decomposed ABA signals. The results show that the trained neural network can predict the height and location of a single wheel flat from orthogonal energy features with high accuracy.      
### 8.Collision Free Navigation with Interacting, Non-Communicating Obstacles  [ :arrow_down: ](https://arxiv.org/pdf/2008.12092.pdf)
>  In this paper we consider the problem of navigation and motion control in an area densely populated with other agents. We propose an algorithm that, without explicit communication and based on the information it has, computes the best control action for all the agents and implements its own. Notably, the host agent (the agent executing the algorithm) computes the differences between the other agents' computed and observed control actions and treats them as known disturbances that are fed back into a robust control barrier function (RCBF) based quadratic program. A feedback loop is created because the computed control action for another agent depends on the previously used disturbance estimate. In the case of two interacting agents, stability of the feedback loop is proven and a performance guarantee in terms of constraint adherence is established. This holds whether the other agent executes the same algorithm or not.      
### 9.DrumGAN: Synthesis of Drum Sounds With Timbral Feature Conditioning Using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.12073.pdf)
>  Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.      
### 10.End-to-end Music-mixed Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.12048.pdf)
>  Automatic speech recognition (ASR) in multimedia content is one of the promising applications, but speech data in this kind of content are frequently mixed with background music, which is harmful for the performance of ASR. In this study, we propose a method for improving ASR with background music based on time-domain source separation. We utilize Conv-TasNet as a separation network, which has achieved state-of-the-art performance for multi-speaker source separation, to extract the speech signal from a speech-music mixture in the waveform domain. We also propose joint fine-tuning of a pre-trained Conv-TasNet front-end with an attention-based ASR back-end using both separation and ASR objectives. We evaluated our method through ASR experiments using speech data mixed with background music from a wide variety of Japanese animations. We show that time-domain speech-music separation drastically improves ASR performance of the back-end model trained with mixture data, and the joint optimization yielded a further significant WER reduction. The time-domain separation method outperformed a frequency-domain separation method, which reuses the phase information of the input mixture signal, both in simple cascading and joint training settings. We also demonstrate that our method works robustly for music interference from classical, jazz and popular genres.      
### 11.Estimating Uniqueness of Human Voice UsingI-Vector Representation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11985.pdf)
>  We study the individuality of human voice with re-spect to a widely used feature representation of speech utterances,namely, the i-vector model. As a first step toward this goal, wecompare and contrast uniqueness measures proposed consideringdifferent biometric modalities. Then, we introduce a more appro-priate uniqueness measure that evaluates the entropy of i-vectorswhile taking into account speaker level variations. Estimates areobtained on two newly generated datasets designed to capturevariabilities between and within speakers. The first dataset speechsamples of more than 20 thousand speakers obtained fromTEDx Talks videos. The second one includes samples of morethan one and a half thousand actors that are extracted frommovie dialogues. By using this data, we analyzed how severalfactors, such as the number of speakers, number of samples perspeakers, and different levels of within-speaker variation affectestimates. Most notably, we determined that the discretizationof i-vector elements does not necessarily cause a reduction inspeaker recognition performance. Our results show that thedegree of uniqueness offered by i-vector based representationmay reach 43-52 bits in a confined setting; however, under lessconstrained variations estimates reduce significantly to 13-20 bitlevel, depending on coarseness of quantization.      
### 12.Mixed Noise Removal with Pareto Prior  [ :arrow_down: ](https://arxiv.org/pdf/2008.11935.pdf)
>  Denoising images contaminated by the mixture of additive white Gaussian noise (AWGN) and impulse noise (IN) is an essential but challenging problem. The presence of impulsive disturbances inevitably affects the distribution of noises and thus largely degrades the performance of traditional AWGN denoisers. Existing methods target to compensate the effects of IN by introducing a weighting matrix, which, however, is lack of proper priori and thus hard to be accurately estimated. To address this problem, we exploit the Pareto distribution as the priori of the weighting matrix, based on which an accurate and robust weight estimator is proposed for mixed noise removal. Particularly, a relatively small portion of pixels are assumed to be contaminated with IN, which should have weights with small values and then be penalized out. This phenomenon can be properly described by the Pareto distribution of type 1. Therefore, armed with the Pareto distribution, we formulate the problem of mixed noise removal in the Bayesian framework, where nonlocal self-similarity priori is further exploited by adopting nonlocal low rank approximation. Compared to existing methods, the proposed method can estimate the weighting matrix adaptively, accurately, and robust for different level of noises, thus can boost the denoising performance. Experimental results on widely used image datasets demonstrate the superiority of our proposed method to the state-of-the-arts.      
### 13.Spatiotemporal Modelling of Multi-Gateway LoRa Networks with Imperfect SF Orthogonality  [ :arrow_down: ](https://arxiv.org/pdf/2008.11931.pdf)
>  Meticulous modelling and performance analysis of Low-Power Wide-Area (LPWA) networks are essential for large scale dense Internet-of-Things (IoT) deployments. As Long Range (LoRa) is currently one of the most prominent LPWA technologies, we propose in this paper a stochastic-geometry-based framework to analyse the uplink transmission performance of a multi-gateway LoRa network modelled by a Matern Cluster Process (MCP). The proposed model is first to consider all together the multi-cell topology, imperfect spreading factor (SF) orthogonality, random start times, and geometric data arrival rates. Accounting for all of these factors, we initially develop the SF-dependent collision overlap time function for any start time distribution. Then, we analyse the Laplace transforms of intra-cluster and inter-cluster interference, and formulate the uplink transmission success probability. Through simulation results, we highlight the vulnerability of each SF to interference, illustrate the impact of parameters such as the network density, and the power allocation scheme on the network performance. Uniquely, our results shed light on when it is better to activate adaptive power mechanisms, as we show that an SF-based power allocation that approximates LoRa ADR, negatively impacts nodes near the cluster head. Moreover, we show that the interfering SFs degrading the performance the most depend on the decoding threshold range and the power allocation scheme.      
### 14.Unsupervised MRI Super-Resolution using Deep External Learning and Guided Residual Dense Network with Multimodal Image Priors  [ :arrow_down: ](https://arxiv.org/pdf/2008.11921.pdf)
>  Deep learning techniques have led to state-of-the-art single image super-resolution (SISR) with natural images. Pairs of high-resolution (HR) and low-resolution (LR) images are used to train the deep learning model (mapping function). These techniques have also been applied to medical image super-resolution (SR). Compared with natural images, medical images have several unique characteristics. First, there are no HR images for training in real clinical applications because of the limitations of imaging systems and clinical requirements. Second, other modal HR images are available (e.g., HR T1-weighted images are available for enhancing LR T2-weighted images). In this paper, we propose an unsupervised SISR technique based on simple prior knowledge of the human anatomy; this technique does not require HR images for training. Furthermore, we present a guided residual dense network, which incorporates a residual dense network with a guided deep convolutional neural network for enhancing the resolution of LR images by referring to different HR images of the same subject. Experiments on a publicly available brain MRI database showed that our proposed method achieves better performance than the state-of-the-art methods.      
### 15.Dynamic Noise Embedding: Noise Aware Training and Adaptation for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2008.11920.pdf)
>  Estimating noise information exactly is crucial for noise aware training in speech applications including speech enhancement (SE) which is our focus in this paper. To estimate noise-only frames, we employ voice activity detection (VAD) to detect non-speech frames by applying optimal threshold on speech posterior. Here, the non-speech frames can be regarded as noise-only frames in noisy signal. These estimated frames are used to extract noise embedding, named dynamic noise embedding (DNE), which is useful for an SE module to capture the noise characteristics. The DNE is extracted by a simple neural network, and the SE module with the DNE can be jointly trained to be adaptive to the environment. Experiments are conducted on TIMIT dataset for single-channel denoising task and U-Net is used as a backbone SE module. Experimental results show that the DNE plays an important role in the SE module by increasing the quality and the intelligibility of corrupted signal even if the noise is non-stationary and unseen in training. In addition, we demonstrate that the DNE can be flexibly applied to other neural network-based SE modules.      
### 16.Multi-task Reinforcement Learning in Reproducing Kernel Hilbert Spaces via Cross-learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.11895.pdf)
>  Reinforcement learning (RL) is a framework to optimize a control policy using rewards that are revealed by the system as a response to a control action. In its standard form, RL involves a single agent that uses its policy to accomplish a specific task. These methods require large amounts of reward samples to achieve good performance, and may not generalize well when the task is modified, even if the new task is related. In this paper we are interested in a collaborative scheme in which multiple agents with different tasks optimize their policies jointly. To this end, we introduce cross-learning, in which agents tackling related tasks have their policies constrained to be close to one another. Two properties make our new approach attractive: (i) it produces a multi-task central policy that can be used as a starting point to adapt quickly to one of the tasks trained for, in a situation when the agent does not know which task is currently facing, and (ii) as in meta-learning, it adapts to environments related but different to those seen during training. We focus on continuous policies belonging to reproducing kernel Hilbert spaces for which we bound the distance between the task-specific policies and the cross-learned policy. To solve the resulting optimization problem, we resort to a projected policy gradient algorithm and prove that it converges to a near-optimal solution with high probability. We evaluate our methodology with a navigation example in which agents can move through environments with obstacles of multiple shapes and avoid obstacles not trained for.      
### 17.Lymph Node Gross Tumor Volume Detection and Segmentation via Distance-based Gating using 3D CT/PET Imaging in Radiotherapy  [ :arrow_down: ](https://arxiv.org/pdf/2008.11870.pdf)
>  Finding, identifying and segmenting suspicious cancer metastasized lymph nodes from 3D multi-modality imaging is a clinical task of paramount importance. In radiotherapy, they are referred to as Lymph Node Gross Tumor Volume (GTVLN). Determining and delineating the spread of GTVLN is essential in defining the corresponding resection and irradiating regions for the downstream workflows of surgical resection and radiotherapy of various cancers. In this work, we propose an effective distance-based gating approach to simulate and simplify the high-level reasoning protocols conducted by radiation oncologists, in a divide-and-conquer manner. GTVLN is divided into two subgroups of tumor-proximal and tumor-distal, respectively, by means of binary or soft distance gating. This is motivated by the observation that each category can have distinct though overlapping distributions of appearance, size and other LN characteristics. A novel multi-branch detection-by-segmentation network is trained with each branch specializing on learning one GTVLN category features, and outputs from multi-branch are fused in inference. The proposed method is evaluated on an in-house dataset of $141$ esophageal cancer patients with both PET and CT imaging modalities. Our results validate significant improvements on the mean recall from $72.5\%$ to $78.2\%$, as compared to previous state-of-the-art work. The highest achieved GTVLN recall of $82.5\%$ at $20\%$ precision is clinically relevant and valuable since human observers tend to have low sensitivity (around $80\%$ for the most experienced radiation oncologists, as reported by literature).      
### 18.DeepPrognosis: Preoperative Prediction of Pancreatic Cancer Survival and Surgical Margin via Contrast-Enhanced CT Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.11853.pdf)
>  Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers and carries a dismal prognosis. Surgery remains the best chance of a potential cure for patients who are eligible for initial resection of PDAC. However, outcomes vary significantly even among the resected patients of the same stage and received similar treatments. Accurate preoperative prognosis of resectable PDACs for personalized treatment is thus highly desired. Nevertheless, there are no automated methods yet to fully exploit the contrast-enhanced computed tomography (CE-CT) imaging for PDAC. Tumor attenuation changes across different CT phases can reflect the tumor internal stromal fractions and vascularization of individual tumors that may impact the clinical outcomes. In this work, we propose a novel deep neural network for the survival prediction of resectable PDAC patients, named as 3D Contrast-Enhanced Convolutional Long Short-Term Memory network(CE-ConvLSTM), which can derive the tumor attenuation signatures or patterns from CE-CT imaging studies. We present a multi-task CNN to accomplish both tasks of outcome and margin prediction where the network benefits from learning the tumor resection margin related features to improve survival prediction. The proposed framework can improve the prediction performances compared with existing state-of-the-art survival analysis approaches. The tumor signature built from our model has evidently added values to be combined with the existing clinical staging system.      
### 19.FCN Approach for Dynamically Locating Multiple Speakers  [ :arrow_down: ](https://arxiv.org/pdf/2008.11845.pdf)
>  In this paper, we present a deep neural network-based online multi-speaker localisation algorithm. Following the W-disjoint orthogonality principle in the spectral domain, each time-frequency (TF) bin is dominated by a single speaker, and hence by a single direction of arrival (DOA). A fully convolutional network is trained with instantaneous spatial features to estimate the DOA for each TF bin. The high resolution classification enables the network to accurately and simultaneously localize and track multiple speakers, both static and dynamic. Elaborated experimental study using both simulated and real-life recordings in static and dynamic scenarios, confirms that the proposed algorithm outperforms both classic and recent deep-learning-based algorithms.      
### 20.Robustness Assessment of Hetero-Functional Graph Theory Based Model of Interdependent Urban Utility Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.11831.pdf)
>  The increasing urban population imposes a substantial and growing burden on the supporting infrastructure, such as electricity, water, heating, natural gas, road transportation, etc. This paper presents a Hetero-functional graph theory (HFGT) based modeling framework for these integrated infrastructures followed by an analysis of network robustness. The supporting infrastructures along with the infrastructure repair facilities are considered. In contrast to conventional graph representations, a weighted HFGT model is used to capture the system processes and mutual dependencies among resources. To assess robustness of the inter-dependent networks, impacts of complete/partial and random/targeted attacks are quantified. Specifically, various contingency scenarios are simulated and the vulnerability of the network is evaluated. Additionally, several robustness metrics are proposed to provide a comprehensive evaluation of system robustness. The proposed weighted HFGT modeling and robustness assessment approach is tested using a synthetic interdependent network, comprising of an electrical power system, a water network, a district heating network, a natural gas system and a road transportation network. Results demonstrate that system robustness can be enhanced via securing system information and mitigating attack strength.      
### 21.Smart-PGSim: Using Neural Network to Accelerate AC-OPF Power Grid Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11827.pdf)
>  The optimal power flow (OPF) problem is one of the most important optimization problems for the operation of the power grid. It calculates the optimum scheduling of the committed generation units. In this paper, we develop a neural network approach to the problem of accelerating the current optimal power flow (AC-OPF) by generating an intelligent initial solution. The high quality of the initial solution and guidance of other outputs generated by the neural network enables faster convergence to the solution without losing optimality of final solution as computed by traditional methods. Smart-PGSim generates a novel multitask-learning neural network model to accelerate the AC-OPF simulation. Smart-PGSim also imposes the physical constraints of the simulation on the neural network automatically. Smart-PGSim brings an average of 49.2% performance improvement (up to 91%), computed over 10,000 problem simulations, with respect to the original AC-OPF implementation, without losing the optimality of the final solution.      
### 22.Output Feedback Control of Coupled Linear Parabolic ODE-PDE-ODE Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.11784.pdf)
>  This paper deals with the backstepping design of observer-based compensators for parabolic ODE-PDE-ODE systems. The latter consist of n coupled parabolic PDEs with distinct diffusion coefficients and spatially-varying coefficients, that are bidirectionally coupled to ODEs at both boundaries. The actuation and sensing appears through these ODEs resulting in a challenging control problem. For this setup a systematic backstepping approach is proposed, in order to determine a state feedback controller and an observer. In particular, the state feedback loop and the observer error dynamics are mapped into stable ODE-PDE-ODE cascades by making use of a sequence of transformations. With this, the design can be traced back to the solution of kernel equations already found in the literature as well as initial and boundary value problems, that can be solved numerically. Exponential stability of the closed-loop system is verified, wherein the decay rate can be directly specified in the design. The results of the paper are illustrated by the output feedback control of an unstable ODE-PDE-ODE system with two coupled parabolic PDEs.      
### 23.Domain-Adversarial Learning for Multi-Centre, Multi-Vendor, and Multi-Disease Cardiac MR Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11776.pdf)
>  Cine cardiac magnetic resonance (CMR) has become the gold standard for the non-invasive evaluation of cardiac function. In particular, it allows the accurate quantification of functional parameters including the chamber volumes and ejection fraction. Deep learning has shown the potential to automate the requisite cardiac structure segmentation. However, the lack of robustness of deep learning models has hindered their widespread clinical adoption. Due to differences in the data characteristics, neural networks trained on data from a specific scanner are not guaranteed to generalise well to data acquired at a different centre or with a different scanner. In this work, we propose a principled solution to the problem of this domain shift. Domain-adversarial learning is used to train a domain-invariant 2D U-Net using labelled and unlabelled data. This approach is evaluated on both seen and unseen domains from the M\&amp;Ms challenge dataset and the domain-adversarial approach shows improved performance as compared to standard training. Additionally, we show that the domain information cannot be recovered from the learned features.      
### 24.Exploring British Accents: Modeling the Trap-Bath Split with Functional Data Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.12233.pdf)
>  The sound of our speech is influenced by the places we come from. Great Britain contains a wide variety of distinctive accents which are of interest to linguistics. In particular, the "a" vowel in words like "class" is pronounced differently in the North and the South. Speech recordings of this vowel can be represented as formant curves or as Mel-frequency cepstral coefficient curves. Functional data analysis and generalized additive models offer techniques to model the variation in these curves. Our first aim is to model the difference between typical Northern and Southern vowels, by training two classifiers on the North-South Class Vowels dataset collected for this paper (Koshy 2020). Our second aim is to visualize geographical variation of accents in Great Britain. For this we use speech recordings from a second dataset, the British National Corpus (BNC) audio edition (Coleman et al. 2012). The trained models are used to predict the accent of speakers in the BNC, and then we model the geographical patterns in these predictions using a soap film smoother. This work demonstrates a flexible and interpretable approach to modeling phonetic accent variation in speech recordings.      
### 25.Quantum Cooperative Robotics and Autonomy  [ :arrow_down: ](https://arxiv.org/pdf/2008.12230.pdf)
>  The intersection of Quantum Technologies and Robotics Autonomy is explored in the present paper. The two areas are brought together in establishing an interdisciplinary interface that contributes to advancing the field of system autonomy, and pushes the engineering boundaries beyond the existing techniques. The present research adopts the experimental aspects of quantum entanglement and quantum cryptography, and integrates these established quantum capabilities into distributed robotic platforms, to explore the possibility of achieving increased autonomy for the control of multi-agent robotic systems engaged in cooperative tasks. Experimental quantum capabilities are realized by producing single photons (using spontaneous parametric down-conversion process), polarization of photons, detecting vertical and horizontal polarizations, and single photon detecting/counting. Specifically, such quantum aspects are implemented on network of classical agents, i.e., classical aerial and ground robots/unmanned systems. With respect to classical systems for robotic applications, leveraging quantum technology is expected to lead to guaranteed security, very fast control and communication, and unparalleled quantum capabilities such as entanglement and quantum superposition that will enable novel applications.      
### 26.Development and Analysis of Digging and Soil Removing Mechanisms for Mole-Bot: Bio-Inspired Mole-Like Drilling Robot  [ :arrow_down: ](https://arxiv.org/pdf/2008.12229.pdf)
>  Interests in exploration of new energy resources are increasing due to the exhaustion of existing resources. To explore new energy sources, various studies have been conducted to improve the drilling performance of drilling equipment for deep and strong ground. However, with better performance, the modern drilling equipment is bulky and, furthermore, has become inconvenient in both installation and operation, for it takes complex procedures for complex terrains. Moreover, environmental issues are also a concern because of the excessive use of mud and slurry to remove excavated soil. To overcome these limitations, a mechanism that combines an expandable drill bit and link structure to simulate the function of the teeth and forelimbs of a mole is proposed. In this paper, the proposed expandable drill bit simplifies the complexity and high number of degrees of freedom of the animal head. In addition, a debris removal mechanism mimicking a shoulder structure and forefoot movement is proposed. For efficient debris removal, the proposed mechanism enables the simultaneous rotation and expanding/folding motions of the drill bit by using a single actuator. The performance of the proposed system is evaluated by dynamic simulations and experiments.      
### 27.Random Style Transfer based Domain Generalization Networks Integrating Shape and Spatial Information  [ :arrow_down: ](https://arxiv.org/pdf/2008.12205.pdf)
>  Deep learning (DL)-based models have demonstrated good performance in medical image segmentation. However, the models trained on a known dataset often fail when performed on an unseen dataset collected from different centers, vendors and disease populations. In this work, we present a random style transfer network to tackle the domain generalization problem for multi-vendor and center cardiac image segmentation. Style transfer is used to generate training data with a wider distribution/ heterogeneity, namely domain augmentation. As the target domain could be unknown, we randomly generate a modality vector for the target modality in the style transfer stage, to simulate the domain shift for unknown domains. The model can be trained in a semi-supervised manner by simultaneously optimizing a supervised segmentation and an unsupervised style translation objective. Besides, the framework incorporates the spatial information and shape prior of the target by introducing two regularization terms. We evaluated the proposed framework on 40 subjects from the M\&amp;Ms challenge2020, and obtained promising performance in the segmentation for data from unknown vendors and centers.      
### 28.Context-Dependent Implicit Authentication for Wearable Device User  [ :arrow_down: ](https://arxiv.org/pdf/2008.12145.pdf)
>  As market wearables are becoming popular with a range of services, including making financial transactions, accessing cars, etc. that they provide based on various private information of a user, security of this information is becoming very important. However, users are often flooded with PINs and passwords in this internet of things (IoT) world. Additionally, hard-biometric, such as facial or finger recognition, based authentications are not adaptable for market wearables due to their limited sensing and computation capabilities. Therefore, it is a time demand to develop a burden-free implicit authentication mechanism for wearables using the less-informative soft-biometric data that are easily obtainable from the market wearables. In this work, we present a context-dependent soft-biometric-based wearable authentication system utilizing the heart rate, gait, and breathing audio signals. From our detailed analysis, we find that a binary support vector machine (SVM) with radial basis function (RBF) kernel can achieve an average accuracy of $0.94 \pm 0.07$, $F_1$ score of $0.93 \pm 0.08$, an equal error rate (EER) of about $0.06$ at a lower confidence threshold of 0.52, which shows the promise of this work.      
### 29.Properties Of Winning Tickets On Skin Lesion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2008.12141.pdf)
>  Skin cancer affects a large population every year -- automated skin cancer detection algorithms can thus greatly help clinicians. Prior efforts involving deep learning models have high detection accuracy. However, most of the models have a large number of parameters, with some works even using an ensemble of models to achieve good accuracy. In this paper, we investigate a recently proposed pruning technique called Lottery Ticket Hypothesis. We find that iterative pruning of the network resulted in improved accuracy, compared to that of the unpruned network, implying that -- the lottery ticket hypothesis can be applied to the problem of skin cancer detection and this hypothesis can result in a smaller network for inference. We also examine the accuracy across sub-groups -- created by gender and age -- and it was found that some sub-groups show a larger increase in accuracy than others.      
### 30.DMD: A Large-Scale Multi-Modal Driver Monitoring Dataset for Attention and Alertness Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.12085.pdf)
>  Vision is the richest and most cost-effective technology for Driver Monitoring Systems (DMS), especially after the recent success of Deep Learning (DL) methods. The lack of sufficiently large and comprehensive datasets is currently a bottleneck for the progress of DMS development, crucial for the transition of automated driving from SAE Level-2 to SAE Level-3. In this paper, we introduce the Driver Monitoring Dataset (DMD), an extensive dataset which includes real and simulated driving scenarios: distraction, gaze allocation, drowsiness, hands-wheel interaction and context data, in 41 hours of RGB, depth and IR videos from 3 cameras capturing face, body and hands of 37 drivers. A comparison with existing similar datasets is included, which shows the DMD is more extensive, diverse, and multi-purpose. The usage of the DMD is illustrated by extracting a subset of it, the dBehaviourMD dataset, containing 13 distraction activities, prepared to be used in DL training processes. Furthermore, we propose a robust and real-time driver behaviour recognition system targeting a real-world application that can run on cost-efficient CPU-only platforms, based on the dBehaviourMD. Its performance is evaluated with different types of fusion strategies, which all reach enhanced accuracy still providing real-time response.      
### 31.Compensation Tracker: Data Association Method for Lost Object  [ :arrow_down: ](https://arxiv.org/pdf/2008.12052.pdf)
>  At present, the main research direction of multi-object tracking framework is detection-based tracking method. Although the detection-based tracking model can achieve good results, it is very dependent on the performance of the detector. The tracking results will be affected to a certain extent when the detector has the behaviors of omission and error detection. Therefore, in order to solve the problem of missing detection, this paper designs a compensation tracker based on Kalman filter and forecast correction. Experiments show that after using the compensation tracker designed in this paper, evaluation indicators have improved in varying degrees on MOT Challenge data sets. In particular, the multi-object tracking accuracy reached 66% in the 2020 datasets of dense scenarios. This shows that the proposed method can effectively improve the tracking performance of the model.      
### 32.Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events  [ :arrow_down: ](https://arxiv.org/pdf/2008.11988.pdf)
>  As a vital topic in media content interpretation, video anomaly detection (VAD) has made fruitful progress via deep neural network (DNN). However, existing methods usually follow a reconstruction or frame prediction routine. They suffer from two gaps: (1) They cannot localize video activities in a both precise and comprehensive manner. (2) They lack sufficient abilities to utilize high-level semantics and temporal context information. Inspired by frequently-used cloze test in language study, we propose a brand-new VAD solution named Video Event Completion (VEC) to bridge gaps above: First, we propose a novel pipeline to achieve both precise and comprehensive enclosure of video activities. Appearance and motion are exploited as mutually complimentary cues to localize regions of interest (RoIs). A normalized spatio-temporal cube (STC) is built from each RoI as a video event, which lays the foundation of VEC and serves as a basic processing unit. Second, we encourage DNN to capture high-level semantics by solving a visual cloze test. To build such a visual cloze test, a certain patch of STC is erased to yield an incomplete event (IE). The DNN learns to restore the original video event from the IE by inferring the missing patch. Third, to incorporate richer motion dynamics, another DNN is trained to infer erased patches' optical flow. Finally, two ensemble strategies using different types of IE and modalities are proposed to boost VAD performance, so as to fully exploit the temporal context and modality information for VAD. VEC can consistently outperform state-of-the-art methods by a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified at <a class="link-external link-http" href="http://github.com/yuguangnudt/VEC_VAD" rel="external noopener nofollow">this http URL</a>.      
### 33.Multimodal Learning for Cardiovascular Risk Prediction using EHR Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.11979.pdf)
>  Electronic health records (EHRs) contain structured and unstructured data of significant clinical and research value. Various machine learning approaches have been developed to employ information in EHRs for risk prediction. The majority of these attempts, however, focus on structured EHR fields and lose the vast amount of information in the unstructured texts. To exploit the potential information captured in EHRs, in this study we propose a multimodal recurrent neural network model for cardiovascular risk prediction that integrates both medical texts and structured clinical information. The proposed multimodal bidirectional long short-term memory (BiLSTM) model concatenates word embeddings to classical clinical predictors before applying them to a final fully connected neural network. In the experiments, we compare performance of different deep neural network (DNN) architectures including convolutional neural network and long short-term memory in scenarios of using clinical variables and chest X-ray radiology reports. Evaluated on a data set of real world patients with manifest vascular disease or at high-risk for cardiovascular disease, the proposed BiLSTM model demonstrates state-of-the-art performance and outperforms other DNN baseline architectures.      
### 34.Edge and Identity Preserving Network for Face Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2008.11977.pdf)
>  Face super-resolution has become an indispensable part in security problems such as video surveillance and identification system, but the distortion in facial components is a main obstacle to overcoming the problems. To alleviate it, most stateof-the-arts have utilized facial priors by using deep networks. These methods require extra labels, longer training time, and larger computation memory. Thus, we propose a novel Edge and Identity Preserving Network for Face Super-Resolution Network, named as EIPNet, which minimizes the distortion by utilizing a lightweight edge block and identity information. Specifically, the edge block extracts perceptual edge information and concatenates it to original feature maps in multiple scales. This structure progressively provides edge information in reconstruction procedure to aggregate local and global structural information. Moreover, we define an identity loss function to preserve identification of super-resolved images. The identity loss function compares feature distributions between super-resolved images and target images to solve unlabeled classification problem. In addition, we propose a Luminance-Chrominance Error (LCE) to expand usage of image representation domain. The LCE method not only reduces the dependency of color information by dividing brightness and color components but also facilitates our network to reflect differences between Super-Resolution (SR) and High- Resolution (HR) images in multiple domains (RGB and YUV). The proposed methods facilitate our super-resolution network to elaborately restore facial components and generate enhanced 8x scaled super-resolution images with a lightweight network structure.      
### 35.Adaptive directional Haar tight framelets on bounded domains for digraph signal representations  [ :arrow_down: ](https://arxiv.org/pdf/2008.11966.pdf)
>  Based on hierarchical partitions, we provide the construction of Haar-type tight framelets on any compact set $K\subseteq \mathbb{R}^d$. In particular, on the unit block $[0,1]^d$, such tight framelets can be built to be with adaptivity and directionality. We show that the adaptive directional Haar tight framelet systems can be used for digraph signal representations. Some examples are provided to illustrate results in this paper.      
### 36.Computational Models of Human Decision-Making with Application to the Internet of Everything  [ :arrow_down: ](https://arxiv.org/pdf/2008.11958.pdf)
>  The concept of the Internet of Things (IoT) first appeared a few decades ago. Today, by the ubiquitous wireless connectivity, the boost of machine learning and artificial intelligence, and the advances in big data analytics, it is safe to say that IoT has evolved to a new concept called the Internet of Everything (IoE) or the Internet of All. IoE has four pillars: Things, human, data, and processes, which render it as an inhomogeneous large-scale network. A crucial challenge of such a network is to develop management, analysis, and optimization policies that besides utility-maximizer machines, also take irrational humans into account. We discuss several networking applications in which appropriate modeling of human decision-making is vital. We then provide a brief review of computational models of human decision-making. Based on one such model, we develop a solution for a task offloading problem in fog computing and we analyze the implications of including humans in the loop.      
### 37.Moderately supervised learning: definition and framework  [ :arrow_down: ](https://arxiv.org/pdf/2008.11945.pdf)
>  Supervised learning (SL) has achieved remarkable success in numerous artificial intelligence applications. In the current literature, by referring to the properties of the ground-truth labels prepared for a training data set, SL is roughly categorized as fully supervised learning (FSL) and weakly supervised learning (WSL). However, solutions for various FSL tasks have shown that the given ground-truth labels are not always learnable, and the target transformation from the given ground-truth labels to learnable targets can significantly affect the performance of the final FSL solutions. Without considering the properties of the target transformation from the given ground-truth labels to learnable targets, the roughness of the FSL category conceals some details that can be critical to building the optimal solutions for some specific FSL tasks. Thus, it is desirable to reveal these details. This article attempts to achieve this goal by expanding the categorization of FSL and investigating the subtype that plays the central role in FSL. Taking into consideration the properties of the target transformation from the given ground-truth labels to learnable targets, we first categorize FSL into three narrower subtypes. Then, we focus on the subtype moderately supervised learning (MSL). MSL concerns the situation where the given ground-truth labels are ideal, but due to the simplicity in annotation of the given ground-truth labels, careful designs are required to transform the given ground-truth labels into learnable targets. From the perspectives of definition and framework, we comprehensively illustrate MSL to reveal what details are concealed by the roughness of the FSL category. Finally, discussions on the revealed details suggest that MSL should be given more attention.      
### 38.Integrated Planning of a Solar/Storage Collective  [ :arrow_down: ](https://arxiv.org/pdf/2008.11942.pdf)
>  French regulation allows consumers in low-voltage networks to form collectives to produce, share, and consume local energy under the collective self-consumption framework. A natural consequence of collectively-owned generation projects is the need to allocate production among consumers. In long-term plans, production allocation determines each of the consumers' benefits of joining the collective. In the short-term, energy should be dynamically allocated to reflect operation. This paper presents a framework that integrates long and short-term planning of a collective that shares a solar plus energy storage system. In the long-term planning stage, we maximize the collective's welfare and equitably allocate expected energy to each consumer. For operation, we propose a model predictive control algorithm that minimizes short-term costs and allocates energy to each consumer on a 30-minute basis (as required by French regulation). We adjust the energy allotment ex-post operation to reflect the materialization of uncertainty. We present a case study where we showcase the framework for a 15 consumer collective.      
### 39.Crossing-Domain Generative Adversarial Networks for Unsupervised Multi-Domain Image-to-Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11882.pdf)
>  State-of-the-art techniques in Generative Adversarial Networks (GANs) have shown remarkable success in image-to-image translation from peer domain X to domain Y using paired image data. However, obtaining abundant paired data is a non-trivial and expensive process in the majority of applications. When there is a need to translate images across n domains, if the training is performed between every two domains, the complexity of the training will increase quadratically. Moreover, training with data from two domains only at a time cannot benefit from data of other domains, which prevents the extraction of more useful features and hinders the progress of this research area. In this work, we propose a general framework for unsupervised image-to-image translation across multiple domains, which can translate images from domain X to any a domain without requiring direct training between the two domains involved in image translation. A byproduct of the framework is the reduction of computing time and computing resources, since it needs less time than training the domains in pairs as is done in state-of-the-art works. Our proposed framework consists of a pair of encoders along with a pair of GANs which learns high-level features across different domains to generate diverse and realistic samples from. Our framework shows competing results on many image-to-image tasks compared with state-of-the-art techniques.      
### 40.An exploratory time series analysis of total deaths per month in Brazil since 2015  [ :arrow_down: ](https://arxiv.org/pdf/2008.11805.pdf)
>  In this article, we investigate the historical series of the total number of deaths per month in Brazil since 2015 using time series analysis techniques, in order to assess whether the COVID-19 pandemic caused any change in the series' generating mechanism. The results obtained so far indicate that there was no statistical significant impact.      
