# ArXiv eess --Mon, 31 Aug 2020
### 1.CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.12750.pdf)
>  Ultrafast ultrasound (US) revolutionized biomedical imaging with its capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough modalities such as shear-wave elastography and functional US neuroimaging. Yet, it suffers from strong diffraction artifacts, mainly caused by grating lobes, side lobes, or edge waves. Multiple acquisitions are typically required to obtain a sufficient image quality, at the cost of a reduced frame rate. To answer the increasing demand for high-quality imaging from single-shot acquisitions, we propose a two-step convolutional neural network (CNN)-based image reconstruction method, compatible with real-time imaging. A low-quality estimate is obtained by means of a backprojection-based operation, akin to conventional delay-and-sum beamforming, from which a high-quality image is restored using a residual CNN with multi-scale and multi-channel filtering properties, trained specifically to remove the diffraction artifacts inherent to ultrafast US imaging. To account for both the high dynamic range and the radio frequency property of US images, we introduce the mean signed logarithmic absolute error (MSLAE) as training loss function. Experiments were conducted with a linear transducer array, in single plane wave (PW) imaging. Trainings were performed on a simulated dataset, crafted to contain a wide diversity of structures and echogenicities. Extensive numerical evaluations demonstrate that the proposed approach can reconstruct images from single PWs with a quality similar to that of gold-standard synthetic aperture imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo experiments show that trainings performed on simulated data translate well to experimental settings.      
### 2.Challenges and opportunities of inertia estimation and forecasting in low-inertia power systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.12692.pdf)
>  Accurate inertia estimates and forecasts are crucial to support the system operation in future low-inertia power systems. A large literature on inertia estimation methods is available. This paper aims to provide an overview and classification of inertia estimation methods. The classification considers the time horizon the methods are applicable to, i.e., offline post mortem, online real time and forecasting methods, and the scope of the inertia estimation, e.g., system-wide, regional, generation, demand, individual resource. Shortcomings of the existing inertia estimation methods have been identified and suggestions for future work have been made.      
### 3.On Transfer Learning of Traditional Frequency and Time Domain Features in Turning  [ :arrow_down: ](https://arxiv.org/pdf/2008.12691.pdf)
>  There has been an increasing interest in leveraging machine learning tools for chatter prediction and diagnosis in discrete manufacturing processes. Some of the most common features for studying chatter include traditional signal processing tools such as Fast Fourier Transform (FFT), Power Spectral Density (PSD), and the Auto-correlation Function (ACF). In this study, we use these tools in a supervised learning setting to identify chatter in accelerometer signals obtained from a turning experiment. The experiment is performed using four different tool overhang lengths with varying cutting speed and the depth of cut. We then examine the resulting signals and tag them as either chatter or chatter-free. The tagged signals are then used to train a classifier. The classification methods include the most common algorithms: Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), and Gradient Boost (GB). Our results show that features extracted from the Fourier spectrum are the most informative when training a classifier and testing on data from the same cutting configuration yielding accuracy as high as %96. However, the accuracy drops significantly when training and testing on two different configurations with different structural eigenfrequencies. Thus, we conclude that while these traditional features can be highly tuned to a certain process, their transfer learning ability is limited. We also compare our results against two other methods with rising popularity in the literature: Wavelet Packet Transform (WPT) and Ensemble Empirical Mode Decomposition (EEMD). The latter two methods, especially EEMD, show better transfer learning capabilities for our dataset.      
### 4.Bayesian Neural Networks for Uncertainty Estimation of Imaging Biomarkers  [ :arrow_down: ](https://arxiv.org/pdf/2008.12680.pdf)
>  Image segmentation enables to extract quantitative measures from scans that can serve as imaging biomarkers for diseases. However, segmentation quality can vary substantially across scans, and therefore yield unfaithful estimates in the follow-up statistical analysis of biomarkers. The core problem is that segmentation and biomarker analysis are performed independently. We propose to propagate segmentation uncertainty to the statistical analysis to account for variations in segmentation confidence. To this end, we evaluate four Bayesian neural networks to sample from the posterior distribution and estimate the uncertainty. We then assign confidence measures to the biomarker and propose statistical models for its integration in group analysis and disease classification. Our results for segmenting the liver in patients with diabetes mellitus clearly demonstrate the improvement of integrating biomarker uncertainty in the statistical inference.      
### 5.Vehicle Telematics Via Exteroceptive Sensors: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2008.12632.pdf)
>  Whereas a very large number of sensors are available in the automotive field, currently just a few of them, mostly proprioceptive ones, are used in telematics, automotive insurance, and mobility safety research. In this paper, we show that exteroceptive sensors, like microphones or cameras, could replace proprioceptive ones in many fields. Our main motivation is to provide the reader with alternative ideas for the development of telematics applications when proprioceptive sensors are unusable for technological issues, privacy concerns, or lack of availability in commercial devices. We first introduce a taxonomy of sensors in telematics. Then, we review in detail all exteroceptive sensors of some interest for vehicle telematics, highlighting advantages, drawbacks, and availability in off-the-shelf devices. Successively, we present a list of notable telematics services and applications in research and industry like driving profiling or vehicular safety. For each of them, we report the most recent and important works relying on exteroceptive sensors, as long as the available datasets. We conclude showing open challenges using exteroceptive sensors both for industry and research.      
### 6.Power to Air-transportation via Hydrogen  [ :arrow_down: ](https://arxiv.org/pdf/2008.12630.pdf)
>  This paper proposes a framework to analyze the concept of power to hydrogen (P2H) for fueling the next generation of aircraft. The impact of introducing new P2H loads is investigated from different aspects namely, cost, carbon emission, and wind curtailment. The newly introduced electric load is calculated based on the idea of replacing the busiest international flight route in the Europe, Dublin-London Heathrow, by hydrogen fuel-powered aircraft as a high potential candidate for the next generation of air travel systems to cope with the ambitious targets set in Europe Flight Path 2050 by the Advisory Council for Aeronautics Research in Europe (ACARE). The simulation is performed on a representative Irish transmission network to demonstrate the effectiveness of the proposed solution.      
### 7.Optical oxygen sensing with artificial intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2008.12629.pdf)
>  Luminescence-based sensors for measuring oxygen concentration are widely used both in industry and research due to the practical advantages and sensitivity of this type of sensing. The measuring principle is the luminescence quenching by oxygen molecules, which results in a change of the luminescence decay time and intensity. In the classical approach, this change is related to an oxygen concentration using the Stern-Volmer equation. This equation, which in most of the cases is non-linear, is parametrized through device-specific constants. Therefore, to determine these parameters every sensor needs to be precisely calibrated at one or more known concentrations. This work explores an entirely new artificial intelligence approach and demonstrates the feasibility of oxygen sensing through machine learning. The specifically developed neural network learns very efficiently to relate the input quantities to the oxygen concentration. The results show a mean deviation of the predicted from the measured concentration of 0.5 percent air, comparable to many commercial and low-cost sensors. Since the network was trained using synthetically generated data, the accuracy of the model predictions is limited by the ability of the generated data to describe the measured data, opening up future possibilities for significant improvement by using a large number of experimental measurements for training. The approach described in this work demonstrates the applicability of artificial intelligence to sensing of sensors.      
### 8.Deep Reinforcement Learning for Field Development Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.12627.pdf)
>  The field development optimization (FDO) problem represents a challenging mixed-integer nonlinear programming (MINLP) problem in which we seek to obtain the number of wells, their type, location, and drilling sequence that maximizes an economic metric. Evolutionary optimization algorithms have been effectively applied to solve the FDO problem, however, these methods provide only a deterministic (single) solution which are generally not robust towards small changes in the problem setup. In this work, the goal is to apply convolutional neural network-based (CNN) deep reinforcement learning (DRL) algorithms to the field development optimization problem in order to obtain a policy that maps from different states or representation of the underlying geological model to optimal decisions. The proximal policy optimization (PPO) algorithm is considered with two CNN architectures of varying number of layers and composition. Both networks obtained policies that provide satisfactory results when compared to a hybrid particle swarm optimization - mesh adaptive direct search (PSO-MADS) algorithm that has been shown to be effective at solving the FDO problem.      
### 9.Simulation-supervised deep learning for analysing organelles states and behaviour in living cells  [ :arrow_down: ](https://arxiv.org/pdf/2008.12617.pdf)
>  In many real-world scientific problems, generating ground truth (GT) for supervised learning is almost impossible. The causes include limitations imposed by scientific instrument, physical phenomenon itself, or the complexity of modeling. Performing artificial intelligence (AI) tasks such as segmentation, tracking, and analytics of small sub-cellular structures such as mitochondria in microscopy videos of living cells is a prime example. The 3D blurring function of microscope, digital resolution from pixel size, optical resolution due to the character of light, noise characteristics, and complex 3D deformable shapes of mitochondria, all contribute to making this problem GT hard. Manual segmentation of 100s of mitochondria across 1000s of frames and then across many such videos is not only herculean but also physically inaccurate because of the instrument and phenomena imposed limitations. Unsupervised learning produces less than optimal results and accuracy is important if inferences relevant to therapy are to be derived. In order to solve this unsurmountable problem, we bring modeling and deep learning to a nexus. We show that accurate physics based modeling of microscopy data including all its limitations can be the solution for generating simulated training datasets for supervised learning. We show here that our simulation-supervised segmentation approach is a great enabler for studying mitochondrial states and behaviour in heart muscle cells, where mitochondria have a significant role to play in the health of the cells. We report unprecedented mean IoU score of 91% for binary segmentation (19% better than the best performing unsupervised approach) of mitochondria in actual microscopy videos of living cells. We further demonstrate the possibility of performing multi-class classification, tracking, and morphology associated analytics at the scale of individual mitochondrion.      
### 10.Attributing Uncertainties in the Identification of Hotspots in SPECT Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.12614.pdf)
>  In SPECT imaging, the identification and detection of a lesion rely either on visual inspection of the reconstructed tomographic images or post-processing image analysis methods. Both approaches do not provide the capability to attribute a quantifiable uncertainty to this identification. We present a framework which allows the quantification of this uncertainty and the assignment of a level of confidence to the detection of hotspots. Based on the "Reconstructed Image from Simulations Ensemble" (RISE), an image reconstruction method, the presented scheme uses the set of projection measurements to derive the parameters defining the uptake of radioactivity, the position and the size of a hotspot, and as well as their associated uncertainties. The capabilities of the proposed method are demonstrated with projection data from GATE phantom simulations.      
### 11.A Method with Lower-than-ML Threshold for Frequency Estimation of Multiple Sinusoids  [ :arrow_down: ](https://arxiv.org/pdf/2008.12608.pdf)
>  Estimating the frequencies of multiple sinusoids in the presence of AWGN and when the data record is short is commonly accomplished by subspace-based methods such as ESPRIT, MUSIC, Min-Norm, etc. These methods do not assume that the data are zero outside the observation interval. If we assume otherwise, the threshold SNR is lowered significantly, but the price paid is unacceptable bias. Among all known unbiased estimators, the maximum-likelihood estimator (MLE) has the lowest threshold, but is computationally the most expensive. We propose a new algorithm that carries out, when needed, (i) zero-padding, and (ii) removal and re-estimation. These added steps result in a threshold SNR that is lower than that of the MLE for the examples considered herein, viz., noisy signals containing sinusoids with random parameters and up to five components. The maximum improvement in threshold was 10 dB for the two-sinusoid case. The bias of the estimates is also either equal to or lower than MLE's. Unlike the MLE, the proposed method is very much computationally feasible.      
### 12.Non-Parallel Voice Conversion with Augmented Classifier Star Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.12604.pdf)
>  This paper proposes a method that allows for non-parallel multi-domain voice conversion (VC) by using a variant of generative adversarial networks (GANs) called StarGAN. The main features of our method, which we term StarGAN-VC, are as follows: First, it requires no parallel utterances, transcriptions, or time alignment procedures for speech generator training. Second, it can simultaneously learn mappings across multiple domains using a single generator network so that it can fully use available training data collected from multiple domains by capturing common latent features that can be shared across different domains. Third, it is able to generate converted speech signals quickly enough to allow real-time implementations and requires only several minutes of training examples to generate reasonably realistic-sounding speech. In this paper, we describe three formulations of StarGAN, including a newly introduced novel StarGAN variant called "Augmented classifier StarGAN (A-StarGAN)", and compare them in a non-parallel VC task. We also compare them with several baseline methods.      
### 13.Robust converter-fed motor control based on active rejection of multiple disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2008.12596.pdf)
>  In this work, an advanced motion controller is proposed for buck converter-fed DC motor systems. The design is based on an idea of active disturbance rejection control (ADRC) with its key component being a custom observer capable of reconstructing various types of disturbances (including complex, harmonic signals). A special formulation of the proposed design allows the control action to be expressed in a concise and practically appealing form reducing its implementation requirements. The obtained experimental results show increased performance of the introduced approach over conventionally used methods in tracking precision and disturbance rejection, while keeping similar level of energy consumption. A stability analysis using theory of singular perturbation further supports the validity of proposed control approach.      
### 14.PCB Defect Detection Using Denoising Convolutional Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2008.12589.pdf)
>  Printed Circuit boards (PCBs) are one of the most important stages in making electronic products. A small defect in PCBs can cause significant flaws in the final product. Hence, detecting all defects in PCBs and locating them is essential. In this paper, we propose an approach based on denoising convolutional autoencoders for detecting defective PCBs and to locate the defects. Denoising autoencoders take a corrupted image and try to recover the intact image. We trained our model with defective PCBs and forced it to repair the defective parts. Our model not only detects all kinds of defects and locates them, but it can also repair them as well. By subtracting the repaired output from the input, the defective parts are located. The experimental results indicate that our model detects the defective PCBs with high accuracy (97.5%) compare to state of the art works.      
### 15.Soft Tissue Sarcoma Co-Segmentation in Combined MRI and PET/CT Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.12544.pdf)
>  Tumor segmentation in multimodal medical images has seen a growing trend towards deep learning based methods. Typically, studies dealing with this topic fuse multimodal image data to improve the tumor segmentation contour for a single imaging modality. However, they do not take into account that tumor characteristics are emphasized differently by each modality, which affects the tumor delineation. Thus, the tumor segmentation is modality- and task-dependent. This is especially the case for soft tissue sarcomas, where, due to necrotic tumor tissue, the segmentation differs vastly. Closing this gap, we develop a modalityspecific sarcoma segmentation model that utilizes multimodal image data to improve the tumor delineation on each individual modality. We propose a simultaneous co-segmentation method, which enables multimodal feature learning through modality-specific encoder and decoder branches, and the use of resource-effcient densely connected convolutional layers. We further conduct experiments to analyze how different input modalities and encoder-decoder fusion strategies affect the segmentation result. We demonstrate the effectiveness of our approach on public soft tissue sarcoma data, which comprises MRI (T1 and T2 sequence) and PET/CT scans. The results show that our multimodal co-segmentation model provides better modality-specific tumor segmentation than models using only the PET or MRI (T1 and T2) scan as input.      
### 16.Voice Conversion Challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion  [ :arrow_down: ](https://arxiv.org/pdf/2008.12527.pdf)
>  The voice conversion challenge is a bi-annual scientific event held to compare and understand different voice conversion (VC) systems built on a common dataset. In 2020, we organized the third edition of the challenge and constructed and distributed a new database for two tasks, intra-lingual semi-parallel and cross-lingual VC. After a two-month challenge period, we received 33 submissions, including 3 baselines built on the database. From the results of crowd-sourced listening tests, we observed that VC methods have progressed rapidly thanks to advanced deep learning methods. In particular, speaker similarity scores of several systems turned out to be as high as target speakers in the intra-lingual semi-parallel VC task. However, we confirmed that none of them have achieved human-level naturalness yet for the same task. The cross-lingual conversion task is, as expected, a more difficult task, and the overall naturalness and similarity scores were lower than those for the intra-lingual conversion task. However, we observed encouraging results, and the MOS scores of the best systems were higher than 4.0. We also show a few additional analysis results to aid in understanding cross-lingual VC better.      
### 17.Universal Wireless Power Transfer for Energy Security, Availability and Convenience  [ :arrow_down: ](https://arxiv.org/pdf/2008.12512.pdf)
>  This article proposes a novel system concept named universal wireless power transfer, in which power can be wirelessly transferred between different entities (e.g. vehicles, robots, homes, grid facilities, consumer electronic devices, etc.) equipped with proper energy transmitters and receivers, whether stationary or in motion. This concept generalizes individually existing wireless power transfer systems, where a specific wireless power transfer technology is used, and where the wireless energy transmitter or receiver is fixed. As a result, energy mobility, flexibility, and convenience are significantly improved by the proposed universal wireless power transfer concept in this study. Moreover, factors relevant to system energy efficiency are analyzed according to each utilized wireless power transfer technology. Necessary market mechanisms for such a concept to be successfully deployed are also introduced, along with an analysis of the benefits engendered in terms of improving energy systems, the environment, human comfort, and convenience. Finally, a discussion of the proposed concept, policy implications and recommendations for future research directions which will underpin universal wireless power transfer systems are presented.      
### 18.Electric Vehicle -- Wireless Charging-Discharging Lane Decentralized Peer-to-Peer Energy Trading  [ :arrow_down: ](https://arxiv.org/pdf/2008.12509.pdf)
>  This paper investigates the problem of bidirectional energy exchange between electric vehicles (EVs) and road lanes embedded with wireless power transfer technologies called wireless charging-discharging lanes (WCDLs). As such, EVs could provide better services to the grid, especially for balancing the supply-demand, while bringing convenience for EV users, because no cables and EV stops are needed. To enable this EV--WCDL energy exchange, a novel decentralized peer-to-peer (P2P) trading mechanism is proposed, in which EVs directly negotiate with a WCDL to reach consensus on the energy price and amounts to be traded. Those energy price and amounts are solutions of an optimization problem aiming at optimizing private cost functions of EVs and WCDL. The negotiation process between EVs and WCDL is secured by a privacy-preserving consensus mechanism. Further, to assure successful trading with desired energy price and amounts, an analytical and systematic method is proposed to select cost function parameters by EVs and WCDL in a fully decentralized manner. Simulations are then carried out to validate developed theoretical results, which confirm the effectiveness and scalability of the proposed algorithm.      
### 19.Nonlocal Adaptive Direction-Guided Structure Tensor Total Variation For Image Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2008.12505.pdf)
>  A common strategy in variational image recovery is utilizing the nonlocal self-similarity (NSS) property, when designing energy functionals. One such contribution is nonlocal structure tensor total variation (NLSTV), which lies at the core of this study. This paper is concerned with boosting the NLSTV regularization term through the use of directional priors. More specifically, NLSTV is leveraged so that, at each image point, it gains more sensitivity in the direction that is presumed to have the minimum local variation. The actual difficulty here is capturing this directional information from the corrupted image. In this regard, we propose a method that employs anisotropic Gaussian kernels to estimate directional features to be later used by our proposed model. The experiments validate that our entire two-stage framework achieves better results than the NLSTV model and two other competing local models, in terms of visual and quantitative evaluation.      
### 20.Synthesizing Averaged Virtual Oscillator Dynamics to Control Inverters with an Output LCL Filter  [ :arrow_down: ](https://arxiv.org/pdf/2008.12499.pdf)
>  In commercial inverters, an LCL filter is considered an integral part to filter out the switching harmonics and generate a sinusoidal output voltage. The existing literature on the averaged virtual oscillator controller (VOC) dynamics is for current feedback before the output LCL filter that contains the switching harmonics or for inductive filters ignoring the effect of filter capacitance. In this work, a new version of averaged VOC dynamics is presented for islanded inverters with current feedback after the LCL filter thus avoiding the switching harmonics going into the VOC. The embedded droop-characteristics within the averaged VOC dynamics are identified and a parameter design procedure is presented to regulate the output voltage magnitude and frequency according to the desired ac-performance specifications. Further, a power dispatch technique based on this newer version of averaged VOC dynamics is presented to simultaneously regulate both the active and reactive output power of two parallel-connected islanded inverters. The control laws are derived and a power security constraint is presented to determine the achievable power set-point. Simulation results for load transients and power dispatch validate the proposed version of averaged VOC dynamics.      
### 21.DALE : Dark Region-Aware Low-light Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2008.12493.pdf)
>  In this paper, we present a novel low-light image enhancement method called dark region-aware low-light image enhancement (DALE), where dark regions are accurately recognized by the proposed visual attention module and their brightness are intensively enhanced. Our method can estimate the visual attention in an efficient manner using super-pixels without any complicated process. Thus, the method can preserve the color, tone, and brightness of original images and prevents normally illuminated areas of the images from being saturated and distorted. Experimental results show that our method accurately identifies dark regions via the proposed visual attention, and qualitatively and quantitatively outperforms state-of-the-art methods.      
### 22.Decoding Visual Recognition of Objects from EEG Signals based on Attention-Driven Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.12490.pdf)
>  The ability to perceive and recognize objects is fundamental for the interaction with the external environment. Studies that investigate them and their relationship with brain activity changes have been increasing due to the possible application in an intuitive brain-machine interface (BMI). In addition, the distinctive patterns when presenting different visual stimuli that make data differentiable enough to be classified have been studied. However, reported classification accuracy still low or employed techniques for obtaining brain signals are impractical to use in real environments. In this study, we aim to decode electroencephalography (EEG) signals depending on the provided visual stimulus. Subjects were presented with 72 photographs belonging to 6 different semantic categories. We classified 6 categories and 72 exemplars according to visual stimuli using EEG signals. In order to achieve a high classification accuracy, we proposed an attention driven convolutional neural network and compared our results with conventional methods used for classifying EEG signals. We reported an accuracy of 50.37% and 26.75% for 6-class and 72-class, respectively. These results statistically outperformed other conventional methods. This was possible because of the application of the attention network using human visual pathways. Our findings showed that EEG signals are possible to differentiate when subjects are presented with visual stimulus of different semantic categories and at an exemplar-level with a high classification accuracy; this demonstrates its viability to be applied it in a real-world BMI.      
### 23.Classification of Imagined Speech Using Siamese Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.12487.pdf)
>  Imagined speech is spotlighted as a new trend in the brain-machine interface due to its application as an intuitive communication tool. However, previous studies have shown low classification performance, therefore its use in real-life is not feasible. In addition, no suitable method to analyze it has been found. Recently, deep learning algorithms have been applied to this paradigm. However, due to the small amount of data, the increase in classification performance is limited. To tackle these issues, in this study, we proposed an end-to-end framework using Siamese neural network encoder, which learns the discriminant features by considering the distance between classes. The imagined words (e.g., arriba (up), abajo (down), derecha (right), izquierda (left), adelante (forward), and atrás (backward)) were classified using the raw electroencephalography (EEG) signals. We obtained a 6-class classification accuracy of 31.40% for imagined speech, which significantly outperformed other methods. This was possible because the Siamese neural network, which increases the distance between dissimilar samples while decreasing the distance between similar samples, was used. In this regard, our method can learn discriminant features from a small dataset. The proposed framework would help to increase the classification performance of imagined speech for a small amount of data and implement an intuitive communication system.      
### 24.Human Blastocyst Classification after In Vitro Fertilization Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.12480.pdf)
>  Embryo quality assessment after in vitro fertilization (IVF) is primarily done visually by embryologists. Variability among assessors, however, remains one of the main causes of the low success rate of IVF. This study aims to develop an automated embryo assessment based on a deep learning model. This study includes a total of 1084 images from 1226 embryos. The images were captured by an inverted microscope at day 3 after fertilization. The images were labelled based on Veeck criteria that differentiate embryos to grade 1 to 5 based on the size of the blastomere and the grade of fragmentation. Our deep learning grading results were compared to the grading results from trained embryologists to evaluate the model performance. Our best model from fine-tuning a pre-trained ResNet50 on the dataset results in 91.79% accuracy. The model presented could be developed into an automated embryo assessment method in point-of-care settings.      
### 25.Digital pathology-based study of cell- and tissue-level morphologic features in serous borderline ovarian tumor and high-grade serous ovarian cancer  [ :arrow_down: ](https://arxiv.org/pdf/2008.12479.pdf)
>  Serous borderline ovarian tumor (SBOT) and high-grade serous ovarian cancer (HGSOC) are two distinct subtypes of epithelial ovarian tumors, with markedly different biologic background, behavior, prognosis, and treatment. However, the histologic diagnosis of serous ovarian tumors can be subjectively variable and labor-intensive as multiple tumor slides/blocks need to be thoroughly examined to search for these features. In this study, we aimed to evaluate technical feasibility of using digital pathological approaches to facilitate objective and scalable diagnosis screening for SBOT and HGSOC. Based on Groovy scripts and QuPath, a novel informatics system was developed to facilitate interactive annotation and imaging data exchange for machine learning purposes. Through this developed system, cellular boundaries were detected and expanded set of cellular features were extracted to represent cell- and tissue-level characteristics. According to our evaluation, cell-level classification was accurately achieved for both tumor and stroma cells with greater than 90% accuracy. Upon further re-examinations, 44.2% of the misclassified cells were due to over-/under-segmentations or low-quality of imaging areas. For a total number of 6,485 imaging patches with sufficient tumor and stroma cells (ten of each at least), we achieved 91-95% accuracy to differentiate HGSOC v. SBOT. When all the patches were considered for a WSI to make consensus prediction, 97% accuracy was achieved for accurately classifying all patients, indicating that cellular features digitally extracted from pathological images can be used for cell classification and SBOT v. HGSOC differentiation. Introducing digital pathology into ovarian cancer research could be beneficial to discover potential clinical implications.      
### 26.Counter-Unmanned Aircraft System(s) (C-UAS): State of the Art, Challenges and Future Trends  [ :arrow_down: ](https://arxiv.org/pdf/2008.12461.pdf)
>  Unmanned aircraft systems (UAS), or unmanned aerial vehicles (UAVs), often referred to as drones, have been experiencing healthy growth in the United States and around the world. The positive uses of UAS have the potential to save lives, increase safety and efficiency, and enable more effective science and engineering research. However, UAS are subject to threats stemming from increasing reliance on computer and communication technologies, which place public safety, national security, and individual privacy at risk. To promote safe, secure and privacy-respecting UAS operations, there is an urgent need for innovative technologies for detecting, tracking, identifying and mitigating UAS. A Counter-UAS (C-UAS) system is defined as a system or device capable of lawfully and safely disabling, disrupting, or seizing control of an unmanned aircraft or unmanned aircraft system. Over the past 5 years, significant research efforts have been made to detect, and mitigate UAS: detection technologies are based on acoustic, vision, passive radio frequency, radar, and data fusion; and mitigation technologies include physical capture or jamming. In this paper, we provide a comprehensive survey of existing literature in the area of C-UAS, identify the challenges in countering unauthorized or unsafe UAS, and evaluate the trends of detection and mitigation for protecting against UAS-based threats. The objective of this survey paper is to present a systematic introduction of C-UAS technologies, thus fostering a research community committed to the safe integration of UAS into the airspace system.      
### 27.Data-Driven Security Assessment of the Electric Power System  [ :arrow_down: ](https://arxiv.org/pdf/2008.12429.pdf)
>  The transition to a new low emission energy future results in a changing mix of generation and load types due to significant growth in renewable energy penetration and reduction in system inertia due to the exit of ageing fossil fuel power plants. This increases technical challenges for electrical grid planning and operation. This study introduces a new decomposition approach to account for the system security for short term planning using conventional machine learning tools. The immediate value of this work is that it provides extendable and computationally efficient guidelines for using supervised learning tools to assess first swing transient stability status. To provide an unbiased evaluation of the final model fit on the training dataset, the proposed approach was examined on a previously unseen test set. It distinguished stable and unstable cases in the test set accurately, with only 0.57% error, and showed a high precision in predicting the time of instability, with 6.8% error and mean absolute error as small as 0.0145.      
### 28.Text-Conditioned Transformer for Automatic Pronunciation Error Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.12424.pdf)
>  Automatic pronunciation error detection (APED) plays an important role in the domain of language learning. As for the previous ASR-based APED methods, the decoded results need to be aligned with the target text so that the errors can be found out. However, since the decoding process and the alignment process are independent, the prior knowledge about the target text is not fully utilized. In this paper, we propose to use the target text as an extra condition for the Transformer backbone to handle the APED task. The proposed method can output the error states with consideration of the relationship between the input speech and the target text in a fully end-to-end fashion.Meanwhile, as the prior target text is used as a condition for the decoder input, the Transformer works in a feed-forward manner instead of autoregressive in the inference stage, which can significantly boost the speed in the actual deployment. We set the ASR-based Transformer as the baseline APED model and conduct several experiments on the L2-Arctic dataset. The results demonstrate that our approach can obtain 8.4\% relative improvement on the $F_1$ score metric.      
### 29.W-Net: Dense Semantic Segmentation of Subcutaneous Tissue in Ultrasound Images by Expanding U-Net to Incorporate Ultrasound RF Waveform Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.12413.pdf)
>  We present W-Net, a novel Convolution Neural Network (CNN) framework that employs raw ultrasound waveforms from each A-scan, typically referred to as ultrasound Radio Frequency (RF) data, in addition to the gray ultrasound image to semantically segment and label tissues. Unlike prior work, we seek to label every pixel in the image, without the use of a background class. To the best of our knowledge, this is also the first deep-learning or CNN approach for segmentation that analyses ultrasound raw RF data along with the gray image. International patent(s) pending [PCT/US20/37519]. We chose subcutaneous tissue (SubQ) segmentation as our initial clinical goal since it has diverse intermixed tissues, is challenging to segment, and is an underrepresented research area. SubQ potential applications include plastic surgery, adipose stem-cell harvesting, lymphatic monitoring, and possibly detection/treatment of certain types of tumors. A custom dataset consisting of hand-labeled images by an expert clinician and trainees are used for the experimentation, currently labeled into the following categories: skin, fat, fat fascia/stroma, muscle and muscle fascia. We compared our results with U-Net and Attention U-Net. Our novel \emph{W-Net}'s RF-Waveform input and architecture increased mIoU accuracy (averaged across all tissue classes) by 4.5\% and 4.9\% compared to regular U-Net and Attention U-Net, respectively. We present analysis as to why the Muscle fascia and Fat fascia/stroma are the most difficult tissues to label. Muscle fascia in particular, the most difficult anatomic class to recognize for both humans and AI algorithms, saw mIoU improvements of 13\% and 16\% from our W-Net vs U-Net and Attention U-Net respectively.      
### 30.Design and implementation in USRP of a preamble-based synchronizer for OFDM systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.12404.pdf)
>  The Orthogonal Frequency Division Multiplexing (OFDM) is one of the most widely adopted schemes in wireless technologies such as Wi-Fi and LTE due to its high transmission rates, and the robustness against Intersymbol Interference (ISI). However, OFDM is highly sensitive to synchronism errors, which affects the orthogonality of the carriers. We analyzed several synchronization algorithms based on the correlation of the preamble symbols through the implementation in Software-Defined Radio (SDR) using the Universal Software Radio Peripheral (USRP). Such an implementation was performed in three stages: frame detection, comparing the autocorrelation output and the average power of the received signal; time synchronism, where the cross-correlation based on the short and long preamble symbols was implemented; and the frequency synchronism, where the Carrier Frequency Offset (CFO) added by the channel was detected and corrected. The synchronizer performance was verified through the USRP implementation. The results serve as a practical guide to selecting the optimal synchronism scheme and show the versatility of the USRP to implement digital communication systems efficiently.      
### 31.Consensus for Clusters of Agents with Cooperative and Antagonistic Relationships  [ :arrow_down: ](https://arxiv.org/pdf/2008.12398.pdf)
>  In this paper we address the consensus problem in the context of networked agents whose communication graph can be split into a certain number of clusters in such a way that interactions between agents in the same clusters are cooperative, while interactions between agents belonging to different clusters are antagonistic. This problem set-up arises in the context of social networks and opinion dynamics, where reaching consensus means that the opinions of the agents in the same cluster converge to the same decision. The consensus problem is here investigated under the assumption that agents in the same cluster have the same constant and pre-fixed amount of trust (/distrust) to be distributed among their cooperators (/adversaries). The proposed solution establishes how much agents in the same group must be conservative about their opinions in order to converge to a common decision.      
### 32.Speech Sentiment and Customer Satisfaction Estimation in Socialbot Conversations  [ :arrow_down: ](https://arxiv.org/pdf/2008.12376.pdf)
>  For an interactive agent, such as task-oriented spoken dialog systems or chatbots, measuring and adapting to Customer Satisfaction (CSAT) is critical in order to understand user perception of an agent's behavior and increase user engagement and retention. However, an agent often relies on explicit customer feedback for measuring CSAT. Such explicit feedback may result in potential distraction to users and it can be challenging to capture continuously changing user's satisfaction. To address this challenge, we present a new approach to automatically estimate CSAT using acoustic and lexical information in the Alexa Prize Socialbot data. We first explore the relationship between CSAT and sentiment scores at both the utterance and conversation level. We then investigate static and temporal modeling methods that use estimated sentiment scores as a mid-level representation. The results show that the sentiment scores, particularly valence and satisfaction, are correlated with CSAT. We also demonstrate that our proposed temporal modeling approach for estimating CSAT achieves competitive performance, relative to static baselines as well as human performance. This work provides insights into open domain social conversations between real users and socialbots, and the use of both acoustic and lexical information for understanding the relationship between CSAT and sentiment scores.      
### 33.Improving the Segmentation of Scanning Probe Microscope Images using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.12371.pdf)
>  A wide range of techniques can be considered for segmentation of images of nanostructured surfaces. Manually segmenting these images is time-consuming and results in a user-dependent segmentation bias, while there is currently no consensus on the best automated segmentation methods for particular techniques, image classes, and samples. Any image segmentation approach must minimise the noise in the images to ensure accurate and meaningful statistical analysis can be carried out. Here we develop protocols for the segmentation of images of 2D assemblies of gold nanoparticles formed on silicon surfaces via deposition from an organic solvent. The evaporation of the solvent drives far-from-equilibrium self-organisation of the particles, producing a wide variety of nano- and micro-structured patterns. We show that a segmentation strategy using the U-Net convolutional neural network outperforms traditional automated approaches and has particular potential in the processing of images of nanostructured systems.      
### 34.Structured Autocorrelation Matrix Estimation for Coprime Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2008.12369.pdf)
>  A coprime array receiver processes a collection of received-signal snapshots to estimate the autocorrelation matrix of a larger (virtual) uniform linear array, known as coarray. By the received-signal model, this matrix has to be (i) Positive-Definite, (ii) Hermitian, (iii) Toeplitz, and (iv) its noise-subspace eigenvalues have to be equal. Existing coarray autocorrelation matrix estimates satisfy a subset of the above conditions. In this work, we propose an optimization framework which offers a novel estimate satisfying all four conditions. Numerical studies illustrate that the proposed estimate outperforms standard counterparts, both in autocorrelation matrix estimation error and Direction-of-Arrival estimation.      
### 35.Tracking the time course of reproduction number and lockdown's effect during SARS-CoV-2 epidemic: nonparametric estimation  [ :arrow_down: ](https://arxiv.org/pdf/2008.12337.pdf)
>  Accurate modeling of lockdown effects on SARS-CoV-2 epidemic evolution is a key issue in order e.g. to inform health-care decisions on emergency management. The compartmental and spatial models so far proposed use parametric descriptions of the contact rate, often assuming a time-invariant effect of the lockdown. In this paper we show that these assumptions may lead to erroneous evaluations on the ongoing pandemic. Thus, we develop a new class of nonparametric compartmental models able to describe how the impact of the lockdown varies in time. Exploiting regularization theory, hospitalized data are mapped into an infinite-dimensional space, hence obtaining a function which takes into account also how social distancing measures and people's growing awareness of infection's risk evolves as time progresses. This permits to reconstruct a continuous-time profile of SARS-CoV-2 reproduction number with a resolution never reached before. When applied to data collected in Lombardy, the most affected Italian region, our model illustrates how people behaviour changed during the restrictions and its importance to contain the epidemic. Results also indicate that, at the end of the lockdown, around 12% of people in Lombardy and 5% in Italy was affected by SARS-CoV-2. Then, we discuss how the situation evolved after the end of the lockdown showing that the reproduction number is dangerously increasing in the last weeks due to holiday relax especially in the younger population and increased migrants arrival, reaching values larger than one on August 1, 2020. Since several countries still observe a growing epidemic, including Italy, and all could be subject to a second wave after the summer, the proposed reproduction number tracking methodology can be of great help to health care authorities to prevent another SARS-CoV-2 diffusion or to assess the impact of lockdown restrictions to contain the spread.      
### 36.Link Budget Analysis for Reconfigurable Smart Surfaces in Aerial Platforms  [ :arrow_down: ](https://arxiv.org/pdf/2008.12334.pdf)
>  In this paper, we derive the link budget relations for communications assisted by reconfigurable smart surfaces (RSS). Specifically, under specular and scattering paradigms, we provide link budget expressions for an RSS-assisted communication on the ground, where the RSS is either mounted on a building, or on an aerial platform, such as an unmanned aerial vehicle (UAV), a high altitude platform station (HAPS), or a low-earth orbit satellite (LEO). The obtained numerical results provide design guidelines for RSS-assisted communication systems, including the recommended aerial platform to use, the size of RSS for each type of the platforms, and the operating frequencies.      
### 37.Dynamic Graph Neural Network for Traffic Forecasting in Wide Area Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.12767.pdf)
>  Wide area networking infrastructures (WANs), particularly science and research WANs, are the backbone for moving large volumes of scientific data between experimental facilities and data centers. With demands growing at exponential rates, these networks are struggling to cope with large data volumes, real-time responses, and overall network performance. Network operators are increasingly looking for innovative ways to manage the limited underlying network resources. Forecasting network traffic is a critical capability for proactive resource management, congestion mitigation, and dedicated transfer provisioning. To this end, we propose a nonautoregressive graph-based neural network for multistep network traffic forecasting. Specifically, we develop a dynamic variant of diffusion convolutional recurrent neural networks to forecast traffic in research WANs. We evaluate the efficacy of our approach on real traffic from ESnet, the U.S. Department of Energy's dedicated science network. Our results show that compared to classical forecasting methods, our approach explicitly learns the dynamic nature of spatiotemporal traffic patterns, showing significant improvements in forecasting accuracy. Our technique can surpass existing statistical and deep learning approaches by achieving approximately 20% mean absolute percentage error for multiple hours of forecasts despite dynamic network traffic settings.      
### 38.Non-Local Musical Statistics as Guides for Audio-to-Score Piano Transcription  [ :arrow_down: ](https://arxiv.org/pdf/2008.12710.pdf)
>  We present an automatic piano transcription system that converts polyphonic audio recordings into musical scores. This has been a long-standing problem of music information processing and the two main components, multipitch detection and rhythm quantization, have been studied actively. Given the recent remarkable progress in these domains, we study a method integrating deep-neural-network-based multipitch detection and statistical-model-based rhythm quantization. In the first part of the study, we conducted systematic evaluations and found that while the present method achieved high transcription accuracies at the note level, global characteristics such as tempo scale, metre (time signature), and bar line positions were often incorrectly estimated. In the second part, we formulated non-local statistics of pitch and rhythmic content that are derived from musical knowledge and studied their effects in inferring those global characteristics. We found an optimal combination of these statistics for significantly improving the transcription results, which suggests using statistics obtained from separated hand parts. The integrated method can generate transcriptions that can be partially used for music performance and assisting human transcribers, which demonstrates the potential for a practical audio-to-score piano transcription system.      
### 39.Comparison Between Genetic Fuzzy Methodology and Q-learning for Collaborative Control Design  [ :arrow_down: ](https://arxiv.org/pdf/2008.12678.pdf)
>  A comparison between two machine learning approaches viz., Genetic Fuzzy Methodology and Q-learning, is presented in this paper. The approaches are used to model controllers for a set of collaborative robots that need to work together to bring an object to a target position. The robots are fixed and are attached to the object through elastic cables. A major constraint considered in this problem is that the robots cannot communicate with each other. This means that at any instant, each robot has no motion or control information of the other robots and it can only pull or release its cable based only on the motion states of the object. This decentralized control problem provides a good example to test the capabilities and restrictions of these two machine learning approaches. The system is first trained using a set of training scenarios and then applied to an extensive test set to check the generalization achieved by each method.      
### 40.Data-driven control on encrypted data  [ :arrow_down: ](https://arxiv.org/pdf/2008.12671.pdf)
>  We provide an efficient and private solution to the problem of encryption-aware data-driven control. We investigate a Control as a Service scenario, where a client employs a specialized outsourced control solution from a service provider. The privacy-sensitive model parameters of the client's system are either not available or variable. Hence, we require the service provider to perform data-driven control in a privacy-preserving manner on the input-output data samples from the client. To this end, we co-design the control scheme with respect to both control performance and privacy specifications. First, we formulate our control algorithm based on recent results from the behavioral framework, and we prove closeness between the classical formulation and our formulation that accounts for noise and precision errors arising from encryption. Second, we use a state-of-the-art leveled homomorphic encryption scheme to enable the service provider to perform high complexity computations on the client's encrypted data, ensuring privacy. Finally, we streamline our solution by exploiting the rich structure of data, and meticulously employing ciphertext batching and rearranging operations to enable parallelization. This solution achieves more than twofold runtime and memory improvements compared to our prior work.      
### 41.Defending Water Treatment Networks: Exploiting Spatio-temporal Effects for Cyber Attack Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.12618.pdf)
>  While Water Treatment Networks (WTNs) are critical infrastructures for local communities and public health, WTNs are vulnerable to cyber attacks. Effective detection of attacks can defend WTNs against discharging contaminated water, denying access, destroying equipment, and causing public fear. While there are extensive studies in WTNs attack detection, they only exploit the data characteristics partially to detect cyber attacks. After preliminary exploring the sensing data of WTNs, we find that integrating spatio-temporal knowledge, representation learning, and detection algorithms can improve attack detection accuracy. To this end, we propose a structured anomaly detection framework to defend WTNs by modeling the spatio-temporal characteristics of cyber attacks in WTNs. In particular, we propose a spatio-temporal representation framework specially tailored to cyber attacks after separating the sensing data of WTNs into a sequence of time segments. This framework has two key components. The first component is a temporal embedding module to preserve temporal patterns within a time segment by projecting the time segment of a sensor into a temporal embedding vector. We then construct Spatio-Temporal Graphs (STGs), where a node is a sensor and an attribute is the temporal embedding vector of the sensor, to describe the state of the WTNs. The second component is a spatial embedding module, which learns the final fused embedding of the WTNs from STGs. In addition, we devise an improved one class-SVM model that utilizes a new designed pairwise kernel to detect cyber attacks. The devised pairwise kernel augments the distance between normal and attack patterns in the fused embedding space. Finally, we conducted extensive experimental evaluations with real-world data to demonstrate the effectiveness of our framework.      
### 42.A Realistic Fish-Habitat Dataset to Evaluate Algorithms for Underwater Visual Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.12603.pdf)
>  Visual analysis of complex fish habitats is an important step towards sustainable fisheries for human consumption and environmental protection. Deep Learning methods have shown great promise for scene analysis when trained on large-scale datasets. However, current datasets for fish analysis tend to focus on the classification task within constrained, plain environments which do not capture the complexity of underwater fish habitats. To address this limitation, we present DeepFish as a benchmark suite with a large-scale dataset to train and test methods for several computer vision tasks. The dataset consists of approximately 40 thousand images collected underwater from 20 \green{habitats in the} marine-environments of tropical Australia. The dataset originally contained only classification labels. Thus, we collected point-level and segmentation labels to have a more comprehensive fish analysis benchmark. These labels enable models to learn to automatically monitor fish count, identify their locations, and estimate their sizes. Our experiments provide an in-depth analysis of the dataset characteristics, and the performance evaluation of several state-of-the-art approaches based on our benchmark. Although models pre-trained on ImageNet have successfully performed on this benchmark, there is still room for improvement. Therefore, this benchmark serves as a testbed to motivate further development in this challenging domain of underwater computer vision. Code is available at: <a class="link-external link-https" href="https://github.com/alzayats/DeepFish" rel="external noopener nofollow">this https URL</a>      
### 43.Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows  [ :arrow_down: ](https://arxiv.org/pdf/2008.12577.pdf)
>  The detection of manufacturing errors is crucial in fabrication processes to ensure product quality and safety standards. Since many defects occur very rarely and their characteristics are mostly unknown a priori, their detection is still an open research question. To this end, we propose DifferNet: It leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using normalizing flows. Normalizing flows are well-suited to deal with low dimensional data distributions. However, they struggle with the high dimensionality of images. Therefore, we employ a multi-scale feature extractor which enables the normalizing flow to assign meaningful likelihoods to the images. Based on these likelihoods we develop a scoring function that indicates defects. Moreover, propagating the score back to the image enables pixel-wise localization. To achieve a high robustness and performance we exploit multiple transformations in training and evaluation. In contrast to most other methods, ours does not require a large number of training samples and performs well with as low as 16 images. We demonstrate the superior performance over existing approaches on the challenging and newly proposed MVTec AD and Magnetic Tile Defects datasets.      
### 44.Investigating Taxi and Uber competition in New York City: Multi-agent modeling by reinforcement-learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.12530.pdf)
>  The taxi business has been overly regulated for many decades. Regulations are supposed to ensure safety and fairness within a controlled competitive environment. By influencing both drivers and riders choices and behaviors, emerging e-hailing services (e.g., Uber and Lyft) have been reshaping the existing competition in the last few years. This study investigates the existing competition between the mainstream hailing services (i.e., Yellow and Green Cabs) and e-hailing services (i.e., Uber) in New York City. Their competition is investigated in terms of market segmentation, emerging demands, and regulations. Data visualization techniques are employed to find existing and new patterns in travel behavior. For this study, we developed a multi-agent model and applied reinforcement learning techniques to imitate drivers behaviors. The model is verified by the patterns recognized in our data visualization results. The model is then used to evaluate multiple new regulations and competition scenarios. Results of our study illustrate that e-hailers dominate low-travel-density areas (e.g., residential areas), and that e-hailers quickly identify and respond to change in travel demand. This leads to diminishing market size for hailers. Furthermore, our results confirm the indirect impact of Green Cabs regulations on the existing competition. This investigation, along with our proposed scenarios, can aid policymakers and authorities in designing policies that could effectively address demand while assuring a healthy competition for the hailing and e-haling sectors. <br>Keywords: taxi; Uber, policy; E-hailing; multi-agent simulation; reinforcement learning;      
### 45.Color and Edge-Aware Adversarial Image Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2008.12454.pdf)
>  Adversarial perturbation of images, in which a source image is deliberately modified with the intent of causing a classifier to misclassify the image, provides important insight into the robustness of image classifiers. In this work we develop two new methods for constructing adversarial perturbations, both of which are motivated by minimizing human ability to detect changes between the perturbed and source image. The first of these, the Edge-Aware method, reduces the magnitude of perturbations permitted in smooth regions of an image where changes are more easily detected. Our second method, the Color-Aware method, performs the perturbation in a color space which accurately captures human ability to distinguish differences in colors, thus reducing the perceived change. The Color-Aware and Edge-Aware methods can also be implemented simultaneously, resulting in image perturbations which account for both human color perception and sensitivity to changes in homogeneous regions. Though Edge-Aware and Color-Aware modifications exist for many image perturbations techniques, we focus on easily computed perturbations. We empirically demonstrate that the Color-Aware and Edge-Aware perturbations we consider effectively cause misclassification, are less distinguishable to human perception, and are as easy to compute as the most efficient image perturbation techniques. Code and demo available at <a class="link-external link-https" href="https://github.com/rbassett3/Color-and-Edge-Aware-Perturbations" rel="external noopener nofollow">this https URL</a>      
### 46.Long-term map maintenance pipeline for autonomous vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.12449.pdf)
>  For autonomous vehicles to operate persistently in a typical urban environment, it is essential to have high accuracy position information. This requires a mapping and localisation system that can adapt to changes over time. A localisation approach based on a single-survey map will not be suitable for long-term operation as it does not incorporate variations in the environment. In this paper, we present new algorithms to maintain a featured-based map. A map maintenance pipeline is proposed that can continuously update a map with the most relevant features taking advantage of the changes in the surroundings. Our pipeline detects and removes transient features based on their geometrical relationships with the vehicle's pose. Newly identified features became part of a new feature map and are assessed by the pipeline as candidates for the localisation map. By purging out-of-date features and adding newly detected features, we continually update the prior map to more accurately represent the most recent environment. We have validated our approach using the USyd Campus Dataset, which includes more than 18 months of data. The results presented demonstrate that our maintenance pipeline produces a resilient map which can provide sustained localisation performance over time.      
### 47.Deep sr-DDL: Deep Structurally Regularized Dynamic Dictionary Learning to Integrate Multimodal and Dynamic Functional Connectomics data for Multidimensional Clinical Characterizations  [ :arrow_down: ](https://arxiv.org/pdf/2008.12410.pdf)
>  We propose a novel integrated framework that jointly models complementary information from resting-state functional MRI (rs-fMRI) connectivity and diffusion tensor imaging (DTI) tractography to extract biomarkers of brain connectivity predictive of behavior. Our framework couples a generative model of the connectomics data with a deep network that predicts behavioral scores. The generative component is a structurally-regularized Dynamic Dictionary Learning (sr-DDL) model that decomposes the dynamic rs-fMRI correlation matrices into a collection of shared basis networks and time varying subject-specific loadings. We use the DTI tractography to regularize this matrix factorization and learn anatomically informed functional connectivity profiles. The deep component of our framework is an LSTM-ANN block, which uses the temporal evolution of the subject-specific sr-DDL loadings to predict multidimensional clinical characterizations. Our joint optimization strategy collectively estimates the basis networks, the subject-specific time-varying loadings, and the neural network weights. We validate our framework on a dataset of neurotypical individuals from the Human Connectome Project (HCP) database to map to cognition and on a separate multi-score prediction task on individuals diagnosed with Autism Spectrum Disorder (ASD) in a five-fold cross validation setting. Our hybrid model outperforms several state-of-the-art approaches at clinical outcome prediction and learns interpretable multimodal neural signatures of brain organization.      
### 48.Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2008.12380.pdf)
>  Fluorescence microscopy allows for a detailed inspection of cells, cellular networks, and anatomical landmarks by staining with a variety of carefully-selected markers visualized as color channels. Quantitative characterization of structures in acquired images often relies on automatic image analysis methods. Despite the success of deep learning methods in other vision applications, their potential for fluorescence image analysis remains underexploited. One reason lies in the considerable workload required to train accurate models, which are normally specific for a given combination of markers, and therefore applicable to a very restricted number of experimental settings. We herein propose Marker Sampling and Excite, a neural network approach with a modality sampling strategy and a novel attention module that together enable ($i$) flexible training with heterogeneous datasets with combinations of markers and ($ii$) successful utility of learned models on arbitrary subsets of markers prospectively. We show that our single neural network solution performs comparably to an upper bound scenario where an ensemble of many networks is naïvely trained for each possible marker combination separately. In addition, we demonstrate the feasibility of our framework in high-throughput biological analysis by revising a recent quantitative characterization of bone marrow vasculature in 3D confocal microscopy datasets. Not only can our work substantially ameliorate the use of deep learning in fluorescence microscopy analysis, but it can also be utilized in other fields with incomplete data acquisitions and missing modalities.      
### 49.Motion correction for PET using subspace-based real-time MR imaging in simultaneous PET/MR  [ :arrow_down: ](https://arxiv.org/pdf/2008.12359.pdf)
>  Image quality of PET reconstructions is degraded by subject motion occurring during the acquisition. MR-based motion correction approaches have been studied for PET/MR scanners and have been successful at capturing regular motion patterns, when used in conjunction with surrogate signals (e.g. navigators) to detect motion. However, handling irregular respiratory motion and bulk motion remains challenging. In this work, we propose an MR-based motion correction method relying on subspace-based real-time MR imaging to estimate motion fields used to correct PET reconstructions. We take advantage of the low-rank characteristics of dynamic MR images to reconstruct high-resolution MR images at high frame rates from highly undersampled k-space data. Reconstructed dynamic MR images are used to determine motion phases for PET reconstruction and estimate phase-to-phase nonrigid motion fields able to capture complex motion patterns such as irregular respiratory and bulk motion. MR-derived binning and motion fields are used for PET reconstruction to generate motion-corrected PET images. The proposed method was evaluated on in vivo data with irregular motion patterns. MR reconstructions accurately captured motion, outperforming state-of-the-art dynamic MR reconstruction techniques. Evaluation of PET reconstructions demonstrated the benefits of the proposed method over standard methods in terms of motion artifact reduction. The proposed method can improve the image quality of motion-corrected PET reconstructions in clinical applications.      
### 50.A Scene-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video  [ :arrow_down: ](https://arxiv.org/pdf/2008.12328.pdf)
>  Abnormal event detection in video is a complex computer vision problem that has attracted significant attention in recent years. The complexity of the task arises from the commonly-agreed definition of an abnormal event, that is, a rarely occurring event that typically depends on the surrounding context. Following the standard formulation of abnormal event detection as outlier detection, we propose a scene-agnostic framework that learns from training videos containing only normal events. Our framework is composed of an object detector, a set of appearance and motion auto-encoders, and a discriminator. Since our framework only looks at object detections, it can be applied to different scenes, provided that abnormal events are defined identically across scenes. This makes our method scene agnostic, as we rely strictly on objects that can cause anomalies, and not on the background. To overcome the lack of abnormal data during training, we propose an adversarial learning strategy for the auto-encoders. We create a scene-agnostic set of out-of-domain adversarial examples, which are correctly reconstructed by the auto-encoders before applying gradient ascent on the adversarial examples. We further utilize the adversarial examples to serve as abnormal examples when training a binary classifier to discriminate between normal and abnormal latent features and reconstructions. Furthermore, to ensure that the auto-encoders focus only on the main object inside each bounding box image, we introduce a branch that learns to segment the main object. We compare our framework with the state-of-the-art methods on three benchmark data sets, using various evaluation metrics. Compared to existing methods, the empirical results indicate that our approach achieves favorable performance on all data sets.      
### 51.Learning Compact Physics-Aware Delayed Photocurrent Models Using Dynamic Mode Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2008.12319.pdf)
>  Radiation-induced photocurrent in semiconductor devices can be simulated using complex physics-based models, which are accurate, but computationally expensive. This presents a challenge for implementing device characteristics in high-level circuit simulations where it is computationally infeasible to evaluate detailed models for multiple individual circuit elements. In this work we demonstrate a procedure for learning compact delayed photocurrent models that are efficient enough to implement in large-scale circuit simulations, but remain faithful to the underlying physics. Our approach utilizes Dynamic Mode Decomposition (DMD), a system identification technique for learning reduced order discrete-time dynamical systems from time series data based on singular value decomposition. To obtain physics-aware device models, we simulate the excess carrier density induced by radiation pulses by solving numerically the Ambipolar Diffusion Equation, then use the simulated internal state as training data for the DMD algorithm. Our results show that the significantly reduced order delayed photocurrent models obtained via this method accurately approximate the dynamics of the internal excess carrier density -- which can be used to calculate the induced current at the device boundaries -- while remaining compact enough to incorporate into larger circuit simulations.      
