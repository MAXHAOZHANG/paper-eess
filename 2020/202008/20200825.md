# ArXiv eess --Tue, 25 Aug 2020
### 1.Joint Design of RF and gradient waveforms via auto-differentiation for 3D tailored excitation in MRI  [ :arrow_down: ](https://arxiv.org/pdf/2008.10594.pdf)
>  This paper proposes a new method for joint design of radiofrequency (RF) and gradient waveforms in Magnetic Resonance Imaging (MRI), and applies it to the design of 3D spatially tailored saturation and inversion pulses. The joint design of both waveforms is characterized by the ODE Bloch equations, to which there is no known direct solution. Existing approaches therefore typically rely on simplified problem formulations based on, e.g., the small-tip approximation or constraining the gradient waveforms to particular shapes, and often apply only to specific objective functions for a narrow set of design goals (e.g., ignoring hardware constraints). This paper develops and exploits an auto-differentiable Bloch simulator to directly obtain Jacobians of the (Bloch-simulated) excitation pattern with respect to RF and gradient waveforms. This approach is compatible with arbitrary sub-differentiable loss functions, and optimizes the RF and gradients directly without restricting the waveform shapes. To reduce computation, we derive and implement explicit Bloch simulator Jacobians (approximately halving computation time and memory usage). To enforce hardware limits (peak RF, gradient, and slew rate), we use a change of variables that makes the 3D pulse design problem effectively unconstrained; we then optimize the resulting problem directly using the proposed auto-differentiation framework. We demonstrate our approach with two kinds of 3D excitation pulses that cannot be easily designed with conventional approaches: Outer-volume saturation (90Â° flip angle), and inner-volume inversion.      
### 2.Accurate Alignment Inspection System for Low-resolution Automotive and Mobility LiDAR  [ :arrow_down: ](https://arxiv.org/pdf/2008.10584.pdf)
>  A misalignment of LiDAR as low as a few degrees could cause a significant error in obstacle detection and mapping that could cause safety and quality issues. In this paper, an accurate inspection system is proposed for estimating a LiDAR alignment error after sensor attachment on a mobility system such as a vehicle or robot. The proposed method uses only a single target board at the fixed position to estimate the three orientations (roll, tilt, and yaw) and the horizontal position of the LiDAR attachment with sub-degree and millimeter level accuracy. After the proposed preprocessing steps, the feature beam points that are the closest to each target corner are extracted and used to calculate the sensor attachment pose with respect to the target board frame using a nonlinear optimization method and with a low computational cost. The performance of the proposed method is evaluated using a test bench that can control the reference yaw and horizontal translation of LiDAR within ranges of 3 degrees and 30 millimeters, respectively. The experimental results for a low-resolution 16 channel LiDAR (Velodyne VLP-16) confirmed that misalignment could be estimated with accuracy within 0.2 degrees and 4 mm. The high accuracy and simplicity of the proposed system make it practical for large-scale industrial applications such as automobile or robot manufacturing process that inspects the sensor attachment for the safety quality control.      
### 3.Automatic LiDAR Extrinsic Calibration System using Photodetector and Planar Board for Large-scale Applications  [ :arrow_down: ](https://arxiv.org/pdf/2008.10542.pdf)
>  This paper presents a novel automatic calibration system to estimate the extrinsic parameters of LiDAR mounted on a mobile platform for sensor misalignment inspection in the large-scale production of highly automated vehicles. To obtain subdegree and subcentimeter accuracy levels of extrinsic calibration, this study proposed a new concept of a target board with embedded photodetector arrays, named the PD-target system, to find the precise position of the correspondence laser beams on the target surface. Furthermore, the proposed system requires only the simple design of the target board at the fixed pose in a close range to be readily applicable in the automobile manufacturing environment. The experimental evaluation of the proposed system on low-resolution LiDAR showed that the LiDAR offset pose can be estimated within 0.1 degree and 3 mm levels of precision. The high accuracy and simplicity of the proposed calibration system make it practical for large-scale applications for the reliability and safety of autonomous systems.      
### 4.Structural Analysis for Fault Diagnosis and Sensor Placement in Battery Packs  [ :arrow_down: ](https://arxiv.org/pdf/2008.10533.pdf)
>  Energy storage systems for transportation and grid applications, and in the future for aeronautical applications, require the ability of providing accurate diagnosis to insure system availability and reliability. In such applications, battery packs may consist of hundreds or thousands of interconnected cells,and of the associated electrical/electronic hardware. This paper presents a systematic methodology for approaching some aspects of the design of battery packs, and in particular the development of diagnostic strategies, using cell models and structural diagnosis methods. First, the analytical redundancy that is intrinsic in the battery system is determined. Then, graph-theoretic tools are used to construct general structural models of two common battery pack topologies, and illustrate how the redundancy present in different measurements (current, voltage, and temperature) can be used to improve monitoring and diagnosis of a battery system. Possible sensor placement strategies that would enable the diagnosis of individual sensor faults and individual cell faults for different battery topologies are analyzed as well.While the work presented in this paper is only one step in the design of a large battery pack design, it is an important and needed advancement.      
### 5.Stochastic Hybrid Combining Design for Quantized Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.10520.pdf)
>  Both the power-dissipation and cost of massive multiple-input multiple-output (mMIMO) systems may be substantially reduced by using low-resolution analog-to-digital converters (LADCs) at the receivers. However, both the coarse quantization of LADCs and the inaccurate instantaneous channel state information (ICSI) degrade the performance of quantized mMIMO systems. To overcome these challenges, we propose a novel stochastic hybrid analog-digital combiner (SHC) scheme for adapting the hybrid combiner to the long-term statistics of the channel state information (SCSI). We seek to minimize the transmit power by jointly optimizing the SHC subject to average rate constraints. For the sake of solving the resultant nonconvex stochastic optimization problem, we develop a relaxed stochastic successive convex approximation (RSSCA) algorithm. Simulations are carried out to confirm the benefits of our proposed scheme over the benchmarkers.      
### 6.Improving Tail Performance of a Deliberation E2E ASR Model Using a LargeText Corpus  [ :arrow_down: ](https://arxiv.org/pdf/2008.10491.pdf)
>  End-to-end (E2E) automatic speech recognition (ASR) systems lack the distinct language model (LM) component that characterizes traditional speech systems. While this simplifies the model architecture, it complicates the task of incorporating text-only data into training, which is important to the recognition of tail words that do not occur often in audio-text pairs. While shallow fusion has been proposed as a method for incorporating a pre-trained LM into an E2E model at inference time, it has not yet been explored for very large text corpora, and it has been shown to be very sensitive to hyperparameter settings in the beam search. In this work, we apply shallow fusion to incorporate a very large text corpus into a state-of-the-art E2EASR model. We explore the impact of model size and show that intelligent pruning of the training set can be more effective than increasing the parameter count. Additionally, we show that incorporating the LM in minimum word error rate (MWER) fine tuning makes shallow fusion far less dependent on optimal hyperparameter settings, reducing the difficulty of that tuning problem.      
### 7.Dynamic Attitude Estimation Improvement for Low-cost MEMS IMU by Integrating Low-cost GPS  [ :arrow_down: ](https://arxiv.org/pdf/2008.10469.pdf)
>  This paper proposes a low-cost six Degree-of-Freedom (6-DOF) navigation system for small aerial robots based on the integration of Global Position System (GPS) receiver with sensors of inertional Microelectromechanical Systems (MEMS). In the problem of fusing Inertial Measurement Unit (IMU) with low-cost GPS, the effect of time synchronization error on attitude estimation is concerned. A fusion algorithm which can estimate the motion states and the time synchronization error simultaneously is proposed. This algorithm adds a time estimation loop to improve estimation accuracy. Compared with another states augmented estimation approach, this method has the advantages of lower computation burden, avoidance of the discretization error in the low sample rate. The estimation algorithm is implemented in an low-cost embedded microprocessor where the update rate of algorithm can achieve more than 100 Hz, and therefore high-performance computational units are not necessary. In robotic experiment, the proposed algorithm serves as the navigation solution for a small aerial robot. The accuracy and reliability of the self-designed system are tested when the robot is moving with significant acceleration.      
### 8.On-line Capacity Estimation for Lithium-ion Battery Cells via an Electrochemical Model-based Adaptive Interconnected Observer  [ :arrow_down: ](https://arxiv.org/pdf/2008.10467.pdf)
>  Battery aging is a natural process that contributes to capacity and power fade, resulting in a gradual performance degradation over time and usage. State of Charge (SOC) and State of Health (SOH) monitoring of an aging battery poses a challenging task to the Battery Management System (BMS) due to the lack of direct measurements. Estimation algorithms based on an electrochemical model that take into account the impact of aging on physical battery parameters can provide accurate information on lithium concentration and cell capacity over a battery's usable lifespan. A temperature-dependent electrochemical model, the Enhanced Single Particle Model (ESPM), forms the basis for the synthesis of an adaptive interconnected observer that exploits the relationship between capacity and power fade, due to the growth of Solid Electrolyte Interphase layer (SEI), to enable combined estimation of states (lithium concentration in both electrodes and cell capacity) and aging-sensitive transport parameters (anode diffusion coefficient and SEI layer ionic conductivity). The practical stability conditions for the adaptive observer are derived using Lyapunov's theory. Validation results against experimental data show a bounded capacity estimation error within 2% of its true value. Further, effectiveness of capacity estimation is tested for two cells at different stages of aging. Robustness of capacity estimates under measurement noise and sensor bias are studied.      
### 9.Modeling Communication Networks in a Real-Time Simulation Environment for Evaluating Controls of Shipboard Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.10441.pdf)
>  Interest by the U.S. Navy in the development and deployment of advanced controls in future shipboard platforms has motivated the development of the Controls Evaluation Framework (CEF) for use in investigating dynamics present in complex automated systems. This paper reports on the implementation and investigation of a communication network component within the CEF. This implementation is designed to augment the CEF's available feature set, permitting the exploration of various communication conditions on advanced control performance. Results obtained from controller hardware-in-the-loop testing are presented and analyzed to demonstrate performance characteristics pertaining to the implemented module.      
### 10.An Analysis of Sampling Effect on the Absolute Stability of Discrete-time Bilateral Teleoperation Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.10439.pdf)
>  Absolute stability of discrete-time teleoperation systems can be jeopardized by choosing inappropriate sampling time architecture. A modified structure is presented for the bilateral teleoperation system including continuous-time slave robot, master robot, human operator, and the environment with sampled-data PD-like + dissipation controllers which make the system absolute stable in the presence of the time delay and sampling rates in the communication network. The output position and force signals are quantized with uniform sampling periods. Input-delay approach is used in this paper to convert the sampled-data system to a continuous-time counterpart. The main contribution of this paper is calculating a lower bound on the maximum sampling period as a stability condition. Also, the presented method imposes upper bounds on the damping of robots and notifies the sampling time importance on the transparency and stability of the system. Both simulation and experimental results are performed to show the validity of the proposed conditions and verify the effectiveness of the sampling scheme.      
### 11.Designing adaptive robust extended Kalman filter based on Lyapunov-based controller for robotics manipulators  [ :arrow_down: ](https://arxiv.org/pdf/2008.10438.pdf)
>  In this paper, a position and velocity estimation method for robotic manipulators which are affected by constant bounded disturbances is considered. The tracking control problem is formulated as a disturbance rejection problem, with all the unknown parameters and dynamic uncertainties lumped into disturbance. Using adaptive robust extended Kalman filter(AREKF) the movement and velocity of each joint is predicted to use in discontinuous Lyapunov-based controller structure. The parameters of the error dynamics have been validated off-line by real data. Computer simulation results given for a two degree of freedom manipulator demonstrate the efficacy of the improved Kalman Filter by comparing the performance of EKF and improved AREKF. Although it is shown that accurate trajectory tracking can be achieved by using the proposed controller.      
### 12.The Smart Mask: Active Closed-Loop Protection against Airborne Pathogens  [ :arrow_down: ](https://arxiv.org/pdf/2008.10420.pdf)
>  Face masks provide effective, easy-to-use, and low-cost protection against airborne pathogens or infectious agents, including SARS-CoV-2. There is a wide variety of face masks available on the market for various applications, but they are all passive in nature, i.e., simply act as air filters for the nasal passage and/or mouth. In this paper, we present a new "active mask" paradigm, in which the wearable device is equipped with smart sensors and actuators to both detect the presence of airborne pathogens in real time and take appropriate action to mitigate the threat. The proposed approach is based on a closed-loop control system that senses airborne particles of different sizes close to the mask and then makes intelligent decisions to reduce their concentrations. This paper presents a specific implementation of this concept in which the on-board controller determines ambient air quality via a commercial particulate matter sensor, and if necessary activates a piezoelectric actuator that generates a mist spray to load these particles, thus causing them to fall to the ground. The proposed system communicates with the user via a smart phone application that provides various alerts, including notification of the need to recharge and/or decontaminate the mask prior to reuse. The application also enables a user to override the on-board control system and manually control the mist generator if necessary. Experimental results from a functional prototype demonstrate significant reduction in airborne particulate counts near the mask when the active protection system is enabled.      
### 13.Optimization of operation parameters towards sustainable WWTP based on deep reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.10417.pdf)
>  A large amount of wastewater has been produced nowadays. Wastewater treatment plants (WWTPs) are designed to eliminate pollutants and alleviate environmental pollution resulting from human activities. However, the construction and operation of WWTPs still have negative impacts. WWTPs are complex to control and optimize because of high nonlinearity and variation. This study used a novel technique, multi-agent deep reinforcement learning (DRL), to optimize dissolved oxygen (DO) and dosage in a hypothetical WWTP. The reward function is specially designed as LCA-based form to achieve sustainability optimization. Four scenarios: baseline, LCA-oriented, cost-oriented and effluent-oriented are considered. The result shows that optimization based on LCA has lowest environmental impacts. The comparison of different SRT indicates that a proper SRT can reduce negative impacts greatly. It is worth mentioning that the retrofitting of WWTPs should be implemented with the consideration of other environmental impacts except cost. Moreover, the comparison between DRL and genetic algorithm (GA) indicates that DRL can solve optimization problems effectively and has great extendibility. In a nutshell, there are still limits and shortcomings of this work, future studies are required.      
### 14.Effect of noise on output-only modal identification of beams  [ :arrow_down: ](https://arxiv.org/pdf/2008.10416.pdf)
>  In most cases, structural health monitoring depends on the determination of the modal parameters of the first few modes of a structure. The data that are used to identify these modes by output-only methods include the structure response together with noise. The presence of noise in the structure response affects the modal parameters. The current study was undertaken to examine the possibility and accuracy of identifying the modal parameters of beams in the presence of noise. For this purpose, the modal parameters of the different modes of single-span beams were obtained using output data with different signal-to-noise ratios. The acceleration signals were obtained by transient analysis and then different powers of noise were generated and added to the signals. The modal parameters of the beams were obtained using output-only methods. Parameters having signal-to-noise ratios of greater than 13.98 dB for all modes considered were identified. At a signal-to-noise ratio of -6.02 to 13.98 dB (higher noise level), it was not possible to identify the modal parameters of the first mode of beams, but the parameters of the higher modes were identified with good accuracy.      
### 15.Time irreversibility and amplitude irreversibility measures for nonequilibrium processes in complex systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.10415.pdf)
>  Time irreversibility, which characterizes nonequilibrium processes, can be measured based on the probabilistic differences between symmetric vectors. To simplify the quantification of time irreversibility, symmetric permutations instead of symmetric vectors have been employed in some studies. However, although effective in practical applications, this approach is conceptually incorrect. Time irreversibility should be measured based on the permutations of symmetric vectors rather than symmetric permutations, whereas symmetric permutations can instead be employed to determine the quantitative amplitude irreversibility -- a novel parameter proposed in this paper for nonequilibrium calculated by means of the probabilistic difference in amplitude fluctuations. Through theoretical and experimental analyses, we highlight the strong similarities and close associations between the time irreversibility and amplitude irreversibility measures. Our paper clarifies the connections of and the differences between the two types of permutation-based parameters for quantitative nonequilibrium, and by doing so, we bridge the concepts of amplitude irreversibility and time irreversibility and broaden the selection of quantitative tools for studying nonequilibrium processes in complex systems.      
### 16.Generate High Resolution Images With Generative Variational Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2008.10399.pdf)
>  In this work, we present a novel neural network to generate high resolution images. We replace the decoder of VAE with a discriminator while using the encoder as it is. The encoder uses data from a normal distribution while the generator from a gaussian distribution. The combination from both is given to a discriminator which tells whether the generated images are correct or not. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA-HQ dataset. Our network beats the previous state of the art using MMD, SSIM, log likelihood, reconstruction error, ELBO and KL divergence as the evaluation metrics while generating much sharper images. This work is potentially very exciting as we are able to combine the advantages of generative models and inference models in a principled bayesian manner.      
### 17.Short-Packet Communications for MIMO NOMA Systems over Nakagami-m Fading: BLER and Minimum Blocklength Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.10390.pdf)
>  Recently, ultra-reliable and low-latency communications (URLLC) using short-packets has been proposed to fulfill the stringent requirements regarding reliability and latency of emerging applications in 5G and beyond networks. In addition, multiple-input multiple-output non-orthogonal multiple access (MIMO NOMA) is a potential candidate to improve the spectral efficiency, reliability, latency, and connectivity of wireless systems. In this paper, we investigate short-packet communications (SPC) in a multiuser downlink MIMO NOMA system over Nakagami-m fading, and propose two antenna-user selection methods considering two clusters of users having different priority levels. In contrast to the widely-used long data-packet assumption, the SPC analysis requires the redesign of the communication protocols and novel performance metrics. Given this context, we analyze the SPC performance of MIMO NOMA systems using the average block error rate (BLER) and minimum blocklength, instead of the conventional metrics such as ergodic capacity and outage capacity. More specifically, to characterize the system performance regarding SPC, asymptotic (in the high signal-to-noise ratio regime) and approximate closed-form expressions of the average BLER at the users are derived. Based on the asymptotic behavior of the average BLER, an analysis of the diversity order, minimum blocklength, and optimal power allocation is carried out. The achieved results show that MIMO NOMA can serve multiple users simultaneously using a smaller blocklength compared with MIMO OMA, thus demonstrating the benefits of MIMO NOMA for SPC in minimizing the transmission latency. Furthermore, our results indicate that the proposed methods not only improve the BLER performance but also guarantee full diversity gains for the respective users.      
### 18.Network-Aware Demand-side Management Framework with A Community Energy Storage System Considering Voltage Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2008.10385.pdf)
>  This paper studies the feasibility of integrating a community energy storage (CES) system with rooftop photovoltaic (PV) power generation for demand-side management of a neighbourhood while maintaining the distribution network voltages within allowed limits. To this end, we develop a decentralized energy trading system between a CES provider and users with rooftop PV systems. By leveraging a linearized branch flow model for radial distribution networks, a voltage-constrained leader-follower Stackelberg game is developed wherein the CES provider maximizes revenue and the users minimize their personal energy costs by trading energy with the CES system and the grid. The Stackelberg game has a unique equilibrium at which the CES provider maximizes revenue and the users minimize energy costs at a unique Nash equilibrium. A case study, with realistic PV power generation and demand data, confirms that the energy trading system can reduce peak energy demand and prevent network voltage excursions, while delivering financial benefits to the users and the CES provider. Further, simulations highlight that, in comparison with a centralized system, the decentralized energy trading system provides greater economic benefits to the users with less energy storage capacity.      
### 19.An Incentive-compatible Energy Trading Framework for Neighborhood Area Networks with Shared Energy Storage  [ :arrow_down: ](https://arxiv.org/pdf/2008.10384.pdf)
>  Here, a novel energy trading system is proposed for demand-side management of a neighborhood area network (NAN) consisting of a shared energy storage (SES) provider, users with non-dispatchable energy generation, and an electricity retailer. In a leader-follower Stackelberg game, the SES provider first maximizes their revenue by setting a price signal and trading energy with the grid. Then, by following the SES provider's actions, the retailer minimizes social cost for the users, i.e., the sum of the total users' cost when they interact with the SES and the total cost for supplying grid energy to the users. A pricing strategy, which incorporates mechanism design, is proposed to make the system incentive-compatible by rewarding users who disclose true energy usage information. A unique Stackelberg equilibrium is achieved where the SES provider's revenue is maximized and the user-level social cost is minimized, which also rewards the retailer. A case study with realistic energy demand and generation data demonstrates 28\%~-~45\% peak demand reduction of the NAN, depending on the number of participating users, compared to a system without SES. Simulation results confirm that the retailer can also benefit financially, in addition to the SES provider and the users.      
### 20.Community-Aware Graph Signal Processing  [ :arrow_down: ](https://arxiv.org/pdf/2008.10375.pdf)
>  The emerging field of graph signal processing (GSP) allows to transpose classical signal processing operations (e.g., filtering) to signals on graphs. The GSP framework is generally built upon the graph Laplacian, which plays a crucial role to study graph properties and measure graph signal smoothness. Here instead, we propose the graph modularity matrix as the centerpiece of GSP, in order to incorporate knowledge about graph community structure when processing signals on the graph, but without the need for community detection. We study this approach in several generic settings such as filtering, optimal sampling and reconstruction, surrogate data generation, and denoising. Feasibility is illustrated by a small-scale example and a transportation network dataset, as well as one application in human neuroimaging where community-aware GSP reveals relationships between behavior and brain features that are not shown by Laplacian-based GSP. This work demonstrates how concepts from network science can lead to new meaningful operations on graph signals.      
### 21.Low-Complexity Geometric Shaping  [ :arrow_down: ](https://arxiv.org/pdf/2008.10330.pdf)
>  Approaching Shannon's capacity via geometric shaping has usually been regarded as challenging due to modulation and demodulation complexity, requiring look-up tables to store the constellation points and constellation bit labeling. To overcome these challenges, in this paper, we study lattice-based geometrically shaped modulation formats in multidimensional Euclidean space. We describe and evaluate fast and low complexity modulation and demodulation algorithms that make these modulation formats practical, even with extremely high constellation sizes with more than $10^{28}$ points. The uncoded bit error rate performance of these constellations is compared with the conventional QAM formats in the additive white Gaussian noise and nonlinear fiber channels. At a spectral efficiency of 2 bits/sym/polarization, compared with 4-QAM format, transmission reach improvement of more than 38% is shown at the hard-decision forward error correction threshold of $2.26\times 10^{-4}$.      
### 22.Fidelity-Controllable Extreme Image Compression with Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.10314.pdf)
>  We propose a GAN-based image compression method working at extremely low bitrates below 0.1bpp. Most existing learned image compression methods suffer from blur at extremely low bitrates. Although GAN can help to reconstruct sharp images, there are two drawbacks. First, GAN makes training unstable. Second, the reconstructions often contain unpleasing noise or artifacts. To address both of the drawbacks, our method adopts two-stage training and network interpolation. The two-stage training is effective to stabilize the training. Moreover, the network interpolation utilizes the models in both stages and reduces undesirable noise and artifacts, while maintaining important edges. Hence, we can control the trade-off between perceptual quality and fidelity without re-training models. The experimental results show that our model can reconstruct high quality images. Furthermore, our user study confirms that our reconstructions are preferable to state-of-the-art GAN-based image compression model. The code will be available.      
### 23.Inpainting-based Video Compression in FullHD  [ :arrow_down: ](https://arxiv.org/pdf/2008.10273.pdf)
>  Compression methods based on inpainting have been an active field of research in the past decade. Videos are especially challenging for this kind of methods, since real-time decompression requires highly efficient algorithms. Dedicated inpainting-based video codecs have so far focused on efficient frame-by-frame reconstruction without exploiting redundancies in time. As a remedy, we propose a modular framework that combines a classical prediction and correction approach with suitable structures for fully inpainting-based methods. The core idea of these techniques is to store values only at a small number of positions, and reconstruct missing regions via inpainting. Our generic framework supports any algorithm that generates such sparse representations. As a concrete demonstrator, we provide a prototypical implementation of our framework by supplementing all modules with methods based on partial differential equations (PDEs): Dense variational optic flow fields yield accurate motion-compensated predictions, while homogeneous diffusion inpainting and pseudodifferential equations are applied as intra prediction and residual compression techniques. With these components, we are able to outperform other inpainting-based video codecs in terms of quality and speed. For the first time in inpainting-based video compression, we can decompress FullHD (1080p) videos in real-time with a fully CPU-based implementation.      
### 24.A Computational Analysis of Real-World DJ Mixes using Mix-To-Track Subsequence Alignment  [ :arrow_down: ](https://arxiv.org/pdf/2008.10267.pdf)
>  A DJ mix is a sequence of music tracks concatenated seamlessly, typically rendered for audiences in a live setting by a DJ on stage. As a DJ mix is produced in a studio or the live version is recorded for music streaming services, computational methods to analyze DJ mixes, for example, extracting track information or understanding DJ techniques, have drawn research interests. Many of previous works are, however, limited to identifying individual tracks in a mix or segmenting it, and the sizes of the datasets are usually small. In this paper, we provide an in-depth analysis of DJ music by aligning a mix to its original music tracks. We set up the subsequence alignment such that the audio features are less sensitive to the tempo or key change of the original track in a mix. This approach provides temporally tight mix-to-track matching from which we can obtain cue-points, transition length, mix segmentation, and musical changes in DJ performance. Using 1,557 mixes from 1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the proposed analysis and show a wide range of statistics, which may elucidate the creative process of DJ music making.      
### 25.Real-time internal boundary control of lane-free automated vehicle traffic  [ :arrow_down: ](https://arxiv.org/pdf/2008.10255.pdf)
>  A recently proposed paradigm for vehicular traffic in the era of CAV (connected and automated vehicles), called TrafficFluid, involves lane-free vehicle movement. Lane-free traffic implies that incremental road widening (narrowing) leads to corresponding incremental increase (decrease) of capacity; and this opens the way for consideration of real-time internal boundary control on highways and arterials, in order to flexibly share the total (both directions) road width and capacity among the two directions in dependence of the bi-directional demand and traffic conditions, so as to maximize the total (two directions) flow efficiency. The problem is formulated as a convex QP (Quadratic Programming) problem that may be solved efficiently, and representative case studies shed light on and demonstrate the features, capabilities and potential of the novel control action.      
### 26.Managing connected and automated vehicles with flexible routing at "lane-allocation-free'' intersections  [ :arrow_down: ](https://arxiv.org/pdf/2008.10239.pdf)
>  Trajectory planning and coordination for connected and automated vehicles (CAVs) have been studied at isolated ``signal-free'' intersections and in ``signal-free'' corridors under the fully CAV environment in the literature. Most of the existing studies are based on the definition of approaching and exit lanes. The route a vehicle takes to pass through an intersection is determined from its movement. That is, only the origin and destination arms are included. This study proposes a mixed-integer linear programming (MILP) model to optimize vehicle trajectories at an isolated ``signal-free'' intersection without lane allocation, which is denoted as ``lane-allocation-free'' (LAF) control. Each lane can be used as both approaching and exit lanes for all vehicle movements including left-turn, through, and right-turn. A vehicle can take a flexible route by way of multiple arms to pass through the intersection. In this way, the spatial-temporal resources are expected to be fully utilized. The interactions between vehicle trajectories are modeled explicitly at the microscopic level. Vehicle routes and trajectories (i.e., car-following and lane-changing behaviors) at the intersection are optimized in one unified framework for system optimality in terms of total vehicle delay. Considering varying traffic conditions, the planning horizon is adaptively adjusted in the implementation procedure of the proposed model to make a balance between solution feasibility and computational burden. Numerical studies validate the advantages of the proposed LAF control in terms of both vehicle delay and throughput with different demand structures and temporal safety gaps.      
### 27.AMRConvNet: AMR-Coded Speech Enhancement Using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.10233.pdf)
>  Speech is converted to digital signals using speech coding for efficient transmission. However, this often lowers the quality and bandwidth of speech. This paper explores the application of convolutional neural networks for Artificial Bandwidth Expansion (ABE) and speech enhancement on coded speech, particularly Adaptive Multi-Rate (AMR) used in 2G cellular phone calls. In this paper, we introduce AMRConvNet: a convolutional neural network that performs ABE and speech enhancement on speech encoded with AMR. The model operates directly on the time-domain for both input and output speech but optimizes using combined time-domain reconstruction loss and frequency-domain perceptual loss. AMRConvNet resulted in an average improvement of 0.425 Mean Opinion Score - Listening Quality Objective (MOS-LQO) points for AMR bitrate of 4.75k, and 0.073 MOS-LQO points for AMR bitrate of 12.2k. AMRConvNet also showed robustness in AMR bitrate inputs. Finally, an ablation test showed that our combined time-domain and frequency-domain loss leads to slightly higher MOS-LQO and faster training convergence than using either loss alone.      
### 28.Initial-Value Privacy of Linear Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.10193.pdf)
>  This paper studies initial-value privacy problems of linear dynamical systems. We consider a standard linear time-invariant system with random process and measurement noises. For such a system, eavesdroppers having access to system output trajectories may infer the system initial states, leading to initial-value privacy risks. When a finite number of output trajectories are eavesdropped, we consider a requirement that any guess about the initial values can be plausibly denied. When an infinite number of output trajectories are eavesdropped, we consider a requirement that the initial values should not be uniquely recoverable. In view of these two privacy requirements, we define differential initial-value privacy and intrinsic initial-value privacy, respectively, for the system as metrics of privacy risks. First of all, we prove that the intrinsic initial-value privacy is equivalent to unobservability, while the differential initial-value privacy can be achieved for a privacy budget depending on an extended observability matrix of the system and the covariance of the noises. Next, the inherent network nature of the considered linear system is explored, where each individual state corresponds to a node and the state and output matrices induce interaction and sensing graphs, leading to a network system. Under this network system perspective, we allow the initial states at some nodes to be public, and investigate the resulting intrinsic initial-value privacy of each individual node. We establish necessary and sufficient conditions for such individual node initial-value privacy, and also prove that the intrinsic initial-value privacy of individual nodes is generically determined by the network structure. These results may be extended to linear systems with time-varying dynamics under the same analysis framework.      
### 29.Sampling-based Reachability Analysis: A Random Set Theory Approach with Adversarial Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2008.10180.pdf)
>  Reachability analysis is at the core of many applications, from neural network verification, to safe trajectory planning of uncertain systems. However, this problem is notoriously challenging, and current approaches tend to be either too restrictive, too slow, too conservative, or approximate and therefore lack guarantees. In this paper, we propose a simple yet effective sampling-based approach to perform reachability analysis for arbitrary dynamical systems. Our key novel idea consists of using random set theory to give a rigorous interpretation of our method, and prove that it returns sets which are guaranteed to converge to the convex hull of the true reachable sets. Additionally, we leverage recent work on robust deep learning and propose a new adversarial sampling approach to robustify our algorithm and accelerate its convergence. We show that our method is faster and less conservative than other approaches, present results for approximate reachability analysis of neural networks and robust trajectory optimization of high-dimensional uncertain nonlinear systems, and discuss future applications.      
### 30.Model-Free Adaptive Control based on Modified Full-Form-Dynamic-Linearization  [ :arrow_down: ](https://arxiv.org/pdf/2008.10164.pdf)
>  Current model-free adaptive control (MFAC) method has no been analysed in linear system and is not straightforward for the practical engineers to understand accurately. This correspondence presents a family of MFAC based on a modified equivalent-dynamic-linearization model (EDLM), which facilitates to show the working principle of method more directly and objectively. Compared to the current work, i) the researches on MFAC focus on linear model, which is easy to understand its working behaviour; ii) the full-form EDLM is extended with unmeasured stochastic and measured disturbance, respectively. Then the controllers is modified correspondingly; iii) the stability analysis of system cannot be proved by current contraction mapping technique when the sign of leading coefficient of control input changes, therefore we prove it by analysing the function of the closed-loop poles. Several simulated examples are used to show the principles of this kind of method.      
### 31.Deep Neural Network based Wide-Area Event Classification in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.10151.pdf)
>  This paper presents a wide-area event classification in transmission power grids. The deep neural network (DNN) based classifier is developed based on the availability of data from time-synchronized phasor measurement units (PMUs). The proposed DNN is trained using Bayesian optimization to search for the best hyperparameters. The effectiveness of the proposed event classification is validated through the real-world dataset of the U.S. transmission grids. This dataset includes line outage, transformer outage, frequency event, and oscillation events. The validation process also includes different PMU outputs, such as voltage magnitude, angle, current magnitude, frequency, and rate of change of frequency (ROCOF). The simulation results show that ROCOF as input feature gives the best classification performance. In addition, it is shown that the classifier trained with higher sampling rate PMUs and a larger dataset has higher accuracy.      
### 32.ECG-Based Blood Pressure Estimation Using Mechano-Electric Coupling Concept  [ :arrow_down: ](https://arxiv.org/pdf/2008.10099.pdf)
>  The Electrocardiograph signal represents the heart's electrical activity while blood pressure results from the heart's mechanical activity. Previous studies have investigated how the heart's electrical and mechanical activities are related and have referred to their relationship as the Mechano-Electric Coupling term. A new method to estimate the blood pressure including is proposed which uses only the Electrocardiograph signal. In spite of studies performed on feature extraction based on the signals' physiological parameters (Parameter-based), in this work, the feature vectors are formed with samples of the Electrocardiograph signal in a particular time frame (Whole-based) and these vectors are input into Adaptive Boosting Regression to estimate blood pressure. The nonlinear relationship which correlates blood pressure with the Electrocardiograph signal is concluded by the results of this study. According to the results, the used algorithms, for estimating both diastolic blood pressures and mean arterial pressure, are in compliance with the standards of the Association for the Advancement of Medical Instrumentation. Also, according to the British Hypertension Society standard, estimating diastolic blood pressures and mean arterial pressure with the proposed method attain an A grade while it achieves B for systolic blood pressure. The results indicate that using the introduced method, blood pressure can be estimated continuously, noninvasively, without cuff, calibration-free and by using only the Electrocardiograph signal.      
### 33.Adaptive Subband Compression of Streaming Data for Power System Monitoring and Control  [ :arrow_down: ](https://arxiv.org/pdf/2008.10092.pdf)
>  A data compression system capable of providing high-fidelity high-resolution streaming of power system real-time measurements is proposed. Referred to as adaptive subband compression, the proposed technique partitions the signal space into subbands and adaptively compresses subband signals based on each subband's active bandwidth. The proposed technique conforms to existing industry standards for phasor measurement units. It applies to the streaming of phasor measurements or high-frequency point on waveform samples of power system signals. Experiments on synthetic and real data show that the prototyped technology reduces the required communication data rate by several orders of magnitude while maintaining the precision required by the industry standards.      
### 34.Federated Learning for Cellular-connected UAVs: Radio Mapping and Path Planning  [ :arrow_down: ](https://arxiv.org/pdf/2008.10054.pdf)
>  To prolong the lifetime of the unmanned aerial vehicles (UAVs), the UAVs need to fulfill their missions in the shortest possible time. In addition to this requirement, in many applications, the UAVs require a reliable internet connection during their flights. In this paper, we minimize the travel time of the UAVs, ensuring that a probabilistic connectivity constraint is satisfied. To solve this problem, we need a global model of the outage probability in the environment. Since the UAVs have different missions and fly over different areas, their collected data carry local information on the network's connectivity. As a result, the UAVs can not rely on their own experiences to build the global model. This issue affects the path planning of the UAVs. To address this concern, we utilize a two-step approach. In the first step, by using Federated Learning (FL), the UAVs collaboratively build a global model of the outage probability in the environment. In the second step, by using the global model obtained in the first step and rapidly-exploring random trees (RRTs), we propose an algorithm to optimize UAVs' paths. Simulation results show the effectiveness of this two-step approach for UAV networks.      
### 35.Independent Vector Analysis via Log-Quadratically Penalized Quadratic Minimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.10048.pdf)
>  We propose a new algorithm for blind source separation of convolutive mixtures using independent vector analysis. This is an improvement over the popular auxiliary function based independent vector analysis (AuxIVA) with iterative projection (IP) or iterative source steering (ISS). We introduce iterative projection with adjustment (IPA), whereas we update one demixing filter and jointly adjust all the other sources along its current direction. We implement this scheme as multiplicative updates by a rank-2 perturbation of the identity matrix. Each update involves solving a non-convex minimization problem that we term log-quadratically penalized quadratic minimization (LQPQM), that we think is of interest beyond this work. We find that the global minimum of an LQPQM can be efficiently computed. In the general case, we show that all its stationary points can be characterized as zeros of a kind of secular equation, reminiscent of modified eigenvalue problems. We further prove that the global minimum corresponds to the largest of these zeros. We propose a simple procedure based on Newton-Raphson seeded with a good initial point to efficiently compute it. We validate the performance of the proposed method for blind acoustic source separation via numerical experiments with reverberant speech mixtures. We show that not only is the convergence speed faster in terms of iterations, but each update is also computationally cheaper. Notably, for four and five sources, AuxIVA with IPA converges more than twice as fast as competing methods.      
### 36.A systematic approach towards robust stability analysis of integral delay systems with general interval kernels  [ :arrow_down: ](https://arxiv.org/pdf/2008.10036.pdf)
>  Robust stability problem of integral delay systems with uncertain kernel matrix functions is addressed in this paper. On the basis of characteristic equation and the argument principle, an algorithm is generated which is shown to outperform the Lyapunov-Krasovskii (LK) approaches with respect to conservatism in the presented examples. Despite the conventional manual use of Nyquist criterion, the proposed algorithm is fully algebraic, cheaper and easily implemented in computer programs. In addition, the proposed method is applicable to a wider range of uncertain systems compared to the existing literature. Namely, despite the previously published results on this problem, the kernel matrix function here is not limited to exponential type and can include any real function within known bounds as its elements.      
### 37.Time-Varying and Nonlinearly Scaled Consensus of Multiagent Systems: A Generic Attracting Law Approach  [ :arrow_down: ](https://arxiv.org/pdf/2008.10028.pdf)
>  This paper presents the design and analysis of the finite/fixed-time scaled consensus for multiagent systems. A study on a generic attracting law, the certain classes of nonlinear systems that admit attractors with finite/fixed-time convergence, is at first given for the consensus purpose. The estimates for the lower and upper bounds on the settling time functions are provided through the two-phase analysis. The given estimates are initial state dependent, but the durations are finite, without regarding the values that the initial states take. According to the generic attracting law, distributed protocols are proposed for multiagent systems with undirected and detail-balanced directed graphs, respectively, where the scaled strategies, including time-varying and nonlinear scales, are adopted. It is shown that the finite/fixed-time consensus for the multiagent system undertaken can still be achieved, even though both time-varying and nonlinear scales are taken among agents. Numerical simulation of two illustrative examples are given to verify effectiveness of the proposed finite-duration consensus protocols.      
### 38.Low-Complexity Joint Power Allocation and Trajectory Design for UAV-Enabled Secure Communications with Power Splitting  [ :arrow_down: ](https://arxiv.org/pdf/2008.10015.pdf)
>  An unmanned aerial vehicle (UAV)-aided secure communication system is conceived and investigated, where the UAV transmits legitimate information to a ground user in the presence of an eavesdropper (Eve). To guarantee the security, the UAV employs a power splitting approach, where its transmit power can be divided into two parts for transmitting confidential messages and artificial noise (AN), respectively. We aim to maximize the average secrecy rate by jointly optimizing the UAV's trajectory, the transmit power levels and the corresponding power splitting ratios allocated to different time slots during the whole flight time, subject to both the maximum UAV speed constraint, the total mobility energy constraint, the total transmit power constraint, and other related constraints. To efficiently tackle this non-convex optimization problem, we propose an iterative algorithm by blending the benefits of the block coordinate descent (BCD) method, the concave-convex procedure (CCCP) and the alternating direction method of multipliers (ADMM). Specially, we show that the proposed algorithm exhibits very low computational complexity and each of its updating steps can be formulated in a nearly closed form. Our simulation results validate the efficiency of the proposed algorithm.      
### 39.They are wearing a mask! Identification of Subjects Wearing a Surgical Mask from their Speech by means of x-vectors and Fisher Vectors  [ :arrow_down: ](https://arxiv.org/pdf/2008.10014.pdf)
>  Challenges based on Computational Paralinguistics in the INTERSPEECH Conference have always had a good reception among the attendees owing to its competitive academic and research demands. This year, the INTERSPEECH 2020 Computational Paralinguistics Challenge offers three different problems; here, the Mask Sub-Challenge is of specific interest. This challenge involves the classification of speech recorded from subjects while wearing a surgical mask. In this study, to address the above-mentioned problem we employ two different types of feature extraction methods. The x-vectors embeddings, which is the current state-of-the-art approach for Speaker Recognition; and the Fisher Vector (FV), that is a method originally intended for Image Recognition, but here we utilize it to discriminate utterances. These approaches employ distinct frame-level representations: MFCC and PLP. Using Support Vector Machines (SVM) as the classifier, we perform a technical comparison between the performances of the FV encodings and the x-vector embeddings for this particular classification task. We find that the Fisher vector encodings provide better representations of the utterances than the x-vectors do for this specific dataset. Moreover, we show that a fusion of our best configurations outperforms all the baseline scores of the Mask Sub-Challenge.      
### 40.Development and Validation of a Comprehensive Helicopter Flight Dynamics Code  [ :arrow_down: ](https://arxiv.org/pdf/2008.09873.pdf)
>  A comprehensive helicopter flight dynamics code is developed based on the UH-60 helicopter and named Texas A\&amp;M University Rotorcraft Analysis Code (TRAC). This is a complete software package, which could perform trim analysis to autonomous flight simulation and the capability to model any helicopter configuration. Different components of the helicopter such as the main rotor, tail rotor, fuselage, vertical tail, and horizontal tail are modeled individually as different modules in the code and integrated to develop a complete UH-60 model. Since the code is developed on a module basis, it can be easily modified to adopt another component or configure a different helicopter. TRAC can predict the dynamic responses of both the articulated rotor blades and the helicopter fuselage and yields the required pilot control inputs to achieve trim condition for different flight regimes such as hover, forward flight, coordinated turn, climb/descent, etc. These trim results are validated with the test data obtained from the UH-60 flight tests conducted by the US Army. Beyond trim analysis, TRAC can also generate linearized models at various flight conditions based on a first-order Taylor series expansion. The extracted linear models show realistic helicopter dynamic behavior and were used to simulate a fully autonomous flight that involves a UH-60 helicopter approaching a ship and landing on the deck by implementing a Linear Quadratic Regulator (LQR) optimal controller.      
### 41.A Discrete-Time Matching Filtering Differentiator  [ :arrow_down: ](https://arxiv.org/pdf/2008.09863.pdf)
>  This paper presents a time discretization of the robust exact filtering differentiator, a sliding mode differentiator coupled to filter, which provides a suitable approximation to the derivatives of some noisy signals. This proposal takes advantage of the homogeneity of the differentiator, allowing the use of similar techniques to those of the linear systems. As in the original case, the convergence robust exact filtering differentiator depends on the bound of a higher-order derivative; nevertheless, this new realization can be implemented with or without the knowledge of such constant. It is demonstrated that the system's trajectories converge to a neighborhood of the origin with a free-noise input. Finally, comparisons between the behavior of the differentiator with different design parameters are presented.      
### 42.Large Intelligent Surfaces with Channel Estimation Overhead: Achievable Rate and Optimal Configuration  [ :arrow_down: ](https://arxiv.org/pdf/2008.09843.pdf)
>  Large intelligent surfaces (LIS) present a promising new technology for enhancing the performance of wireless communication systems. So far, the significant performance gains brought by LIS have mainly been shown under the assumption that perfect channel state information is available. In practice however, acquiring accurate channel knowledge poses a significant challenge, and the corresponding overhead can be large. Here, we study the achievable rate of a LIS-assisted communication system accounting for such channel estimation overhead. As a main observation, we demonstrate that there exists a trade-off between the number of LIS elements $K$ and achievable rate. More specifically, there exists an optimal $K^{*}$, beyond which, the achievable rate starts to decline, since the power gains offered by LIS are outweighed by the channel estimation overhead. We present analytical approximations for $K^{*}$, based on maximizing an analytical upper bound on achievable rate that we derive.      
### 43.Solution space of optimal heat pump schedules  [ :arrow_down: ](https://arxiv.org/pdf/2008.09794.pdf)
>  We study the space of optimal schedules for a heat pump with thermal energy storage used in heating a residential building. We model the heating system as a Mixed Integer Linear Program with the objective to minimise the cost of heating. We generate a large number of realistic daily heat demands and calculate the optimal schedule for the heat pump. In addition to cost savings stemming from optimal running, we find that the space of optimal schedules is large in practice, even for the simplest model of the heating system we use, and that the optimal schedules are difficult to reproduce with statistical models. These findings strengthen the case for the use of mathematical optimisation in real-life applications.      
### 44.Study of Intelligent Reflective Surface Assisted Communications with One-bit Phase Adjustments  [ :arrow_down: ](https://arxiv.org/pdf/2008.09770.pdf)
>  We analyse the performance of a communication link assisted by an intelligent reflective surface (IRS) positioned in the far field of both the source and the destination. A direct link between the transmitting and receiving devices is assumed to exist. Perfect and imperfect phase adjustments at the IRS are considered. For the perfect phase configuration, we derive an approximate expression for the outage probability in closed form. For the imperfect phase configuration, we assume that each element of the IRS has a one-bit phase shifter (0Â°, 180Â°) and an expression for the outage probability is obtained in the form of an integral. Our formulation admits an exact asymptotic (high SNR) analysis, from which we obtain the diversity orders for systems with and without phase errors. We show these are N + 1 and (N + 3)/2, respectively. Numerical results confirm the theoretical analysis and verify that the reported results are more accurate than methods based on the central limit theorem (CLT).      
### 45.Self-Tuning Control based on Modified Equivalent-Dynamic-Linearization Mode  [ :arrow_down: ](https://arxiv.org/pdf/2008.09751.pdf)
>  The current model-free adaptive control (MFAC) method is designed on the basis of the equivalent-dynamic-linearization model (EDLM) with neglect of the time delay and disturbance in practical. By comparisons with the current works about MFAC, i) a class of self-tuning controller is proposed based a new EDLM modified by the introduction of time delay and disturbance so as to reflect the real system more objectively. Thereafter, we classify the proposed controller into four cases to enable easier applications; ii) the controller design and stability analysis of system are achievable by analyzing the function of the closed-loop poles. In addition, the issue of how to choose the parameter \lambda in current MFAC by quantity is firstly finished by the analysis of zeros-poles placement and static error of system, whereas this can hardly be realized by the previous contraction mapping method; iii) the study on the proposed method is focused on linear model for easily mastering its working behavior; At last, two examples are used to demonstrate the effectiveness of the proposed method and to point out the deficiencies in the current MFAC theory.      
### 46.Set-based state estimation and fault diagnosis of linear discrete-time descriptor systems using constrained zonotopes  [ :arrow_down: ](https://arxiv.org/pdf/2008.09732.pdf)
>  This paper presents new methods for set-valued state estimation and active fault diagnosis of linear descriptor systems. The algorithms are based on constrained zonotopes, a generalization of zonotopes capable of describing strongly asymmetric convex sets, while retaining the computational advantages of zonotopes. Additionally, unlike other set representations like intervals, zonotopes, ellipsoids, paralletopes, among others, linear static constraints on the state variables, typical of descriptor systems, can be directly incorporated in the mathematical description of constrained zonotopes. Therefore, the proposed methods lead to more accurate results in state estimation in comparison to existing methods based on the previous sets without requiring rank assumptions on the structure of the descriptor system and with a fair trade-off between accuracy and efficiency. These advantages are highlighted in two numerical examples.      
### 47.Comparative performance analysis of the ResNet backbones of Mask RCNN to segment the signs of COVID-19 in chest CT scans  [ :arrow_down: ](https://arxiv.org/pdf/2008.09713.pdf)
>  COVID-19 has been detrimental in terms of the number of fatalities and rising number of critical patients across the world. According to the UNDP (United National Development Programme) Socio-Economic programme, aimed at the COVID-19 crisis, the pandemic is far more than a health crisis: it is affecting societies and economies at their core. There has been greater developments recently in the chest X-ray-based imaging technique as part of the COVID-19 diagnosis especially using Convolution Neural Networks (CNN) for recognising and classifying images. However, given the limitation of supervised labelled imaging data, the classification and predictive risk modelling of medical diagnosis tend to compromise. This paper aims to identify and monitor the effects of COVID-19 on the human lungs by employing Deep Neural Networks on axial CT (Chest Computed Tomography) scan of lungs. We have adopted Mask RCNN, with ResNet50 and ResNet101 as its backbone, to segment the regions, affected by COVID-19 coronavirus. Using the regions of human lungs, where symptoms have manifested, the model classifies condition of the patient as either "Mild" or "Alarming". Moreover, the model is deployed on the Google Cloud Platform (GCP) to simulate the online usage of the model for performance evaluation and accuracy improvement. The ResNet101 backbone model produces an F1 score of 0.85 and faster prediction scores with an average time of 9.04 seconds per inference.      
### 48.Spectral independent component analysis with noise modeling for M/EEG source separation  [ :arrow_down: ](https://arxiv.org/pdf/2008.09693.pdf)
>  Background: Independent Component Analysis (ICA) is a widespread tool for exploration and denoising of electroencephalography (EEG) or magnetoencephalography (MEG) signals. In its most common formulation, ICA assumes that the signal matrix is a noiseless linear mixture of independent sources that are assumed non-Gaussian. A limitation is that it enforces to estimate as many sources as sensors or to rely on a detrimental PCA step. <br>Methods: We present the Spectral Matching ICA (SMICA) model. Signals are modelled as a linear mixing of independent sources corrupted by additive noise, where sources and the noise are stationary Gaussian time series. Thanks to the Gaussian assumption, the negative log-likelihood has a simple expression as a sum of divergences between the empirical spectral covariance matrices of the signals and those predicted by the model. The model parameters can then be estimated by the expectation-maximization (EM) algorithm. <br>Results: Experiments on phantom MEG datasets show that SMICA can recover dipole locations more precisely than usual ICA algorithms or Maxwell filtering when the dipole amplitude is low. Experiments on EEG datasets show that SMICA identifies a source subspace which contains sources that have less pairwise mutual information, and are better explained by the projection of a single dipole on the scalp. <br>Comparison with existing methods: Noiseless ICA models lead to degenerate likelihood when there are fewer sources than sensors, while SMICA succeeds without resorting to prior dimension reduction. <br>Conclusions: SMICA is a promising alternative to other noiseless ICA models based on non-Gaussian assumptions.      
### 49.On PMU Data Integrity under GPS Spoofing Attacks: A Sparse Error Correction Framework  [ :arrow_down: ](https://arxiv.org/pdf/2008.09691.pdf)
>  Consider the problem of mitigating the impact on data integrity of phasor measurement units (PMUs) given a GPS spoofing attack. We present a sparse error correction framework to treat PMU measurements that are potentially corrupted due to a GPS spoofing attack. We exploit the sparse nature of a GPS spoofing attack, which is that only a small fraction of PMUs are affected by the attack. We first present attack identifiability conditions (in terms of network topology, PMU locations, and the number of spoofed PMUs) under which data manipulation by the spoofing attack is identifiable. The identifiability conditions have important implications on how the locations of PMUs affect their resilience to GPS spoofing attacks. To effectively correct spoofed PMU data, we present a sparse error correction approach wherein computation tasks are decomposed into smaller zones to ensure scalability. We present experimental results obtained from numerical simulations with the IEEE RTS-96 test network to demonstrate the effectiveness of the proposed approach.      
### 50.Experimental Black-box System identification and control of a Torus Cassegrain Telescope  [ :arrow_down: ](https://arxiv.org/pdf/2008.09686.pdf)
>  The Astronomical Observatory of the Sergio Arboleda University (Bogota, Colombia) has as main instrument a Torus Classic Cassegrain telescope. In order to improve its precision and accuracy in the tracking of celestial objects, the Torus telescope requires to be properly automated. The motors, drivers and sensors of the telescope are modelled as a black-box systems and experimentally identified. Using the identified model several control laws are developed such as Proportional, Integral and Derivative (PID) control and State-feedback control for the velocity and position tracking and disturbance-rejection tasks.      
### 51.Eco-Routing Navigation System for Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.09674.pdf)
>  The goal of this project is to determine the feasibility of calculating a travel route for an electric vehicle (EV) that will require the least amount of energy for the trip, and thus extending the range of the EV. To achieve this goal, the research team set up several objectives, including 1) to demonstrate the accuracy of measurements from the data acquisition system; 2) to demonstrate the validity of the proposed algorithm in estimating energy consumption of the test EV (2013 NISSAN LEAF) in real-time; and 3) to demonstrate the benefit from the prototype system in terms energy savings for the test EV. The results from the project show that the proposed system can fulfill the aforementioned objectives and provide up to 51% of energy reduction, compared with the conventional navigation strategies (i.e., shortest travel distance and least travel duration).      
### 52.Efficient neural speech synthesis for low-resource languages through multilingual modeling  [ :arrow_down: ](https://arxiv.org/pdf/2008.09659.pdf)
>  Recent advances in neural TTS have led to models that can produce high-quality synthetic speech. However, these models typically require large amounts of training data, which can make it costly to produce a new voice with the desired quality. Although multi-speaker modeling can reduce the data requirements necessary for a new voice, this approach is usually not viable for many low-resource languages for which abundant multi-speaker data is not available. In this paper, we therefore investigated to what extent multilingual multi-speaker modeling can be an alternative to monolingual multi-speaker modeling, and explored how data from foreign languages may best be combined with low-resource language data. We found that multilingual modeling can increase the naturalness of low-resource language speech, showed that multilingual models can produce speech with a naturalness comparable to monolingual multi-speaker models, and saw that the target language naturalness was affected by the strategy used to add foreign language data.      
### 53.HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN  [ :arrow_down: ](https://arxiv.org/pdf/2008.09646.pdf)
>  In this paper, we present a novel network for high resolution video generation. Our network uses ideas from Wasserstein GANs by enforcing k-Lipschitz constraint on the loss term and Conditional GANs using class labels for training and testing. We present Generator and Discriminator network layerwise details along with the combined network architecture, optimization details and algorithm used in this work. Our network uses a combination of two loss terms: mean square pixel loss and an adversarial loss. The datasets used for training and testing our network are UCF101, Golf and Aeroplane Datasets. Using Inception Score and FrÃ©chet Inception Distance as the evaluation metrics, our network outperforms previous state of the art networks on unsupervised video generation.      
### 54.Low-complexity Architecture for AR(1) Inference  [ :arrow_down: ](https://arxiv.org/pdf/2008.09633.pdf)
>  In this Letter, we propose a low-complexity estimator for the correlation coefficient based on the signed $\operatorname{AR}(1)$ process. The introduced approximation is suitable for implementation in low-power hardware architectures. Monte Carlo simulations reveal that the proposed estimator performs comparably to the competing methods in literature with maximum error in order of $10^{-2}$. However, the hardware implementation of the introduced method presents considerable advantages in several relevant metrics, offering more than 95% reduction in dynamic power and doubling the maximum operating frequency when compared to the reference method.      
### 55.Representing Long-term Impact of Residential Building Energy Management using Stochastic Dynamic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2008.10528.pdf)
>  Scheduling a residential building short-term to optimize the electricity bill can be difficult with the inclusion of capacity-based grid tariffs. Scheduling the building based on a proposed measured-peak (MP) grid tariff, which is a cost based on the highest peak power over a period, requires the user to consider the impact the current decision-making has in the future. Therefore, the authors propose a mathematical model using stochastic dynamic programming (SDP) that tries to represent the long-term impact of current decision-making. The SDP algorithm calculates non-linear expected future cost curves (EFCC) for the building based on the peak power backwards for each day over a month. The uncertainty in load demand and weather are considered using a discrete Markov chain setup. The model is applied to a case study for a Norwegian building with smart control of flexible loads, and compared against methods where the MP grid tariff is not accurately represented, and where the user has perfect information of the whole month. The results showed that the SDP algorithm performs 0.3 % better than a scenario with no accurate way of presenting future impacts, and performs 3.6 % worse compared to a scenario where the user had perfect information.      
### 56.The role of late photons in diffuse optical imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.10465.pdf)
>  The ability to image through turbid media such as organic tissues, is a highly attractive prospect for biological and medical imaging. This is challenging however, due to the highly scattering properties of tissues which scramble the image information. The earliest photons that arrive at the detector are often associated with ballistic transmission, whilst the later photons are associated with complex paths due to multiple independent scattering events and are therefore typically considered to be detrimental to the final image formation process. In this work we report on the importance of these highly diffuse, "late" photons for computational time-of-flight diffuse optical imaging. In thick scattering materials, &gt;80 transport mean free paths, we provide evidence that including late photons in the inverse retrieval enhances the image reconstruction quality. We also show that the late photons alone have sufficient information to retrieve images of a similar quality to early photon gated data. This result emphasises the importance in the strongly diffusive regime discussed here, of fully time-resolved imaging techniques.      
### 57.FOCAL: A Forgery Localization Framework based on Video Coding Self-Consistency  [ :arrow_down: ](https://arxiv.org/pdf/2008.10454.pdf)
>  Forgery operations on video contents are nowadays within the reach of anyone, thanks to the availability of powerful and user-friendly editing software. Integrity verification and authentication of videos represent a major interest in both journalism (e.g., fake news debunking) and legal environments dealing with digital evidence (e.g., a court of law). While several strategies and different forensics traces have been proposed in recent years, latest solutions aim at increasing the accuracy by combining multiple detectors and features. This paper presents a video forgery localization framework that verifies the self-consistency of coding traces between and within video frames, by fusing the information derived from a set of independent feature descriptors. The feature extraction step is carried out by means of an explainable convolutional neural network architecture, specifically designed to look for and classify coding artifacts. The overall framework was validated in two typical forgery scenarios: temporal and spatial splicing. Experimental results show an improvement to the state-of-the-art on temporal splicing localization and also promising performance in the newly tackled case of spatial splicing, on both synthetic and real-world videos.      
### 58.CRNNs for Urban Sound Tagging with spatiotemporal context  [ :arrow_down: ](https://arxiv.org/pdf/2008.10413.pdf)
>  This paper describes CRNNs we used to participate in Task 5 of the DCASE 2020 challenge. This task focuses on hierarchical multi-label urban sound tagging with spatiotemporal context. The code is available to our GitHub repository at <a class="link-external link-https" href="https://github.com/multitel-ai/urban-sound-tagging" rel="external noopener nofollow">this https URL</a>.      
### 59.Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images, and Noisy OSM Training Labels  [ :arrow_down: ](https://arxiv.org/pdf/2008.10271.pdf)
>  We present a novel multi-view training framework and CNN architecture for combining information from multiple overlapping satellite images and noisy training labels derived from OpenStreetMap (OSM) for semantic labeling of buildings and roads across large geographic regions (100 km$^2$). Our approach to multi-view semantic segmentation yields a 4-7% improvement in the per-class IoU scores compared to the traditional approaches that use the views independently of one another. A unique (and, perhaps, surprising) property of our system is that modifications that are added to the tail-end of the CNN for learning from the multi-view data can be discarded at the time of inference with a relatively small penalty in the overall performance. This implies that the benefits of training using multiple views are absorbed by all the layers of the network. Additionally, our approach only adds a small overhead in terms of the GPU-memory consumption even when training with as many as 32 views per scene. The system we present is end-to-end automated, which facilitates comparing the classifiers trained directly on true orthophotos vis-a-vis first training them on the off-nadir images and subsequently translating the predicted labels to geographical coordinates. With no human supervision, our IoU scores for the buildings and roads classes are 0.8 and 0.64 respectively which are better than state-of-the-art approaches that use OSM labels and that are not completely automated.      
### 60.Two methods to approximate the Koopman operator with a reservoir computer  [ :arrow_down: ](https://arxiv.org/pdf/2008.10263.pdf)
>  The Koopman operator provides a powerful framework for data-driven analysis of dynamical systems. In the last few years, a wealth of numerical methods providing finite-dimensional approximations of the operator have been proposed (e.g. extended dynamic mode decomposition (EDMD) and its variants). While convergence results for EDMD require an infinite number of dictionary elements, recent studies have shown that only few dictionary elements can yield an efficient approximation of the Koopman operator, provided that they are well-chosen through a proper training process. However, this training process typically relies on nonlinear optimization techniques. In this paper, we propose two novel methods based on a reservoir computer to train the dictionary. These methods rely solely on linear convex optimization. We illustrate the efficiency of the method with several numerical examples in the context of data reconstruction, prediction, and computation of the Koopman operator spectrum. These results pave the way to the use of the reservoir computer in the Koopman operator framework.      
### 61.A Newton Tracking Algorithm with Exact Linear Convergence Rate for Decentralized Consensus Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.10157.pdf)
>  This paper considers the decentralized consensus optimization problem defined over a network where each node holds a second-order differentiable local objective function. Our goal is to minimize the summation of local objective functions and find the exact optimal solution using only local computation and neighboring communication. We propose a novel Newton tracking algorithm, where each node updates its local variable along a local Newton direction modified with neighboring and historical information. We investigate the connections between the proposed Newton tracking algorithm and several existing methods, including gradient tracking and second-order algorithms. Under the strong convexity assumption, we prove that it converges to the exact optimal solution at a linear rate. Numerical experiments demonstrate the efficacy of Newton tracking and validate the theoretical findings.      
### 62.Drive Safe: Cognitive-Behavioral Mining for Intelligent Transportation Cyber-Physical System  [ :arrow_down: ](https://arxiv.org/pdf/2008.10148.pdf)
>  This paper presents a cognitive behavioral-based driver mood repairment platform in intelligent transportation cyber-physical systems (IT-CPS) for road safety. In particular, we propose a driving safety platform for distracted drivers, namely \emph{drive safe}, in IT-CPS. The proposed platform recognizes the distracting activities of the drivers as well as their emotions for mood repair. Further, we develop a prototype of the proposed drive safe platform to establish proof-of-concept (PoC) for the road safety in IT-CPS. In the developed driving safety platform, we employ five AI and statistical-based models to infer a vehicle driver's cognitive-behavioral mining to ensure safe driving during the drive. Especially, capsule network (CN), maximum likelihood (ML), convolutional neural network (CNN), Apriori algorithm, and Bayesian network (BN) are deployed for driver activity recognition, environmental feature extraction, mood recognition, sequential pattern mining, and content recommendation for affective mood repairment of the driver, respectively. Besides, we develop a communication module to interact with the systems in IT-CPS asynchronously. Thus, the developed drive safe PoC can guide the vehicle drivers when they are distracted from driving due to the cognitive-behavioral factors. Finally, we have performed a qualitative evaluation to measure the usability and effectiveness of the developed drive safe platform. We observe that the P-value is 0.0041 (i.e., &lt; 0.05) in the ANOVA test. Moreover, the confidence interval analysis also shows significant gains in prevalence value which is around 0.93 for a 95% confidence level. The aforementioned statistical results indicate high reliability in terms of driver's safety and mental state.      
### 63.Learning Dynamical Systems with Side Information  [ :arrow_down: ](https://arxiv.org/pdf/2008.10135.pdf)
>  We present a mathematical and computational framework for the problem of learning a dynamical system from noisy observations of a few trajectories and subject to side information. Side information is any knowledge we might have about the dynamical system we would like to learn besides trajectory data. It is typically inferred from domain-specific knowledge or basic principles of a scientific discipline. We are interested in explicitly integrating side information into the learning process in order to compensate for scarcity of trajectory observations. We identify six types of side information that arise naturally in many applications and lead to convex constraints in the learning problem. First, we show that when our model for the unknown dynamical system is parameterized as a polynomial, one can impose our side information constraints computationally via semidefinite programming. We then demonstrate the added value of side information for learning the dynamics of basic models in physics and cell biology, as well as for learning and controlling the dynamics of a model in epidemiology. Finally, we study how well polynomial dynamical systems can approximate continuously-differentiable ones while satisfying side information (either exactly or approximately). Our overall learning methodology combines ideas from convex optimization, real algebra, dynamical systems, and functional approximation theory, and can potentially lead to new synergies between these areas.      
### 64.Kernel-based Graph Learning from Smooth Signals: A Functional Viewpoint  [ :arrow_down: ](https://arxiv.org/pdf/2008.10065.pdf)
>  The problem of graph learning concerns the construction of an explicit topological structure revealing the relationship between nodes representing data entities, which plays an increasingly important role in the success of many graph-based representations and algorithms in the field of machine learning and graph signal processing. In this paper, we propose a novel graph learning framework that incorporates the node-side and observation-side information, and in particular the covariates that help to explain the dependency structures in graph signals. To this end, we consider graph signals as functions in the reproducing kernel Hilbert space associated with a Kronecker product kernel, and integrate functional learning with smoothness-promoting graph learning to learn a graph representing the relationship between nodes. The functional learning increases the robustness of graph learning against missing and incomplete information in the graph signals. In addition, we develop a novel graph-based regularisation method which, when combined with the Kronecker product kernel, enables our model to capture both the dependency explained by the graph and the dependency due to graph signals observed under different but related circumstances, e.g. different points in time. The latter means the graph signals are free from the i.i.d. assumptions required by the classical graph learning models. Experiments on both synthetic and real-world data show that our methods outperform the state-of-the-art models in learning a meaningful graph topology from graph signals, in particular under heavy noise, missing values, and multiple dependency.      
### 65.Learning Dynamical Systems using Local Stability Priors  [ :arrow_down: ](https://arxiv.org/pdf/2008.10053.pdf)
>  A coupled computational approach to simultaneously learn a vector field and the region of attraction of an equilibrium point from generated trajectories of the system is proposed. The nonlinear identification leverages the local stability information as a prior on the system, effectively endowing the estimate with this important structural property. In addition, the knowledge of the region of attraction plays an experiment design role by informing the selection of initial conditions from which trajectories are generated and by enabling the use of a Lyapunov function of the system as a regularization term. Numerical results show that the proposed method allows efficient sampling and provides an accurate estimate of the dynamics in an inner approximation of its region of attraction.      
### 66.Multi-kernel Passive Stochastic Gradient Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2008.10020.pdf)
>  This paper develops a novel passive stochastic gradient algorithm. In passive stochastic approximation, the stochastic gradient algorithm does not have control over the location where noisy gradients of the cost function are evaluated. Classical passive stochastic gradient algorithms use a kernel that approximates a Dirac delta to weigh the gradients based on how far they are evaluated from the desired point. In this paper we construct a multi-kernel passive stochastic gradient algorithm. The algorithm performs substantially better in high dimensional problems and incorporates variance reduction. We analyze the weak convergence of the multi-kernel algorithm and its rate of convergence. In numerical examples, we study the multi-kernel version of the LMS algorithm to compare the performance with the classical passive version.      
### 67.Ptolemy: Architecture Support for Robust Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.09954.pdf)
>  Deep learning is vulnerable to adversarial attacks, where carefully-crafted input perturbations could mislead a well-trained Deep Neural Network to produce incorrect results. Today's countermeasures to adversarial attacks either do not have capability to detect adversarial samples at inference time, or introduce prohibitively high overhead to be practical at inference time. <br>We propose Ptolemy, an algorithm-architecture co-designed system that detects adversarial attacks at inference time with low overhead and high accuracy.We exploit the synergies between DNN inference and imperative program execution: an input to a DNN uniquely activates a set of neurons that contribute significantly to the inference output, analogous to the sequence of basic blocks exercised by an input in a conventional program. Critically, we observe that adversarial samples tend to activate distinctive paths from those of benign inputs. Leveraging this insight, we propose an adversarial sample detection framework, which uses canary paths generated from offline profiling to detect adversarial samples at runtime. The Ptolemy compiler along with the co-designed hardware enable efficient execution by exploiting the unique algorithmic characteristics. Extensive evaluations show that Ptolemy achieves higher or similar adversarial example detection accuracy than today's mechanisms with a much lower runtime (as low as 2%) overhead.      
### 68.DMRO:A Deep Meta Reinforcement Learning-based Task Offloading Framework for Edge-Cloud Computing  [ :arrow_down: ](https://arxiv.org/pdf/2008.09930.pdf)
>  With the continuous growth of mobile data and the unprecedented demand for computing power, resource-constrained edge devices cannot effectively meet the requirements of Internet of Things (IoT) applications and Deep Neural Network (DNN) computing. As a distributed computing paradigm, edge offloading that migrates complex tasks from IoT devices to edge-cloud servers can break through the resource limitation of IoT devices, reduce the computing burden and improve the efficiency of task processing. However, the problem of optimal offloading decision-making is NP-hard, traditional optimization methods are difficult to achieve results efficiently. Besides, there are still some shortcomings in existing deep learning methods, e.g., the slow learning speed and the failure of the original network parameters when the environment changes. To tackle these challenges, we propose a Deep Meta Reinforcement Learning-based offloading (DMRO) algorithm, which combines multiple parallel DNNs with Q-learning to make fine-grained offloading decisions. By aggregating the perceptive ability of deep learning, the decision-making ability of reinforcement learning, and the rapid environment learning ability of meta-learning, it is possible to quickly and flexibly obtain the optimal offloading strategy from the IoT environment. Simulation results demonstrate that the proposed algorithm achieves obvious improvement over the Deep Q-Learning algorithm and has strong portability in making real-time offloading decisions even in time-varying IoT environments.      
### 69.One Weight Bitwidth to Rule Them All  [ :arrow_down: ](https://arxiv.org/pdf/2008.09916.pdf)
>  Weight quantization for deep ConvNets has shown promising results for applications such as image classification and semantic segmentation and is especially important for applications where memory storage is limited. However, when aiming for quantization without accuracy degradation, different tasks may end up with different bitwidths. This creates complexity for software and hardware support and the complexity accumulates when one considers mixed-precision quantization, in which case each layer's weights use a different bitwidth. Our key insight is that optimizing for the least bitwidth subject to no accuracy degradation is not necessarily an optimal strategy. This is because one cannot decide optimality between two bitwidths if one has a smaller model size while the other has better accuracy. In this work, we take the first step to understand if some weight bitwidth is better than others by aligning all to the same model size using a width-multiplier. Under this setting, somewhat surprisingly, we show that using a single bitwidth for the whole network can achieve better accuracy compared to mixed-precision quantization targeting zero accuracy degradation when both have the same model size. In particular, our results suggest that when the number of channels becomes a target hyperparameter, a single weight bitwidth throughout the network shows superior results for model compression.      
### 70.Symbolic Semantic Segmentation and Interpretation of COVID-19 Lung Infections in Chest CT volumes based on Emergent Languages  [ :arrow_down: ](https://arxiv.org/pdf/2008.09866.pdf)
>  The coronavirus disease (COVID-19) has resulted in a pandemic crippling the a breadth of services critical to daily life. Segmentation of lung infections in computerized tomography (CT) slices could be be used to improve diagnosis and understanding of COVID-19 in patients. Deep learning systems lack interpretability because of their black box nature. Inspired by human communication of complex ideas through language, we propose a symbolic framework based on emergent languages for the segmentation of COVID-19 infections in CT scans of lungs. We model the cooperation between two artificial agents - a Sender and a Receiver. These agents synergistically cooperate using emergent symbolic language to solve the task of semantic segmentation. Our game theoretic approach is to model the cooperation between agents unlike Generative Adversarial Networks (GANs). The Sender retrieves information from one of the higher layers of the deep network and generates a symbolic sentence sampled from a categorical distribution of vocabularies. The Receiver ingests the stream of symbols and cogenerates the segmentation mask. A private emergent language is developed that forms the communication channel used to describe the task of segmentation of COVID infections. We augment existing state of the art semantic segmentation architectures with our symbolic generator to form symbolic segmentation models. Our symbolic segmentation framework achieves state of the art performance for segmentation of lung infections caused by COVID-19. Our results show direct interpretation of symbolic sentences to discriminate between normal and infected regions, infection morphology and image characteristics. We show state of the art results for segmentation of COVID-19 lung infections in CT.      
### 71.Assign and Appraise: Achieving Optimal Performance in Collaborative Teams  [ :arrow_down: ](https://arxiv.org/pdf/2008.09817.pdf)
>  Tackling complex team problems requires understanding each team member's skills in order to devise a task assignment maximizing the team performance. This paper proposes a novel quantitative model describing the decentralized process by which individuals in a team learn who has what abilities, while concurrently assigning tasks to each of the team members. In the model, the appraisal network represents team member's evaluations of one another and each team member chooses their own workload. The appraisals and workload assignment change simultaneously: each member builds their own local appraisal of neighboring members based on the performance exhibited on previous tasks, while the workload is redistributed based on the current appraisal estimates. We show that the appraisal states can be reduced to a lower dimension due to the presence of conserved quantities associated to the cycles of the appraisal network. Building on this, we provide rigorous results characterizing the ability, or inability, of the team to learn each other's skill and thus converge to an allocation maximizing the team performance. We complement our analysis with extensive numerical experiments.      
### 72.Retinal blood flow reversal in out-of-plane vessels imaged with laser Doppler holography  [ :arrow_down: ](https://arxiv.org/pdf/2008.09813.pdf)
>  Laser Doppler holography is a planar blood flow imaging technique recently introduced in ophthalmology. We here present a digital method to reveal the local direction of blood flow with respect to the optical axis based on the Doppler spectrum asymmetry. This directional information is overlaid on standard grayscale blood flow images to depict flow towards the camera in red, and flow away from the camera in blue. The local axial direction of blood flow can be revealed with a high temporal resolution allowing to evidence potential blood flow reversal in out-of-plane retinal vessels. Spectrograms can also be advantageously used to visualize the axial direction of blood flow and perform quantitative blood velocity measurements.      
### 73.Multidomain Multimodal Fusion For Human Action Recognition Using Inertial Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2008.09748.pdf)
>  One of the major reasons for misclassification of multiplex actions during action recognition is the unavailability of complementary features that provide the semantic information about the actions. In different domains these features are present with different scales and intensities. In existing literature, features are extracted independently in different domains, but the benefits from fusing these multidomain features are not realized. To address this challenge and to extract complete set of complementary information, in this paper, we propose a novel multidomain multimodal fusion framework that extracts complementary and distinct features from different domains of the input modality. We transform input inertial data into signal images, and then make the input modality multidomain and multimodal by transforming spatial domain information into frequency and time-spectrum domain using Discrete Fourier Transform (DFT) and Gabor wavelet transform (GWT) respectively. Features in different domains are extracted by Convolutional Neural networks (CNNs) and then fused by Canonical Correlation based Fusion (CCF) for improving the accuracy of human action recognition. Experimental results on three inertial datasets show the superiority of the proposed method when compared to the state-of-the-art.      
### 74.Online Adaptive Learning for Runtime Resource Management of Heterogeneous SoCs  [ :arrow_down: ](https://arxiv.org/pdf/2008.09728.pdf)
>  Dynamic resource management has become one of the major areas of research in modern computer and communication system design due to lower power consumption and higher performance demands. The number of integrated cores, level of heterogeneity and amount of control knobs increase steadily. As a result, the system complexity is increasing faster than our ability to optimize and dynamically manage the resources. Moreover, offline approaches are sub-optimal due to workload variations and large volume of new applications unknown at design time. This paper first reviews recent online learning techniques for predicting system performance, power, and temperature. Then, we describe the use of predictive models for online control using two modern approaches: imitation learning (IL) and an explicit nonlinear model predictive control (NMPC). Evaluations on a commercial mobile platform with 16 benchmarks show that the IL approach successfully adapts the control policy to unknown applications. The explicit NMPC provides 25% energy savings compared to a state-of-the-art algorithm for multi-variable power management of modern GPU sub-systems.      
### 75.Many-shot from Low-shot: Learning to Annotate using Mixed Supervision for Object Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.09694.pdf)
>  Annotating such datasets is highly time consuming and expensive, which motivates the development of weakly supervised and few-shot object detection methods. However, these methods largely underperform with respect to their strongly supervised counterpart, as weak training signals \emph{often} result in partial or oversized detections. Towards solving this problem we introduce, for the first time, an online annotation module (OAM) that learns to generate a many-shot set of \emph{reliable} annotations from a larger volume of weakly labelled images. Our OAM can be jointly trained with any fully supervised two-stage object detection method, providing additional training annotations on the fly. This results in a fully end-to-end strategy that only requires a low-shot set of fully annotated images. The integration of the OAM with Fast(er) R-CNN improves their performance by $17\%$ mAP, $9\%$ AP50 on PASCAL VOC 2007 and MS-COCO benchmarks, and significantly outperforms competing methods using mixed supervision.      
### 76.Biased Mixtures Of Experts: Enabling Computer Vision Inference Under Data Transfer Limitations  [ :arrow_down: ](https://arxiv.org/pdf/2008.09662.pdf)
>  We propose a novel mixture-of-experts class to optimize computer vision models in accordance with data transfer limitations at test time. Our approach postulates that the minimum acceptable amount of data allowing for highly-accurate results can vary for different input space partitions. Therefore, we consider mixtures where experts require different amounts of data, and train a sparse gating function to divide the input space for each expert. By appropriate hyperparameter selection, our approach is able to bias mixtures of experts towards selecting specific experts over others. In this way, we show that the data transfer optimization between visual sensing and processing can be solved as a convex optimization <a class="link-external link-http" href="http://problem.To" rel="external noopener nofollow">this http URL</a> demonstrate the relation between data availability and performance, we evaluate biased mixtures on a range of mainstream computer vision problems, namely: (i) single shot detection, (ii) image super resolution, and (iii) realtime video action classification. For all cases, and when experts constitute modified baselines to meet different limits on allowed data utility, biased mixtures significantly outperform previous work optimized to meet the same constraints on available data.      
