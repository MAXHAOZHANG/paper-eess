# ArXiv eess --Fri, 21 Aug 2020
### 1.Implementation of model predictive control for tracking in embedded systems using a sparse extended ADMM algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2008.09071.pdf)
>  This article presents an implementation of a sparse, low-memory footprint optimization algorithm for the implementation of the model predictive control for tracking formulation in embedded systems. The algorithm is based on an extension of the alternating direction method of multipliers to problems with three separable functions in the objective function. One of the main advantages of the proposed algorithm is that its memory requirements grow linearly with the prediction horizon of the controller. Its sparse implementation is attained by identification of the particular structure of the optimization problem, and not by employing the common sparse algebra techniques, leading to a very computationally efficient implementation. We describe the controller formulation and provide a detailed description of the proposed algorithm, including its pseudocode. We also provide a simple (and sparse) warmstarting procedure that can significantly reduce the number of iterations. Finally, we show some preliminary numerical results of the performance of the algorithm.      
### 2.Maxwell Parallel Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.09042.pdf)
>  Purpose: To develop a general framework for Parallel Imaging (PI) with the use of Maxwell regularization for the estimation of the sensitivity maps (SMs) and constrained optimization for the parameter-free image reconstruction. <br>Theory and Methods: Certain characteristics of both the SMs and the images are routinely used to regularize the otherwise ill-posed optimization-based joint reconstruction from highly accelerated PI data. In this paper we rely on a fundamental property of SMs--they are solutions of Maxwell equations-- we construct the subspace of all possible SM distributions supported in a given field-of-view, and we promote solutions of SMs that belong in this subspace. In addition, we propose a constrained optimization scheme for the image reconstruction, as a second step, once an accurate estimation of the SMs is available. The resulting method, dubbed Maxwell Parallel Imaging (MPI), works seamlessly for arbitrary sequences (both 2D and 3D) with any trajectory and minimal calibration signals. <br>Results: The effectiveness of MPI is illustrated for a wide range of datasets with various undersampling schemes, including radial, variable-density Poisson-disc, and Cartesian, and is compared against the state-of-the-art PI methods. Finally, we include some numerical experiments that demonstrate the memory footprint reduction of the constructed Maxwell basis with the help of tensor decomposition, thus allowing the use of MPI for full 3D image reconstructions. <br>Conclusions: The MPI framework provides a physics-inspired optimization method for the accurate and efficient image reconstruction from arbitrary accelerated scans.      
### 3.Direct Adversarial Training for GANs  [ :arrow_down: ](https://arxiv.org/pdf/2008.09041.pdf)
>  There is an interesting discovery that several neural networks are vulnerable to adversarial examples. That is, many machines learning models misclassify the samples with only a little change which will not be noticed by human eyes. Generative adversarial networks (GANs) are the most popular models for image generation by jointly optimizing discriminator and generator. With stability train, some regularization and normalization have been used to let the discriminator satisfy Lipschitz consistency. In this paper, we have analyzed that the generator may produce adversarial examples for discriminator during the training process, which may cause the unstable training of GANs. For this reason, we propose a direct adversarial training method for GANs. At the same time, we prove that this direct adversarial training can limit the lipschitz constant of the discriminator and accelerate the convergence of the generator. We have verified the advanced performs of the method on multiple baseline networks, such as DCGAN, WGAN, WGAN-GP, and WGAN-LP.      
### 4.Battery State of Charge Modeling for Solar PV Array using Polynomial Regression  [ :arrow_down: ](https://arxiv.org/pdf/2008.09038.pdf)
>  In this manuscript, we have investigated the response of the State of Charge (SoC) and the open-circuit voltage across the dynamic battery model under the variable voltage and current during the charging cycle of the battery. These variable input voltage and current have been obtained using the variable irradiance and surface temperature of a Solar PV array which is connected as an input of the dynamic battery model to store the energy within it. In order to match the Simulation result with reality, these variable irradiance and surface temperature of Solar PV Array with respect to time has been simulated. After forming and storing the energy within the dynamic battery model; the SoC of the battery has been estimated using the Kalman filter approach. After the successful estimation of SoC; the Open Circuit Voltage (OCV) and State of Charge (SoC) have been plotted using the polynomial regression technique. The regression plots between the OCV and SoC have been drawn for the polynomial degree of 2, 3,4, and 5. Results reveal that R$^2$ keeps increasing as we increase the degrees of regression. Simultaneously the value of RMSE keeps decreasing as we increase the degree of the polynomial regression.      
### 5.Co-Design to Enable User-Friendly Tools to Assess the Impact of Future Mobility Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2008.08975.pdf)
>  The design of future mobility solutions (autonomous vehicles, micromobility solutions, etc.) and the design of the mobility systems they enable are closely coupled. Indeed, knowledge about the intended service of novel mobility solutions would impact their design and deployment process, whilst insights about their technological development could significantly affect transportation management policies. This requires tools to study such a coupling and co-design future mobility systems in terms of different objectives. This paper presents a framework to address such co-design problems. In particular, we leverage the recently developed mathematical theory of co-design to frame and solve the problem of designing and deploying an intermodal mobility system, whereby autonomous vehicles service travel demands jointly with micromobility solutions such as shared bikes and e-scooters, and public transit, in terms of fleets sizing, vehicle characteristics, and public transit service frequency. Our framework is modular and compositional, allowing one to describe the design problem as the interconnection of its individual components and to tackle it from a system-level perspective. Moreover, it only requires very general monotonicity assumptions and it naturally handles multiple objectives, delivering the rational solutions on the Pareto front and thus enabling policy makers to select a policy. To showcase our methodology, we present a real-world case study for Washington D.C., USA. Our work suggests that it is possible to create user-friendly optimization tools to systematically assess the costs and benefits of interventions, and that such analytical techniques might inform policy-making in the future.      
### 6.Online inverse reinforcement learning with limited data  [ :arrow_down: ](https://arxiv.org/pdf/2008.08972.pdf)
>  This paper addresses the problem of online inverse reinforcement learning for systems with limited data and uncertain dynamics. In the developed approach, the state and control trajectories are recorded online by observing an agent perform a task, and reward function estimation is performed in real-time using a novel inverse reinforcement learning approach. Parameter estimation is performed concurrently to help compensate for uncertainties in the agent's dynamics. Data insufficiency is resolved by developing a data-driven update law to estimate the optimal feedback controller. The estimated controller can then be queried to artificially create additional data to drive reward function estimation.      
### 7.Transactive Community Microgrids to Share Energy Storage Resources in Portugal  [ :arrow_down: ](https://arxiv.org/pdf/2008.08971.pdf)
>  The contribution of renewable energy sources to Portugal's energy generation portfolio is significant and on the path to achieving 100\% renewable generation by 2050. Most of the new renewable generation capacity will be procured from distributed photovoltaic (PV) generation installed at buildings. The inherent intermittence of PV output combined with a mismatch with demand profile are challenging the operation and resiliency of the electrical grid. Addressing these issues requires leveraging spatio-temporal flexibility of controllable energy resources such as batteries and Electric Vehicles (EV). This need is recognized by regulators in Portugal and the recent renewable generation self-consumption legislation enables generation-surplus trading in communities. Implementing intra-community trading and utilizing the potentials of renewable generation requires oversight and coordination at the community level in the context of transactive energy systems. This paper focuses on addressing energy sharing through a transactive energy market in community microgrids. The proposed framework considers public and commercial buildings with on-site battery storage and numerous EV charging stations as the main source of flexibility. The formulation is tested using real data from a community of buildings on a Portuguese University campus. The results showcase the achieved increase in renewable self-consumption at building and community levels, as well as the reduction in electricity costs.      
### 8.asya: Mindful verbal communication using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.08965.pdf)
>  asya is a mobile application that consists of deep learning models which analyze spectra of a human voice and do noise detection, speaker diarization, gender detection, tempo estimation, and classification of emotions using only voice. All models are language agnostic and capable of running in real-time. Our speaker diarization models have accuracy over 95% on the test data set. These models can be applied for a variety of areas like customer service improvement, sales effective conversations, psychology and couples therapy.      
### 9.Signal Separation Using a Mathematical Model of Physiological Signals for the Measurement of Heart Pulse Wave Propagation with Array Radar  [ :arrow_down: ](https://arxiv.org/pdf/2008.08948.pdf)
>  The arterial pulse wave, which propagates along the artery, is an important indicator of various cardiovascular diseases. By measuring the displacement at multiple parts of the human body, pulse wave velocity can be estimated from the pulse transit time. This paper proposes a technique for signal separation using an antenna array so that pulse wave propagation can be measured in a non-contacting manner. The body displacements due to the pulse wave at different body parts are highly correlated, and cannot be accurately separated using techniques that assume independent or uncorrelated signals. The proposed method formulates the signal separation as an optimization problem, based on a mathematical model of the arterial pulse wave. The objective function in the optimization comprises four terms that are derived based on a small-displacement approximation, unimodal impulse response approximation, and a causality condition. The optimization process was implemented using a genetic algorithm. The effectiveness of the proposed method is demonstrated through numerical simulations and experiments.      
### 10.Distributed Vehicle Grid Integration Over Communication and Physical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.08939.pdf)
>  This paper proposes a distributed framework for vehicle grid integration (VGI) taking into account the communication and physical networks. To this end, we model the electric vehicle (EV) behaviour that includes time of departure, time of arrival, state of charge, required energy, and its objectives, e.g., avoid battery degradation. Next, we formulate the centralised day ahead distribution market (DADM) which explicitly represents the physical system, supports unbalanced three phase networks with delta and wye connections, and incorporates the charging needs of EVs. The solution of the centralised market requires knowledge of EV information in terms of desired energy, departure and arrival times that EV owners are reluctant in providing. Moreover, the computational effort required to solve the DADM in cases of numerous EVs is very intensive. As such, we propose a distributed solution of the DADM clearing mechanism over a time-varying communication network. We illustrate the proposed VGI framework through the 13-bus, 33- bus, and 141-bus distribution feeders.      
### 11.Generating Music with a Self-Correcting Non-Chronological Autoregressive Model  [ :arrow_down: ](https://arxiv.org/pdf/2008.08927.pdf)
>  We describe a novel approach for generating music using a self-correcting, non-chronological, autoregressive model. We represent music as a sequence of edit events, each of which denotes either the addition or removal of a note---even a note previously generated by the model. During inference, we generate one edit event at a time using direct ancestral sampling. Our approach allows the model to fix previous mistakes such as incorrectly sampled notes and prevent accumulation of errors which autoregressive models are prone to have. Another benefit is a finer, note-by-note control during human and AI collaborative composition. We show through quantitative metrics and human survey evaluation that our approach generates better results than orderless NADE and Gibbs sampling approaches.      
### 12.Construction of control barrier function and $C^2$ reference trajectory for constrained attitude maneuvers  [ :arrow_down: ](https://arxiv.org/pdf/2008.08921.pdf)
>  Constrained attitude maneuvers have numerous applications in robotics and aerospace. In our previous work, a general framework to this problem was proposed with resolution completeness guarantee. However, a smooth reference trajectory and a low-level safety-critical controller were lacking. In this work, we propose a novel construction of a $C^2$ continuous reference trajectory based on Bézier curves on $ SO(3) $ that evolves within predetermined cells and eliminates previous stop-and-go behavior. Moreover, we propose a novel zeroing control barrier function on $ SO(3) $ that provides a safety certificate over a set of overlapping cells on $ SO(3) $ while avoiding nonsmooth analysis. The safety certificate is given as a linear constraint on the control input and implemented in real-time. A remedy is proposed to handle the states where the coefficient of the control input in the linear constraint vanishes. Numerical simulations are given to verify the advantages of the proposed method.      
### 13.A Review on Coexistence Issues of Broadband Millimeter-Wave Communications  [ :arrow_down: ](https://arxiv.org/pdf/2008.08916.pdf)
>  With higher frequencies and broader spectrum than conventional frequency bands, the millimeter-wave (mmWave) band is suitable for next-generation wireless networks featuring short-distance high-rate communications. As a newcomer, mmWaves are expected to have the backward compatibility with existing services and collaborate with other technologies in order to enhance system performances. Therefore, the coexistence issues become an essential topic for next-generation wireless communications. In this paper, we systematically review the coexistence issues of broadband mmWave communications and their corresponding solutions proposed in the literature, helping shed light on the insights of the mmWave design. Particularly, the works surveyed in this paper can be classified into four categories: coexistence with microwave communications, coexistence with fixed services, coexistence with non-orthogonal multiple access (NOMA), and other coexistence issues. Results of numerical evaluations inspired by the literature are presented for a deeper analysis. We also point out some challenges and future directions for each category as a roadmap to further investigate the coexistence issues of broadband mmWave communications.      
### 14.State Observation of LTV Systems with Delayed Measurements: A Parameter Estimation-based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2008.08913.pdf)
>  In this paper we address the problem of state observation of linear time-varying systems with delayed measurements, which has attracted the attention of many researchers|see [7] and references therein. We show that, adopting the parameter estimationbased approach proposed in [3,4], we can provide a very simple solution to the problem with reduced prior knowledge.      
### 15.A Data-Efficient Deep Learning Based Smartphone Application For Detection Of Pulmonary Diseases Using Chest X-rays  [ :arrow_down: ](https://arxiv.org/pdf/2008.08912.pdf)
>  This paper introduces a paradigm of smartphone application based disease diagnostics that may completely revolutionise the way healthcare services are being provided. Although primarily aimed to assist the problems in rendering the healthcare services during the coronavirus pandemic, the model can also be extended to identify the exact disease that the patient is caught with from a broad spectrum of pulmonary diseases. The app inputs Chest X-Ray images captured from the mobile camera which is then relayed to the AI architecture in a cloud platform, and diagnoses the disease with state of the art accuracy. Doctors with a smartphone can leverage the application to save the considerable time that standard COVID-19 tests take for preliminary diagnosis. The scarcity of training data and class imbalance issues were effectively tackled in our approach by the use of Data Augmentation Generative Adversarial Network (DAGAN) and model architecture based as a Convolutional Siamese Network with attention mechanism. The backend model was tested for robustness us-ing publicly available datasets under two different classification scenarios(Binary/Multiclass) with minimal and noisy data. The model achieved pinnacle testing accuracy of 99.30% and 98.40% on the two respective scenarios, making it completely reliable for its users. On top of that a semi-live training scenario was introduced, which helps improve the app performance over time as data accumulates. Overall, the problems of generalisability of complex models and data inefficiency is tackled through the model architecture. The app based setting with semi live training helps in ease of access to reliable healthcare in the society, as well as help ineffective research of rare diseases in a minimal data setting.      
### 16.Speaker-Utterance Dual Attention for Speaker and Utterance Verification  [ :arrow_down: ](https://arxiv.org/pdf/2008.08901.pdf)
>  In this paper, we study a novel technique that exploits the interaction between speaker traits and linguistic content to improve both speaker verification and utterance verification performance. We implement an idea of speaker-utterance dual attention (SUDA) in a unified neural network. The dual attention refers to an attention mechanism for the two tasks of speaker and utterance verification. The proposed SUDA features an attention mask mechanism to learn the interaction between the speaker and utterance information streams. This helps to focus only on the required information for respective task by masking the irrelevant counterparts. The studies conducted on RSR2015 corpus confirm that the proposed SUDA outperforms the framework without attention mask as well as several competitive systems for both speaker and utterance verification.      
### 17.Deep learning-based transformation of the H&amp;E stain into special stains improves kidney disease diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2008.08871.pdf)
>  Pathology is practiced by visual inspection of histochemically stained slides. Most commonly, the hematoxylin and eosin (H&amp;E) stain is used in the diagnostic workflow and it is the gold standard for cancer diagnosis. However, in many cases, especially for non-neoplastic diseases, additional "special stains" are used to provide different levels of contrast and color to tissue components and allow pathologists to get a clearer diagnostic picture. In this study, we demonstrate the utility of supervised learning-based computational stain transformation from H&amp;E to different special stains (Masson's Trichrome, periodic acid-Schiff and Jones silver stain) using tissue sections from kidney needle core biopsies. Based on evaluation by three renal pathologists, followed by adjudication by a fourth renal pathologist, we show that the generation of virtual special stains from existing H&amp;E images improves the diagnosis in several non-neoplastic kidney diseases, sampled from 16 unique subjects. Adjudication of N=48 diagnoses from the three pathologists revealed that the virtually generated special stains yielded 22 improvements (45.8%), 23 concordances (47.9%) and 3 discordances (6.3%), when compared against the use of H&amp;E stained tissue only. As the virtual transformation of H&amp;E images into special stains can be achieved in less than 1 min per patient core specimen slide, this stain-to-stain transformation framework can improve the quality of the preliminary diagnosis when additional special stains are needed, along with significant savings in time and cost, reducing the burden on healthcare system and patients.      
### 18.Using Multi-Resolution Feature Maps with Convolutional Neural Networks for Anti-Spoofing in ASV  [ :arrow_down: ](https://arxiv.org/pdf/2008.08865.pdf)
>  This paper presents a simple but effective method that uses multi-resolution feature maps with convolutional neural networks (CNNs) for anti-spoofing in automatic speaker verification (ASV). The central idea is to alleviate the problem that the feature maps commonly used in anti-spoofing networks are insufficient for building discriminative representations of audio segments, as they are often extracted by a single-length sliding window. Resulting trade-offs between time and frequency resolutions restrict the information in single spectrograms. The proposed method improves both frequency resolution and time resolution by stacking multiple spectrograms that are extracted using different window lengths. These are fed into a convolutional neural network in the form of multiple channels, making it possible to extract more information from input signals while only marginally increasing computational costs. The efficiency of the proposed method has been conformed on the ASVspoof 2019 database. We show that the use of the proposed multiresolution inputs consistently outperforms that of score fusion across different CNN architectures. Moreover, computational cost remains small.      
### 19.Image quality assessment for closed-loop computer-assisted lung ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2008.08840.pdf)
>  We describe a novel, two-stage computer assistance system for lung anomaly detection using ultrasound imaging in the intensive care setting to improve operator performance and patient stratification during coronavirus pandemics. The proposed system consists of two deep-learning-based models. A quality assessment module automates predictions of image quality, and a diagnosis assistance module determines the likelihood-of-anomaly in ultrasound images of sufficient quality. Our two-stage strategy uses a novelty detection algorithm to address the lack of control cases available for training a quality assessment classifier. The diagnosis assistance module can then be trained with data that are deemed of sufficient quality, guaranteed by the closed-loop feedback mechanism from the quality assessment module. Integrating the two modules yields accurate, fast, and practical acquisition guidance and diagnostic assistance for patients with suspected respiratory conditions at the point-of-care. Using more than 25,000 ultrasound images from 37 COVID-19-positive patients scanned at two hospitals, plus 12 control cases, this study demonstrates the feasibility of using the proposed machine learning approach. We report an accuracy of 86% when classifying between sufficient and insufficient quality images by the quality assessment module. For data of sufficient quality, the mean classification accuracy in detecting COVID-19-positive cases was 95% on five holdout test data sets, unseen during the training of any networks within the proposed system.      
### 20.Uncertainty Estimation in Medical Image Denoising with Bayesian Deep Image Prior  [ :arrow_down: ](https://arxiv.org/pdf/2008.08837.pdf)
>  Uncertainty quantification in inverse medical imaging tasks with deep learning has received little attention. However, deep models trained on large data sets tend to hallucinate and create artifacts in the reconstructed output that are not anatomically present. We use a randomly initialized convolutional network as parameterization of the reconstructed image and perform gradient descent to match the observation, which is known as deep image prior. In this case, the reconstruction does not suffer from hallucinations as no prior training is performed. We extend this to a Bayesian approach with Monte Carlo dropout to quantify both aleatoric and epistemic uncertainty. The presented method is evaluated on the task of denoising different medical imaging modalities. The experimental results show that our approach yields well-calibrated uncertainty. That is, the predictive uncertainty correlates with the predictive error. This allows for reliable uncertainty estimates and can tackle the problem of hallucinations and artifacts in inverse medical imaging tasks.      
### 21.Period and Signal Reconstruction from the Curve of Sample-Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2008.08832.pdf)
>  A sequence of samples of a periodic signal can be treated as a point in a multi-dimensional space. All such sequences of a given length and taken at a given sampling rate form a closed curve. We prove that this curve determines the period of the sampled signal, even if the sequences are of short length and are taken at a sub-Nyquist rate. This result is obtained with a help of the theory of rotation numbers developed by Poincaré. We also prove that the curve of sample-sequences determines the sampled signal up to a time shift provided that the ratio of the sampling period to the period of the signal is irrational. Eventually, we give an example, which shows that the same is not true if this ratio is rational.      
### 22.A Generalized Framework for Domain Adaptation of PLDA in Speaker Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.08815.pdf)
>  This paper proposes a generalized framework for domain adaptation of Probabilistic Linear Discriminant Analysis (PLDA) in speaker recognition. It not only includes several existing supervised and unsupervised domain adaptation methods but also makes possible more flexible usage of available data in different domains. In particular, we introduce here the two new techniques described below. (1) Correlation-alignment-based interpolation and (2) covariance regularization. The proposed correlation-alignment-based interpolation method decreases minCprimary up to 30.5% as compared with that from an out-of-domain PLDA model before adaptation, and minCprimary is also 5.5% lower than with a conventional linear interpolation method with optimal interpolation weights. Further, the proposed regularization technique ensures robustness in interpolations w.r.t. varying interpolation weights, which in practice is essential.      
### 23.Assessing the Quality-of-Experience of Adaptive Bitrate Video Streaming  [ :arrow_down: ](https://arxiv.org/pdf/2008.08804.pdf)
>  The diversity of video delivery pipeline poses a grand challenge to the evaluation of adaptive bitrate (ABR) streaming algorithms and objective quality-of-experience (QoE) models. Here we introduce so-far the largest subject-rated database of its kind, namely WaterlooSQoE-IV, consisting of 1350 adaptive streaming videos created from diverse source contents, video encoders, network traces, ABR algorithms, and viewing devices. We collect human opinions for each video with a series of carefully designed subjective experiments. Subsequent data analysis and testing/comparison of ABR algorithms and QoE models using the database lead to a series of novel observations and interesting findings, in terms of the effectiveness of subjective experiment methodologies, the interactions between user experience and source content, viewing device and encoder type, the heterogeneities in the bias and preference of user experiences, the behaviors of ABR algorithms, and the performance of objective QoE models. Most importantly, our results suggest that a better objective QoE model, or a better understanding of human perceptual experience and behaviour, is the most dominating factor in improving the performance of ABR algorithms, as opposed to advanced optimization frameworks, machine learning strategies or bandwidth predictors, where a majority of ABR research has been focused on in the past decade. On the other hand, our performance evaluation of 11 QoE models shows only a moderate correlation between state-of-the-art QoE models and subjective ratings, implying rooms for improvement in both QoE modeling and ABR algorithms. The database is made publicly available at: \url{<a class="link-external link-https" href="https://ece.uwaterloo.ca/~zduanmu/waterloosqoe4/" rel="external noopener nofollow">this https URL</a>}.      
### 24.Distributed Stochastic Subgradient Optimization Algorithms Over Random and Noisy Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.08796.pdf)
>  We study distributed stochastic optimization by networked nodes to cooperatively minimize a sum of convex cost functions. The network is modeled by a sequence of time-varying random digraphs with each node representing a local optimizer and each edge representing a communication link. We consider the distributed subgradient optimization algorithm with noisy measurements of local cost functions' subgradients, additive and multiplicative noises among information exchanging between each pair of nodes. By stochastic Lyapunov method, convex analysis, algebraic graph theory and martingale convergence theory, it is proved that if the local subgradient functions grow linearly and the sequence of digraphs is conditionally balanced and uniformly conditionally jointly connected, then proper algorithm step sizes can be designed so that all nodes' states converge to the global optimal solution almost surely.      
### 25.Joint Visual and Wireless Signal Feature based Approach for High-Precision Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2008.08790.pdf)
>  The existing localization systems for indoor applications basically rely on wireless signal. With the massive deployment of low-cost cameras, the visual image based localization become attractive as well. However, in the existing literature, the hybrid visual and wireless approaches simply combine the above schemes in a straight forward manner, and fail to explore the interactions between them. In this paper, we propose a joint visual and wireless signal feature based approach for high-precision indoor localization system. In this joint scheme, WiFi signals are utilized to compute the coarse area with likelihood probability and visual images are used to fine-tune the localization result. Based on the numerical results, we show that the proposed scheme can achieve 0.62m localization accuracy with near real-time running time.      
### 26.Single Image Super-Resolution via a Holistic Attention Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.08767.pdf)
>  Informative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer. However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions. Specifically, the proposed LAM adaptively emphasizes hierarchical features by considering correlations among layers. Meanwhile, CSAM learns the confidence at all the positions of each channel to selectively capture more informative features. Extensive experiments demonstrate that the proposed HAN performs favorably against the state-of-the-art single image super-resolution approaches.      
### 27.Semi-Blind and l1 Robust System Identification for Anemia Management  [ :arrow_down: ](https://arxiv.org/pdf/2008.08758.pdf)
>  Chronic diseases such as cancer, diabetes, heart diseases, chronic kidney disease (CKD) require a drug management system that ensures a stable and robust output of the patient's condition in response to drug dosage. In the case of CKD, the patients suffer from the deficiency of red blood cell count and external human recombinant erythropoietin (EPO) is required to maintain healthy levels of hemoglobin (Hb). Anemia is a common comorbidity in patients with CKD. For an efficient and robust anemia management system for CKD patients instead of traditional population-based approaches, individualized patient-specific approaches are needed. Hence, individualized system (patient) models for patient-specific drug-dose responses are required. In this research, system identification for CKD is performed for individual patients. For control-oriented system identification, two robust identification techniques are applied: (1) l1 robust identification considering zero initial conditions and (2) semi-blind robust system identification considering non-zero initial conditions. The EPO data of patients are used as the input and Hb data is used as the output of the system. For this study, individualized patient models are developed by using patient-specific data. The ARX one-step-ahead prediction technique is used for model validation at real patient data. The performance of these two techniques is compared by calculating minimum means square error (MMSE). By comparison, we show that the semi-blind robust identification technique gives better results as compared to l1 robust identification.      
### 28.Model-free optimal control of discrete-time systems with additive and multiplicative noises  [ :arrow_down: ](https://arxiv.org/pdf/2008.08734.pdf)
>  This paper investigates the optimal control problem for a class of discrete-time stochastic systems subject to additive and multiplicative noises. A stochastic Lyapunov equation and a stochastic algebra Riccati equation are established for the existence of the optimal admissible control policy. A model-free reinforcement learning algorithm is proposed to learn the optimal admissible control policy using the data of the system states and inputs without requiring any knowledge of the system matrices. It is proven that the learning algorithm converges to the optimal admissible control policy. The implementation of the model-free algorithm is based on batch least squares and numerical average. The proposed algorithm is illustrated through a numerical example, which shows our algorithm outperforms other policy iteration algorithms.      
### 29.GPR-based Subsurface Object Detection and Reconstruction Using Random Motion and DepthNet  [ :arrow_down: ](https://arxiv.org/pdf/2008.08731.pdf)
>  Ground Penetrating Radar (GPR) is one of the most important non-destructive evaluation (NDE) devices to detect the subsurface objects (i.e. rebars, utility pipes) and reveal the underground scene. One of the biggest challenges in GPR based inspection is the subsurface targets reconstruction. In order to address this issue, this paper presents a 3D GPR migration and dielectric prediction system to detect and reconstruct underground targets. This system is composed of three modules: 1) visual inertial fusion (VIF) module to generate the pose information of GPR device, 2) deep neural network module (i.e., DepthNet) which detects B-scan of GPR image, extracts hyperbola features to remove the noise in B-scan data and predicts dielectric to determine the depth of the objects, 3) 3D GPR migration module which synchronizes the pose information with GPR scan data processed by DepthNet to reconstruct and visualize the 3D underground targets. Our proposed DepthNet processes the GPR data by removing the noise in B-scan image as well as predicting depth of subsurface objects. For DepthNet model training and testing, we collect the real GPR data in the concrete test pit at Geophysical Survey System Inc. (GSSI) and create the synthetic GPR data by using gprMax3.0 simulator. The dataset we create includes 350 labeled GPR images. The DepthNet achieves an average accuracy of 92.64% for B-scan feature detection and an 0.112 average error for underground target depth prediction. In addition, the experimental results verify that our proposed method improve the migration accuracy and performance in generating 3D GPR image compared with the traditional migration methods.      
### 30.Total Cost of Ownership Optimization for Direct Air-to-Ground System Design  [ :arrow_down: ](https://arxiv.org/pdf/2008.08728.pdf)
>  Aircraft cabin is one of the last venues without mobile broadband. Considering future 5G applications and connectivity requirements, direct air-to-ground communications (DA2GC) is the only technique which can provide high capacity and low latency backhaul link for aircraft via a direct communication link. To this end, we propose an analytical framework to investigate the ground station deployment problem for DA2GC network employing multi-user beamforming with dual-polarized hybrid DA2GC antenna arrays. In addition, the proposed framework is utilized to analyze and optimize the total cost of ownership (TCO) of the DA2GC network to provide coverage for European airspace. We present the interplay between different network parameters: the number of ground stations, array size, transmit power and bandwidth, and TCO optimizing deployment parameters are calculated in order to satisfy capacity requirements. At the end, we show that, depending on the cost of different network resources, a terrestrial cellular network can be designed to cover the whole European airspace with limited number of ground stations with a certain array size, i.e., 900 and 361 antenna elements for ground station and air station, respectively.      
### 31.Self-Supervised Ultrasound to MRI Fetal Brain Image Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2008.08698.pdf)
>  Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the developing brain but is not suitable for second-trimester anomaly screening, for which ultrasound (US) is employed. Although expert sonographers are adept at reading US images, MR images which closely resemble anatomical images are much easier for non-experts to interpret. Thus in this paper we propose to generate MR-like images directly from clinical US images. In medical image analysis such a capability is potentially useful as well, for instance for automatic US-MRI registration and fusion. The proposed model is end-to-end trainable and self-supervised without any external annotations. Specifically, based on an assumption that the US and MRI data share a similar anatomical latent space, we first utilise a network to extract the shared latent features, which are then used for MRI synthesis. Since paired data is unavailable for our study (and rare in practice), pixel-level constraints are infeasible to apply. We instead propose to enforce the distributions to be statistically indistinguishable, by adversarial learning in both the image domain and feature space. To regularise the anatomical structures between US and MRI during synthesis, we further propose an adversarial structural constraint. A new cross-modal attention technique is proposed to utilise non-local spatial information, by encouraging multi-modal knowledge fusion and propagation. We extend the approach to consider the case where 3D auxiliary information (e.g., 3D neighbours and a 3D location index) from volumetric data is also available, and show that this improves image synthesis. The proposed approach is evaluated quantitatively and qualitatively with comparison to real fetal MR images and other approaches to synthesis, demonstrating its feasibility of synthesising realistic MR images.      
### 32.Inner Cell Mass and Trophectoderm Segmentation in Human Blastocyst Images using Deep Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.08676.pdf)
>  Embryo quality assessment based on morphological attributes is important for achieving higher pregnancy rates from in vitro fertilization (IVF). The accurate segmentation of the embryo's inner cell mass (ICM) and trophectoderm epithelium (TE) is important, as these parameters can help to predict the embryo viability and live birth potential. However, segmentation of the ICM and TE is difficult due to variations in their shape and similarities in their textures, both with each other and with their surroundings. To tackle this problem, a deep neural network (DNN) based segmentation approach was implemented. The DNN can identify the ICM region with 99.1% accuracy, 94.9% precision, 93.8% recall, a 94.3% Dice Coefficient, and a 89.3% Jaccard Index. It can extract the TE region with 98.3% accuracy, 91.8% precision, 93.2% recall, a 92.5% Dice Coefficient, and a 85.3% Jaccard Index.      
### 33.Context-aware Goodness of Pronunciation for Computer-Assisted Pronunciation Training  [ :arrow_down: ](https://arxiv.org/pdf/2008.08647.pdf)
>  Mispronunciation detection is an essential component of the Computer-Assisted Pronunciation Training (CAPT) systems. State-of-the-art mispronunciation detection models use Deep Neural Networks (DNN) for acoustic modeling, and a Goodness of Pronunciation (GOP) based algorithm for pronunciation scoring. <br>However, GOP based scoring models have two major limitations: i.e., (i) They depend on forced alignment which splits the speech into phonetic segments and independently use them for scoring, which neglects the transitions between phonemes within the segment; <br>(ii) They only focus on phonetic segments, which fails to consider the context effects across phonemes (such as liaison, omission, incomplete plosive sound, etc.). <br>In this work, we propose the Context-aware Goodness of Pronunciation (CaGOP) scoring model. Particularly, two factors namely the transition factor and the duration factor are injected into CaGOP scoring. <br>The transition factor identifies the transitions between phonemes and applies them to weight the frame-wise GOP. Moreover, a self-attention based phonetic duration modeling is proposed to introduce the duration factor into the scoring model. <br>The proposed scoring model significantly outperforms baselines, achieving 20% and 12% relative improvement over the GOP model on the phoneme-level and sentence-level mispronunciation detection respectively.      
### 34.Generative View Synthesis: From Single-view Semantics to Novel-view Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.09106.pdf)
>  Content creation, central to applications such as virtual reality, can be a tedious and time-consuming. Recent image synthesis methods simplify this task by offering tools to generate new views from as little as a single input image, or by converting a semantic map into a photorealistic image. We propose to push the envelope further, and introduce \emph{Generative View Synthesis} (GVS), which can synthesize multiple photorealistic views of a scene given a single semantic map. We show that the sequential application of existing techniques, e.g., semantics-to-image translation followed by monocular view synthesis, fail at capturing the scene's structure. In contrast, we solve the semantics-to-image translation in concert with the estimation of the 3D layout of the scene, thus producing geometrically consistent novel views that preserve semantic structures. We first lift the input 2D semantic map onto a 3D layered representation of the scene in feature space, thereby preserving the semantic labels of 3D geometric structures. We then project the layered features onto the target views to generate the final novel-view images. We verify the strengths of our method and compare it with several advanced baselines on three different datasets. Our approach also allows for style manipulation and image editing operations, such as the addition or removal of objects, with simple manipulations of the input style images and semantic maps respectively. Visit the project page at <a class="link-external link-https" href="https://gvsnet.github.io" rel="external noopener nofollow">this https URL</a>.      
### 35.A review of deep learning in medical imaging: Image traits, technology trends, case studies with progress highlights, and future promises  [ :arrow_down: ](https://arxiv.org/pdf/2008.09104.pdf)
>  Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first highlight both clinical needs and technical challenges in medical imaging and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.      
### 36.A Plug-and-play Scheme to Adapt Image Saliency Deep Model for Video Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.09103.pdf)
>  With the rapid development of deep learning techniques, image saliency deep models trained solely by spatial information have occasionally achieved detection performance for video data comparable to that of the models trained by both spatial and temporal information. However, due to the lesser consideration of temporal information, the image saliency deep models may become fragile in the video sequences dominated by temporal information. Thus, the most recent video saliency detection approaches have adopted the network architecture starting with a spatial deep model that is followed by an elaborately designed temporal deep model. However, such methods easily encounter the performance bottleneck arising from the single stream learning methodology, so the overall detection performance is largely determined by the spatial deep model. In sharp contrast to the current mainstream methods, this paper proposes a novel plug-and-play scheme to weakly retrain a pretrained image saliency deep model for video data by using the newly sensed and coded temporal information. Thus, the retrained image saliency deep model will be able to maintain temporal saliency awareness, achieving much improved detection performance. Moreover, our method is simple yet effective for adapting any off-the-shelf pre-trained image saliency deep model to obtain high-quality video saliency detection. Additionally, both the data and source code of our method are publicly available.      
### 37.Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.09092.pdf)
>  Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods. Project page: <a class="link-external link-https" href="https://nv-tlabs.github.io/meta-sim-structure/" rel="external noopener nofollow">this https URL</a>.      
### 38.Substrate engineering of inductors on SOI for improvement of Q-factor and application in LNA  [ :arrow_down: ](https://arxiv.org/pdf/2008.09030.pdf)
>  High Q-factor inductors are critical in designing high performance RF/microwave circuits on SOI technology. Substrate losses is a key limiting factor when designing inductors with high Q-factors. In this context, we report a substrate engineering method that enables improvement of quality factors of already fabricated inductors on SOI. A novel femtosecond laser milling process is utilized for the fabrication of locally suspended membranes of inductors with handler silicon completely etched. Such flexible membranes suspended freely on the BOX show up to 92 % improvement in Q factor for single turn inductor. The improvement in Q-factor is reported on large sized inductors due to reduced parallel capacitance which allows enhanced operation of inductors at high frequencies. A compact model extraction methodology has been developed to model inductor membranes. These membranes have been utilized for the improvement of noise performance of LNA working in the 4.9,5.9 GHz range. A 0.1 dB improvement in noise figure has been reported by taking an existing design and suspending the input side inductors of the LNA circuit. The substrate engineering method reported in this work is not only applicable to inductors but also to active circuits, making it a powerful tool for enhancement of RF devices.      
### 39.Detecting Aedes Aegypti Mosquitoes through Audio Classification with Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.09024.pdf)
>  The incidence of mosquito-borne diseases is significant in under-developed regions, mostly due to the lack of resources to implement aggressive control measurements against mosquito proliferation. A potential strategy to raise community awareness regarding mosquito proliferation is building a live map of mosquito incidences using smartphone apps and crowdsourcing. In this paper, we explore the possibility of identifying Aedes aegypti mosquitoes using machine learning techniques and audio analysis captured from commercially available smartphones. In summary, we downsampled Aedes aegypti wingbeat recordings and used them to train a convolutional neural network (CNN) through supervised learning. As a feature, we used the recording spectrogram to represent the mosquito wingbeat frequency over time visually. We trained and compared three classifiers: a binary, a multiclass, and an ensemble of binary classifiers. In our evaluation, the binary and ensemble models achieved accuracy of 97.65% ($\pm$ 0.55) and 94.56% ($\pm$ 0.77), respectively, whereas the multiclass had an accuracy of 78.12% ($\pm$ 2.09). The best sensitivity was observed in the ensemble approach (96.82% $\pm$ 1.62), followed by the multiclass for the particular case of Aedes aegypti (90.23% $\pm$ 3.83) and the binary (88.49% $\pm$ 6.68). The binary classifier and the multiclass classifier presented the best balance between precision and recall, with F1-measure close to 90%. Although the ensemble classifier achieved the lowest precision, thus impairing its F1-measure (79.95% $\pm$ 2.13), it was the most powerful classifier to detect Aedes aegypti in our dataset.      
### 40.Balanced Order Batching with Task-Oriented Graph Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2008.09018.pdf)
>  Balanced order batching problem (BOBP) arises from the process of warehouse picking in Cainiao, the largest logistics platform in China. Batching orders together in the picking process to form a single picking route, reduces travel distance. The reason for its importance is that order picking is a labor intensive process and, by using good batching methods, substantial savings can be obtained. The BOBP is a NP-hard combinational optimization problem and designing a good problem-specific heuristic under the quasi-real-time system response requirement is non-trivial. In this paper, rather than designing heuristics, we propose an end-to-end learning and optimization framework named Balanced Task-orientated Graph Clustering Network (BTOGCN) to solve the BOBP by reducing it to balanced graph clustering optimization problem. In BTOGCN, a task-oriented estimator network is introduced to guide the type-aware heterogeneous graph clustering networks to find a better clustering result related to the BOBP objective. Through comprehensive experiments on single-graph and multi-graphs, we show: 1) our balanced task-oriented graph clustering network can directly utilize the guidance of target signal and outperforms the two-stage deep embedding and deep clustering method; 2) our method obtains an average 4.57m and 0.13m picking distance ("m" is the abbreviation of the meter (the SI base unit of length)) reduction than the expert-designed algorithm on single and multi-graph set and has a good generalization ability to apply in practical scenario.      
### 41.Randomness in appendage coordination facilitates strenuous ground self-righting  [ :arrow_down: ](https://arxiv.org/pdf/2008.08983.pdf)
>  Randomness is common in biological and artificial systems, resulting either from stochasticity of the environment or noise in organisms or devices themselves. In locomotor control, randomness is typically considered a nuisance. For example, during dynamic walking, randomness in stochastic terrain leads to metastable dynamics, which must be mitigated to stabilize the system around limit cycles. Here, we studied whether randomness in motion is beneficial for strenuous locomotor tasks. Our study used robotic simulation modeling of strenuous, leg-assisted, winged ground self-righting observed in cockroaches, in which unusually large randomness in wing and leg motions is present. We developed a simplified simulation robot capable of generating similar self-righting behavior and varied the randomness level in wing-leg coordination. During each wing opening attempt, the more randomness added to the time delay between wing opening and leg swinging, the more likely it was for the naive robot (which did not know what coordination is best) to self-right within a finite time. Wing-leg coordination, measured by the phase between wing and leg oscillations, had a crucial impact on self-righting outcome. Without randomness, periodic wing and leg oscillations often limited the system to visit a few bad phases, leading to failure to escape from the metastable state. With randomness, the system explored phases thoroughly and had a better chance of encountering good phases to self-right. Our study complements previous work by demonstrating that randomness helps destabilize locomotor systems from being trapped in undesired metastable states, a situation common in strenuous locomotion.      
### 42.Coordinated appendages accumulate more energy to self-right on the ground  [ :arrow_down: ](https://arxiv.org/pdf/2008.08981.pdf)
>  Animals and robots must right themselves after flipping over on the ground. The discoid cockroach pushes its wings against the ground in an attempt to dynamically self-right by a somersault. However, because this maneuver is strenuous, the animal often fails to overcome the potential energy barrier and makes continual attempts. In this process, the animal flails its legs, whose lateral perturbation eventually leads it to roll to the side to self-right. Our previous work developed a cockroach-inspired robot capable of leg-assisted, winged self-righting, and a robot simulation study revealed that the outcome of this strategy depends sensitively on wing-leg coordination (measured by the phase between their motions). Here, we further elucidate why this is the case by developing a template to model the complex hybrid dynamics resulting from discontinuous contact and actuation. We used the template to calculate the potential energy barrier that the body must overcome to self-right, mechanical energy contribution by wing pushing and leg flailing, and mechanical energy dissipation due to wing-ground collision. The template revealed that wing-leg coordination (phase) strongly affects self-righting outcome by changing mechanical energy budget. Well-coordinated appendage motions (good phase) accumulate more mechanical energy than poorly-coordinated motions (bad phase), thereby better overcoming the potential energy barrier to self-right more successfully. Finally, we demonstrated practical use of the template for predicting a new control strategy to further increase self-righting performance and informing robot design.      
### 43.Generating Adjacency Matrix for Video-Query based Video Moment Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2008.08977.pdf)
>  In this paper, we continue our work on Video-Query based Video Moment retrieval task. Based on using graph convolution to extract intra-video and inter-video frame features, we improve the method by using similarity-metric based graph convolution, whose weighted adjacency matrix is achieved by calculating similarity metric between features of any two different timesteps in the graph. Experiments on ActivityNet v1.2 and Thumos14 dataset shows the effectiveness of this improvement, and it outperforms the state-of-the-art methods.      
### 44.Improving Text to Image Generation using Mode-seeking Function  [ :arrow_down: ](https://arxiv.org/pdf/2008.08976.pdf)
>  Generative Adversarial Networks (GANs) have long been used to understand the semantic relationship between the text and image. However, there are problems with mode collapsing in the image generation that causes some preferred output modes. Our aim is to improve the training of the network by using a specialized mode-seeking loss function to avoid this issue. In the text to image synthesis, our loss function differentiates two points in latent space for the generation of distinct images. We validate our model on the Caltech Birds (CUB) dataset and the Microsoft COCO dataset by changing the intensity of the loss function during the training. Experimental results demonstrate that our model works very well compared to some state-of-the-art approaches.      
### 45.Regularization And Normalization For Generative Adversarial Networks: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2008.08930.pdf)
>  Generative adversarial networks(GANs) is a popular generative model. With the development of the deep network, its application is more and more widely. By now, people think that the training of GANs is a two-person zero-sum game(discriminator and generator). The lack of strong supervision information makes the training very difficult, such as non-convergence, mode collapses, gradient disappearance, and the sensitivity of hyperparameters. As we all know, regularization and normalization are commonly used for stability training. This paper reviews and summarizes the research in the regularization and normalization for GAN. All the methods are classified into six groups: Gradient penalty, Norm normalization and regularization, Jacobian regularization, Layer normalization, Consistency regularization, and Self-supervision.      
### 46.A Passive Circuit-Emulator for a Current-controlled Memristor  [ :arrow_down: ](https://arxiv.org/pdf/2008.08925.pdf)
>  A memristor is an electrical element, which has been conjectured in 1971 to complete the lumped circuit theory. Currently, researchers use memristors emulators through diodes and other passive (or active) elements to study circuits with possible attractors, chaos, and ways of implementing nonlinear transformations for low-voltage novel computing paradigms. However, to date, such passive memristor emulators have been voltage-controlled. In this study, the first circuit realization of a current-controlled passive emulator is established. The formal theory and simulations validate the proposed circuit.      
### 47.Generative Adversarial Networks for Spatio-temporal Data: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2008.08903.pdf)
>  Generative Adversarial Networks (GANs) have shown remarkable success in the computer vision area for producing realistic-looking images. Recently, GAN-based techniques are shown to be promising for spatiotemporal-based applications such as trajectory prediction, events generation and time-series data imputation. While several reviews for GANs in computer vision been presented, nobody has considered addressing the practical applications and challenges relevant to spatio-temporal data. In this paper, we conduct a comprehensive review of the recent developments of GANs in spatio-temporal data. we summarise the popular GAN architectures in spatio-temporal data and common practices for evaluating the performance of spatio-temporal applications with GANs. In the end, we point out the future directions with the hope of benefiting researchers interested in this area.      
### 48.Coded Caching over Multicast Routing Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.08900.pdf)
>  The coded caching scheme originally proposed by Maddah-Ali and Niesen (MAN) transmits coded multicast messages from a server to users equipped with caches via a capacitated shared-link and was shown to be information theoretically optimal within a constant multiplicative factor. This work extends the MAN scheme to a class of two-hop wired-wireless networks including one server connected via fronthaul links to a layer of $H$ helper nodes (access points/base stations), which in turns communicate via a wireless access network to $K$ users, each equipped with its own cache. Two variants are considered, which differ in the modeling of the access segment. Both models should be regarded as abstractions at the network layer for physical scenarios such as local area networks and cellular networks, spatially distributed over a certain coverage area. The key focus of our approach consists of routing MAN-type multicast messages through the network and formulating the optimal routing scheme as an optimization problem that can be solved exactly or for which we give powerful heuristic algorithms. Our approach solves at once many of the open practical problems identified as stumbling blocks for the application of coded caching in practical scenarios, namely: asynchronous streaming sessions, finite file size, scalability of the scheme to large and spatially distributed networks, user mobility and random activity (users joining and leaving the system at arbitrary times), decentralized prefetching of the cache contents, end-to-end encryption of HTTPS requests, which renders the helper nodes oblivious of the user demands.      
### 49.DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss  [ :arrow_down: ](https://arxiv.org/pdf/2008.08823.pdf)
>  In this work we consider UAVs as cooperative agents supporting human users in their operations. In this context, the 3D localisation of the UAV assistant is an important task that can facilitate the exchange of spatial information between the user and the UAV. To address this in a data-driven manner, we design a data synthesis pipeline to create a realistic multimodal dataset that includes both the exocentric user view, and the egocentric UAV view. We then exploit the joint availability of photorealistic and synthesized inputs to train a single-shot monocular pose estimation model. During training we leverage differentiable rendering to supplement a state-of-the-art direct regression objective with a novel smooth silhouette loss. Our results demonstrate its qualitative and quantitative performance gains over traditional silhouette objectives. Our data and code are available at <a class="link-external link-https" href="https://vcl3d.github.io/DronePose" rel="external noopener nofollow">this https URL</a>      
### 50.Facial movement synergies and Action Unit detection from distal wearable Electromyography and Computer Vision  [ :arrow_down: ](https://arxiv.org/pdf/2008.08791.pdf)
>  Distal facial Electromyography (EMG) can be used to detect smiles and frowns with reasonable accuracy. It capitalizes on volume conduction to detect relevant muscle activity, even when the electrodes are not placed directly on the source muscle. The main advantage of this method is to prevent occlusion and obstruction of the facial expression production, whilst allowing EMG measurements. However, measuring EMG distally entails that the exact source of the facial movement is unknown. We propose a novel method to estimate specific Facial Action Units (AUs) from distal facial EMG and Computer Vision (CV). This method is based on Independent Component Analysis (ICA), Non-Negative Matrix Factorization (NNMF), and sorting of the resulting components to determine which is the most likely to correspond to each CV-labeled action unit (AU). Performance on the detection of AU06 (Orbicularis Oculi) and AU12 (Zygomaticus Major) was estimated by calculating the agreement with Human Coders. The results of our proposed algorithm showed an accuracy of 81% and a Cohen's Kappa of 0.49 for AU6; and accuracy of 82% and a Cohen's Kappa of 0.53 for AU12. This demonstrates the potential of distal EMG to detect individual facial movements. Using this multimodal method, several AU synergies were identified. We quantified the co-occurrence and timing of AU6 and AU12 in posed and spontaneous smiles using the human-coded labels, and for comparison, using the continuous CV-labels. The co-occurrence analysis was also performed on the EMG-based labels to uncover the relationship between muscle synergies and the kinematics of visible facial movement.      
### 51.Optimal Network Compression  [ :arrow_down: ](https://arxiv.org/pdf/2008.08733.pdf)
>  This paper introduces a formulation of the optimal network compression problem for financial systems. This general formulation is presented for different levels of network compression or rerouting allowed from the initial interbank network. We prove that this problem is, generically, NP-hard. We focus on objective functions generated by systemic risk measures under systematic shocks to the financial network. We conclude by studying the optimal compression problem for specific networks; this permits us to study the so-called robust fragility of certain network topologies more generally as well as the potential benefits and costs of network compression.      
### 52.iPhantom: a framework for automated creation of individualized computational phantoms and its application to CT organ dosimetry  [ :arrow_down: ](https://arxiv.org/pdf/2008.08730.pdf)
>  Objective: This study aims to develop and validate a novel framework, iPhantom, for automated creation of patient-specific phantoms or digital-twins (DT) using patient medical images. The framework is applied to assess radiation dose to radiosensitive organs in CT imaging of individual patients. Method: From patient CT images, iPhantom segments selected anchor organs (e.g. liver, bones, pancreas) using a learning-based model developed for multi-organ CT segmentation. Organs challenging to segment (e.g. intestines) are incorporated from a matched phantom template, using a diffeomorphic registration model developed for multi-organ phantom-voxels. The resulting full-patient phantoms are used to assess organ doses during routine CT exams. Result: iPhantom was validated on both the XCAT (n=50) and an independent clinical (n=10) dataset with similar accuracy. iPhantom precisely predicted all organ locations with good accuracy of Dice Similarity Coefficients (DSC) &gt;0.6 for anchor organs and DSC of 0.3-0.9 for all other organs. iPhantom showed less than 10% dose errors for the majority of organs, which was notably superior to the state-of-the-art baseline method (20-35% dose errors). Conclusion: iPhantom enables automated and accurate creation of patient-specific phantoms and, for the first time, provides sufficient and automated patient-specific dose estimates for CT dosimetry. Significance: The new framework brings the creation and application of CHPs to the level of individual CHPs through automation, achieving a wider and precise organ localization, paving the way for clinical monitoring, and personalized optimization, and large-scale research.      
### 53.Generalizing Fault Detection Against Domain Shifts Using Stratification-Aware Cross-Validation  [ :arrow_down: ](https://arxiv.org/pdf/2008.08713.pdf)
>  Incipient anomalies present milder symptoms compared to severe ones, and are more difficult to detect and diagnose due to their close resemblance to normal operating conditions. The lack of incipient anomaly examples in the training data can pose severe risks to anomaly detection methods that are built upon Machine Learning (ML) techniques, because these anomalies can be easily mistaken as normal operating conditions. To address this challenge, we propose to utilize the uncertainty information available from ensemble learning to identify potential misclassified incipient anomalies. We show in this paper that ensemble learning methods can give improved performance on incipient anomalies and identify common pitfalls in these models through extensive experiments on two real-world datasets. Then, we discuss how to design more effective ensemble models for detecting incipient anomalies.      
### 54.Using Ensemble Classifiers to Detect Incipient Anomalies  [ :arrow_down: ](https://arxiv.org/pdf/2008.08710.pdf)
>  Incipient anomalies present milder symptoms compared to severe ones, and are more difficult to detect and diagnose due to their close resemblance to normal operating conditions. The lack of incipient anomaly examples in the training data can pose severe risks to anomaly detection methods that are built upon Machine Learning (ML) techniques, because these anomalies can be easily mistaken as normal operating conditions. To address this challenge, we propose to utilize the uncertainty information available from ensemble learning to identify potential misclassified incipient anomalies. We show in this paper that ensemble learning methods can give improved performance on incipient anomalies and identify common pitfalls in these models through extensive experiments on two real-world datasets. Then, we discuss how to design more effective ensemble models for detecting incipient anomalies.      
### 55.Image Segmentation of Zona-Ablated Human Blastocysts  [ :arrow_down: ](https://arxiv.org/pdf/2008.08673.pdf)
>  Automating human preimplantation embryo grading offers the potential for higher success rates with in vitro fertilization (IVF) by providing new quantitative and objective measures of embryo quality. Current IVF procedures typically use only qualitative manual grading, which is limited in the identification of genetically abnormal embryos. The automatic quantitative assessment of blastocyst expansion can potentially improve sustained pregnancy rates and reduce health risks from abnormal pregnancies through a more accurate identification of genetic abnormality. The expansion rate of a blastocyst is an important morphological feature to determine the quality of a developing embryo. In this work, a deep learning based human blastocyst image segmentation method is presented, with the goal of facilitating the challenging task of segmenting irregularly shaped blastocysts. The type of blastocysts evaluated here has undergone laser ablation of the zona pellucida, which is required prior to trophectoderm biopsy. This complicates the manual measurements of the expanded blastocyst's size, which shows a correlation with genetic abnormalities. The experimental results on the test set demonstrate segmentation greatly improves the accuracy of expansion measurements, resulting in up to 99.4% accuracy, 98.1% precision, 98.8% recall, a 98.4% Dice Coefficient, and a 96.9% Jaccard Index.      
