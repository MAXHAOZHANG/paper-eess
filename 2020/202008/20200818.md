# ArXiv eess --Tue, 18 Aug 2020
### 1.Do face masks introduce bias in speech technologies? The case of automated scoring of speaking proficiency  [ :arrow_down: ](https://arxiv.org/pdf/2008.07520.pdf)
>  The COVID-19 pandemic has led to a dramatic increase in the use of face masks worldwide. Face coverings can affect both acoustic properties of the signal as well as speech patterns and have unintended effects if the person wearing the mask attempts to use speech processing technologies. In this paper we explore the impact of wearing face masks on the automated assessment of English language proficiency. We use a dataset from a large-scale speaking test for which test-takers were required to wear face masks during the test administration, and we compare it to a matched control sample of test-takers who took the same test before the mask requirements were put in place. We find that the two samples differ across a range of acoustic measures and also show a small but significant difference in speech patterns. However, these differences do not lead to differences in human or automated scores of English language proficiency. Several measures of bias showed no differences in scores between the two groups.      
### 2.Cybersecurity of Electric Vehicle Smart Charging Management Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07511.pdf)
>  In concept, a smart charging management system (SCMS) optimizes the charging of plug-in vehicles (PEVs) and provides various grid services including voltage control, frequency regulation, peak shaving, renewable energy integration support, spinning reserve, and emergency demand response. These functionalities largely depend upon data collected from various entities such as PEVs, electric vehicle supply equipment (EVSE), service providers, and utilities. SCMS can be susceptible to both cyber and physical threats (e.g. man-in-the-middle attack, data intrigued attack, denial of charging, physical-attack) due to interactions of and interdependencies between cyber and physical components. Cyber-physical threats through highly connected malware vectors raise various concerns including public safety hazards to vehicle operators and those in the immediate vicinity as well as disruptions to electric grid operations. This paper describes the concept of SCMS and provides a comprehensive review of the cybersecurity aspects of EVSEs and SCMSs with their possible impacts on the power grid and society. It also contributes to the development of cybersecurity measures to the SCMSs. Various functions of SCMS are reviewed in detail including peak shaving, demand charge reduction, frequency regulation, spinning reserve, renewable integration support, distribution congestion management, reactive power compensation, and emergency demand response with unidirectional PEVs charging. Also, a critical literature survey on current practices of SCMS cybersecurity is provided to explore major impacts and challenges of cyber-physical attacks and to identify research gaps and vulnerabilities in currently available SCMSs technologies.      
### 3.Strong Structural Controllability of Diffusively Coupled Networks: Comparison of Bounds Based on Distances and Zero Forcing  [ :arrow_down: ](https://arxiv.org/pdf/2008.07495.pdf)
>  We study the strong structural controllability (SSC) of diffusively coupled networks, where the external control inputs are injected to only some nodes, namely the leaders. For such systems, one measure of controllability is the dimension of strong structurally controllable subspace, which is equal to the smallest possible rank of controllability matrix under admissible (positive) coupling weights. In this paper, we compare two tight lower bounds on the dimension of strong structurally controllable subspace: one based on the distances of followers to leaders, and the other based on the graph coloring process known as zero forcing. We show that the distance-based lower bound is usually better than the zero-forcing-based bound when the leaders do not constitute a zero-forcing set. On the other hand, we also show that any set of leaders that can be shown to achieve complete SSC via the distance-based bound is necessarily a zero-forcing set. These results indicate that while the zero-forcing based approach may be preferable when the focus is only on verifying complete SSC, the distance-based approach is usually more informative when partial SSC is also of interest. Furthermore, we also present a novel bound based on the combination of these two approaches, which is always at least as good as, and in some cases strictly greater than, the maximum of the two bounds. We support our analysis with numerical results for various graphs and leader sets.      
### 4.Control Communication Co-Design for Wide Area Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07492.pdf)
>  Wide Area Cyber-Physical Systems (WA-CPSs) are a class of control systems that integrate low-powered sensors, heterogeneous actuators and computer controllers into large infrastructure that span multi-kilometre distances. Current wireless communication technologies are incapable of meeting the communication requirements of range and bounded delays needed for the control of WA-CPSs. To solve this problem, we use a Control-Communication Co-design approach for WA-CPSs, that we refer to as the $C^3$ approach, to design a novel Low-Power Wide Area (LPWA) MAC protocol called \textit{Ctrl-MAC} and its associated event-triggered controller that can guarantee the closed-loop stability of a WA-CPS. This is the first paper to show that LPWA wireless communication technologies can support the control of WA-CPSs. LPWA technologies are designed to support one-way communication for monitoring and are not appropriate for control. We present this work using an example of a water distribution network application which we evaluate both through a co-simulator (modelling both physical and cyber subsystems) and testbed deployments. Our evaluation demonstrates full control stability, with up to $50$\% better packet delivery ratios and $80$\% less average end-to-end delays when compared to a state of the art LPWA technology. We also evaluate our scheme against an idealised, wired, centralised, control architecture and show that the controller maintains stability and the overshoots remain within bounds.      
### 5.Absorption in Time-Varying Markov Chains: Graph-Based Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2008.07475.pdf)
>  We investigate absorption, i.e., almost sure convergence to an absorbing state, in time-varying (non-homogeneous) discrete-time Markov chains with finite state space. We consider systems that can switch among a finite set of transition matrices, which we call the modes. Our analysis is focused on two properties: 1) almost sure convergence to an absorbing state under any switching, and 2) almost sure convergence to a desired set of absorbing states via a proper switching policy. We derive necessary and sufficient conditions based on the structures of the transition graphs of modes. More specifically, we show that a switching policy that ensures almost sure convergence to a desired set of absorbing states from any initial state exists if and only if those absorbing states are reachable from any state on the union of simplified transition graphs. We then show three sufficient conditions for absorption under arbitrary switching. While the first two conditions depend on the acyclicity (weak acyclicity) of the union (intersection) of simplified transition graphs, the third condition is based on the distances of each state to the absorbing states in all the modes. These graph theoretic conditions can verify the stability and stabilizability of absorbing states based only on the feasibility of transitions in each mode.      
### 6.Spaceborne Synthetic Aperture Radar Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.07457.pdf)
>  In this paper we present the Spaceborne Synthetic Aperture Radar (SAR) imaging process for small squint angle case in stripmap mode. We describe the entire SAR image reconstruction process. <br>We then use experimental data gathered by RADARSAR-1 satellite from Vancouver, Canada and reconstruct the SAR image and show the results.      
### 7.Fast and Accurate Modeling of Transient-state Gradient-Spoiled Sequences by Recurrent Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.07440.pdf)
>  Fast and accurate modeling of MR signal responses are typically required for various quantitative MRI applications, such as MR Fingerprinting and MR-STAT. This work uses a new EPG-Bloch model for accurate simulation of transient-state gradient-spoiled MR sequences, and proposes a Recurrent Neural Network (RNN) as a fast surrogate of the EPG-Bloch model for computing large-scale MR signals and derivatives. The computational efficiency of the RNN model is demonstrated by comparing with other existing models, showing one to three orders of acceleration comparing to the latest GPU-accelerated open-source EPG package. By using numerical and in-vivo brain data, two use cases, namely MRF dictionary generation and optimal experimental design, are also provided. Results show that the RNN surrogate model can be efficiently used for computing large-scale dictionaries of transient-states signals and derivatives within tens of seconds, resulting in several orders of magnitude acceleration with respect to state-of-the-art implementations. The practical application of transient-states quantitative techniques can therefore be substantially facilitated.      
### 8.Siloed Federated Learning for Multi-Centric Histopathology Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2008.07424.pdf)
>  While federated learning is a promising approach for training deep learning models over distributed sensitive datasets, it presents new challenges for machine learning, especially when applied in the medical domain where multi-centric data heterogeneity is common. Building on previous domain adaptation works, this paper proposes a novel federated learning approach for deep learning architectures via the introduction of local-statistic batch normalization (BN) layers, resulting in collaboratively-trained, yet center-specific models. This strategy improves robustness to data heterogeneity while also reducing the potential for information leaks by not sharing the center-specific layer activation statistics. We benchmark the proposed method on the classification of tumorous histopathology image patches extracted from the Camelyon16 and Camelyon17 datasets. We show that our approach compares favorably to previous state-of-the-art methods, especially for transfer learning across datasets.      
### 9.Extended mathematical derivations for the decentralized loss minimization algorithm with the use of inverters  [ :arrow_down: ](https://arxiv.org/pdf/2008.07384.pdf)
>  This document contains extended mathematical derivations for the communication- and model-free loss minimization algorithm. The algorithm is applied in the distribution grids and exploits the capabilities of the inverters to control the reactive power output.      
### 10.First U-Net Layers Contain More Domain Specific Information Than The Last Ones  [ :arrow_down: ](https://arxiv.org/pdf/2008.07357.pdf)
>  MRI scans appearance significantly depends on scanning protocols and, consequently, the data-collection institution. These variations between clinical sites result in dramatic drops of CNN segmentation quality on unseen domains. Many of the recently proposed MRI domain adaptation methods operate with the last CNN layers to suppress domain shift. At the same time, the core manifestation of MRI variability is a considerable diversity of image intensities. We hypothesize that these differences can be eliminated by modifying the first layers rather than the last ones. To validate this simple idea, we conducted a set of experiments with brain MRI scans from six domains. Our results demonstrate that 1) domain-shift may deteriorate the quality even for a simple brain extraction segmentation task (surface Dice Score drops from 0.85-0.89 even to 0.09); 2) fine-tuning of the first layers significantly outperforms fine-tuning of the last layers in almost all supervised domain adaptation setups. Moreover, fine-tuning of the first layers is a better strategy than fine-tuning of the whole network, if the amount of annotated data from the new domain is strictly limited.      
### 11.Coverage Analysis for Dense Heterogeneous Networks with Cooperative NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2008.07348.pdf)
>  In a heterogeneous cellular network (HetNet) consisting of $M$ tiers of densely-deployed base stations (BSs), consider that each of the BSs in the HetNet that are associated with multiple users is able to simultaneously schedule and serve two users in a downlink time slot by performing the (power-domain) non-orthogonal multiple access (NOMA) scheme. This paper aims at the preliminary study on the downlink coverage performance of the HetNet with the \textit{non-cooperative} and the proposed \textit{cooperative} NOMA schemes. First, we study the coverage probability of the NOMA users for the non-cooperative NOMA scheme in which no BSs are coordinated to jointly transmit the NOMA signals for a particular cell and the coverage probabilities of the two NOMA users of the BSs in each tier are derived. We show that the coverage probabilities can be largely reduced if allocated transmit powers for the NOMA users are not satisfied with some constraints. Next, we study and derive the coverage probabilities for the proposed cooperative NOMA scheme in which the void BSs that are not tagged by any users are coordinated to enhance the far NOMA user in a particular cell. Our analyses show that cooperative NOMA can significantly improve the coverage of all NOMA users as long as the transmit powers for the NOMA users are properly allocated.      
### 12.Data-Driven Distributed Mitigation Strategies and Analysis of Mutating Epidemic Processes  [ :arrow_down: ](https://arxiv.org/pdf/2008.07317.pdf)
>  In this paper we study a discrete-time SIS (susceptible-infected-susceptible) model, where the infection and healing parameters and the underlying network may change over time. We provide conditions for the model to be well-defined and study its stability. For systems with homogeneous infection rates over symmetric graphs,we provide a sufficient condition for global exponential stability (GES) of the healthy state, that is, where the virus is eradicated. For systems with heterogeneous virus spread over directed graphs, provided that the variation is not too fast, a sufficient condition for GES of the healthy state is established.      
### 13.Robust Autoencoder GAN for Cryo-EM Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2008.07307.pdf)
>  The cryo-electron microscopy (Cryo-EM) becomes popular for macromolecular structure determination. However, the 2D images which Cryo-EM detects are of high noise and often mixed with multiple heterogeneous conformations or contamination, imposing a challenge for denoising. Traditional image denoising methods can not remove Cryo-EM image noise well when the signal-noise-ratio (SNR) of images is meager. Thus it is desired to develop new effective denoising techniques to facilitate further research such as 3D reconstruction, 2D conformation classification, and so on. In this paper, we approach the robust image denoising problem in Cryo-EM by a joint Autoencoder and Generative Adversarial Networks (GAN) method. Equipped with robust $\ell_1$ Autoencoder and some designs of robust $\beta$-GANs, one can stabilize the training of GANs and achieve the state-of-the-art performance of robust denoising with low SNR data and against possible information contamination. The method is evaluated by both a heterogeneous conformational dataset on the Thermus aquaticus RNA Polymerase (RNAP) and a homogenous dataset on the Plasmodium falciparum 80S ribosome dataset (EMPIRE-10028), in terms of Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), as well as heterogeneous conformation clustering. These results suggest that our proposed methodology provides an effective tool for Cryo-EM 2D image denoising.      
### 14.Classification of diffraction patterns in single particle imaging experiments performed at X-ray free-electron lasers using a convolutional neural network  [ :arrow_down: ](https://arxiv.org/pdf/2008.07288.pdf)
>  Single particle imaging (SPI) is a promising method for native structure determination which has undergone a fast progress with the development of X-ray Free-Electron Lasers. Large amounts of data are collected during SPI experiments, driving the need for automated data analysis. The necessary data analysis pipeline has a number of steps including binary object classification (single versus multiple hits). Classification and object detection are areas where deep neural networks currently outperform other approaches. In this work, we use the fast object detector networks YOLOv2 and YOLOv3. By exploiting transfer learning, a moderate amount of data is sufficient for training of the neural network. We demonstrate here that a convolutional neural network (CNN) can be successfully used to classify data from SPI experiments. We compare the results of classification for the two different networks, with different depth and architecture, by applying them to the same SPI data with different data representation. The best results are obtained for YOLOv2 color images linear scale classification, which shows an accuracy of about 97% with the precision and recall of about 52% and 61%, respectively, which is in comparison to manual data classification.      
### 15.An Architectural Design for Measurement Uncertainty Evaluation in Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07282.pdf)
>  Several use cases from the areas of manufacturing and process industry, require highly accurate sensor data. As sensors always have some degree of uncertainty, methods are needed to increase their reliability. The common approach is to regularly calibrate the devices to enable traceability according to national standards and Système international (SI) units - which follows costly processes. However, sensor networks can also be represented as Cyber Physical Systems (CPS) and a single sensor can have a digital representation (Digital Twin) to use its data further on. To propagate uncertainty in a reliable way in the network, we present a system architecture to communicate measurement uncertainties in sensor networks utilizing the concept of Asset Administration Shells alongside methods from the domain of Organic Computing. The presented approach contains methods for uncertainty propagation as well as concepts from the Machine Learning domain that combine the need for an accurate uncertainty estimation. The mathematical description of the metrological uncertainty of fused or propagated values can be seen as a first step towards the development of a harmonized approach for uncertainty in distributed CPSs in the context of Industrie 4.0. In this paper, we present basic use cases, conceptual ideas and an agenda of how to proceed further on.      
### 16.On Mean Absolute Error for Deep Neural Network Based Vector-to-Vector Regression  [ :arrow_down: ](https://arxiv.org/pdf/2008.07281.pdf)
>  In this paper, we exploit the properties of mean absolute error (MAE) as a loss function for the deep neural network (DNN) based vector-to-vector regression. The goal of this work is two-fold: (i) presenting performance bounds of MAE, and (ii) demonstrating new properties of MAE that make it more appropriate than mean squared error (MSE) as a loss function for DNN based vector-to-vector regression. First, we show that a generalized upper-bound for DNN-based vector- to-vector regression can be ensured by leveraging the known Lipschitz continuity property of MAE. Next, we derive a new generalized upper bound in the presence of additive noise. Finally, in contrast to conventional MSE commonly adopted to approximate Gaussian errors for regression, we show that MAE can be interpreted as an error modeled by Laplacian distribution. Speech enhancement experiments are conducted to corroborate our proposed theorems and validate the performance advantages of MAE over MSE for DNN based regression.      
### 17.One Bit to Rule Them All : Binarizing the Reconstruction in 1-bit Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2008.07264.pdf)
>  This work focuses on the reconstruction of sparse signals from their 1-bit measurements. The context is the one of 1-bit compressive sensing where the measurements amount to quantizing (dithered) random projections. Our main contribution shows that, in addition to the measurement process, we can additionally reconstruct the signal with a binarization of the sensing matrix. This binary representation of both the measurements and sensing matrix can dramatically simplify the hardware architecture on embedded systems, enabling cheaper and more power efficient alternatives. Within this framework, given a sensing matrix respecting the restricted isometry property (RIP), we prove that for any sparse signal the quantized projected back-projection (QPBP) algorithm achieves a reconstruction error decaying like O(m-1/2)when the number of measurements m increases. Simulations highlight the practicality of the developed scheme for different sensing scenarios, including random partial Fourier sensing.      
### 18.MLBF-Net: A Multi-Lead-Branch Fusion Network for Multi-Class Arrhythmia Classification Using 12-Lead ECG  [ :arrow_down: ](https://arxiv.org/pdf/2008.07263.pdf)
>  Automatic arrhythmia detection using 12-lead electrocardiogram (ECG) signal plays a critical role in early prevention and diagnosis of cardiovascular diseases. In the previous studies on automatic arrhythmia detection, most methods concatenated 12 leads of ECG into a matrix, and then input the matrix to a variety of feature extractors or deep neural networks for extracting useful information. Under such frameworks, these methods had the ability to extract comprehensive features (known as integrity) of 12-lead ECG since the information of each lead interacts with each other during training. However, the diverse lead-specific features (known as diversity) among 12 leads were neglected, causing inadequate information learning for 12-lead ECG. To maximize the information learning of multi-lead ECG, the information fusion of comprehensive features with integrity and lead-specific features with diversity should be taken into account. In this paper, we propose a novel Multi-Lead-Branch Fusion Network (MLBF-Net) architecture for arrhythmia classification by integrating multi-loss optimization to jointly learning diversity and integrity of multi-lead ECG. MLBF-Net is composed of three components: 1) multiple lead-specific branches for learning the diversity of multi-lead ECG; 2) cross-lead features fusion by concatenating the output feature maps of all branches for learning the integrity of multi-lead ECG; 3) multi-loss co-optimization for all the individual branches and the concatenated network. We demonstrate our MLBF-Net on China Physiological Signal Challenge 2018 which is an open 12-lead ECG dataset. The experimental results show that MLBF-Net obtains an average $F_1$ score of 0.855, reaching the highest arrhythmia classification performance. The proposed method provides a promising solution for multi-lead ECG analysis from an information fusion perspective.      
### 19.Deep Learning Based Open Set Acoustic Scene Classification  [ :arrow_down: ](https://arxiv.org/pdf/2008.07247.pdf)
>  In this work, we compare the performance of three selected techniques in open set acoustic scenes classification (ASC). We test thresholding of the softmax output of a deep network classifier, which is the most popular technique nowadays employed in ASC. Further we compare the results with the Openmax classifier which is derived from the computer vision field. As the third model, we use the Adapted Class-Conditioned Autoencoder (Adapted C2AE) which is our variation of another computer vision related technique called C2AE. Adapted C2AE encompasses a more fair comparison of the given experiments and simplifies the original inference procedure, making it more applicable in the real-life scenarios. We also analyse two training scenarios: without additional knowledge of unknown classes and another where a limited subset of examples from the unknown classes is available. We find that the C2AE based method outperforms the thresholding and Openmax, obtaining $85.5\%$ Area Under the Receiver Operating Characteristic curve (AUROC) and $66\%$ of open set accuracy on data used in Detection and Classification of Acoustic Scenes and Events Challenge 2019 Task 1C.      
### 20.Efficient Low-Latency Speech Enhancement with Mobile Audio Streaming Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.07244.pdf)
>  We propose Mobile Audio Streaming Networks (MASnet) for efficient low-latency speech enhancement, which is particularly suitable for mobile devices and other applications where computational capacity is a limitation. MASnet processes linear-scale spectrograms, transforming successive noisy frames into complex-valued ratio masks which are then applied to the respective noisy frames. MASnet can operate in a low-latency incremental inference mode which matches the complexity of layer-by-layer batch mode. Compared to a similar fully-convolutional architecture, MASnet incorporates depthwise and pointwise convolutions for a large reduction in fused multiply-accumulate operations per second (FMA/s), at the cost of some reduction in SNR.      
### 21.Model-Reference Reinforcement Learning for Collision-Free Tracking Control of Autonomous Surface Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.07240.pdf)
>  This paper presents a novel model-reference reinforcement learning algorithm for the intelligent tracking control of uncertain autonomous surface vehicles with collision avoidance. The proposed control algorithm combines a conventional control method with reinforcement learning to enhance control accuracy and intelligence. In the proposed control design, a nominal system is considered for the design of a baseline tracking controller using a conventional control approach. The nominal system also defines the desired behaviour of uncertain autonomous surface vehicles in an obstacle-free environment. Thanks to reinforcement learning, the overall tracking controller is capable of compensating for model uncertainties and achieving collision avoidance at the same time in environments with obstacles. In comparison to traditional deep reinforcement learning methods, our proposed learning-based control can provide stability guarantees and better sample efficiency. We demonstrate the performance of the new algorithm using an example of autonomous surface vehicles.      
### 22.StoRIR: Stochastic Room Impulse Response Generation for Audio Data Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07231.pdf)
>  In this paper we introduce StoRIR - a stochastic room impulse response generation method dedicated to audio data augmentation in machine learning applications. This technique, in contrary to geometrical methods like image-source or ray tracing, does not require prior definition of room geometry, absorption coefficients or microphone and source placement and is dependent solely on the acoustic parameters of the room. The method is intuitive, easy to implement and allows to generate RIRs of very complicated enclosures. We show that StoRIR, when used for audio data augmentation in a speech enhancement task, allows deep learning models to achieve better results on a wide range of metrics than when using the conventional image-source method, effectively improving many of them by more than 5 %. We publish a Python implementation of StoRIR online      
### 23.Scoring the Terabit/s Goal:Broadband Connectivity in 6G  [ :arrow_down: ](https://arxiv.org/pdf/2008.07220.pdf)
>  This paper explores the road to vastly improving the broadband connectivity in future 6G wireless systems. Different categories of use cases are considered, from extreme capacity with peak data rates up to 1 Tbps, to raising the typical data rates by orders-of-magnitude, and supporting broadband connectivity at railway speeds up to 1000 km/h. To achieve these, not only the terrestrial networks will be evolved but they will also be integrated with satellite networks, all facilitating autonomous systems and various interconnected structures. We believe that several categories of enablers at the infrastructure, spectrum, and protocol/algorithmic levels are required to realize the connectivity goals in 6G. At the infrastructure level, we consider ultra-massive MIMO technology (possibly implemented using holographic radio), intelligent reflecting surfaces, user-centric cell-free networking, integrated access and backhaul, and integrated space and terrestrial networks. At the spectrum level, the network must seamlessly utilize sub-6 GHz bands for coverage and spatial multiplexing of many devices, while higher bands will be used for pushing the peak rates of point-to-point links. The latter path will lead to (sub-)Terahertz communications complemented by visible light communications in specific scenarios. At the protocol/algorithmic level, the enablers include improved coding, modulation, and waveforms to achieve lower latency, higher reliability, and reduced complexity. The resource efficiency can be further improved by using various combinations of full-duplex radios, interference management based on rate-splitting, machine-learning based optimization, coded caching, and broadcasting. Finally, the three levels of enablers must be utilized also to provide full-coverage broadband connectivity which must be one of the key outcomes of 6G.      
### 24.Deep Variational Generative Models for Audio-visual Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07191.pdf)
>  In this paper, we are interested in audio-visual speech separation given a single-channel audio recording as well as visual information (lips movements) associated with each speaker. We propose an unsupervised technique based on audio-visual generative modeling of clean speech. More specifically, during training, a latent variable generative model is learned from clean speech spectrograms using a variational auto-encoder (VAE). To better utilize the visual information, the posteriors of the latent variables are inferred from mixed speech (instead of clean speech) as well as the visual data. The visual modality also serves as a prior for latent variables, through a visual network. At test time, the learned generative model (both for speaker-independent and speaker-dependent scenarios) is combined with an unsupervised non-negative matrix factorization (NMF) variance model for background noise. All the latent variables and noise parameters are then estimated by a Monte Carlo expectation-maximization algorithm. Our experiments show that the proposed unsupervised VAE-based method yields better separation performance than NMF-based approaches as well as a supervised deep learning-based technique.      
### 25.Distributed Adaptive Formation Control for Multi-UAV to Enable Connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2008.07143.pdf)
>  There is increasing demand for control of multi-robot and as well distributing large amounts of content to cluster of Unmanned Aerial Vehicles (UAV) on the operation. In recent years several large-scale accidents have happened. To facilitate rescue operations and gather information, the technology that can access and map inaccessible areas is needed. This paper presents a disruptive approach to address the issues with communication, data collection and data sharing for UAV units in inaccessible or dead zones and We demonstrated feasibility of the approach and evaluate its advantages over the Ad Hoc architecture involving autonomous gateways      
### 26.Extension of causal decomposition in the mutual complex dynamic process  [ :arrow_down: ](https://arxiv.org/pdf/2008.07135.pdf)
>  Causal decomposition depicts a cause-effect relationship that is not based on the concept of prediction, but based on the phase dependence of time series. It has been validated in both stochastic and deterministic systems and is now anticipated for its application in the complex dynamic process. Here, we present an extension of causal decomposition in the mutual complex dynamic process: cause and effect of time series are inherited in the decomposition of intrinsic components in a similar time scale. Furthermore, we illustrate comparative studies with predominate methods used in neuroscience, and show the applicability of the method particularly to physiological time series in brain-muscle interactions, implying the potential to the causality analysis in the complex physiological process.      
### 27.PIANOTREE VAE: Structured Representation Learning for Polyphonic Music  [ :arrow_down: ](https://arxiv.org/pdf/2008.07118.pdf)
>  The dominant approach for music representation learning involves the deep unsupervised model family variational autoencoder (VAE). However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning. The experiments prove the validity of the PianoTree VAE via (i)-semantically meaningful latent code for polyphonic segments; (ii)-more satisfiable reconstruction aside of decent geometry learned in the latent space; (iii)-this model's benefits to the variety of the downstream music generation.      
### 28.AnciNet: An Efficient Deep Learning Approach for Feedback Compression of Estimated CSI in Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07112.pdf)
>  Accurate channel state information (CSI) feedback plays a vital role in improving the performance gain of massive multiple-input multiple-output (m-MIMO) systems, where the dilemma is excessive CSI overhead versus limited feedback bandwith. By considering the noisy CSI due to imperfect channel estimation, we propose a novel deep neural network architecture, namely AnciNet, to conduct the CSI feedback with limited bandwidth. AnciNet extracts noise-free features from the noisy CSI samples to achieve effective CSI compression for the feedback. Experimental results verify that the proposed AnciNet approach outperforms the existing techniques under various conditions.      
### 29.Semi-Supervised Learning with GANs for Device-Free Fingerprinting Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2008.07111.pdf)
>  Device-free wireless indoor localization is a key enabling technology for the Internet of Things (IoT). Fingerprint-based indoor localization techniques are a commonly used solution. This paper proposes a semi-supervised, generative adversarial network (GAN)-based device-free fingerprinting indoor localization system. The proposed system uses a small amount of labeled data and a large amount of unlabeled data (i.e., semi-supervised), thus considerably reducing the expensive data labeling effort. Experimental results show that, as compared to the state-of-the-art supervised scheme, the proposed semi-supervised system achieves comparable performance with equal, sufficient amount of labeled data, and significantly superior performance with equal, highly limited amount of labeled data. Besides, the proposed semi-supervised system retains its performance over a broad range of the amount of labeled data. The interactions between the generator, discriminator, and classifier models of the proposed GAN-based system are visually examined and discussed. A mathematical description of the proposed system is also presented.      
### 30.Multi-Task Learning for Interpretable Weakly Labelled Sound Event Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.07085.pdf)
>  Weakly Labelled learning has garnered lot of attention in recent years due to its potential to scale Sound Event Detection (SED). The paper proposes a Multi-Task Learning (MTL) framework for learning from Weakly Labelled Audio data which encompasses the traditional Multiple Instance Learning (MIL) setup. The MTL framework uses two-step attention mechanism and reconstructs Time Frequency (T-F) representation of audio as the auxiliary task. By breaking the attention into two steps, the network retains better time level information without compromising classification performance. The auxiliary task uses an auto-encoder structure to encourage the network for retaining source specific information. This indirectly de-noises internal T- F representation and improves classification performance under noisy recordings. For evaluation of proposed methodology, we remix the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 db SNR. The proposed network outperforms existing benchmark models over all SNRs, specifically 22.3 %, 12.8 %, 5.9 % improvement over benchmark model on 0, 10 and 20 dB SNR respectively. The results and ablation study performed demonstrates the usefulness of auto-encoder for auxiliary task and verifies that the output of decoder portion provides a cleaned Time Frequency (T-F) representation of audio/sources which can be further used for source separation. The code is publicly released.      
### 31.Towards Cardiac Intervention Assistance: Hardware-aware Neural Architecture Exploration for Real-Time 3D Cardiac Cine MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07071.pdf)
>  Real-time cardiac magnetic resonance imaging (MRI) plays an increasingly important role in guiding various cardiac interventions. In order to provide better visual assistance, the cine MRI frames need to be segmented on-the-fly to avoid noticeable visual lag. In addition, considering reliability and patient data privacy, the computation is preferably done on local hardware. State-of-the-art MRI segmentation methods mostly focus on accuracy only, and can hardly be adopted for real-time application or on local hardware. In this work, we present the first hardware-aware multi-scale neural architecture search (NAS) framework for real-time 3D cardiac cine MRI segmentation. The proposed framework incorporates a latency regularization term into the loss function to handle real-time constraints, with the consideration of underlying hardware. In addition, the formulation is fully differentiable with respect to the architecture parameters, so that stochastic gradient descent (SGD) can be used for optimization to reduce the computation cost while maintaining optimization quality. Experimental results on ACDC MICCAI 2017 dataset demonstrate that our hardware-aware multi-scale NAS framework can reduce the latency by up to 3.5 times and satisfy the real-time constraints, while still achieving competitive segmentation accuracy, compared with the state-of-the-art NAS segmentation framework.      
### 32.Secure Multigroup Multicast Communication Systems via Intelligent Reflecting Surface  [ :arrow_down: ](https://arxiv.org/pdf/2008.07066.pdf)
>  This paper considers a secure multigroup multicast multiple-input single-output (MISO) communication system aided by an intelligent reflecting surface (IRS). Specifically, we aim to minimize the transmit power at the Alice via jointly optimizing the transmit beamformer, AN vector and phase shifts at the IRS subject to the secrecy rate constraints as well as the unit modulus constraints of IRS phase shifts. However, the optimization problem is non-convex and directly solving it is intractable. To tackle the optimization problem, we first transform it into a semidefinite relaxation (SDR) problem, and then alternately update the transmit beamformer and AN matrix as well as the phase shifts at the IRS. In order to reduce the high computational complexity, we further propose a low-complexity algorithm based on second-order cone programming (SOCP). We decouple the optimization problem into two sub-problems and optimize the transmit beamformer, AN vector and the phase shifts alternately by solving two corresponding SOCP sub-problem. Simulation results show that the proposed SDR and SOCP schemes require half or less transmit power than the scheme without IRS, which demonstrates the advantages of introducing IRS and the effectiveness of the proposed methods.      
### 33.Exploiting Fully Convolutional Network and Visualization Techniques on Spontaneous Speech for Dementia Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.07052.pdf)
>  In this paper, we exploit a Fully Convolutional Network (FCN) to analyze the audio data of spontaneous speech for dementia detection. A fully convolutional network accommodates speech samples with varying lengths, thus enabling us to analyze the speech sample without manual segmentation. Specifically, we first obtain the Mel Frequency Cepstral Coefficient (MFCC) feature map from each participant's audio data and convert the speech classification task on audio data to an image classification task on MFCC feature maps. Then, to solve the data insufficiency problem, we apply transfer learning by adopting a pre-trained backbone Convolutional Neural Network (CNN) model from the MobileNet architecture and the ImageNet dataset. We further build a convolutional layer to produce a heatmap using Otsu's method for visualization, enabling us to understand the impact of the time-series audio segments on the classification results. We demonstrate that our classification model achieves 66.7% over the testing dataset, 62.5% of the baseline model provided in the ADReSS challenge. Through the visualization technique, we can evaluate the impact of audio segments, such as filled pauses from the participants and repeated questions from the investigator, on the classification results.      
### 34.Training CNN Classifiers for Semantic Segmentation using Partially Annotated Images: with Application on Human Thigh and Calf MRI  [ :arrow_down: ](https://arxiv.org/pdf/2008.07030.pdf)
>  Objective: Medical image datasets with pixel-level labels tend to have a limited number of organ or tissue label classes annotated, even when the images have wide anatomical coverage. With supervised learning, multiple classifiers are usually needed given these partially annotated datasets. In this work, we propose a set of strategies to train one single classifier in segmenting all label classes that are heterogeneously annotated across multiple datasets without moving into semi-supervised learning. Methods: Masks were first created from each label image through a process we termed presence masking. Three presence masking modes were evaluated, differing mainly in weightage assigned to the annotated and unannotated classes. These masks were then applied to the loss function during training to remove the influence of unannotated classes. Results: Evaluation against publicly available CT datasets shows that presence masking is a viable method for training class-generic classifiers. Our class-generic classifier can perform as well as multiple class-specific classifiers combined, while the training duration is similar to that required for one class-specific classifier. Furthermore, the class-generic classifier can outperform the class-specific classifiers when trained on smaller datasets. Finally, consistent results are observed from evaluations against human thigh and calf MRI datasets collected in-house. Conclusion: The evaluation outcomes show that presence masking is capable of significantly improving both training and inference efficiency across imaging modalities and anatomical regions. Improved performance may even be observed on small datasets. Significance: Presence masking strategies can reduce the computational resources and costs involved in manual medical image annotations. All codes are publicly available at <a class="link-external link-https" href="https://github.com/wong-ck/DeepSegment" rel="external noopener nofollow">this https URL</a>.      
### 35.LfEdNet: A Task-based Day-ahead Load Forecasting Model for Stochastic Economic Dispatch  [ :arrow_down: ](https://arxiv.org/pdf/2008.07025.pdf)
>  Load forecasting is one of the most important and studied topics in modern power systems. Most of the existing researches on day-ahead load forecasting try to build a good model to improve the forecasting accuracy. The forecasted load is then used as the input to generation scheduling with the ultimate goal of minimizing the cost of generation schedules. However, existing day-ahead load forecasting models do not consider this ultimate goal at the training/forecasting stage. This paper proposes a task-based day-ahead load forecasting model labeled as LfEdNet that combines two individual layers in one model, including a load forecasting layer based on deep neural network (Lf layer) and a day-ahead stochastic economic dispatch (SED) layer (Ed layer). The training of LfEdNet aims to minimize the cost of the day-ahead SED in the Ed layer by updating the parameters of the Lf layer. Sequential quadratic programming (SQP) is used to solve the day-ahead SED in the Ed layer. The test results demonstrate that the forecasted results produced by LfEdNet can lead to lower cost of day-ahead SED while maintaining a relatively high forecasting accuracy.      
### 36.Enforcing Safety at Runtime for Systems with Disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2008.07019.pdf)
>  Safety for control systems is often posed as an invariance constraint; the system is said to be safe if state trajectories avoid some unsafe region of the statespace for all time. An assured controller is one that enforces safety online by filtering a desired control input at runtime, and control barrier functions (CBFs) provide an assured controller that renders a safe subset of the state-space forward invariant. Recent extensions propose CBF-based assured controllers that allow the system to leave a known safe set so long as a given backup control strategy eventually returns to the safe set, however, these methods have yet to be extended to consider systems subjected to unknown disturbance inputs. <br>In this work, we present a problem formulation for CBF-based runtime assurance for systems with disturbances, and controllers which solve this problem must, in some way, incorporate the online computation of reachable sets. In general, computing reachable sets in the presence of disturbances is computationally costly and cannot be directly incorporated in a CBF framework. To that end, we present a particular solution to the problem, whereby reachable sets are approximated via the mixed-monotonicity property. Efficient algorithms exist for overapproximating reachable sets for mixed-monotone systems with hyperrectangles, and we show that such approximations are suitable for incorporating into a CBF-based runtime assurance framework.      
### 37.Spontaneous preterm birth prediction using convolutional neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.07000.pdf)
>  An estimated 15 million babies are born too early every year. Approximately 1 million children die each year due to complications of preterm birth (PTB). Many survivors face a lifetime of disability, including learning disabilities and visual and hearing problems. Although manual analysis of ultrasound images (US) is still prevalent, it is prone to errors due to its subjective component and complex variations in the shape and position of organs across patients. In this work, we introduce a conceptually simple convolutional neural network (CNN) trained for segmenting prenatal ultrasound images and classifying task for the purpose of preterm birth detection. Our method efficiently segments different types of cervixes in transvaginal ultrasound images while simultaneously predicting a preterm birth based on extracted image features without human oversight. We employed three popular network models: U-Net, Fully Convolutional Network, and Deeplabv3 for the cervix segmentation task. Based on the conducted results and model efficiency, we decided to extend U-Net by adding a parallel branch for classification task. The proposed model is trained and evaluated on a dataset consisting of 354 2D transvaginal ultrasound images and achieved a segmentation accuracy with a mean Jaccard coefficient index of 0.923 $\pm$ 0.081 and a classification sensitivity of 0.677 $\pm$ 0.042 with a 3.49\% false positive rate. Our method obtained better results in the prediction of preterm birth based on transvaginal ultrasound images compared to state-of-the-art methods.      
### 38.RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty Quantification in Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.06999.pdf)
>  Quantifying segmentation uncertainty has become an important issue in medical image analysis due to the inherent ambiguity of anatomical structures and its pathologies. Recently, neural network-based uncertainty quantification methods have been successfully applied to various problems. One of the main limitations of the existing techniques is the high memory requirement during training; which limits their application to processing smaller field-of-views (FOVs) and/or using shallower architectures. In this paper, we investigate the effect of using reversible blocks for building memory-efficient neural network architectures for quantification of segmentation uncertainty. The reversible architecture achieves memory saving by exactly computing the activations from the outputs of the subsequent layers during backpropagation instead of storing the activations for each layer. We incorporate the reversible blocks into a recently proposed architecture called PHiSeg that is developed for uncertainty quantification in medical image segmentation. The reversible architecture, RevPHiSeg, allows training neural networks for quantifying segmentation uncertainty on GPUs with limited memory and processing larger FOVs. We perform experiments on the LIDC-IDRI dataset and an in-house prostate dataset, and present comparisons with PHiSeg. The results demonstrate that RevPHiSeg consumes ~30% less memory compared to PHiSeg while achieving very similar segmentation accuracy.      
### 39.Deep Learning Predicts Cardiovascular Disease Risks from Lung Cancer Screening Low Dose Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2008.06997.pdf)
>  The high risk population of cardiovascular disease (CVD) is simultaneously at high risk of lung cancer. Given the dominance of low dose computed tomography (LDCT) for lung cancer screening, the feasibility of extracting information on CVD from the same LDCT scan would add major value to patients at no additional radiation dose. However, with strong noise in LDCT images and without electrocardiogram (ECG) gating, CVD risk analysis from LDCT is highly challenging. Here we present an innovative deep learning model to address this challenge. Our deep model was trained with 30,286 LDCT volumes and achieved the state-of-the-art performance (area under the curve (AUC) of 0.869) on 2,085 National Lung Cancer Screening Trial (NLST) subjects, and effectively identified patients with high CVD mortality risks (AUC of 0.768). Our deep model was further calibrated against the clinical gold standard CVD risk scores from ECG-gated dedicated cardiac CT, including coronary artery calcification (CAC) score, CAD-RADS score and MESA 10-year CHD risk score from an independent dataset of 106 subjects. In this validation study, our model achieved AUC of 0.942, 0.809 and 0.817 for CAC, CAD-RADS and MESA scores, respectively. Our deep learning model has the potential to convert LDCT for lung cancer screening into dual-screening quantitative tool for CVD risk estimation.      
### 40.ADL-MVDR: All deep learning MVDR beamformer for target speech separation  [ :arrow_down: ](https://arxiv.org/pdf/2008.06994.pdf)
>  Speech separation algorithms are often used to separate the target speech from other interfering sources. However, purely neural network based speech separation systems often cause nonlinear distortion that is harmful for ASR systems. The conventional mask-based minimum variance distortionless response (MVDR) beamformer can be used to minimize the distortion, but comes with high level of residual noise. Furthermore, the matrix inversion and eigenvalue decomposition processes involved in the conventional MVDR solution are not stable when jointly trained with neural networks. In this paper, we propose a novel all deep learning MVDR framework, where the matrix inversion and eigenvalue decomposition are replaced by two recurrent neural networks (RNNs), to resolve both issues at the same time. The proposed method can greatly reduce the residual noise while keeping the target speech undistorted by leveraging on the RNN-predicted frame-wise beamforming weights. The system is evaluated on a Mandarin audio-visual corpus and compared against several state-of-the-art (SOTA) speech separation systems. Experimental results demonstrate the superiority of the proposed method across several objective metrics and ASR accuracy.      
### 41.Deep Learning Enables Robust and Precise Light Focusing on Treatment Needs  [ :arrow_down: ](https://arxiv.org/pdf/2008.06975.pdf)
>  If light passes through the body tissues, focusing only on areas where treatment needs, such as tumors, will revolutionize many biomedical imaging and therapy technologies. So how to focus light through deep inhomogeneous tissues overcoming scattering is Holy Grail in biomedical areas. In this paper, we use deep learning to learn and accelerate the process of phase pre-compensation using wavefront shaping. We present an approach (LoftGAN, light only focuses on treatment needs) for learning the relationship between phase domain X and speckle domain Y . Our goal is not just to learn an inverse mapping F:Y-&gt;X such that we can know the corresponding X needed for imaging Y like most work, but also to make focusing that is susceptible to disturbances more robust and precise by ensuring that the phase obtained can be forward mapped back to speckle. So we introduce different constraints to enforce F(Y)=X and H(F(Y))=Y with the transmission mapping H:X-&gt;Y. Both simulation and physical experiments are performed to investigate the effects of light focusing to demonstrate the effectiveness of our method and comparative experiments prove the crucial improvement of robustness and precision. Codes are available at <a class="link-external link-https" href="https://github.com/ChangchunYang/LoftGAN" rel="external noopener nofollow">this https URL</a>.      
### 42.Real-Time Spatio-Temporal LiDAR Point Cloud Compression  [ :arrow_down: ](https://arxiv.org/pdf/2008.06972.pdf)
>  Compressing massive LiDAR point clouds in real-time is critical to autonomous machines such as drones and self-driving cars. While most of the recent prior work has focused on compressing individual point cloud frames, this paper proposes a novel system that effectively compresses a sequence of point clouds. The idea to exploit both the spatial and temporal redundancies in a sequence of point cloud frames. We first identify a key frame in a point cloud sequence and spatially encode the key frame by iterative plane fitting. We then exploit the fact that consecutive point clouds have large overlaps in the physical space, and thus spatially encoded data can be (re-)used to encode the temporal stream. Temporal encoding by reusing spatial encoding data not only improves the compression rate, but also avoids redundant computations, which significantly improves the compression speed. Experiments show that our compression system achieves 40x to 90x compression rate, significantly higher than the MPEG's LiDAR point cloud compression standard, while retaining high end-to-end application accuracies. Meanwhile, our compression system has a compression speed that matches the point cloud generation rate by today LiDARs and out-performs existing compression systems, enabling real-time point cloud transmission.      
### 43.Physical Action Categorization using Signal Analysis and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.06971.pdf)
>  Daily life of thousands of individuals around the globe suffers due to physical or mental disability related to limb movement. The quality of life for such individuals can be made better by use of assistive applications and systems. In such scenario, mapping of physical actions from movement to a computer aided application can lead the way for solution. Surface Electromyography (sEMG) presents a non-invasive mechanism through which we can translate the physical movement to signals for classification and use in applications. In this paper, we propose a machine learning based framework for classification of 4 physical actions. The framework looks into the various features from different modalities which contribution from time domain, frequency domain, higher order statistics and inter channel statistics. Next, we conducted a comparative analysis of k-NN, SVM and ELM classifier using the feature set. Effect of different combinations of feature set has also been recorded. Finally, the classifier accuracy with SVM and 1-NN based classifier for a subset of features gives an accuracy of 95.21 and 95.83 respectively. Additionally, we have also proposed that dimensionality reduction by use of PCA leads to only a minor drop of less than 5.55% in accuracy while using only 9.22% of the original feature set. These finding are useful for algorithm designer to choose the best approach keeping in mind the resources available for execution of algorithm.      
### 44.Automated Detection of Congenital HeartDisease in Fetal Ultrasound Screening  [ :arrow_down: ](https://arxiv.org/pdf/2008.06966.pdf)
>  Prenatal screening with ultrasound can lower neonatal mor-tality significantly for selected cardiac abnormalities. However, the needfor human expertise, coupled with the high volume of screening cases,limits the practically achievable detection rates. In this paper we discussthe potential for deep learning techniques to aid in the detection of con-genital heart disease (CHD) in fetal ultrasound. We propose a pipelinefor automated data curation and classification. During both training andinference, we exploit an auxiliary view classification task to bias featurestoward relevant cardiac structures. This bias helps to improve in F1-scores from 0.72 and 0.77 to 0.87 and 0.85 for healthy and CHD classesrespectively.      
### 45.A Survey of Machine Learning Methods for Detecting False Data Injection Attacks in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.06926.pdf)
>  Over the last decade, the number of cyberattacks targeting power systems and causing physical and economic damages has increased rapidly. Among them, False Data Injection Attacks (FDIAs) is a class of cyberattacks against power grid monitoring systems. Adversaries can successfully perform FDIAs in order to manipulate the power system State Estimation (SE) by compromising sensors or modifying system data. SE is an essential process performed by the Energy Management System (EMS) towards estimating unknown state variables based on system redundant measurements and network topology. SE routines include Bad Data Detection (BDD) algorithms to eliminate errors from the acquired measurements, e.g., in case of sensor failures. FDIAs can bypass BDD modules to inject malicious data vectors into a subset of measurements without being detected, and thus manipulate the results of the SE process. In order to overcome the limitations of traditional residual-based BDD approaches, data-driven solutions based on machine learning algorithms have been widely adopted for detecting malicious manipulation of sensor data due to their fast execution times and accurate results. This paper provides a comprehensive review of the most up-to-date machine learning methods for detecting FDIAs against power system SE algorithms.      
### 46.Virtual brightfield and fluorescence staining for Fourier ptychography via unsupervised deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.06916.pdf)
>  Fourier ptychographic microscopy (FPM) is a computational approach geared towards creating high-resolution and large field-of-view images without mechanical scanning. To acquire color images of histology slides, it often requires sequential acquisitions with red, green, and blue illuminations. The color reconstructions often suffer from coherent artifacts that are not presented in regular incoherent microscopy images. As a result, it remains a challenge to employ FPM for digital pathology applications, where resolution and color accuracy are of critical importance. Here we report a deep learning approach for performing unsupervised image-to-image translation of FPM reconstructions. A cycle-consistent adversarial network with multiscale structure similarity loss is trained to perform virtual brightfield and fluorescence staining of the recovered FPM images. In the training stage, we feed the network with two sets of unpaired images: 1) monochromatic FPM recovery, and 2) color or fluorescence images captured using a regular microscope. In the inference stage, the network takes the FPM input and outputs a virtually stained image with reduced coherent artifacts and improved image quality. We test the approach on various samples with different staining protocols. High-quality color and fluorescence reconstructions validate its effectiveness.      
### 47.DRL-Based QoS-Aware Resource Allocation Scheme for Coexistence of Licensed and Unlicensed Users in LTE and Beyond  [ :arrow_down: ](https://arxiv.org/pdf/2008.06905.pdf)
>  In this paper, we employ deep reinforcement learning to develop a novel radio resource allocation and packet scheduling scheme for different Quality of Service (QoS) requirements applicable to LTEadvanced and 5G networks. In addition, regarding the scarcity of spectrum in below 6GHz bands, the proposed algorithm dynamically allocates the resource blocks (RBs) to licensed users in a way to mostly preserve the continuity of unallocated RBs. This would improve the efficiency of communication among the unlicensed entities by increasing the chance of uninterrupted communication and reducing the load of coordination overheads. The optimization problem is formulated as a Markov Decision Process (MDP), observing the entire queue of the demands, where failing to meet QoS constraints penalizes the goal with a multiplicative factor. Furthermore, a notion of continuity for unallocated resources is taken into account as an additive term in the objective function. Considering the variations in both channel coefficients and users requests, we utilize a deep reinforcement learning algorithm as an online and numerically efficient approach to solve the MDP. Numerical results show that the proposed method achieves higher average spectral efficiency, while considering delay budget and packet loss ratio, compared to the conventional greedy min-delay and max-throughput schemes, in which a fixed part of the spectrum is forced to be vacant for unlicensed entities.      
### 48.Unsupervised Acoustic Unit Representation Learning for Voice Conversion using WaveNet Auto-encoders  [ :arrow_down: ](https://arxiv.org/pdf/2008.06892.pdf)
>  Unsupervised representation learning of speech has been of keen interest in recent years, which is for example evident in the wide interest of the ZeroSpeech challenges. This work presents a new method for learning frame level representations based on WaveNet auto-encoders. Of particular interest in the ZeroSpeech Challenge 2019 were models with discrete latent variable such as the Vector Quantized Variational Auto-Encoder (VQVAE). However these models generate speech with relatively poor quality. In this work we aim to address this with two approaches: first WaveNet is used as the decoder and to generate waveform data directly from the latent representation; second, the low complexity of latent representations is improved with two alternative disentanglement learning methods, namely instance normalization and sliced vector quantization. The method was developed and tested in the context of the recent ZeroSpeech challenge 2020. The system output submitted to the challenge obtained the top position for naturalness (Mean Opinion Score 4.06), top position for intelligibility (Character Error Rate 0.15), and third position for the quality of the representation (ABX test score 12.5). These and further analysis in this paper illustrates that quality of the converted speech and the acoustic units representation can be well balanced.      
### 49.Attractive Ellipsoid Sliding Mode Observer Design for State of Charge Estimation of Lithium-ion Cells  [ :arrow_down: ](https://arxiv.org/pdf/2008.06871.pdf)
>  This work investigates the real-time estimation of the state-of-charge (SoC) of Lithium-ion (Li-ion) cells for reliable, safe and efficient utilization. A novel attractive ellipsoid based sliding-mode observer (AESMO) algorithm is designed to estimate the SoC in real-time. The algorithm utilizes standard equivalent circuit model of a Li-ion cell and provides reliable and efficient SoC estimate in the presence of bounded uncertainties in the battery parameters as well as exogenous disturbances. The theoretical framework of the observer design is not limited to the SoC estimation problem of Li-ion cell but applicable to a wider class of nonlinear systems with both matched and mismatched uncertainties. The main advantage of the proposed observer is to provide a fast and optimal SoC estimate based on minimization over the uncertainty bound. The proposed method is experimentally tested and evaluated using the hybrid pulse power characterization test (HPPC)and urban dynamometer driving schedule (UDDS) test data, which demonstrate its effectiveness and feasibility.      
### 50.Audio Dequantization for High Fidelity Audio Generation in Flow-based Neural Vocoder  [ :arrow_down: ](https://arxiv.org/pdf/2008.06867.pdf)
>  In recent works, a flow-based neural vocoder has shown significant improvement in real-time speech generation task. The sequence of invertible flow operations allows the model to convert samples from simple distribution to audio samples. However, training a continuous density model on discrete audio data can degrade model performance due to the topological difference between latent and actual distribution. To resolve this problem, we propose audio dequantization methods in flow-based neural vocoder for high fidelity audio generation. Data dequantization is a well-known method in image generation but has not yet been studied in the audio domain. For this reason, we implement various audio dequantization methods in flow-based neural vocoder and investigate the effect on the generated audio. We conduct various objective performance assessments and subjective evaluation to show that audio dequantization can improve audio generation quality. From our experiments, using audio dequantization produces waveform audio with better harmonic structure and fewer digital artifacts.      
### 51.High-speed computational ghost imaging with compressed sensing based on a convolutional neural network  [ :arrow_down: ](https://arxiv.org/pdf/2008.06842.pdf)
>  Computational ghost imaging (CGI) has recently been intensively studied as an indirect imaging technique. However, the speed of CGI cannot meet the requirements of practical applications. Here, we propose a novel CGI scheme for high-speed imaging. In our scenario, the conventional CGI data processing algorithm is optimized to a new compressed sensing (CS) algorithm based on a convolutional neural network (CNN). CS is used to process the data collected by a conventional CGI device. Then, the processed data are trained by a CNN to reconstruct the image. The experimental results show that our scheme can produce high-quality images with much less sampling than conventional CGI. Moreover, detailed comparisons between the images reconstructed using our approach and with conventional CS and deep learning (DL) show that our scheme outperforms the conventional approach and achieves a faster imaging speed.      
### 52.Adversarial Filters for Secure Modulation Classification  [ :arrow_down: ](https://arxiv.org/pdf/2008.06785.pdf)
>  Modulation Classification (MC) refers to the problem of classifying the modulation class of a wireless signal. In the wireless communications pipeline, MC is the first operation performed on the received signal and is critical for reliable decoding. This paper considers the problem of secure modulation classification, where a transmitter (Alice) wants to maximize MC accuracy at a legitimate receiver (Bob) while minimizing MC accuracy at an eavesdropper (Eve). <br>The contribution of this work is to design novel adversarial learning techniques for secure MC. In particular, we present adversarial filtering based algorithms for secure MC, in which Alice uses a carefully designed adversarial filter to mask the transmitted signal, that can maximize MC accuracy at Bob while minimizing MC accuracy at Eve. We present two filtering based algorithms, namely gradient ascent filter (GAF), and a fast gradient filter method (FGFM), with varying levels of complexity. <br>Our proposed adversarial filtering based approaches significantly outperform additive adversarial perturbations (used in the traditional ML community and other prior works on secure MC) and also have several other desirable properties. In particular, GAF and FGFM algorithms are a) computational efficient (allow fast decoding at Bob), b) power-efficient (do not require excessive transmit power at Alice); and c) SNR efficient (i.e., perform well even at low SNR values at Bob).      
### 53.Automated Detection of Cortical Lesions in Multiple Sclerosis Patients with 7T MRI  [ :arrow_down: ](https://arxiv.org/pdf/2008.06780.pdf)
>  The automated detection of cortical lesions (CLs) in patients with multiple sclerosis (MS) is a challenging task that, despite its clinical relevance, has received very little attention. Accurate detection of the small and scarce lesions requires specialized sequences and high or ultra-high field MRI. For supervised training based on multimodal structural MRI at 7T, two experts generated ground truth segmentation masks of 60 patients with 2014 CLs. We implemented a simplified 3D U-Net with three resolution levels (3D U-Net-). By increasing the complexity of the task (adding brain tissue segmentation), while randomly dropping input channels during training, we improved the performance compared to the baseline. Considering a minimum lesion size of 0.75 {\mu}L, we achieved a lesion-wise cortical lesion detection rate of 67% and a false positive rate of 42%. However, 393 (24%) of the lesions reported as false positives were post-hoc confirmed as potential or definite lesions by an expert. This indicates the potential of the proposed method to support experts in the tedious process of CL manual segmentation.      
### 54.FEARLESS STEPS Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.06764.pdf)
>  The Fearless Steps Initiative by UTDallas-CRSS led to the digitization, recovery, and diarization of 19,000 hours of original analog audio data, as well as the development of algorithms to extract meaningful information from this multi-channel naturalistic data resource. The 2020 FEARLESS STEPS (FS-2) Challenge is the second annual challenge held for the Speech and Language Technology community to motivate supervised learning algorithm development for multi-party and multi-stream naturalistic audio. In this paper, we present an overview of the challenge sub-tasks, data, performance metrics, and lessons learned from Phase-2 of the Fearless Steps Challenge (FS-2). We present advancements made in FS-2 through extensive community outreach and feedback. We describe innovations in the challenge corpus development, and present revised baseline results. We finally discuss the challenge outcome and general trends in system development across both phases (Phase FS-1 Unsupervised, and Phase FS-2 Supervised) of the challenge, and its continuation into multi-channel challenge tasks for the upcoming Fearless Steps Challenge Phase-3.      
### 55.Damage Detection in Bridge Structures: An Edge Computing Approach  [ :arrow_down: ](https://arxiv.org/pdf/2008.06724.pdf)
>  Wireless sensor network (WSN) based SHM systems have shown significant improvement as compared to traditional wired-SHM systems in terms of cost, accuracy, and reliability of the monitoring. However, due to the resource-constrained nature of the sensor nodes, it is a challenge to process a large amount of sensed vibration data in real-time. Existing mechanisms of data processing are centralized and use cloud or remote servers to analyze the data to characterize the state of the bridge, i.e., healthy or damaged. These methods are feasible for wired-SHM systems, however, transmitting huge data-sets in WSNs has been found to be arduous. In this paper, we propose a mechanism named as ``in-network damage detection on edge (INDDE)" which extracts the statistical features from raw acceleration measurements corresponding to the healthy condition of the bridge and use them to train a probabilistic model, i.e., estimating the probability density function (PDF) of multivariate Gaussian distribution. The trained model helps to identify the anomalous behaviour of the new data points collected from the unknown condition of the bridge in real-time. Each edge device classifies the condition of the bridge as either "healthy" or "damaged" around its deployment region depending on their respective trained model. Experimentation results showcase a promising 96-100% damage detection accuracy with the advantage of no data transmission from sensor nodes to the cloud for processing.      
### 56.Deep Architectures for Modulation Recognition with Multiple Receive Antennas  [ :arrow_down: ](https://arxiv.org/pdf/2008.06720.pdf)
>  Modulation recognition using deep neural networks has shown promising advantage over conventional algorithms. However, most existing research focuses on single receive antenna. In this paper, modulation recognition with multiple receive antennas using deep neural networks is investigated and four different architectures are introduced, including equal-gain CNN, multi-view CNN, 2-dimensional CNN and 3-dimensional CNN. Each architecture is constructed based on a ResNet and tuned to the extent that its performance does not further improve when the network size and parameters change with a given dataset. These architectures are then compared in terms of classification accuracy. Simulation results show that 3-dimensional CNN yields the overall best performance, while the equal-gain CNN leads to the lowest performance. Further, both 2-dimensional CNN and 3-dimensional CNN, which jointly extract features from multiple receive antennas with different feature encoding, outperforms either equal-gain or multi-view CNN, which fuses extracted features from each antenna. This indicates that utilizing inherent structures within deep neural networks to jointly extract features from different antennas can achieve better performance than the schemes that combine individually encoded features from each antenna, and extending the dimension of CNN from two to three can enhance feature extraction capabilities in the context of modulation recognition.      
### 57.Single image dehazing for a variety of haze scenarios using back projected pyramid network  [ :arrow_down: ](https://arxiv.org/pdf/2008.06713.pdf)
>  Learning to dehaze single hazy images, especially using a small training dataset is quite challenging. We propose a novel generative adversarial network architecture for this problem, namely back projected pyramid network (BPPNet), that gives good performance for a variety of challenging haze conditions, including dense haze and inhomogeneous haze. Our architecture incorporates learning of multiple levels of complexities while retaining spatial context through iterative blocks of UNets and structural information of multiple scales through a novel pyramidal convolution block. These blocks together for the generator and are amenable to learning through back projection. We have shown that our network can be trained without over-fitting using as few as 20 image pairs of hazy and non-hazy images. We report the state of the art performances on NTIRE 2018 homogeneous haze datasets for indoor and outdoor images, NTIRE 2019 denseHaze dataset, and NTIRE 2020 non-homogeneous haze dataset.      
### 58.Experimental investigations of psychoacoustic characteristics of household vacuum cleaners  [ :arrow_down: ](https://arxiv.org/pdf/2008.06702.pdf)
>  Vacuum cleaners are one of the most widely used household appliances associated with unpleasant noises. Previous studies have indicated the severity of vacuum cleaner noise and its impact on the users nearby. The quantified measurements of the generated noise standalone are not sufficient for the selection or designing of vacuum cleaners. The human perception must also be included for a better assessment of the quality of sound. A hybrid approach known as psychoacoustics, which comprises subjective and objective evaluations of sounds, is widely used in recent times. This paper focuses on the experimental assessment of psychoacoustical matrices for household vacuum cleaners. Three vacuum cleaners with different specifications have been selected as test candidates, and their sound qualities have been analyzed. Besides, the annoyance index has been assessed for these vacuum cleaners.      
### 59.Deep Learning in Industrial Internet of Things: Potentials, Challenges, and Emerging Applications  [ :arrow_down: ](https://arxiv.org/pdf/2008.06701.pdf)
>  The recent advancements in the Internet of Things (IoT) are giving rise to the proliferation of interconnected devices, enabling various smart applications. These enormous number of IoT devices generates a large capacity of data that further require intelligent data analysis and processing methods, such as Deep Learning (DL). Notably, the DL algorithms, when applied in the Industrial Internet of Things (IIoT), can enable various applications such as smart assembling, smart manufacturing, efficient networking, and accident detection-and-prevention. Therefore, motivated by these numerous applications; in this paper, we present the key potentials of DL in IIoT. First, we review various DL techniques, including convolutional neural networks, auto-encoders, and recurrent neural networks and there use in different industries. Then, we outline numerous use cases of DL for IIoT systems, including smart manufacturing, smart metering, smart agriculture, etc. Moreover, we categorize several research challenges regarding the effective design and appropriate implementation of DL-IIoT. Finally, we present several future research directions to inspire and motivate further research in this area.      
### 60.Discrete-time control of bilateral teleoperation systems: a review  [ :arrow_down: ](https://arxiv.org/pdf/2008.06685.pdf)
>  The possibility of operating in remote environments using teleoperation systems has been considered widely in the control literature. This paper presents a review on the discrete-time teleoperation systems, including issues such as stability, passivity and time delays. Using discrete-time methods for a master-slave teleoperation system can simplify control implementation. Varieties of control schemes have been proposed for these systems and major concerns such as passivity, stability and transparency have been studied. Recently, unreliable communication networks affected by packet loss and variable transmission delays have been received much attention. Thus, it is worth considering discrete-time theories for bilateral teleoperation architectures, which are formulated on the same lines as the continuous-time systems. Despite the extensive amount of researches concerning continuous-time teleoperation systems, only a few papers have been published on the analysis and controller design for discrete bilateral forms. This paper takes into account the challenges for the discrete structure of bilateral teleoperation systems and notifies the recent contributions in this area. The effect of sampling time on the stability-transparency trade-off and the task performance is taken into consideration in this review. These studies can help to design guidelines to have better transparency and stable teleoperation systems.      
### 61.Stability analysis of the linear discrete teleoperation systems with stochastic sampling and data dropout  [ :arrow_down: ](https://arxiv.org/pdf/2008.06683.pdf)
>  This paper addresses the stability conditions of the sampled-data teleoperation systems consisting continuous time master, slave, operator, and environment with discrete time controllers over general communication networks. The output signals of the slave and master robots are quantized with stochastic sampling periods which are modeled as being from a finite set. By applying an input delay method, the probabilistic sampling system is converted into a continuous-time system including stochastic parameters in the system matrices. The main contribution of this paper is the derivation of the less conservative stability conditions for linear discrete teleoperation systems taking into account the challenges such as the stochastic sampling rate, constant time delay and the possibility of data packet dropout. The numbers of dropouts are driven by a finite state Markov chain. First, the problem of finding a lower bound on the maximum sampling period that preserves the stability is formulated. This problem is constructed as a convex optimization program in terms of linear matrix inequalities (LMI). Next, Lyapunov Krasovskii based approaches are applied to propose sufficient conditions for stochastic and exponential stability of closed-loop sampled-data bilateral teleoperation system. The proposed criterion notifies the effect of sampling time on the stability transparency trade-off and imposes bounds on the sampling time, control gains and the damping of robots. Neglecting this study undermines both the stability and transparency of teleoperation systems. Numerical simulation results are used to verify the proposed stability criteria and illustrate the effectiveness of the sampling architecture.      
### 62.Jointly Fine-Tuning "BERT-like" Self Supervised Models to Improve Multimodal Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.06682.pdf)
>  Multimodal emotion recognition from speech is an important area in affective computing. Fusing multiple data modalities and learning representations with limited amounts of labeled data is a challenging task. In this paper, we explore the use of modality-specific "BERT-like" pretrained Self Supervised Learning (SSL) architectures to represent both speech and text modalities for the task of multimodal speech emotion recognition. By conducting experiments on three publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning "BERT-like" SSL architectures achieve state-of-the-art (SOTA) results. We also evaluate two methods of fusing speech and text modalities and show that a simple fusion mechanism can outperform more complex ones when using SSL models that have similar architectural properties to BERT.      
### 63.Evaluation of Three Nonlinear Control Methods to Reject the Constant Bounded Disturbance for Robotic Manipulators  [ :arrow_down: ](https://arxiv.org/pdf/2008.06676.pdf)
>  In this paper, we consider the tracking control problem for robot manipulators which are affected by constant bounded disturbances. Three control schemes are applied for the problem, which composed of integral action and tracking controllers. The goal is improving the accuracy of tracking procedure for a robot manipulator to track a specified reference signal in the presence of constant bounded disturbances. Inverse dynamics controller, improved Lyapunov-based controller with integral action and discontinuous Lyapunov-based controller are three schemes that are evaluated in this paper. Third one is a novel controller that achieves trajectory following without requiring exact knowledge of the nonlinear dynamics. Based on the disturbance rejection scheme, tracking controllers are constructed which are asymptotically stabilizing in the sense of Lyapunov. Furthermore the closed loop system has strong disturbance rejection property. It is shown that how under proper assumptions, the proposed schemes succeed in achieving disturbance rejection at the input of a nonlinear system. Computer simulation results given for a two degree of freedom manipulator with a large payload and fast maneuver, demonstrate that accurate trajectory tracking can be achieved by using the proposed controllers.      
### 64.Advancing Multiple Instance Learning with Attention Modeling for Categorical Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.06667.pdf)
>  Categorical speech emotion recognition is typically performed as a sequence-to-label problem, i.e., to determine the discrete emotion label of the input utterance as a whole. One of the main challenges in practice is that most of the existing emotion corpora do not give ground truth labels for each segment; instead, we only have labels for whole utterances. To extract segment-level emotional information from such weakly labeled emotion corpora, we propose using multiple instance learning (MIL) to learn segment embeddings in a weakly supervised manner. Also, for a sufficiently long utterance, not all of the segments contain relevant emotional information. In this regard, three attention-based neural network models are then applied to the learned segment embeddings to attend the most salient part of a speech utterance. Experiments on the CASIA corpus and the IEMOCAP database show better or highly competitive results than other state-of-the-art approaches.      
### 65.EigenEmo: Spectral Utterance Representation Using Dynamic Mode Decomposition for Speech Emotion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2008.06665.pdf)
>  Human emotional speech is, by its very nature, a variant signal. This results in dynamics intrinsic to automatic emotion classification based on speech. In this work, we explore a spectral decomposition method stemming from fluid-dynamics, known as Dynamic Mode Decomposition (DMD), to computationally represent and analyze the global utterance-level dynamics of emotional speech. Specifically, segment-level emotion-specific representations are first learned through an Emotion Distillation process. This forms a multi-dimensional signal of emotion flow for each utterance, called Emotion Profiles (EPs). The DMD algorithm is then applied to the resultant EPs to capture the eigenfrequencies, and hence the fundamental transition dynamics of the emotion flow. Evaluation experiments using the proposed approach, which we call EigenEmo, show promising results. Moreover, due to the positive combination of their complementary properties, concatenating the utterance representations generated by EigenEmo with simple EPs averaging yields noticeable gains.      
### 66.A Two-Stage Optimal Bidding Algorithm for Incentive-based Aggregation of Electric Vehicles in Workplace Parking Lots  [ :arrow_down: ](https://arxiv.org/pdf/2008.06644.pdf)
>  This paper proposes an incentive-based aggregator for electric vehicles (EVs) in workplace parking lots to participate in the energy and regulation markets. With the implementation of seasonal Autoregressive Integrated Moving Average (ARIMA) model to predict the market information, a Day-Ahead (DA) planning model coordinated with the EV's responses to the incentive is formulated to determine the days to activate the aggregation program and the optimal incentives that are broadcasted to the EV owners in the first stage. Given the determined incentives and EV's responses, an real-time (RT) optimal bidding algorithm complying with EVs' energy demand in the second stage is designed to maximize the aggregator's profits in the markets. The proposed models are tested using the data collected from PJM's energy and regulation markets. The results show the incentive-based aggregator can benefit both the EV owners and the aggregator from the credits obtained from the markets. Meanwhile, the results also indicate the proposed optimal bidding algorithm is capable of handling the uncertainty of regulation signals and following the signals with high precision.      
### 67.Dehaze-GLCGAN: Unpaired Single Image De-hazing via Adversarial Training  [ :arrow_down: ](https://arxiv.org/pdf/2008.06632.pdf)
>  Single image de-hazing is a challenging problem, and it is far from solved. Most current solutions require paired image datasets that include both hazy images and their corresponding haze-free ground-truth images. However, in reality, lighting conditions and other factors can produce a range of haze-free images that can serve as ground truth for a hazy image, and a single ground truth image cannot capture that range. This limits the scalability and practicality of paired image datasets in real-world applications. In this paper, we focus on unpaired single image de-hazing and we do not rely on the ground truth image or physical scattering model. We reduce the image de-hazing problem to an image-to-image translation problem and propose a dehazing Global-Local Cycle-consistent Generative Adversarial Network (Dehaze-GLCGAN). Generator network of Dehaze-GLCGAN combines an encoder-decoder architecture with residual blocks to better recover the haze free scene. We also employ a global-local discriminator structure to deal with spatially varying haze. Through ablation study, we demonstrate the effectiveness of different factors in the performance of the proposed network. Our extensive experiments over three benchmark datasets show that our network outperforms previous work in terms of PSNR and SSIM while being trained on smaller amount of data compared to other methods.      
### 68.Model-Free Optimal Control of Linear Multi-Agent Systems via Decomposition and Hierarchical Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2008.06604.pdf)
>  Designing the optimal linear quadratic regulator (LQR) for a large-scale multi-agent system (MAS) is time-consuming since it involves solving a large-size matrix Riccati equation. The situation is further exasperated when the design needs to be done in a model-free way using schemes such as reinforcement learning (RL). To reduce this computational complexity, we decompose the large-scale LQR design problem into multiple sets of smaller-size LQR design problems. We consider the objective function to be specified over an undirected graph, and cast the decomposition as a graph clustering problem. The graph is decomposed into two parts, one consisting of multiple decoupled subgroups of connected components, and the other containing edges that connect the different subgroups. Accordingly, the resulting controller has a hierarchical structure, consisting of two components. The first component optimizes the performance of each decoupled subgroup by solving the smaller-size LQR design problem in a model-free way using an RL algorithm. The second component accounts for the objective coupling different subgroups, which is achieved by solving a least squares problem in one shot. Although suboptimal, the hierarchical controller adheres to a particular structure as specified by the inter-agent coupling in the objective function and by the decomposition strategy. Mathematical formulations are established to find a decomposition that minimizes required communication links or reduces the optimality gap. Numerical simulations are provided to highlight the pros and cons of the proposed designs.      
### 69.An Approximate Maximum Likelihood Time Synchronization Algorithm for Zero-padded OFDM in Channels with Impulsive Gaussian Noise  [ :arrow_down: ](https://arxiv.org/pdf/2008.06586.pdf)
>  In wireless communication systems, Orthogonal Frequency-Division Multiplexing (OFDM) includes variants using either a cyclic prefix (CP) or a zero padding (ZP) as the guard interval to avoid inter-symbol interference. OFDM is ideally suited to deal with frequency-selective channels and additive white Gaussian noise (AWGN); however, its performance may be dramatically degraded in the presence of impulse noise. While the ZP variants of OFDM exhibit lower bit error rate (BER)and higher energy efficiency compared to their CP counterparts,they demand strict time synchronization, which is challenging in the absence of pilot and CP. Moreover, on the contrary to AWGN, impulse noise severely corrupts data. In this paper, a new low-complexity timing offset (TO) estimator for ZP-OFDM for practical impulsive-noise environments is proposed, where relies on the second-other statistics of the multipath fading channel and noise. Performance comparison with existing TO estimators demonstrates either a superior performance in terms of lock-in probability or a significantly lower complexity over a wide range of signal-to-noise ratio (SNR) for various practical scenarios.      
### 70.Adaptation Algorithms for Speech Recognition: An Overview  [ :arrow_down: ](https://arxiv.org/pdf/2008.06580.pdf)
>  We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.      
### 71.Performance characterization of a novel deep learning-based MR image reconstruction pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2008.06559.pdf)
>  A novel deep learning-based magnetic resonance imaging reconstruction pipeline was designed to address fundamental image quality limitations of conventional reconstruction to provide high-resolution, low-noise MR images. This pipeline's unique aims were to convert truncation artifact into improved image sharpness while jointly denoising images to improve image quality. This new approach, now commercially available at AIR Recon DL (GE Healthcare, Waukesha, WI), includes a deep convolutional neural network (CNN) to aid in the reconstruction of raw data, ultimately producing clean, sharp images. Here we describe key features of this pipeline and its CNN, characterize its performance in digital reference objects, phantoms, and in-vivo, and present sample images and protocol optimization strategies that leverage image quality improvement for reduced scan time. This new deep learning-based reconstruction pipeline represents a powerful new tool to increase the diagnostic and operational performance of an MRI scanner.      
### 72.Multi-Party Private Set Intersection: An Information-Theoretic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2008.07504.pdf)
>  We investigate the problem of multi-party private set intersection (MP-PSI). In MP-PSI, there are $M$ parties, each storing a data set $\mathcal{p}_i$ over $N_i$ replicated and non-colluding databases, and we want to calculate the intersection of the data sets $\cap_{i=1}^M \mathcal{p}_i$ without leaking any information beyond the set intersection to any of the parties. We consider a specific communication protocol where one of the parties, called the leader party, initiates the MP-PSI protocol by sending queries to the remaining parties which are called client parties. The client parties are not allowed to communicate with each other. We propose an information-theoretic scheme that privately calculates the intersection $\cap_{i=1}^M \mathcal{p}_i$ with a download cost of $D = \min_{t \in \{1, \cdots, M\}} \sum_{i \in \{1, \cdots M\}\setminus {t}} \left\lceil \frac{|\mathcal{p}_t|N_i}{N_i-1}\right\rceil$. Similar to the 2-party PSI problem, our scheme builds on the connection between the PSI problem and the multi-message symmetric private information retrieval (MM-SPIR) problem. Our scheme is a non-trivial generalization of the 2-party PSI scheme as it needs an intricate design of the shared common randomness. Interestingly, in terms of the download cost, our scheme does not incur any penalty due to the more stringent privacy constraints in the MP-PSI problem compared to the 2-party PSI problem.      
### 73.Wireless Powered Mobile Edge Computing: Offloading Or Local Computation?  [ :arrow_down: ](https://arxiv.org/pdf/2008.07500.pdf)
>  Mobile-edge computing (MEC) and wireless power transfer are technologies that can assist in the implementation of next generation wireless networks, which will deploy a large number of computational and energy limited devices. In this letter, we consider a point-to-point MEC system, where the device harvests energy from the access point's (AP's) transmitted signal to power the offloading and/or the local computation of a task. By taking into account the non-linearities of energy harvesting, we provide analytical expressions for the probability of successful computation and for the average number of successfully computed bits. Our results show that a hybrid scheme of partial offloading and local computation is not always efficient. In particular, the decision to offload and/or compute locally, depends on the system's parameters such as the distance to the AP and the number of bits that need to be computed.      
### 74.Analysis and Optimization for Large-Scale LoRa Networks: Throughput Fairness and Scalability  [ :arrow_down: ](https://arxiv.org/pdf/2008.07438.pdf)
>  With growing popularity, LoRa networks are pivotally enabling Long Range connectivity to low-cost and power-constrained user equipments (UEs). Due to its wide coverage area, a critical issue is to effectively allocate wireless resources to support potentially massive UEs while resolving the prominent near-far fairness problem in the LoRa network, which is challenging due to the lack of tractable analytical model and its practical requirement for low-complexity and low-overhead design. To achieve massive connectivity with fairness, we aim to maximize the minimum throughput of all UEs, and propose high-level policies of joint spreading factor (SF) allocation, power control, and duty cycle adjustment based only on average channel statistics and spatial UE distribution. By leveraging on the Poisson rain model along with tailored modifications to our considered LoRa network under both single-cell and multi-cell setups, we are able to account for channel fading, aggregate interference, accurate packet overlapping, and/or multi-gateway packet reception, and still obtain tractable and accurate formulas for the packet success probability and hence throughput. We further propose an iterative balancing (IB) method to allocate the SFs in the cell such that the overall max-min throughput can be achieved. Numerical results show that the proposed scheme with optimized design greatly alleviates the near-far fairness issue and also reduces the spatial power consumption, while significantly improving the cell-edge throughput as well as the spatial (sum) throughput for the majority of UEs, especially for large-scale LoRa networks with massive UEs and high gateway density.      
### 75.A near-optimal stochastic gradient method for decentralized non-convex finite-sum optimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.07428.pdf)
>  This paper describes a $near$-$optimal$ stochastic first-order gradient method for decentralized finite-sum minimization of smooth non-convex functions. Specifically, we propose GT-SARAH that employs a local SARAH-type variance reduction and global gradient tracking to address the stochastic and decentralized nature of the problem. Considering a total number of $N$ cost functions, equally divided over a directed network of $n$ nodes, we show that GT-SARAH finds an $\epsilon$-accurate first-order stationary point in ${\mathcal{O}(N^{1/2}\epsilon^{-1})}$ gradient computations across all nodes, independent of the network topology, when ${n\leq\mathcal{O}(N^{1/2}(1-\lambda)^{3})}$, where ${(1-\lambda)}$ is the spectral gap of the network weight matrix. In this regime, GT-SARAH is thus, to the best our knowledge, the first decentralized method that achieves the algorithmic lower bound for this class of problems. Moreover, GT-SARAH achieves a $non$-$asymptotic$ $linear$ $speedup$, in that, the total number of gradient computations at each node is reduced by a factor of $1/n$ compared to the near-optimal algorithms for this problem class that process all data at a single node. We also establish the convergence rate of GT-SARAH in other regimes, in terms of the relative sizes of the number of nodes $n$, total number of functions $N$, and the network spectral gap $(1-\lambda)$. Over infinite time horizon, we establish the almost sure and mean-squared convergence of GT-SARAH to a first-order stationary point.      
### 76.Improving Emergency Response during Hurricane Season using Computer Vision  [ :arrow_down: ](https://arxiv.org/pdf/2008.07418.pdf)
>  We have developed a framework for crisis response and management that incorporates the latest technologies in computer vision (CV), inland flood prediction, damage assessment and data visualization. The framework uses data collected before, during, and after the crisis to enable rapid and informed decision making during all phases of disaster response. Our computer-vision model analyzes spaceborne and airborne imagery to detect relevant features during and after a natural disaster and creates metadata that is transformed into actionable information through web-accessible mapping tools. In particular, we have designed an ensemble of models to identify features including water, roads, buildings, and vegetation from the imagery. We have investigated techniques to bootstrap and reduce dependency on large data annotation efforts by adding use of open source labels including OpenStreetMaps and adding complementary data sources including Height Above Nearest Drainage (HAND) as a side channel to the network's input to encourage it to learn other features orthogonal to visual characteristics. Modeling efforts include modification of connected U-Nets for (1) semantic segmentation, (2) flood line detection, and (3) for damage assessment. In particular for the case of damage assessment, we added a second encoder to U-Net so that it could learn pre-event and post-event image features simultaneously. Through this method, the network is able to learn the difference between the pre- and post-disaster images, and therefore more effectively classify the level of damage. We have validated our approaches using publicly available data from the National Oceanic and Atmospheric Administration (NOAA)'s Remote Sensing Division, which displays the city and street-level details as mosaic tile images as well as data released as part of the Xview2 challenge.      
### 77.Rotation-Invariant Gait Identification with Quaternion Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.07393.pdf)
>  A desireable property of accelerometric gait-based identification systems is robustness to new device orientations presented by users during testing but unseen during the training phase. However, traditional Convolutional neural networks (CNNs) used in these systems compensate poorly for such transformations. In this paper, we target this problem by introducing Quaternion CNN, a network architecture which is intrinsically layer-wise equivariant and globally invariant under 3D rotations of an array of input vectors. We show empirically that this network indeed significantly outperforms a traditional CNN in a multi-user rotation-invariant gait classification setting .Lastly, we demonstrate how the kernels learned by this QCNN can also be visualized as basis-independent but origin- and chirality-dependent trajectory fragments in the euclidean space, thus yielding a novel mode of feature visualization and extraction.      
### 78.Dissecting liabilities in adversarial surgical robot failures: A national (Danish) and European law perspective  [ :arrow_down: ](https://arxiv.org/pdf/2008.07381.pdf)
>  Being connected to a network exposes surgical robots to cyberattacks, which can damage the patient or the operator. These injuries are normally caused by safety failures, such as accidents with industrial robots, but cyberattacks are caused by security failures instead. Surgical robots are increasingly sold and used in the European Union, so we decide to uncover whether this change has been considered by EU law, and which legal remedies and actions a patient or manufacturer would have in a single national legal system in the union. We first conduct a case study, where we analyse which legal remedies a patient can make use of, if they are injured by a surgical robot caused by a cyberattack in the national legal system. We also explore whether cybersecurity and cyberattacks are considered by the upcoming Medical Device Regulation of the EU. We show that the selected national legal system is adequate. This is because of its flexibility and in a certain approach even to ignore the distinction between safety and security to the benefit of the patient, and in one situation to remove liability from the manufacturer by erasing its status as party. Otherwise, unless the operator or other parties have made the cyberattack more likely to occur, the manufacturer is liable. We find that the regulation does not directly consider security defects, requiring interpretation and use of guidance to show it. Due to the risk cyberattacks pose on medical equipment, we find this to not be adequate. We further find that the regulators of medical devices, including surgical robots, will not necessarily have adequate staff or rules of enforcement, as this has been left to the member states to solve. But, we also find, due to the comprehensive number of rules that can be applied cumulatively, together with the possibility for further rules and compliance later on, that these issues could be solved in the future.      
### 79.SoftPoolNet: Shape Descriptor for Point Cloud Completion and Classification  [ :arrow_down: ](https://arxiv.org/pdf/2008.07358.pdf)
>  Point clouds are often the default choice for many applications as they exhibit more flexibility and efficiency than volumetric data. Nevertheless, their unorganized nature -- points are stored in an unordered way -- makes them less suited to be processed by deep learning pipelines. In this paper, we propose a method for 3D object completion and classification based on point clouds. We introduce a new way of organizing the extracted features based on their activations, which we name soft pooling. For the decoder stage, we propose regional convolutions, a novel operator aimed at maximizing the global activation entropy. Furthermore, inspired by the local refining procedure in Point Completion Network (PCN), we also propose a patch-deforming operation to simulate deconvolutional operations for point clouds. This paper proves that our regional activation can be incorporated in many point cloud architectures like AtlasNet and PCN, leading to better performance for geometric completion. We evaluate our approach on different 3D tasks such as object completion and classification, achieving state-of-the-art accuracy.      
### 80.Estimating action plans for smart poultry houses  [ :arrow_down: ](https://arxiv.org/pdf/2008.07356.pdf)
>  In poultry farming, the systematic choice, update, and implementation of periodic (t) action plans define the feed conversion rate (FCR[t]), which is an acceptable measure for successful production. Appropriate action plans provide tailored resources for broilers, allowing them to grow within the so-called thermal comfort zone, without wast or lack of resources. Although the implementation of an action plan is automatic, its configuration depends on the knowledge of the specialist, tending to be inefficient and error-prone, besides to result in different FCR[t] for each poultry house. In this article, we claim that the specialist's perception can be reproduced, to some extent, by computational intelligence. By combining deep learning and genetic algorithm techniques, we show how action plans can adapt their performance over the time, based on previous well succeeded plans. We also implement a distributed network infrastructure that allows to replicate our method over distributed poultry houses, for their smart, interconnected, and adaptive control. A supervision system is provided as interface to users. Experiments conducted over real data show that our method improves 5% on the performance of the most productive specialist, staying very close to the optimal FCR[t].      
### 81.POP909: A Pop-song Dataset for Music Arrangement Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07142.pdf)
>  Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.      
### 82.Learning Interpretable Representation for Controllable Polyphonic Music Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07122.pdf)
>  While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.      
### 83.Computer-Generated Music for Tabletop Role-Playing Games  [ :arrow_down: ](https://arxiv.org/pdf/2008.07009.pdf)
>  In this paper we present Bardo Composer, a system to generate background music for tabletop role-playing games. Bardo Composer uses a speech recognition system to translate player speech into text, which is classified according to a model of emotion. Bardo Composer then uses Stochastic Bi-Objective Beam Search, a variant of Stochastic Beam Search that we introduce in this paper, with a neural model to generate musical pieces conveying the desired emotion. We performed a user study with 116 participants to evaluate whether people are able to correctly identify the emotion conveyed in the pieces generated by the system. In our study we used pieces generated for Call of the Wild, a Dungeons and Dragons campaign available on YouTube. Our results show that human subjects could correctly identify the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces written by humans.      
### 84.Learning Disentangled Expression Representations from Facial Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.07001.pdf)
>  Face images are subject to many different factors of variation, especially in unconstrained in-the-wild scenarios. For most tasks involving such images, e.g. expression recognition from video streams, having enough labeled data is prohibitively expensive. One common strategy to tackle such a problem is to learn disentangled representations for the different factors of variation of the observed data using adversarial learning. In this paper, we use a formulation of the adversarial loss to learn disentangled representations for face images. The used model facilitates learning on single-task datasets and improves the state-of-the-art in expression recognition with an accuracy of60.53%on the AffectNetdataset, without using any additional data.      
### 85.Power and the Pandemic: Exploring Global Changes in Electricity Demand During COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2008.06988.pdf)
>  Understanding how efforts to limit exposure to COVID-19 have altered electricity demand provides insights not only into how dramatic restrictions shape electricity demand but also about future electricity use in a post-COVID-19 world. We develop a unified modeling framework to quantify and compare electricity usage changes in 58 countries and regions around the world from January-May 2020. We find that daily electricity demand declined as much as 10% in April 2020 compared to modelled demand, controlling for weather, seasonal and temporal effects, but with significant variation. Clustering techniques show that four impact groups capture systematic differences in timing and depth of electricity usage changes, ranging from a mild decline of 2% to an extreme decline of 26%. These groupings do not align with geography, with almost every continent having at least one country or region that experienced a dramatic reduction in demand and one that did not. Instead, we find that such changes relate to government restrictions and mobility. Government restrictions have a non-linear effect on demand that generally saturates at its most restrictive levels and sustains even as restrictions ease. Mobility offers a sharper focus on electricity demand change with workplace and residential mobility strongly linked to demand changes at the daily level. Steep declines in electricity usage are associated with workday hourly load patterns that resemble pre-COVID weekend usage. Quantifying these impacts is a crucial first step in understanding the impacts of crises like the pandemic and the associated societal response on electricity demand.      
### 86.Modification of Gesture-Determined-Dynamic Function with Consideration of Margins for Motion Planning of Humanoid Robots  [ :arrow_down: ](https://arxiv.org/pdf/2008.06899.pdf)
>  The gesture-determined-dynamic function (GDDF) offers an effective way to handle the control problems of humanoid robots. Specifically, GDDF is utilized to constrain the movements of dual arms of humanoid robots and steer specific gestures to conduct demanding tasks under certain conditions. However, there is still a deficiency in this scheme. Through experiments, we found that the joints of the dual arms, which can be regarded as the redundant manipulators, could exceed their limits slightly at the joint angle level. The performance straightly depends on the parameters designed beforehand for the GDDF, which causes a lack of adaptability to the practical applications of this method. In this paper, a modified scheme of GDDF with consideration of margins (MGDDF) is proposed. This MGDDF scheme is based on quadratic programming (QP) framework, which is widely applied to solving the redundancy resolution problems of robot arms. Moreover, three margins are introduced in the proposed MGDDF scheme to avoid joint limits. With consideration of these margins, the joints of manipulators of the humanoid robots will not exceed their limits, and the potential damages which might be caused by exceeding limits will be completely avoided. Computer simulations conducted on MATLAB further verify the feasibility and superiority of the proposed MGDDF scheme.      
### 87.Adaptive Shape Servoing of Elastic Rods using Parameterized Regression Features and Auto-Tuning Motion Controls  [ :arrow_down: ](https://arxiv.org/pdf/2008.06896.pdf)
>  In this paper, we present a new vision-based method to control the shape of elastic rods with robot manipulators. Our new method computes parameterized regression features from online sensor measurements that enable to automatically quantify the object's configuration and establish an explicit shape servo-loop. To automatically deform the rod into a desired shape, our adaptive controller iteratively estimates the differential transformation between the robot's motion and the relative shape changes; This valuable capability allows to effectively manipulate objects with unknown mechanical models. An auto-tuning algorithm is introduced to adjust the robot's shaping motion in real-time based on optimal performance criteria. To validate the proposed theory, we present a detailed numerical and experimental study with vision-guided robotic manipulators.      
### 88.Detection of Gait Abnormalities caused by Neurological Disorders  [ :arrow_down: ](https://arxiv.org/pdf/2008.06861.pdf)
>  In this paper, we leverage gait to potentially detect some of the important neurological disorders, namely Parkinson's disease, Diplegia, Hemiplegia, and Huntington's Chorea. Persons with these neurological disorders often have a very abnormal gait, which motivates us to target gait for their potential detection. Some of the abnormalities involve the circumduction of legs, forward-bending, involuntary movements, etc. To detect such abnormalities in gait, we develop gait features from the key-points of the human pose, namely shoulders, elbows, hips, knees, ankles, etc. To evaluate the effectiveness of our gait features in detecting the abnormalities related to these diseases, we build a synthetic video dataset of persons mimicking the gait of persons with such disorders, considering the difficulty in finding a sufficient number of people with these disorders. We name it \textit{NeuroSynGait} video dataset. Experiments demonstrated that our gait features were indeed successful in detecting these abnormalities.      
### 89.A Deep Convolutional Neural Network for the Detection of Polyps in Colonoscopy Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.06721.pdf)
>  Computerized detection of colonic polyps remains an unsolved issue because of the wide variation in the appearance, texture, color, size, and presence of the multiple polyp-like imitators during colonoscopy. In this paper, we propose a deep convolutional neural network based model for the computerized detection of polyps within colonoscopy images. The proposed model comprises 16 convolutional layers with 2 fully connected layers, and a Softmax layer, where we implement a unique approach using different convolutional kernels within the same hidden layer for deeper feature extraction. We applied two different activation functions, MISH and rectified linear unit activation functions for deeper propagation of information and self regularized smooth non-monotonicity. Furthermore, we used a generalized intersection of union, thus overcoming issues such as scale invariance, rotation, and shape. Data augmentation techniques such as photometric and geometric distortions are adapted to overcome the obstacles faced in polyp detection. Detailed benchmarked results are provided, showing better performance in terms of precision, sensitivity, F1- score, F2- score, and dice-coefficient, thus proving the efficacy of the proposed model.      
### 90.On the Relationship Between Network Topology and Throughput in Mesh Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.06708.pdf)
>  The relationship between topology and network throughput of arbitrarily-connected mesh networks is studied. Taking into account nonlinear channel properties, it is shown that throughput decreases logarithmically with physical network size with minor dependence on network ellipticity.      
### 91.Evolving Deep Convolutional Neural Networks for Hyperspectral Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2008.06634.pdf)
>  Hyperspectral images (HSIs) are susceptible to various noise factors leading to the loss of information, and the noise restricts the subsequent HSIs object detection and classification tasks. In recent years, learning-based methods have demonstrated their superior strengths in denoising the HSIs. Unfortunately, most of the methods are manually designed based on the extensive expertise that is not necessarily available to the users interested. In this paper, we propose a novel algorithm to automatically build an optimal Convolutional Neural Network (CNN) to effectively denoise HSIs. Particularly, the proposed algorithm focuses on the architectures and the initialization of the connection weights of the CNN. The experiments of the proposed algorithm have been well-designed and compared against the state-of-the-art peer competitors, and the experimental results demonstrate the competitive performance of the proposed algorithm in terms of the different evaluation metrics, visual assessments, and the computational complexity.      
### 92.Audio-Visual Event Localization via Recursive Fusion by Joint Co-Attention  [ :arrow_down: ](https://arxiv.org/pdf/2008.06581.pdf)
>  The major challenge in audio-visual event localization task lies in how to fuse information from multiple modalities effectively. Recent works have shown that attention mechanism is beneficial to the fusion process. In this paper, we propose a novel joint attention mechanism with multimodal fusion methods for audio-visual event localization. Particularly, we present a concise yet valid architecture that effectively learns representations from multiple modalities in a joint manner. Initially, visual features are combined with auditory features and then turned into joint representations. Next, we make use of the joint representations to attend to visual features and auditory features, respectively. With the help of this joint co-attention, new visual and auditory features are produced, and thus both features can enjoy the mutually improved benefits from each other. It is worth noting that the joint co-attention unit is recursive meaning that it can be performed multiple times for obtaining better joint representations progressively. Extensive experiments on the public AVE dataset have shown that the proposed method achieves significantly better results than the state-of-the-art methods.      
### 93.AntiDote: Attention-based Dynamic Optimization for Neural Network Runtime Efficiency  [ :arrow_down: ](https://arxiv.org/pdf/2008.06543.pdf)
>  Convolutional Neural Networks (CNNs) achieved great cognitive performance at the expense of considerable computation load. To relieve the computation load, many optimization works are developed to reduce the model redundancy by identifying and removing insignificant model components, such as weight sparsity and filter pruning. However, these works only evaluate model components' static significance with internal parameter information, ignoring their dynamic interaction with external inputs. With per-input feature activation, the model component significance can dynamically change, and thus the static methods can only achieve sub-optimal results. Therefore, we propose a dynamic CNN optimization framework in this work. Based on the neural network attention mechanism, we propose a comprehensive dynamic optimization framework including (1) testing-phase channel and column feature map pruning, as well as (2) training-phase optimization by targeted dropout. Such a dynamic optimization framework has several benefits: (1) First, it can accurately identify and aggressively remove per-input feature redundancy with considering the model-input interaction; (2) Meanwhile, it can maximally remove the feature map redundancy in various dimensions thanks to the multi-dimension flexibility; (3) The training-testing co-optimization favors the dynamic pruning and helps maintain the model accuracy even with very high feature pruning ratio. Extensive experiments show that our method could bring 37.4% to 54.5% FLOPs reduction with negligible accuracy drop on various of test networks.      
