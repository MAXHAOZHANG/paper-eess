# ArXiv eess --Thu, 6 Aug 2020
### 1.Remote atrial fibrillation burden estimation using deep recurrent neural network  [ :arrow_down: ](https://arxiv.org/pdf/2008.02228.pdf)
>  The atrial fibrillation burden (AFB) is defined as the percentage of time spend in atrial fibrillation (AF) over a long enough monitoring period. Recent research has demonstrated the added prognosis value that becomes available by using the AFB as compared with the binary diagnosis. We evaluate, for the first time, the ability to estimate the AFB over long-term continuous recordings, using a deep recurrent neutral network (DRNN) approach. Methods: The models were developed and evaluated on a large database of p=2,891 patients, totaling t=68,800 hours of continuous electrocardiography (ECG) recordings acquired at the University of Virginia heart station. Specifically, 24h beat-to-beat time series were obtained from a single portable ECG channel. The network, denoted ArNet, was benchmarked against a gradient boosting (XGB) model, trained on 21 features including the coefficient of sample entropy (CosEn) and AFEvidence. Data were divided into training and test sets, while patients were stratified by the presence and severity of AF. The generalizations of ArNet and XGB were also evaluated on the independent test PhysioNet LTAF database. Results: the absolute AF burden estimation error |E_AF|, median and interquartile, on the test set, was 1.2 (0.1-6.7) for ArNet and 3.1 (0.0-11.7) for XGB for AF individuals. Generalization results on LTAF were consistent with E_AF of 2.6 (1.1-14.7) for ArNet and 3.6 (1.0-16.7) for XGB. Conclusion: This research demonstrates the feasibility of AFB estimation from 24h beat-to-beat interval time series utilizing recent advances in DRNN. Significance: The novel data-driven approach enables robust remote diagnosis and phenotyping of AF.      
### 2.Resource Allocation in 6G Optical Wireless Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.02188.pdf)
>  The abundant optical spectrum is a promising part of the electromagnetic spectrum for 6G communication systems. The visible light spectrum which is a part of the optical spectrum, can be used to provide communication and illumination simultaneously. Visible light communication (VLC) systems have been widely researched, however, little work has focused on the area of multiple access. This chapter studies wavelength division multiple access (WDMA) techniques in VLC systems to support multiple users. In addition, the optimization of resource allocation is considered in this chapter by developing a mixed-integer linear programming (MILP) model that can be used to maximize the signal to noise and interference ratio (SINR) while supporting multiple users. The optimized resource allocation results in the best assignment of access points (APs) and wavelengths to users. Different indoor environments such as office, data center and aircraft cabins are evaluated in this chapter. A laser diode (LD) with four wavelengths (red, green, yellow and blue) is used to provide high bandwidth for communication and white light for illumination. Also, an angle diversity receiver (ADR) is utilized to receive signals and reduce noise and interference by exploiting the spatial domain.      
### 3.A Novel Method For Designing Transferable Soft Sensors And Its Application  [ :arrow_down: ](https://arxiv.org/pdf/2008.02186.pdf)
>  In this paper, a new approach is proposed for designing transferable soft sensors. Soft sensing is one of the significant applications of data-driven methods in the condition monitoring of plants. While hard sensors can be easily used in various plants, soft sensors are confined to the specific plant they are designed for and cannot be used in a new plant or even used in some new working conditions in the same plant. In this paper, a solution is proposed for this underlying obstacle in data-driven condition monitoring systems. Data-driven methods suffer from the fact that the distribution of the data by which the models are constructed may not be the same as the distribution of the data to which the model will be applied. This ultimately leads to the decline of models accuracy. We proposed a new transfer learning (TL) based regression method, called Domain Adversarial Neural Network Regression (DANN-R), and employed it for designing transferable soft sensors. We used data collected from the SCADA system of an industrial power plant to comprehensively investigate the functionality of the proposed method. The result reveals that the proposed transferable soft sensor can successfully adapt to new plants and new working conditions.      
### 4.Personal Identification Using Ultrawideband Radar Measurement of Walking and Sitting Motions and a Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.02182.pdf)
>  This study proposes a personal identification technique that applies machine learning with a two-layered convolutional neural network to spectrogram images obtained from radar echoes of a target person in motion. The walking and sitting motions of six participants were measured using an ultrawideband radar system. Time-frequency analysis was applied to the radar signal to generate spectrogram images containing the micro-Doppler components associated with limb movements. A convolutional neural network was trained using the spectrogram images with personal labels to achieve radar-based personal identification. The personal identification accuracies were evaluated experimentally to demonstrate the effectiveness of the proposed technique.      
### 5.Stabilization of Cascaded Two-Port Networked Systems with Simultaneous Nonlinear Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2008.02152.pdf)
>  We introduce a versatile framework to model and study networked control systems (NCSs). An NCS is described as a feedback interconnection of a plant and a controller communicating through a bidirectional channel modelled by cascaded nonlinear two-port networks. This model is sufficiently rich to capture various properties of a real-world communication channel, such as distortion, interference, and nonlinearity. Uncertainties in the plant, controller and communication channels can be handled simultaneously in the framework. We provide a necessary and sufficient condition for the robust finite-gain stability of an NCS when the model uncertainties in the plant and controller are measured by the gap metric and those in the nonlinear communication channels are measured by operator norms of the uncertain elements. This condition is given by an inequality involving "arcsine" of the uncertainty bounds and is derived from novel geometric insights underlying the robustness of a standard closed-loop system in the presence of conelike nonlinear perturbations on the system graphs.      
### 6.COVID-19 in CXR: from Detection and Severity Scoring to Patient Disease Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2008.02150.pdf)
>  In this work, we estimate the severity of pneumonia in COVID-19 patients and conduct a longitudinal study of disease progression. To achieve this goal, we developed a deep learning model for simultaneous detection and segmentation of pneumonia in chest Xray (CXR) images and generalized to COVID-19 pneumonia. The segmentations were utilized to calculate a "Pneumonia Ratio" which indicates the disease severity. The measurement of disease severity enables to build a disease extent profile over time for hospitalized patients. To validate the model relevance to the patient monitoring task, we developed a validation strategy which involves a synthesis of Digital Reconstructed Radiographs (DRRs - synthetic Xray) from serial CT scans; we then compared the disease progression profiles that were generated from the DRRs to those that were generated from CT volumes.      
### 7.Modeling and simulation of adaptive cruise control system  [ :arrow_down: ](https://arxiv.org/pdf/2008.02103.pdf)
>  In this report, linear quadratic regulator is used to design adaptive cruise control system. In the regulator, Q and R parameters vary with time according to current traffic situations. Phase-plant method is used to give constraints on Q and R parameters, and coefficient descent method is applied to solve for the constrained optimization problem. Meanwhile, data based controller design method is also introduced in this paper, where the time vary vehicle dynamic parameters are no longer considered. Q-function, which consists of Markovian state and action penalty, is introduced to indicate the cost function. According to current traffic states, Q-function is generalized and minimized by directly using least error method, whose stability is ensured by nonlinear regression theory. Simulation is conducted and results show the advantages of using time varying parameter linear quadratic regulator over other controller discussed in this paper. Vehicle tests are also conducted to ensure the feasibility and efficiency of the controller.      
### 8.Structure Preserving Stain Normalization of Histopathology Images Using Self-Supervised Semantic Guidance  [ :arrow_down: ](https://arxiv.org/pdf/2008.02101.pdf)
>  Although generative adversarial network (GAN) based style transfer is state of the art in histopathology color-stain normalization, they do not explicitly integrate structural information of tissues. We propose a self-supervised approach to incorporate semantic guidance into a GAN based stain normalization framework and preserve detailed structural information. Our method does not require manual segmentation maps which is a significant advantage over existing methods. We integrate semantic information at different layers between a pre-trained semantic network and the stain color normalization network. The proposed scheme outperforms other color normalization methods leading to better classification and segmentation performance.      
### 9.Speaker dependent acoustic-to-articulatory inversion using real-time MRI of the vocal tract  [ :arrow_down: ](https://arxiv.org/pdf/2008.02098.pdf)
>  Acoustic-to-articulatory inversion (AAI) methods estimate articulatory movements from the acoustic speech signal, which can be useful in several tasks such as speech recognition, synthesis, talking heads and language tutoring. Most earlier inversion studies are based on point-tracking articulatory techniques (e.g. EMA or XRMB). The advantage of rtMRI is that it provides dynamic information about the full midsagittal plane of the upper airway, with a high 'relative' spatial resolution. In this work, we estimated midsagittal rtMRI images of the vocal tract for speaker dependent AAI, using MGC-LSP spectral features as input. We applied FC-DNNs, CNNs and recurrent neural networks, and have shown that LSTMs are the most suitable for this task. As objective evaluation we measured normalized MSE, Structural Similarity Index (SSIM) and its complex wavelet version (CW-SSIM). The results indicate that the combination of FC-DNNs and LSTMs can achieve smooth generated MR images of the vocal tract, which are similar to the original MRI recordings (average CW-SSIM: 0.94).      
### 10.Comparison of Source Coding Techniques for the Vehicle to Vehicle Communication  [ :arrow_down: ](https://arxiv.org/pdf/2008.02097.pdf)
>  Autonomous driving is gaining its importance due to the advancements in technology. With the intention of safety during human driving and with the longer-term aim to act as a communication enabler for autonomous driving, vehicle to vehicle communication is gaining its importance. In this paper, we discuss and compare various source coding techniques that can be used for vehicle to vehicle communication. We propose abbreviation-based and probability-based source coding methods for the vehicle to vehicle communication. We compare the proposed application-specific source coding methods with other techniques like Huffman, Arithmetic, and Lempel-Ziv-Welch coding. Experimental results show that the proposed probability-based source coding has better values of the compression ratio to the time required for all the messages considered.      
### 11.A Generative Machine Learning-Based Approach for Inverse Design of Multilayer Metasurfaces  [ :arrow_down: ](https://arxiv.org/pdf/2008.02074.pdf)
>  The synthesis of a metasurface exhibiting a specific set of desired scattering properties is a time-consuming and resource-demanding process, which conventionally relies on many cycles of full-wave simulations. It requires an experienced designer to choose the number of the metallic layers, the scatterer shapes and dimensions, and the type and the thickness of the separating substrates. Here, we propose a generative machine learning (ML)-based approach to solve this one-to-many mapping and automate the inverse design of dual- and triple-layer metasurfaces. Using this approach, it is possible to solve multiobjective optimization problems by synthesizing thin structures composed of potentially brand-new scatterer designs, in cases where the inter-layer coupling between the layers is non-negligible and synthesis by traditional methods becomes cumbersome. Various examples to provide specific magnitude and phase responses of $x$- and $y$-polarized scattering coefficients across a frequency range as well as mask-based responses for different metasurface applications are presented to verify the practicality of the proposed method.      
### 12.A Comparative study of Artificial Neural Networks Using Reinforcement learning and Multidimensional Bayesian Classification Using Parzen Density Estimation for Identification of GC-EIMS Spectra of Partially Methylated Alditol Acetates  [ :arrow_down: ](https://arxiv.org/pdf/2008.02072.pdf)
>  This study reports the development of a pattern recognition search engine for a World Wide Web-based database of gas chromatography-electron impact mass spectra (GC-EIMS) of partially methylated Alditol Acetates (PMAAs). Here, we also report comparative results for two pattern recognition techniques that were employed for this study. The first technique is a statistical technique using Bayesian classifiers and Parzen density estimators. The second technique involves an artificial neural network module trained with reinforcement learning. We demonstrate here that both systems perform well in identifying spectra with small amounts of noise. Both system's performance degrades with degrading signal-to-noise ratio (SNR). When dealing with partial spectra (missing data), the artificial neural network system performs better. The developed system is implemented on the world wide web, and is intended to identify PMAAs using submitted spectra of these molecules recorded on any GC-EIMS instrument. The system, therefore, is insensitive to instrument and column dependent variations in GC-EIMS spectra.      
### 13.Content based singing voice source separation via strong conditioning using aligned phonemes  [ :arrow_down: ](https://arxiv.org/pdf/2008.02070.pdf)
>  Informed source separation has recently gained renewed interest with the introduction of neural networks and the availability of large multitrack datasets containing both the mixture and the separated sources. These approaches use prior information about the target source to improve separation. Historically, Music Information Retrieval researchers have focused primarily on score-informed source separation, but more recent approaches explore lyrics-informed source separation. However, because of the lack of multitrack datasets with time-aligned lyrics, models use weak conditioning with non-aligned lyrics. In this paper, we present a multimodal multitrack dataset with lyrics aligned in time at the word level with phonetic information as well as explore strong conditioning using the aligned phonemes. Our model follows a U-Net architecture and takes as input both the magnitude spectrogram of a musical mixture and a matrix with aligned phonetic information. The phoneme matrix is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. These layers condition the U-Net feature maps to adapt the separation process to the presence of different phonemes via affine transformations. We show that phoneme conditioning can be successfully applied to improve singing voice source separation.      
### 14.Backward Simulation for Sets of Trajectories  [ :arrow_down: ](https://arxiv.org/pdf/2008.02051.pdf)
>  This paper presents a solution for recovering full trajectory information, via the calculation of the posterior of the set of trajectories, from a sequence of multitarget (unlabelled) filtering densities and the multitarget dynamic model. Importantly, the proposed solution opens an avenue of trajectory estimation possibilities for multitarget filters that do not explicitly estimate trajectories. In this paper, we first derive a general multitrajectory forward-backward smoothing equation based on sets of trajectories and the random finite set framework. Then we show how to sample sets of trajectories using backward simulation when the multitarget filtering densities are multi-Bernoulli processes. The proposed approach is demonstrated in a simulation study.      
### 15.Extracting and Leveraging Nodule Features with Lung Inpainting for Local Feature Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.02030.pdf)
>  Chest X-ray (CXR) is the most common examination for fast detection of pulmonary abnormalities. Recently, automated algorithms have been developed to classify multiple diseases and abnormalities in CXR scans. However, because of the limited availability of scans containing nodules and the subtle properties of nodules in CXRs, state-of-the-art methods do not perform well on nodule classification. To create additional data for the training process, standard augmentation techniques are applied. However, the variance introduced by these methods are limited as the images are typically modified globally. In this paper, we propose a method for local feature augmentation by extracting local nodule features using a generative inpainting network. The network is applied to generate realistic, healthy tissue and structures in patches containing nodules. The nodules are entirely removed in the inpainted representation. The extraction of the nodule features is processed by subtraction of the inpainted patch from the nodule patch. With arbitrary displacement of the extracted nodules in the lung area across different CXR scans and further local modifications during training, we significantly increase the nodule classification performance and outperform state-of-the-art augmentation methods.      
### 16.Learning to Denoise Historical Music  [ :arrow_down: ](https://arxiv.org/pdf/2008.02027.pdf)
>  We propose an audio-to-audio neural network model that learns to denoise old music recordings. Our model internally converts its input into a time-frequency representation by means of a short-time Fourier transform (STFT), and processes the resulting complex spectrogram using a convolutional neural network. The network is trained with both reconstruction and adversarial objectives on a synthetic noisy music dataset, which is created by mixing clean music with real noise samples extracted from quiet segments of old recordings. We evaluate our method quantitatively on held-out test examples of the synthetic dataset, and qualitatively by human rating on samples of actual historical recordings. Our results show that the proposed method is effective in removing noise, while preserving the quality and details of the original music.      
### 17.Multiple Sclerosis Lesion Activity Segmentation with Attention-Guided Two-Path CNNs  [ :arrow_down: ](https://arxiv.org/pdf/2008.02001.pdf)
>  Multiple sclerosis is an inflammatory autoimmune demyelinating disease that is characterized by lesions in the central nervous system. Typically, magnetic resonance imaging (MRI) is used for tracking disease progression. Automatic image processing methods can be used to segment lesions and derive quantitative lesion parameters. So far, methods have focused on lesion segmentation for individual MRI scans. However, for monitoring disease progression, \textit{lesion activity} in terms of new and enlarging lesions between two time points is a crucial biomarker. For this problem, several classic methods have been proposed, e.g., using difference volumes. Despite their success for single-volume lesion segmentation, deep learning approaches are still rare for lesion activity segmentation. In this work, convolutional neural networks (CNNs) are studied for lesion activity segmentation from two time points. For this task, CNNs are designed and evaluated that combine the information from two points in different ways. In particular, two-path architectures with attention-guided interactions are proposed that enable effective information exchange between the two time point's processing paths. It is demonstrated that deep learning-based methods outperform classic approaches and it is shown that attention-guided interactions significantly improve performance. Furthermore, the attention modules produce plausible attention maps that have a masking effect that suppresses old, irrelevant lesions. A lesion-wise false positive rate of 26.4% is achieved at a true positive rate of 74.2%, which is not significantly different from the interrater performance.      
### 18.Jointly Sparse Signal Recovery and Support Recovery via Deep Learning with Applications in MIMO-based Grant-Free Random Access  [ :arrow_down: ](https://arxiv.org/pdf/2008.01992.pdf)
>  In this paper, we investigate jointly sparse signal recovery and jointly sparse support recovery in Multiple Measurement Vector (MMV) models for complex signals, which arise in many applications in communications and signal processing. Recent key applications include channel estimation and device activity detection in MIMO-based grant-free random access which is proposed to support massive machine-type communications (mMTC) for \lscr{ Internet of Things (IoT)}. Utilizing techniques in compressive sensing, optimization and deep learning, we propose \wq{two model-driven} approaches, based on the standard auto-encoder structure for real numbers. One is to jointly design the common measurement matrix and jointly sparse signal recovery method, and the other aims to jointly design the common measurement matrix and jointly sparse support recovery method. The proposed model-driven approaches can effectively \lscr{utilize features of} sparsity patterns in \lscr{designing common} measurement matrices and \lscr{adjusting \wq{model-driven}} decoders, and can greatly benefit from the underlying state-of-the-art recovery methods with theoretical guarantee. Hence, \lscr{the obtained common measurement matrices and recovery methods} \lscb{can significantly outperform} the underlying advanced recovery methods. We conduct extensive numerical results on channel estimation and device activity detection in MIMO-based grant-free random access. The numerical results show that the proposed approaches \lscb{provide} pilot sequences and channel estimation or device activity detection methods \lscb{which can} achieve \lscb{higher} \lscr{estimation or detection} accuracy with shorter computation time than \lscb{existing ones}. Furthermore, the numerical results explain how such gains are achieved \lscb{via the} proposed approaches.      
### 19.Ergodic capacity analysis of reconfigurable intelligent surface assisted wireless systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.01931.pdf)
>  This paper presents the analytic framework for evaluating the ergodic capacity (EC) of the reconfigurable intelligent surface (RIS) assisted systems. Moreover, high-signal-to-noise-ratio and high-number of reflection units (RUs) approximations for the EC are provided. Finally, the special case in which the RIS is equipped with a single RU is investigated. Our analysis is verified through respective Monte Carlo simulations, which highlight the accuracy of the proposed framework.      
### 20.Hierarchical Amortized Training for Memory-efficient High Resolution 3D GAN  [ :arrow_down: ](https://arxiv.org/pdf/2008.01910.pdf)
>  Generative Adversarial Networks (GAN) have many potential medical imaging applications, including data augmentation, domain adaptation, and model explanation. Due to the limited embedded memory of Graphical Processing Units (GPUs), most current 3D GAN models are trained on low-resolution medical images. In this work, we propose a novel end-to-end GAN architecture that can generate high-resolution 3D images. We achieve this goal by separating training and inference. During training, we adopt a hierarchical structure that simultaneously generates a low-resolution version of the image and a randomly selected sub-volume of the high-resolution image. The hierarchical design has two advantages: First, the memory demand for training on high-resolution images is amortized among subvolumes. Furthermore, anchoring the high-resolution subvolumes to a single low-resolution image ensures anatomical consistency between subvolumes. During inference, our model can directly generate full high-resolution images. We also incorporate an encoder with a similar hierarchical structure into the model to extract features from the images. Experiments on 3D thorax CT and brain MRI demonstrate that our approach outperforms state of the art in image generation, image reconstruction, and clinical-relevant variables prediction.      
### 21.Integrated Traffic Simulation-Prediction System using Neural Networks with Application to the Los Angeles International Airport Road Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.01902.pdf)
>  Transportation networks are highly complex and the design of efficient traffic management systems is difficult due to lack of adequate measured data and accurate predictions of the traffic states. Traffic simulation models can capture the complex dynamics of transportation networks by using limited available traffic data and can help central traffic authorities in their decision-making, if appropriate input is fed into the simulator. In this paper, we design an integrated simulation-prediction system which estimates the Origin-Destination (OD) matrix of a road network using only flow rate information and predicts the behavior of the road network in different simulation scenarios. The proposed system includes an optimization-based OD matrix generation method, a Neural Network (NN) model trained to predict OD matrices via the pattern of traffic flow and a microscopic traffic simulator with a Dynamic Traffic Assignment (DTA) scheme to predict the behavior of the transportation system. We test the proposed system on the road network of the central terminal area (CTA) of the Los Angeles International Airport (LAX), which demonstrates that the integrated traffic simulation-prediction system can be used to simulate the effects of several real world scenarios such as lane closures, curbside parking and other changes. The model is an effective tool for learning the impact and possible benefits of changes in the network and for analyzing scenarios at a very low cost without disrupting the network.      
### 22.Machine Learning and Feature Engineering for Predicting Pulse Status during Chest Compressions  [ :arrow_down: ](https://arxiv.org/pdf/2008.01901.pdf)
>  Objective: Current resuscitation protocols require pausing chest compressions during cardiopulmonary resuscitation (CPR) to check for a pulse. However, pausing CPR during a pulseless rhythm can worsen patient outcome. Our objective is to design an ECG-based algorithm that predicts pulse status during uninterrupted CPR and evaluate its performance. Methods: We evaluated 383 patients being treated for out-of-hospital cardiac arrest using defibrillator data. We collected paired and immediately adjacent ECG segments having an organized rhythm. Segments were collected during the 10s period of ongoing CPR prior to a pulse check, and 5s segments without CPR during the pulse check. ECG segments with or without a pulse were identified by the audio annotation of a paramedic's pulse check findings and recorded blood pressures. We developed an algorithm to predict the clinical pulse status based on the wavelet transform of the bandpass-filtered ECG, applying principle component analysis. We then trained a linear discriminant model using 3 principle component modes. Model performance was evaluated on test group segments with and without CPR using receiver operating curves and according to the initial arrest rhythm. Results: There were 230 patients (540 pulse checks) in the training set and 153 patients (372 pulse checks) in the test set. Overall 38% (351/912) of checks had a spontaneous pulse. The areas under the receiver operating characteristic curve (AUCs) for predicting pulse status with and without CPR on test data were 0.84 and 0.89, respectively. Conclusion: A novel ECG-based algorithm demonstrates potential to improve resuscitation by predicting presence of a spontaneous pulse without pausing CPR. Significance: Our algorithm predicts pulse status during uninterrupted CPR, allowing for CPR to proceed unimpeded by pauses to check for a pulse and potentially improving resuscitation performance.      
### 23.A coarse-to-fine framework for unsupervised multi-contrast MR image deformable registration with dual consistency constraint  [ :arrow_down: ](https://arxiv.org/pdf/2008.01896.pdf)
>  Multi-contrast magnetic resonance (MR) image registration is essential in the clinic to achieve fast and accurate imaging-based disease diagnosis and treatment planning. Nevertheless, the efficiency and performance of the existing registration algorithms can still be improved. In this paper, we propose a novel unsupervised learning-based framework to achieve accurate and efficient multi-contrast MR image registrations. Specifically, an end-to-end coarse-to-fine network architecture consisting of affine and deformable transformations is designed to get rid of both the multi-step iteration process and the complex image preprocessing operations. Furthermore, a dual consistency constraint and a new prior knowledge-based loss function are developed to enhance the registration performances. The proposed method has been evaluated on a clinical dataset that consists of 555 cases, with encouraging performances achieved. Compared to the commonly utilized registration methods, including Voxelmorph, SyN, and LDDMM, the proposed method achieves the best registration performance with a Dice score of 0.826 in identifying stroke lesions. More robust performance in low-signal areas is also observed. With regards to the registration speed, our method is about 17 times faster than the most competitive method of SyN when testing on a same CPU.      
### 24.Stabilizing Deep Tomographic Reconstruction Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.01846.pdf)
>  While the field of deep tomographic reconstruction has been advancing rapidly since 2016, there are constant debates and major challenges with the recently published PNAS paper on instabilities of deep learning in image reconstruction as a primary example, in which three kinds of unstable phenomena are demonstrated: (1) tiny perturbation on input generating strong output artifacts, (2) small structural features going undetected, and (3) increased input data leading to decreased performance. In this article, we show that key algorithmic ingredients of analytic inversion, compressed sensing, iterative reconstruction, and deep learning can be synergized to stabilize deep neural networks for optimal tomographic image reconstruction. With the same or similar datasets used in the PNAS paper and relative to the same state of the art compressed sensing algorithm, our proposed analytic, compressed, iterative deep (ACID) network produces superior imaging performance that are both accurate and robust with respect to noise, under adversarial attack, and as the number of input data is increased. We believe that deep tomographic reconstruction networks can be designed to produce accurate and robust results, improve clinical and other important applications, and eventually dominate the tomographic imaging field.      
### 25.Secrecy Limits of Energy Harvesting IoT Networks under Channel Imperfections  [ :arrow_down: ](https://arxiv.org/pdf/2008.01835.pdf)
>  Simultaneous wireless information and power transfer (SWIPT) has recently gathered much research interest from both academia and industry as a key enabler of energy harvesting Internet-of-things (IoT) networks. Due to a number of growing use cases of such networks, it is important to study their performance limits from the perspective of physical layer security (PLS). With this intent, this work aims to provide a novel analysis of the ergodic secrecy capacity of a SWIPT system is provided for Rician and Nakagami-m faded communication links. For a realistic evaluation of the system, the imperfections of channel estimations for different receiver designs of the SWIPT-based IoT systems have been taken into account. Subsequently, the closedform expressions of the ergodic secrecy capacities for the considered scenario are provided and, then, validated through extensive simulations. The results indicate that an error ceiling appears due to imperfect channel estimation at high values of signal-to-noise ratio (SNR). More importantly, the secrecy capacity under different channel conditions stops increasing beyond a certain limit, despite an increase of the main link SNR. The in-depth analysis of secrecy-energy trade-off has also been performed and a comparison has been provided for imperfect and perfect channel estimation cases. As part of the continuous evolution of IoT networks, the results provided in this work can help in identifying the secrecy limits of IoT networks in the presence of multiple eavesdroppers.      
### 26.Future Vector Enhanced LSTM Language Model for LVCSR  [ :arrow_down: ](https://arxiv.org/pdf/2008.01832.pdf)
>  Language models (LM) play an important role in large vocabulary continuous speech recognition (LVCSR). However, traditional language models only predict next single word with given history, while the consecutive predictions on a sequence of words are usually demanded and useful in LVCSR. The mismatch between the single word prediction modeling in trained and the long term sequence prediction in read demands may lead to the performance degradation. In this paper, a novel enhanced long short-term memory (LSTM) LM using the future vector is proposed. In addition to the given history, the rest of the sequence will be also embedded by future vectors. This future vector can be incorporated with the LSTM LM, so it has the ability to model much longer term sequence level information. Experiments show that, the proposed new LSTM LM gets a better result on BLEU scores for long term sequence prediction. For the speech recognition rescoring, although the proposed LSTM LM obtains very slight gains, the new model seems obtain the great complementary with the conventional LSTM LM. Rescoring using both the new and conventional LSTM LMs can achieve a very large improvement on the word error rate.      
### 27.Fast Nonconvex $T_2^*$ Mapping Using ADMM  [ :arrow_down: ](https://arxiv.org/pdf/2008.01806.pdf)
>  Magnetic resonance (MR)-$T_2^*$ mapping is widely used to study hemorrhage, calcification and iron deposition in various clinical applications, it provides a direct and precise mapping of desired contrast in the tissue. However, the long acquisition time required by conventional 3D high-resolution $T_2^*$ mapping method causes discomfort to patients and introduces motion artifacts to reconstructed images, which limits its wider applicability. In this paper we address this issue by performing $T_2^*$ mapping from undersampled data using compressive sensing (CS). We formulate the reconstruction as a nonconvex problem that can be decomposed into two subproblems. They can be solved either separately via the standard approach or jointly via the alternating direction method of multipliers (ADMM). Compared to previous CS-based approaches that only apply sparse regularization on the spin density $\boldsymbol X_0$ and the relaxation rate $\boldsymbol R_2^*$, our formulation enforces additional sparse priors on the $T_2^*$-weighted images at multiple echoes to improve the reconstruction performance. We performed convergence analysis of the proposed algorithm, evaluated its performance on in vivo data, and studied the effects of different sampling schemes. Experimental results showed that the proposed joint-recovery approach generally outperforms the state-of-the-art method, especially in the low-sampling rate regime, making it a preferred choice to perform fast 3D $T_2^*$ mapping in practice. The framework adopted in this work can be easily extended to other problems arising from MR or other imaging modalities with non-linearly coupled variables.      
### 28.Learning Long-term Visual Dynamics with Region Proposal Interaction Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.02265.pdf)
>  Learning long-term dynamics models is the key to understanding physical common sense. Most existing approaches on learning dynamics from visual input sidestep long-term predictions by resorting to rapid re-planning with short-term models. This not only requires such models to be super accurate but also limits them only to tasks where an agent can continuously obtain feedback and take action at each step until completion. In this paper, we aim to leverage the ideas from success stories in visual recognition tasks to build object representations that can capture inter-object and object-environment interactions over a long range. To this end, we propose Region Proposal Interaction Networks (RPIN), which reason about each object's trajectory in a latent region-proposal feature space. Thanks to the simple yet effective object representation, our approach outperforms prior methods by a significant margin both in terms of prediction quality and their ability to plan for downstream tasks, and also generalize well to novel environments. Our code is available at <a class="link-external link-https" href="https://github.com/HaozhiQi/RPIN" rel="external noopener nofollow">this https URL</a>.      
### 29.A Fast Certificate for Power System Small-Signal Stability  [ :arrow_down: ](https://arxiv.org/pdf/2008.02263.pdf)
>  Swing equations are an integral part of a large class of power system dynamical models used in rotor angle stability assessment. Despite intensive studies, some fundamental properties of lossy swing equations are still not fully understood. In this paper, we develop a sufficient condition for certifying the stability of equilibrium points (EPs) of these equations, and illustrate the effects of damping, inertia, and network topology on the stability properties of such EPs. The proposed certificate is suitable for real-time monitoring and fast stability assessment, as it is purely algebraic and can be evaluated in a parallel manner. Moreover, we provide a novel approach to quantitatively measure the degree of stability in power grids using the proposed certificate. Extensive computational experiments are conducted, demonstrating the practicality and effectiveness of the proposal.      
### 30.Fully Automated and Standardized Segmentation of Adipose Tissue Compartments by Deep Learning in Three-dimensional Whole-body MRI of Epidemiological Cohort Studies  [ :arrow_down: ](https://arxiv.org/pdf/2008.02251.pdf)
>  Purpose: To enable fast and reliable assessment of subcutaneous and visceral adipose tissue compartments derived from whole-body MRI. Methods: Quantification and localization of different adipose tissue compartments from whole-body MR images is of high interest to examine metabolic conditions. For correct identification and phenotyping of individuals at increased risk for metabolic diseases, a reliable automatic segmentation of adipose tissue into subcutaneous and visceral adipose tissue is required. In this work we propose a 3D convolutional neural network (DCNet) to provide a robust and objective segmentation. In this retrospective study, we collected 1000 cases (66$\pm$ 13 years; 523 women) from the Tuebingen Family Study and from the German Center for Diabetes research (TUEF/DZD), as well as 300 cases (53$\pm$ 11 years; 152 women) from the German National Cohort (NAKO) database for model training, validation, and testing with a transfer learning between the cohorts. These datasets had variable imaging sequences, imaging contrasts, receiver coil arrangements, scanners and imaging field strengths. The proposed DCNet was compared against a comparable 3D UNet segmentation in terms of sensitivity, specificity, precision, accuracy, and Dice overlap. Results: Fast (5-7seconds) and reliable adipose tissue segmentation can be obtained with high Dice overlap (0.94), sensitivity (96.6%), specificity (95.1%), precision (92.1%) and accuracy (98.4%) from 3D whole-body MR datasets (field of view coverage 450x450x2000mm${}^3$). Segmentation masks and adipose tissue profiles are automatically reported back to the referring physician. Conclusion: Automatic adipose tissue segmentation is feasible in 3D whole-body MR data sets and is generalizable to different epidemiological cohort studies with the proposed DCNet.      
### 31.Meta Continual Learning via Dynamic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2008.02219.pdf)
>  Meta-continual learning algorithms seek to rapidly train a model when faced with similar tasks sampled sequentially from a task distribution. Although impressive strides have been made in this area, there is no theoretical framework that enables systematic analysis of key learning challenges, such as generalization and catastrophic forgetting. We introduce a new theoretical framework for meta-continual learning using dynamic programming, analyze generalization and catastrophic forgetting, and establish conditions of optimality. We show that existing meta-continual learning methods can be derived from the proposed dynamic programming framework. Moreover, we develop a new dynamic-programming-based meta-continual approach that adopts stochastic-gradient-driven alternating optimization method. We show that, on meta-continual learning benchmark data sets, our theoretically grounded meta-continual learning approach is better than or comparable to the purely empirical strategies adopted by the existing state-of-the-art methods.      
### 32.Machine Learning in Nano-Scale Biomedical Engineering  [ :arrow_down: ](https://arxiv.org/pdf/2008.02195.pdf)
>  Machine learning (ML) empowers biomedical systems with the capability to optimize their performance through modeling of the available data extremely well, without using strong assumptions about the modeled system. Especially in nano-scale biosystems, where the generated data sets are too vast and complex to mentally parse without computational assist, ML is instrumental in analyzing and extracting new insights, accelerating material and structure discoveries and designing experience as well as supporting nano-scale communications and networks. However, despite these efforts, the use of ML in nano-scale biomedical engineering remains still under-explored in certain areas and research challenges are still open in fields such as structure and material design and simulations, communications and signal processing, and bio-medicine applications. In this article, we review the existing research regarding the use of ML in nano-scale biomedical engineering. In more detail, we first identify and discuss the main challenges that can be formulated as ML problems. These challenges are classified in the three aforementioned main categories. Next, we discuss the state of the art ML methodologies that are used to countermeasure the aforementioned challenges. For each of the presented methodologies, special emphasis is given to its principles, applications and limitations. Finally, we conclude the article with insightful discussions, that reveals research gaps and highlights possible future research directions.      
### 33.On the Characterization of Expressive Performance in Classical Music: First Results of the Con Espressione Game  [ :arrow_down: ](https://arxiv.org/pdf/2008.02194.pdf)
>  A piece of music can be expressively performed, or interpreted, in a variety of ways. With the help of an online questionnaire, the Con Espressione Game, we collected some 1,500 descriptions of expressive character relating to 45 performances of 9 excerpts from classical piano pieces, played by different famous pianists. More specifically, listeners were asked to describe, using freely chosen words (preferably: adjectives), how they perceive the expressive character of the different performances. In this paper, we offer a first account of this new data resource for expressive performance research, and provide an exploratory analysis, addressing three main questions: (1) how similarly do different listeners describe a performance of a piece? (2) what are the main dimensions (or axes) for expressive character emerging from this?; and (3) how do measurable parameters of a performance (e.g., tempo, dynamics) and mid- and high-level features that can be predicted by machine learning models (e.g., articulation, arousal) relate to these expressive dimensions? The dataset that we publish along with this paper was enriched by adding hand-corrected score-to-performance alignments, as well as descriptive audio features such as tempo and dynamics curves.      
### 34.Learning from Sparse Demonstrations  [ :arrow_down: ](https://arxiv.org/pdf/2008.02159.pdf)
>  This paper proposes an approach which enables a robot to learn an objective function from sparse demonstrations of an expert. The demonstrations are given by a small number of sparse waypoints; the waypoints are desired outputs of the robot's trajectory at certain time instances, sparsely located within a demonstration time horizon. The duration of the expert's demonstration may be different from the actual duration of the robot's execution. The proposed method enables to jointly learn an objective function and a time-warping function such that the robot's reproduced trajectory has minimal distance to the sparse demonstration waypoints. Unlike existing inverse reinforcement learning techniques, the proposed approach uses the differential Pontryagin's maximum principle, which allows direct minimization of the distance between the robot's trajectory and the sparse demonstration waypoints and enables simultaneous learning of an objective function and a time-warping function. We demonstrate the effectiveness of the proposed approach in various simulated scenarios. We apply the method to learn motion planning/control of a 6-DoF maneuvering unmanned aerial vehicle (UAV) and a robot arm in environments with obstacles. The results show that a robot is able to learn a valid objective function to avoid obstacles with few demonstrated waypoints.      
### 35.Multiscale Stochastic Simulation of the US Pacific Northwest Using Distributed Computing and Databases with Integrated Inflow and Variable Renewable Energy  [ :arrow_down: ](https://arxiv.org/pdf/2008.02155.pdf)
>  Modelling challenges of the United States Pacific Northwest system have grown in the last decade. Besides classical modelling difficulties such as a complex hydro cascade with many operational constraints, we have seen higher penetration of variable renewable energy inside and outside the system leading to internal issues and completely different power exchanges with the West Coast System. The analysis of adequacy and reliability of this system motivated the design and implementation of a five-step simulator including four planning phases and an operation step. The five levels were modeled as mathematical programs that are linked from top to bottom by fixed decisions and improvements in forecasts, on the other hand they are also linked from bottom to top by system updated states. The solution of the millions of resulting mathematical programs was made possible by applying state of the art optimization techniques and high performance data bases in a massively parallel environment.      
### 36.Underlay Radar-Massive MIMO Spectrum Sharing: Modeling Fundamentals and Performance Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.02100.pdf)
>  In this work, we investigate underlay radar-massive MIMO cellular coexistence in LoS/near-LoS channels, where both systems are capable of 3D beamforming. The base station (BS) locations are modeled using a homogeneous Poisson point process (PPP), and a single radar is located at the center of a circular exclusion zone (EZ) within which the BSs are prohibited from operating. We derive a tight upper bound on the average interference power at the radar due to the massive MIMO downlink. This is based on a novel construction in which each Poisson Voronoi (PV) cell is bounded by its circumcircle in order to bound the effect of the random cell shapes on average interference. However, this model is intractable for characterizing the interference distribution, due to the correlation between the shapes/sizes of adjacent PV cells. Hence, we propose a tractable nominal interference model where each PV cell is modeled as a circular disk with area equal to the average area of the typical cell. We quantify the gap in the average interference power between these two models, and show that the upper bound is tight for realistic deployment parameters. Under the nominal interference model, we derive the equal interference contour in closed-form, and characterize the interference distribution using the dominant interferer approximation. Finally, we use tractable expressions for the interference distribution to characterize important radar performance metrics such as the spatial probability of false alarm/detection in a quasi-static target tracking scenario. We validate the accuracy of our analytical approximations using numerical results, which reveal useful trends in the average interference as a function of the deployment parameters, and provide useful system design insights in the form of radar receiver operating characteristic (ROC) curves for current and future radar-cellular spectrum sharing scenarios.      
### 37.A Novel Method for Scalable VLSI Implementation of Hyperbolic Tangent Function  [ :arrow_down: ](https://arxiv.org/pdf/2008.02078.pdf)
>  Hyperbolic tangent and Sigmoid functions are used as non-linear activation units in the artificial and deep neural networks. Since, these networks are computationally expensive, customized accelerators are designed for achieving the required performance at lower cost and power. The activation function and MAC units are the key building blocks of these neural networks. A low complexity and accurate hardware implementation of the activation function is required to meet the performance and area targets of such neural network accelerators. Moreover, a scalable implementation is required as the recent studies show that the DNNs may use different precision in different layers. This paper presents a novel method based on trigonometric expansion properties of the hyperbolic function for hardware implementation which can be easily tuned for different accuracy and precision requirements.      
### 38.Data Cleansing with Contrastive Learning for Vocal Note Event Annotations  [ :arrow_down: ](https://arxiv.org/pdf/2008.02069.pdf)
>  Data cleansing is a well studied strategy for cleaning erroneous labels in datasets, which has not yet been widely adopted in Music Information Retrieval. Previously proposed data cleansing models do not consider structured (e.g. time varying) labels, such as those common to music data. We propose a novel data cleansing model for time-varying, structured labels which exploits the local structure of the labels, and demonstrate its usefulness for vocal note event annotations in music. %Our model is trained in a contrastive learning manner by automatically creating local deformations of likely correct labels. Our model is trained in a contrastive learning manner by automatically contrasting likely correct labels pairs against local deformations of them. We demonstrate that the accuracy of a transcription model improves greatly when trained using our proposed strategy compared with the accuracy when trained using the original dataset. Additionally we use our model to estimate the annotation error rates in the DALI dataset, and highlight other potential uses for this type of model.      
### 39.Compact Graph Architecture for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.02063.pdf)
>  We propose a deep graph approach to address the task of speech emotion recognition. A compact, efficient and scalable way to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a graph convolution network (GCN)-based architecture that can perform an \emph{accurate} graph convolution in contrast to the approximate convolution used in standard GCNs. We evaluated the performance of our model for speech emotion recognition on the popular IEMOCAP database. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves state-of-the-art performance (4-class, $65.29\%$) with significantly fewer learnable parameters.      
### 40.Neural Loop Combiner: Neural Network Models for Assessing the Compatibility of Loops  [ :arrow_down: ](https://arxiv.org/pdf/2008.02011.pdf)
>  Music producers who use loops may have access to thousands in loop libraries, but finding ones that are compatible is a time-consuming process; we hope to reduce this burden with automation. State-of-the-art systems for estimating compatibility, such as AutoMashUpper, are mostly rule-based and could be improved on with machine learning. To train a model, we need a large set of loops with ground truth compatibility values. No such dataset exists, so we extract loops from existing music to obtain positive examples of compatible loops, and propose and compare various strategies for choosing negative examples. For reproducibility, we curate data from the Free Music Archive. Using this data, we investigate two types of model architectures for estimating the compatibility of loops: one based on a Siamese network, and the other a pure convolutional neural network (CNN). We conducted a user study in which participants rated the quality of the combinations suggested by each model, and found the CNN to outperform the Siamese network. Both model-based approaches outperformed the rule-based one. We have opened source the code for building the models and the dataset.      
### 41.Unsupervised seismic facies classification using deep convolutional autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2008.01995.pdf)
>  With the increased size and complexity of seismic surveys, manual labeling of seismic facies has become a significant challenge. Application of automatic methods for seismic facies interpretation could significantly reduce the manual labor and subjectivity of a particular interpreter present in conventional methods. A recently emerged group of methods is based on deep neural networks. These approaches are data-driven and require large labeled datasets for network training. We apply a deep convolutional autoencoder for unsupervised seismic facies classification, which does not require manually labeled examples. The facies maps are generated by clustering the deep-feature vectors obtained from the input data. Our method yields accurate results on real data and provides them instantaneously. The proposed approach opens up possibilities to analyze geological patterns in real time without human intervention.      
### 42.MultiCheXNet: A Multi-Task Learning Deep Network For Pneumonia-like Diseases Diagnosis From X-ray Scans  [ :arrow_down: ](https://arxiv.org/pdf/2008.01973.pdf)
>  We present MultiCheXNet, an end-to-end Multi-task learning model, that is able to take advantage of different X-rays data sets of Pneumonia-like diseases in one neural architecture, performing three tasks at the same time; diagnosis, segmentation and localization. The common encoder in our architecture can capture useful common features present in the different tasks. The common encoder has another advantage of efficient computations, which speeds up the inference time compared to separate models. The specialized decoders heads can then capture the task-specific features. We employ teacher forcing to address the issue of negative samples that hurt the segmentation and localization performance. Finally,we employ transfer learning to fine tune the classifier on unseen pneumonia-like diseases. The MTL architecture can be trained on joint or dis-joint labeled data sets. The training of the architecture follows a carefully designed protocol, that pre trains different sub-models on specialized datasets, before being integrated in the joint MTL model. Our experimental setup involves variety of data sets, where the baseline performance of the 3 tasks is compared to the MTL architecture performance. Moreover, we evaluate the transfer learning mode to COVID-19 data set,both from individual classifier model, and from MTL architecture classification head.      
### 43.Modelling epidemic dynamics under collective decision making  [ :arrow_down: ](https://arxiv.org/pdf/2008.01971.pdf)
>  During the course of an epidemic, individuals constantly make decisions on how to fight against epidemic spreading. Collectively, these individual decisions are critical to the global outcome of the epidemic, especially when no pharmaceutical interventions are available. However, existing epidemic models lack the ability to capture this complex decision-making process, which is shaped by an interplay of factors including government-mandated policy interventions, expected socio-economic costs, perceived infection risks and social influences. Here, we introduce a novel parsimonious model, grounded in evolutionary game theory, able to capture decision-making dynamics over heterogeneous time scales. Using real data, we analyse three case studies in the spreading of gonorrhoea, the 1918--19 Spanish flu and COVID-19. Behavioural factors shaping the course of the epidemic are intelligibly mapped onto a minimal set of model parameters, and their interplay gives rise to characteristic phenomena, such as sustained periodic outbreaks, multiple epidemic waves, or a successful eradication of the disease. Our model enables a direct assessment of the epidemiological and socio-economic impact of different policy interventions implemented to combat epidemic outbreaks. Besides the common-sense finding that stringent non-pharmaceutical interventions are essential to taming the initial phases of the outbreak, the duration of such interventions and the way they are phased out are key for an eradication in the medium-to-long term. Surprisingly, our findings reveal that social influence is a double-edged sword in the control of epidemics, helping strengthen collective adoption of self-protective behaviours during the early stages of the epidemic, but then accelerating their rejection upon lifting of non-pharmaceutical containment interventions.      
### 44.MusPy: A Toolkit for Symbolic Music Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.01951.pdf)
>  In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at <a class="link-external link-https" href="https://github.com/salu133445/muspy" rel="external noopener nofollow">this https URL</a> .      
### 45.Optimal Pooling Matrix Design for Group Testing with Dilution (Row Degree) Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2008.01944.pdf)
>  In this paper, we consider the problem of designing optimal pooling matrix for group testing (for example, for COVID-19 virus testing) with the constraint that no more than $r&gt;0$ samples can be pooled together, which we call "dilution constraint". This problem translates to designing a matrix with elements being either 0 or 1 that has no more than $r$ '1's in each row and has a certain performance guarantee of identifying anomalous elements. We explicitly give pooling matrix designs that satisfy the dilution constraint and have performance guarantees of identifying anomalous elements, and prove their optimality in saving the largest number of tests, namely showing that the designed matrices have the largest width-to-height ratio among all constraint-satisfying 0-1 matrices.      
### 46.A feature-supervised generative adversarial network for environmental monitoring during hazy days  [ :arrow_down: ](https://arxiv.org/pdf/2008.01942.pdf)
>  The adverse haze weather condition has brought considerable difficulties in vision-based environmental applications. While, until now, most of the existing environmental monitoring studies are under ordinary conditions, and the studies of complex haze weather conditions have been ignored. Thence, this paper proposes a feature-supervised learning network based on generative adversarial networks (GAN) for environmental monitoring during hazy days. Its main idea is to train the model under the supervision of feature maps from the ground truth. Four key technical contributions are made in the paper. First, pairs of hazy and clean images are used as inputs to supervise the encoding process and obtain high-quality feature maps. Second, the basic GAN formulation is modified by introducing perception loss, style loss, and feature regularization loss to generate better results. Third, multi-scale images are applied as the input to enhance the performance of discriminator. Finally, a hazy remote sensing dataset is created for testing our dehazing method and environmental detection. Extensive experimental results show that the proposed method has achieved better performance than current state-of-the-art methods on both synthetic datasets and real-world remote sensing images.      
### 47.A Probabilistic Model for Planar Sliding of Objects with Unknown Material Properties: Identification and Robust Planning  [ :arrow_down: ](https://arxiv.org/pdf/2008.01921.pdf)
>  This paper introduces a new technique for learning probabilistic models of mass and friction distributions of unknown objects, and performing robust sliding actions by using the learned models. The proposed method is executed in two consecutive phases. In the exploration phase, a table-top object is poked by a robot from different angles. The observed motions of the object are compared against simulated motions with various hypothesized mass and friction models. The simulation-to-reality gap is then differentiated with respect to the unknown mass and friction parameters, and the analytically computed gradient is used to optimize those parameters. Since it is difficult to disentangle the mass from the friction coefficients in low-data and quasi-static motion regimes, our approach retains a set of locally optimal pairs of mass and friction models. A probability distribution on the models is computed based on the relative accuracy of each pair of models. In the exploitation phase, a probabilistic planner is used to select a goal configuration and waypoints that are stable with a high confidence. The proposed technique is evaluated on real objects and using a real manipulator. The results show that this technique can not only identify accurately mass and friction coefficients of non-uniform heterogeneous objects, but can also be used to successfully slide an unknown object to the edge of a table and pick it up from there, without any human assistance or feedback.      
### 48.Generalization Guarantees for Multi-Modal Imitation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.01913.pdf)
>  Control policies from imitation learning can often fail to generalize to novel environments due to imperfect demonstrations or the inability of imitation learning algorithms to accurately infer the expert's policies. In this paper, we present rigorous generalization guarantees for imitation learning by leveraging the Probably Approximately Correct (PAC)-Bayes framework to provide upper bounds on the expected cost of policies in novel environments. We propose a two-stage training method where a latent policy distribution is first embedded with multi-modal expert behavior using a conditional variational autoencoder, and then "fine-tuned" in new training environments to explicitly optimize the generalization bound. We demonstrate strong generalization bounds and their tightness relative to empirical performance in simulation for (i) grasping diverse mugs, (ii) planar pushing with visual feedback, and (iii) vision-based indoor navigation, as well as through hardware experiments for the two manipulation tasks.      
### 49.Tunable THz Switch-Filter Based on Magneto-Plasmonic Graphene Nanodisk  [ :arrow_down: ](https://arxiv.org/pdf/2008.01906.pdf)
>  We propose and analyze a multifunctional THz device which can operate as a tunable switch and a filter. The device consists of a circular graphene nanodisk coupled to two nanoribbons oriented at $90^\circ$ to each other. The graphene elements are placed on a dielectric substrate. The nanodisk is magnetized by a DC magnetic field normal to its plane. The physical principle of the device is based on the propagation of surface plasmon-polariton waves in the graphene nanoribbons and excitation of dipole modes in the nanodisk. Numerical simulations show that 0.61T DC magnetic field provides transmission (regime ON) at the frequency 5.33 THz with the bandwidth 12.7$\%$ and filtering with the Q-factor equals to 7.8. At the central frequency, the insertion loss is around -2 dB and the reflection coefficient is -43 dB. The regime OFF can be achieved by means of switching DC magnetic field to zero value or by switching chemical potential of the nanodisk to zero with the ON/OFF ratio better than 20 dB. A small central frequency tuning by chemical potential is possible with a fixed DC magnetic field.      
### 50.Validation and Comparison of Instrumented Mouthguards for Measuring Head Kinematics and Assessing Brain Deformation in Football Impacts  [ :arrow_down: ](https://arxiv.org/pdf/2008.01903.pdf)
>  Because of the relatively rigid coupling between the upper dentition and the skull, instrumented mouthguards have been shown to be a viable way of measuring head impact kinematics for assisting in understanding the underlying biomechanics of concussions. This has led various companies and institutions to further develop instrumented mouthguards. However, their use as a research tool for understanding concussive impacts makes quantification of their accuracy critical, especially given the conflicting results from various recent studies. Here we present a study that uses a pneumatic impactor to deliver impacts characteristic to football to a Hybrid III headform, in order to validate and compare five of the most commonly used instrumented mouthguards. We found that all tested mouthguards gave accurate measurements for the peak angular acceleration (mean relative error, MRE &lt; 13%), the peak angular velocity (MRE &lt; 8%), brain injury criteria values (MRE &lt; 13%) and brain deformation (described as maximum principal strain and fiber strain, calculated by a convolutional neural network based brain model, MRE &lt; 9%). Finally, we found that the accuracy of the measurement varies with the impact locations yet is not sensitive to the impact velocity for the most part.      
### 51.Multimodality Biomedical Image Registration using Free Point Transformer Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.01885.pdf)
>  We describe a point-set registration algorithm based on a novel free point transformer (FPT) network, designed for points extracted from multimodal biomedical images for registration tasks, such as those frequently encountered in ultrasound-guided interventional procedures. FPT is constructed with a global feature extractor which accepts unordered source and target point-sets of variable size. The extracted features are conditioned by a shared multilayer perceptron point transformer module to predict a displacement vector for each source point, transforming it into the target space. The point transformer module assumes no vicinity or smoothness in predicting spatial transformation and, together with the global feature extractor, is trained in a data-driven fashion with an unsupervised loss function. In a multimodal registration task using prostate MR and sparsely acquired ultrasound images, FPT yields comparable or improved results over other rigid and non-rigid registration methods. This demonstrates the versatility of FPT to learn registration directly from real, clinical training data and to generalize to a challenging task, such as the interventional application presented.      
### 52.A Relearning Approach to Reinforcement Learning for Control of Smart Buildings  [ :arrow_down: ](https://arxiv.org/pdf/2008.01879.pdf)
>  This paper demonstrates that continual relearning of control policies using incremental deep reinforcement learning (RL) can improve policy learning for non-stationary processes. We demonstrate this approach for a data-driven 'smart building environment' that we use as a test-bed for developing HVAC controllers for reducing energy consumption of large buildings on our university campus. The non-stationarity in building operations and weather patterns makes it imperative to develop control strategies that are adaptive to changing conditions. On-policy RL algorithms, such as Proximal Policy Optimization (PPO) represent an approach for addressing this non-stationarity, but exploration on the actual system is not an option for safety-critical systems. As an alternative, we develop an incremental RL technique that simultaneously reduces building energy consumption without sacrificing overall comfort. We compare the performance of our incremental RL controller to that of a static RL controller that does not implement the relearning function. The performance of the static controller diminishes significantly over time, but the relearning controller adjusts to changing conditions while ensuring comfort and optimal energy performance.      
### 53.Statistical Analysis of Downlink Zero-Forcing Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2008.01875.pdf)
>  We analyze the mean and the variance of the useful signal and interference powers in a multi-cell network using zero-forcing beamforming (ZF-BF) with two beamformer normalization approaches. While the mean has been the main focus in earlier studies on ZF-BF, analysis of the variance has not been tackled. Our analysis provides a complete statistical study, sheds light on the importance of the variance by deriving closed-form expressions for the signals' two moments, and provides a practical use for these expressions; we use the gamma or lognormal distribution for the interference power to analytically calculate the outage.      
### 54.Detector tilt considerations in high-energy Bragg coherent diffraction imaging: a simulation study  [ :arrow_down: ](https://arxiv.org/pdf/2008.01843.pdf)
>  This paper addresses three-dimensional signal distortion and image reconstruction issues in x-ray Bragg coherent diffraction imaging (BCDI) in the event of a non-orthogonal orientation of the area detector with respect to the diffracted beam. Growing interest in novel BCDI adaptations at future fourth-generation synchrotron light sources (for example, at beam energies &gt;= 50 keV) has necessitated improvisations in the experimental configuration and the subsequent data analysis. One such possibly unavoidable improvisation that is envisioned in this paper and related to diffractometer stability is a photon counting area detector whose face is tilted away from the perpendicular to the Bragg-diffracted beam during acquisition of the coherent diffraction signal. Working within the context of high-energy BCDI we describe a likely circumstance in which one would require such a detector configuration. Using physically accurate diffraction simulations from synthetic scatterers in the presence of such tilted detectors, we analyze the general nature of the observed signal distortion qualitatively and quantitatively, and provide a prescription to correct for it during image reconstruction. Our simulations and reconstructions are based on an adaptation of the known theory of BCDI sampling geometry as well as recently developed geometry-aware projection-based methods of wavefield propagation. We demonstrate that such configurational modifications and their numerical remedies are crucial to realizing high-energy BCDI at synchrotron facilities and eventually paving the way for novel materials characterization experiments in the future.      
### 55.Physics-informed Tensor-train ConvLSTM for Volumetric Velocity Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2008.01798.pdf)
>  According to the National Academies, a weekly forecast of velocity, vertical structure, and duration of the Loop Current (LC) and its eddies is critical for understanding the oceanography and ecosystem, and for mitigating outcomes of anthropogenic and natural disasters in the Gulf of Mexico (GoM). However, this forecast is a challenging problem since the LC behaviour is dominated by long-range spatial connections across multiple timescales. In this paper, we extend spatiotemporal predictive learning, showing its effectiveness beyond video prediction, to a 4D model, i.e., a novel Physics-informed Tensor-train ConvLSTM (PITT-ConvLSTM) for temporal sequences of 3D geospatial data forecasting. Specifically, we propose 1) a novel 4D higher-order recurrent neural network with empirical orthogonal function analysis to capture the hidden uncorrelated patterns of each hierarchy, 2) a convolutional tensor-train decomposition to capture higher-order space-time correlations, and 3) to incorporate prior physic knowledge that is provided from domain experts by informing the learning in latent space. The advantage of our proposed method is clear: constrained by physical laws, it simultaneously learns good representations for frame dependencies (both short-term and long-term high-level dependency) and inter-hierarchical relations within each time frame. Experiments on geospatial data collected from the GoM demonstrate that PITT-ConvLSTM outperforms the state-of-the-art methods in forecasting the volumetric velocity of the LC and its eddies for a period of over one week.      
### 56.Deep Learning Based Early Diagnostics of Parkinsons Disease  [ :arrow_down: ](https://arxiv.org/pdf/2008.01792.pdf)
>  In the world, about 7 to 10 million elderly people are suffering from Parkinson's Disease (PD) disease. Parkinson's disease is a common neurological degenerative disease, and its clinical characteristics are Tremors, rigidity, bradykinesia, and decreased autonomy. Its clinical manifestations are very similar to Multiple System Atrophy (MSA) disorders. Studies have shown that patients with Parkinson's disease often reach an irreparable situation when diagnosed, so As Parkinson's disease can be distinguished from MSA disease and get an early diagnosis, people are constantly exploring new methods. With the advent of the era of big data, deep learning has made major breakthroughs in image recognition and classification. Therefore, this study proposes to use The deep learning method to realize the diagnosis of Parkinson's disease, multiple system atrophy, and healthy people. This data source is from Istanbul University Cerrahpasa Faculty of Medicine Hospital. The processing of the original magnetic resonance image (Magnetic Resonance Image, MRI) is guided by the doctor of Istanbul University Cerrahpasa Faculty of Medicine Hospital. The focus of this experiment is to improve the existing neural network so that it can obtain good results in medical image recognition and diagnosis. An improved algorithm was proposed based on the pathological characteristics of Parkinson's disease, and good experimental results were obtained by comparing indicators such as model loss and accuracy.      
### 57.An artificial intelligence system for predicting the deterioration of COVID-19 patients in the emergency department  [ :arrow_down: ](https://arxiv.org/pdf/2008.01774.pdf)
>  During the COVID-19 pandemic, rapid and accurate triage of patients at the emergency department is critical to inform decision-making. We propose a data-driven approach for automatic prediction of deterioration risk using a deep neural network that learns from chest X-ray images, and a gradient boosting model that learns from routine clinical variables. Our AI prognosis system, trained using data from 3,661 patients, achieves an AUC of 0.786 (95% CI: 0.742-0.827) when predicting deterioration within 96 hours. The deep neural network extracts informative areas of chest X-ray images to assist clinicians in interpreting the predictions, and performs comparably to two radiologists in a reader study. In order to verify performance in a real clinical setting, we silently deployed a preliminary version of the deep neural network at NYU Langone Health during the first wave of the pandemic, which produced accurate predictions in real-time. In summary, our findings demonstrate the potential of the proposed system for assisting front-line physicians in the triage of COVID-19 patients.      
### 58.1-Bit Compressive Sensing via Approximate Message Passing with Built-in Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2007.07679.pdf)
>  1-bit compressive sensing aims to recover sparse signals from quantized 1-bit measurements. Designing efficient approaches that could handle noisy 1-bit measurements is important in a variety of applications. In this paper we use the approximate message passing (AMP) to achieve this goal due to its high computational efficiency and state-of-the-art performance. In AMP the signal of interest is assumed to follow some prior distribution, and its posterior distribution can be computed and used to recover the signal. In practice, the parameters of the prior distributions are often unknown and need to be estimated. Previous works tried to find the parameters that maximize either the measurement likelihood or the Bethe free entropy, which becomes increasingly difficult to solve in the case of complicated probability models. Here we propose to treat the parameters as unknown variables and compute their posteriors via AMP as well, so that the parameters and the signal can be recovered jointly. This leads to a much simpler way to perform parameter estimation compared to previous methods and enables us to work with noisy 1-bit measurements. We further extend the proposed approach to the general quantization noise model that outputs multi-bit measurements. Experimental results show that the proposed approach generally perform much better than the other state-of-the-art methods in the zero-noise and moderate-noise regimes, and outperforms them in most of the cases in the high-noise regime.      
### 59.Reachability Analysis of Large Linear Systems with Uncertain Inputs in the Krylov Subspace  [ :arrow_down: ](https://arxiv.org/pdf/1712.00369.pdf)
>  One often wishes for the ability to formally analyze large-scale systems---typically, however, one can either formally analyze a rather small system or informally analyze a large-scale system. This work tries to further close this performance gap for reachability analysis of linear systems. Reachability analysis can capture the whole set of possible solutions of a dynamic system and is thus used to prove that unsafe states are never reached; this requires full consideration of arbitrarily varying uncertain inputs, since sensor noise or disturbances usually do not follow any patterns. We use Krylov methods in this work to compute reachable sets for large-scale linear systems. While Krylov methods have been used before in reachability analysis, we overcome the previous limitation that inputs must be (piecewise) constant. As a result, we can compute reachable sets of systems with several thousand state variables for bounded, but arbitrarily varying inputs.      
