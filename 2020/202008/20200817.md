# ArXiv eess --Mon, 17 Aug 2020
### 1.Data augmentation and loss normalization for deep noise suppression  [ :arrow_down: ](https://arxiv.org/pdf/2008.06412.pdf)
>  Speech enhancement using neural networks is recently receiving large attention in research and being integrated in commercial devices and applications. In this work, we investigate data augmentation techniques for supervised deep learning-based speech enhancement. We show that not only augmenting SNR values to a broader range and a continuous distribution helps to regularize training, but also augmenting the spectral and dynamic level diversity. However, to not degrade training by level augmentation, we propose a modification to signal-based loss functions by applying sequence level normalization. We show in experiments that this normalization overcomes the degradation caused by training on sequences with imbalanced signal levels, when using a level-dependent loss function.      
### 2.Pulsed Waveforms and Intermittently Nonlinear Filtering in Synthesis of Low-SNR and Covert Communications  [ :arrow_down: ](https://arxiv.org/pdf/2008.06390.pdf)
>  In traditional spread-spectrum techniques, a wideband transmit signal is obtained by modulating a wideband carrier by a narrowband signal containing a relatively low-rate message. In the receiver, the respective demodulation/despreading restores the information-carrying narrowband signal. In this paper, we introduce an alternative approach, where the low-rate information is encoded directly into a wideband waveform of a given bandwidth, without physical "spreading" of the carrier's frequency. The main advantages of this approach lie in extended options for encoding the information, and in retaining a reversible control over the temporal and amplitude structures of the modulating wideband waveforms. Significant "excess bandwidth" (over that needed to carry the information) enables us to use allpass filters to manage statistical properties and time-domain appearances of these waveforms without changing their spectral composition. For example, a mixture of transmitted waveforms can be shaped as a low-crest-factor signal (e.g. to reduce the burden on the power amplifier), and/or made statistically indistinguishable from Gaussian noise (e.g. for covert transmissions and physical layer steganography), while the selected components of the received waveform can be transformed into high-crest-factor pulse trains suitable for multiplexing and/or low-SNR communications. Further, control over the temporal and amplitude structures of wideband waveforms carrying low-rate information enables effective use of nonlinear filtering techniques. Such techniques can be employed for robust real-time asynchronous extraction of the information, as well as for separation of wideband signal components with identical spectral content from each other. This can facilitate development of a large variety of low-SNR and covert communication configurations.      
### 3.A Centralized and Scalable Uplink Power Control Algorithm in Low SINR: A Case Study for UAV Communications  [ :arrow_down: ](https://arxiv.org/pdf/2008.06369.pdf)
>  Interference management through power control is essential to optimize the system capacity. With the introduction of aerial user equipments in cellular networks, resulting in an increase of line of sight links, power control is becoming more and more vital to enable the (uplink) high-throughput data streaming and protect the users on the ground. The investigation in [1] shows that in the high signal-to-interference-plus-noise (SINR) regime, geometrical programming (GP) can be used to efficiently and reliably solve the problem. In the low SINR regime, a series of GPs are solved by condensation. However, the condensation method proposed in [1] is non-scalable, which hinders its application to a large-scale network, e.g. a densified network, where many more cells could be jointly optimized. In this communication, by transforming the original problem into a standard form introducing auxiliary variables, a new condensation method is proposed. Its complexity linearly increases with the number of links increasing, which makes the power control practically solvable for both small- and large-scale networks. A case study for the up-link UAV communications in cellular networks is performed using the proposed algorithm.      
### 4.Semi-supervised learning using teacher-student models for vocal melody extraction  [ :arrow_down: ](https://arxiv.org/pdf/2008.06358.pdf)
>  The lack of labeled data is a major obstacle in many music information retrieval tasks such as melody extraction, where labeling is extremely laborious or costly. Semi-supervised learning (SSL) provides a solution to alleviate the issue by leveraging a large amount of unlabeled data. In this paper, we propose an SSL method using teacher-student models for vocal melody extraction. The teacher model is pre-trained with labeled data and guides the student model to make identical predictions given unlabeled input in a self-training setting. We examine three setups of teacher-student models with different data augmentation schemes and loss functions. Also, considering the scarcity of labeled data in the test phase, we artificially generate large-scale testing data with pitch labels from unlabeled data using an analysis-synthesis method. The results show that the SSL method significantly increases the performance against supervised learning only and the improvement depends on the teacher-student models, the size of unlabeled data, the number of self-training iterations, and other training details. We also find that it is essential to ensure that the unlabeled audio has vocal parts. Finally, we show that the proposed SSL method enables a baseline convolutional recurrent neural network model to achieve performance comparable to state-of-the-arts.      
### 5.A study of uncompensated latency in ADS-B reports  [ :arrow_down: ](https://arxiv.org/pdf/2008.06352.pdf)
>  The total latency (TL) in ADS-B is the difference between the true TOA of the original position measured by the onboard GPS and the TOA in the ADS-B report at the GS. We processed the ADS-B reports extracted from live-data recorded at the Chicago to produce the results in this report. We observed that 99% of aircraft in our data set were broadcasting in non-UTC coupled mode. For these aircraft, STARS exclusively used ADS-B reports to update tracks when available. The calculated ULs for the non-UTC coupled ADS-B reports were well within the budget (-200 ms to +400 ms) allowed by the DO-260B MOPS. Moreover, the mean latencies from both methods were less than +/-50 ms. The latencies obtained from the MTPES method were observed to be in close agreement with the means from the ATPE method. About 1% of aircraft were broadcasting in UTC coupled mode. The reports from these aircraft were identified to have a non-compliant link version, causing STARS to reject these reports for tracking. Moreover, the reported position and TOA were found to be unsynchronized, which conflicts with the MOPS requirement for UTC coupled operation.      
### 6.Generalized Coordinated Multipoint Framework for 5G and Beyond  [ :arrow_down: ](https://arxiv.org/pdf/2008.06343.pdf)
>  The characteristic feature of 5G is the diversity of its services for different user needs. However, the requirements for these services are competing in nature, which impresses the necessity of a coordinated and flexible network architecture. Although coordinated multipoint (CoMP) systems were primarily proposed to improve the cell edge performance in 4G, their collaborative nature can be leveraged to support the diverse requirements and enabling technologies of 5G and beyond networks. To this end, we propose generalization of CoMP to a proactive and efficient resource utilization framework capable of supporting different user requirements such as reliability, latency, throughput, and security while considering network constraints. This article elaborates on the multiple aspects, inputs, and outputs of the generalized CoMP (GCoMP) framework. Apart from user requirements, the GCoMP decision mechanism also considers the CoMP scenario and network architecture to decide upon outputs such as CoMP technique or appropriate coordinating clusters. To enable easier understanding of the concept, popular use cases, such as vehicle-to-everything (V2X) communication and eHealth, are studied. Additionally, interesting challenges and open areas in GCoMP are discussed.      
### 7.Integrating uncertainty in deep neural networks for MRI based stroke analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.06332.pdf)
>  At present, the majority of the proposed Deep Learning (DL) methods provide point predictions without quantifying the models uncertainty. However, a quantification of the reliability of automated image analysis is essential, in particular in medicine when physicians rely on the results for making critical treatment decisions. In this work, we provide an entire framework to diagnose ischemic stroke patients incorporating Bayesian uncertainty into the analysis procedure. We present a Bayesian Convolutional Neural Network (CNN) yielding a probability for a stroke lesion on 2D Magnetic Resonance (MR) images with corresponding uncertainty information about the reliability of the prediction. For patient-level diagnoses, different aggregation methods are proposed and evaluated, which combine the single image-level predictions. Those methods take advantage of the uncertainty in image predictions and report model uncertainty at the patient-level. In a cohort of 511 patients, our Bayesian CNN achieved an accuracy of 95.33% at the image-level representing a significant improvement of 2% over a non-Bayesian counterpart. The best patient aggregation method yielded 95.89% of accuracy. Integrating uncertainty information about image predictions in aggregation models resulted in higher uncertainty measures to false patient classifications, which enabled to filter critical patient diagnoses that are supposed to be closer examined by a medical doctor. We therefore recommend using Bayesian approaches not only for improved image-level prediction and uncertainty estimation but also for the detection of uncertain aggregations at the patient-level.      
### 8.Automated detection and quantification of COVID-19 airspace disease on chest radiographs: A novel approach achieving radiologist-level performance using a CNN trained on digital reconstructed radiographs (DRRs) from CT-based ground-truth  [ :arrow_down: ](https://arxiv.org/pdf/2008.06330.pdf)
>  Purpose: To leverage volumetric quantification of airspace disease (AD) derived from a superior modality (CT) serving as ground truth, projected onto digitally reconstructed radiographs (DRRs) to: 1) train a convolutional neural network to quantify airspace disease on paired CXRs; and 2) compare the DRR-trained CNN to expert human readers in the CXR evaluation of patients with confirmed COVID-19. <br>Materials and Methods: We retrospectively selected a cohort of 86 COVID-19 patients (with positive RT-PCR), from March-May 2020 at a tertiary hospital in the northeastern USA, who underwent chest CT and CXR within 48 hrs. The ground truth volumetric percentage of COVID-19 related AD (POv) was established by manual AD segmentation on CT. The resulting 3D masks were projected into 2D anterior-posterior digitally reconstructed radiographs (DRR) to compute area-based AD percentage (POa). A convolutional neural network (CNN) was trained with DRR images generated from a larger-scale CT dataset of COVID-19 and non-COVID-19 patients, automatically segmenting lungs, AD and quantifying POa on CXR. CNN POa results were compared to POa quantified on CXR by two expert readers and to the POv ground-truth, by computing correlations and mean absolute errors. <br>Results: Bootstrap mean absolute error (MAE) and correlations between POa and POv were 11.98% [11.05%-12.47%] and 0.77 [0.70-0.82] for average of expert readers, and 9.56%-9.78% [8.83%-10.22%] and 0.78-0.81 [0.73-0.85] for the CNN, respectively. <br>Conclusion: Our CNN trained with DRR using CT-derived airspace quantification achieved expert radiologist level of accuracy in the quantification of airspace disease on CXR, in patients with positive RT-PCR for COVID-19.      
### 9.Resilient Abstraction-Based Controller Design  [ :arrow_down: ](https://arxiv.org/pdf/2008.06315.pdf)
>  We consider the computation of resilient controllers for perturbed non-linear dynamical systems w.r.t. linear-time temporal logic specifications. We address this problem through the paradigm of Abstraction-Based Controller Design (ABCD) where a finite state abstraction of the perturbed system dynamics is constructed and utilized for controller synthesis. In this context, our contribution is twofold: (I) We construct abstractions which model the impact of occasional high disturbance spikes on the system via so called disturbance edges. (II) We show that the application of resilient reactive synthesis techniques to these abstract models results in closed loop systems which are optimally resilient to these occasional high disturbance spikes. We have implemented this resilient ABCD workflow on top of SCOTS and showcase our method through multiple robot planning examples.      
### 10.Secret Key Generation for Intelligent Reflecting Surface Assisted Wireless Communication Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.06304.pdf)
>  We propose and analyze secret key generation using intelligent reflecting surface (IRS) assisted wireless communication networks. To this end, we first formulate the minimum achievable secret key capacity for an IRS acting as a passive beamformer in the presence of multiple eavesdroppers. Next, we develop an optimization framework for the IRS reflecting coefficients based on the secret key capacity lower bound. To derive a tractable and efficient solution, we design and analyze a semidefinite relaxation (SDR) and successive convex approximation (SCA) based algorithm for the proposed optimization. Simulation results show that employing our IRS-based algorithm can significantly improve the secret key generation capacity for a wide-range of wireless channel parameters.      
### 11.Homotopic Gradients of Generative Density Priors for MR Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2008.06284.pdf)
>  Deep learning, particularly the generative model, has demonstrated tremendous potential to significantly speed up image reconstruction with reduced measurements recently. Rather than the existing generative models that often optimize the density priors, in this work, by taking advantage of the denoising score matching, homotopic gradients of generative density priors (HGGDP) are proposed for magnetic resonance imaging (MRI) reconstruction. More precisely, to tackle the low-dimensional manifold and low data density region issues in generative density prior, we estimate the target gradients in higher-dimensional space. We train a more powerful noise conditional score network by forming high-dimensional tensor as the network input at the training phase. More artificial noise is also injected in the embedding space. At the reconstruction stage, a homotopy method is employed to pursue the density prior, such as to boost the reconstruction performance. Experiment results imply the remarkable performance of HGGDP in terms of high reconstruction accuracy; only 10% of the k-space data can still generate images of high quality as effectively as standard MRI reconstruction with the fully sampled data.      
### 12.The Impact of Label Noise on a Music Tagger  [ :arrow_down: ](https://arxiv.org/pdf/2008.06273.pdf)
>  We explore how much can be learned from noisy labels in audio music tagging. Our experiments show that carefully annotated labels result in highest figures of merit, but even high amounts of noisy labels contain enough information for successful learning. Artificial corruption of curated data allows us to quantize this contribution of noisy labels.      
### 13.Simulation Comparisons of Vehicle-based and Phase-based Traffic Control for Autonomous Vehicles at Isolated Intersections  [ :arrow_down: ](https://arxiv.org/pdf/2008.06256.pdf)
>  With the advent of autonomous driving technologies, traffic control at intersections is expected to experience revolutionary changes. Various novel intersection control methods have been proposed in the existing literature, and they can be roughly divided into two categories: vehicle-based traffic control and phase-based traffic control. Phase-based traffic control can be treated as updated versions of the current intersection signal control with the incorporation of the performance of autonomous vehicle functions. Meanwhile, vehicle-based traffic control utilizes some brand-new methods, mostly in real-time fashion, to organize traffic at intersections for safe and efficient vehicle passages. However, to date, no systematic comparison between these two control categories has been performed to suggest their advantages and disadvantages. This paper conducts a series of numerical simulations under various traffic scenarios to perform a fair comparison of their performances. Specifically, we allow trajectory adjustments of incoming vehicles under phasebased traffic control, while for its vehicle-based counterpart, we implement two strategies, i.e., the first-come-first-serve strategy and the conflict-point based rolling-horizon optimization strategy. Overall, the simulation results show that vehicle-based traffic control generally incurs a negligible delay when traffic demand is low but lead to an excessive queuing time as the traffic volume becomes high. However, performance of vehicle-based traffic control may benefit from reduction in conflicting vehicle pairs. We also discovered that when autonomous driving technologies are not mature, the advantages of phase-based traffic control are much more distinct.      
### 14.Performance Evaluation of Reconfigurable Intelligent Surface Assisted D-band Wireless Communication  [ :arrow_down: ](https://arxiv.org/pdf/2008.06227.pdf)
>  In the recent years, the proliferation of wireless data traffic has led the scientific community to explore the use of higher unallocated frequency bands, such as the millimeter wave and terahertz (0.1-10 THz) bands. However, they are prone to blockages from obstacles laid in the transceiver path. To address this, in this work, the use of a reconfigurable-intelligent-surface (RIS) to restore the link between a transmitter (TX) and a receiver (RX), operating in the D-band (110-170 GHz) is investigated. The system performance is evaluated in terms of pathgain and capacity considering the RIS design parameters, the TX/RX-RIS distance and the elevation angles from the center of the RIS to the transceivers.      
### 15.Adaptable Multi-Domain Language Model for Transformer ASR  [ :arrow_down: ](https://arxiv.org/pdf/2008.06208.pdf)
>  We propose an adapter based multi-domain Transformer based language model (LM) for Transformer ASR. The model consists of a big size common LM and small size adapters. The model can perform multi-domain adaptation with only the small size adapters and its related layers. The proposed model can reuse the full fine-tuned LM which is fine-tuned using all layers of an original model. The proposed LM can be expanded to new domains by adding about 2% of parameters for a first domain and 13% parameters for after second domain. The proposed model is also effective in reducing the model maintenance cost because it is possible to omit the costly and time-consuming common LM pre-training process. Using proposed adapter based approach, we observed that a general LM with adapter can outperform a dedicated music domain LM in terms of word error rate (WER).      
### 16.Generalized Radio Environment Monitoring for Next Generation Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.06203.pdf)
>  Radio environment mapping (REM) has long been considered an enabler of cognitive radios (CRs). However, the limitations of radios effectively confined the application of REM to spectrum sensing and interference mapping. With the advent of more capable software-defined radio and advanced networks, the idea of a truly cognitive communication system can be realized. To this effect, we propose the generalization of REM concept to include all aspects of the radio environment. This paper describes a generalized REM framework comprising of different information sources, sensing methods, sensing modes, and mapping techniques to enable the realization of more reliable, secure, efficient, and faster future wireless networks.      
### 17.Leveraging Weakly-hard Constraints for Improving System Fault Tolerance with Functional and Timing Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2008.06192.pdf)
>  Many safety-critical real-time systems operate under harsh environment and are subject to soft errors caused by transient or intermittent faults. It is critical and yet often very challenging to apply fault tolerance techniques in these systems, due to their resource limitations and stringent constraints on timing and functionality. In this work, we leverage the concept of weakly-hard constraints, which allows task deadline misses in a bounded manner, to improve system's capability to accommodate fault tolerance techniques while ensuring timing and functional correctness. In particular, we 1) quantitatively measure control cost under different deadline hit/miss scenarios and identify weak-hard constraints that guarantee control stability, 2) employ typical worst-case analysis (TWCA) to bound the number of deadline misses and approximate system control cost, 3) develop an event-based simulation method to check the task execution pattern and evaluate system control cost for any given solution and 4) develop a meta-heuristic algorithm that consists of heuristic methods and a simulated annealing procedure to explore the design space. Our experiments on an industrial case study and a set of synthetic examples demonstrate the effectiveness of our approach.      
### 18.Weakly-supervised Learning for Single-step Quantitative Susceptibility Mapping  [ :arrow_down: ](https://arxiv.org/pdf/2008.06187.pdf)
>  Quantitative susceptibility mapping (QSM) utilizes MRI phase information to estimate tissue magnetic susceptibility. The generation of QSM requires solving ill-posed background field removal (BFR) and field-to-source inversion problems. Because current QSM techniques struggle to generate reliable QSM in clinical contexts, QSM clinical translation is greatly hindered. Recently, deep learning (DL) approaches for QSM reconstruction have shown impressive performance. Due to inherent non-existent ground-truth, these DL techniques use either calculation of susceptibility through multiple orientation sampling (COSMOS) maps or synthetic data for training, which are constrained by the availability and accuracy of COSMOS maps or domain shift when training data and testing data have different domains. To address these limitations, we propose a weakly-supervised single-step QSM reconstruction method, denoted as wTFI, to directly reconstruct QSM from the total field without BFR. wTFI uses the BFR method RESHARP local fields as supervision to perform a multi-task learning of local tissue fields and QSM, and is capable of recovering magnetic susceptibility estimates near the edges of the brain where are eroded in RESHARP and realize whole brain QSM estimation. Quantitative and qualitative evaluation shows that wTFI can generate high-quality local field and susceptibility maps in a variety of neuroimaging contexts.      
### 19.Online Speaker Adaptation for WaveNet-based Neural Vocoders  [ :arrow_down: ](https://arxiv.org/pdf/2008.06182.pdf)
>  In this paper, we propose an online speaker adaptation method for WaveNet-based neural vocoders in order to improve their performance on speaker-independent waveform generation. In this method, a speaker encoder is first constructed using a large speaker-verification dataset which can extract a speaker embedding vector from an utterance pronounced by an arbitrary speaker. At the training stage, a speaker-aware WaveNet vocoder is then built using a multi-speaker dataset which adopts both acoustic feature sequences and speaker embedding vectors as <a class="link-external link-http" href="http://conditions.At" rel="external noopener nofollow">this http URL</a> the generation stage, we first feed the acoustic feature sequence from a test speaker into the speaker encoder to obtain the speaker embedding vector of the utterance. Then, both the speaker embedding vector and acoustic features pass the speaker-aware WaveNet vocoder to reconstruct speech waveforms. Experimental results demonstrate that our method can achieve a better objective and subjective performance on reconstructing waveforms of unseen speakers than the conventional speaker-independent WaveNet vocoder.      
### 20.Unsupervised Image Restoration Using Partially Linear Denoisers  [ :arrow_down: ](https://arxiv.org/pdf/2008.06164.pdf)
>  Deep neural network based methods are the state of the art in various image restoration problems. Standard supervised learning frameworks require a set of noisy measurement and clean image pairs for which a distance between the output of the restoration model and the ground truth, clean images is minimized. The ground truth images, however, are often unavailable or very expensive to acquire in real-world applications. We circumvent this problem by proposing a class of structured denoisers that can be decomposed as the sum of a nonlinear image-dependent mapping, a linear noise-dependent term and a small residual term. We show that these denoisers can be trained with only noisy images under the condition that the noise has zero mean and known variance. The exact distribution of the noise, however, is not assumed to be known. We show the superiority of our approach for image denoising, and demonstrate its extension to solving other restoration problems such as blind deblurring where the ground truth is not available. Our method outperforms some recent unsupervised and self-supervised deep denoising models that do not require clean images for their training. For blind deblurring problems, the method, using only one noisy and blurry observation per image, reaches a quality not far away from its fully supervised counterparts on a benchmark dataset.      
### 21.Energy-Efficient Control Adaptation with Safety Guarantees for Learning-Enabled Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.06162.pdf)
>  Neural networks have been increasingly applied for control in learning-enabled cyber-physical systems (LE-CPSs) and demonstrated great promises in improving system performance and efficiency, as well as reducing the need for complex physical models. However, the lack of safety guarantees for such neural network based controllers has significantly impeded their adoption in safety-critical CPSs. In this work, we propose a controller adaptation approach that automatically switches among multiple controllers, including neural network controllers, to guarantee system safety and improve energy efficiency. Our approach includes two key components based on formal methods and machine learning. First, we approximate each controller with a Bernstein-polynomial based hybrid system model under bounded disturbance, and compute a safe invariant set for each controller based on its corresponding hybrid system. Intuitively, the invariant set of a controller defines the state space where the system can always remain safe under its control. The union of the controllers' invariants sets then define a safe adaptation space that is larger than (or equal to) that of each controller. Second, we develop a deep reinforcement learning method to learn a controller switching strategy for reducing the control/actuation energy cost, while with the help of a safety guard rule, ensuring that the system stays within the safe space. Experiments on a linear adaptive cruise control system and a non-linear Van der Pol's oscillator demonstrate the effectiveness of our approach on energy saving and safety enhancement.      
### 22.Interpretation of Brain Morphology in Association to Alzheimer's Disease Dementia Classification Using Graph Convolutional Networks on Triangulated Meshes  [ :arrow_down: ](https://arxiv.org/pdf/2008.06151.pdf)
>  We propose a mesh-based technique to aid in the classification of Alzheimer's disease dementia (ADD) using mesh representations of the cortex and subcortical structures. Deep learning methods for classification tasks that utilize structural neuroimaging often require extensive learning parameters to optimize. Frequently, these approaches for automated medical diagnosis also lack visual interpretability for areas in the brain involved in making a diagnosis. This work: (a) analyzes brain shape using surface information of the cortex and subcortical structures, (b) proposes a residual learning framework for state-of-the-art graph convolutional networks which offer a significant reduction in learnable parameters, and (c) offers visual interpretability of the network via class-specific gradient information that localizes important regions of interest in our inputs. With our proposed method leveraging the use of cortical and subcortical surface information, we outperform other machine learning methods with a 96.35% testing accuracy for the ADD vs. healthy control problem. We confirm the validity of our model by observing its performance in a 25-trial Monte Carlo cross-validation. The generated visualization maps in our study show correspondences with current knowledge regarding the structural localization of pathological changes in the brain associated to dementia of the Alzheimer's type.      
### 23.End-to-End Trainable Self-Attentive Shallow Network for Text-Independent Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2008.06146.pdf)
>  Generalized end-to-end (GE2E) model is widely used in speaker verification (SV) fields due to its expandability and generality regardless of specific languages. However, the long-short term memory (LSTM) based on GE2E has two limitations: First, the embedding of GE2E suffers from vanishing gradient, which leads to performance degradation for very long input sequences. Secondly, utterances are not represented as a properly fixed dimensional vector. In this paper, to overcome issues mentioned above, we propose a novel framework for SV, end-to-end trainable self-attentive shallow network (SASN), incorporating a time-delay neural network (TDNN) and a self-attentive pooling mechanism based on the self-attentive x-vector system during an utterance embedding phase. We demonstrate that the proposed model is highly efficient, and provides more accurate speaker verification than GE2E. For VCTK dataset, with just less than half the size of GE2E, the proposed model showed significant performance improvement over GE2E of about 63%, 67%, and 85% in EER (Equal error rate), DCF (Detection cost function), and AUC (Area under the curve), respectively. Notably, when the input length becomes longer, the DCF score improvement of the proposed model is about 17 times greater than that of GE2E.      
### 24.Landmark detection in Cardiac Magnetic Resonance Imaging Using A Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.06142.pdf)
>  Purpose: To develop a convolutional neural network (CNN) solution for robust landmark detection in cardiac MR images. <br>Methods: This retrospective study included cine, LGE and T1 mapping scans from two hospitals. The training set included 2,329 patients and 34,019 images. A hold-out test set included 531 patients and 7,723 images. CNN models were developed to detect two mitral valve plane and apical points on long-axis (LAX) images. On short-axis (SAX) images, anterior and posterior RV insertion points and LV center were detected. Model outputs were compared to manual labels by two operators for accuracy with a t-test for statistical significance. The trained model was deployed to MR scanners. <br>Results: For the LAX images, success detection was 99.8% for cine, 99.4% for LGE. For the SAX, success rate was 96.6%, 97.6% and 98.9% for cine, LGE and T1-mapping. The L2 distances between model and manual labels were 2 to 3.5 mm, indicating close agreement between model landmarks to manual labels. No significant differences were found for the anterior RV insertion angle and LV length by the models and operators for all views and imaging sequences. Model inference on MR scanner took 610ms/5.6s on GPU/CPU, respectively, for a typical cardiac cine series. <br>Conclusions: This study developed, validated and deployed a CNN solution for robust landmark detection in both long and short-axis CMR images for cine, LGE and T1 mapping sequences, with the accuracy comparable to the inter-operator variation.      
### 25.A Sum-of-Squares-Based Procedure to Approximate the Pontryagin Difference of Semialgebraic Sets  [ :arrow_down: ](https://arxiv.org/pdf/2008.06126.pdf)
>  The P-difference between two sets $\mathcal{A}$ and $\mathcal{B}$ is the set of all points, $\mathcal{C}$, such that the addition of $\mathcal{B}$ to any of the points in $\mathcal{C}$ is contained in $\mathcal{A}$. Such a set difference plays an important role in robust model predictive control and in set-theoretic control. In the paper we demonstrate that an inner approximation of the P-difference between two semialgebraic sets can be computed using the Sums of Squares Programming, and we illustrate the procedure using several computational examples.      
### 26.LSTM Acoustic Models Learn to Align and Pronounce with Graphemes  [ :arrow_down: ](https://arxiv.org/pdf/2008.06121.pdf)
>  Automated speech recognition coverage of the world's languages continues to expand. However, standard phoneme based systems require handcrafted lexicons that are difficult and expensive to obtain. To address this problem, we propose a training methodology for a grapheme-based speech recognizer that can be trained in a purely data-driven fashion. Built with LSTM networks and trained with the cross-entropy loss, the grapheme-output acoustic models we study are also extremely practical for real-world applications as they can be decoded with conventional ASR stack components such as language models and FST decoders, and produce good quality audio-to-grapheme alignments that are useful in many speech applications. We show that the grapheme models are competitive in WER with their phoneme-output counterparts when trained on large datasets, with the advantage that grapheme models do not require explicit linguistic knowledge as an input. We further compare the alignments generated by the phoneme and grapheme models to demonstrate the quality of the pronunciations learnt by them using four Indian languages that vary linguistically in spoken and written forms.      
### 27.Restless bandits: indexability and computation of Whittle index  [ :arrow_down: ](https://arxiv.org/pdf/2008.06111.pdf)
>  Restless bandits are a class of sequential resource allocation problems concerned with allocating one or more resources among several alternative processes where the evolution of the process depends on the resource allocated to them. Such models capture the fundamental trade-offs between exploration and exploitation. In 1988, Whittle developed an index heuristic for restless bandit problems which has emerged as a popular solution approach due to its simplicity and strong empirical performance. The Whittle index heuristic is applicable if the model satisfies a technical condition known as indexability. In this paper, we present two general sufficient conditions for indexability and identify simpler to verify refinements of these conditions. We then present a general algorithm to compute Whittle index for indexable restless bandits. Finally, we present a detailed numerical study which affirms the strong performance of the Whittle index heuristic.      
### 28.A Technical Overview of AV1  [ :arrow_down: ](https://arxiv.org/pdf/2008.06091.pdf)
>  The AV1 video compression format is developed by the Alliance for Open Media consortium. It achieves more than 30% reduction in bit-rate compared to its predecessor VP9 for the same decoded video quality. This paper provides a technical overview of the AV1 codec design that enables the compression performance gains with considerations for hardware feasibility.      
### 29.Reconfigurable and Intelligent Ultra-Wideband Angular Sensing: Prototype Design and Validation  [ :arrow_down: ](https://arxiv.org/pdf/2008.06085.pdf)
>  The emergence of beyond-licensed spectrum sharing in FR1 (0.45-6 GHz) and FR2 (24 - 52 GHz) along with the multi-antenna narrow-beam based directional transmissions demand a wideband spectrum sensing in temporal as well as spatial domains. We referred to it as ultra-wideband angular spectrum sensing (UWAS), and it consists of digitization followed by characterization of the wideband spectrum. In this paper, we design and develop state-of-the-art UWAS prototype using USRPs and LabVIEW NXG for the validation in the real-radio environment. Since 5G is expected to co-exist with LTE, the transmitter generates the multi-directional multi-user wideband traffic via LTE specific single carrier frequency division multiple access (SC-FDMA) approach. At the receiver, the first step of wideband spectrum digitization is accomplished using a novel approach of integrating sparse antenna-array with reconfigurable sub-Nyquist sampling (SNS). The reconfigurable SNS allows the digitization of non-contiguous spectrum via low-rate analog-to-digital converters, but it needs intelligence to choose the frequency bands for digitization. We explore the multi-play multi-armed bandit based learning algorithm to embed intelligence. Compared to previous works, the proposed characterization (frequency band status and direction-of-arrival estimation) approach does not need prior knowledge of received signal distribution. The detailed experimental results for various spectrum statistics, power gains and antenna array arrangements along with lower complexity validate the functional correctness, superiority and feasibility of the proposed UWAS over state-of-the-art approaches.      
### 30.MIXCAPS: A Capsule Network-based Mixture of Experts for Lung Nodule Malignancy Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2008.06072.pdf)
>  Lung diseases including infections such as Pneumonia, Tuberculosis, and novel Coronavirus (COVID-19), together with Lung Cancer are significantly widespread and are, typically, considered life threatening. In particular, lung cancer is among the most common and deadliest cancers with a low 5-year survival rate. Timely diagnosis of lung cancer is, therefore, of paramount importance as it can save countless lives. In this regard, deep learning radiomics solutions have the promise of extracting the most useful features on their own in an end-to-end fashion without having access to the annotated boundaries. Among different deep learning models, Capsule Networks are proposed to overcome shortcomings of the Convolutional Neural Networks (CNN) such as their inability to recognize detailed spatial relations. Capsule networks have so far shown satisfying performance in medical imaging problems. Capitalizing on their success, in this study, we propose a novel capsule network-based mixture of experts, referred to as the MIXCAPS. The proposed MIXCAPS architecture takes advantage of not only the capsule network's capabilities to handle small datasets, but also automatically splitting dataset through a convolutional gating network. MIXCAPS enables capsule network experts to specialize on different subsets of the data. Our results show that MIXCAPS outperforms a single capsule network and a mixture of CNNs, with an accuracy of 92.88%, sensitivity of 93.2%, specificity of 92.3% and area under the curve of 0.963. Our experiments also show that there is a relation between the gate outputs and a couple of hand-crafted features, illustrating explainable nature of the proposed MIXCAPS. To further evaluate generalization capabilities of the proposed MIXCAPS architecture, additional experiments on a brain tumor dataset are performed showing potentials of MIXCAPS for detection of tumors related to other organs.      
### 31.Machine learning for COVID-19 detection and prognostication using chest radiographs and CT scans: a systematic methodological review  [ :arrow_down: ](https://arxiv.org/pdf/2008.06388.pdf)
>  Background: Machine learning methods offer great potential for fast and accurate detection and prognostication of COVID-19 from standard-of-care chest radiographs (CXR) and computed tomography (CT) images. In this systematic review we critically evaluate the machine learning methodologies employed in the rapidly growing literature. <br>Methods: In this systematic review we reviewed EMBASE via OVID, MEDLINE via PubMed, bioRxiv, medRxiv and arXiv for published papers and preprints uploaded from Jan 1, 2020 to June 24, 2020. Studies which consider machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images were included. A methodology quality review of each paper was performed against established benchmarks to ensure the review focusses only on high-quality reproducible papers. This study is registered with PROSPERO [CRD42020188887]. <br>Interpretation: Our review finds that none of the developed models discussed are of potential clinical use due to methodological flaws and underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. Typically, we find that the documentation of a model's development is not sufficient to make the results reproducible and therefore of 168 candidate papers only 29 are deemed to be reproducible and subsequently considered in this review. We therefore encourage authors to use established machine learning checklists to ensure sufficient documentation is made available, and to follow the PROBAST (prediction model risk of bias assessment tool) framework to determine the underlying biases in their model development process and to mitigate these where possible. This is key to safe clinical implementation which is urgently needed.      
### 32.Survey of XAI in digital pathology  [ :arrow_down: ](https://arxiv.org/pdf/2008.06353.pdf)
>  Artificial intelligence (AI) has shown great promise for diagnostic imaging assessments. However, the application of AI to support medical diagnostics in clinical routine comes with many challenges. The algorithms should have high prediction accuracy but also be transparent, understandable and reliable. Thus, explainable artificial intelligence (XAI) is highly relevant for this domain. We present a survey on XAI within digital pathology, a medical imaging sub-discipline with particular characteristics and needs. The review includes several contributions. Firstly, we give a thorough overview of current XAI techniques of potential relevance for deep learning methods in pathology imaging, and categorise them from three different aspects. In doing so, we incorporate uncertainty estimation methods as an integral part of the XAI landscape. We also connect the technical methods to the specific prerequisites in digital pathology and present findings to guide future research efforts. The survey is intended for both technical researchers and medical professionals, one of the objectives being to establish a common ground for cross-disciplinary discussions.      
### 33.Computation Offloading in Heterogeneous Vehicular Edge Networks: On-line and Off-policy Bandit Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2008.06302.pdf)
>  With the rapid advancement in vehicular communications and intelligent transportation systems technologies, task offloading in vehicular networking scenarios is emerging as a promising, yet challenging, paradigm in mobile edge computing. In this paper, we study the computation offloading problem from mobile vehicles/users, more specifically, the network- and base station selection problem, in a heterogeneous Vehicular Edge Computing (VEC) scenario, where networks have different traffic loads. In a fast-varying vehicular environment, the latency in computation offloading that arises as a result of network congestion (e.g. at the edge computing servers co-located with the base stations) is a key performance metric. However, due to the non-stationary property of such environments, predicting network congestion is an involved task. To address this challenge, we propose an on-line algorithm and an off-policy learning algorithm based on bandit theory. To dynamically select the least congested network in a piece-wise stationary environment, from the offloading history, these algorithms learn the latency that the offloaded tasks experience. In addition, to minimize the task loss due to the mobility of the vehicles, we develop a method for base station selection and a relaying mechanism in the chosen network based on the sojourn time of the vehicles. Through extensive numerical analysis, we demonstrate that the proposed learning-based solutions adapt to the traffic changes of the network by selecting the least congested network. Moreover, the proposed approaches improve the latency of offloaded tasks.      
### 34.Rb-PaStaNet: A Few-Shot Human-Object Interaction Detection Based on Rules and Part States  [ :arrow_down: ](https://arxiv.org/pdf/2008.06285.pdf)
>  Existing Human-Object Interaction (HOI) Detection approaches have achieved great progress on nonrare classes while rare HOI classes are still not well-detected. In this paper, we intend to apply human prior knowledge into the existing work. So we add human-labeled rules to PaStaNet and propose Rb-PaStaNet aimed at improving rare HOI classes detection. Our results show a certain improvement of the rare classes, while the non-rare classes and the overall improvement is more considerable.      
### 35.MIMO SWIPT Systems with Power Amplifier Nonlinearities and Memory Effects  [ :arrow_down: ](https://arxiv.org/pdf/2008.06281.pdf)
>  In this letter, we study the impact of nonlinear high power amplifier (HPA) on simultaneous wireless information and power transfer (SWIPT), for a point-to-point multiple-input multiple-output communication system. We derive the rate-energy (RE) region by taking into account the HPA nonlinearities and its associated memory effects. We show that HPA significantly degrades the achievable RE region, and a predistortion technique is investigated for compensation. The performance of the proposed predistortion scheme is evaluated in terms of RE region enhancement. Numerical results demonstrate that approximately 24% improvement is obtained for both power-splitting and time-splitting SWIPT architectures.      
### 36.Balancing Accuracy and Complexity in Optimisation Models of Distributed Energy Systems and Microgrids: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2008.06278.pdf)
>  Optimisation and simulation models presented in literature for the design and operation of distributed energy systems (DES) often exclude the inherent nonlinearities related to power flow and generation and storage units, to maintain an accuracy-complexity balance. Such models may provide sub-optimal or even infeasible designs and dispatch schedules. In DES, optimal power flow (OPF) is often treated as a standalone problem, consisting of highly nonlinear, nonconvex constraints related to the underlying distribution network. This aspect of the optimisation problem has often been overlooked by researchers in the process systems and optimisation area. In this review we address the disparity between OPF and DES models, highlighting the importance of including elements of OPF in DES design and operational models to obtain feasible designs and operational schedules. We identify a subset of models that contribute to bridging this gap and provide recommendations for future work based on the different optimisation approaches. We also highlight simulation tools and popular software packages with OPF capabilities that can be utilised alongside DES optimisation models. Detailed representation of commonly used technologies within DES optimisation models is also discussed. The review is aimed at a multidisciplinary audience of researchers and stakeholders who are interested in modelling DES to support the development of more robust and accurate optimisation models for the future.      
### 37.A Learning-based Method for Online Adjustment of C-arm Cone-Beam CT Source Trajectories for Artifact Avoidance  [ :arrow_down: ](https://arxiv.org/pdf/2008.06262.pdf)
>  During spinal fusion surgery, screws are placed close to critical nerves suggesting the need for highly accurate screw placement. Verifying screw placement on high-quality tomographic imaging is essential. C-arm Cone-beam CT (CBCT) provides intraoperative 3D tomographic imaging which would allow for immediate verification and, if needed, revision. However, the reconstruction quality attainable with commercial CBCT devices is insufficient, predominantly due to severe metal artifacts in the presence of pedicle screws. These artifacts arise from a mismatch between the true physics of image formation and an idealized model thereof assumed during reconstruction. Prospectively acquiring views onto anatomy that are least affected by this mismatch can, therefore, improve reconstruction quality. We propose to adjust the C-arm CBCT source trajectory during the scan to optimize reconstruction quality with respect to a certain task, i.e. verification of screw placement. Adjustments are performed on-the-fly using a convolutional neural network that regresses a quality index for possible next views given the current x-ray image. Adjusting the CBCT trajectory to acquire the recommended views results in non-circular source orbits that avoid poor images, and thus, data inconsistencies. We demonstrate that convolutional neural networks trained on realistically simulated data are capable of predicting quality metrics that enable scene-specific adjustments of the CBCT source trajectory. Using both realistically simulated data and real CBCT acquisitions of a semi-anthropomorphic phantom, we show that tomographic reconstructions of the resulting scene-specific CBCT acquisitions exhibit improved image quality particularly in terms of metal artifacts. Since the optimization objective is implicitly encoded in a neural network, the proposed approach overcomes the need for 3D information at run-time.      
### 38.Unsupervised vs. transfer learning for multimodal one-shot matching of speech and images  [ :arrow_down: ](https://arxiv.org/pdf/2008.06258.pdf)
>  We consider the task of multimodal one-shot speech-image matching. An agent is shown a picture along with a spoken word describing the object in the picture, e.g. cookie, broccoli and ice-cream. After observing one paired speech-image example per class, it is shown a new set of unseen pictures, and asked to pick the "ice-cream". Previous work attempted to tackle this problem using transfer learning: supervised models are trained on labelled background data not containing any of the one-shot classes. Here we compare transfer learning to unsupervised models trained on unlabelled in-domain data. On a dataset of paired isolated spoken and visual digits, we specifically compare unsupervised autoencoder-like models to supervised classifier and Siamese neural networks. In both unimodal and multimodal few-shot matching experiments, we find that transfer learning outperforms unsupervised training. We also present experiments towards combining the two methodologies, but find that transfer learning still performs best (despite idealised experiments showing the benefits of unsupervised learning).      
### 39.An Improved Deep Convolutional Neural Network-Based Autonomous Road Inspection Scheme Using Unmanned Aerial Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.06189.pdf)
>  Advancements in artificial intelligence (AI) gives a great opportunity to develop an autonomous devices. The contribution of this work is an improved convolutional neural network (CNN) model and its implementation for the detection of road cracks, potholes, and yellow lane in the road. The purpose of yellow lane detection and tracking is to realize autonomous navigation of unmanned aerial vehicle (UAV) by following yellow lane while detecting and reporting the road cracks and potholes to the server through WIFI or 5G medium. The fabrication of own data set is a hectic and time-consuming task. The data set is created, labeled and trained using default and an improved model. The performance of both these models is benchmarked with respect to accuracy, mean average precision (mAP) and detection time. In the testing phase, it was observed that the performance of the improved model is better in respect of accuracy and mAP. The improved model is implemented in UAV using the robot operating system for the autonomous detection of potholes and cracks in roads via UAV front camera vision in real-time.      
### 40.Speech To Semantics: Improve ASR and NLU Jointly via All-Neural Interfaces  [ :arrow_down: ](https://arxiv.org/pdf/2008.06173.pdf)
>  We consider the problem of spoken language understanding (SLU) of extracting natural language intents and associated slot arguments or named entities from speech that is primarily directed at voice assistants. Such a system subsumes both automatic speech recognition (ASR) as well as natural language understanding (NLU). An end-to-end joint SLU model can be built to a required specification opening up the opportunity to deploy on hardware constrained scenarios like devices enabling voice assistants to work offline, in a privacy preserving manner, whilst also reducing server costs. <br>We first present models that extract utterance intent directly from speech without intermediate text output. We then present a compositional model, which generates the transcript using the Listen Attend Spell ASR system and then extracts interpretation using a neural NLU model. Finally, we contrast these methods to a jointly trained end-to-end joint SLU model, consisting of ASR and NLU subsystems which are connected by a neural network based interface instead of text, that produces transcripts as well as NLU interpretation. We show that the jointly trained model shows improvements to ASR incorporating semantic information from NLU and also improves NLU by exposing it to ASR confusion encoded in the hidden layer.      
### 41.On Social Interactions of Merging Behaviors at Highway On-Ramps in Congested Traffic  [ :arrow_down: ](https://arxiv.org/pdf/2008.06156.pdf)
>  Merging at highway on-ramps while interacting with other human-driven vehicles is challenging for autonomous vehicles (AVs). An efficient route to this challenge requires exploring and then exploiting knowledge of the interaction process from demonstrations by humans. However, it is unclear what information (or the environment states) is utilized by the human driver to guide their behavior over the whole merging process. This paper provides quantitative analysis and evaluation of the merging behavior at highway on-ramps with congested traffic in a volume of time and space. Two types of social interaction scenarios are considered based on the social preferences of surrounding vehicles: courteous and rude. The significant levels of environment states for characterizing the interactive merging process are empirically analyzed based on the real-world INTERACTION dataset. Experimental results reveal two fundamental mechanisms in the merging process: 1) Human driver selects different states to make sequential decisions at different moments of task execution and 2) the social preference of surrounding vehicles has an impact on variable selection for making decisions. It implies that for autonomous driving, efficient decision-making design should filter out irrelevant information while considering the social preference of the surrounding vehicles, to reach a comparable human-level performance. These essential findings shed light on developing new decision-making approaches for AVs.      
### 42.Effect of Architectures and Training Methods on the Performance of Learned Video Frame Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2008.06106.pdf)
>  We analyze the performance of feedforward vs. recurrent neural network (RNN) architectures and associated training methods for learned frame prediction. To this effect, we trained a residual fully convolutional neural network (FCNN), a convolutional RNN (CRNN), and a convolutional long short-term memory (CLSTM) network for next frame prediction using the mean square loss. We performed both stateless and stateful training for recurrent networks. Experimental results show that the residual FCNN architecture performs the best in terms of peak signal to noise ratio (PSNR) at the expense of higher training and test (inference) computational complexity. The CRNN can be trained stably and very efficiently using the stateful truncated backpropagation through time procedure, and it requires an order of magnitude less inference runtime to achieve near real-time frame prediction with an acceptable performance.      
### 43.Geometric Deep Learning for Post-Menstrual Age Prediction based on the Neonatal White Matter Cortical Surface  [ :arrow_down: ](https://arxiv.org/pdf/2008.06098.pdf)
>  Accurate estimation of the age in neonates is essential for measuring neurodevelopmental, medical, and growth outcomes. In this paper, we propose a novel approach to predict the post-menstrual age (PA) at scan, using techniques from geometric deep learning, based on the neonatal white matter cortical surface. We utilize and compare multiple specialized neural network architectures that predict the age using different geometric representations of the cortical surface; we compare MeshCNN, Pointnet++, GraphCNN, and a volumetric benchmark. The dataset is part of the Developing Human Connectome Project (dHCP), and is a cohort of healthy and premature neonates. We evaluate our approach on 650 subjects (727scans) with PA ranging from 27 to 45 weeks. Our results show accurate prediction of the estimated PA, with mean error less than one week.      
### 44.Technical Considerations when using Verasonics Research Ultrasound Platform for Developing a Photoacoustic Imaging System  [ :arrow_down: ](https://arxiv.org/pdf/2008.06086.pdf)
>  Photoacoustic imaging (PAI) is an emerging functional and molecular imaging technology that has attracted much attention in the past decade. Recently, many researchers have used the Vantage Verasonics research system for simultaneous ultrasound (US) and photoacoustic (PA) imaging. This was the motivation to write on the details of US/PA imaging system implementation and characterization using Verasonics platform. We describe the subtle details of US/PA imaging system setup, study the performance parameters of the system, and explain sequencing of the US/PA signal generation and signal amplification as well as the details required for efficient use of the hardware of the system and data processing protocols. We focused on linear-array based PAI due to its popularity and simple setup, as well as its high potential for clinical translatability. We have shown the sequencing of the US/PA signal generation, signal amplification, and related data processing protocols. A step-by-step guideline to develop and characterize PAI system using Vantage 128 has been presented. Some of the limitations of the vantage system are also listed. Photoacoustic imaging is a complement to the already established US imaging technique and may significantly increase its scope of application in diagnostic imaging and therapeutic monitoring. Combining with commercial medical US systems, the development of PAI can be accelerated by taking advantage of US image reconstruction and processing. With the information we presented in the body of this review and the four appendices, we described most of the experimental considerations one should know when working with the Vantage system for PAI tests.      
### 45.Push-SAGA: A decentralized stochastic algorithm with variance reduction over directed graphs  [ :arrow_down: ](https://arxiv.org/pdf/2008.06082.pdf)
>  In this paper, we propose Push-SAGA, a decentralized stochastic first-order method for finite-sum minimization over a directed network of nodes. Push-SAGA combines node-level variance reduction to remove the uncertainty caused by stochastic gradients, network-level gradient tracking to address the distributed nature of the data, and push-sum consensus to tackle the challenge of directed communication links. We show that Push-SAGA achieves linear convergence to the exact solution for smooth and strongly convex problems and is thus the first linearly-convergent stochastic algorithm over arbitrary strongly connected directed graphs. We also characterize the regimes in which Push-SAGA achieves a linear speed-up compared to its centralized counterpart and achieves a network-independent convergence rate. We illustrate the behavior and convergence properties of Push-SAGA with the help of numerical experiments on strongly convex and non-convex problems.      
### 46.Minimum-gain Pole Placement with Sparse Static Feedback  [ :arrow_down: ](https://arxiv.org/pdf/1805.08762.pdf)
>  The minimum-gain eigenvalue assignment/pole placement problem (MGEAP) is a classical problem in LTI systems with static state feedback. In this paper, we study the MGEAP when the state feedback has arbitrary sparsity constraints. We formulate the sparse MGEAP problem as an equality-constrained optimization problem and present an analytical characterization of its locally optimal solution in terms of eigenvector matrices of the closed loop system. This result is used to provide a geometric interpretation of the solution of the non-sparse MGEAP, thereby providing additional insights for this classical problem. Further, we develop an iterative projected gradient descent algorithm to obtain local solutions for the sparse MGEAP using a parametrization based on the Sylvester equation. We present a heuristic algorithm to compute the projections, which also provides a novel method to solve the sparse EAP. Also, a relaxed version of the sparse MGEAP is presented and an algorithm is developed to obtain approximately sparse local solutions to the MGEAP. Finally, numerical studies are presented to compare the properties of the algorithms, which suggest that the proposed projection algorithm converges in most cases.      
