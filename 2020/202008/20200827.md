# ArXiv eess --Thu, 27 Aug 2020
### 1.Orientation-Disentangled Unsupervised Representation Learning for Computational Pathology  [ :arrow_down: ](https://arxiv.org/pdf/2008.11673.pdf)
>  Unsupervised learning enables modeling complex images without the need for annotations. The representation learned by such models can facilitate any subsequent analysis of large image datasets. <br>However, some generative factors that cause irrelevant variations in images can potentially get entangled in such a learned representation causing the risk of negatively affecting any subsequent use. The orientation of imaged objects, for instance, is often arbitrary/irrelevant, thus it can be desired to learn a representation in which the orientation information is disentangled from all other factors. <br>Here, we propose to extend the Variational Auto-Encoder framework by leveraging the group structure of rotation-equivariant convolutional networks to learn orientation-wise disentangled generative factors of histopathology images. This way, we enforce a novel partitioning of the latent space, such that oriented and isotropic components get separated. <br>We evaluated this structured representation on a dataset that consists of tissue regions for which nuclear pleomorphism and mitotic activity was assessed by expert pathologists. We show that the trained models efficiently disentangle the inherent orientation information of single-cell images. In comparison to classical approaches, the resulting aggregated representation of sub-populations of cells produces higher performances in subsequent tasks.      
### 2.DeepVOX: Discovering Features from Raw Audio for Speaker Recognition in Degraded Audio Signals  [ :arrow_down: ](https://arxiv.org/pdf/2008.11668.pdf)
>  Automatic speaker recognition algorithms typically use pre-defined filterbanks, such as Mel-Frequency and Gammatone filterbanks, for characterizing speech audio. The design of these filterbanks is based on domain-knowledge and limited empirical observations. The resultant features, therefore, may not generalize well to different types of audio degradation. In this work, we propose a deep learning-based technique to induce the filterbank design from vast amounts of speech audio. The purpose of such a filterbank is to extract features robust to degradations in the input audio. To this effect, a 1D convolutional neural network is designed to learn a time-domain filterbank called DeepVOX directly from raw speech audio. Secondly, an adaptive triplet mining technique is developed to efficiently mine the data samples best suited to train the filterbank. Thirdly, a detailed ablation study of the DeepVOX filterbanks reveals the presence of both vocal source and vocal tract characteristics in the extracted features. Experimental results on VOXCeleb2, NIST SRE 2008 and 2010, and Fisher speech datasets demonstrate the efficacy of the DeepVOX features across a variety of audio degradations, multi-lingual speech data, and varying-duration speech audio. The DeepVOX features also improve the performance of existing speaker recognition algorithms, such as the xVector-PLDA and the iVector-PLDA.      
### 3.Large-scale neuromorphic optoelectronic computing with a reconfigurable diffractive processing unit  [ :arrow_down: ](https://arxiv.org/pdf/2008.11659.pdf)
>  Application-specific optical processors have been considered disruptive technologies for modern computing that can fundamentally accelerate the development of artificial intelligence (AI) by offering substantially improved computing performance. Recent advancements in optical neural network architectures for neural information processing have been applied to perform various machine learning tasks. However, the existing architectures have limited complexity and performance; and each of them requires its own dedicated design that cannot be reconfigured to switch between different neural network models for different applications after deployment. Here, we propose an optoelectronic reconfigurable computing paradigm by constructing a diffractive processing unit (DPU) that can efficiently support different neural networks and achieve a high model complexity with millions of neurons. It allocates almost all of its computational operations optically and achieves extremely high speed of data modulation and large-scale network parameter updating by dynamically programming optical modulators and photodetectors. We demonstrated the reconfiguration of the DPU to implement various diffractive feedforward and recurrent neural networks and developed a novel adaptive training approach to circumvent the system imperfections. We applied the trained networks for high-speed classifying of handwritten digit images and human action videos over benchmark datasets, and the experimental results revealed a comparable classification accuracy to the electronic computing approaches. Furthermore, our prototype system built with off-the-shelf optoelectronic components surpasses the performance of state-of-the-art graphics processing units (GPUs) by several times on computing speed and more than an order of magnitude on system energy efficiency.      
### 4.High-resolution Multi-spectral Imaging with Diffractive Lenses  [ :arrow_down: ](https://arxiv.org/pdf/2008.11625.pdf)
>  Spectral imaging is a fundamental diagnostic technique with widespread application. Existing spectral imaging approaches have intrinsic limitations on spatial and spectral resolutions due to the physical components they rely on. To overcome these physical limitations, in this paper, we develop a novel multi-spectral imaging modality that enables higher spatial and spectral resolutions. In the developed computational imaging modality, we exploit a diffractive lens, such as a photon sieve, for both dispersing and focusing the optical field, and achieve measurement diversity by changing the focusing behavior of this lens. Because the focal length of a diffractive lens is wavelength-dependent, each measurement is a superposition of differently blurred spectral components. To reconstruct the individual spectral images from these superimposed and blurred measurements, a fast reconstruction algorithm is developed with analysis priors using the alternating direction method of multipliers. Finally, the effectiveness and performance of the developed technique is illustrated for an application in astrophysical imaging under various observation scenarios in the extreme ultraviolet (EUV) regime. The results demonstrate that the technique provides not only diffraction-limited high spatial resolution, as enabled by diffractive lenses, but also the capability of resolving close-by spectral sources that would not otherwise be possible with the existing techniques. This work enables high resolution multi-spectral imaging with low cost designs for a variety of applications and spectral regimes.      
### 5.Adversarially Training for Audio Classifiers  [ :arrow_down: ](https://arxiv.org/pdf/2008.11618.pdf)
>  In this paper, we investigate the potential effect of the adversarially training on the robustness of six advanced deep neural networks against a variety of targeted and non-targeted adversarial attacks. We firstly show that, the ResNet-56 model trained on the 2D representation of the discrete wavelet transform appended with the tonnetz chromagram outperforms other models in terms of recognition accuracy. Then we demonstrate the positive impact of adversarially training on this model as well as other deep architectures against six types of attack algorithms (white and black-box) with the cost of the reduced recognition accuracy and limited adversarial perturbation. We run our experiments on two benchmarking environmental sound datasets and show that without any imposed limitations on the budget allocations for the adversary, the fooling rate of the adversarially trained models can exceed 90\%. In other words, adversarial attacks exist in any scales, but they might require higher adversarial perturbations compared to non-adversarially trained models.      
### 6.Coupling BM3D with directional wavelet packets for image denoising  [ :arrow_down: ](https://arxiv.org/pdf/2008.11595.pdf)
>  The paper presents an image denoising algorithm by combining a method that is based on directional quasi-analytic wavelet packets (qWPs) with the popular BM3D algorithm. The qWPs and its corresponding transforms are designed in [1]. The denoising algorithm qWP (qWPdn) applies an adaptive localized soft thresholding to the transform coefficients using the Bivariate Shrinkage methodology. The combined method consists of several iterations of qWPdn and BM3D algorithms, where the output from one algorithm updates the input to the other (cross-boosting).The qWPdn and BM3D methods complement each other. The qWPdn capabilities to capture edges and fine texture patterns are coupled with utilizing the sparsity in real images and self-similarity of patches in the image that is inherent in the BM3D. The obtained results are quite competitive with the best state-of-the-art algorithms. We compare the performance of the combined methodology with the performances of cptTP-CTF6, DAS-2 algorithms, which use directional frames, and the BM3D algorithm. In the overwhelming majority of the experiments, the combined algorithm outperformed the above methods.      
### 7.Learned Transferable Architectures Can Surpass Hand-Designed Architectures for Large Scale Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.11589.pdf)
>  In this paper, we explore the neural architecture search (NAS) for automatic speech recognition (ASR) systems. With reference to the previous works in the computer vision field, the transferability of the searched architecture is the main focus of our work. The architecture search is conducted on the small proxy dataset, and then the evaluation network, constructed with the searched architecture, is evaluated on the large dataset. Especially, we propose a revised search space for speech recognition tasks which theoretically facilitates the search algorithm to explore the architectures with low complexity. Extensive experiments show that: (i) the architecture searched on the small proxy dataset can be transferred to the large dataset for the speech recognition tasks. (ii) the architecture learned in the revised search space can greatly reduce the computational overhead and GPU memory usage with mild performance degradation. (iii) the searched architecture can achieve more than 20% and 15% (average on the four test sets) relative improvements respectively on the AISHELL-2 dataset and the large (10k hours) dataset, compared with our best hand-designed DFSMN-SAN architecture. To the best of our knowledge, this is the first report of NAS results with large scale dataset (up to 10K hours), indicating the promising application of NAS to industrial ASR systems.      
### 8.Event Cause Analysis in Distribution Networks using Synchro Waveform Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2008.11582.pdf)
>  This paper presents a machine learning method for event cause analysis to enhance situational awareness in distribution networks. The data streams are captured using time-synchronized high sampling rates synchro waveform measurement units (SWMU). The proposed method is formulated based on a machine learning method, the convolutional neural network (CNN). This method is capable of capturing the spatiotemporal feature of the measurements effectively and perform the event cause analysis. Several events are considered in this paper to encompass a range of possible events in real distribution networks, including capacitor bank switching, transformer energization, fault, and high impedance fault (HIF). The dataset for our study is generated using the real-time digital simulator (RTDS) to simulate real-world events. The event cause analysis is performed using only one cycle of the voltage waveforms after the event is detected. The simulation results show the effectiveness of the proposed machine learning-based method compared to the state-of-the-art classifiers.      
### 9.3D Semantic Segmentation of Brain Tumor for Overall Survival Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2008.11576.pdf)
>  Glioma, the malignant brain tumor, requires immediate treatment to improve the survival of patients. Gliomas heterogeneous nature makes the segmentation difficult, especially for sub-regions like necrosis, enhancing tumor, non-enhancing tumor, and Edema. Deep neural networks like full convolution neural networks and ensemble of fully convolution neural networks are successful for Glioma segmentation. The paper demonstrates the use of a 3D fully convolution neural network with a three layer encoder decoder approach for layer arrangement. The encoder blocks include the dense modules, and decoder blocks include convolution modules. The input to the network is 3D patches. The loss function combines dice loss and focal loss functions. The validation set dice score of the network is 0.74, 0.88, and 0.73 for enhancing tumor, whole tumor, and tumor core, respectively. The Random Forest Regressor uses shape, volumetric, and age features extracted from ground truth for overall survival prediction. The regressor achieves an accuracy of 44.8% on the validation set.      
### 10.On the Composition and Limitations of Publicly Available COVID-19 X-Ray Imaging Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2008.11572.pdf)
>  Machine learning based methods for diagnosis and progression prediction of COVID-19 from imaging data have gained significant attention in the last months, in particular by the use of deep learning models. In this context hundreds of models where proposed with the majority of them trained on public datasets. Data scarcity, mismatch between training and target population, group imbalance, and lack of documentation are important sources of bias, hindering the applicability of these models to real-world clinical practice. Considering that datasets are an essential part of model building and evaluation, a deeper understanding of the current landscape is needed. This paper presents an overview of the currently public available COVID-19 chest X-ray datasets. Each dataset is briefly described and potential strength, limitations and interactions between datasets are identified. In particular, some key properties of current datasets that could be potential sources of bias, impairing models trained on them are pointed out. These descriptions are useful for model building on those datasets, to choose the best dataset according the model goal, to take into account the specific limitations to avoid reporting overconfident benchmark results, and to discuss their impact on the generalisation capabilities in a specific clinical setting      
### 11.Parameterization of All Output-Rectifying Retrofit Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2008.11552.pdf)
>  This study investigates a parameterization of all output-rectifying retrofit controllers for distributed design of a structured controller. It has been discovered that all retrofit controllers can be characterized as a constrained Youla parameterization, which is difficult to solve analytically. For synthesis, a tractable and insightful class of retrofit controllers, referred to as output-rectifying retrofit controllers, has been introduced. An unconstrained parameterization of all output-rectifying retrofit controllers can be derived under a technical assumption on measurability of particular signals. The aim of this note is to reveal the structure of all output-rectifying retrofit controllers in the general output-feedback case. It is found out that the existing developments can be generalized based on the notions of state projection and an inverse system. The result leads to the conclusion that output-rectifying retrofit controllers can readily be designed even in the general case.      
### 12.TIV.lib: an open-source library for the tonal description of musical audio  [ :arrow_down: ](https://arxiv.org/pdf/2008.11529.pdf)
>  In this paper, we present TIV.lib, an open-source library for the content-based tonal description of musical audio signals. Its main novelty relies on the perceptually-inspired Tonal Interval Vector space based on the Discrete Fourier transform, from which multiple instantaneous and global representations, descriptors and metrics are computed - e.g., harmonic change, dissonance, diatonicity, and musical key. The library is cross-platform, implemented in Python and the graphical programming language Pure Data, and can be used in both online and offline scenarios. Of note is its potential for enhanced Music Information Retrieval, where tonal descriptors sit at the core of numerous methods and applications.      
### 13.Relative limitations of increasing the number of modulation levels in computer generated holography  [ :arrow_down: ](https://arxiv.org/pdf/2008.11519.pdf)
>  Phase and amplitude spatial light modulators (SLMs) capable of both binary and multi-level modulation are widely available and offer a wide range of technologies to choose from for holographic applications. While the replay fields generated with multi-level phase-only SLMs are of a significantly higher quality than those generated by equivalent binary phase-only SLMs, evidence is presented in this letter that this improvement is not as marked for amplitude SLMs, where multi-level devices offer only a small benefit over their binary counterparts. Heuristic and numerical justifications for this are discussed and conclusions drawn.      
### 14.Novel Predictive Search Algorithm for Phase Holography  [ :arrow_down: ](https://arxiv.org/pdf/2008.11518.pdf)
>  We present a novel algorithm for generating high quality holograms for Computer Generated Holography - Holographic Predictive Search. This approach is presented as an alternative to traditional Holographic Search Algorithms such as Direct Search (DS) and Simulated Annealing (SA). We first introduce the current search based methods and then introduce an analytical model of the underlying Fourier elements. This is used to make prescient judgements regarding the next iteration of the algorithm. This new approach is developed for the case of phase modulating devices with phase sensitive reconstructions. When compared to conventional iterative approaches such as DS and SA on a multi-phase device, Holographic Predictive Search offered improvements in quality of 5x as well up to 10x improvements in convergence time. This is at the cost of an increased iteration overhead.      
### 15.Improving performance of single-pass real-time holographic projection  [ :arrow_down: ](https://arxiv.org/pdf/2008.11517.pdf)
>  This work describes a novel approach to time-multiplexed holographic projection on binary phase devices. Unlike other time-multiplexed algorithms where each frame is the inverse transform of independently modified target images, Single-Transform Time-Multiplexed (STTM) hologram generation produces multiple sub-frames from a single inverse transform. Uniformly spacing complex rotations on the diffraction field then allows the emulation of devices containing 2^N modulation levels on binary devices by using N sub-frames. In comparison to One-Step Phase Retrieval (OSPR), STTM produces lower mean squared error for up to N = 5 than the equivalent number of OSPR sub-frames with a generation time of 1/N of the equivalent OSPR frame. A mathematical justification of the STTM approach is presented and a hybrid approach is introduced allowing STTM to be used in conjunction with OSPR in order to combine performance benefits.      
### 16.Disentangled Representations for Domain-generalized Cardiac Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11514.pdf)
>  Robust cardiac image segmentation is still an open challenge due to the inability of the existing methods to achieve satisfactory performance on unseen data of different domains. Since the acquisition and annotation of medical data are costly and time-consuming, recent work focuses on domain adaptation and generalization to bridge the gap between data from different populations and scanners. In this paper, we propose two data augmentation methods that focus on improving the domain adaptation and generalization abilities of state-to-the-art cardiac segmentation models. In particular, our "Resolution Augmentation" method generates more diverse data by rescaling images to different resolutions within a range spanning different scanner protocols. Subsequently, our "Factor-based Augmentation" method generates more diverse data by projecting the original samples onto disentangled latent spaces, and combining the learned anatomy and modality factors from different domains. Our extensive experiments demonstrate the importance of efficient adaptation between seen and unseen domains, as well as model generalization ability, to robust cardiac image segmentation.      
### 17.Detection of Retinal Blood Vessels by using Gabor filter with Entropic threshold  [ :arrow_down: ](https://arxiv.org/pdf/2008.11508.pdf)
>  Diabetic retinopathy is the basic reason for visual deficiency. This paper introduces a programmed strategy to identify and dispense with the blood vessels. The location of the blood vessels is the fundamental stride in the discovery of diabetic retinopathy because the blood vessels are the typical elements of the retinal picture. The location of the blood vessels can help the ophthalmologists to recognize the sicknesses prior and quicker. The blood vessels recognized and wiped out by utilizing Gobar filter on two freely accessible retinal databases which are STARE and DRIVE. The exactness of segmentation calculation is assessed quantitatively by contrasting the physically sectioned pictures and the comparing yield pictures, the Gabor filter with Entropic threshold vessel pixel segmentation by Entropic thresholding is better vessels with less false positive portion rate.      
### 18.The Freesound Loop Dataset and Annotation Tool  [ :arrow_down: ](https://arxiv.org/pdf/2008.11507.pdf)
>  Music loops are essential ingredients in electronic music production, and there is a high demand for pre-recorded loops in a variety of styles. Several commercial and community databases have been created to meet this demand, but most are not suitable for research due to their strict licensing. We present the Freesound Loop Dataset (FSLD), a new large-scale dataset of music loops annotated by experts. The loops originate from Freesound, a community database of audio recordings released under Creative Commons licenses, so the audio in our dataset may be redistributed. The annotations include instrument, tempo, meter, key and genre tags. We describe the methodology used to assemble and annotate the data, and report on the distribution of tags in the data and inter-annotator agreement. We also present to the community an online loop annotator tool that we developed. To illustrate the usefulness of FSLD, we present short case studies on using it to estimate tempo and key, generate music tracks, and evaluate a loop separation algorithm. We anticipate that the community will find yet more uses for the data, in applications from automatic loop characterisation to algorithmic composition.      
### 19.DRR4Covid: Learning Automated COVID-19 Infection Segmentation from Digitally Reconstructed Radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2008.11478.pdf)
>  Automated infection measurement and COVID-19 diagnosis based on Chest X-ray (CXR) imaging is important for faster examination. We propose a novel approach, called DRR4Covid, to learn automated COVID-19 diagnosis and infection segmentation on CXRs from digitally reconstructed radiographs (DRRs). DRR4Covid comprises of an infection-aware DRR generator, a classification and/or segmentation network, and a domain adaptation module. The infection-aware DRR generator is able to produce DRRs with adjustable strength of radiological signs of COVID-19 infection, and generate pixel-level infection annotations that match the DRRs precisely. The domain adaptation module is introduced to reduce the domain discrepancy between DRRs and CXRs by training networks on unlabeled real CXRs and labeled DRRs together.We provide a simple but effective implementation of DRR4Covid by using a domain adaptation module based on Maximum Mean Discrepancy (MMD), and a FCN-based network with a classification header and a segmentation header. Extensive experiment results have confirmed the efficacy of our method; specifically, quantifying the performance by accuracy, AUC and F1-score, our network without using any annotations from CXRs has achieved a classification score of (0.954, 0.989, 0.953) and a segmentation score of (0.957, 0.981, 0.956) on a test set with 794 normal cases and 794 positive cases. Besides, we estimate the sensitive of X-ray images in detecting COVID-19 infection by adjusting the strength of radiological signs of COVID-19 infection in synthetic DRRs. The estimated detection limit of the proportion of infected voxels in the lungs is 19.43%, and the estimated lower bound of the contribution rate of infected voxels is 20.0% for significant radiological signs of COVID-19 infection. Our codes will be made publicly available at <a class="link-external link-https" href="https://github.com/PengyiZhang/DRR4Covid" rel="external noopener nofollow">this https URL</a>.      
### 20.Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution using Dynamic Filters  [ :arrow_down: ](https://arxiv.org/pdf/2008.11449.pdf)
>  Light field cameras have been proved to be powerful tools for 3D reconstruction and virtual reality applications. However, the limited resolution of light field images brings a lot of difficulties for further information display and extraction. In this paper, we introduce a novel learning-based framework to improve the spatial resolution of light fields. First, features from different dimensions are parallelly extracted and fused together in our multi-dimension fusion architecture. These features are then used to generate dynamic filters, which extract subpixel information from micro-lens images and also implicitly consider the disparity information. Finally, more high-frequency details learned in the residual branch are added to the upsampled images and the final super-resolved light fields are obtained. Experimental results show that the proposed method uses fewer parameters but achieves better performances than other state-of-the-art methods in various kinds of datasets. Our reconstructed images also show sharp details and distinct lines in both sub-aperture images and epipolar plane images.      
### 21.Better Than Reference In Low Light Image Enhancement: Conditional Re-Enhancement Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.11434.pdf)
>  Low light images suffer from severe noise, low brightness, low contrast, etc. In previous researches, many image enhancement methods have been proposed, but few methods can deal with these problems simultaneously. In this paper, to solve these problems simultaneously, we propose a low light image enhancement method that can combined with supervised learning and previous HSV (Hue, Saturation, Value) or Retinex model based image enhancement methods. First, we analyse the relationship between the HSV color space and the Retinex theory, and show that the V channel (V channel in HSV color space, equals the maximum channel in RGB color space) of the enhanced image can well represent the contrast and brightness enhancement process. Then, a data-driven conditional re-enhancement network (denoted as CRENet) is proposed. The network takes low light images as input and the enhanced V channel as condition, then it can re-enhance the contrast and brightness of the low light image and at the same time reduce noise and color distortion. It should be noted that during the training process, any paired images with different exposure time can be used for training, and there is no need to carefully select the supervised images which will save a lot. In addition, it takes less than 20 ms to process a color image with the resolution 400*600 on a 2080Ti GPU. Finally, some comparative experiments are implemented to prove the effectiveness of the method. The results show that the method proposed in this paper can significantly improve the quality of the enhanced image, and by combining with other image contrast enhancement methods, the final enhancement result can even be better than the reference image in contrast and brightness. (Code will be available at <a class="link-external link-https" href="https://github.com/hitzhangyu/image-enhancement-with-denoise" rel="external noopener nofollow">this https URL</a>)      
### 22.Beyond 5G Wireless Localization with Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2008.11431.pdf)
>  5G radio positioning exploits information in both angle and delay, by virtue of increased bandwidth and large antenna arrays. When large arrays are embedded in surfaces, they can passively steer electromagnetic waves in preferred directions of space. Reconfigurable intelligent surfaces (RIS), which are seen as a transformative "beyond 5G" technology, can thus control the physical propagation environment. Whereas such RIS have been mainly intended for communication purposes so far, we herein state and analyze a RIS-aided downlink positioning problem from the Fisher Information perspective. Then, based on this analysis, we propose a two-step optimization scheme that selects the best RIS combination to be activated and controls the phases of their constituting elements so as to improve positioning performance. Preliminary simulation results show coverage and accuracy gains in comparison with natural scattering, while pointing out limitations in terms of low signal to noise ratio (SNR) and inter-path interference.      
### 23.Time-frequency equivalence using chirp signals for electrochemical impedance spectroscopy  [ :arrow_down: ](https://arxiv.org/pdf/2008.11393.pdf)
>  Frequency response analysis (FRA) of systems is a well-researched area. Frequency response of electrochemical systems are identified using the electrochemical impedance spectroscopy (EIS) technique. EIS is unarguably the most used technique for diagnostic applications in several electrochemical systems that have relevance in renewable energy, corrosion resistance, sensors, and environmental applications. For years, EIS has been performed using input signals, which are a series of sinusoids or a sum of sinusoids. This results in large experimentation time, particularly when the system has to be probed at lower frequencies. In this work, we describe a previously unknown time-frequency duality for linear systems when probed through a specific signal. It is surprising that this result had not been uncovered given that FRA has been used in multiple disciplines for more than hundred years. The implication of this result is that orders of magnitude reduction in experimentation time over standard EIS techniques is possible. Theoretical and simulation studies support our claims.      
### 24.Actuator Dynamics Compensation in Stabilization of Abstract Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.11333.pdf)
>  This is the first part of four series papers, aiming at the problem of actuator dynamics compensation for linear systems. We consider the stabilization of a type of cascade abstract linear systems which model the actuator dynamics compensation for linear systems where both the control plant and its actuator dynamics can be infinite-dimensional. We develop a systematic way to stabilize the cascade systems by a full state feedback. Both the well-posedness and the exponential stability of the resulting closed-loop system are established in the abstract framework. A sufficient condition of the existence of compensator for ordinary differential equation (ODE) with partial differential equation (PDE) actuator dynamics is obtained. The feedback design is based on a novelly constructed upper-block-triangle transform and the Lyapunov function design is not needed in the stability analysis. As applications, an ODE with input delay and an unstable heat equation with ODE actuator dynamics are investigated to validate the theoretical results. The numerical simulations for the unstable heat system are carried out to validate the proposed approach visually.      
### 25.On Structured-Closed-Loop versus Structured-Controller Design: the Case of Relative Measurement Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2008.11291.pdf)
>  We consider optimal distributed controller design problems with two separate structural constraints: locality and relative feedback. The locality constraint accounts for subcontroller interactions restricted to local neighborhoods as specified by an underlying graph structure. We provide a detailed characterization of such locality constraints imposed on an implementation of a controller (sparsity of state space matrices), in contrast to locality constraints on the sparsity of the controller input-output mapping (transfer function), which is the main tool used in the recently developed System Level Synthesis (SLS) framework. We formally show how the latter problem is a convex relaxation of the former, original problem. We then formulate and show how to append relative feedback constraints that are natural in many problems where only differential sensors are available, such as in vehicular formation and consensus-type problems. We show that for a class of plants, these constraints can be imposed in a convex manner. Our main result demonstrates that when relative feedback constraints are imposed in addition to locality, the SLS convex relaxation can become infeasible. We use these insights to provide a comparative context for the various structured distributed control design methods such as SLS, funnel causality and quadratic invariance, as well as the open problems of structured and networked realizations.      
### 26.Multichannel Convolutive Speech Separation with Estimated Density Models  [ :arrow_down: ](https://arxiv.org/pdf/2008.11273.pdf)
>  We consider the separation of convolutive speech mixtures in the framework of independent component analysis (ICA). Multivariate Laplace distribution is widely used for such tasks. But, it fails to capture the fine structures of speech signals, and limits the performance of separation. Here, we first time show that it is possible to efficiently learn the derivative of speech density with universal approximators like deep neural networks by optimizing certain proxy separation related performance indices. Specifically, we consider neural network density models for speech signals represented in the time-frequency domain, and compare them against the classic multivariate Laplace model for independent vector analysis (IVA). Experimental results suggest that the neural network density models significantly outperform multivariate Laplace one in tasks that require real time implementations, or involve the separation of a large number of speech sources.      
### 27.Extending fibre nonlinear interference power modelling to account for general dual-polarisation 4D modulation formats  [ :arrow_down: ](https://arxiv.org/pdf/2008.11243.pdf)
>  In optical communications, four-dimensional (4D) modulation formats encode information onto the quadrature components of two arbitrary orthogonal states of polarisation of the optical field. These formats have recently regained attention due their potential power efficiency, nonlinearity tolerance, and ultimately to their still unexplored shaping gains. As in the fibre-optic channel the shaping gain is closely related to the nonlinearity tolerance of a given modulation format, predicting the effect of nonlinearity is key to effectively optimise the transmitted constellation. Many analytical models available in the optical communication literature allow, within a first-order perturbation framework, the computation of the average power of the nonlinear interference (NLI) accumulated in coherent fibre-optic transmission systems. However, all current models only operate under the assumption of a transmitted two-dimensional, polarisation-multiplexed (2D-PM) modulation format. 2D-PM formats represent a limited subset of the possible dual-polarisation 4D formats, namely, only those where data transmitted on each polarisation channel are mutually independent and identically distributed. This document presents a step-by-step mathematical derivation of the extension of existing NLI models to the class of arbitrary dual-polarisation 4D modulation formats. In particular, the methodology adopted follows the one of the popular enhanced Gaussian noise model, albeit dropping most assumptions on the geometry and statistic of the transmitted 4D modulation format. The resulting expressions show that, whilst in the 2D-PM case the NLI power depends only on different statistical high-order moments of each polarisation component, for a general 4D constellation also several others cross-polarisation correlations need to be taken into account.      
### 28.A Computationally Efficient Multiclass Time-Frequency Common Spatial Pattern Analysis on EEG Motor Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2008.11227.pdf)
>  Common spatial pattern (CSP) is a popular feature extraction method for electroencephalogram (EEG) motor imagery (MI). This study modifies the conventional CSP algorithm to improve the multi-class MI classification accuracy and ensure the computation process is efficient. The EEG MI data is gathered from the Brain-Computer Interface (BCI) Competition IV. At first, a bandpass filter and a time-frequency analysis are performed for each experiment trial. Then, the optimal EEG signals for every experiment trials are selected based on the signal energy for CSP feature extraction. In the end, the extracted features are classified by three classifiers, linear discriminant analysis (LDA), naïve Bayes (NVB), and support vector machine (SVM), in parallel for classification accuracy comparison. The experiment results show the proposed algorithm average computation time is 37.22% less than the FBCSP (1st winner in the BCI Competition IV) and 4.98% longer than the conventional CSP method. For the classification rate, the proposed algorithm kappa value achieved 2nd highest compared with the top 3 winners in BCI Competition IV.      
### 29.A Survey and Tutorial of EEG-Based Brain Monitoring for Driver State Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.11226.pdf)
>  Drivers cognitive and physiological states affect their ability to control their vehicles. Thus, these driver states are important to the safety of automobiles. The design of advanced driver assistance systems (ADAS) or autonomous vehicles will depend on their ability to interact effectively with the driver. A deeper understanding of the driver state is, therefore, paramount. EEG is proven to be one of the most effective methods for driver state monitoring and human error detection. This paper discusses EEG-based driver state detection systems and their corresponding analysis algorithms over the last three decades. First, the commonly used EEG system setup for driver state studies is introduced. Then, the EEG signal preprocessing, feature extraction, and classification algorithms for driver state detection are reviewed. Finally, EEG-based driver state monitoring research is reviewed in-depth, and its future development is discussed. It is concluded that the current EEG-based driver state monitoring algorithms are promising for safety applications. However, many improvements are still required in EEG artifact reduction, real-time processing, and between-subject classification accuracy.      
### 30.Safe Model-Based Meta-Reinforcement Learning: A Sequential Exploration-Exploitation Framework  [ :arrow_down: ](https://arxiv.org/pdf/2008.11700.pdf)
>  Safe deployment of autonomous robots in diverse environments requires agents that are capable of safe and efficient adaptation to new scenarios. Indeed, achieving both data efficiency and well-calibrated safety has been a central problem in robotic learning and adaptive control due in part to the tension between these objectives. In this work, we develop a framework for probabilistically safe operation with uncertain dynamics. This framework relies on Bayesian meta-learning for efficient inference of system dynamics with calibrated uncertainty. We leverage the model structure to construct confidence bounds which hold throughout the learning process, and factor this uncertainty into a model-based planning framework. By decomposing the problem of control under uncertainty into discrete exploration and exploitation phases, our framework extends to problems with high initial uncertainty while maintaining probabilistic safety and persistent feasibility guarantees during every phase of operation. We validate our approach on the problem of a nonlinear free flying space robot manipulating a payload in cluttered environments, and show it can safely learn and reach a goal.      
### 31.5G Utility Pole Planner Using Google Street View and Mask R-CNN  [ :arrow_down: ](https://arxiv.org/pdf/2008.11689.pdf)
>  With the advances of fifth-generation (5G) cellular networks technology, many studies and work have been carried out on how to build 5G networks for smart cities. In the previous research, street lighting poles and smart light poles are capable of being a 5G access point. In order to determine the position of the points, this paper discusses a new way to identify poles based on Mask R-CNN, which extends Fast R-CNNs by making it employ recursive Bayesian filtering and perform proposal propagation and reuse. The dataset contains 3,000 high-resolution images from google map. To make training faster, we used a very efficient GPU implementation of the convolution operation. We achieved a train error rate of 7.86% and a test error rate of 32.03%. At last, we used the immune algorithm to set 5G poles in the smart cities.      
### 32.DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment in COVID-19 Pandemic  [ :arrow_down: ](https://arxiv.org/pdf/2008.11672.pdf)
>  Social distancing is a recommended solution by the World Health Organisation (WHO) to minimise the spread of COVID-19 in public places. The majority of governments and national health authorities have set the 2-meter physical distancing as a mandatory safety measure in shopping centres, schools and other covered areas. In this research, we develop a generic Deep Neural Network-Based model for automated people detection, tracking, and inter-people distances estimation in the crowd, using common CCTV security cameras. The proposed model includes a YOLOv4-based framework and inverse perspective mapping for accurate people detection and social distancing monitoring in challenging conditions, including people occlusion, partial visibility, and lighting variations. We also provide an online risk assessment scheme by statistical analysis of the Spatio-temporal data from the moving trajectories and the rate of social distancing violations. We identify high-risk zones with the highest possibility of virus spread and infections. This may help authorities to redesign the layout of a public place or to take precaution actions to mitigate high-risk zones. The efficiency of the proposed methodology is evaluated on the Oxford Town Centre dataset, with superior performance in terms of accuracy and speed compared to three state-of-the-art methods.      
### 33.Analysis of deep machine learning algorithms in COVID-19 disease diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2008.11639.pdf)
>  The aim of the work is to use deep neural network models for solving the problem of image recognition. These days, every human being is threatened by a harmful coronavirus disease, also called COVID-19 disease. The spread of coronavirus affects the economy of many countries in the world. To find COVID-19 patients early is very essential to avoid the spread and harm to society. Pathological tests and Chromatography(CT) scans are helpful for the diagnosis of COVID-19. However, these tests are having drawbacks such as a large number of false positives, and cost of these tests are so expensive. Hence, it requires finding an easy, accurate, and less expensive way for the detection of the harmful COVID-19 disease. Chest-x-ray can be useful for the detection of this disease. Therefore, in this work chest, x-ray images are used for the diagnosis of suspected COVID-19 patients using modern machine learning techniques. The analysis of the results is carried out and conclusions are made about the effectiveness of deep machine learning algorithms in image recognition problems.      
### 34.Assessment of Reward Functions for Reinforcement Learning Traffic Signal Control under Real-World Limitations  [ :arrow_down: ](https://arxiv.org/pdf/2008.11634.pdf)
>  Adaptive traffic signal control is one key avenue for mitigating the growing consequences of traffic congestion. Incumbent solutions such as SCOOT and SCATS require regular and time-consuming calibration, can't optimise well for multiple road use modalities, and require the manual curation of many implementation plans. A recent alternative to these approaches are deep reinforcement learning algorithms, in which an agent learns how to take the most appropriate action for a given state of the system. This is guided by neural networks approximating a reward function that provides feedback to the agent regarding the performance of the actions taken, making it sensitive to the specific reward function chosen. Several authors have surveyed the reward functions used in the literature, but attributing outcome differences to reward function choice across works is problematic as there are many uncontrolled differences, as well as different outcome metrics. This paper compares the performance of agents using different reward functions in a simulation of a junction in Greater Manchester, UK, across various demand profiles, subject to real world constraints: realistic sensor inputs, controllers, calibrated demand, intergreen times and stage sequencing. The reward metrics considered are based on the time spent stopped, lost time, change in lost time, average speed, queue length, junction throughput and variations of these magnitudes. The performance of these reward functions is compared in terms of total waiting time. We find that speed maximisation resulted in the lowest average waiting times across all demand levels, displaying significantly better performance than other rewards previously introduced in the literature.      
### 35.A Bilateral Game Approach for Task Outsourcing in Multi-access Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2008.11617.pdf)
>  Multi-access edge computing (MEC) is a promising architecture to provide low-latency applications for future Internet of Things (IoT)-based network systems. Together with the increasing scholarly attention on task offloading, the problem of edge servers' resource allocation has been widely studied. Most of previous works focus on a single edge server (ES) serving multiple terminal entities (TEs), which restricts their access to sufficient resources. In this paper, we consider a MEC resource transaction market with multiple ESs and multiple TEs, which are interdependent and mutually influence each other. However, this many-to-many interaction requires resolving several problems, including task allocation, TEs' selection on ESs and conflicting interests of both parties. Game theory can be used as an effective tool to realize the interests of two or more conflicting individuals in the trading market. Therefore, we propose a bilateral game framework among multiple ESs and multiple TEs by modeling the task outsourcing problem as two noncooperative games: the supplier and customer side games. In the first game, the supply function bidding mechanism is employed to model the ESs' profit maximization problem. The ESs submit their bids to the scheduler, where the computing service price is computed and sent to the TEs. While in the second game, TEs determine the optimal demand profiles according to ESs' bids to maximize their payoff. The existence and uniqueness of the Nash equilibrium in the aforementioned games are proved. A distributed task outsourcing algorithm (DTOA) is designed to determine the equilibrium. Simulation results have demonstrated the superior performance of DTOA in increasing the ESs' profit and TEs' payoff, as well as flattening the peak and off-peak load.      
### 36.Reproducible Pruning System on Dynamic Natural Plants for Field Agricultural Robots  [ :arrow_down: ](https://arxiv.org/pdf/2008.11613.pdf)
>  Pruning is the art of cutting unwanted and unhealthy plant branches and is one of the difficult tasks in the field robotics. It becomes even more complex when the plant branches are moving. Moreover, the reproducibility of robot pruning skills is another challenge to deal with due to the heterogeneous nature of vines in the vineyard. This research proposes a multi-modal framework to deal with the dynamic vines with the aim of sim2real skill transfer. The 3D models of vines are constructed in blender engine and rendered in simulated environment as a need for training the robot. The Natural Admittance Controller (NAC) is applied to deal with the dynamics of vines. It uses force feedback and compensates the friction effects while maintaining the passivity of system. The faster R-CNN is used to detect the spurs on the vines and then statistical pattern recognition algorithm using K-means clustering is applied to find the effective pruning points. The proposed framework is tested in simulated and real environments.      
### 37.Robust Reinforcement Learning: A Case Study in Linear Quadratic Regulation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11592.pdf)
>  This paper studies the robustness aspect of reinforcement learning algorithms in the presence of errors. Specifically, we revisit the benchmark problem of discrete-time linear quadratic regulation (LQR) and study the long-standing open question: Under what conditions is the policy iteration method robustly stable for dynamical systems with unbounded, continuous state and action spaces? Using advanced stability results in control theory, it is shown that policy iteration for LQR is inherently robust to small errors and enjoys local input-to-state stability: whenever the error in each iteration is bounded and small, the solutions of the policy iteration algorithm are also bounded, and, moreover, enter and stay in a small neighborhood of the optimal LQR solution. As an application, a novel off-policy optimistic least-squares policy iteration for the LQR problem is proposed, when the system dynamics are subjected to additive stochastic disturbances. The proposed new results in robust reinforcement learning are validated by a numerical example.      
### 38.Leveraging Kernelized Synergies on Shared Subspace for Precision Grasp and Dexterous Manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11574.pdf)
>  Manipulation in contrast to grasping is a trajectorial task that needs to use dexterous hands. Improving the dexterity of robot hands, increases the controller complexity and thus requires to use the concept of postural synergies. Inspired from postural synergies, this research proposes a new framework called kernelized synergies that focuses on the re-usability of the same subspace for precision grasping and dexterous manipulation. In this work, the computed subspace of postural synergies; parameterized by probabilistic movement primitives, is treated with kernel to preserve its grasping and manipulation characteristics and allows its reuse for new objects. The grasp stability of the proposed framework is assessed with a force closure quality index. For performance evaluation, the proposed framework is tested on two different simulated robot hand models using the Syngrasp toolbox and experimentally, four complex grasping and manipulation tasks are performed and reported. The results confirm the hand agnostic approach of the proposed framework and its generalization to distinct objects irrespective of their shape and size.      
### 39.PyNX: high performance computing toolkit for coherent X-ray imaging based on operators  [ :arrow_down: ](https://arxiv.org/pdf/2008.11511.pdf)
>  The open-source PyNX toolkit [Favre-Nicolin et al (2011) <a class="link-https" data-arxiv-id="1010.2641" href="https://arxiv.org/abs/1010.2641">arXiv:1010.2641</a>, Mandula et al (2016)] has been extended to provide tools for coherent X-ray imaging data analysis and simulation. All calculations can be executed on graphical processing units (GPU) to achieve high performance computing speeds. This can be used for Coherent Diffraction Imaging (CDI), Ptychography and wavefront propagation, in the far or near field regime. Moreover, all imaging operations (propagation, projections, algorithm cycles..) can be used in Python as simple mathematical operators, an approach which can be used to easily combine basic algorithms in a tailored chain. Calculations can also be distributed to multiple GPUs, e.g. for large Ptychography datasets. Command-line scripts are also available for on-line CDI and Ptychography analysis, either from raw beamline datasets or using the Coherent X-ray Imaging data format [Maia (2012)].      
### 40.Approximating Power Flow and Transmission Losses in Coordinated Capacity Expansion Problems  [ :arrow_down: ](https://arxiv.org/pdf/2008.11510.pdf)
>  With rising shares of renewables and the need to properly assess trade-offs between transmission, storage and sectoral integration as balancing options, building a bridge between energy system models and detailed power flow studies becomes increasingly important, but is computationally challenging. <br>W compare approximations for two nonlinear phenomena, power flow and transmission losses, in linear capacity expansion problems that co-optimise investments in generation, storage and transmission infrastructure. We evaluate different flow representations discussing differences in investment decisions, nodal prices, the deviation of optimised flows and losses from simulated AC power flows, and the computational performance. By using the open European power system model PyPSA-Eur we obtain detailed and reproducible results aiming at facilitating the selection of a suitable power flow model. <br>Given the differences in complexity, the optimal choice depends on the application, the user's available computational resources, and the level of spatial detail considered. Although the commonly used transport model can already identify key features of a cost-efficient system while being computationally performant, deficiencies under high loading conditions arise due to the lack of a physical grid representation. Moreover, disregarding transmission losses overestimates optimal grid expansion by 20%. Adding a convex relaxation of quadratic losses with two or three tangents to the linearised power flow equations and accounting for changing line impedances as the network is reinforced suffices to represent power flows and losses adequately in design studies. We show that the obtained investment and dispatch decisions are then sufficiently physical to be used in more detailed nonlinear simulations of AC power flow in order to better assess their technical feasibility.      
### 41.On the Realization of Hidden Markov Models and Tensor Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2008.11487.pdf)
>  The minimum realization problem of hidden Markov models (HMM's) is a fundamental question of stationary discrete-time processes with a finite alphabet. It was shown in the literature that tensor decomposition methods give the hidden Markov model with the minimum number of states generically. However, the tensor decomposition approach does not solve the minimum HMM realization problem when the observation is a deterministic function of the state, which is an important class of HMM's not captured by a generic argument. In this paper, we show that the reduction of the number of rank-one tensors necessary to decompose the third-order tensor constructed from the probabilities of the process is possible when the reachable subspace is not the whole space or the null space is not the zero space. In fact, the rank of the tensor is not greater than the dimension of the effective subspace or the rank of the generalized Hankel matrix.      
### 42.Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2008.11479.pdf)
>  Cosplay has grown from its origins at fan conventions into a billion-dollar global dress phenomenon. To facilitate imagination and reinterpretation from animated images to real garments, this paper presents an automatic costume image generation method based on image-to-image translation. Cosplay items can be significantly diverse in their styles and shapes, and conventional methods cannot be directly applied to the wide variation in clothing images that are the focus of this study. To solve this problem, our method starts by collecting and preprocessing web images to prepare a cleaned, paired dataset of the anime and real domains. Then, we present a novel architecture for generative adversarial networks (GANs) to facilitate high-quality cosplay image generation. Our GAN consists of several effective techniques to fill the gap between the two domains and improve both the global and local consistency of generated images. Experiments demonstrated that, with two types of evaluation metrics, the proposed GAN achieves better performance than existing methods. We also showed that the images generated by the proposed method are more realistic than those generated by the conventional methods. Our codes and pretrained model are available on the web.      
### 43.Bellman filtering for state-space models  [ :arrow_down: ](https://arxiv.org/pdf/2008.11477.pdf)
>  This article presents a new filter for state-space models based on Bellman's dynamic programming principle applied to the posterior mode. The proposed Bellman filter generalises the Kalman filter including its extended and iterated versions, while remaining equally inexpensive computationally. The Bellman filter is also (unlike the Kalman filter) robust under heavy-tailed observation noise and applicable to a wider range of models. Simulation studies reveal that the mean absolute error of the Bellman-filtered states using estimated parameters typically falls within a few percent of that produced by the mode estimator evaluated at the true parameters, which is optimal but generally infeasible.      
### 44.Fusion of Global-Local Features for Image Quality Inspection of Shipping Label  [ :arrow_down: ](https://arxiv.org/pdf/2008.11440.pdf)
>  The demands of automated shipping address recognition and verification have increased to handle a large number of packages and to save costs associated with misdelivery. A previous study proposed a deep learning system where the shipping address is recognized and verified based on a camera image capturing the shipping address and barcode area. Because the system performance depends on the input image quality, inspection of input image quality is necessary for image preprocessing. In this paper, we propose an input image quality verification method combining global and local features. Object detection and scale-invariant feature transform in different feature spaces are developed to extract global and local features from several independent convolutional neural networks. The conditions of shipping label images are classified by fully connected fusion layers with concatenated global and local features. The experimental results regarding real captured and generated images show that the proposed method achieves better performance than other methods. These results are expected to improve the shipping address recognition and verification system by applying different image preprocessing steps based on the classified conditions.      
### 45.Disentangled Adversarial Autoencoder for Subject-Invariant Physiological Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2008.11426.pdf)
>  Recent developments in biosignal processing have enabled users to exploit their physiological status for manipulating devices in a reliable and safe manner. One major challenge of physiological sensing lies in the variability of biosignals across different users and tasks. To address this issue, we propose an adversarial feature extractor for transfer learning to exploit disentangled universal representations. We consider the trade-off between task-relevant features and user-discriminative information by introducing additional adversary and nuisance networks in order to manipulate the latent representations such that the learned feature extractor is applicable to unknown users and various tasks. Results on cross-subject transfer evaluations exhibit the benefits of the proposed framework, with up to 8.8% improvement in average accuracy of classification, and demonstrate adaptability to a broader range of subjects.      
### 46.Low Tensor Train- and Low Multilinear Rank Approximations for De-speckling and Compression of 3D Optical Coherence Tomography Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.11414.pdf)
>  This paper proposes low tensor-train (TT) rank and low multilinear (ML) rank approximations for de-speckling and compression of 3D optical coherence tomography (OCT) images for a given compression ratio (CR). To this end, we derive the alternating direction method of multipliers based algorithms for the related problems constrained with the low TT- and low ML rank. Rank constraints are implemented through the Schatten-p (Sp) norm, p e {0, 1/2, 2/3, 1}, of unfolded matrices. We provide the proofs of global convergence towards a stationary point for both algorithms. Rank adjusted 3D OCT image tensors are finally approximated through tensor train- and Tucker alternating least squares decompositions. We comparatively validate the low TT- and low ML rank methods on twenty-two 3D OCT images with the JPEG2000 and 3D SPIHT compression methods, as well as with no compression 2D bilateral filtering (BF), 2D median filtering (MF), and enhanced low-rank plus sparse matrix decomposition (ELRpSD) methods. For the CR&lt;10, the low Sp TT rank method with pe{0, 1/2, 2/3} yields either highest or comparable signal-to-noise ratio (SNR), and comparable or better contrast-to-noise ratio (CNR), mean segmentation errors (SEs) of retina layers and expert-based image quality score (EIQS) than original image and image compression methods. It compares favorably in terms of CNR, fairly in terms of SE and EIQS with the no image compression methods. Thus, for CR&lt;10 the low S2/3 TT rank approximation can be considered a good choice for visual inspection based diagnostics. For 2&lt;CR&lt;60, the low S1 ML rank method compares favorably in terms of SE with image compression methods and with 2D BF and ELRpSD. It is slightly inferior to 2D MF. Thus, for 2&lt;CR&lt;60, the low S1 ML rank approximation can be considered a good choice for segmentation based diagnostics either on-site or in the remote mode of operation.      
### 47.SNE-RoadSeg: Incorporating Surface Normal Information into Semantic Segmentation for Accurate Freespace Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.11351.pdf)
>  Freespace detection is an essential component of visual perception for self-driving cars. The recent efforts made in data-fusion convolutional neural networks (CNNs) have significantly improved semantic driving scene segmentation. Freespace can be hypothesized as a ground plane, on which the points have similar surface normals. Hence, in this paper, we first introduce a novel module, named surface normal estimator (SNE), which can infer surface normal information from dense depth/disparity images with high accuracy and efficiency. Furthermore, we propose a data-fusion CNN architecture, referred to as RoadSeg, which can extract and fuse features from both RGB images and the inferred surface normal information for accurate freespace detection. For research purposes, we publish a large-scale synthetic freespace detection dataset, named Ready-to-Drive (R2D) road dataset, collected under different illumination and weather conditions. The experimental results demonstrate that our proposed SNE module can benefit all the state-of-the-art CNNs for freespace detection, and our SNE-RoadSeg achieves the best overall performance among different datasets.      
### 48.Blind Inference of Eigenvector Centrality Rankings  [ :arrow_down: ](https://arxiv.org/pdf/2008.11330.pdf)
>  We consider the problem of estimating a network's eigenvector centrality only from data on the nodes, with no information about network topology. Leveraging the versatility of graph filters to model network processes, data supported on the nodes is modeled as a graph signal obtained via the output of a graph filter applied to white noise. We seek to simplify the downstream task of centrality ranking by bypassing network topology inference methods and, instead, inferring the centrality structure of the graph directly from the graph signals. To this end, we propose two simple algorithms for ranking a set of nodes connected by an unobserved set of edges. We derive asymptotic and non-asymptotic guarantees for these algorithms, revealing key features that determine the complexity of the task at hand. Finally, we illustrate the behavior of the proposed algorithms on synthetic and real-world datasets.      
### 49.Multi-Face: Self-supervised Multiview Adaptation for Robust Face Clustering in Videos  [ :arrow_down: ](https://arxiv.org/pdf/2008.11289.pdf)
>  Robust face clustering is a key step towards computational understanding of visual character portrayals in media. Face clustering for long-form content such as movies is challenging because of variations in appearance and lack of large-scale labeled video resources. However, local face tracking in videos can be used to mine samples belonging to same/different persons by examining the faces co-occurring in a video frame. In this work, we use this idea of self-supervision to harvest large amounts of weakly labeled face tracks in movies. We propose a nearest-neighbor search in the embedding space to mine hard examples from the face tracks followed by domain adaptation using multiview shared subspace learning. Our benchmarking on movie datasets demonstrate the robustness of multiview adaptation for face verification and clustering. We hope that the large-scale data resources developed in this work can further advance automatic character labeling in videos.      
### 50.Structural Systems Theory: an overview of the last 15 years  [ :arrow_down: ](https://arxiv.org/pdf/2008.11223.pdf)
>  In this paper, we provide an overview of the research conducted in the context of structural systems since the latest survey by Dion et al. in 2003. We systematically consider all the papers that cite this survey as well as the seminal work in this field that took place on and after the publication of the later survey, are published in peer-reviewed venues and in English. Structural systems theory deals with parametric systems where parameters might be unknown and, therefore, addresses the study of systems properties that depend only on the system's structure (or topology) described by the inter-dependencies between state variables. Remarkably, structural systems properties hold generically (i.e., almost always) under the assumption that parameters are independent. Therefore, it constitutes an approach to assess necessary conditions that systems should satisfy. In recent years, structural systems theory was applied to design systems that attain such properties, as well as to endure resilient/security and privacy properties. Furthermore, structural systems theory enables the formulation of such topics as combinatorial optimization problems, which allow us to understand their computational complexity and find algorithms that are efficiently deployed in the context of large-scale systems. In particular, we present an overview of how structural systems theory has been used in the context of linear time-invariant systems, as well as other dynamical models, for which a brief description of the different problem statements and solutions approaches are presented. Next, we describe recent variants of structural systems theory, as well as different applications of the classical and new approaches. Finally, we provide an overview of recent and future directions in this field.      
