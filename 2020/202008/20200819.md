# ArXiv eess --Wed, 19 Aug 2020
### 1.Energy-Optimal Control of a Submarine-Launched Cruise Missile  [ :arrow_down: ](https://arxiv.org/pdf/2008.08068.pdf)
>  A typical mission profile of submarine-launched cruise missiles begins with the launch phase which covers the motion of the missile from the launch to the water-exit and continues with the boost phase which lasts from the water-exit to the beginning of the cruise phase. In order to achieve the desired range of the launch and boost phases, efficient utilization of available energy which carries the missile to the beginning of the cruise phase is necessary. For this purpose, this study presents a new approach for energy-optimal control of the underwater and air motion of a submarine-launched cruise missile. In this approach, the aforementioned problem is modeled and solved as a minimum-effort optimal control problem. Then, the effects of initial and final conditions on energy need are investigated, and the optimal conditions that result with the minimum energy need are determined. Prior to the guidance and control design steps, six degrees of freedom (6 DOF) motion equations are derived and the hydrodynamic and aerodynamic parameters are retrieved. The nonlinear 6 DOF motion model is simplified and linearized before minimum-effort optimal control design part. Results of the designed guidance and control strategies are presented through the nonlinear 6 DOF simulations. Finally, some comments are made and future studies are mentioned based on theoretical and simulation studies.      
### 2.TactileSGNet: A Spiking Graph Neural Network for Event-based Tactile Object Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.08046.pdf)
>  Tactile perception is crucial for a variety of robot tasks including grasping and in-hand manipulation. New advances in flexible, event-driven, electronic skins may soon endow robots with touch perception capabilities similar to humans. These electronic skins respond asynchronously to changes (e.g., in pressure, temperature), and can be laid out irregularly on the robot's body or end-effector. However, these unique features may render current deep learning approaches such as convolutional feature extractors unsuitable for tactile learning. In this paper, we propose a novel spiking graph neural network for event-based tactile object recognition. To make use of local connectivity of taxels, we present several methods for organizing the tactile data in a graph structure. Based on the constructed graphs, we develop a spiking graph convolutional network. The event-driven nature of spiking neural network makes it arguably more suitable for processing the event-based data. Experimental results on two tactile datasets show that the proposed method outperforms other state-of-the-art spiking methods, achieving high accuracies of approximately 90\% when classifying a variety of different household objects.      
### 3.Algorithm on the Basis of One Monocular Video Delivers Excellently Valid and Reliable Gait Parameters Compared to a Gold-Standard Assessment Tool  [ :arrow_down: ](https://arxiv.org/pdf/2008.08045.pdf)
>  Despite its paramount importance for manifold use-cases, a sufficiently valid and reliable gait parameter measurement is still limited to the domain of high-tech gait laboratories in big clinics. This is mainly because the majority of gold standard assessment tools are very costly and complex in their respective setup and daily operation routines. Here, we demonstrate the excellent validity and test-retest repeatability of a novel gait assessment system which is built upon modern convolutional neuronal networks to extract three-dimensional skeleton joints from monocular frontal-view videos of walking humans. The present validity study is achieved in comparison to a previously validated pressure-sensitive walkway system (GAITRite, GS). All measured gait parameters showed excellent level of concurrent validity. This is proven by Inter-Class-Correlations possessing values between 0.92 and 0.99 for multiple walk trials, at normal and fast gait speeds. Furthermore, the average measure of difference between the two systems is below 5% of corresponding gait parameter mean value across all measured parameters (0.2% - 4.5%). The percentage error values of the assessed system in relation to GS are between 6% and 14% of corresponding gait parameter mean values, hence being significantly below the threshold of clinical acceptability (30%). The test-retest-repeatability yields ICC values between 0.87 and 0.95, being on the same level with the GS system. In conclusion, we are convinced that our results can pave the way for cost, space and operation effective gait analysis in the broad mainstream. Most sensor-based systems are costly, have to be operated by extensively trained personnel or possess considerable complexity (e.g. wearable sensors). In contrast, a sufficient video for the assessment method presented here can be acquired by anyone, without much training, via a smartphone camera.      
### 4.Deep Learning Based on Generative Adversarial and Convolutional Neural Networks for Financial Time Series Predictions  [ :arrow_down: ](https://arxiv.org/pdf/2008.08041.pdf)
>  In the big data era, deep learning and intelligent data mining technique solutions have been applied by researchers in various areas. Forecast and analysis of stock market data have represented an essential role in today's economy, and a significant challenge to the specialist since the market's tendencies are immensely complex, chaotic and are developed within a highly dynamic environment. There are numerous researches from multiple areas intending to take on that challenge, and Machine Learning approaches have been the focus of many of them. There are multiple models of Machine Learning algorithms been able to obtain competent outcomes doing that class of foresight. This paper proposes the implementation of a generative adversarial network (GAN), which is composed by a bi-directional Long short-term memory (LSTM) and convolutional neural network(CNN) referred as Bi-LSTM-CNN to generate synthetic data that agree with existing real financial data so the features of stocks with positive or negative trends can be retained to predict future trends of a stock. The novelty of this proposed solution that distinct from previous solutions is that this paper introduced the concept of a hybrid system (Bi-LSTM-CNN) rather than a sole LSTM model. It was collected data from multiple stock markets such as TSX, SHCOMP, KOSPI 200 and the S&amp;P 500, proposing an adaptative-hybrid system for trends prediction on stock market prices, and carried a comprehensive evaluation on several commonly utilized machine learning prototypes, and it is concluded that the proposed solution approach outperforms preceding models. Additionally, during the research stage from preceding works, gaps were found between investors and researchers who dedicated to the technical domain.      
### 5.The agnostic sampling transceiver  [ :arrow_down: ](https://arxiv.org/pdf/2008.08040.pdf)
>  Increasing capacity demands in the access networks require inventive concepts for the transmission and distribution of digital as well as analog signals over the same network. Here a new transceiver system, which is completely agnostic for the signals to be transmitted is presented. Nyquist sampling and time multiplexing of N phase and intensity modulated digital and analog channels with one single modulator, as well as the transmission and demultiplexing with another modulator have been demonstrated. The aggregate symbol rate corresponds to the modulator bandwidth and can be further increased by a modification of the setup. No high-speed electronic signal processing or high bandwidth photonics is required. Apart from its simplicity and the possibility to process high bandwidth signals with low bandwidth electronics and photonics, the method has the potential to be easily integrated into any platform and thus, might be a solution for the increasing data rates in future access networks.      
### 6.Spectral Processing of COVID-19 Time-Series Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.08039.pdf)
>  The presence of oscillations in aggregated COVID-19 data not only raises questions about the data's accuracy, it hinders understanding of the pandemic. A spectral analysis is presented, and the oscillations in the data are replicated using sinusoidal resynthesis. The precise behavior of the seven-day moving average is also discussed, specifically, the cause of its jaggedness and the phase error it introduces. In comparison, other filtering techniques and Fourier processing produce superior smoothing and have zero phase error. Both of these are presented, and they are extended to isolate several frequency ranges. This extracts some of the same short-term variability that is resynthesized, and it shows that fluctuations with periods between 8 and 21 days are present in U.S. mortality data.      
### 7.Short-term prediction of urban rail transit origin-destination flow: A channel-wise attentive split-convolutional neural network method  [ :arrow_down: ](https://arxiv.org/pdf/2008.08036.pdf)
>  Short-term origin-destination (OD) flow prediction in urban rail transit (URT) plays a crucial role in smart and real-time URT operation and management. Different from other short-term traffic forecasting methods, the short-term OD flow prediction possesses three unique characteristics. 1) data availability, real-time OD flow is not available during the prediction; 2) data dimensionality, the dimension of the OD flow is much higher than the cardinality of transportation networks; 3) data sparsity, URT OD flow is spatiotemporally sparse. There is a great need to develop novel OD flow forecasting method that explicitly considers the unique characteristics of the URT system. To this end, a channel-wise attentive split-convolutional neural network (CAS-CNN) is proposed. The proposed model consists of many novel components such as the channel-wise attention mechanism and split CNN. In particular, an inflow-gated mechanism is innovatively introduced to address the data availability issue. We further originally propose a masked loss function to solve the data dimensionality and data sparsity issues. The model interpretability is also discussed in detail. The CAS-CNN model is tested on two large-scale real-world datasets form Beijing Subway, and it outperforms the rest of benchmarking methods.The proposed model contributes to the development of short-term OD flow prediction, and it also lays the foundations of real-time URT operation and management.      
### 8.Predicting Coordinated Actuated Traffic Signal Change Times using LSTM Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.08035.pdf)
>  Vehicle acceleration and deceleration maneuvers at traffic signals results in significant fuel and energy consumption levels. Green light optimal speed advisory systems require reliable estimates of signal switching times to improve vehicle fuel efficiency. Obtaining these estimates is difficult for actuated signals where the length of each green indication changes to accommodate varying traffic conditions. This study details a four-step Long Short-Term Memory deep learning-based methodology that can be used to provide reasonable switching time estimates from green to red and vice versa while being robust to missing data. The four steps are data gathering, data preparation, machine learning model tuning, and model testing and evaluation. The input to the models included controller logic, signal timing parameters, time of day, traffic state from detectors, vehicle actuation data, and pedestrian actuation data. The methodology is applied and evaluated on data from an intersection in Northern Virginia. A comparative analysis is conducted between different loss functions including the mean squared error, mean absolute error, and mean relative error used in LSTM and a new loss function is proposed. The results show that while the proposed loss function outperforms conventional loss functions in terms of overall absolute error values, the choice of the loss function is dependent on the prediction horizon. In particular, the proposed loss function is outperformed by the mean relative error for very short prediction horizons and mean squared error for very long prediction horizons.      
### 9.Experimental Analysis on Variations and Accuracy of Crosstalk in Trench-Assisted Multi-core Fibers  [ :arrow_down: ](https://arxiv.org/pdf/2008.08034.pdf)
>  Space division multiplexing using multi-core fiber (MCF) is a promising solution to cope with the capacity crunch in standard single-mode fiber based optical communication systems. Nevertheless, the achievable capacity of MCF is limited by inter-core crosstalk (IC-XT). Many existing researches treat IC-XT as a static interference, however, recent research shows that IC-XT varies with time, wavelength and baud rate. This inherent stochastic feature requires a comprehensive characterization of the behaviour of MCF to its application in practical transmission systems and the theoretical understanding of IC-XT phenomenon. In this paper, we experimentally investigate the IC-XT behaviour of an 8-core trench-assisted MCF in a temperature-controlled environment, using popular modulation formats. We compare the measured results with the theoretical prediction to validate the analytical IC-XT models previously developed. Moreover, we explore the effects of the measurement configurations on the IC-XT accuracy and present an analysis on the IC-XT step distribution. Our results indicate that a number of transmission parameters have significant influence on the strength and volatility of IC-XT. Moreover, the averaging time of the power meter and the observation time window can affect the value of the observed IC-XT, the degrees of the effects vary with the type of the source signals.      
### 10.Self-supervised Denoising via Diffeomorphic Template Estimation: Application to Optical Coherence Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2008.08024.pdf)
>  Optical Coherence Tomography (OCT) is pervasive in both the research and clinical practice of Ophthalmology. However, OCT images are strongly corrupted by noise, limiting their interpretation. Current OCT denoisers leverage assumptions on noise distributions or generate targets for training deep supervised denoisers via averaging of repeat acquisitions. However, recent self-supervised advances allow the training of deep denoising networks using only repeat acquisitions without clean targets as ground truth, reducing the burden of supervised learning. Despite the clear advantages of self-supervised methods, their use is precluded as OCT shows strong structural deformations even between sequential scans of the same subject due to involuntary eye motion. Further, direct nonlinear alignment of repeats induces correlation of the noise between images. In this paper, we propose a joint diffeomorphic template estimation and denoising framework which enables the use of self-supervised denoising for motion deformed repeat acquisitions, without empirically registering their noise realizations. Strong qualitative and quantitative improvements are achieved in denoising OCT images, with generic utility in any imaging modality amenable to multiple exposures.      
### 11.Offloading Optimization in Edge Computing for Deep Learning Enabled Target Tracking by Internet-of-UAVs  [ :arrow_down: ](https://arxiv.org/pdf/2008.08001.pdf)
>  The empowering unmanned aerial vehicles (UAVs) have been extensively used in providing intelligence such as target tracking. In our field experiments, a pre-trained convolutional neural network (CNN) is deployed at the UAV to identify a target (a vehicle) from the captured video frames and enable the UAV to keep tracking. However, this kind of visual target tracking demands a lot of computational resources due to the desired high inference accuracy and stringent delay requirement. This motivates us to consider offloading this type of deep learning (DL) tasks to a mobile edge computing (MEC) server due to limited computational resource and energy budget of the UAV, and further improve the inference accuracy. Specifically, we propose a novel hierarchical DL tasks distribution framework, where the UAV is embedded with lower layers of the pre-trained CNN model, while the MEC server with rich computing resources will handle the higher layers of the CNN model. An optimization problem is formulated to minimize the weighted-sum cost including the tracking delay and energy consumption introduced by communication and computing of the UAVs, while taking into account the quality of data (e.g., video frames) input to the DL model and the inference errors. Analytical results are obtained and insights are provided to understand the tradeoff between the weighted-sum cost and inference error rate in the proposed framework. Numerical results demonstrate the effectiveness of the proposed offloading framework.      
### 12.Comparison of Convolutional neural network training parameters for detecting Alzheimers disease and effect on visualization  [ :arrow_down: ](https://arxiv.org/pdf/2008.07981.pdf)
>  Convolutional neural networks (CNN) have become a powerful tool for detecting patterns in image data. Recent papers report promising results in the domain of disease detection using brain MRI data. Despite the high accuracy obtained from CNN models for MRI data so far, almost no papers provided information on the features or image regions driving this accuracy as adequate methods were missing or challenging to apply. Recently, the toolbox iNNvestigate has become available, implementing various state of the art methods for deep learning visualizations. Currently, there is a great demand for a comparison of visualization algorithms to provide an overview of the practical usefulness and capability of these algorithms. <br>Therefore, this thesis has two goals: 1. To systematically evaluate the influence of CNN hyper-parameters on model accuracy. 2. To compare various visualization methods with respect to the quality (i.e. randomness/focus, soundness).      
### 13.High Accurate Time-of-Arrival Estimation with Fine-Grained Feature Generation for Internet-of-Things Applications  [ :arrow_down: ](https://arxiv.org/pdf/2008.07932.pdf)
>  Conventional schemes often require extra reference signals or more complicated algorithms to improve the time-of-arrival (TOA) estimation accuracy. However, in this letter, we propose to generate fine-grained features from the full band and resource block (RB) based reference signals, and calculate the cross-correlations accordingly to improve the observation resolution as well as the TOA estimation results. Using the spectrogram-like cross-correlation feature map, we apply the machine learning technology with decoupled feature extraction and fitting to understand the variations in the time and frequency domains and project the features directly into TOA results. Through numerical examples, we show that the proposed high accurate TOA estimation with fine-grained feature generation can achieve at least 51% root mean square error (RMSE) improvement in the static propagation environments and 38 ns median TOA estimation errors for multipath fading environments, which is equivalently 36% and 25% improvement if compared with the existing MUSIC and ESPRIT algorithms, respectively.      
### 14.A Spanning Tree-based Genetic Algorithm for Distribution Network Reconfiguration  [ :arrow_down: ](https://arxiv.org/pdf/2008.07908.pdf)
>  This paper presents a spanning tree-based genetic algorithm (GA) for the reconfiguration of electrical distribution systems with the objective of minimizing active power losses. Due to low voltage levels at distribution systems, power losses are high and sensitive to system configuration. Therefore, optimal reconfiguration is an important factor in the operation of distribution systems to minimize active power losses. Smart and automated electric distribution systems are able to reconfigure as a response to changes in load levels to minimize active power losses. The proposed method searches spanning trees of potential configurations and finds the optimal spanning tree using a genetic algorithm in two steps. In the first step, all invalid combinations of branches and tie-lines (i.e., switching combinations that do not provide power to some of loads or violate the radiality and connectivity conditions) generated by initial population of GA are filtered out with the help of spanning-tree search algorithm. In the second step, power flow analyses are performed only for combinations that form spanning trees. The optimal configuration is then determined based on the amount of active power losses (optimal configuration is the one that results in minimum power losses). The proposed method is implemented on several systems including the well-known 33-node and 69-node systems. The results show that the proposed method is accurate and efficient in comparison with existing methods.      
### 15.Optimal Sizing and Siting of Multi-purpose Utility-scale Shared Energy Storage Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07900.pdf)
>  This paper proposes a nondominated sorting genetic algorithm II (NSGA-II) based approach to determine optimal or near-optimal sizing and siting of multi-purpose (e.g., voltage regulation and loss minimization), community-based, utility-scale shared energy storage in distribution systems with high penetration of solar photovoltaic energy systems. Small-scale behind-the-meter (BTM) batteries are expensive, not fully utilized, and their net value is difficult to generalize and to control for grid services. On the other hand, utility-scale shared energy storage (USSES) systems have the potential to provide primary (e.g., demand-side management, deferral of system upgrade, and demand charge reduction) as well as secondary (e.g., frequency regulation, resource adequacy, and energy arbitrage) grid services. Under the existing cost structure, storage deployed only for primary purpose cannot justify the economic benefit to owners. However, the delivery of storage for primary service utilizes only 1-50\% of total battery lifetime capacity. In the proposed approach, for each candidate set of locations and sizes, the contribution of USSES systems to grid voltage deviation and power loss are evaluated and diverse Pareto-optimal front is created. USSES systems are dispersed through a new chromosome representation approach. From the list of Pareto-optimal front, distribution system planners will have the opportunity to select appropriate locations based on desired objectives. The proposed approach is demonstrated on the IEEE 123-node distribution test feeder with utility-scale PV and USSES systems.      
### 16.Accelerometric Method for Cuffless Continuous Blood Pressure Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2008.07899.pdf)
>  Pulse transit time (PTT) has been widely used for cuffless blood pressure (BP) measurement. But, it requires more than one cardiovascular signals involving more than one sensing device. In this paper, we propose a method for continuous cuffless blood pressure measurement with the help of left ventricular ejection time (LVET). The LVET is estimated using a signal obtained through a micro-electromechanical system (MEMS)-based accelerometric sensor. The sensor acquires a seismocardiogram (SCG) signal at the chest surface, and the LVET information is extracted. Both systolic blood pressure (SBP) and diastolic blood pressure (DBP) are estimated by calibrating the system with the original arterial blood pressure values of the subjects. The proposed method is evaluated using different quantitative measures on the signals collected from ten subjects under the supine position. The performance of the proposed method is also compared with two earlier approaches, where PTT intervals are estimated from electrocardiogram (ECG)-photoplethysmogram (PPG) and SCG-PPG, respectively. The performance results clearly show that the proposed method is comparable with the state-of-the-art methods. Also, the computed blood pressure is compared with the original one, measured through a CNAP system. It gives the mean errors of the estimated systolic BP and diastolic BP within the range of -0.19 +/- 3.3 mmHg and -1.29 +/- 2.6 mmHg, respectively. The mean absolute errors for systolic BP and diastolic BP are 3.2 mmHg and 2.6 mmHg, respectively. The accuracy of BPs estimated from the proposed method satisfies the requirements of the IEEE standard of 5 +/- 8 mmHg deviation, and thus, it may be used for ubiquitous long term blood pressure monitoring.      
### 17.A Sensitivity-based Approach for Optimal Siting of Distributed Energy Resources  [ :arrow_down: ](https://arxiv.org/pdf/2008.07896.pdf)
>  This paper presents a sensitivity-based approach for the placement of distributed energy resources (DERs) in power systems. The approach is based on the fact that most planning studies utilize some form of optimization, and solutions to these optimization problems provide insights into the sensitivity of many system variables to operating conditions and constraints. However, most of the existing sensitivity-based planning criteria do not capture ranges of effectiveness of these solutions (i.e., ranges of the effectiveness of Lagrange multipliers). The proposed method detects the ranges of the effectiveness of Lagrange multipliers and uses them to determine optimal solution alternatives. Profiles for existing generation and loads, and transmission constraints are taken into consideration. The proposed method is used to determine the impacts of DERs at different locations, in the presence of a stochastic element (load variability). This method consists of sequentially calculating Lagrange multipliers of the dual solution of the optimization problem for various load buses for all load scenarios. Optimal sizes and sites of resources are jointly determined in a sequential manner based on the validity of active constraints. The effectiveness of the proposed method is demonstrated through several case studies on various test systems including the IEEE reliability test system (IEEE RTS), the IEEE 14, and 30 bus systems. In comparison with conventional sensitivity-based approaches (i.e., without considering ranges of validity of Lagrange multipliers), the proposed approach provides more accurate results for active constraints.      
### 18.Elimination of Central Artefacts of L-SPECT with Modular Partial Ring Detectors by Shifting Center of Scanning  [ :arrow_down: ](https://arxiv.org/pdf/2008.07893.pdf)
>  The Lightfield Single Photon Emission Computed Tomography (L-SPECT) system is developed to overcome some of the drawbacks in conventional SPECT by applying the idea of plenoptic imaging. This system displayed improved performance in terms of reduced information loss and scanning time when compared to the SPECT system which has a conventional collimator. The SPECT system is transformed into L-SPECT system by replacing the conventional collimators with micro-range multi-pinhole arrays. The field of view (FOV) of the L-SPECT system is enhanced by reshaping the detector head into ring-type by tiling small detector modules. The L-SPECT system with modular partial ring detectors (MPRD L-SPECT) exhibits cylindrical artefacts during volumetric reconstruction. Hence, here the work is focused to remove the cylindrical artefacts in the reconstruction of the examined objects by changing the scanning orbit. The enhancement is done such that the center of scanning of the L-SPECT system with MPRD L-SPECT is shifted at different values. The reconstruction quality of MPRD L-SPECT with and without center shifting is evaluated in terms of the Full Width at Half Maximum (FWHM) and Modulation Transfer Function (MTF). Moreover, the visual comparison is also examined. The results indicate that center shifting of MPRD L-SPECT overcomes the problem of the central artefact with improved FWHM. By increasing MPRD's scanning center shift gap, the spatial resolution can be further improved.      
### 19.Grading Loss: A Fracture Grade-based Metric Loss for Vertebral Fracture Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.07831.pdf)
>  Osteoporotic vertebral fractures have a severe impact on patients' overall well-being but are severely under-diagnosed. These fractures present themselves at various levels of severity measured using the Genant's grading scale. Insufficient annotated datasets, severe data-imbalance, and minor difference in appearances between fractured and healthy vertebrae make naive classification approaches result in poor discriminatory performance. Addressing this, we propose a representation learning-inspired approach for automated vertebral fracture detection, aimed at learning latent representations efficient for fracture detection. Building on state-of-art metric losses, we present a novel Grading Loss for learning representations that respect Genant's fracture grading scheme. On a publicly available spine dataset, the proposed loss function achieves a fracture detection F1 score of 81.5%, a 10% increase over a naive classification baseline.      
### 20.On the Dynamic Range of Digital Correlative Time Domain Radio Channel Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2008.07805.pdf)
>  For the concise characterization of radio channel measurement setups, different performance metrics like dynamic range and maximum measurable path loss have been proposed. This work further elaborates on these metrics and discusses practical bounds for the class of digital correlative time domain channel sounders. From a general instrumentation model, relevant nonidealities and the resulting effects on measurement performance are identified. These considerations yield the tools to select appropriate measurement parameters for a given scenario and assess the overall performance of a particular measurement setup. The findings are further illustrated with data acquired from an instrument-based measurement setup.      
### 21.CinC-GAN for Effective F0 prediction for Whisper-to-Normal Speech Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2008.07788.pdf)
>  Recently, Generative Adversarial Networks (GAN)-based methods have shown remarkable performance for the Voice Conversion and WHiSPer-to-normal SPeeCH (WHSP2SPCH) conversion. One of the key challenges in WHSP2SPCH conversion is the prediction of fundamental frequency (F0). Recently, authors have proposed state-of-the-art method Cycle-Consistent Generative Adversarial Networks (CycleGAN) for WHSP2SPCH conversion. The CycleGAN-based method uses two different models, one for Mel Cepstral Coefficients (MCC) mapping, and another for F0 prediction, where F0 is highly dependent on the pre-trained model of MCC mapping. This leads to additional non-linear noise in predicted F0. To suppress this noise, we propose Cycle-in-Cycle GAN (i.e., CinC-GAN). It is specially designed to increase the effectiveness in F0 prediction without losing the accuracy of MCC mapping. We evaluated the proposed method on a non-parallel setting and analyzed on speaker-specific, and gender-specific tasks. The objective and subjective tests show that CinC-GAN significantly outperforms the CycleGAN. In addition, we analyze the CycleGAN and CinC-GAN for unseen speakers and the results show the clear superiority of CinC-GAN.      
### 22.Tdcgan: Temporal Dilated Convolutional Generative Adversarial Network for End-to-end Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2008.07787.pdf)
>  In this paper, in order to further deal with the performance degradation caused by ignoring the phase information in conventional speech enhancement systems, we proposed a temporal dilated convolutional generative adversarial network (TDCGAN) in the end-to-end based speech enhancement architecture. For the first time, we introduced the temporal dilated convolutional network with depthwise separable convolutions into the GAN structure so that the receptive field can be greatly increased without increasing the number of parameters. We also first explored the effect of signal-to-noise ratio (SNR) penalty item as regularization of the loss function of generator on improving the SNR of enhanced speech. The experimental results demonstrated that our proposed method outperformed the state-of-the-art end-to-end GAN-based speech enhancement. Moreover, compared with previous GAN-based methods, the proposed TDCGAN could greatly decreased the number of parameters. As expected, the work also demonstrated that the SNR penalty item as regularization was more effective than $L1$ on improving the SNR of enhanced speech.      
### 23.Fully automated deep learning based segmentation of normal, infarcted and edema regions from multiple cardiac MRI sequences  [ :arrow_down: ](https://arxiv.org/pdf/2008.07770.pdf)
>  Myocardial characterization is essential for patients with myocardial infarction and other myocardial diseases, and the assessment is often performed using cardiac magnetic resonance (CMR) sequences. In this study, we propose a fully automated approach using deep convolutional neural networks (CNN) for cardiac pathology segmentation, including left ventricular (LV) blood pool, right ventricular blood pool, LV normal myocardium, LV myocardial edema (ME) and LV myocardial scars (MS). The input to the network consists of three CMR sequences, namely, late gadolinium enhancement (LGE), T2 and balanced steady state free precession (bSSFP). The proposed approach utilized the data provided by the MyoPS challenge hosted by MICCAI 2020 in conjunction with STACOM. The training set for the CNN model consists of images acquired from 25 cases, and the gold standard labels are provided by trained raters and validated by radiologists. The proposed approach introduces a data augmentation module, linear encoder and decoder module and a network module to increase the number of training samples and improve the prediction accuracy for LV ME and MS. The proposed approach is evaluated by the challenge organizers with a test set including 20 cases and achieves a mean dice score of $46.8\%$ for LV MS and $55.7\%$ for LV ME+MS      
### 24.Open-Loop Distributed Beamforming Using Scalable High Accuracy Localization  [ :arrow_down: ](https://arxiv.org/pdf/2008.07748.pdf)
>  We present a distributed antenna array supporting open-loop distributed beamforming at 1.5 GHz. Based on a scalable, high-accuracy internode ranging technique, we demonstrate open-loop beamforming experiments using three transmitting nodes. To support distributed beamforming without feedback from the destination, the relative positions of the nodes in the distributed array must be known with accuracies below $\lambda/15$ of the beamforming carrier frequency to ensure that the array maintains at least 90\% coherent beamforming gain at the receive location. For operations in the microwave range, this leads to range estimation accuracies of centimeters or lower. We present a scalable, high-accuracy waveform and new approaches to refine range measurements to significantly improve the estimation accuracy. Using this waveform with a three-node array, we demonstrate high-accuracy ranging simultaneously between multiple nodes, from which phase corrections on two secondary nodes are implemented to maintain beamforming with the primary node, thereby supporting open-loop distributed beamforming. Upon movement of the nodes, the range estimation is used to dynamically update the phase correction, maintaining beamforming as the nodes move. We show the first open-loop distributed beamforming at 1.5 GHz with two-node and three-node arrays, demonstrating the ability to implement and maintain phase-based beamforming without feedback from the destination.      
### 25.Massively Distributed Antenna Systems with Non-Ideal Optical Fiber Front-hauls: A Promising Technology for 6G Wireless Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07745.pdf)
>  Employing massively distributed antennas brings radio access points (RAPs) closer to users, thus enables aggressive spectrum reuse that can bridge gaps between the scarce spectrum resource and extremely high connection densities in future wireless systems. Examples include cloud radio access network (C-RAN), ultra-dense network (UDN), and cell-free massive multiple-input multiple-output (MIMO) systems. These systems are usually designed in the form of fiber-wireless communications (FWC), where distributed antennas or RAPs are connected to a central unit (CU) through optical front-hauls. A large number of densely deployed antennas or RAPs requires an extensive infrastructure of optical front-hauls. Consequently, the cost, complexity, and power consumption of the network of optical front-hauls may dominate the performance of the entire system. This article provides an overview and outlook on the architecture, modeling, design, and performance of massively distributed antenna systems with non-ideal optical front-hauls. Complex interactions between optical front-hauls and wireless access links require optimum designs across the optical and wireless domains by jointly exploiting their unique characteristics. It is demonstrated that systems with analog radio-frequency-over-fiber (RFoF) links outperform their baseband-over-fiber (BBoF) or intermediate-frequency-over-fiber (IFoF) counterparts for systems with shorter fiber length and more RAPs, which are all desired properties for future wireless communication systems.      
### 26.UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods and Results  [ :arrow_down: ](https://arxiv.org/pdf/2008.07742.pdf)
>  This paper is the report of the first Under-Display Camera (UDC) image restoration challenge in conjunction with the RLQ workshop at ECCV 2020. The challenge is based on a newly-collected database of Under-Display Camera. The challenge tracks correspond to two types of display: a 4k Transparent OLED (T-OLED) and a phone Pentile OLED (P-OLED). Along with about 150 teams registered the challenge, eight and nine teams submitted the results during the testing phase for each track. The results in the paper are state-of-the-art restoration performance of Under-Display Camera Restoration. Datasets and paper are available at <a class="link-external link-https" href="https://yzhouas.github.io/projects/UDC/udc.html" rel="external noopener nofollow">this https URL</a>.      
### 27.A Real-time Robot-based Auxiliary System for Risk Evaluation of COVID-19 Infection  [ :arrow_down: ](https://arxiv.org/pdf/2008.07695.pdf)
>  In this paper, we propose a real-time robot-based auxiliary system for risk evaluation of COVID-19 infection. It combines real-time speech recognition, temperature measurement, keyword detection, cough detection and other functions in order to convert live audio into actionable structured data to achieve the COVID-19 infection risk assessment function. In order to better evaluate the COVID-19 infection, we propose an end-to-end method for cough detection and classification for our proposed system. It is based on real conversation data from human-robot, which processes speech signals to detect cough and classifies it if detected. The structure of our model are maintained concise to be implemented for real-time applications. And we further embed this entire auxiliary diagnostic system in the robot and it is placed in the communities, hospitals and supermarkets to support COVID-19 testing. The system can be further leveraged within a business rules engine, thus serving as a foundation for real-time supervision and assistance applications. Our model utilizes a pretrained, robust training environment that allows for efficient creation and customization of customer-specific health states.      
### 28.Adversarial Attack and Defense Strategies for Deep Speaker Recognition Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07685.pdf)
>  Robust speaker recognition, including in the presence of malicious attacks, is becoming increasingly important and essential, especially due to the proliferation of several smart speakers and personal agents that interact with an individual's voice commands to perform diverse, and even sensitive tasks. Adversarial attack is a recently revived domain which is shown to be effective in breaking deep neural network-based classifiers, specifically, by forcing them to change their posterior distribution by only perturbing the input samples by a very small amount. Although, significant progress in this realm has been made in the computer vision domain, advances within speaker recognition is still limited. The present expository paper considers several state-of-the-art adversarial attacks to a deep speaker recognition system, employing strong defense methods as countermeasures, and reporting on several ablation studies to obtain a comprehensive understanding of the problem. The experiments show that the speaker recognition systems are vulnerable to adversarial attacks, and the strongest attacks can reduce the accuracy of the system from 94% to even 0%. The study also compares the performances of the employed defense methods in detail, and finds adversarial training based on Projected Gradient Descent (PGD) to be the best defense method in our setting. We hope that the experiments presented in this paper provide baselines that can be useful for the research community interested in further studying adversarial robustness of speaker recognition systems.      
### 29.Deep Learning Based Source Separation Applied To Choir Ensembles  [ :arrow_down: ](https://arxiv.org/pdf/2008.07645.pdf)
>  Choral singing is a widely practiced form of ensemble singing wherein a group of people sing simultaneously in polyphonic harmony. The most commonly practiced setting for choir ensembles consists of four parts; Soprano, Alto, Tenor and Bass (SATB), each with its own range of fundamental frequencies (F$0$s). The task of source separation for this choral setting entails separating the SATB mixture into the constituent parts. Source separation for musical mixtures is well studied and many deep learning based methodologies have been proposed for the same. However, most of the research has been focused on a typical case which consists in separating vocal, percussion and bass sources from a mixture, each of which has a distinct spectral structure. In contrast, the simultaneous and harmonic nature of ensemble singing leads to high structural similarity and overlap between the spectral components of the sources in a choral mixture, making source separation for choirs a harder task than the typical case. This, along with the lack of an appropriate consolidated dataset has led to a dearth of research in the field so far. In this paper we first assess how well some of the recently developed methodologies for musical source separation perform for the case of SATB choirs. We then propose a novel domain-specific adaptation for conditioning the recently proposed U-Net architecture for musical source separation using the fundamental frequency contour of each of the singing groups and demonstrate that our proposed approach surpasses results from domain-agnostic architectures.      
### 30.Control Node Selection Algorithm for Nonlinear Dynamic Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.07640.pdf)
>  The coupled problems of selecting control nodes and designing control actions for nonlinear network dynamics are fundamental scientific problems with applications in many diverse fields. These problems are thoroughly studied for linear dynamics; however, in spite of a number of open research questions, methods for nonlinear network dynamics are less developed. As observed by various studies, the prevailing graph-based controllability approaches for selecting control nodes might result in significantly suboptimal control performance for nonlinear dynamics. Herein we present a new, intuitive, and simple method for simultaneous control node selection and control sequence design for complex networks with nonlinear dynamics. The method is developed by incorporating the control node selection problem into an open-loop predictive control cost function and by solving the resulting mixed-integer optimization problem using a mesh adaptive direct search method. The developed framework is numerically robust and can deal with stiff networks, networks with non-smooth dynamics, as well as with control and actuator constraints. Good numerical performance of the method is demonstrated by testing it on prototypical Duffing oscillator and associative memory networks. The developed codes that can easily be adapted to models of other complex systems are available online.      
### 31.Bridging the Gap between Optimal Trajectory Planning and Safety-Critical Control with Applications to Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.07632.pdf)
>  We address the problem of optimizing the performance of a dynamic system while satisfying hard safety constraints at all times. Implementing an optimal control solution is limited by the computational cost required to derive it in real time, especially when constraints become active, as well as the need to rely on simple linear dynamics, simple objective functions, and ignoring noise. The recently proposed Control Barrier Function (CBF) method may be used for safety-critical control at the expense of sub-optimal performance. In this paper, we develop a real-time control framework that combines optimal trajectories generated through optimal control with the computationally efficient CBF method providing safety guarantees. We use Hamiltonian analysis to obtain a tractable optimal solution for a linear or linearized system, then employ High Order CBFs (HOCBFs) and Control Lyapunov Functions (CLFs) to account for constraints with arbitrary relative degrees and to track the optimal state, respectively. We further show how to deal with noise in arbitrary relative degree systems. The proposed framework is then applied to the optimal traffic merging problem for Connected and Automated Vehicles (CAVs) where the objective is to jointly minimize the travel time and energy consumption of each CAV subject to speed, acceleration, and speed-dependent safety constraints. In addition, when considering more complex objective functions, nonlinear dynamics and passenger comfort requirements for which analytical optimal control solutions are unavailable, we adapt the HOCBF method to such problems. Simulation examples are included to compare the performance of the proposed framework to optimal solutions (when available) and to a baseline provided by human-driven vehicles with results showing significant improvements in all metrics.      
### 32.A Deep Network for Joint Registration and Reconstruction of Images with Pathologies  [ :arrow_down: ](https://arxiv.org/pdf/2008.07628.pdf)
>  Registration of images with pathologies is challenging due to tissue appearance changes and missing correspondences caused by the pathologies. Moreover, mass effects as observed for brain tumors may displace tissue, creating larger deformations over time than what is observed in a healthy brain. Deep learning models have successfully been applied to image registration to offer dramatic speed up and to use surrogate information (e.g., segmentations) during training. However, existing approaches focus on learning registration models using images from healthy patients. They are therefore not designed for the registration of images with strong pathologies for example in the context of brain tumors, and traumatic brain injuries. In this work, we explore a deep learning approach to register images with brain tumors to an atlas. Our model learns an appearance mapping from images with tumors to the atlas, while simultaneously predicting the transformation to atlas space. Using separate decoders, the network disentangles the tumor mass effect from the reconstruction of quasi-normal images. Results on both synthetic and real brain tumor scans show that our approach outperforms cost function masking for registration to the atlas and that reconstructed quasi-normal images can be used for better longitudinal registrations.      
### 33.Speech Recognition using EEG signals recorded using dry electrodes  [ :arrow_down: ](https://arxiv.org/pdf/2008.07621.pdf)
>  In this paper, we demonstrate speech recognition using electroencephalography (EEG) signals obtained using dry electrodes on a limited English vocabulary consisting of three vowels and one word using a deep learning model. We demonstrate a test accuracy of 79.07 percent on a subset vocabulary consisting of two English vowels. Our results demonstrate the feasibility of using EEG signals recorded using dry electrodes for performing the task of speech recognition.      
### 34.Incorporating Broad Phonetic Information for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2008.07618.pdf)
>  In noisy conditions, knowing speech contents facilitates listeners to more effectively suppress background noise components and to retrieve pure speech signals. Previous studies have also confirmed the benefits of incorporating phonetic information in a speech enhancement (SE) system to achieve better denoising performance. To obtain the phonetic information, we usually prepare a phoneme-based acoustic model, which is trained using speech waveforms and phoneme labels. Despite performing well in normal noisy conditions, when operating in very noisy conditions, however, the recognized phonemes may be erroneous and thus misguide the SE process. To overcome the limitation, this study proposes to incorporate the broad phonetic class (BPC) information into the SE process. We have investigated three criteria to build the BPC, including two knowledge-based criteria: place and manner of articulatory and one data-driven criterion. Moreover, the recognition accuracies of BPCs are much higher than that of phonemes, thus providing more accurate phonetic information to guide the SE process under very noisy conditions. Experimental results demonstrate that the proposed SE with the BPC information framework can achieve notable performance improvements over the baseline system and an SE system using monophonic information in terms of both speech quality intelligibility on the TIMIT dataset.      
### 35.DeepSlicing: Deep Reinforcement Learning Assisted Resource Allocation for Network Slicing  [ :arrow_down: ](https://arxiv.org/pdf/2008.07614.pdf)
>  Network slicing enables multiple virtual networks run on the same physical infrastructure to support various use cases in 5G and beyond. These use cases, however, have very diverse network resource demands, e.g., communication and computation, and various performance metrics such as latency and throughput. To effectively allocate network resources to slices, we propose DeepSlicing that integrates the alternating direction method of multipliers (ADMM) and deep reinforcement learning (DRL). DeepSlicing decomposes the network slicing problem into a master problem and several slave problems. The master problem is solved based on convex optimization and the slave problem is handled by DRL method which learns the optimal resource allocation policy. The performance of the proposed algorithm is validated through network simulations.      
### 36.SWAN: Swarm-Based Low-Complexity Scheme for PAPR Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2008.07600.pdf)
>  Cyclically shifted partial transmit sequences (CSPTS) has conventionally been used in SISO systems for PAPR reduction of OFDM signals. Compared to other techniques, CS-PTS attains superior performance. Nevertheless, due to the exhaustive search requirement, it demands excessive computational complexity. In this paper, we adapt CS-PTS to operate in a MIMO framework, where singular value decomposition (SVD) precoding is employed. We also propose SWAN, a novel optimization method based on swarm intelligence to circumvent the exhaustive search. SWAN not only provides a significant reduction in computational complexity, but it also attains a fair balance between optimality and complexity. Through simulations, we show that SWAN achieves near-optimal performance at a much lower complexity than other competing approaches.      
### 37.Nonlinear Attitude Filter on SO(3): Fast Adaptation and Robustness  [ :arrow_down: ](https://arxiv.org/pdf/2008.07595.pdf)
>  Nonlinear attitude filters have been recognized to have simpler structure and better tracking performance when compared with Gaussian attitude filters and other methods of attitude determination. A key element of nonlinear attitude filter design is the selection of error criteria. The conventional design of nonlinear attitude filters has a trade-off between fast adaptation and robustness. In this work, a new functional approach based on fuzzy rules for on-line continuous tuning of the nonlinear attitude filter adaptation gain is proposed. The input and output membership functions are optimally tuned using artificial bee colony optimization algorithm taking into account both attitude error and rate of change of attitude error. The proposed approach results of high adaptation gain at large error and small adaptation gain at small error. Thereby, the proposed approach allows fast convergence properties with high measures of robustness. The simulation results demonstrate that the proposed approach offers robust and high convergence capabilities against large error in initialization and uncertain measurements.      
### 38.Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07588.pdf)
>  Deep learning motivated by convolutional neural networks has been highly successful in a range of medical imaging problems like image classification, image segmentation, image synthesis etc. However for validation and interpretability, not only do we need the predictions made by the model but also how confident it is while making those predictions. This is important in safety critical applications for the people to accept it. In this work, we used an encoder decoder architecture based on variational inference techniques for segmenting brain tumour images. We compare different backbones architectures like U-Net, V-Net and FCN as sampling data from the conditional distribution for the encoder. We evaluate our work on the publicly available BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over Union (IOU) as the evaluation metrics. Our model outperforms previous state of the art results while making use of uncertainty quantification in a principled bayesian manner.      
### 39.Anatomy-Aware Cardiac Motion Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07579.pdf)
>  Cardiac motion estimation is critical to the assessment of cardiac function. Myocardium feature tracking (FT) can directly estimate cardiac motion from cine MRI, which requires no special scanning procedure. However, current deep learning-based FT methods may result in unrealistic myocardium shapes since the learning is solely guided by image intensities without considering anatomy. On the other hand, motion estimation through learning is challenging because ground-truth motion fields are almost impossible to obtain. In this study, we propose a novel Anatomy-Aware Tracker (AATracker) for cardiac motion estimation that preserves anatomy by weak supervision. A convolutional variational autoencoder (VAE) is trained to encapsulate realistic myocardium shapes. A baseline dense motion tracker is trained to approximate the motion fields and then refined to estimate anatomy-aware motion fields under the weak supervision from the VAE. We evaluate the proposed method on long-axis cardiac cine MRI, which has more complex myocardium appearances and motions than short-axis. Compared with other methods, AATracker significantly improves the tracking performance and provides visually more realistic tracking results, demonstrating the effectiveness of the proposed weakly-supervision scheme in cardiac motion estimation.      
### 40.Modeling of Natural Disasters and Extreme Events in Power System Resilience Enhancement and Evaluation Methods  [ :arrow_down: ](https://arxiv.org/pdf/2008.07560.pdf)
>  The frequency of disruptive and newly emerging threats (e.g. man-made attacks--cyber and physical attacks; extreme natural events--hurricanes, earthquakes, and floods) has escalated dramatically in the last decade. Impacts of these events are very severe ranging from long power outage duration, major power system equipment (e.g. power generation plants, transmission and distribution lines, and substation) destruction, and complete blackout. Accurate modeling of these events is vitally important as they serve as mathematical tools for the assessment and evaluation of various operations and planning investment strategies to harden power systems against these events. This paper provides a comprehensive and critical review of current practices in the modeling of extreme events, system components, and system response for resilience evaluation and enhancement, which is a very important stepping stone toward the development of complete, accurate, and computationally attractive modeling techniques. The paper starts with reviewing existing technologies to model the propagation of extreme events and then discusses the approaches used to model impacts of these events on power system components and system response. This paper also discusses the research gaps and associated challenges, and potential solutions to the limitations of the existing modeling approaches.      
### 41.Sizing of Movable Energy Resources for Service Restoration and Reliability Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2008.07557.pdf)
>  The frequency of extreme events (e.g., hurricanes, earthquakes, and floods) and man-made attacks (cyber and physical attacks) has increased dramatically in recent years. These events have severely impacted power systems ranging from long outage times to major equipment (e.g., substations, transmission lines, power plants, and distribution system) destruction. Distribution system failures and outages are major contributors to power supply interruptions. Network reconfiguration and movable energy resources (MERs) can play a vital role in supplying loads during and after contingencies. This paper proposes a two-stage strategy to determine the minimum sizes of MERs with network reconfiguration for distribution service restoration and supplying local and isolated loads. Sequential Monte Carlo simulations are used to model the outages of distribution system components. After a contingency, the first stage determines the network reconfiguration based on the spanning tree search algorithm. In the second stage, if some system loads cannot be fed by network reconfiguration, MERs are deployed and the optimal routes to reach isolated areas are determined based on the DSPA. The traveling time obtained from the DSPA is incorporated with the proposed sequential Monte Carlo simulation-based approach to determine the sizes of MERs. The proposed method is applied on several distribution systems including the IEEE-13 and IEEE-123 node test feeders. The results show that network reconfiguration can reduce the required sizes of MERs to supply isolated areas.      
### 42.On the Complexity Reduction of Uplink Sparse Code Multiple Access for Spatial Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07556.pdf)
>  Multi-user spatial modulation (SM) assisted by sparse code multiple access (SCMA) has been recently proposed to provide uplink high spectral efficiency transmission. The message passing algorithm (MPA) is employed to detect the transmitted signals, which suffers from high complexity. This paper proposes three low-complexity algorithms for the first time to the SM-SCMA. The first algorithm is referred to as successive user detection (SUD), while the second algorithm is the modified version of SUD, namely modified SUD (MSUD). Then, for the first time, the tree-search of the SM-SCMA is constructed. Based on that tree-search, another variant of the sphere decoder (SD) is proposed for the SM-SCMA, referred to as fixed-complexity SD (FCSD). SUD provides a benchmark for decoding complexity at the expense of bit-error-rate (BER) performance. Further, MSUD slightly increases the complexity of SUD with a significant improvement in BER performance. Finally, FCSD provides a near-optimum BER with a considerable reduction of the complexity compared to the MPA decoder and also supports parallel hardware implementation. The proposed algorithms provide flexible design choices for practical implementation based on system design demands. The complexity analysis and Monte-Carlo simulations of the BER are provided for the proposed algorithms.      
### 43.Music Boundary Detection using Convolutional Neural Networks: A comparative analysis of combined input features  [ :arrow_down: ](https://arxiv.org/pdf/2008.07527.pdf)
>  The analysis of the structure of musical pieces is a task that remains a challenge for Artificial Intelligence, especially in the field of Deep Learning. It requires prior identification of structural boundaries of the music pieces. This structural boundary analysis has recently been studied with unsupervised methods and \textit{end-to-end} techniques such as Convolutional Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features (MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as inputs and trained with human annotations. Several studies have been published divided into unsupervised and \textit{end-to-end} methods in which pre-processing is done in different ways, using different distance metrics and audio characteristics, so a generalized pre-processing method to compute model inputs is missing. The objective of this work is to establish a general method of pre-processing these inputs by comparing the inputs calculated from different pooling strategies, distance metrics and audio characteristics, also taking into account the computing time to obtain them. We also establish the most effective combination of inputs to be delivered to the CNN in order to establish the most efficient way to extract the limits of the structure of the music pieces. With an adequate combination of input matrices and pooling strategies we obtain a measurement accuracy $F_1$ of 0.411 that outperforms the current one obtained under the same conditions.      
### 44.Personalized Deep Learning for Ventricular Arrhythmias Detection on Medical IoT Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.08060.pdf)
>  Life-threatening ventricular arrhythmias (VA) are the leading cause of sudden cardiac death (SCD), which is the most significant cause of natural death in the US. The implantable cardioverter defibrillator (ICD) is a small device implanted to patients under high risk of SCD as a preventive treatment. The ICD continuously monitors the intracardiac rhythm and delivers shock when detecting the life-threatening VA. Traditional methods detect VA by setting criteria on the detected rhythm. However, those methods suffer from a high inappropriate shock rate and require a regular follow-up to optimize criteria parameters for each ICD recipient. To ameliorate the challenges, we propose the personalized computing framework for deep learning based VA detection on medical IoT systems. The system consists of intracardiac and surface rhythm monitors, and the cloud platform for data uploading, diagnosis, and CNN model personalization. We equip the system with real-time inference on both intracardiac and surface rhythm monitors. To improve the detection accuracy, we enable the monitors to detect VA collaboratively by proposing the cooperative inference. We also introduce the CNN personalization for each patient based on the computing framework to tackle the unlabeled and limited rhythm data problem. When compared with the traditional detection algorithm, the proposed method achieves comparable accuracy on VA rhythm detection and 6.6% reduction in inappropriate shock rate, while the average inference latency is kept at 71ms.      
### 45.The MRS UAV System: Pushing the Frontiers of Reproducible Research, Real-world Deployment, and Education with Autonomous Unmanned Aerial Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.08050.pdf)
>  We present a multirotor Unmanned Aerial Vehicle control (UAV) and estimation system for supporting replicable research through realistic simulations and real-world experiments. We propose a unique multi-frame localization paradigm for estimating the states of a UAV in various frames of reference using multiple sensors simultaneously. The system enables complex missions in GNSS and GNSS-denied environments, including outdoor-indoor transitions and the execution of redundant estimators for backing up unreliable localization sources. Two feedback control designs are presented: one for precise and aggressive maneuvers, and the other for stable and smooth flight with a noisy state estimate. The proposed control and estimation pipeline are constructed without using the Euler/Tait-Bryan angle representation of orientation in 3D. Instead, we rely on rotation matrices and a novel heading-based convention to represent the one free rotational degree-of-freedom in 3D of a standard multirotor helicopter. We provide an actively maintained and well-documented open-source implementation, including realistic simulation of UAV, sensors, and localization systems. The proposed system is the product of years of applied research on multi-robot systems, aerial swarms, aerial manipulation, motion planning, and remote sensing. All our results have been supported by real-world system deployment that shaped the system into the form presented here. In addition, the system was utilized during the participation of our team from the CTU in Prague in the prestigious MBZIRC 2017 and 2020 robotics competitions, and also in the DARPA SubT challenge. Each time, our team was able to secure top places among the best competitors from all over the world. On each occasion, the challenges has motivated the team to improve the system and to gain a great amount of high-quality experience within tight deadlines.      
### 46.MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images in the Context of COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2008.08016.pdf)
>  The wearing of the face masks appears as a solution for limiting the spread of COVID-19. In this context, efficient recognition systems are expected for checking that people faces are masked in regulated areas. To perform this task, a large dataset of masked faces is necessary for training deep learning models towards detecting people wearing masks and those not wearing masks. Some large datasets of masked faces are available in the literature. However, at the moment, there are no available large dataset of masked face images that permits to check if detected masked faces are correctly worn or not. Indeed, many people are not correctly wearing their masks due to bad practices, bad behaviors or vulnerability of individuals (e.g., children, old people). For these reasons, several mask wearing campaigns intend to sensitize people about this problem and good practices. In this sense, this work proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net). Realistic masked face datasets are proposed with a twofold objective: i) to detect people having their faces masked or not masked, ii) to detect faces having their masks correctly worn or incorrectly worn (e.g.; at airport portals or in crowds). To the best of our knowledge, no large dataset of masked faces provides such a granularity of classification towards permitting mask wearing analysis. Moreover, this work globally presents the applied mask-to-face deformable model for permitting the generation of other masked face images, notably with specific masks. Our datasets of masked face images (137,016 images) are available at <a class="link-external link-https" href="https://github.com/cabani/MaskedFace-Net" rel="external noopener nofollow">this https URL</a>.      
### 47.A port-Hamiltonian approach to modeling the structural dynamics of complex systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.07985.pdf)
>  With this contribution, we give a complete and comprehensive framework for modeling the dynamics of complex mechanical structures as port-Hamiltonian systems. This is motivated by research on the potential of lightweight construction using active load-bearing elements integrated into the structure. Such adaptive structures are of high complexity and very heterogeneous in nature. Port-Hamiltonian systems theory provides a promising approach for their modeling and control. Subsystem dynamics can be formulated in a domain-independent way and interconnected by means of power flows. The modular approach is also suitable for robust decentralized control schemes. Starting from a distributed-parameter port-Hamiltonian formulation of beam dynamics, we show the application of an existing structure-preserving mixed finite element method to arrive at finite-dimensional approximations. In contrast to the modeling of single bodies with a single boundary, we consider complex structures composed of many simple elements interconnected at the boundary. This is analogous to the usual way of modeling civil engineering structures which has not been transferred to port-Hamiltonian systems before. A block diagram representation of the interconnected systems is used to generate coupling constraints which leads to differential algebraic equations of index one. After the elimination of algebraic constraints, systems in input-state-output(ISO) port-Hamiltonian form are obtained. Port-Hamiltonian system models for the considered class of systems can also be constructed from the mass and stiffness matrices obtained via conventional finite element methods. We show how this relates to the presented approach and discuss the differences, promoting a better understanding across engineering disciplines.      
### 48.Idle vehicle repositioning for dynamic ride-sharing  [ :arrow_down: ](https://arxiv.org/pdf/2008.07957.pdf)
>  In dynamic ride-sharing systems, intelligent repositioning of idle vehicles enables service providers to maximize vehicle utilization and minimize request rejection rates as well as customer waiting times. In current practice, this task is often performed decentrally by individual drivers. We present a centralized approach to idle vehicle repositioning in the form of a forecast-driven repositioning algorithm. The core part of our approach is a novel mixed-integer programming model that aims to maximize coverage of forecasted demand while minimizing travel times for repositioning movements. This model is embedded into a planning service also encompassing other relevant tasks such as vehicle dispatching. We evaluate our approach through extensive simulation studies on real-world datasets from Hamburg, New York City, and Manhattan. We test our forecast-driven repositioning approach under a perfect demand forecast as well as a naive forecast and compare it to a reactive strategy. The results show that our algorithm is suitable for real-time usage even in large-scale scenarios. Compared to the reactive algorithm, rejection rates of trip requests are decreased by an average of 2.5 percentage points and customer waiting times see an average reduction of 13.2%.      
### 49.Deep Learning-based Signal Strength Prediction Using Geographical Images and Expert Knowledge  [ :arrow_down: ](https://arxiv.org/pdf/2008.07747.pdf)
>  Methods for accurate prediction of radio signal quality parameters are crucial for optimization of mobile networks, and a necessity for future autonomous driving solutions. The power-distance relation of current empirical models struggles with describing the specific local geo-statistics that influence signal quality parameters. The use of empirical models commonly results in an over- or under-estimation of the signal quality parameters and require additional calibration studies. In this paper, we present a novel model-aided deep learning approach for path loss prediction, which implicitly extracts radio propagation characteristics from top-view geographical images of the receiver location. In a comprehensive evaluation campaign, we apply the proposed method on an extensive real-world data set consisting of five different scenarios and more than 125.000 individual measurements. It is found that 1) the novel approach reduces the average prediction error by up to 53% in comparison to ray-tracing techniques, 2) A distance of 250-300 meters spanned by the images offer the necessary level of detail, 3) Predictions with a root-mean-squared error of approximately 6 dB is achieved across inherently different data sources.      
### 50.Domain Generalizer: A Few-shot Meta Learning Framework for Domain Generalization in Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.07724.pdf)
>  Deep learning models perform best when tested on target (test) data domains whose distribution is similar to the set of source (train) domains. However, model generalization can be hindered when there is significant difference in the underlying statistics between the target and source domains. In this work, we adapt a domain generalization method based on a model-agnostic meta-learning framework to biomedical imaging. The method learns a domain-agnostic feature representation to improve generalization of models to the unseen test distribution. The method can be used for any imaging task, as it does not depend on the underlying model architecture. We validate the approach through a computed tomography (CT) vertebrae segmentation task across healthy and pathological cases on three datasets. Next, we employ few-shot learning, i.e. training the generalized model using very few examples from the unseen domain, to quickly adapt the model to new unseen data distribution. Our results suggest that the method could help generalize models across different medical centers, image acquisition protocols, anatomies, different regions in a given scan, healthy and diseased populations across varied imaging modalities.      
### 51.Contact Area Detector using Cross View Projection Consistency for COVID-19 Projects  [ :arrow_down: ](https://arxiv.org/pdf/2008.07712.pdf)
>  The ability to determine what parts of objects and surfaces people touch as they go about their daily lives would be useful in understanding how the COVID-19 virus spreads. To determine whether a person has touched an object or surface using visual data, images, or videos, is a hard problem. Computer vision 3D reconstruction approaches project objects and the human body from the 2D image domain to 3D and perform 3D space intersection directly. However, this solution would not meet the accuracy requirement in applications due to projection error. Another standard approach is to train a neural network to infer touch actions from the collected visual data. This strategy would require significant amounts of training data to generalize over scale and viewpoint variations. A different approach to this problem is to identify whether a person has touched a defined object. In this work, we show that the solution to this problem can be straightforward. Specifically, we show that the contact between an object and a static surface can be identified by projecting the object onto the static surface through two different viewpoints and analyzing their 2D intersection. The object contacts the surface when the projected points are close to each other; we call this cross view projection consistency. Instead of doing 3D scene reconstruction or transfer learning from deep networks, a mapping from the surface in the two camera views to the surface space is the only requirement. For planar space, this mapping is the Homography transformation. This simple method can be easily adapted to real-life applications. In this paper, we apply our method to do office occupancy detection for studying the COVID-19 transmission pattern from an office desk in a meeting room using the contact information.      
### 52.PopMAG: Pop Music Accompaniment Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.07703.pdf)
>  In pop music, accompaniments are usually played by multiple instruments (tracks) such as drum, bass, string and guitar, and can make a song more expressive and contagious by arranging together with its melody. Previous works usually generate multiple tracks separately and the music notes from different tracks not explicitly depend on each other, which hurts the harmony modeling. To improve harmony, in this paper, we propose a novel MUlti-track MIDI representation (MuMIDI), which enables simultaneous multi-track generation in a single sequence and explicitly models the dependency of the notes from different tracks. While this greatly improves harmony, unfortunately, it enlarges the sequence length and brings the new challenge of long-term music modeling. We further introduce two new techniques to address this challenge: 1) We model multiple note attributes (e.g., pitch, duration, velocity) of a musical note in one step instead of multiple steps, which can shorten the length of a MuMIDI sequence. 2) We introduce extra long-context as memory to capture long-term dependency in music. We call our system for pop music accompaniment generation as PopMAG. We evaluate PopMAG on multiple datasets (LMD, FreeMidi and CPMD, a private dataset of Chinese pop songs) with both subjective and objective metrics. The results demonstrate the effectiveness of PopMAG for multi-track harmony modeling and long-term context modeling. Specifically, PopMAG wins 42\%/38\%/40\% votes when comparing with ground truth musical pieces on LMD, FreeMidi and CPMD datasets respectively and largely outperforms other state-of-the-art music accompaniment generation models and multi-track MIDI representations in terms of subjective and objective metrics.      
### 53.On the Error Exponent of Approximate Sufficient Statistics for M-ary Hypothesis Testing  [ :arrow_down: ](https://arxiv.org/pdf/2008.07693.pdf)
>  Consider the problem of detecting one of M i.i.d. Gaussian signals corrupted in white Gaussian noise. Conventionally, matched filters are used for detection. We first show that the outputs of the matched filter form a set of asymptotically optimal sufficient statistics in the sense of maximizing the error exponent of detecting the true signal. In practice, however, M may be large which motivates the design and analysis of a reduced set of N statistics which we term approximate sufficient statistics. Our construction of these statistics is based on a small set of filters that project the outputs of the matched filters onto a lower-dimensional vector using a sensing matrix. We consider a sequence of sensing matrices that has the desiderata of row orthonormality and low coherence. We analyze the performance of the resulting maximum likelihood (ML) detector, which leads to an achievable bound on the error exponent based on the approximate sufficient statistics; this bound recovers the original error exponent when N = M. We compare this to a bound that we obtain by analyzing a modified form of the Reduced Dimensionality Detector (RDD) proposed by Xie, Eldar, and Goldsmith [IEEE Trans. on Inform. Th., 59(6):3858-3874, 2013]. We show that by setting the sensing matrices to be column-normalized group Hadamard matrices, the exponents derived are ensemble-tight, i.e., our analysis is tight on the exponential scale given the sensing matrices and the decoding rule. Finally, we derive some properties of the exponents, showing, in particular, that they increase linearly in the compression ratio N/M.      
### 54.Revisiting the Application of Feature Selection Methods to Speech Imagery BCI Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2008.07660.pdf)
>  Brain-computer interface (BCI) aims to establish and improve human and computer interactions. There has been an increasing interest in designing new hardware devices to facilitate the collection of brain signals through various technologies, such as wet and dry electroencephalogram (EEG) and functional near-infrared spectroscopy (fNIRS) devices. The promising results of machine learning methods have attracted researchers to apply these methods to their data. However, some methods can be overlooked simply due to their inferior performance against a particular dataset. This paper shows how relatively simple yet powerful feature selection/ranking methods can be applied to speech imagery datasets and generate significant results. To do so, we introduce two approaches, horizontal and vertical settings, to use any feature selection and ranking methods to speech imagery BCI datasets. Our primary goal is to improve the resulting classification accuracies from support vector machines, $k$-nearest neighbour, decision tree, linear discriminant analysis and long short-term memory recurrent neural network classifiers. Our experimental results show that using a small subset of channels, we can retain and, in most cases, improve the resulting classification accuracies regardless of the classifier.      
### 55.The economics of utility-scale portable energy storage systems in a high-renewable grid  [ :arrow_down: ](https://arxiv.org/pdf/2008.07635.pdf)
>  Battery storage is expected to play a crucial role in the low-carbon transformation of energy systems. The deployment of battery storage in the power gird, however, is currently severely limited by its low economic viability, which results from not only high capital costs but also the lack of flexible and efficient utilization schemes and business models. Making utility-scale battery storage portable through trucking unlocks its capability to provide various on-demand services. We introduce the potential applications of utility-scale portable energy storage and investigate its economics in California using a spatiotemporal decision model that determines the optimal operation and transportation schedules of portable storage. We show that mobilizing energy storage can increase its life-cycle revenues by 70% in some areas and improve renewable energy integration by relieving local transmission congestion. The life-cycle revenue of spatiotemporal arbitrage can fully compensate for the costs of portable energy storage system in several regions in California, including San Diego and the San Francisco Bay Area.      
### 56.Hydrogen Supply Chain Planning with Flexible Transmission and Storage Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2008.07611.pdf)
>  Hydrogen is becoming an increasingly appealing energy carrier, as the costs of renewable energy generation and water electrolysis continue to decline. Developing modelling and decision tools for the H$_{2}$ supply chain that fully capture the flexibility of various resources is essential to understanding the overall cost-competitiveness of H$_{2}$ use. To address this need, we have developed a H$_{2}$ supply chain planning model that determines the least-cost mix of H$_{2}$ generation, storage, transmission, and compression facilities to meet H$_{2}$ demands and is coupled with power systems through electricity prices. We incorporate flexible scheduling for H$_{2}$ trucks and pipeline, allowing them to serve as both H$_{2}$ transmission and storage resources to shift H$_{2}$ demand/production across space and time. The case study results in the U.S. Northeast indicate that the proposed framework for flexible scheduling of H$_{2}$ transmission and storage resources is critical not only to cost minimization but also to the choice of H$_{2}$ production pathways between electrolyzer and centralized natural-gas-based production facilities. Trucks as mobile storage could make electrolyzer more competitive by providing extra spatiotemporal flexibility to respond to the electricity price variability while meeting H$_{2}$ demands. The proposed model also provides a reasonable trade-off between modeling accuracy and solution times.      
### 57.Polyth-Net: Classification of Polythene Bags for Garbage Segregation Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.07592.pdf)
>  Polythene has always been a threat to the environment since its invention. It is non-biodegradable and very difficult to recycle. Even after many awareness campaigns and practices, Separation of polythene bags from waste has been a challenge for human civilization. The primary method of segregation deployed is manual handpicking, which causes a dangerous health hazards to the workers and is also highly inefficient due to human errors. In this paper I have designed and researched on image-based classification of polythene bags using a deep-learning model and its efficiency. This paper focuses on the architecture and statistical analysis of its performance on the data set as well as problems experienced in the classification. It also suggests a modified loss function to specifically detect polythene irrespective of its individual features. It aims to help the current environment protection endeavours and save countless lives lost to the hazards caused by current methods.      
### 58.A Rolling Optimized Nonlinear Grey Bernoulli Model RONGBM(1,1) and application in predicting total COVID-19 infected cases  [ :arrow_down: ](https://arxiv.org/pdf/2008.07581.pdf)
>  The Nonlinear Grey Bernoulli Model NGBM(1, 1) is a recently developed grey model which has various applications in different fields, mainly due to its accuracy in handling small time-series datasets with nonlinear variations. In this paper, to fully improve the accuracy of this model, a novel model is proposed, namely Rolling Optimized Nonlinear Grey Bernoulli Model RONGBM(1, 1). This model combines the rolling mechanism with the simultaneous optimization of all model parameters (exponential, background value and initial condition). The accuracy of this new model has significantly been proven through forecasting Vietnam's GDP from 2013 to 2018, before it is applied to predict the total COVID-19 infected cases globally by day.      
### 59.PROTEUS: Rule-Based Self-Adaptation in Photonic NoCs for Loss-Aware Co-Management of Laser Power and Performance  [ :arrow_down: ](https://arxiv.org/pdf/2008.07566.pdf)
>  The performance of on-chip communication in the state-of-the-art multi-core processors that use the traditional electron-ic NoCs has already become severely energy-constrained. To that end, emerging photonic NoCs (PNoC) are seen as a po-tential solution to improve the energy-efficiency (performance per watt) of on-chip communication. However, existing PNoC designs cannot realize their full potential due to their exces-sive laser power consumption. Prior works that attempt to improve laser power efficiency in PNoCs do not consider all key factors that affect the laser power requirement of PNoCs. Therefore, they cannot yield the desired balance between the reduction in laser power, achieved performance and energy-efficiency in PNoCs. In this paper, we present PROTEUS framework that employs rule-based self-adaptation in PNoCs. Our approach not only reduces the laser power consumption, but also minimizes the average packet latency by opportunis-tically increasing the communication data rate in PNoCs, and thus, yields the desired balance between the laser power re-duction, performance, and energy-efficiency in PNoCs. Our evaluation with PARSEC benchmarks shows that our PROTEUS framework can achieve up to 24.5% less laser power consumption, up to 31% less average packet latency, and up to 20% less energy-per-bit, compared to another laser power management technique from prior work.      
