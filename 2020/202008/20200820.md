# ArXiv eess --Thu, 20 Aug 2020
### 1.Slide-free MUSE Microscopy to H&amp;E Histology Modality Conversion via Unpaired Image-to-Image Translation GAN Models  [ :arrow_down: ](https://arxiv.org/pdf/2008.08579.pdf)
>  MUSE is a novel slide-free imaging technique for histological examination of tissues that can serve as an alternative to traditional histology. In order to bridge the gap between MUSE and traditional histology, we aim to convert MUSE images to resemble authentic hematoxylin- and eosin-stained (H&amp;E) images. We evaluated four models: a non-machine-learning-based color-mapping unmixing-based tool, CycleGAN, DualGAN, and GANILLA. CycleGAN and GANILLA provided visually compelling results that appropriately transferred H&amp;E style and preserved MUSE content. Based on training an automated critic on real and generated H&amp;E images, we determined that CycleGAN demonstrated the best performance. We have also found that MUSE color inversion may be a necessary step for accurate modality conversion to H&amp;E. We believe that our MUSE-to-H&amp;E model can help improve adoption of novel slide-free methods by bridging a perceptual gap between MUSE imaging and traditional histology.      
### 2.Blur-Attention: A boosting mechanism for non-uniform blurred image restoration  [ :arrow_down: ](https://arxiv.org/pdf/2008.08526.pdf)
>  Dynamic scene deblurring is a challenging problem in computer vision. It is difficult to accurately estimate the spatially varying blur kernel by traditional methods. Data-driven-based methods usually employ kernel-free end-to-end mapping schemes, which are apt to overlook the kernel estimation. To address this issue, we propose a blur-attention module to dynamically capture the spatially varying features of non-uniform blurred images. The module consists of a DenseBlock unit and a spatial attention unit with multi-pooling feature fusion, which can effectively extract complex spatially varying blur features. We design a multi-level residual connection structure to connect multiple blur-attention modules to form a blur-attention network. By introducing the blur-attention network into a conditional generation adversarial framework, we propose an end-to-end blind motion deblurring method, namely Blur-Attention-GAN (BAG), for a single image. Our method can adaptively select the weights of the extracted features according to the spatially varying blur features, and dynamically restore the images. Experimental results show that the deblurring capability of our method achieved outstanding objective performance in terms of PSNR, SSIM, and subjective visual quality. Furthermore, by visualizing the features extracted by the blur-attention module, comprehensive discussions are provided on its effectiveness.      
### 3."Name that manufacturer". Relating image acquisition bias with task complexity when training deep learning models: experiments on head CT  [ :arrow_down: ](https://arxiv.org/pdf/2008.08525.pdf)
>  As interest in applying machine learning techniques for medical images continues to grow at a rapid pace, models are starting to be developed and deployed for clinical applications. In the clinical AI model development lifecycle (described by Lu et al. [1]), a crucial phase for machine learning scientists and clinicians is the proper design and collection of the data cohort. The ability to recognize various forms of biases and distribution shifts in the dataset is critical at this step. While it remains difficult to account for all potential sources of bias, techniques can be developed to identify specific types of bias in order to mitigate their impact. In this work we analyze how the distribution of scanner manufacturers in a dataset can contribute to the overall bias of deep learning models. We evaluate convolutional neural networks (CNN) for both classification and segmentation tasks, specifically two state-of-the-art models: ResNet [2] for classification and U-Net [3] for segmentation. We demonstrate that CNNs can learn to distinguish the imaging scanner manufacturer and that this bias can substantially impact model performance for both classification and segmentation tasks. By creating an original synthesis dataset of brain data mimicking the presence of more or less subtle lesions we also show that this bias is related to the difficulty of the task. Recognition of such bias is critical to develop robust, generalizable models that will be crucial for clinical applications in real-world data distributions.      
### 4.Correcting Data Imbalance for Semi-Supervised Covid-19 Detection Using X-ray Chest Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.08496.pdf)
>  The Corona Virus (COVID-19) is an internationalpandemic that has quickly propagated throughout the world. The application of deep learning for image classification of chest X-ray images of Covid-19 patients, could become a novel pre-diagnostic detection methodology. However, deep learning architectures require large labelled datasets. This is often a limitation when the subject of research is relatively new as in the case of the virus outbreak, where dealing with small labelled datasets is a challenge. Moreover, in the context of a new highly infectious disease, the datasets are also highly imbalanced,with few observations from positive cases of the new disease. In this work we evaluate the performance of the semi-supervised deep learning architecture known as MixMatch using a very limited number of labelled observations and highly imbalanced labelled dataset. We propose a simple approach for correcting data imbalance, re-weight each observationin the loss function, giving a higher weight to the observationscorresponding to the under-represented class. For unlabelled observations, we propose the usage of the pseudo and augmentedlabels calculated by MixMatch to choose the appropriate weight. The MixMatch method combined with the proposed pseudo-label based balance correction improved classification accuracy by up to 10%, with respect to the non balanced MixMatch algorithm, with statistical significance. We tested our proposed approach with several available datasets using 10, 15 and 20 labelledobservations. Additionally, a new dataset is included among thetested datasets, composed of chest X-ray images of Costa Rican adult patients      
### 5.Bayesian Joint Synchronization and Localization Based on Asymmetric Time-stamp Exchange  [ :arrow_down: ](https://arxiv.org/pdf/2008.08481.pdf)
>  In this work, we study the joint synchronization and localization (sync&amp;loc) of Mobile Nodes (MNs) in ultra dense networks. In particular, we deploy an asymmetric timestamp exchange mechanism between MNs and Access Nodes (ANs), that, traditionally, provides us with information about the MNs' clock offset and skew. However, information about the distance between an AN and a MN is also intrinsic to the propagation delay experienced by exchanged time-stamps. In addition, we utilize Angle of Arrival (AoA) estimation to determine the incoming direction of time-stamp exchange packets, which gives further information about the MNs' location. Finally, we employ Bayesian Recursive Filtering (BRF) to combine the aforementioned pieces of information and jointly estimate the position and clock parameters of MNs. The simulation results indicate that the Root Mean Square Errors (RMSEs) of position and clock offset estimation are kept below 1 meter and 1 ns, respectively.      
### 6.On the inverse Potts functional for single-image super-resolution problems  [ :arrow_down: ](https://arxiv.org/pdf/2008.08470.pdf)
>  We consider a variational model for single-image super-resolution based on the assumption that the image gradient of the target image is sparse. To promote jump sparsity, we use an isotropic and anisotropic $\ell^{0}$ inverse Potts gradient regularisation term combined with a quadratic data fidelity, similarly as studied in [1] for general problems in signal recovery. For the numerical realisation of the model, we consider a converging ADMM algorithm. Differently from [1], [2], where approximate graph cuts and dynamic programming techniques were used for solving the non-convex substeps in the case of multivariate data, the proposed splitting allows to compute explicitly their solution by means of hard-thresholding and standard conjugate-gradient solvers. We compare quantitatively our results with several convex, nonconvex and deep-learning-based approaches for several synthetic and real-world data. Our numerical results show that combining super-resolution with gradient sparsity is particularly helpful for object detection and labelling tasks (such as QR scanning and land-cover classification), for which our results are shown to improve the classification precision of standard clustering algorithms and state-of-the art deep architectures [3].      
### 7.Designing inverse dynamic controller with integral action for motion planning of surgical robot in the presence of bounded disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2008.08456.pdf)
>  Robotic laparoscopic grasper is a surgical tool with minimal invasion. In this robot, achieve goals like precise tracking, stability and disturbance rejection are very important. In this paper, first the stages of modeling and simulating of laparoscopic robot will be discussed and the reasons for selecting the appropriate materials for different parts of proposed practical robot will be explained. Inverse dynamic controller with integral action is applied to improve the accuracy of tracking procedure for a surgical manipulator to track a specified reference signal in the presence of tremor that is modeled as constant bounded disturbance. Based on the disturbance rejection scheme, tracking controller is constructed which is asymptotically stabilizing in the sense of Lyapunov. It is shown that how under proper assumptions; the selected schemes succeed in achieving disturbance rejection at the input of a nonlinear system. Computer simulation results demonstrate that accurate trajectory tracking can be achieved by using the proposed controller.      
### 8.A 3D Motion Vector Database for Dynamic Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2008.08438.pdf)
>  Due to the large amount of data that point clouds represent and the differences in geometry of successive frames, the generation of motion vectors for an entire point cloud dataset may require a significant amount of time and computational resources. With that in mind, we provide a 3D motion vector database for all frames of two popular dynamic point cloud datasets. The motion vectors were obtained through translational motion estimation procedure that partitions the point clouds into blocks of dimensions M x M x M , and for each block, a motion vector is estimated. Our database contains motion vectors for M = 8 and M = 16. The goal of this work is to describe this publicly available 3D motion vector database that can be used for different purposes, such as compression of dynamic point clouds.      
### 9.Improving Blind Spot Denoising for Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2008.08414.pdf)
>  Many microscopy applications are limited by the total amount of usable light and are consequently challenged by the resulting levels of noise in the acquired images. This problem is often addressed via (supervised) deep learning based denoising. Recently, by making assumptions about the noise statistics, self-supervised methods have emerged. Such methods are trained directly on the images that are to be denoised and do not require additional paired training data. While achieving remarkable results, self-supervised methods can produce high-frequency artifacts and achieve inferior results compared to supervised approaches. Here we present a novel way to improve the quality of self-supervised denoising. Considering that light microscopy images are usually diffraction-limited, we propose to include this knowledge in the denoising process. We assume the clean image to be the result of a convolution with a point spread function (PSF) and explicitly include this operation at the end of our neural network. As a consequence, we are able to eliminate high-frequency artifacts and achieve self-supervised results that are very close to the ones achieved with traditional supervised methods.      
### 10.HpRNet : Incorporating Residual Noise Modeling for Violin in a Variational Parametric Synthesizer  [ :arrow_down: ](https://arxiv.org/pdf/2008.08405.pdf)
>  Generative Models for Audio Synthesis have been gaining momentum in the last few years. More recently, parametric representations of the audio signal have been incorporated to facilitate better musical control of the synthesized output. In this work, we investigate a parametric model for violin tones, in particular the generative modeling of the residual bow noise to make for more natural tone quality. To aid in our analysis, we introduce a dataset of Carnatic Violin Recordings where bow noise is an integral part of the playing style of higher pitched notes in specific gestural contexts. We obtain insights about each of the harmonic and residual components of the signal, as well as their interdependence, via observations on the latent space derived in the course of variational encoding of the spectral envelopes of the sustained sounds.      
### 11.Deep Controllable Backlight Dimming  [ :arrow_down: ](https://arxiv.org/pdf/2008.08352.pdf)
>  Dual-panel displays require local dimming algorithms in order to reproduce content with high fidelity and high dynamic range. In this work, a novel deep learning based local dimming method is proposed for rendering HDR images on dual-panel HDR displays. The method uses a Convolutional Neural Network to predict backlight values, using as input the HDR image that is to be displayed. The model is designed and trained via a controllable power parameter that allows a user to trade off between power and quality. The proposed method is evaluated against six other methods on a test set of 105 HDR images, using a variety of quantitative quality metrics. Results demonstrate improved display quality and better power consumption when using the proposed method compared to the best alternatives.      
### 12.Self-Tuning State Estimation for Adaptive Truss Structures Using Strain Gauges and Camera-Based Position Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2008.08303.pdf)
>  In the context of control of smart structures, we present an approach for state estimation of adaptive buildings with active load-bearing elements. For obtaining information on structural deformation, a system composed of a digital camera and optical emitters affixed to selected nodal points is introduced as a complement to conventional strain gauge sensors. Sensor fusion for this novel combination of sensors is carried out using a Kalman filter that operates on a reduced-order structure model obtained by modal analysis. Signal delay caused by image processing is compensated for by an out-of-sequence measurement update which provides for a flexible and modular estimation algorithm. Since the camera system is very precise, a self-tuning algorithm that adjusts model along with observer parameters is introduced to reduce discrepancy between system dynamic model and actual structural behavior. We further employ optimal sensor placement to limit the number of sensors to be placed on a given structure and examine the impact on estimation accuracy. A laboratory scale model of an adaptive high-rise with actuated columns and diagonal bracings is used for experimental demonstration of the proposed estimation scheme.      
### 13.Development of novel algorithm to visualize blood vessels on 3D ultrasound images during liver surgery  [ :arrow_down: ](https://arxiv.org/pdf/2008.08280.pdf)
>  Volume visualization is a method that displays three-dimensional (3D) data in two-dimensional (2D) space. Using 3D datasets instead of 2D traditional images improves the visualization of anatomical structures, and volume visualization helps radiologists and surgeons to review large datasets comprehensively so that diagnosis and treatment can be enhanced. In liver surgery, blood vessel detection is important. Liver vessels have various shapes and due to the presence of noise in the ultrasound images, they can be confused with noise. Suboptimal images can sometimes lead to surgical errors where the surgeon may cut the blood vessel in error. The ultrasound system is versatile and portable and has the advantage of being able to be used in the operating theatre. Due to the nature of B-mode ultrasound, 1-D transfer function volume visualization of images cannot abrogate shadow artifacts. While multi-dimensional transfer function improves the ability to define features of interest, the high dimensionality in the parameter domain renders it unwieldy and difficult for clinicians to work with. To overcome these limitations, an algorithm for volume visualization that can provide effective 3D visualization of noisy B-mode ultrasound images, which can be useful for clinicians, is proposed. We propose a method that is appropriate for liver ultrasound images focusing on vessels and tumors (if present) in order to delineate their structure and positions clearly to preempt surgical error during operation. This method can prevent possible errors during liver surgery by providing more detailed high quality 3D images for clinicians. Key Words: Visualization, 3D ultrasound image, Volume Rendering, Liver surgery, Liver vessels.      
### 14.DONet: Dual Objective Networks for Skin Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2008.08278.pdf)
>  Skin lesion segmentation is a crucial step in the computer-aided diagnosis of dermoscopic images. In the last few years, deep learning based semantic segmentation methods have significantly advanced the skin lesion segmentation results. However, the current performance is still unsatisfactory due to some challenging factors such as large variety of lesion scale and ambiguous difference between lesion region and background. In this paper, we propose a simple yet effective framework, named Dual Objective Networks (DONet), to improve the skin lesion segmentation. Our DONet adopts two symmetric decoders to produce different predictions for approaching different objectives. Concretely, the two objectives are actually defined by different loss functions. In this way, the two decoders are encouraged to produce differentiated probability maps to match different optimization targets, resulting in complementary predictions accordingly. The complementary information learned by these two objectives are further aggregated together to make the final prediction, by which the uncertainty existing in segmentation maps can be significantly alleviated. Besides, to address the challenge of large variety of lesion scales and shapes in dermoscopic images, we additionally propose a recurrent context encoding module (RCEM) to model the complex correlation among skin lesions, where the features with different scale contexts are efficiently integrated to form a more robust representation. Extensive experiments on two popular benchmarks well demonstrate the effectiveness of the proposed DONet. In particular, our DONet achieves 0.881 and 0.931 dice score on ISIC 2018 and $\text{PH}^2$, respectively. Code will be made public available.      
### 15.Intelligent Radio Signal Processing: A Contemporary Survey  [ :arrow_down: ](https://arxiv.org/pdf/2008.08264.pdf)
>  Intelligent signal processing for wireless communications is a vital task in modern wireless systems, but it faces new challenges because of network heterogeneity, diverse service requirements, a massive number of connections, and various radio characteristics. Owing to recent advancements in big data and computing technologies, artificial intelligence (AI) has become a useful tool for radio signal processing and has enabled the realization of intelligent radio signal processing. This survey covers four intelligent signal processing topics for the wireless physical layer, including modulation classification, signal detection, beamforming, and channel estimation. In particular, each theme is presented in a dedicated section, starting with the most fundamental principles, followed by a review of up-to-date studies and a summary. To provide the necessary background, we first present a brief overview of AI techniques such as machine learning, deep learning, and federated learning. Finally, we highlight a number of research challenges and future directions in the area of intelligent radio signal processing. We expect this survey to be a good source of information for anyone interested in intelligent radio signal processing, and the perspectives we provide therein will stimulate many more novel ideas and contributions in the future.      
### 16.Enhanced MRI Reconstruction Network using Neural Architecture Search  [ :arrow_down: ](https://arxiv.org/pdf/2008.08248.pdf)
>  The accurate reconstruction of under-sampled magnetic resonance imaging (MRI) data using modern deep learning technology, requires significant effort to design the necessary complex neural network architectures. The cascaded network architecture for MRI reconstruction has been widely used, while it suffers from the "vanishing gradient" problem when the network becomes deep. In addition, homogeneous architecture degrades the representation capacity of the network. In this work, we present an enhanced MRI reconstruction network using a residual in residual basic block. For each cell in the basic block, we use the differentiable neural architecture search (NAS) technique to automatically choose the optimal operation among eight variants of the dense block. This new heterogeneous network is evaluated on two publicly available datasets and outperforms all current state-of-the-art methods, which demonstrates the effectiveness of our proposed method.      
### 17.LIRA: Lifelong Image Restoration from Unknown Blended Distortions  [ :arrow_down: ](https://arxiv.org/pdf/2008.08242.pdf)
>  Most existing image restoration networks are designed in a disposable way and catastrophically forget previously learned distortions when trained on a new distortion removal task. To alleviate this problem, we raise the novel lifelong image restoration problem for blended distortions. We first design a base fork-join model in which multiple pre-trained expert models specializing in individual distortion removal task work cooperatively and adaptively to handle blended distortions. When the input is degraded by a new distortion, inspired by adult neurogenesis in human memory system, we develop a neural growing strategy where the previously trained model can incorporate a new expert branch and continually accumulate new knowledge without interfering with learned knowledge. Experimental results show that the proposed approach can not only achieve state-of-the-art performance on blended distortions removal tasks in both PSNR/SSIM metrics, but also maintain old expertise while learning new restoration tasks.      
### 18.Uncertainty Analysis in SPECT Reconstruction based on Probabilistic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2008.08230.pdf)
>  Single Photon Emission Computed Tomography (SPECT) is one of the nuclear medicine imaging modalities used for functional analysis of animal or human organs. Gamma rays emitted from the scanned body are filtered with collimators and detected by the SPECT head. The conventional reconstruction algorithms do not consider the uncertainty in the process introduced by the field of view of the collimators. In this paper, we incorporate the probabilistic programming approach for SPECT image reconstruction. No-U-Turn Sampler (NUTS) is used to estimate the scanned object system by considering uncertainty. Results indicate that the current work can include uncertainty in reconstruction compared to conventional approaches like MLEM and MAP. However, reconstruction time need to be improved for phantom sizes of 128x128x128 voxels and higher.      
### 19.Kinematic Resolutions of Redundant Robot Manipulators using Integration-Enhanced RNNs  [ :arrow_down: ](https://arxiv.org/pdf/2008.08228.pdf)
>  Recently, a time-varying quadratic programming (QP) framework that describes the tracking operations of redundant robot manipulators is introduced to handle the kinematic resolutions of many robot control tasks. Based on the generalization of such a time-varying QP framework, two schemes, i.e., the Repetitive Motion Scheme and the Hybrid Torque Scheme, are proposed. However, measurement noises are unavoidable when a redundant robot manipulator is executing a tracking task. To solve this problem, a novel integration-enhanced recurrent neural network (IE-RNN) is proposed in this paper. Associating with the aforementioned two schemes, the tracking task can be accurately completed by IE-RNN. Both theoretical analyses and simulations results prove that the residual errors of IE-RNN can converge to zero under different kinds of measurement noises. Moreover, practical experiments are elaborately made to verify the excellent convergence and strong robustness properties of the proposed IE-RNN.      
### 20.Joint Channel Assignment and Power Allocation for Multi-UAV Communication  [ :arrow_down: ](https://arxiv.org/pdf/2008.08212.pdf)
>  Unmanned aerial vehicle (UAV) swarm has emerged as a promising novel paradigm to achieve better coverage and higher capacity for future wireless network by exploiting the more favorable line-of-sight (LoS) propagation. To reap the potential gains of UAV swarm, the remote control signal sent by ground control unit (GCU) is essential, whereas the control signal quality are susceptible in practice due to the effect of the adjacent channel interference (ACI) and the external interference (EI) from radiation sources distributed across the region. To tackle these challenges, this paper considers priority-aware resource coordination in a multi-UAV communication system, where multiple UAVs are controlled by a GCU to perform certain tasks with a pre-defined trajectory. Specifically, we maximize the minimum signal-to-interference-plus-noise ratio (SINR) among all the UAVs by jointly optimizing channel assignment and power allocation strategy under stringent resource availability constraints. According to the intensity of ACI, we consider the corresponding problem in two scenarios, i.e., Null-ACI and ACI systems. By virtue of the particular problem structure in Null-ACI case, we first recast the formulation into an equivalent yet more tractable form and obtain the global optimal solution via Hungarian algorithm. For general ACI systems, we develop an efficient iterative algorithm for its solution based on the smooth approximation and alternating optimization methods. Extensive simulation results demonstrate that the proposed algorithms can significantly enhance the minimum SINR among all the UAVs and adapt the allocation of communication resources to diverse mission priority.      
### 21.BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.08198.pdf)
>  X-ray diffraction based microscopy techniques such as high energy diffraction microscopy rely on knowledge of position of diffraction peaks with high resolution. These positions are typically computed by fitting the observed intensities in detector data to a theoretical peak shape such as pseudo-Voigt. As experiments become more complex and detector technologies evolve, the computational cost of such peak shape fitting becomes the biggest hurdle to the rapid analysis required for real-time feedback for experiments. To this end, this paper proposes BraggNN, a machine learning-based method that can localize Bragg peak much more rapidly than conventional pseudo-Voigt peak fitting. When applied to our test dataset, BraggNN gives errors of less than 0.29 and 0.57 voxels, relative to conventional method, for 75% and 95% of the peaks, respectively. When applied to a real experiment dataset, a 3D reconstruction using peak positions located by BraggNN yields an average grain position difference of 17 micrometer and size difference of 1.3 micrometer as compared to the results obtained when the reconstruction used peaks from conventional 2D pseudo-Voigt fitting. Recent advances in deep learning method implementations and special-purpose model inference accelerators allow BraggNN to deliver enormous performance improvements relative to the conventional method, running, for example, more than 200 times faster than a conventional method when using a GPU card with out-of-the-box software.      
### 22.PRNU Estimation from Encoded Videos Using Block-Based Weighting  [ :arrow_down: ](https://arxiv.org/pdf/2008.08138.pdf)
>  Estimating the photo-response non-uniformity (PRNU) of an imaging sensor from videos is a challenging task due to complications created by several processing steps in the camera imaging pipeline. Among these steps, video coding is one of the most disruptive to PRNU estimation because of its lossy nature. Since videos are always stored in a compressed format, the ability to cope with disruptive effects of encoding is central to reliable attribution. In this work, by focusing on the block-based operation of widely used video coding standards, we present an improved approach to PRNU estimation that exploits this behavior. To this purpose, several PRNU weighting schemes that utilize block-level parameters, such as encoding block type, quantization strength, and rate-distortion values, are proposed and compared. Our results show that the use of the coding rate and the distortion introduced to a block serve as better estimators for the strength of PRNU with almost three times improvement in the matching statistic at low to medium coding bitrates as compared to the basic estimation method developed for photos.      
### 23.Planning and Operations of Mixed Fleets in Mobility-on-Demand Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.08131.pdf)
>  Automated vehicles (AVs) are expected to be beneficial for Mobility-on-Demand (MoD), thanks to their ability of being globally coordinated. To facilitate the steady transition towards full autonomy, we consider the transition period of AV deployment, whereby an MoD system operates a mixed fleet of automated vehicles (AVs) and human-driven vehicles (HVs). In such systems, AVs are centrally coordinated by the operator, and the HVs might strategically respond to the coordination of AVs. We devise computationally tractable strategies to coordinate mixed fleets in MoD systems. Specifically, we model an MoD system with a mixed fleet using a Stackelberg framework where the MoD operator serves as the leader and human-driven vehicles serve as the followers. We develop two models: 1) a steady-state model to analyze the properties of the problem and determine the planning variables (e.g., compensations, prices, and the fleet size of AVs), and 2) a time-varying model to design a real-time coordination algorithm for AVs. The proposed models are validated using a case study inspired by real operational data of a MoD service in Singapore. Results show that the proposed algorithms can significantly improve system performance.      
### 24.Review of Machine Learning Algorithms for Brain Stroke Diagnosis and Prognosis by EEG Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.08118.pdf)
>  Currently, strokes are the leading cause of adult disability in the United States. Traditional treatment and rehabilitation options such as physical therapy and tissue plasminogen activator are limited in their effectiveness and ability to restore mobility and function to the patient. As a result, there exists an opportunity to greatly improve the treatment for strokes. Machine learning, specifically techniques that utilize Brain-Computer Interfaces (BCIs) to help the patient either restore neurologic pathways or effectively communicate with an electronic prosthetic, show promising results when applied to both stroke diagnosis and rehabilitation. In this review, sources that design and implement BCIs for treatment of stroke patients are evaluated and categorized based on their successful applications for stroke diagnosis or stroke rehabilitation. The various machine learning techniques and algorithms that are addressed and combined with BCI technology show that the use of BCIs for stroke treatment is a promising and rapidly expanding field.      
### 25.Complementary Language Model and Parallel Bi-LRNN for False Trigger Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2008.08113.pdf)
>  False triggers in voice assistants are unintended invocations of the assistant, which not only degrade the user experience but may also compromise privacy. False trigger mitigation (FTM) is a process to detect the false trigger events and respond appropriately to the user. In this paper, we propose a novel solution to the FTM problem by introducing a parallel ASR decoding process with a special language model trained from "out-of-domain" data sources. Such language model is complementary to the existing language model optimized for the assistant task. A bidirectional lattice RNN (Bi-LRNN) classifier trained from the lattices generated by the complementary language model shows a $38.34\%$ relative reduction of the false trigger (FT) rate at the fixed rate of $0.4\%$ false suppression (FS) of correct invocations, compared to the current Bi-LRNN model. In addition, we propose to train a parallel Bi-LRNN model based on the decoding lattices from both language models, and examine various ways of implementation. The resulting model leads to further reduction in the false trigger rate by $10.8\%$.      
### 26.Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification  [ :arrow_down: ](https://arxiv.org/pdf/2008.08528.pdf)
>  Person re-identification (Re-ID) aims at retrieving an input person image from a set of images captured by multiple cameras. Although recent Re-ID methods have made great success, most of them extract features in terms of the attributes of clothing (e.g., color, texture). However, it is common for people to wear black clothes or be captured by surveillance systems in low light illumination, in which cases the attributes of the clothing are severely missing. We call this problem the Black Re-ID problem. To solve this problem, rather than relying on the clothing information, we propose to exploit head-shoulder features to assist person Re-ID. The head-shoulder adaptive attention network (HAA) is proposed to learn the head-shoulder feature and an innovative ensemble method is designed to enhance the generalization of our model. Given the input person image, the ensemble method would focus on the head-shoulder feature by assigning a larger weight if the individual insides the image is in black clothing. Due to the lack of a suitable benchmark dataset for studying the Black Re-ID problem, we also contribute the first Black-reID dataset, which contains 1274 identities in training set. Extensive evaluations on the Black-reID, Market1501 and DukeMTMC-reID datasets show that our model achieves the best result compared with the state-of-the-art Re-ID methods on both Black and conventional Re-ID problems. Furthermore, our method is also proved to be effective in dealing with person Re-ID in similar clothing. Our code and dataset are avaliable on <a class="link-external link-https" href="https://github.com/xbq1994/" rel="external noopener nofollow">this https URL</a>.      
### 27.Scene Text Detection with Selected Anchor  [ :arrow_down: ](https://arxiv.org/pdf/2008.08523.pdf)
>  Object proposal technique with dense anchoring scheme for scene text detection were applied frequently to achieve high recall. It results in the significant improvement in accuracy but waste of computational searching, regression and classification. In this paper, we propose an anchor selection-based region proposal network (AS-RPN) using effective selected anchors instead of dense anchors to extract text proposals. The center, scales, aspect ratios and orientations of anchors are learnable instead of fixing, which leads to high recall and greatly reduced numbers of anchors. By replacing the anchor-based RPN in Faster RCNN, the AS-RPN-based Faster RCNN can achieve comparable performance with previous state-of-the-art text detecting approaches on standard benchmarks, including COCO-Text, ICDAR2013, ICDAR2015 and MSRA-TD500 when using single-scale and single model (ResNet50) testing only.      
### 28.Statistical CSI based Design for Intelligent Reflecting Surface Assisted MISO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.08453.pdf)
>  This paper considers an intelligent reflecting surface (IRS) aided multiple-input single-output communication system, where statistical channel state information (CSI) is exploited for transmit beamforming and IRS beamforming. A tight upper bound is derived for the ergodic capacity of the system. Based on which, the joint optimization of transmit beam and IRS beam are studied. Depending on whether a line-of-sight path exists between the access point and user, two different cases, namely, Rician fading and Rayleigh fading, are separately treated. Specifically, for the Rician fading case, an iterative algorithm is proposed, which is guaranteed to converge. For the Rayleigh fading case, closed-form designs are obtained for the transmit beam and IRS beam. Simulation results show the proposed beamforming scheme achieves similar performance as the benchmark algorithm requiring instantaneous CSI.      
### 29.A Maximum Independent Set Method for Scheduling Earth Observing Satellite Constellations  [ :arrow_down: ](https://arxiv.org/pdf/2008.08446.pdf)
>  Operating Earth observing satellites requires efficient planning methods that coordinate activities of multiple spacecraft. The satellite task planning problem entails selecting actions that best satisfy mission objectives for autonomous execution. Task scheduling is often performed by human operators assisted by heuristic or rule-based planning tools. This approach does not efficiently scale to multiple assets as heuristics frequently fail to properly coordinate actions of multiple vehicles over long horizons. Additionally, the problem becomes more difficult to solve for large constellations as the complexity of the problem scales exponentially in the number of requested observations and linearly in the number of spacecraft. It is expected that new commercial optical and radar imaging constellations will require automated planning methods to meet stated responsiveness and throughput objectives. This paper introduces a new approach for solving the satellite scheduling problem by generating an infeasibility-based graph representation of the problem and finding a maximal independent set of vertices for the graph. The approach is tested on a scenarios of up to 10,000 requested imaging locations for the Skysat constellation of optical satellites as well as simulated constellations of up to 24 satellites. Performance is compared with contemporary graph-traversal and mixed-integer linear programming approaches. Empirical results demonstrate improvements in both the solution time along with the number of scheduled collections beyond baseline methods. For large problems, the maximum independent set approach is able find a feasible schedule with 8% more collections in 75% less time.      
### 30.Deep Neural Networks for automatic extraction of features in time series satellite images  [ :arrow_down: ](https://arxiv.org/pdf/2008.08432.pdf)
>  Many earth observation programs such as Landsat, Sentinel, SPOT, and Pleiades produce huge volume of medium to high resolution multi-spectral images every day that can be organized in time series. In this work, we exploit both temporal and spatial information provided by these images to generate land cover maps. For this purpose, we combine a fully convolutional neural network with a convolutional long short-term memory. Implementation details of the proposed spatio-temporal neural network architecture are provided. Experimental results show that the temporal information provided by time series images allows increasing the accuracy of land cover classification, thus producing up-to-date maps that can help in identifying changes on earth.      
### 31.Graudally Applying Weakly Supervised and Active Learning for Mass Detection in Breast Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.08416.pdf)
>  We propose a method for effectively utilizing weakly annotated image data in an object detection tasks of breast ultrasound images. Given the problem setting where a small, strongly annotated dataset and a large, weakly annotated dataset with no bounding box information are available, training an object detection model becomes a non-trivial problem. We suggest a controlled weight for handling the effect of weakly annotated images in a two stage object detection model. We~also present a subsequent active learning scheme for safely assigning weakly annotated images a strong annotation using the trained model. Experimental results showed a 24\% point increase in correct localization (CorLoc) measure, which is the ratio of correctly localized and classified images, by assigning the properly controlled weight. Performing active learning after a model is trained showed an additional increase in CorLoc. We tested the proposed method on the Stanford Dog datasets to assure that it can be applied to general cases, where strong annotations are insufficient to obtain resembling results. The presented method showed that higher performance is achievable with lesser annotation effort.      
### 32.Deep Volumetric Ambient Occlusion  [ :arrow_down: ](https://arxiv.org/pdf/2008.08345.pdf)
>  We present a novel deep learning based technique for volumetric ambient occlusion in the context of direct volume rendering. Our proposed Deep Volumetric Ambient Occlusion (DVAO) approach can predict per-voxel ambient occlusion in volumetric data sets, while considering global information provided through the transfer function. The proposed neural network only needs to be executed upon change of this global information, and thus supports real-time volume interaction. Accordingly, we demonstrate DVAOs ability to predict volumetric ambient occlusion, such that it can be applied interactively within direct volume rendering. To achieve the best possible results, we propose and analyze a variety of transfer function representations and injection strategies for deep neural networks. Based on the obtained results we also give recommendations applicable in similar volume learning scenarios. Lastly, we show that DVAO generalizes to a variety of modalities, despite being trained on computed tomography data only.      
### 33.DIRECT-Net: a unified mutual-domain material decomposition network for quantitative dual-energy CT imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.08331.pdf)
>  By acquiring two sets of tomographic measurements at distinct X-ray spectra, the dual-energy CT (DECT) enables quantitative material-specific imaging. However, the conventionally decomposed material basis images may encounter severe image noise amplification and artifacts, resulting in degraded image quality and decreased quantitative accuracy. Iterative DECT image reconstruction algorithms incorporating either the sinogram or the CT image prior information have shown potential advantages in noise and artifact suppression, but with the expense of large computational resource, prolonged reconstruction time, and tedious manual selections of algorithm parameters. To partially overcome these limitations, we develop a domain-transformation enabled end-to-end deep convolutional neural network (DIRECT-Net) to perform high quality DECT material decomposition. Specifically, the proposed DIRECT-Net has immediate accesses to mutual-domain data, and utilizes stacked convolution neural network (CNN) layers for noise reduction and material decomposition. The training data are numerically simulated based on the underlying physics of DECT imaging.The XCAT digital phantom, iodine solutions phantom, and biological specimen are used to validate the performance of DIRECT-Net. The qualitative and quantitative results demonstrate that this newly developed DIRECT-Net is promising in suppressing noise, improving image accuracy, and reducing computation time for future DECT imaging.      
### 34.DIR-DBTnet: Deep iterative reconstruction network for 3D digital breast tomosynthesis imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.08322.pdf)
>  Purpose: The goal of this study is to develop a novel deep learning (DL) based reconstruction framework to improve the digital breast tomosynthesis (DBT) imaging performance. Methods: In this work, the DIR-DBTnet is developed for DBT image reconstruction by unrolling the standard iterative reconstruction algorithm within the deep learning framework. In particular, such network learns the regularizer and the iteration parameters automatically through network training with a large amount of simulated DBT data. Afterwards, both numerical and experimental data are used to evaluate its performance. Quantitative metrics such as the artifact spread function (ASF), breast density, and the signal difference to noise ratio (SDNR) are used for image quality assessment. Results: For both numerical and experimental data, the proposed DIR-DBTnet generates reduced in-plane shadow artifacts and out-of-plane artifacts compared with the filtered back projection (FBP) and total variation (TV) methods. Quantitatively, the full width half maximum (FWHM) of the measured ASF curve from the numerical data is 33.4% and 19.7% smaller than those obtained with the FBP and TV methods, respectively; the breast density of the network reconstructed DBT images is more accurate and consistent with the ground truth. Conclusions: In conclusion, a deep iterative reconstruction network, DIR-DBTnet, has been proposed. Both qualitative and quantitative analyses of the numerical and experimental results show superior DBT imaging performance than the FBP and iterative algorithms.      
### 35.Deep Relighting Networks for Image Light Source Manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2008.08298.pdf)
>  Manipulating the light source of given images is an interesting task and useful in various applications, including photography and cinematography. Existing methods usually require additional information like the geometric structure of the scene, which may not be available for most images. In this paper, we formulate the single image relighting task and propose a novel Deep Relighting Network (DRN) with three parts: 1) scene reconversion, which aims to reveal the primary scene structure through a deep auto-encoder network, 2) shadow prior estimation, to predict light effect from the new light direction through adversarial learning, and 3) re-renderer, to combine the primary structure with the reconstructed shadow view to form the required estimation under the target light source. Experimental results show that the proposed method outperforms other possible methods, both qualitatively and quantitatively. Specifically, the proposed DRN has achieved the best PSNR in the "AIM2020 - Any to one relighting challenge" of the 2020 ECCV conference.      
### 36.Noncoherent OOK Symbol Detection with Supervised-Learning Approach for BCC  [ :arrow_down: ](https://arxiv.org/pdf/2008.08286.pdf)
>  There has been a continuing demand for improving the accuracy and ease of use of medical devices used on or around the human body. Communication is critical to medical applications, and wireless body area networks (WBANs) have the potential to revolutionize diagnosis. Despite its importance, WBAN technology is still in its infancy and requires much research. We consider body channel communication (BCC), which uses the whole body as well as the skin as a medium for communication. BCC is sensitive to the body's natural circulation and movement, which requires a noncoherent model for wireless communication. To accurately handle practical applications for electronic devices working on or inside a human body, we configure a realistic system model for BCC with on-off keying (OOK) modulation. We propose novel detection techniques for OOK symbols and improve the performance by exploiting distributed reception and supervised-learning approaches. Numerical results show that the proposed techniques are valid for noncoherent OOK transmissions for BCC.      
### 37.Heteroscedastic Uncertainty for Robust Generative Latent Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2008.08157.pdf)
>  Learning or identifying dynamics from a sequence of high-dimensional observations is a difficult challenge in many domains, including reinforcement learning and control. The problem has recently been studied from a generative perspective through latent dynamics: high-dimensional observations are embedded into a lower-dimensional space in which the dynamics can be learned. Despite some successes, latent dynamics models have not yet been applied to real-world robotic systems where learned representations must be robust to a variety of perceptual confounds and noise sources not seen during training. In this paper, we present a method to jointly learn a latent state representation and the associated dynamics that is amenable for long-term planning and closed-loop control under perceptually difficult conditions. As our main contribution, we describe how our representation is able to capture a notion of heteroscedastic or input-specific uncertainty at test time by detecting novel or out-of-distribution (OOD) inputs. We present results from prediction and control experiments on two image-based tasks: a simulated pendulum balancing task and a real-world robotic manipulator reaching task. We demonstrate that our model produces significantly more accurate predictions and exhibits improved control performance, compared to a model that assumes homoscedastic uncertainty only, in the presence of varying degrees of input degradation.      
