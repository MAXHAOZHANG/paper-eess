# ArXiv eess --Fri, 31 Jul 2020
### 1.Simultaneous state and parameter estimation: the role of sensitivity analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.15607.pdf)
>  State and parameter estimation is essential for process monitoring and control. Observability plays an important role in both state and parameter estimation. In simultaneous state and parameter estimation, the parameters are often augmented as extra states of the original system. When the augmented system is observable, various existing state estimation approaches may be used to estimate the states and parameters simultaneously. However, when the augmented system is not observable, how we should proceed to maximally extract the information contained in the measured outputs is not clear. This paper concerns about simultaneous state and parameter estimation when the augmented system is not fully observable. Specifically, we first show how sensitivity analysis is related to observability of a dynamical system, and then illustrate how it may be used to select variables for simultaneous estimation. We also propose a moving horizon state estimation (MHE) design that can use the variable selection results in a natural way. Extensive simulations are carried out to show the efficiency of the proposed approach.      
### 2.Performance Analysis of Metaheuristic Optimization Algorithms in Estimating the Interfacial Heat Transfer Coefficient on Directional Solidification  [ :arrow_down: ](https://arxiv.org/pdf/2007.15583.pdf)
>  In this paper is proposed an evaluation of ten metaheuristic optimization algorithms applied on the inverse optimization of the Interfacial Heat Transfer Coefficient (IHTC) coupled on the solidification phenomenon. It was considered an upward directional solidification system for Al-7wt.% Si alloy and, for IHTC model, a exponential time function. All thermophysical properties of the alloy were considered constant. Scheil Rule was used as segregation model ahead phase-transformation interface. Optimization results from Markov Chain Monte Carlo method (MCMC) were considered as reference. Based on average, quantiles 95% and 5%, kurtosis, average iterations and absolute errors of the metaheuristic methods, in relation to MCMC results, the Flower Pollination Algorithm (FPA) and Moth-Flame Optimization (MFO) presented the most appropriate results, outperforming the other methods in this particular phenomenon, based on these metrics. The regions with the most probable values for parameters in IHTC time function were also determined.      
### 3.Host Load Prediction with Bi-directional Long Short-Term Memory in Cloud Computing  [ :arrow_down: ](https://arxiv.org/pdf/2007.15582.pdf)
>  Host load prediction is the basic decision information for managing the computing resources usage on the cloud platform, its accuracy is critical for achieving the servicelevel agreement. Host load data in cloud environment is more high volatility and noise compared to that of grid computing, traditional data-driven methods tend to have low predictive accuracy when dealing with host load of cloud computing, Thus, we have proposed a host load prediction method based on Bidirectional Long Short-Term Memory (BiLSTM) in this paper. Our BiLSTM-based apporach improve the memory capbility and nonlinear modeling ability of LSTM and LSTM Encoder-Decoder (LSTM-ED), which is used in the recent previous work, In order to evaluate our approach, we have conducted experiments using a 1-month trace of a Google data centre with more than twelve thousand machines. our BiLSTM-based approach successfully achieves higher accuracy than other previous models, including the recent LSTM one and LSTM-ED one.      
### 4.Introduction to UAN Power Equipment Condition Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2007.15581.pdf)
>  Power systems are equipment and data intensive. In todays big data era, a large amount of data is being generated from power system equipment via various inspection and testing activities. The industry is encouraged to make optimal decisions for power equipment maintenance, replacement, configuration and planning. Utility companies are trying to leverage the use of big data and advanced analytics for equipment management in order to balance system reliability, performance and cost. To advance such applications and promote collaborations with external researchers, the Utility Analytics Network has made the effort to gather 15 datasets from multiple utility companies in North America on various types of power equipment. These datasets are now shared in public and this paper serves the purpose of providing a detailed introduction to the datasets.      
### 5.Deep-Learning based Inverse Modeling Approaches: A Subsurface Flow Example  [ :arrow_down: ](https://arxiv.org/pdf/2007.15580.pdf)
>  Deep-learning has achieved good performance and shown great potential for solving forward and inverse problems. In this work, two categories of innovative deep-learning based inverse modeling methods are proposed and compared. The first category is deep-learning surrogate-based inversion methods, in which the Theory-guided Neural Network (TgNN) is constructed as a deep-learning surrogate for problems with uncertain model parameters. By incorporating physical laws and other constraints, the TgNN surrogate can be constructed with limited simulation runs and accelerate the inversion process significantly. Three TgNN surrogate-based inversion methods are proposed, including the gradient method, the iterative ensemble smoother (IES), and the training method. The second category is direct-deep-learning-inversion methods, in which TgNN constrained with geostatistical information, named TgNN-geo, is proposed for direct inverse modeling. In TgNN-geo, two neural networks are introduced to approximate the respective random model parameters and the solution. Since the prior geostatistical information can be incorporated, the direct-inversion method based on TgNN-geo works well, even in cases with sparse spatial measurements or imprecise prior statistics. Although the proposed deep-learning based inverse modeling methods are general in nature, and thus applicable to a wide variety of problems, they are tested with several subsurface flow problems. It is found that satisfactory results are obtained with a high efficiency. Moreover, both the advantages and disadvantages are further analyzed for the proposed two categories of deep-learning based inversion methods.      
### 6.A Brain Emotional Learning-inspired Model For the Prediction of Geomagnetic Storms  [ :arrow_down: ](https://arxiv.org/pdf/2007.15579.pdf)
>  This study suggests a new data-driven model for the prediction of geomagnetic storm. The model which is an instance of Brain Emotional Learning Inspired Models (BELIMs), is known as the Brain Emotional Learning-based Prediction Model (BELPM). BELPM consists of four main subsystems; the connection between these subsystems has been mimicked by the corresponding regions of the emotional system. The functions of these subsystems are explained using adaptive networks. The learning algorithm of BELPM is defined using the steepest descent (SD) and the least square estimator (LSE). BELPM is employed to predict geomagnetic storms using two geomagnetic indices, Auroral Electrojet (AE) Index and Disturbance Time (Dst) Index. To evaluate the performance of BELPM, the obtained results have been compared with ANFIS, WKNN and other instances of BELIMs. The results verify that BELPM has the capability to achieve a reasonable accuracy for both the short-term and the long-term geomagnetic storms prediction.      
### 7.Radar pulse correlation from uniformly translating targets  [ :arrow_down: ](https://arxiv.org/pdf/2007.15578.pdf)
>  Radars often use correlation of received signals with transmitted signals to identify targets. However, when a target translates at a high uniform speed, the correlation of the transmitted and received signals depends strongly on the target's velocity, even after the two-way Doppler shift has been removed. This is true whether the target is made of highly dispersive material or not.      
### 8.Comparative study of deep learning methods for the automatic segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients  [ :arrow_down: ](https://arxiv.org/pdf/2007.15546.pdf)
>  Recent research on COVID-19 suggests that CT imaging provides useful information to assess disease progression and assist diagnosis, in addition to help understanding the disease. There is an increasing number of studies that propose to use deep learning to provide fast and accurate quantification of COVID-19 using chest CT scans. The main tasks of interest are the automatic segmentation of lung and lung lesions in chest CT scans of confirmed or suspected COVID-19 patients. In this study, we compare twelve deep learning algorithms using a multi-center dataset, including both open-source and in-house developed algorithms. Results show that ensembling different methods can boost the overall test set performance for lung segmentation, binary lesion segmentation and multiclass lesion segmentation, resulting in mean Dice scores of 0.982, 0.724 and 0.469, respectively. The resulting binary lesions were segmented with a mean absolute volume error of 91.3 ml. In general, the task of distinguishing different lesion types was more difficult, with a mean absolute volume difference of 152 ml and mean Dice scores of 0.369 and 0.523 for consolidation and ground glass opacity, respectively. All methods perform binary lesion segmentation with an average volume error that is better than visual assessment by human raters, suggesting these methods are mature enough for a large-scale evaluation for use in clinical practice.      
### 9.Simultaneous Estimation of State and Packet-Loss Occurrences in Networked Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.15515.pdf)
>  Estimating the occurrence of packet losses in a networked control systems (NCS) can be used to improve the control performance and to detect failures or cyber-attacks. This study considers simultaneous estimation of the plant state and the packet loss occurrences at each time step. After formulation of the problem, two solutions are proposed. In the first one, an input-output representation of the NCS model is used to design a recursive filter for estimation of the packet loss occurrences. This estimation is then used for state estimation through Kalman filtering. In the second solution, a state space model of NCS is used to design an estimator for both the plant state and the packet loss occurrences which employs a Kalman filter. The effectiveness of the solutions is shown during an example and comparisons are made between the proposed solutions and another solution based on the interacting multiple model estimation method.      
### 10.multiMap: A Gradient Spoiled Sequence for Simultaneously Measuring B1+, B0, T1/M0, T2, T2*, and Fat Fraction of a Slice  [ :arrow_down: ](https://arxiv.org/pdf/2007.15495.pdf)
>  We propose multiMap, a single scan that can generate several quantitative maps simultaneously. The sequence acquires multiple images in a time-efficient manner, which can be modeled for T_2, T2*, main- and transmit-field inhomogeneity, T_1:equilibrium magnetization, and water and fat content. The sequence is constructed so that cross-dependencies between parameters are isolated. Thus, each parameter can be estimated independently. Estimates of all parameters are shown on bottle phantoms, the brain, and the knee. The results are compared to estimates from established techniques.      
### 11.Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling  [ :arrow_down: ](https://arxiv.org/pdf/2007.15474.pdf)
>  High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate "sliding faders" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the "faders" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.      
### 12.A Practical Application of Sliding Mode Control in the Motion Control of a High Precision Piezoelectric Motor  [ :arrow_down: ](https://arxiv.org/pdf/2007.15462.pdf)
>  This paper proposes a practical implementation of sliding mode control (SMC) that utilizes partial modeling compensation. Sliding mode control is well known for its effectiveness as a model free control approach, however, its effectiveness is degraded if there is a constraint on the control gain or limitation on the switching frequency in digital implementation. This is especially the case with systems that involve static friction. This approach aims to enhance the effectiveness of SMC by partial model compensation. Rigorous stability proofs are presented to validate the approach. In addition, experiments are carried out on a piezoelectric motor driven linear stage and the control approach is compared with the Discrete-Time Integral Sliding Mode (DTISMC) approach proposed by Abidi et al. as well as conventional PI control. The results show that the proposed control approach has a superior performance in comparison to the other approaches tested.      
### 13.Robust Output Feedback Stabilization of MIMO Invertible Nonlinear Systems with Output-Dependent Multipliers (extended version)  [ :arrow_down: ](https://arxiv.org/pdf/2007.15435.pdf)
>  This note studies the robust output feedback stabilization problem of multi-input multi-output invertible nonlinear systems with output-dependent multipliers. An "ideal" state feedback is first designed under certain mild assumptions. Then, a set of extended low-power high-gain observers is systematically designed, providing a complete estimation of the "ideal" feedback law. This yields a robust output feedback stabilizer such that the origin of the closed-loop system is semiglobally asymptotically stable, while improving the numerical implementation with the power of high-gain parameters up to 2.      
### 14.A Nonlinear Dynamic Boiler Model accounting for Highly Variable Load Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2007.15433.pdf)
>  This paper describes a new nonlinear dynamic model for a natural circulation boiler. The model is based on physical principles, i.e. mass, energy and momentum balances. A systematic approach is followed leading to new insights into the physics of drum water level and downcomer mass flow. The final model captures fast dynamic responses that are necessary to describe the operation of a boiler under highly variable load conditions. New features of the model include (i) a multi-compartment model for the risers, (ii) a new model for drum water level, and (iii) a new dynamic model for the flow of water in the downcomers. Implications of the model for control system design are also explored.      
### 15.Searching for Pneumothorax in Half a Million Chest X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.15429.pdf)
>  Pneumothorax, a collapsed or dropped lung, is a fatal condition typically detected on a chest X-ray by an experienced radiologist. Due to shortage of such experts, automated detection systems based on deep neural networks have been developed. Nevertheless, applying such systems in practice remains a challenge. These systems, mostly compute a single probability as output, may not be enough for diagnosis. On the contrary, content-based medical image retrieval (CBIR) systems, such as image search, can assist clinicians for diagnostic purposes by enabling them to compare the case they are examining with previous (already diagnosed) cases. However, there is a lack of study on such attempt. In this study, we explored the use of image search to classify pneumothorax among chest X-ray images. All chest X-ray images were first tagged with deep pretrained features, which were obtained from existing deep learning models. Given a query chest X-ray image, the majority voting of the top K retrieved images was then used as a classifier, in which similar cases in the archive of past cases are provided besides the probability output. In our experiments, 551,383 chest X-ray images were obtained from three large recently released public datasets. Using 10-fold cross-validation, it is shown that image search on deep pretrained features achieved promising results compared to those obtained by traditional classifiers trained on the same features. To the best of knowledge, it is the first study to demonstrate that deep pretrained features can be used for CBIR of pneumothorax in half a million chest X-ray images.      
### 16.Very Deep Super-Resolution of Remotely Sensed Images with Mean Square Error and Var-norm Estimators as Loss Functions  [ :arrow_down: ](https://arxiv.org/pdf/2007.15417.pdf)
>  In this work, very deep super-resolution (VDSR) method is presented for improving the spatial resolution of remotely sensed (RS) images for scale factor 4. The VDSR net is re-trained with Sentinel-2 images and with drone aero orthophoto images, thus becomes RS-VDSR and Aero-VDSR, respectively. A novel loss function, the Var-norm estimator, is proposed in the regression layer of the convolutional neural network during re-training and prediction. According to numerical and optical comparisons, the proposed nets RS-VDSR and Aero-VDSR can outperform VDSR during prediction with RS images. RS-VDSR outperforms VDSR up to 3.16 dB in terms of PSNR in Sentinel-2 images.      
### 17.Sampling-based 3-D Line-of-Sight PWA Model Predictive Control for Autonomous Rendezvous and Docking with a Tumbling Target  [ :arrow_down: ](https://arxiv.org/pdf/2007.15390.pdf)
>  In this paper, a model predictive control (MPC) framework is employed to realize autonomous rendezvous and docking (AR&amp;D) with a tumbling target, using the piecewise affine (PWA) model of the 3-D line-of-sight (LOS) dynamics and Euler attitude dynamics. Consider the error between the predictions obtained by the approximate linear model and the actual states of nonlinear dynamics, a sampling-based PWA MPC is proposed to sample the predictions in the closer neighborhood of the actual states. Besides, novel constructions of constraints are presented to reduce the on-board computation cost and time-delay. Furthermore, a singularity-free strategy is provided to realize crossing the singularity of angle states smoothly. Then, the mission is achieved by continuous 6-DOF pose (position and attitude) tracking of the target's docking port, with the coupling between the position and attitude of the target's docking port is taken into account. Finally, numerical results are presented to demonstrate the above theories.      
### 18.Dense Small Satellite Networks for Modern Terrestrial Communication Systems: Benefits, Infrastructure, and Technologies  [ :arrow_down: ](https://arxiv.org/pdf/2007.15377.pdf)
>  Dense small satellite networks (DSSN) in low earth orbits (LEO) can benefit several mobile terrestrial communication systems (MTCS). However, the potential benefits can only be achieved through careful consideration of DSSN infrastructure and identification of suitable DSSN technologies. In this paper, we discuss several components of DSSN infrastructure including satellite formations, orbital paths, inter-satellite communication (ISC) links, and communication architectures for data delivery from source to destination. We also review important technologies for DSSN as well as the challenges involved in the use of these technologies in DSSN. Several open research directions to enhance the benefits of DSSN for MTCS are also identified in the paper. A case study showing the integration benefits of DSSN in MTCS is also included.      
### 19.Complex-valued Imaging with Total Variation Regularization: An Application to Full-Waveform Inversion in Visco-acoustic Media  [ :arrow_down: ](https://arxiv.org/pdf/2007.15332.pdf)
>  Full waveform inversion (FWI) is a nonlinear PDE constrained optimization problem, which seeks to estimate constitutive parameters of a medium such as phase velocity, density, and anisotropy, by fitting waveforms. Attenuation is an additional parameter that needs to be taken into account in viscous media to exploit the full potential of FWI. Attenuation is more easily implemented in the frequency domain by using complex-valued velocities in the time-harmonic wave equation. These complex velocities are frequency-dependent to guarantee causality and account for dispersion. Since estimating a complex frequency-dependent velocity at each grid point in space is not realistic, the optimization is generally performed in the real domain by processing the phase velocity (or slowness) at a reference frequency and attenuation (or quality factor) as separate real parameters. This real parametrization requires an a priori empirical relation (such as the nonlinear Kolsky-Futterman (KF) or standard linear solid (SLS) attenuation models) between the complex velocity and the two real quantities, which is prone to generate modeling errors if it does not represent accurately the attenuation behavior of the subsurface. Moreover, it leads to a multivariate inverse problem, which is twice larger than the actual size of the medium and ill-posed due to the cross-talk between the two classes of real parameters. To alleviate these issues, we present a mono-variate algorithm that solves directly the optimization problem in the complex domain by processing in sequence narrow bands of frequencies under the assumption of band-wise frequency dependence of the sought complex velocities.      
### 20.A Comparative Re-Assessment of Feature Extractors for Deep Speaker Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2007.15283.pdf)
>  Modern automatic speaker verification relies largely on deep neural networks (DNNs) trained on mel-frequency cepstral coefficient (MFCC) features. While there are alternative feature extraction methods based on phase, prosody and long-term temporal operations, they have not been extensively studied with DNN-based methods. We aim to fill this gap by providing extensive re-assessment of 14 feature extractors on VoxCeleb and SITW datasets. Our findings reveal that features equipped with techniques such as spectral centroids, group delay function, and integrated noise suppression provide promising alternatives to MFCCs for deep speaker embeddings extraction. Experimental results demonstrate up to 16.3\% (VoxCeleb) and 25.1\% (SITW) relative decrease in equal error rate (EER) to the baseline.      
### 21.Speaking Speed Control of End-to-End Speech Synthesis using Sentence-Level Conditioning  [ :arrow_down: ](https://arxiv.org/pdf/2007.15281.pdf)
>  This paper proposes a controllable end-to-end text-to-speech (TTS) system to control the speaking speed (speed-controllable TTS; SCTTS) of synthesized speech with sentence-level speaking-rate value as an additional input. The speaking-rate value, the ratio of the number of input phonemes to the length of input speech, is adopted in the proposed system to control the speaking speed. Furthermore, the proposed SCTTS system can control the speaking speed while retaining other speech attributes, such as the pitch, by adopting the global style token-based style encoder. The proposed SCTTS does not require any additional well-trained model or an external speech database to extract phoneme-level duration information and can be trained in an end-to-end manner. In addition, our listening tests on fast-, normal-, and slow-speed speech showed that the SCTTS can generate more natural speech than other phoneme duration control approaches which increase or decrease duration at the same rate for the entire sentence, especially in the case of slow-speed speech.      
### 22.VocGAN: A High-Fidelity Real-time Vocoder with a Hierarchically-nested Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.15256.pdf)
>  We present a novel high-fidelity real-time neural vocoder called VocGAN. A recently developed GAN-based vocoder, MelGAN, produces speech waveforms in real-time. However, it often produces a waveform that is insufficient in quality or inconsistent with acoustic characteristics of the input mel spectrogram. VocGAN is nearly as fast as MelGAN, but it significantly improves the quality and consistency of the output waveform. VocGAN applies a multi-scale waveform generator and a hierarchically-nested discriminator to learn multiple levels of acoustic properties in a balanced way. It also applies the joint conditional and unconditional objective, which has shown successful results in high-resolution image synthesis. In experiments, VocGAN synthesizes speech waveforms 416.7x faster on a GTX 1080Ti GPU and 3.24x faster on a CPU than real-time. Compared with MelGAN, it also exhibits significantly improved quality in multiple evaluation metrics including mean opinion score (MOS) with minimal additional overhead. Additionally, compared with Parallel WaveGAN, another recently developed high-fidelity vocoder, VocGAN is 6.98x faster on a CPU and exhibits higher MOS.      
### 23.Unsupervised Event Detection, Clustering, and Use Case Exposition in Micro-PMU Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2007.15237.pdf)
>  Distribution-level phasor measurement units, a.k.a, micro-PMUs, report a large volume of high resolution phasor measurements which constitute a variety of event signatures of different phenomena that occur all across power distribution feeders. In order to implement an event-based analysis that has useful applications for the utility operator, one needs to extract these events from a large volume of micro-PMU data. However, due to the infrequent, unscheduled, and unknown nature of the events, it is often a challenge to even figure out what kind of events are out there to capture and scrutinize. In this paper, we seek to address this open problem by developing an unsupervised approach, which requires minimal prior human knowledge. First, we develop an unsupervised event detection method based on the concept of Generative Adversarial Networks (GAN). It works by training deep neural networks that learn the characteristics of the normal trends in micro-PMU measurements; and accordingly detect an event when there is any abnormality. We also propose a two-step unsupervised clustering method, based on a novel linear mixed integer programming formulation. It helps us categorize events based on their origin in the first step and their similarity in the second step. The active nature of the proposed clustering method makes it capable of identifying new clusters of events on an ongoing basis. The proposed unsupervised event detection and clustering methods are applied to real-world micro-PMU data. Results show that they can outperform the prevalent methods in the literature. These methods also facilitate our further analysis to identify important clusters of events that lead to unmasking several use cases that could be of value to the utility operator.      
### 24.Conditions for Convergence of Dynamic Regressor Extension and Mixing Parameter Estimator Using LTI Filters  [ :arrow_down: ](https://arxiv.org/pdf/2007.15224.pdf)
>  In this note we study the conditions for convergence of recently introduced dynamic regressor extension and mixing (DREM) parameter estimator when the extended regressor is generated using LTI filters. In particular, we are interested in relating these conditions with the ones required for convergence of the classical gradient (or least squares), namely the well-known persistent excitation (PE) requirement on the original regressor vector, $\phi(t) \in \mathbb{R}^q$, with $q \in \mathbb{N}$ the number of unknown parameters. Moreover, we study the case when only interval excitation (IE) is available, under which DREM, concurrent and composite learning schemes ensure global convergence, being the convergence for DREM in finite time. Regarding PE we prove that if $\phi(t)$ is PE then the scalar regressor of DREM, $\Delta(t) \in \mathbb{R}$, is also PE, ensuring exponential convergence. Concerning IE we prove that if $\phi(t)$ is IE then $\Delta(t)$ is also IE. All these results are established in the almost sure sense, namely proving that the set of filter parameters for which the claims do not hold is zero measure. The main technical tool used in our proof is inspired by a study of Luenberger observers for nonautonomous nonlinear systems recently reported in the literature.      
### 25.Biogeography-Based Optimization and Support Vector Regression for Freeway Travel Time Prediction and Feature Selection  [ :arrow_down: ](https://arxiv.org/pdf/2007.15212.pdf)
>  As travelers make their choices based on travel time, its prior information can be helpful for them in making more informed travel decisions. To achieve this goal, travel time prediction models have been proposed in literature, but identification of important predictors has not received much attention. Identification of important predictors reduces dimensions of input data, which not only lessens computational load, but also provides better understanding of underlying relationship between important predictors and travel time. Moreover, collection of only important predictors can lead to a significant equipment savings in data collection. Therefore, this study proposes a hybrid approach for feature selection (identifying important predictors) along with developing a robust freeway travel time prediction model. A framework integrating biogeography-based optimization (BBO) and support vector regression (SVR) has been developed. It was validated by predicting travel time at 36.1 km long segment of National Taiwan Freeway No. 1. The proposed hybrid approach is able to develop a prediction model with only six predictors, which is found to have accuracy equivalent to a stand-alone SVR prediction model developed with all forty three predictors.      
### 26.Matlab-Vissim Interface for online optimization of Green Time splits  [ :arrow_down: ](https://arxiv.org/pdf/2007.15208.pdf)
>  VISSIM is a widely used microscopic traffic simulator, which not only provides a graphical user interface to simulate simple static controls (pre-timed or fixed-time) but also offers flexibility to dynamically control simulation through versatile programming languages (C++ and Java etc.). However, to implement various traffic control techniques, integration of computational tools may save lots of effort and time as compared to standard programming platforms. MATLAB falls in the category of a widely used computational tool and also fulfills the primary requirements to control VISSIM simulation dynamically. Therefore, this study proposes and develops a direct interface between MATLAB and VISSIM to extensively harness the computational power of MATLAB. The significance of developed interface is demonstrated on a practical scenario, by conducting an online optimization of green time splits on a study network.      
### 27.Third-Order Statistics Reconstruction from Compressive Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2007.15202.pdf)
>  Estimation of third-order statistics relies on the availability of a huge amount of data records, which can pose severe challenges on the data collecting hardware in terms of considerable storage costs, overwhelming energy consumption, and unaffordably high sampling rate especially when dealing with high-dimensional data such as wideband signals. To overcome these challenges, this paper focuses on the reconstruction of the third-order cumulants under the compressive sensing framework. Specifically, this paper derives a transformed linear system that directly connects the cross-cumulants of compressive measurements to the desired third-order statistics. We provide sufficient conditions for lossless third-order statistics reconstruction via solving simple least-squares, along with the strongest achievable compression ratio. To reduce the computational burden, we also propose an approach to recover diagonal cumulant slices directly from compressive measurements, which is useful when the cumulant slices are sufficient for the inference task at hand. All the proposed techniques are tested via extensive simulations. The developed joint sampling and reconstruction approach to third-order statistics estimation is able to reduce the required sampling rates significantly by exploiting the cumulant structure resulting from signal stationarity, even in the absence of any sparsity constraints on the signal or cumulants.      
### 28.Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability  [ :arrow_down: ](https://arxiv.org/pdf/2007.15188.pdf)
>  Because of its streaming nature, recurrent neural network transducer (RNN-T) is a very promising end-to-end (E2E) model that may replace the popular hybrid model for automatic speech recognition. In this paper, we describe our recent development of RNN-T models with reduced GPU memory consumption during training, better initialization strategy, and advanced encoder modeling with future lookahead. When trained with Microsoft's 65 thousand hours of anonymized training data, the developed RNN-T model surpasses a very well trained hybrid model with both better recognition accuracy and lower latency. We further study how to customize RNN-T models to a new domain, which is important for deploying E2E models to practical scenarios. By comparing several methods leveraging text-only data in the new domain, we found that updating RNN-T's prediction and joint networks using text-to-speech generated from domain-specific text is the most effective.      
### 29.An Insurance Contract Design to Boost Storage Participation in the Electricity Market  [ :arrow_down: ](https://arxiv.org/pdf/2007.15115.pdf)
>  Energy storage technologies are key to improving grid flexibility in the presence of increasing amounts of intermittent renewable generation. We propose an insurance contract that suitably compensates energy storage systems for providing flexibility. Such a contract provides a wider range of market opportunities for these systems while also incentivizing higher renewable penetration in the grid. We consider a day-ahead market in which generators, including renewables and storage owners, bid to be scheduled for the next operating day. Due to production uncertainty, renewable generators may be unable to meet their day-ahead production schedule, and thus be subject to a penalty. As a hedge against these penalties, we propose an insurance contract between a renewable producer and a storage owner, in which the storage reserves some energy to be used in case of renewable shortfalls. We show that such a contract incentivizes the renewable player to bid higher, thus increasing renewable participation in the electricity mix. It also provides an extra source of revenue for storage owners that may not be profitable with a purely arbitrage-based strategy in the day-ahead market. Further, we prove this contract is economically beneficial for both players. We validate our analysis through two case studies.      
### 30.Control Strategies for COVID-19 Epidemic with Vaccination, Shield Immunity and Quarantine: A Metric Temporal Logic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.15114.pdf)
>  Ever since the outbreak of the COVID-19 epidemic, various public health control strategies have been proposed and tested against SARS-CoV-2. In this paper, we study three specific COVID-19 epidemic control models: the susceptible, exposed, infectious, recovered (SEIR) model with vaccination control, the SEIR model with shield immunity control, and the susceptible, un-quarantined infected, quarantined infected, confirmed infected (SUQC) model with quarantine control. We express the control requirement in metric temporal logic (MTL) formulas and develop methods for synthesizing control inputs based on three specific COVID-19 epidemic models with MTL specifications. To the best of our knowledge, this is the first paper to provide automatically-synthesized and fine-tuned control synthesis for the COVID-19 epidemic models with formal specifications. We provide simulation results in three different case studies: vaccination control for the COVID-19 epidemic with model parameters estimated from data in Lombardy, Italy; shield immunity control for the COVID-19 epidemic with model parameters estimated from data in Lombardy, Italy; and quarantine control for the COVID-19 epidemic with model parameters estimated from data in Wuhan, China. The results show that the proposed synthesis approach can generate control inputs within a relatively short time (within 5 seconds) such that the time-varying numbers of individuals in each category (e.g., infectious, immune) satisfy the MTL specifications. The results are also consistent with the claim that control in the early phases of COVID-19 is the most effective in the mitigation.      
### 31.Localization with One-Bit Passive Radars in Narrowband Internet-of-Things using Multivariate Polynomial Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.15108.pdf)
>  Several Internet-of-Things (IoT) applications provide location-based services, wherein it is critical to obtain accurate position estimates by aggregating information from individual sensors. In the recently proposed narrowband IoT (NB-IoT) standard, which trades off bandwidth to gain wide coverage, the location estimation is compounded by the low sampling rate receivers and limited-capacity links. We address both of these NB-IoT drawbacks in the framework of passive sensing devices that receive signals from the target-of-interest. We consider the limiting case where each node receiver employs one-bit analog-to-digital-converters and propose a novel low-complexity nodal delay estimation method using constrained-weighted least squares minimization. To support the low-capacity links to the fusion center (FC), the range estimates obtained at individual sensors are then converted to one-bit data. At the FC, we propose target localization with the aggregated one-bit range vector using both optimal and sub-optimal techniques. The computationally expensive former approach is based on Lasserre's method for multivariate polynomial optimization while the latter employs our less complex iterative joint range-target location estimation (ANTARES) algorithm. Our overall one-bit framework not only complements the low NB-IoT bandwidth but also supports the design goal of inexpensive NB-IoT location sensing. Numerical experiments demonstrate feasibility of the proposed one-bit approach with a $0.6$% increase in the normalized localization error for the small set of $20$-$60$ nodes over the full-precision case. When the number of nodes is sufficiently large ($&gt;80$), the one-bit methods yield the same performance as the full precision.      
### 32.Robust Linear Estimation with Non-parametric Uncertainty: Average and Worst-case Performance  [ :arrow_down: ](https://arxiv.org/pdf/2007.15090.pdf)
>  In this paper, two types of linear estimators are considered for three related estimation problems involving set-theoretic uncertainty pertaining to $\mathcal{H}_{2}$ and $\mathcal{H}_{\infty}$ balls of frequency-responses. The problems at stake correspond to robust $\mathcal{H}_{2}$ and $\mathcal{H}_{\infty}$ in the face of non-parametric "channel-model" uncertainty and to a nominal $\mathcal{H}_{\infty}$ estimation problem. The estimators considered here are defined by minimizing the worst-case squared estimation error over the "uncertainty set" and by minimizing an average cost under the constraint that the worst-case error of any admissible estimator does not exceed a prescribed value. The main point is to explore the derivation of estimators which may be viewed as less conservative alternatives to minimax estimators, or in other words, that allow for trade-offs between worst-case performance and better performance over "large" subsets of the uncertainty set. The "average costs" over $\mathcal{H}_{2}-$signal balls are obtained as limits of averages over sets of finite impulse responses, as their length grows unbounded. The estimator design problems for the two types of estimators and the three problems addressed here are recast as semi-definite programming problems (SDPs, for short). These SDPs are solved in the case of simple examples to illustrate the potential of the "average cost/worst-case constraint" estimators to mitigate the inherent conservatism of the minimax estimators.      
### 33.Exploiting Cross-Lingual Knowledge in Unsupervised Acoustic Modeling for Low-Resource Languages  [ :arrow_down: ](https://arxiv.org/pdf/2007.15074.pdf)
>  (Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.      
### 34.Privacy-preserving Voice Analysis via Disentangled Representations  [ :arrow_down: ](https://arxiv.org/pdf/2007.15064.pdf)
>  Voice User Interfaces (VUIs) are increasingly popular and built into smartphones, home assistants, and Internet of Things (IoT) devices. Despite offering an always-on convenient user experience, VUIs raise new security and privacy concerns for their users. In this paper, we focus on attribute inference attacks in the speech domain, demonstrating the potential for an attacker to accurately infer a target user's sensitive and private attributes (e.g. their emotion, sex, or health status) from deep acoustic models. To defend against this class of attacks, we design, implement, and evaluate a user-configurable, privacy-aware framework for optimizing speech-related data sharing mechanisms. Our objective is to enable primary tasks such as speech recognition and user identification, while removing sensitive attributes in the raw speech data before sharing it with a cloud service provider. We leverage disentangled representation learning to explicitly learn independent factors in the raw data. Based on a user's preferences, a supervision signal informs the filtering out of invariant factors while retaining the factors reflected in the selected preference. Our experimental evaluation over five datasets shows that the proposed framework can effectively defend against attribute inference attacks by reducing their success rates to approximately that of guessing at random, while maintaining accuracy in excess of 99% for the tasks of interest. We conclude that negotiable privacy settings enabled by disentangled representations can bring new opportunities for privacy-preserving applications.      
### 35.Transcending conventional biometry frontiers: Diffusive Dynamics PPG Biometry  [ :arrow_down: ](https://arxiv.org/pdf/2007.15060.pdf)
>  In the first half of the 20th century, a first pulse oximeter was available to measure blood flow changes in the peripheral vascular net. However, it was not until recent times the PhotoPlethysmoGraphic (PPG) signal used to monitor many physiological parameters in clinical environments. Over the last decade, its use has extended to the area of biometrics, with different methods that allow the extraction of characteristic features of each individual from the PPG signal morphology, highly varying with time and the physical states of the subject. In this paper, we present a novel PPG-based biometric authentication system based on convolutional neural networks. Contrary to previous approaches, our method extracts the PPG signal's biometric characteristics from its diffusive dynamics, characterized by geometric patterns image in the (p, q)-planes specific to the 0-1 test. The diffusive dynamics of the PPG signal are strongly dependent on the vascular bed's biostructure, which is unique to each individual, and highly stable over time and other psychosomatic conditions. Besides its robustness, our biometric method is anti-spoofing, given the convoluted nature of the blood network. Our biometric authentication system reaches very low Equal Error Rates (ERRs) with a single attempt, making it possible, by the very nature of the envisaged solution, to implement it in miniature components easily integrated into wearable biometric systems.      
### 36.Stopping Criterion Design for Recursive Bayesian Classification: Analysis and Decision Geometry  [ :arrow_down: ](https://arxiv.org/pdf/2007.15568.pdf)
>  Systems that are based on recursive Bayesian updates for classification limit the cost of evidence collection through certain stopping/termination criteria and accordingly enforce decision making. Conventionally, two termination criteria based on pre-defined thresholds over (i) the maximum of the state posterior distribution; and (ii) the state posterior uncertainty are commonly used. In this paper, we propose a geometric interpretation over the state posterior progression and accordingly we provide a point-by-point analysis over the disadvantages of using such conventional termination criteria. For example, through the proposed geometric interpretation we show that confidence thresholds defined over maximum of the state posteriors suffer from stiffness that results in unnecessary evidence collection whereas uncertainty based thresholding methods are fragile to number of categories and terminate prematurely if some state candidates are already discovered to be unfavorable. Moreover, both types of termination methods neglect the evolution of posterior updates. We then propose a new stopping/termination criterion with a geometrical insight to overcome the limitations of these conventional methods and provide a comparison in terms of decision accuracy and speed. We validate our claims using simulations and using real experimental data obtained through a brain computer interfaced typing system.      
### 37.Quantitative Distortion Analysis of Flattening Applied to the Scroll from En-Gedi  [ :arrow_down: ](https://arxiv.org/pdf/2007.15551.pdf)
>  Non-invasive volumetric imaging can now capture the internal structure and detailed evidence of ink-based writing from within the confines of damaged and deteriorated manuscripts that cannot be physically opened. As demonstrated recently on the En-Gedi scroll, our "virtual unwrapping" software pipeline enables the recovery of substantial ink-based text from damaged artifacts at a quality high enough for serious critical textual analysis. However, the quality of the resulting images is defined by the subjective evaluation of scholars, and a choice of specific algorithms and parameters must be available at each stage in the pipeline in order to maximize the output quality.      
### 38.Capacity of Remote Classification Over Wireless Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.15480.pdf)
>  Wireless connectivity creates a computing paradigm that merges communication and inference. A basic operation in this paradigm is the one where a device offloads classification tasks to the edge servers. We term this remote classification, with a potential to enable intelligent applications. Remote classification is challenged by the finite and variable data rate of the wireless channel, which affects the capability to transfer high-dimensional features and thus limits the classification resolution. We introduce a set of metrics under the name of classification capacity that are defined as the maximum number of classes that can be discerned over a given communication channel while meeting a target classification error probability. The objective is to choose a subset of classes from a library that offers satisfactory performance over a given channel. We treat two cases of subset selection. First, a device can select the subset by pruning the class library until arriving at a subset that meets the targeted error probability while maximizing the classification capacity. Adopting a subspace data model, we prove the equivalence of classification capacity maximization to Grassmannian packing. The results show that the classification capacity grows exponentially with the instantaneous communication rate, and super-exponentially with the dimensions of each data cluster. This also holds for ergodic and outage capacities with fading if the instantaneous rate is replaced with an average rate and a fixed rate, respectively. In the second case, a device has a preference of class subset for every communication rate, which is modeled as an instance of uniformly sampling the library. Without class selection, the classification capacity and its ergodic and outage counterparts are proved to scale linearly with their corresponding communication rates instead of the exponential growth in the last case.      
### 39.Clustering and Power Allocation for UAV-assisted NOMA-VLC Systems: A Swarm Intelligence Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.15430.pdf)
>  Integrating unmanned aerial vehicles (UAV) to non-orthogonal multiple access (NOMA) visible light communications (VLC) exposes many potentials over VLC and NOMA-VLC systems. In this circumstance, user grouping is of importance to reduce the NOMA decoding complexity when the number of users is large; however, this issue has not been considered in the existing study. In this paper, we aim to maximize the weighted sum-rate of all the users by jointly optimizing UAV placement, user grouping, and power allocation in downlink NOMA-VLC systems. We first consider an efficient user clustering strategy, then apply a swarm intelligence approach, namely Harris Hawk Optimization (HHO), to solve the joint UAV placement and power allocation problem. Simulation results show outperformance of the proposed algorithm in comparison with four alternatives: OMA, NOMA without pairing, NOMA-VLC with fixed UAV placement, and random user clustering.      
### 40.Smart Testing and Selective Quarantine for the Control of Epidemics  [ :arrow_down: ](https://arxiv.org/pdf/2007.15412.pdf)
>  This paper is based on the observation that, during epidemics, the choice of which individuals must be tested has an important impact on the effectiveness of selective confinement measures. This decision problem is closely related to the problem of optimal sensor selection, which is a very active research subject in control engineering. The goal of this paper is to propose a policy to smartly select the individuals to be tested. The main idea is to model the epidemics as a stochastic dynamic system and to select the individual to be tested accordingly to some optimality criteria, e.g. to minimize the probability of undetected asymptomatic cases. Every day, the probability of infection of the different individuals is updated making use of the stochastic model of the phenomenon and of the information collected in the previous day. Simulations for a closed community of 10000 individuals show that the proposed technique, coupled with a selective confinement policy, can reduce the spread of the disease while limiting the number of individuals confined if compared to random strategies.      
### 41.A deep learning algorithm for the stable manifolds of the Hamilton-Jacobi equations  [ :arrow_down: ](https://arxiv.org/pdf/2007.15350.pdf)
>  In this paper, we propose a deep learning method to approximate the stable manifolds of the Hamilton-Jacobi equations from nonlinear control systems, and numerically compute optimal feedback controls. Instead of discretizing the phase space, the neural network (NN) is trained on the set of randomly samples firstly and then is refined on enlarged sample set by adaptively generating samples near the points with large errors after the previous training round. Such kind of data generation may make the training more effective. Since our algorithm is meshfree basically, it has a potential to apply to various high-dimensional nonlinear systems. We illustrate the effectiveness of our method by swinging up and stabilizing the Reaction Wheel Pendulums.      
### 42.Who Is Charging My Phone? Identifying Wireless Chargers via Fingerprinting  [ :arrow_down: ](https://arxiv.org/pdf/2007.15348.pdf)
>  With the increasing popularity of the Internet of Things(IoT) devices, the demand for fast and convenient battery charging services grows rapidly. Wireless charging is a promising technology for such a purpose and its usage has become ubiquitous. However, the close distance between the charger and the device being charged not only makes proximity-based and near field communication attacks possible, but also introduces a new type of vulnerabilities. In this paper, we propose to create fingerprints for wireless chargers based on the intrinsic non-linear distortion effects of the underlying charging circuit. Using such fingerprints, we design the WirelessID system to detect potential short-range malicious wireless charging attacks. WirelessID collects signals in the standby state of the charging process and sends them to a trusted server, which can extract the fingerprint and then identify the charger. We conduct experiments on 8 commercial chargers over a period of 5 months and collect 8000 traces of signal. We use 10% of the traces as the training dataset and the rest for testing. Results show that on the standard performance metrics, we have achieved 99.0% precision, 98.9% recall and 98.9% F1-score.      
### 43.New approach to MPI program execution time prediction  [ :arrow_down: ](https://arxiv.org/pdf/2007.15338.pdf)
>  The problem of MPI programs execution time prediction on a certain set of computer installations is considered. This problem emerges with orchestration and provisioning a virtual infrastructure in a cloud computing environment over a heterogeneous network of computer installations: supercomputers or clusters of servers (e.g. mini data centers). One of the key criteria for the effectiveness of the cloud computing environment is the time staying by the program inside the environment. This time consists of the waiting time in the queue and the execution time on the selected physical computer installation, to which the computational resource of the virtual infrastructure is dynamically mapped. One of the components of this problem is the estimation of the MPI programs execution time on a certain set of computer installations. This is necessary to determine a proper choice of order and place for program execution. The article proposes two new approaches to the program execution time prediction problem. The first one is based on computer installations grouping based on the Pearson correlation coefficient. The second one is based on vector representations of computer installations and MPI programs, so-called embeddings. The embedding technique is actively used in recommendation systems, such as for goods (Amazon), for articles (<a class="link-external link-http" href="http://Arxiv.org" rel="external noopener nofollow">Arxiv.org</a>), for videos (YouTube, Netflix). The article shows how the embeddings technique helps to predict the execution time of a MPI program on a certain set of computer installations.      
### 44.Coloured Tobit Kalman Filter  [ :arrow_down: ](https://arxiv.org/pdf/2007.15335.pdf)
>  This paper deals with the Tobit Kalman filtering (TKF) process when the one-dimensional measurements are censored and the noises of the state-space model are coloured. Two improvements of the standard TKF process are proposed. Firstly, the exact moments of the censored measurements are calculated via the moment generating function of the censored measurements. Secondly, coloured noises are considered in the proposed method in order to tackle real-life problems, where the white noises are not common. The designed process is evaluated using two experiments-simulations. The results show that the proposed method outperforms other methods in minimizing the Root Mean Square Error (RMSE) in both experiments.      
### 45.Searching Collaborative Agents for Multi-plane Localization in 3D Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2007.15273.pdf)
>  3D ultrasound (US) is widely used due to its rich diagnostic information, portability and low cost. Automated standard plane (SP) localization in US volume not only improves efficiency and reduces user-dependence, but also boosts 3D US interpretation. In this study, we propose a novel Multi-Agent Reinforcement Learning (MARL) framework to localize multiple uterine SPs in 3D US simultaneously. Our contribution is two-fold. First, we equip the MARL with a one-shot neural architecture search (NAS) module to obtain the optimal agent for each plane. Specifically, Gradient-based search using Differentiable Architecture Sampler (GDAS) is employed to accelerate and stabilize the training process. Second, we propose a novel collaborative strategy to strengthen agents' communication. Our strategy uses recurrent neural network (RNN) to learn the spatial relationship among SPs effectively. Extensively validated on a large dataset, our approach achieves the accuracy of 7.05 degree/2.21mm, 8.62 degree/2.36mm and 5.93 degree/0.89mm for the mid-sagittal, transverse and coronal plane localization, respectively. The proposed MARL framework can significantly increase the plane localization accuracy and reduce the computational cost and model size.      
### 46.Quantum Teleportation for Control of Dynamic Systems and Autonomy  [ :arrow_down: ](https://arxiv.org/pdf/2007.15249.pdf)
>  The application of Quantum Teleportation for control of classical dynamic systems and autonomy is proposed in this paper. Quantum teleportation is an intrinsically quantum phenomenon, and was first introduced by teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels in 1993. In this paper, we consider the possibility of applying this quantum technique to autonomous mobile classical platforms for control and autonomy purposes for the first time in this research. First, a review of how Quantum Entanglement and Quantum Cryptography can be integrated into macroscopic mechanical systems for controls and autonomy applications is presented, as well as how quantum teleportation concepts may be applied to the classical domain. In quantum teleportation, an entangled pair of photons which are correlated in their polarizations are generated and sent to two autonomous platforms, which we call the Alice Robot and the Bob Robot. Alice has been given a quantum system, i.e. a photon, prepared in an unknown state, in addition to receiving an entangled photon. Alice measures the state of her entangled photon and her unknown state jointly and sends the information through a classical channel to Bob. Although Alice original unknown state is collapsed in the process of measuring the state of the entangled photon (due to the quantum non-cloning phenomenon), Bob can construct an accurate replica of Alice state by applying a unitary operator. This paper, and the previous investigations of the applications of hybrid classical-quantum capabilities in control of dynamical systems, are aimed to promote the adoption of quantum capabilities and its advantages to the classical domain particularly for autonomy and control of autonomous classical systems.      
### 47.Solid-state laser refrigeration of nanodiamond quantum sensors  [ :arrow_down: ](https://arxiv.org/pdf/2007.15247.pdf)
>  The negatively-charged nitrogen vacancy (NV$^-$) centre in diamond is a remarkable optical quantum sensor for a range of applications including, nanoscale thermometry, magnetometry, single photon generation, quantum computing, and communication. However, to date the performance of these techniques using NV$^-$ centres has been limited by the thermally-induced spectral wandering of NV$^-$ centre photoluminescence due to detrimental photothermal heating. Here we demonstrate that solid-state laser refrigeration can be used to enable rapid (ms) optical temperature control of nitrogen vacancy doped nanodiamond (NV$^-$:ND) quantum sensors in both atmospheric and \textit{in vacuo} conditions. Nanodiamonds are attached to ceramic microcrystals including 10\% ytterbium doped yttrium lithium fluoride (Yb:LiYF$_4$) and sodium yttrium fluoride (Yb:NaYF$_4$) by van der Waals bonding. The fluoride crystals were cooled through the efficient emission of upconverted infrared photons excited by a focused 1020 nm laser beam. Heat transfer to the ceramic microcrystals cooled the adjacent NV$^-$:NDs by 10 and 27 K at atmospheric pressure and $\sim$10$^{-3}$ Torr, respectively. The temperature of the NV$^-$:NDs was measured using both Debye-Waller factor (DWF) thermometry and optically detected magnetic resonance (ODMR), which agree with the temperature of the laser cooled ceramic microcrystal. Stabilization of thermally-induced spectral wandering of the NV$^{-}$ zero-phonon-line (ZPL) is achieved by modulating the 1020 nm laser irradiance. The demonstrated cooling of NV$^-$:NDs using an optically cooled microcrystal opens up new possibilities for rapid feedback-controlled cooling of a wide range of nanoscale quantum materials.      
### 48.Identification of Failure Regions for Programs with Numeric Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2007.15231.pdf)
>  Failure region, where failure-causing inputs reside, has provided many insights to enhance testing effectiveness of many testing methods. Failure region may also provide some important information to support other processes such as software debugging. When a testing method detects a software failure, indicating that a failure-causing input is identified, the next important question is about how to identify the failure region based on this failure-causing input, i.e., Identification of Failure Regions (IFR). In this paper, we introduce a new IFR strategy, namely Search for Boundary (SB), to identify an approximate failure region of a numeric input domain. SB attempts to identify additional failure-causing inputs that are as close to the boundary of the failure region as possible. To support SB, we provide a basic procedure, and then propose two methods, namely Fixed-orientation Search for Boundary (FSB) and Diverse-orientation Search for Boundary (DSB). In addition, we implemented an automated experimentation platform to integrate these methods. In the experiments, we evaluated the proposed SB methods using a series of simulation studies andempirical studies with different types of failure regions. The results show that our methods can effectively identify a failure region, within the limited testing resources.      
### 49.ISS Estimates in the Spatial Sup-Norm for Nonlinear 1-D Parabolic PDEs  [ :arrow_down: ](https://arxiv.org/pdf/2007.15204.pdf)
>  This paper provides novel Input-to-State Stability (ISS)-style maximum principle estimates for classical solutions of highly nonlinear 1-D parabolic Partial Differential Equations (PDEs). The derivation of the ISS-style maximum principle estimates is performed by using an ISS Lyapunov Functional for the sup norm. The estimates provide fading memory ISS estimates in the sup norm of the state with respect to distributed and boundary inputs. The obtained results can handle parabolic PDEs with nonlinear and non-local in-domain terms/boundary conditions. Three illustrative examples show the efficiency of the proposed methodology for the derivation of ISS estimates in the sup norm of the state.      
### 50.Towards a Sustainable Microgrid on Alderney Island Using a Python-based Energy Planning Tool  [ :arrow_down: ](https://arxiv.org/pdf/2007.15165.pdf)
>  In remote or islanded communities, the use of microgrids (MGs) is necessary to ensure electrification and resilience of supply. However, even in small-scale systems, it is computationally and mathematically challenging to design low-cost, optimal, sustainable solutions taking into consideration all the uncertainties of load demands and power generations from renewable energy sources (RESs). This paper uses the open-source Python-based Energy Planning (PyEPLAN) tool, developed for the design of sustainable MGs in remote areas, on the Alderney island, the 3$^{rd}$ largest of the Channel Islands with a population of about 2000 people. A two-stage stochastic model is used to optimally invest in battery storage, solar power, and wind power units. Moreover, the AC power flow equations are modelled by a linearised version of the DistFlow model in PyEPLAN, where the investment variables are here-and-now decisions and not a function of uncertain parameters while the operation variables are wait-and-see decisions and a function of uncertain parameters. The $k$-means clustering technique is used to generate a set of best (risk-seeker), nominal (risk-neutral), and worst (risk-averse) scenarios capturing the uncertainty spectrum using the yearly historical patterns of load demands and solar/wind power generations. The proposed investment planning tool is a mixed-integer linear programming (MILP) model and is coded with Pyomo in PyEPLAN.      
### 51.Benchmarking and Comparing Multi-exposure Image Fusion Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2007.15156.pdf)
>  Multi-exposure image fusion (MEF) is an important area in computer vision and has attracted increasing interests in recent years. Apart from conventional algorithms, deep learning techniques have also been applied to multi-exposure image fusion. However, although much efforts have been made on developing MEF algorithms, the lack of benchmark makes it difficult to perform fair and comprehensive performance comparison among MEF algorithms, thus significantly hindering the development of this field. In this paper, we fill this gap by proposing a benchmark for multi-exposure image fusion (MEFB) which consists of a test set of 100 image pairs, a code library of 16 algorithms, 20 evaluation metrics, 1600 fused images and a software toolkit. To the best of our knowledge, this is the first benchmark in the field of multi-exposure image fusion. Extensive experiments have been conducted using MEFB for comprehensive performance evaluation and for identifying effective algorithms. We expect that MEFB will serve as an effective platform for researchers to compare performances and investigate MEF algorithms.      
### 52.Foveation for Segmentation of Ultra-High Resolution Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.15124.pdf)
>  Segmentation of ultra-high resolution images is challenging because of their enormous size, consisting of millions or even billions of pixels. Typical solutions include dividing input images into patches of fixed size and/or down-sampling to meet memory constraints. Such operations incur information loss in the field-of-view (FoV) i.e., spatial coverage and the image resolution. The impact on segmentation performance is, however, as yet understudied. In this work, we start with a motivational experiment which demonstrates that the trade-off between FoV and resolution affects the segmentation performance on ultra-high resolution images---and furthermore, its influence also varies spatially according to the local patterns in different areas. We then introduce foveation module, a learnable "dataloader" which, for a given ultra-high resolution image, adaptively chooses the appropriate configuration (FoV/resolution trade-off) of the input patch to feed to the downstream segmentation model at each spatial location of the image. The foveation module is jointly trained with the segmentation network to maximise the task performance. We demonstrate on three publicly available high-resolution image datasets that the foveation module consistently improves segmentation performance over the cases trained with patches of fixed FoV/resolution trade-off. Our approach achieves the SoTA performance on the DeepGlobe aerial image dataset. On the Gleason2019 histopathology dataset, our model achieves better segmentation accuracy for the two most clinically important and ambiguous classes (Gleason Grade 3 and 4) than the top performers in the challenge by 13.1% and 7.5%, and improves on the average performance of 6 human experts by 6.5% and 7.5%. Our code and trained models are available at <a class="link-external link-https" href="https://github.com/lxasqjc/Foveation-Segmentation" rel="external noopener nofollow">this https URL</a>.      
### 53.Energy-Efficient UAV-Assisted Mobile Edge Computing: Resource Allocation and Trajectory Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.15105.pdf)
>  In this paper, we study unmanned aerial vehicle (UAV) assisted mobile edge computing (MEC) with the objective to optimize computation offloading with minimum UAV energy consumption. In the considered scenario, a UAV plays the role of an aerial cloudlet to collect and process the computation tasks offloaded by ground users. Given the service requirements of users, we aim to maximize UAV energy efficiency by jointly optimizing the UAV trajectory, the user transmit power, and computation load allocation. The resulting optimization problem corresponds to nonconvex fractional programming, and the Dinkelbach algorithm and the successive convex approximation (SCA) technique are adopted to solve it. Furthermore, we decompose the problem into multiple subproblems for distributed and parallel problem solving. To cope with the case when the knowledge of user mobility is limited, we adopt a spatial distribution estimation technique to predict the location of ground users so that the proposed approach can still be applied. Simulation results demonstrate the effectiveness of the proposed approach for maximizing the energy efficiency of UAV.      
### 54.A Vision and Framework for the High Altitude Platform Station (HAPS) Networks of the Future  [ :arrow_down: ](https://arxiv.org/pdf/2007.15088.pdf)
>  A High Altitude Platform Station (HAPS) is a network node that operates in the stratosphere at an altitude around 20 km and is instrumental for providing communication services. Triggered by the technological innovations in the areas of autonomous avionics, array antennas, solar panel efficiency levels and the battery energy density, and fueled by the flourishing industry ecosystems, the HAPS exerts itself as an indispensable component of the next generations of wireless networks. In this article, we provide a vision and framework for the HAPS networks of the future supported by a comprehensive and state-of-the-art literature survey. We highlight the undiscovered potential of HAPS systems, and elaborate on their unique ability to serve metropolitan areas. The latest advancements and promising technologies in the HAPS energy and payload systems are discussed. The integration of the emerging Reconfigurable Smart Surface (RSS) technology in the communications payload of HAPS systems for providing a costeffective deployment is proposed. A detailed overview of the radio resource management in HAPS systems is presented along with synergistic physical layer techniques, including Faster-Than-Nyquist (FTN) signaling. Numerous aspects of handoff management in HAPS systems are delineated. The notable contribution of Artificial Intelligence (AI) in HAPS, including machine learning in the design, topology management, handoff, and resource allocation aspects are emphasized. The provided extensive overview of the literature is crucial for substantiating our vision that that depicts the expected deployment opportunities and challenges in the next 10 years (next-generation networks), as well as in the subsequent 10 years (next-next-generation networks).      
### 55.dMelodies: A Music Dataset for Disentanglement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.15067.pdf)
>  Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (approx. 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised disentanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.      
### 56.Greedy Placement of Measurement Devices on Distribution Grids based on Enhanced Distflow State Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2007.15050.pdf)
>  The needs for improving observability of medium and low voltage distribution networks has been significantly increased, in recent year. In this paper, we focus on practical approaches for placement of affordable Measurement Devices (MDs), which are providing three phases voltage, current, and power measurements with certain level of precision. The placement procedure is composed of a state-estimation algorithm and of a greedy placement scheme. The proposed state-estimation algorithm is based on the Distflow model, enhanced to consider the shunt elements (e.g., cable capacitances) of the network, which are not negligible in low voltage networks with underground cables. The greedy placement scheme is formulated such that it finds the location of minimum required number of MDs while certain grid observability limits are satisfied. These limits are defined as the accuracy of state-estimation results in terms of voltage magnitudes and line currents over all nodes and lines, respectively. The effectiveness of the proposed placement procedure has been validated on a realistic test grid of 10 medium voltage nodes and 75 low voltage nodes, whose topology and parameters were made available from the Distribution System Operator (DSO) of the city of Geneva, Switzerland.      
