# ArXiv eess --Wed, 15 Jul 2020
### 1.Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.07230.pdf)
>  Current deep learning based segmentation models often generalize poorly between domains due to insufficient training data. In real-world clinical applications, cross-domain image analysis tools are in high demand since medical images from different domains are often needed to achieve a precise diagnosis. An important example in radiology is generalizing from non-contrast CT to contrast enhanced CTs. Contrast enhanced CT scans at different phases are used to enhance certain pathologies or organs. Many existing cross-domain image-to-image translation models have been shown to improve cross-domain segmentation of large organs. However, such models lack the ability to preserve fine structures during the translation process, which is significant for many clinical applications, such as segmenting small calcified plaques in the aorta and pelvic arteries. In order to preserve fine structures during medical image translation, we propose a patch-based model using shared latent variables from a Gaussian mixture model. We compare our image translation framework to several state-of-the-art methods on cross-domain image translation and show our model does a better job preserving fine structures. The superior performance of our model is verified by performing two tasks with the translated images - detection and segmentation of aortic plaques and pancreas segmentation. We expect the utility of our framework will extend to other problems beyond segmentation due to the improved quality of the generated images and enhanced ability to preserve small structures.      
### 2.Outage Probability Analysis of THz Relaying Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.07186.pdf)
>  This paper focuses on quantifying the outage performance of terahertz (THz) relaying systems. In this direction, novel closed-form expressions for the outage probability of a dual-hop relaying system, in which both the source-relay and relay-destination links suffer from fading and stochastic beam misalignment, are extracted. Our results reveal the importance of taking into account the impact of beam misalignment when characterizing the outage performance of the system as well as when selecting the transmission frequencies.      
### 3.Traffic Simulator for Multibeam Satellite Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.07148.pdf)
>  Assume that a multibeam satellite communication system is designed from scratch to serve a particular area with maximal resource utilization and to satisfactorily accommodate the expected traffic demand. The main design challenge here is setting optimal system parameters such as number of serving beams, beam directions and sizes, and transmit power. This paper aims at developing a tool, multibeam satellite traffic simulator, that helps addressing these fundamental challenges, and more importantly, provides an understanding to the spatial-temporal traffic pattern of satellite networks in large-scale environments. Specifically, traffic demand distribution is investigated by processing credible datasets included three major input categories of information: (i) population distribution for broadband Fixed Satellite Services (FSS), (ii) aeronautical satellite communications, and (iii) vessel distribution for maritime services. This traffic simulator combines this three-dimensional information in addition to time, locations of terminals, and traffic demand. Moreover, realistic satellite beam patterns have been considered in this work, and thus, an algorithm has been proposed to delimit the coverage boundaries of each satellite beam, and then compute the heterogeneous traffic demand at the footprint of each beam. Furthermore, another algorithm has been developed to capture the inherent attributes of satellite channels and the effects of multibeam interference. Data-driven modeling for satellite traffic is crucial nowadays to design innovative communication systems, e.g., precoding and beam hopping, and to devise efficient resource management algorithms.      
### 4.Nonlinear MPC for Tracking for a Class of Non-Convex Admissible Output Sets  [ :arrow_down: ](https://arxiv.org/pdf/2007.07139.pdf)
>  This paper presents an extension to the nonlinear Model Predictive Control for Tracking scheme able to guarantee convergence even in cases of non-convex output admissible sets. This is achieved by incorporating a convexifying homeomorphism in the optimization problem, allowing it to be solved in the convex space. A novel class of non-convex sets is also defined for which a systematic procedure to construct a convexifying homeomorphism is provided. This homeomorphism is then embedded in the Model Predictive Control optimization problem in such a way that the homeomorphism is no longer required in closed form. Finally, the effectiveness of the proposed method is showcased through an illustrative example.      
### 5.Stochastic MPC with Dynamic Feedback Gain Selection and Discounted Probabilistic Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2007.07134.pdf)
>  This paper considers linear discrete-time systems with additive disturbances, and designs a Model Predictive Control (MPC) law incorporating a dynamic feedback gain to minimise a quadratic cost function subject to a single chance constraint. The feedback gain is selected from a set of candidates generated by solutions of multiobjective optimisation problems solved by Dynamic Programming (DP). We provide two methods for gain selection based on minimising upper bounds on predicted costs. The chance constraint is defined as a discounted sum of violation probabilities on an infinite horizon. By penalising violation probabilities close to the initial time and ignoring violation probabilities in the far future, this form of constraint allows for an MPC law with guarantees of recursive feasibility without an assumption of boundedness of the disturbance. A computationally convenient MPC optimisation problem is formulated using Chebyshev's inequality and we introduce an online constraint-tightening technique to ensure recursive feasibility. The closed loop system is guaranteed to satisfy the chance constraint and a quadratic stability condition. With dynamic feedback gain selection, the conservativeness of Chebyshev's inequality is mitigated and closed loop cost is reduced with a larger set of feasible initial conditions. A numerical example is given to show these properties.      
### 6.Accuracy vs. Complexity for mmWave Ray-Tracing: A Full Stack Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2007.07125.pdf)
>  The millimeter wave (mmWave) band will provide multi-gigabits-per-second connectivity in the radio access of future wireless systems. The high propagation loss in this portion of the spectrum calls for the deployment of large antenna arrays to compensate for the loss through high directional gain, thus introducing a spatial dimension in the channel model to accurately represent the performance of a mmWave network. In this perspective, ray-tracing can characterize the channel in terms of Multi Path Components (MPCs) to provide a highly accurate model, at the price of extreme computational complexity (e.g., for processing detailed environment information about the propagation), which limits the scalability of the simulations. In this paper, we present possible simplifications to improve the trade-off between accuracy and complexity in ray-tracing simulations at mmWaves by reducing the total number of MPCs. The effect of such simplifications is evaluated from a full-stack perspective through end-to-end simulations, testing different configuration parameters, propagation scenarios, and higher-layer protocol implementations. We then provide guidelines on the optimal degree of simplification, for which it is possible to reduce the complexity of simulations with a minimal reduction in accuracy for different deployment scenarios.      
### 7.MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2007.07099.pdf)
>  In this paper, we propose a novel convolutional neural network (CNN) architecture, MFRNet, for post-processing (PP) and in-loop filtering (ILF) in the context of video compression. This network consists of four Multi-level Feature review Residual dense Blocks (MFRBs), which are connected using a cascading structure. Each MFRB extracts features from multiple convolutional layers using dense connections and a multi-level residual learning structure. In order to further improve information flow between these blocks, each of them also reuses high dimensional features from the previous MFRB. This network has been integrated into PP and ILF coding modules for both HEVC (HM 16.20) and VVC (VTM 7.0), and fully evaluated under the JVET Common Test Conditions using the Random Access configuration. The experimental results show significant and consistent coding gains over both anchor codecs (HEVC HM and VVC VTM) and also over other existing CNN-based PP/ILF approaches based on Bjontegaard Delta measurements using both PSNR and VMAF for quality assessment. When MFRNet is integrated into HM 16.20, gains up to 16.0% (BD-rate VMAF) are demonstrated for ILF, and up to 21.0% (BD-rate VMAF) for PP. The respective gains for VTM 7.0 are up to 5.1% for ILF and up to 7.1% for PP.      
### 8.Nonlinear Adaptive Cruise Control of Vehicular Platoons  [ :arrow_down: ](https://arxiv.org/pdf/2007.07054.pdf)
>  The paper deals with the design of nonlinear adaptive cruise controllers for vehicular platoons operating on an open road or a ring-road. The constructed feedback controllers are nonlinear functions of the distance between successive vehicles and their speeds. It is shown that the proposed novel controllers guarantee safety (collision avoidance) and bounded vehicle speeds by explicitly characterizing the set of allowable inputs. Moreover, we guarantee global asymptotic stability of the platoon to a desired configuration as well as string stability. Certain macroscopic properties are also investigated. The efficiency of the nonlinear adaptive cruise controllers is demonstrated by means of a numerical example.      
### 9.A Quasi-Doppler Method for Doubling Transmission Efficiency Through Two Orthogonal Directions  [ :arrow_down: ](https://arxiv.org/pdf/2007.07023.pdf)
>  Inspired by the anisotropy of Doppler effect with wave propagations, we propose a new method to leverage one information symbol serving two users located in two geometrically orthogonal directions. Specifically in broadband wireless communication, we use multiple antennas with the proposed signal switching method to emulate a moving emission source and yield the frequency shift, referred to as Quasi Doppler effect, which is converted to the discrete phase modulation. Further, using this discrete modulated phase can adjust the phase of one transmit symbol in achieving two different phases in different directions. The modulation mechanism is explained through theoretical derivations with the analysis on the performance robustness in the application-scenarios of crossroads having small geometric deviations. In contrast to the use of conventional symbols, this approach can double the transmission efficiency which is confirmed by our simulations results.      
### 10.A Weakly Supervised Region-Based Active Learning Method for COVID-19 Segmentation in CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.07012.pdf)
>  One of the key challenges in the battle against the Coronavirus (COVID-19) pandemic is to detect and quantify the severity of the disease in a timely manner. Computed tomographies (CT) of the lungs are effective for assessing the state of the infection. Unfortunately, labeling CT scans can take a lot of time and effort, with up to 150 minutes per scan. We address this challenge introducing a scalable, fast, and accurate active learning system that accelerates the labeling of CT scan images. Conventionally, active learning methods require the labelers to annotate whole images with full supervision, but that can lead to wasted efforts as many of the annotations could be redundant. Thus, our system presents the annotator with unlabeled regions that promise high information content and low annotation cost. Further, the system allows annotators to label regions using point-level supervision, which is much cheaper to acquire than per-pixel annotations. Our experiments on open-source COVID-19 datasets show that using an entropy-based method to rank unlabeled regions yields to significantly better results than random labeling of these regions. Also, we show that labeling small regions of images is more efficient than labeling whole images. Finally, we show that with only 7\% of the labeling effort required to label the whole training set gives us around 90\% of the performance obtained by training the model on the fully annotated training set. Code is available at: \url{<a class="link-external link-https" href="https://github.com/IssamLaradji/covid19_active_learning" rel="external noopener nofollow">this https URL</a>}.      
### 11.Deep Transformer based Data Augmentation with Subword Units for Morphologically Rich Online ASR  [ :arrow_down: ](https://arxiv.org/pdf/2007.06949.pdf)
>  Recently Deep Transformer models have proven to be particularly powerful in language modeling tasks for ASR. Their high complexity, however, makes them very difficult to apply in the first (single) pass of an online system. Recent studies showed that a considerable part of the knowledge of neural network Language Models (LM) can be transferred to traditional n-grams by using neural text generation based data augmentation. In our paper, we pre-train a GPT-2 Transformer LM on a general text corpus and fine-tune it on our Hungarian conversational call center ASR task. We show that although data augmentation with Transformer-generated text works well for isolating languages, it causes a vocabulary explosion in a morphologically rich language. Therefore, we propose a new method called subword-based neural text augmentation, where we retokenize the generated text into statistically derived subwords. We show that this method can significantly reduce the WER while greatly reducing vocabulary size and memory requirements. Finally, we also show that subword-based neural text augmentation outperforms the word-based approach not only in terms of overall WER but also in recognition of OOV words.      
### 12.Global Minimax Approximations and Bounds for the Gaussian Q-Function by Sums of Exponentials  [ :arrow_down: ](https://arxiv.org/pdf/2007.06939.pdf)
>  This paper presents a novel systematic methodology to obtain new simple and tight approximations, lower bounds, and upper bounds for the Gaussian Q-function, and functions thereof, in the form of a weighted sum of exponential functions. They are based on minimizing the maximum absolute or relative error, resulting in globally uniform error functions with equalized extrema. In particular, we construct sets of equations that describe the behaviour of the targeted error functions and solve them numerically in order to find the optimized sets of coefficients for the sum of exponentials. This also allows for establishing a trade-off between absolute and relative error by controlling weights assigned to the error functions' extrema. We further extend the proposed procedure to derive approximations and bounds for any polynomial of the Q-function, which in turn allows approximating and bounding many functions of the Q-function that meet the Taylor series conditions, and consider the integer powers of the Q-function as a special case. In the numerical results, other known approximations of the same and different forms as well as those obtained directly from quadrature rules are compared with the proposed approximations and bounds to demonstrate that they achieve increasingly better accuracy in terms of the global error, thus requiring significantly lower number of sum terms to achieve the same level of accuracy than any reference approach of the same form.      
### 13.Distributed Receivers for Extra-Large Scale MIMO Arrays: A Message Passing Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.06930.pdf)
>  We study the design of receivers in extra-large scale MIMO (XL-MIMO) systems, i.e. systems in which the base station is equipped with an antenna array of extremely large dimensions. While XL-MIMO can significantly increase the system's spectral efficiency, they present two important challenges. One is the increased computational cost of the multi-antenna processing. The second is the variations of user energy distribution over the antenna elements and therefore spatial non-stationarities in these energy distributions. Such non-stationarities limit the performance of the system. In this paper, we propose a distributed receiver for such an XL-MIMO system that can address both challenges. Based on variational message passing (VMP), We propose a set of receiver options providing a range of complexity-performance characteristics to adapt to different requirements. Furthermore, we distribute the processing into local processing units (LPU), that can perform most of the complex processing in parallel, before sharing their outcome with a central processing unit (CPU). Our designs are specifically tailored to exploit the spatial non-stationarities and require lower computations than linear receivers such as zero-forcing. Our simulation study, performed with a channel model accounting for the special characteristics of XL-MIMO channels, confirms the superior performance of our proposals compared to the state of the art methods.      
### 14.Efficient Power-Splitting and Resource Allocation for Cellular V2X Communications  [ :arrow_down: ](https://arxiv.org/pdf/2007.06928.pdf)
>  The research efforts on cellular vehicle-to-everything (V2X) communications are gaining momentum with each passing year. It is considered as a paradigm-altering approach to connect a large number of vehicles with minimal cost of deployment and maintenance. This article aims to further push the state-of-the-art of cellular V2X communications by providing an optimization framework for wireless charging, power allocation, and resource block assignment. Specifically, we design a network model where roadside objects use wireless power from RF signals of electric vehicles for charging and information processing. Moreover, due to the resource-constraint nature of cellular V2X, the power allocation and resource block assignment are performed to efficiently use the resources. The proposed optimization framework shows an improvement in terms of the overall energy efficiency of the network when compared with the baseline technique. The performance gains of the proposed solution clearly demonstrate its feasibility and utility for cellular V2X communications.      
### 15.SRDCNN: Strongly Regularized Deep Convolution Neural Network Architecture for Time-series Sensor Signal Classification Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2007.06909.pdf)
>  Deep Neural Networks (DNN) have been successfully used to perform classification and regression tasks, particularly in computer vision based applications. Recently, owing to the widespread deployment of Internet of Things (IoT), we identify that the classification tasks for time series data, specifically from different sensors are of utmost importance. In this paper, we present SRDCNN: Strongly Regularized Deep Convolution Neural Network (DCNN) based deep architecture to perform time series classification tasks. The novelty of the proposed approach is that the network weights are regularized by both L1 and L2 norm penalties. Both of the regularization approaches jointly address the practical issues of smaller number of training instances, requirement of quicker training process, avoiding overfitting problem by incorporating sparsification of weight vectors as well as through controlling of weight values. We compare the proposed method (SRDCNN) with relevant state-of-the-art algorithms including different DNNs using publicly available time series classification benchmark (the UCR/UEA archive) time series datasets and demonstrate that the proposed method provides superior performance. We feel that SRDCNN warrants better generalization capability to the deep architecture by profoundly controlling the network parameters to combat the training instance insufficiency problem of real-life time series sensor signals.      
### 16.Advanced Stationary Point Concentration Technique for Leakage Mitigation and Small Drone Detection with FMCW Radar  [ :arrow_down: ](https://arxiv.org/pdf/2007.06880.pdf)
>  As the threats of small drones have grown, developing radars to detect the small drones has become an important issue. In earlier studies, we proposed the stationary point concentration (SPC) technique for the small drone detection with frequency-modulated continuous-wave (FMCW) radar. The SPC technique is a new approach to mitigate the leakage that is an inherent problem in the FMCW radar. The SPC technique improves the signal-to-noise ratio of the small drones by reducing the noise floor and provides accurate distance and velocity information of the small drones. However, the SPC technique has shortcomings in realizing it. In this paper, we present the drawbacks of the SPC technique clearly and propose an advanced SPC (A-SPC) technique. The A-SPC technique can overcome the drawbacks of the SPC technique while taking all the good effects of the SPC technique. The experimental results verify the proposed A-SPC technique and show its robustness and usefulness.      
### 17.Multivariate Signal Denoising Based on Generic Multivariate Detrended Fluctuation Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.06862.pdf)
>  We propose a generic multivariate extension of detrended fluctuation analysis (DFA) that incorporates interchannel dependencies within input multichannel data to perform its long-range correlation analysis. We next demonstrate the utility of the proposed method within multivariate signal denoising problem. Particularly, our denosing approach first obtains data driven multiscale signal representation via multivariate variational mode decomposition (MVMD) method. Then, proposed multivariate extension of DFA (MDFA) is used to reject the predominantly noisy modes based on their randomness scores. The denoised signal is reconstructed using the remaining multichannel modes albeit after removal of the noise traces using the principal component analysis (PCA). The utility of our denoising method is demonstrated on a wide range of synthetic and real life signals.      
### 18.Sudo rm -rf: Efficient Networks for Universal Audio Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2007.06833.pdf)
>  In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRMRF) as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that SuDoRMRF performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements.      
### 19.A Forecast Based Load Management Approach For Commercial Buildings -- Comparing LSTM And Standardized Load Profile Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2007.06832.pdf)
>  Load-forecasting problems have already been widely addressed with different approaches, granularities and objectives. Recent studies focus not only on deep learning methods but also on forecasting loads on single building level. This study aims to research problems and possibilities arising by using different load forecasting techniques to manage loads. For that the behaviour of two neural networks, Long Short-Term Memory and Feed Forward Neural Network and two statistical methods, standardized load profiles and personalized standardized load profiles are analysed and assessed by using a sliding-window forecast approach. The results show that machine learning algorithms have the benefit of being able to adapt to new patterns, whereas the personalized standardized load profile performs similar to the tested deep learning algorithms on the metrics. As a case study for evaluating the support of load-forecasting for applications in Energy management systems, the integration of charging stations into an existing building is simulated by using load forecasts to schedule the charging procedures. It shows that such a system can lead to significantly lower load peaks, exceeding a defined grid limit, and to a lower number of overloads compared to uncontrolled charging.      
### 20.On the Effective Capacity of IRS assisted wireless communication  [ :arrow_down: ](https://arxiv.org/pdf/2007.06825.pdf)
>  This paper provides the effective capacity (EC)analysis in futuristic intelligent reflecting surfaces (IRS) assistedwireless communication. We investigate the two widely deployedsetups for mobile wireless communication: single input singleoutput (SISO) and multi-input and single output (MISO) in thecontext of IRS for EC. We derive the distributions of SNR in bothsetups by exploiting probability theory. Further, we encounterthe two widely known assumptions on channel state information(CSI) (i.e. known/perfect CSI and no CSI at the base station(BS)) and derive EC closed form expressions in each cases. Wealso provide solution to the optimal transmission rate problemwhen no CSI available at BS to further enhance EC, in bothSISO and MISO setups. Simulation results show the relation ofEC with different system parameters i.e. power budget, numberof transmit antennas at BS and number of reflective elements at IRS.      
### 21.Securing the Insecure: A First-Line-of-Defense for Nanoscale Communication Systems Operating in THz Band  [ :arrow_down: ](https://arxiv.org/pdf/2007.06818.pdf)
>  Nanoscale communication systems operating in Ter-ahertz (THz) band are anticipated to revolutionise the healthcaresystems of the future. Global wireless data traffic is undergoinga rapid growth. However, wireless systems, due to their broad-casting nature, are vulnerable to malicious security breaches. Inaddition, advances in quantum computing poses a risk to existingcrypto-based information security. It is of the utmost importanceto make the THz systems resilient to potential active and passiveattacks which may lead to devastating consequences, especiallywhen handling sensitive patient data in healthcare systems. Newstrategies are needed to analyse these malicious attacks and topropose viable countermeasures. In this manuscript, we presenta new authentication mechanism for nanoscale communicationsystems operating in THz band at the physical layer. We assessedan impersonation attack on a THz system. We propose usingpath loss as a fingerprint to conduct authentication via two-stephypothesis testing for a transmission device. We used hiddenMarkov Model (HMM) viterbi algorithm to enhance the outputof hypothesis testing. We also conducted transmitter identificationusing maximum likelihood and Gaussian mixture model (GMM)expectation maximization algorithms. Our simulations showedthat the error probabilities are a decreasing functions of SNR. At 10 dB with 0.2 false alarm, the detection probability was almostone. We further observed that HMM out-performs hypothesistesting at low SNR regime (10% increase in accuracy is recordedat SNR =5 dB) whereas the GMM is useful when groundtruths are noisy. Our work addresses major security gaps facedby communication system either through malicious breachesor quantum computing, enabling new applications of nanoscalesystems for Industry 4.0.      
### 22.Uncertainty Aware Deep Neural Network for Multistatic Localization with Application to Ultrasonic Structural Health Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2007.06814.pdf)
>  Guided ultrasonic wave localization uses spatially distributed multistatic sensor arrays and generalized beamforming strategies to detect and locate damage across a structure. The propagation channel is often very complex. Methods can compare data with models of wave propagation to locate damage. Yet, environmental uncertainty (e.g., temperature or stress variations) often degrade accuracies. This paper uses an uncertainty-aware deep neural network framework to learn robust localization models and represent uncertainty. We use mixture density networks to generate damage location distributions based on training data uncertainty. This is in contrast with most localization methods, which output point estimates. We compare our approach with matched field processing (MFP), a generalized beamforming framework. The proposed approach achieves a localization error of 0.0625 m as compared to 0.1425 m with MFP when data has environmental uncertainty and noise. We also show that the predictive uncertainty scales as environmental uncertainty increases to provide a statistically meaningful metric for assessing localization accuracy.      
### 23.Ternary Policy Iteration Algorithm for Nonlinear Robust Control  [ :arrow_down: ](https://arxiv.org/pdf/2007.06810.pdf)
>  The uncertainties in plant dynamics remain a challenge for nonlinear control problems. This paper develops a ternary policy iteration (TPI) algorithm for solving nonlinear robust control problems with bounded uncertainties. The controller and uncertainty of the system are considered as game players, and the robust control problem is formulated as a two-player zero-sum differential game. In order to solve the differential game, the corresponding Hamilton-Jacobi-Isaacs (HJI) equation is then derived. Three loss functions and three update phases are designed to match the identity equation, minimization and maximization of the HJI equation, respectively. These loss functions are defined by the expectation of the approximate Hamiltonian in a generated state set to prevent operating all the states in the entire state set concurrently. The parameters of value function and policies are directly updated by diminishing the designed loss functions using the gradient descent method. Moreover, zero-initialization can be applied to the parameters of the control policy. The effectiveness of the proposed TPI algorithm is demonstrated through two simulation studies. The simulation results show that the TPI algorithm can converge to the optimal solution for the linear plant, and has high resistance to disturbances for the nonlinear plant.      
### 24.Multi-Objective Vehicle Rebalancing for Ridehailing System using a Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.06801.pdf)
>  The problem of designing a rebalancing algorithm for a large-scale ridehailing system with asymmetric demand is considered here. We pose the rebalancing problem within a semi Markov decision problem (SMDP) framework with closed queues of vehicles serving stationary, but asymmetric demand, over a large city with multiple nodes (representing neighborhoods). We assume that the passengers queue up at every node until they are matched with a vehicle. The goal of the SMDP is to minimize a convex combination of the waiting time of the passengers and the total empty vehicle miles traveled. The resulting SMDP appears to be difficult to solve for closed-form expression for the rebalancing strategy. As a result, we use a deep reinforcement learning algorithm to determine the approximately optimal solution to the SMDP. The trained policy is compared with other well-known algorithms for rebalancing, which are designed to address other objectives (such as to minimize demand drop probability) for the ridehailing problem.      
### 25.Inertial Sensing Meets Artificial Intelligence: Opportunity or Challenge?  [ :arrow_down: ](https://arxiv.org/pdf/2007.06727.pdf)
>  The inertial navigation system (INS) has been widely used to provide self-contained and continuous motion estimation in intelligent transportation systems. Recently, the emergence of chip-level inertial sensors has expanded the relevant applications from positioning, navigation, and mobile mapping to location-based services, unmanned systems, and transportation big data. Meanwhile, benefit from the emergence of big data and the improvement of algorithms and computing power, artificial intelligence (AI) has become a consensus tool that has been successfully applied in various fields. This article reviews the research on using AI technology to enhance inertial sensing from various aspects, including sensor design and selection, calibration and error modeling, navigation and motion-sensing algorithms, multi-sensor information fusion, system evaluation, and practical application. Based on the over 30 representative articles selected from the nearly 300 related publications, this article summarizes the state of the art, advantages, and challenges on each aspect. Finally, it summarizes nine advantages and nine challenges of AI-enhanced inertial sensing and then points out future research directions.      
### 26.Channel Parameter Estimation for Millimeter-Wave Cellular Systems with Hybrid Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2007.06714.pdf)
>  To achieve high data rates defined in 5G, the use of millimeter-waves and massive-MIMO are indispensable. To benefit from these technologies, an accurate estimation of the channel parameters is crucial. We propose a novel two-stage algorithm for channel parameters estimation. In the first stage, coarse estimation is accomplished by applying parameter estimation via interpolation based on DFT grid (PREIDG) with a fixed look-up table (LUT), while the second stage refines the estimates by means of the space-alternating generalized expectation maximization (SAGE) algorithm. The two-stage algorithm uses discrete Fourier transform beamforming vectors which are efficiently implemented by a Butler matrix in the analog domain. We found that this methodology improves the estimates compared to the auxiliary beam pair (ABP) method. The two-stage algorithm shows efficient performance in the low signal to noise ratio regime for the channel parameters i.e. angles of departure, complex path gains and delays of the multipaths. Finally, we derived the Cramér-Rao lower bound (CRLB) to assess the performance of our two-stage estimation algorithm.      
### 27.Learning hidden influences in large-scale dynamical social networks: A data-driven sparsity-based approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.06713.pdf)
>  Interpersonal influence estimation from empirical data is a central challenge in the study of social structures and dynamics. Opinion dynamics theory is a young interdisciplinary science that studies opinion formation in social networks and has a huge potential in applications, such as marketing, advertisement and recommendations. <br>The term social influence refers to the behavioral change of individuals due to the interactions with others in a social system, e.g. organization, community, or society in general. <br>The advent of the Internet has made a huge volume of data easily available that can be used to measure social influence over large populations. Here, we aim at qualitatively and quantitatively infer social influence from data using a systems and control viewpoint. First, we introduce some definitions and models of opinions dynamics and review some structural constraints of online social networks, based on the notion of sparsity. Then, we review the main approaches to infer the network's structure from a set of observed data. Finally, we present some algorithms that exploit the introduced models and structural constraints, focusing on the sample complexity and computational requirements.      
### 28.Fleet Sizing and Charger Allocation in Electric Vehicle Sharing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.06687.pdf)
>  In this paper, we propose a closed queueing network model for performance analysis of electric vehicle sharing systems with a certain number of chargers in each neighborhood. Depending on the demand distribution, we devise algorithms to compute the optimal fleet size and number of chargers required to maximize profit while maintaining a certain quality of service. We show that the profit is concave with respect to the fleet size and the number of chargers at each charging point. If more chargers are installed within the city, we show that it can not only reduce the fleet size, but it also improves the availability of vehicles at all the points within a city. We further show through simulation that two slow chargers may outperform one fast charger when the variance of charging time becomes relatively large in comparison to the mean charging time.      
### 29.Landslide Segmentation with U-Net: Evaluating Different Sampling Methods and Patch Sizes  [ :arrow_down: ](https://arxiv.org/pdf/2007.06672.pdf)
>  Landslide inventory maps are crucial to validate predictive landslide models; however, since most mapping methods rely on visual interpretation or expert knowledge, detailed inventory maps are still lacking. This study used a fully convolutional deep learning model named U-net to automatically segment landslides in the city of Nova Friburgo, located in the mountainous range of Rio de Janeiro, southeastern Brazil. The objective was to evaluate the impact of patch sizes, sampling methods, and datasets on the overall accuracy of the models. The training data used the optical information from RapidEye satellite, and a digital elevation model (DEM) derived from the L-band sensor of the ALOS satellite. The data was sampled using random and regular grid methods and patched in three sizes (32x32, 64x64, and 128x128 pixels). The models were evaluated on two areas with precision, recall, f1-score, and mean intersect over union (mIoU) metrics. The results show that the models trained with 32x32 tiles tend to have higher recall values due to higher true positive rates; however, they misclassify more background areas as landslides (false positives). Models trained with 128x128 tiles usually achieve higher precision values because they make less false positive errors. In both test areas, DEM and augmentation increased the accuracy of the models. Random sampling helped in model generalization. Models trained with 128x128 random tiles from the data that used the RapidEye image, DEM information, and augmentation achieved the highest f1-score, 0.55 in test area one, and 0.58 in test area two. The results achieved in this study are comparable to other fully convolutional models found in the literature, increasing the knowledge in the area.      
### 30.Reinforcement Learning of Musculoskeletal Control from Functional Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2007.06669.pdf)
>  To diagnose, plan, and treat musculoskeletal pathologies, understanding and reproducing muscle recruitment for complex movements is essential. With muscle activations for movements often being highly redundant, nonlinear, and time dependent, machine learning can provide a solution for their modeling and control for anatomy-specific musculoskeletal simulations. Sophisticated biomechanical simulations often require specialized computational environments, being numerically complex and slow, hindering their integration with typical deep learning frameworks. In this work, a deep reinforcement learning (DRL) based inverse dynamics controller is trained to control muscle activations of a biomechanical model of the human shoulder. In a generalizable end-to-end fashion, muscle activations are learned given current and desired position-velocity pairs. A customized reward functions for trajectory control is introduced, enabling straightforward extension to additional muscles and higher degrees of freedom. Using the biomechanical model, multiple episodes are simulated on a cluster simultaneously using the evolving neural models of the DRL being trained. Results are presented for a single-axis motion control of shoulder abduction for the task of following randomly generated angular trajectories.      
### 31.Multi-Intersection Traffic Management for Autonomous Vehicles via Distributed Mixed Integer Linear Programming  [ :arrow_down: ](https://arxiv.org/pdf/2007.06639.pdf)
>  This paper extends our previous work in [1],[2], on optimal scheduling of autonomous vehicle arrivals at intersections, from one to a grid of intersections. A scalable distributed Mixed Integer Linear Program (MILP) is devised that solves the scheduling problem for a grid of intersections. A computational control node is allocated to each intersection and regularly receives position and velocity information from subscribed vehicles. Each node assigns an intersection access time to every subscribed vehicle by solving a local MILP. Neighboring intersections will coordinate with each other in real-time by sharing their solutions for vehicles' access times with each other. Our proposed approach is applied to a grid of nine intersections and its positive impact on traffic flow and vehicles' fuel economy is demonstrated in comparison to conventional intersection control scenarios.      
### 32.Inferring the 3D Standing Spine Posture from 2D Radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2007.06612.pdf)
>  The treatment of degenerative spinal disorders requires an understanding of the individual spinal anatomy and curvature in 3D. An upright spinal pose (i.e. standing) under natural weight bearing is crucial for such bio-mechanical analysis. 3D volumetric imaging modalities (e.g. CT and MRI) are performed in patients lying down. On the other hand, radiographs are captured in an upright pose, but result in 2D projections. This work aims to integrate the two realms, i.e. it combines the upright spinal curvature from radiographs with the 3D vertebral shape from CT imaging for synthesizing an upright 3D model of spine, loaded naturally. Specifically, we propose a novel neural network architecture working vertebra-wise, termed \emph{TransVert}, which takes orthogonal 2D radiographs and infers the spine's 3D posture. We validate our architecture on digitally reconstructed radiographs, achieving a 3D reconstruction Dice of $95.52\%$, indicating an almost perfect 2D-to-3D domain translation. Deploying our model on clinical radiographs, we successfully synthesise full-3D, upright, patient-specific spine models for the first time.      
### 33.FocusLiteNN: High Efficiency Focus Quality Assessment for Digital Pathology  [ :arrow_down: ](https://arxiv.org/pdf/2007.06565.pdf)
>  Out-of-focus microscopy lens in digital pathology is a critical bottleneck in high-throughput Whole Slide Image (WSI) scanning platforms, for which pixel-level automated Focus Quality Assessment (FQA) methods are highly desirable to help significantly accelerate the clinical workflows. Existing FQA methods include both knowledge-driven and data-driven approaches. While data-driven approaches such as Convolutional Neural Network (CNN) based methods have shown great promises, they are difficult to use in practice due to their high computational complexity and lack of transferability. Here, we propose a highly efficient CNN-based model that maintains fast computations similar to the knowledge-driven methods without excessive hardware requirements such as GPUs. We create a training dataset using FocusPath which encompasses diverse tissue slides across nine different stain colors, where the stain diversity greatly helps the model to learn diverse color spectrum and tissue structures. In our attempt to reduce the CNN complexity, we find with surprise that even trimming down the CNN to the minimal level, it still achieves a highly competitive performance. We introduce a novel comprehensive evaluation dataset, the largest of its kind, annotated and compiled from TCGA repository for model assessment and comparison, for which the proposed method exhibits superior precision-speed trade-off when compared with existing knowledge-driven and data-driven FQA approaches.      
### 34.Transformer-XL Based Music Generation with Multiple Sequences of Time-valued Notes  [ :arrow_down: ](https://arxiv.org/pdf/2007.07244.pdf)
>  Current state-of-the-art AI based classical music creation algorithms such as Music Transformer are trained by employing single sequence of notes with time-shifts. The major drawback of absolute time interval expression is the difficulty of similarity computing of notes that share the same note value yet different tempos, in one or among MIDI files. In addition, the usage of single sequence restricts the model to separately and effectively learn music information such as harmony and rhythm. In this paper, we propose a framework with two novel methods to respectively track these two shortages, one is the construction of time-valued note sequences that liberate note values from tempos and the other is the separated usage of four sequences, namely, former note on to current note on, note on to note off, pitch, and velocity, for jointly training of four Transformer-XL networks. Through training on a 23-hour piano MIDI dataset, our framework generates significantly better and hour-level longer music than three state-of-the-art baselines, namely Music Transformer, DeepJ, and single sequence-based Transformer-XL, evaluated automatically and manually.      
### 35.Learning Frame Level Attention for Environmental Sound Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.07241.pdf)
>  Environmental sound classification (ESC) is a challenging problem due to the complexity of sounds. The classification performance is heavily dependent on the effectiveness of representative features extracted from the environmental sounds. However, ESC often suffers from the semantically irrelevant frames and silent frames. In order to deal with this, we employ a frame-level attention model to focus on the semantically relevant frames and salient frames. Specifically, we first propose a convolutional recurrent neural network to learn spectro-temporal features and temporal correlations. Then, we extend our convolutional RNN model with a frame-level attention mechanism to learn discriminative feature representations for ESC. We investigated the classification performance when using different attention scaling function and applying different layers. Experiments were conducted on ESC-50 and ESC-10 datasets. Experimental results demonstrated the effectiveness of the proposed method and our method achieved the state-of-the-art or competitive classification accuracy with lower computational complexity. We also visualized our attention results and observed that the proposed attention mechanism was able to lead the network tofocus on the semantically relevant parts of environmental sounds.      
### 36.Collaborative Unsupervised Domain Adaptation for Medical Image Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2007.07222.pdf)
>  Deep learning based medical image diagnosis has shown great potential in clinical medicine. However, it often suffers two major difficulties in real-world applications: 1) only limited labels are available for model training, due to expensive annotation costs over medical images; 2) labeled images may contain considerable label noise (e.g., mislabeling labels) due to diagnostic difficulties of diseases. To address these, we seek to exploit rich labeled data from relevant domains to help the learning in the target task via {Unsupervised Domain Adaptation} (UDA). Unlike most UDA methods that rely on clean labeled data or assume samples are equally transferable, we innovatively propose a Collaborative Unsupervised Domain Adaptation algorithm, which conducts transferability-aware adaptation and conquers label noise in a collaborative way. We theoretically analyze the generalization performance of the proposed method, and also empirically evaluate it on both medical and general images. Promising experimental results demonstrate the superiority and generalization of the proposed method.      
### 37.Joint Device Scheduling and Resource Allocation for Latency Constrained Wireless Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.07174.pdf)
>  In federated learning (FL), devices contribute to the global training by uploading their local model updates via wireless channels. Due to limited computation and communication resources, device scheduling is crucial to the convergence rate of FL. In this paper, we propose a joint device scheduling and resource allocation policy to maximize the model accuracy within a given total training time budget for latency constrained wireless FL. A lower bound on the reciprocal of the training performance loss, in terms of the number of training rounds and the number of scheduled devices per round, is derived. Based on the bound, the accuracy maximization problem is solved by decoupling it into two sub-problems. First, given the scheduled devices, the optimal bandwidth allocation suggests allocating more bandwidth to the devices with worse channel conditions or weaker computation capabilities. Then, a greedy device scheduling algorithm is introduced, which in each step selects the device consuming the least updating time obtained by the optimal bandwidth allocation, until the lower bound begins to increase, meaning that scheduling more devices will degrade the model accuracy. Experiments show that the proposed policy outperforms state-of-the-art scheduling policies under extensive settings of data distributions and cell radius.      
### 38.Towards Dense People Detection with Deep Learning and Depth images  [ :arrow_down: ](https://arxiv.org/pdf/2007.07171.pdf)
>  This paper proposes a DNN-based system that detects multiple people from a single depth image. Our neural network processes a depth image and outputs a likelihood map in image coordinates, where each detection corresponds to a Gaussian-shaped local distribution, centered at the person's head. The likelihood map encodes both the number of detected people and their 2D image positions, and can be used to recover the 3D position of each person using the depth image and the camera calibration parameters. Our architecture is compact, using separated convolutions to increase performance, and runs in real-time with low budget GPUs. We use simulated data for initially training the network, followed by fine tuning with a relatively small amount of real data. We show this strategy to be effective, producing networks that generalize to work with scenes different from those used during training. We thoroughly compare our method against the existing state-of-the-art, including both classical and DNN-based solutions. Our method outperforms existing methods and can accurately detect people in scenes with significant occlusions.      
### 39.A Deep Learning Approach for Low-Latency Packet Loss Concealment of Audio Signals in Networked Music Performance Applications  [ :arrow_down: ](https://arxiv.org/pdf/2007.07132.pdf)
>  Networked Music Performance (NMP) is envisioned as a potential game changer among Internet applications: it aims at revolutionizing the traditional concept of musical interaction by enabling remote musicians to interact and perform together through a telecommunication network. Ensuring realistic conditions for music performance, however, constitutes a significant engineering challenge due to extremely strict requirements in terms of audio quality and, most importantly, network delay. To minimize the end-to-end delay experienced by the musicians, typical implementations of NMP applications use un-compressed, bidirectional audio streams and leverage UDP as transport protocol. Being connection less and unreliable,audio packets transmitted via UDP which become lost in transit are not re-transmitted and thus cause glitches in the receiver audio playout. This article describes a technique for predicting lost packet content in real-time using a deep learning approach. The ability of concealing errors in real time can help mitigate audio impairments caused by packet losses, thus improving the quality of audio playout in real-world scenarios.      
### 40.Energy-Efficient Resource Management for Federated Edge Learning with CPU-GPU Heterogeneous Computing  [ :arrow_down: ](https://arxiv.org/pdf/2007.07122.pdf)
>  Edge machine learning involves the deployment of learning algorithms at the network edge to leverage massive distributed data and computation resources to train artificial intelligence (AI) models. Among others, the framework of federated edge learning (FEEL) is popular for its data-privacy preservation. FEEL coordinates global model training at an edge server and local model training at edge devices that are connected by wireless links. This work contributes to the energy-efficient implementation of FEEL in wireless networks by designing joint computation-and-communication resource management ($\text{C}^2$RM). The design targets the state-of-the-art heterogeneous mobile architecture where parallel computing using both a CPU and a GPU, called heterogeneous computing, can significantly improve both the performance and energy efficiency. To minimize the sum energy consumption of devices, we propose a novel $\text{C}^2$RM framework featuring multi-dimensional control including bandwidth allocation, CPU-GPU workload partitioning and speed scaling at each device, and $\text{C}^2$ time division for each link. The key component of the framework is a set of equilibriums in energy rates with respect to different control variables that are proved to exist among devices or between processing units at each device. The results are applied to designing efficient algorithms for computing the optimal $\text{C}^2$RM policies faster than the standard optimization tools. Based on the equilibriums, we further design energy-efficient schemes for device scheduling and greedy spectrum sharing that scavenges ``spectrum holes" resulting from heterogeneous $\text{C}^2$ time divisions among devices. Using a real dataset, experiments are conducted to demonstrate the effectiveness of $\text{C}^2$RM on improving the energy efficiency of a FEEL system.      
### 41.Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack  [ :arrow_down: ](https://arxiv.org/pdf/2007.07097.pdf)
>  Image denoising techniques have been widely employed in multimedia devices as an image post-processing operation that can remove sensor noise and produce visually clean images for further AI tasks, e.g., image classification. In this paper, we investigate a new task, adversarial denoise attack, that stealthily embeds attacks inside the image denoising module. Thus it can simultaneously denoise input images while fooling the state-of-the-art deep models. We formulate this new task as a kernel prediction problem and propose the adversarial-denoising kernel prediction that can produce adversarial-noiseless kernels for effective denoising and adversarial attacking simultaneously. Furthermore, we implement an adaptive perceptual region localization to identify semantic-related vulnerability regions with which the attack can be more effective while not doing too much harm to the denoising. Thus, our proposed method is termed as Pasadena (Perceptually Aware and Stealthy Adversarial DENoise Attack). We validate our method on the NeurIPS'17 adversarial competition dataset and demonstrate that our method not only realizes denoising but has advantages of high success rate and transferability over the state-of-the-art attacks.      
### 42.Nodule2vec: a 3D Deep Learning System for Pulmonary Nodule Retrieval Using Semantic Representation  [ :arrow_down: ](https://arxiv.org/pdf/2007.07081.pdf)
>  Content-based retrieval supports a radiologist decision making process by presenting the doctor the most similar cases from the database containing both historical diagnosis and further disease development history. We present a deep learning system that transforms a 3D image of a pulmonary nodule from a CT scan into a low-dimensional embedding vector. We demonstrate that such a vector representation preserves semantic information about the nodule and offers a viable approach for content-based image retrieval (CBIR). We discuss the theoretical limitations of the available datasets and overcome them by applying transfer learning of the state-of-the-art lung nodule detection model. We evaluate the system using the LIDC-IDRI dataset of thoracic CT scans. We devise a similarity score and show that it can be utilized to measure similarity 1) between annotations of the same nodule by different radiologists and 2) between the query nodule and the top four CBIR results. A comparison between doctors and algorithm scores suggests that the benefit provided by the system to the radiologist end-user is comparable to obtaining a second radiologist's opinion.      
### 43.Hidden invexity in model predictive control  [ :arrow_down: ](https://arxiv.org/pdf/2007.07062.pdf)
>  Non-convex optimal control problems occurring in,e.g., water or power systems, typically involve a large number of variables related through non-linear equality constraints. The ideal goal is to find a globally optimal solution, and numerical experience indicates that algorithms aiming for Karush-Kuhn-Tucker points often find (near-)optimal solutions. In our paper, we provide a theoretical underpinning for this phenomenon, showing that on a broad class of problems the objective can be shown to be an invex (invariant convex) function of the control decision variables when state variables are eliminated using implicit function theory. In this way (near) global optimality can be demonstrated, where the exact nature of the global optimality guarantee depends on the position of the solution within the feasible set. In a numerical example, we show how high-quality solutions are obtained for a river control problem where invexity holds.      
### 44.A Systematic Identification of Formal and Semi-formalLanguages and Techniques for Software-intensiveSystems-of-Systems Requirements Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.07031.pdf)
>  Software-intensive Systems-of-Systems (SoS) refer to an arrangement of managerially and operationally independent systems(i.e., constituent systems), which work collaboratively towards the achievement of global missions. Because some SoS are developed for critical domains, such as healthcare and transportation, there is an increasing need to attain higher quality levels, which often justifies additional costs that can be incurred by adopting formal and semi-formal approaches (i.e., languages and techniques) for modeling requirements. Various approaches have been employed, but a detailed landscape is still missing, and it is not well known whether they are appropriate for addressing the inherent characteristics of SoS. The main contribution of this article is to present this landscape by reporting on the state of the art in SoS requirements modeling. This landscape was built by means of a systematic mapping and shows formal and semi-formal approaches grouped from model-based to property-oriented ones. Most of them have been tested in safety-critical domains, where formal approaches such as finite state machines are aimed at critical system parts, while semi-formal approaches (e.g., UML and i*) address non-critical parts. Although formal and semi-formal modeling is an essential activity, the quality of SoS requirements does not rely solely on which formalism is used, but also on the availability of supporting tools/mechanisms that enable, for instance, requirements verification along the SoS lifecycle      
### 45.Pose2RGBD. Generating Depth and RGB images from absolute positions  [ :arrow_down: ](https://arxiv.org/pdf/2007.07013.pdf)
>  We propose a method at the intersection of Computer Vision and Computer Graphics fields, which automatically generates RGBD images using neural networks, based on previously seen and synchronized video, depth and pose signals. Since the models must be able to reconstruct both texture (RGB) and structure (Depth), it creates an implicit representation of the scene, as opposed to explicit ones, such as meshes or point clouds. The process can be thought of as neural rendering, where we obtain a function f : Pose -&gt; RGBD, which we can use to navigate through the generated scene, similarly to graphics simulations. We introduce two new datasets, one based on synthetic data with full ground truth information, while the other one being recorded from a drone flight in an university campus, using only video and GPS signals. Finally, we propose a fully unsupervised method of generating datasets from videos alone, in order to train the Pose2RGBD networks. Code and datasets are available at:: <a class="link-external link-https" href="https://gitlab.com/mihaicristianpirvu/pose2rgbd" rel="external noopener nofollow">this https URL</a>.      
### 46.Emergence of scale-free blackout sizes in power grids  [ :arrow_down: ](https://arxiv.org/pdf/2007.06967.pdf)
>  We model power grids as graphs with heavy-tailed sinks, which represent demand from cities, and study cascading failures on such graphs. Our analysis links the scale-free nature of blackout sizes to the scale-free nature of city sizes, contrasting previous studies suggesting that this nature is governed by self-organized criticality. Our results are based on a new mathematical framework combining the physics of power flow with rare event analysis for heavy-tailed distributions, and are validated using various synthetic networks and the German transmission grid.      
### 47.Learning Semantics-enriched Representation via Self-discovery, Self-classification, and Self-restoration  [ :arrow_down: ](https://arxiv.org/pdf/2007.06959.pdf)
>  Medical images are naturally associated with rich semantics about the human anatomy, reflected in an abundance of recurring anatomical patterns, offering unique potential to foster deep semantic representation learning and yield semantically more powerful models for different medical applications. But how exactly such strong yet free semantics embedded in medical images can be harnessed for self-supervised learning remains largely unexplored. To this end, we train deep models to learn semantically enriched visual representation by self-discovery, self-classification, and self-restoration of the anatomy underneath medical images, resulting in a semantics-enriched, general-purpose, pre-trained 3D model, named Semantic Genesis. We examine our Semantic Genesis with all the publicly-available pre-trained models, by either self-supervision or fully supervision, on the six distinct target tasks, covering both classification and segmentation in various medical modalities (i.e.,CT, MRI, and X-ray). Our extensive experiments demonstrate that Semantic Genesis significantly exceeds all of its 3D counterparts as well as the de facto ImageNet-based transfer learning in 2D. This performance is attributed to our novel self-supervised learning framework, encouraging deep models to learn compelling semantic representation from abundant anatomical patterns resulting from consistent anatomies embedded in medical images. Code and pre-trained Semantic Genesis are available at <a class="link-external link-https" href="https://github.com/JLiangLab/SemanticGenesis" rel="external noopener nofollow">this https URL</a> .      
### 48.Optimized tour planning for drone-based urban traffic monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2007.06911.pdf)
>  Drones or Unmanned Aerial Vehicles (UAVs) have become a reliable and efficient tool for road traffic monitoring. Compared to loop detectors and bluetooth receivers (with high capital and operational expenditure), drones are a low-cost alternative that offers great flexibility and high quality data. In this work, we derive optimized tour plans that a fleet of drones can follow for rapid traffic monitoring across particular regions of transportation network. To derive these tours, we first identify monitoring locations over which drones should fly through and then compute minimum travel-time tours based on realistic resource constraints. Evaluation results are presented over a real road network topology to demonstrate the applicability of the proposed approach.      
### 49.Enabling Adaptive and Enhanced Acoustic Sensing Using Nonlinear Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2007.06902.pdf)
>  Transmission of real-time data is strongly increasing due to remote processing of sensor data, among other things. A route to meet this demand is adaptive sensing, in which sensors acquire only relevant information using pre-processing at sensor level. We present here adaptive acoustic sensors based on mechanical oscillators with integrated sensing and actuation. Their dynamics are shifted into a nonlinear regime using feedback or coupling. This enhances dynamic range, frequency resolution and signal-to-noise ratio. Combining tunable sensing properties with sound analysis could enable acquiring of only relevant information rather than extracting this from irrelevant data by post-processing.      
### 50.Joint Beamforming Design for IRS-Aided Communications with Channel Estimation Errors  [ :arrow_down: ](https://arxiv.org/pdf/2007.06859.pdf)
>  This paper investigates the joint design of the beamforming scheme in intelligent reflecting surface (IRS) assisted multiuser (MU) multiple-input multiple-output (MIMO) downlink transmissions. Channel estimation errors associated with the minimum mean square error (MMSE) estimation are assumed and the weighted sum rate (WSR) is adopted as the performance metric. Low-resolution phase shifters (PSs) in practical implementations are taken into account as well. Under the constraint of the transmit power and discrete phase shifters (PSs), an optimization problem is formulated to maximize the WSR of all users. To obtain the optimal beamforming matrices at the IRS, two solutions based on the majorization-minimization (MM) and successive convex approximation (SCA) methods, respectively, are proposed. Through simulation results, both of the proposed two schemes achieve a significant improvement in WSR. Furthermore, the superiority of the SCA-based solution is demonstrated. Overall, two viable solutions to the joint beamforming design in IRS-aided MU-MIMO downlink communication systems with channel estimation errors are provided.      
### 51.Meta-rPPG: Remote Heart Rate Estimation Using a Transductive Meta-Learner  [ :arrow_down: ](https://arxiv.org/pdf/2007.06786.pdf)
>  Remote heart rate estimation is the measurement of heart rate without any physical contact with the subject and is accomplished using remote photoplethysmography (rPPG) in this work. rPPG signals are usually collected using a video camera with a limitation of being sensitive to multiple contributing factors, e.g. variation in skin tone, lighting condition and facial structure. End-to-end supervised learning approach performs well when training data is abundant, covering a distribution that doesn't deviate too much from the distribution of testing data or during deployment. To cope with the unforeseeable distributional changes during deployment, we propose a transductive meta-learner that takes unlabeled samples during testing (deployment) for a self-supervised weight adjustment (also known as transductive inference), providing fast adaptation to the distributional changes. Using this approach, we achieve state-of-the-art performance on MAHNOB-HCI and UBFC-rPPG.      
### 52.Orthogonal Sparse Superposition Codes  [ :arrow_down: ](https://arxiv.org/pdf/2007.06739.pdf)
>  This paper presents a new class of sparse superposition codes for efficient short-packet and low-rate communication over the AWGN channel. The new codes are orthogonal sparse superposition codes, in which a codeword is constructed by a superposition of orthogonal columns of a dictionary matrix. We propose a successive encoding technique to construct such codewords. In addition, we introduce a near-optimal decoding, named an element-wise maximum a posterior decoding with successive support set cancellation, which has a linear decoding complexity in block lengths. Via simulations, we demonstrate that the proposed encoding and decoding techniques are less complex and better performing than existing coded modulation techniques for reliable short packet communications.      
### 53.DETCID: Detection of Elongated Touching Cells with Inhomogeneous Illumination using a Deep Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.06716.pdf)
>  Clostridioides difficile infection (C. diff) is the most common cause of death due to secondary infection in hospital patients in the United States. Detection of C. diff cells in scanning electron microscopy (SEM) images is an important task to quantify the efficacy of the under-development treatments. However, detecting C. diff cells in SEM images is a challenging problem due to the presence of inhomogeneous illumination and occlusion. An Illumination normalization pre-processing step destroys the texture and adds noise to the image. Furthermore, cells are often clustered together resulting in touching cells and occlusion. In this paper, DETCID, a deep cell detection method using adversarial training, specifically robust to inhomogeneous illumination and occlusion, is proposed. An adversarial network is developed to provide region proposals and pass the proposals to a feature extraction network. Furthermore, a modified IoU metric is developed to allow the detection of touching cells in various orientations. The results indicate that DETCID outperforms the state-of-the-art in detection of touching cells in SEM images by at least 20 percent improvement of mean average precision.      
### 54.Measuring Performance of Generative Adversarial Networks on Devanagari Script  [ :arrow_down: ](https://arxiv.org/pdf/2007.06710.pdf)
>  The working of neural networks following the adversarial philosophy to create a generative model is a fascinating field. Multiple papers have already explored the architectural aspect and proposed systems with potentially good results however, very few papers are available which implement it on a real-world example. Traditionally, people use the famous MNIST dataset as a Hello, World! example for implementing Generative Adversarial Networks (GAN). Instead of going the standard route of using handwritten digits, this paper uses the Devanagari script which has a more complex structure. As there is no conventional way of judging how well the generative models perform, three additional classifiers were built to judge the output of the GAN model. The following paper is an explanation of what this implementation has achieved.      
### 55.Deep Image Orientation Angle Detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.06709.pdf)
>  Estimating and rectifying the orientation angle of any image is a pretty challenging task. Initial work used the hand engineering features for this purpose, where after the invention of deep learning using convolution-based neural network showed significant improvement in this problem. However, this paper shows that the combination of CNN and a custom loss function specially designed for angles lead to a state-of-the-art results. This includes the estimation of the orientation angle of any image or document at any degree (0 to 360 degree),      
### 56.Momentum-Based Policy Gradient Methods  [ :arrow_down: ](https://arxiv.org/pdf/2007.06680.pdf)
>  In the paper, we propose a class of efficient momentum-based policy gradient methods for the model-free reinforcement learning, which use adaptive learning rates and do not require any large batches. Specifically, we propose a fast important-sampling momentum-based policy gradient (IS-MBPG) method based on a new momentum-based variance reduced technique and the importance sampling technique. We also propose a fast Hessian-aided momentum-based policy gradient (HA-MBPG) method based on the momentum-based variance reduced technique and the Hessian-aided technique. Moreover, we prove that both the IS-MBPG and HA-MBPG methods reach the best known sample complexity of $O(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of the nonconcave performance function, which only require one trajectory at each iteration. In particular, we present a non-adaptive version of IS-MBPG method, i.e., IS-MBPG*, which also reaches the best known sample complexity of $O(\epsilon^{-3})$ without any large batches. In the experiments, we apply four benchmark tasks to demonstrate the effectiveness of our algorithms.      
