# ArXiv eess --Mon, 6 Jul 2020
### 1.Pretrained Semantic Speech Embeddings for End-to-End Spoken Language Understanding via Cross-Modal Teacher-Student Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.01836.pdf)
>  Spoken language understanding is typically based on pipeline architectures including speech recognition and natural language understanding steps. Therefore, these components are optimized independently from each other and the overall system suffers from error propagation. In this paper, we propose a novel training method that enables pretrained contextual embeddings such as BERT to process acoustic features. In particular, we extend it with an encoder of pretrained speech recognition systems in order to construct end-to-end spoken language understanding systems. Our proposed method is based on the teacher-student framework across speech and text modalities that aligns the acoustic and the semantic latent spaces. Experimental results in three benchmark datasets show that our system reaches the pipeline architecture performance without using any training data and outperforms it after fine-tuning with only a few examples.      
### 2.Fault Diagnosis of the 10MW Floating Offshore Wind Turbine Benchmark: a Mixed Model and Signal-based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.01708.pdf)
>  Floating Offshore Wind Turbines (FOWTs) operate in the harsh marine environment with limited accessibility and maintainability. Not only failures are more likely to occur than in land-based turbines, but also corrective maintenance is more expensive. In the present study, a mixed model and signal-based Fault Diagnosis (FD) architecture is developed to detect and isolate critical faults in FOWTs. More specifically, a model-based scheme is developed to detect and isolate the faults associated with the turbine system. It is based on a fault detection and approximation estimator and fault isolation estimators, with time-varying adaptive thresholds to guarantee against false-alarms. In addition, a signal-based scheme is established, within the proposed architecture, for detecting and isolating two representative mooring lines faults. For the purpose of verification, a 10MW FOWT benchmark is developed and its operating conditions, which contains predefined faults, are simulated by extending the high-fidelity simulator. Based on it, the effectiveness of the proposed architecture is illustrated. In addition, the advantages and limitations are discussed by comparing its fault detection to the results delivered by other approaches. Results show that the proposed architecture has the best performance in detecting and isolating the critical faults in FOWTs under diverse operating conditions.      
### 3.Fast Computation of Electromagnetic Wave Propagation and Scattering for Quasi-cylindrical Geometry  [ :arrow_down: ](https://arxiv.org/pdf/2007.01702.pdf)
>  The cylindrical Taylor Interpolation through FFT (TI-FFT) algorithm for computation of the near-field and far-field in the quasi-cylindrical geometry has been introduced. The modal expansion coefficient of the vector potentials ${\bf F}$ and ${\bf A}$ within the context of the cylindrical harmonics (TE and TM modes) can be expressed in the closed-form expression through the cylindrical addition theorem. For the quasi-cylindrical geometry, the modal expansion coefficient can be evaluated through FFT with the help of the Taylor Interpolation (TI) technique. The near-field on any arbitrary cylindrical surface can be obtained through the Inverse Fourier Transform (IFT). The far-field can be obtained through the Near-Field Far-Field (NF-FF) transform. The cylindrical TI-FFT algorithm has the advantages of $\mathcal{O} \left( \hbox{N} \log_2 \hbox{N} \right)$ computational complexity for $\hbox{N} = \hbox{N}_\phi \times \hbox{N}_z$ computational grid, small sampling rate (large sampling spacing) and no singularity problem.      
### 4.Safe Reinforcement Learning with Mixture Density Network: A Case Study in Autonomous Highway Driving  [ :arrow_down: ](https://arxiv.org/pdf/2007.01698.pdf)
>  This paper presents a safe reinforcement learning system for automated driving that benefits from multimodal future trajectory predictions. We propose a safety system that consists of two safety components: a heuristic safety and a learning-based safety. The heuristic safety module is based on common driving rules. On the other hand, the learning-based safety module is a data-driven safety rule that learns safety patterns from driving data. Specifically, it utilizes mixture density recurrent neural networks (MD-RNN) for multimodal future trajectory predictions to accelerate the learning progress. Our simulation results demonstrate that the proposed safety system outperforms previously reported results in terms of average reward and number of collisions.      
### 5.Stochastic Variational Bayesian Inference for a Nonlinear Forward Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.01675.pdf)
>  Variational Bayes (VB) has been used to facilitate the calculation of the posterior distribution in the context of Bayesian inference of the parameters of nonlinear models from data. Previously an analytical formulation of VB has been derived for nonlinear model inference on data with additive gaussian noise as an alternative to nonlinear least squares. Here a stochastic solution is derived that avoids some of the approximations required of the analytical formulation, offering a solution that can be more flexibly deployed for nonlinear model inference problems. The stochastic VB solution was used for inference on a biexponential toy case and the algorithmic parameter space explored, before being deployed on real data from a magnetic resonance imaging study of perfusion. The new method was found to achieve comparable parameter recovery to the analytic solution and be competitive in terms of computational speed despite being reliant on sampling.      
### 6.Stabilizing of a Class of Underactuated Euler Lagrange System Using an Approximate Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.01649.pdf)
>  The energy shaping method, Controlled Lagrangian, is a well-known approach to stabilize the under-actuated Euler Lagrange (EL) systems. In this approach, to construct a control rule, some nonlinear, nonhomogeneous partial differential equations (PDEs), which are called matching conditions, must be solved. In this paper, a method is proposed to obtain an approximate solution of these matching conditions for a class of under-actuated EL systems. To develop the method, the potential energy matching condition is transformed to a set of linear PDEs using an approximation of inertia matrices. So the assignable potential energy function and the controlled inertia matrix, both are constructed as a common solution of these PDEs. Afterwards, the gyroscopic and dissipative forces are found as the solution of the kinetic energy matching condition. Finally, the control rule is constructed by adding energy shaping rule and additional dissipation injection to provide asymptotic stability. The stability analysis of the closed loop system which used the control rule derived with the proposed method is also given. To demonstrate the success of the proposed method, the stability problem of the inverted pendulum on a cart is considered.      
### 7.Noise2Filter: fast, self-supervised learning and real-time reconstruction for 3D Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2007.01636.pdf)
>  At X-ray beamlines of synchrotron light sources, the achievable time-resolution for 3D tomographic imaging of the interior of an object has been reduced to a fraction of a second, enabling rapidly changing structures to be examined. The associated data acquisition rates require sizable computational resources for reconstruction. Therefore, full 3D reconstruction of the object is usually performed after the scan has completed. Quasi-3D reconstruction -- where several interactive 2D slices are computed instead of a 3D volume -- has been shown to be significantly more efficient, and can enable the real-time reconstruction and visualization of the interior. However, quasi-3D reconstruction relies on filtered backprojection type algorithms, which are typically sensitive to measurement noise. To overcome this issue, we propose Noise2Filter, a learned filter method that can be trained using only the measured data, and does not require any additional training data. This method combines quasi-3D reconstruction, learned filters, and self-supervised learning to derive a tomographic reconstruction method that can be trained in under a minute and evaluated in real-time. We show limited loss of accuracy compared to training with additional training data, and improved accuracy compared to standard filter-based methods.      
### 8.HDR-GAN: HDR Image Reconstruction from Multi-Exposed LDR Images with Large Motions  [ :arrow_down: ](https://arxiv.org/pdf/2007.01628.pdf)
>  Synthesizing high dynamic range (HDR) images from multiple low-dynamic range (LDR) exposures in dynamic scenes is challenging. There are two major problems caused by the large motions of foreground objects. One is the severe misalignment among the LDR images. The other is the missing content due to the over-/under-saturated regions caused by the moving objects, which may not be easily compensated for by the multiple LDR exposures. Thus, it requires the HDR generation model to be able to properly fuse the LDR images and restore the missing details without introducing artifacts. To address these two problems, we propose in this paper a novel GAN-based model, HDR-GAN, for synthesizing HDR images from multi-exposed LDR images. To our best knowledge, this work is the first GAN-based approach for fusing multi-exposed LDR images for HDR reconstruction. By incorporating adversarial learning, our method is able to produce faithful information in the regions with missing content. In addition, we also propose a novel generator network, with a reference-based residual merging block for aligning large object motions in the feature domain, and a deep HDR supervision scheme for eliminating artifacts of the reconstructed HDR images. Experimental results demonstrate that our model achieves state-of-the-art reconstruction performance over the prior HDR methods on diverse scenes.      
### 9.Deep image prior for 3D magnetic particle imaging: A quantitative comparison of regularization techniques on Open MPI dataset  [ :arrow_down: ](https://arxiv.org/pdf/2007.01593.pdf)
>  Magnetic particle imaging (MPI) is an imaging modality exploiting the nonlinear magnetization behavior of (super-)paramagnetic nanoparticles to obtain a space- and often also time-dependent concentration of a tracer consisting of these nanoparticles. MPI has a continuously increasing number of potential medical applications. One prerequisite for successful performance in these applications is a proper solution to the image reconstruction problem. More classical methods from inverse problems theory, as well as novel approaches from the field of machine learning, have the potential to deliver high-quality reconstructions in MPI. We investigate a novel reconstruction approach based on a deep image prior, which builds on representing the solution by a deep neural network. Novel approaches, as well as variational and iterative regularization techniques, are compared quantitatively in terms of peak signal-to-noise ratios and structural similarity indices on the publicly available Open MPI dataset.      
### 10.Noise-Robust Adaptation Control for Supervised System Identification Exploiting A Noise Dictionary  [ :arrow_down: ](https://arxiv.org/pdf/2007.01579.pdf)
>  We present a noise-robust adaptation control strategy for block-online supervised acoustic system identification by exploiting a noise dictionary. The proposed algorithm takes advantage of the pronounced spectral structure which characterizes many types of interfering noise signals. We model the noisy observations by a linear Gaussian Discrete Fourier Transform-domain state space model whose parameters are estimated by an online generalized Expectation-Maximization algorithm. Unlike all other state-of-the-art approaches we suggest to model the covariance matrix of the observation probability density function by a dictionary model. We propose to learn the noise dictionary from training data, which can be gathered either offline or online whenever the system is not excited, while we infer the activations continuously. The proposed algorithm represents a novel machine-learning-based approach to noise-robust adaptation control for challenging online supervised acoustic system identification applications characterized by high-level and non-stationary interfering noise signals.      
### 11.Distortionless Multi-Channel Target Speech Enhancement for Overlapped Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.01566.pdf)
>  Speech enhancement techniques based on deep learning have brought significant improvement on speech quality and intelligibility. Nevertheless, a large gain in speech quality measured by objective metrics, such as perceptual evaluation of speech quality (PESQ), does not necessarily lead to improved speech recognition performance due to speech distortion in the enhancement stage. In this paper, a multi-channel dilated convolutional network based frequency domain modeling is presented to enhance target speaker in the far-field, noisy and multi-talker conditions. We study three approaches towards distortionless waveforms for overlapped speech recognition: estimating complex ideal ratio mask with an infinite range, incorporating the fbank loss in a multi-objective learning and finetuning the enhancement model by an acoustic model. Experimental results proved the effectiveness of all three approaches on reducing speech distortions and improving recognition accuracy. Particularly, the jointly tuned enhancement model works very well with other standalone acoustic model on real test data.      
### 12.Online Supervised Acoustic System Identification exploiting Prelearned Local Affine Subspace Models  [ :arrow_down: ](https://arxiv.org/pdf/2007.01543.pdf)
>  In this paper we present a novel algorithm for improved block-online supervised acoustic system identification in adverse noise scenarios by exploiting prior knowledge about the space of Room Impulse Responses (RIRs). The method is based on the assumption that the variability of the unknown RIRs is controlled by only few physical parameters, describing, e.g., source position movements, and thus is confined to a low-dimensional manifold which is modelled by a union of affine subspaces. The offsets and bases of the affine subspaces are learned in advance from training data by unsupervised clustering followed by Principal Component Analysis. We suggest to denoise the parameter update of any supervised adaptive filter by projecting it onto an optimal affine subspace which is selected based on a novel computationally efficient approximation of the associated evidence. The proposed method significantly improves the system identification performance of state-of-the-art algorithms in adverse noise scenarios.      
### 13.Symbiotic Radio: Cognitive Backscattering Communications for Future Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.01506.pdf)
>  The heterogenous wireless services and exponentially growing traffic call for novel spectrum- and energy-efficient wireless communication technologies. In this paper, a new technique, called symbiotic radio (SR), is proposed to exploit the benefits and address the drawbacks of cognitive radio (CR) and ambient backscattering communications(AmBC), leading to mutualism spectrum sharing and highly reliable backscattering communications. In particular, the secondary transmitter (STx) in SR transmits messages to the secondary receiver (SRx) over the RF signals originating from the primary transmitter (PTx) based on cognitive backscattering communications, thus the secondary system shares not only the radio spectrum, but also the power, and infrastructure with the primary system. In return, the secondary transmission provides beneficial multipath diversity to the primary system, therefore the two systems form mutualism spectrum sharing. More importantly, joint decoding is exploited at SRx to achieve highly reliable backscattering communications. To exploit the full potential of SR, in this paper, we address three fundamental tasks in SR: (1) enhancing the backscattering link via active load; (2) achieving highly reliable communications through joint decoding; and (3) capturing PTx's RF signals using reconfigurable intelligent surfaces. Emerging applications, design challenges and open research problems will also be discussed.      
### 14.Intelligent Reflecting Surface Aided MISO Uplink Communication Network: Feasibility and SINR Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.01482.pdf)
>  In this paper we consider the signal-to-interference-and-noise ratio (SINR) optimization problem in the multi-user multi-input-single-output (MISO) uplink wireless network assisted by intelligent reflecting surface (IRS) under individual information rate constraints. We perform a comprehensive investigation on various aspects of this problem. First, under the individual rate constraints, we study its feasibility. We present a sufficient condition which guarantees arbitrary set of individual information rates. This result strengthens the feasibility condition in existing literature and is useful to the power control/energy efficiency (EE) maximization problem when IRS is present. Then, based on the penalty dual decomposition (PDD) and nonlinear equality alternative direction method of multipliers (neADMM) method, we present new algorithms to tackle the IRS configuration problems, which simultaneously involves multi-dimensional constant-modulus constraints and other additional constraints. Note that the similar hard-core problem has recurrently appeared in various research work on IRS recently. Convergence property and analytic solutions of our proposed algorithms are carefully examined. Moreover, iterative algorithms are developed to detect the feasibility and maximize the SINR. Extensive numerical results are presented to verify the effectiveness of our proposed algorithms.      
### 15.Rigorous Quantum Formulation of Parity-Time Symmetric Coupled Resonators  [ :arrow_down: ](https://arxiv.org/pdf/2007.01462.pdf)
>  Rigorous quantum formulation of the Parity-Time (PT) symmetry phenomenon in the RF/microwave regime for a coupled coil resonators with lump elements has been presented. The coil resonator is described by the lump-element model that consists of an inductor (L), a resistor (R) and a capacitor (C). Rigorous quantum Hamiltonian for the coupled LRC coil resonators system has been derived through twice basis transforms of the original basis. The first basis transform rotates the original basis such that off-diagonal terms of the governing matrix of the equation system of the coupled coil resonators reduces to constants. Then a second basis transform obtains the quantum Hamiltonian, including the diagonal effective complex frequencies and the off-diagonal coupling terms, together with the transformed basis. With the obtain quantum Hamiltonian, the eigenvalues and eigenvectors of the coupled coil resonators can be obtained as usual as the quantum Hamiltonian. Finally, numerical simulation verifies the correctness of the theory. The quantum formulation of the coupled coil resonators can provide better guideline to design a better PT-symmetric system.      
### 16.Improved RIP-Based Bounds for Guaranteed Performance of Several Compressed Sensing Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2007.01451.pdf)
>  Iterative hard thresholding (IHT), compressive sampling matching pursuit (CoSaMP), and subspace pursuit (SP) are three types of mainstream compressed sensing algorithms using hard thresholding operators for signal recovery and approximation. The guaranteed performance for signal recovery via these algorithms has mainly been analyzed under the condition that the restricted isometry constant of a sensing matrix, denoted by $ \delta_K$ (where $K$ is an integer number), is smaller than a certain threshold value in the interval $(0,1).$ The condition $ \delta_{K}&lt; \delta^*$ for some number $ \delta^* \leq 1 $ ensuring the success of signal recovery with a specific compressed sensing algorithm is called the restricted-isometry-property-based (RIP-based) bound for guaranteed performance of the algorithm. At the moment the best known RIP-based bound for the guaranteed recovery of $k$-sparse signals via IHT is $\delta_{3k}&lt; 1/\sqrt{3}\approx 0.5773,$ the bound for guaranteed recovery via CoSaMP is $\delta_{4k} &lt; 0.4782, $ and the bound via SP is $ \delta_{3k} &lt;0.4859.$ A fundamental question in this area is whether such theoretical results can be further improved. The purpose of this paper is to affirmatively answer this question and rigorously prove that the RIP-based bounds for guaranteed performance of IHT can be significantly improved to $ \delta_{3k} &lt; (\sqrt{5}-1)/2 \approx 0.618, $ the bound for CoSaMP can be improved and pushed to $ \delta_{4k}&lt; 0.5593, $ and the bound for SP can be improved to $ \delta_{3k} &lt; 0.5108.$ These improvements are far from being trivial and are achieved through establishing some deep properties of the hard thresholding operator and certain tight error estimations by efficiently exploiting the structure of the underlying algorithms.      
### 17.Semi-Supervised Training of Optical Flow Convolutional Neural Networks in Ultrasound Elastography  [ :arrow_down: ](https://arxiv.org/pdf/2007.01421.pdf)
>  Convolutional Neural Networks (CNN) have been found to have great potential in optical flow problems thanks to an abundance of data available for training a deep network. The displacement estimation step in UltraSound Elastography (USE) can be viewed as an optical flow problem. Despite the high performance of CNNs in optical flow, they have been rarely used for USE due to unique challenges that both input and output of USE networks impose. Ultrasound data has much higher high-frequency content compared to natural images. The outputs are also drastically different, where displacement values in USE are often smooth without sharp motions or discontinuities. The general trend is currently to use pre-trained networks and fine-tune them on a small simulation ultrasound database. However, realistic ultrasound simulation is computationally expensive. Also, the simulation techniques do not model complex motions, nonlinear and frequency-dependent acoustics, and many sources of artifact in ultrasound imaging. Herein, we propose an unsupervised fine-tuning technique which enables us to employ a large unlabeled dataset for fine-tuning of a CNN optical flow network. We show that the proposed unsupervised fine-tuning method substantially improves the performance of the network and reduces the artifacts generated by networks trained on computer vision databases.      
### 18.Wearable Respiration Monitoring: Interpretable Inference with Context and Sensor Biomarkers  [ :arrow_down: ](https://arxiv.org/pdf/2007.01413.pdf)
>  Breathing rate (BR), minute ventilation (VE), and other respiratory parameters are essential for real-time patient monitoring in many acute health conditions, such as asthma. The clinical standard for measuring respiration, namely Spirometry, is hardly suitable for continuous use. Wearables can track many physiological signals, like ECG and motion, yet not respiration. Deriving respiration from other modalities has become an area of active research. In this work, we infer respiratory parameters from wearable ECG and wrist motion signals. We propose a modular and generalizable classification-regression pipeline to utilize available context information, such as physical activity, in learning context-conditioned inference models. Morphological and power domain novel features from the wearable ECG are extracted to use with these models. Exploratory feature selection methods are incorporated in this pipeline to discover application-specific interpretable biomarkers. Using data from 15 subjects, we evaluate two implementations of the proposed pipeline: for inferring BR and VE. Each implementation compares generalized linear model, random forest, support vector machine, Gaussian process regression, and neighborhood component analysis as contextual regression models. Permutation, regularization, and relevance determination methods are used to rank the ECG features to identify robust ECG biomarkers across models and activities. This work demonstrates the potential of wearable sensors not only in continuous monitoring, but also in designing biomarker-driven preventive measures.      
### 19.An Algebraic Approach for the Stability Analysis of BLDC Motor Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2007.01387.pdf)
>  This paper presents an algebraic technique to compute the maximum time-delay that can be accepted in the control loop of a Brushless DC Motor (BLDCM) speed controller before the closed loop response becomes unstable. Using a recently proposed time-delay stability analysis methodology, we derive accurate stability conditions for the BLDCM speed controller. The results of applying the new method show that tuning the PI controller for very fast response in the order of magnitude of the BLDCM mechanical time constant cause the time-delay to significantly affect the system stability.      
### 20.Deep Interactive Learning: An Efficient Labeling Approach for Deep Learning-Based Osteosarcoma Treatment Response Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2007.01383.pdf)
>  Osteosarcoma is the most common malignant primary bone tumor. Standard treatment includes pre-operative chemotherapy followed by surgical resection. The response to treatment as measured by ratio of necrotic tumor area to overall tumor area is a known prognostic factor for overall survival. This assessment is currently done manually by pathologists by looking at glass slides under the microscope which may not be reproducible due to its subjective nature. Convolutional neural networks (CNNs) can be used for automated segmentation of viable and necrotic tumor on osteosarcoma whole slide images. One bottleneck for supervised learning is that large amounts of accurate annotations are required for training which is a time-consuming and expensive process. In this paper, we describe Deep Interactive Learning (DIaL) as an efficient labeling approach for training CNNs. After an initial labeling step is done, annotators only need to correct mislabeled regions from previous segmentation predictions to improve the CNN model until the satisfactory predictions are achieved. Our experiments show that our CNN model trained by only 7 hours of annotation using DIaL can successfully estimate ratios of necrosis within expected inter-observer variation rate for non-standardized manual surgical pathology task.      
### 21.Regularization of the movement of a material point along a flat trajectory: application to robotics problems  [ :arrow_down: ](https://arxiv.org/pdf/2007.01821.pdf)
>  The control problem of the working tool movement along a predefined trajectory is considered. The integral of kinetic energy and weighted inertia forces for the whole period of motion is considered as a cost functional. The trajectory is assumed to be planar and defined in advance. The problem is reduced to a system of ordinary differential equations of the fourth order. Numerical examples of solving the problem for movement along straight, circular and elliptical trajectories are given.      
### 22.CacheNet: A Model Caching Framework for Deep Learning Inference on the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2007.01793.pdf)
>  The success of deep neural networks (DNN) in machine perception applications such as image classification and speech recognition comes at the cost of high computation and storage complexity. Inference of uncompressed large scale DNN models can only run in the cloud with extra communication latency back and forth between cloud and end devices, while compressed DNN models achieve real-time inference on end devices at the price of lower predictive accuracy. In order to have the best of both worlds (latency and accuracy), we propose CacheNet, a model caching framework. CacheNet caches low-complexity models on end devices and high-complexity (or full) models on edge or cloud servers. By exploiting temporal locality in streaming data, high cache hit and consequently shorter latency can be achieved with no or only marginal decrease in prediction accuracy. Experiments on CIFAR-10 and FVG have shown CacheNet is 58-217% faster than baseline approaches that run inference tasks on end devices or edge servers alone.      
### 23.Supervisory Controller Synthesis for Non-terminating Processes is an Obliging Game  [ :arrow_down: ](https://arxiv.org/pdf/2007.01773.pdf)
>  We present a new algorithm to solve the supervisory control problem over non-terminating processes modeled as $\omega$-regular automata. A solution to the problem was obtained by Thistle in 1995 which uses complex manipulations of automata. This algorithm is notoriously hard to understand and, to the best of our knowledge, has never been implemented. We show a new solution to the problem through a reduction to reactive synthesis. <br>A naive, and incorrect, approach reduces the supervisory control problem to a reactive synthesis problem that asks for a control strategy which ensures the given specification if the plant behaves in accordance to its liveness properties. This is insufficient. A correct control strategy might not fulfill the specification but force the plant to invalidate its liveness property. To prevent such solutions, supervisory control additionally requires that the controlled system is non-conflicting: any finite word compliant with the supervisor should be extendable to a word satisfying the plants' liveness properties. <br>To capture this additional requirement, our solution goes through obliging games instead. An obliging game has two requirements: a strong winning condition as in reactive synthesis and a weak winning condition. A strategy is winning if it satisfies the strong condition and additionally, every partial play can be extended to satisfy the weak condition. Obliging games can be reduced to $\omega$-regular reactive synthesis, for which symbolic algorithms exist. We reduce supervisor synthesis to obliging games. The strong condition is an implication: if the plant behaves in accordance with its liveness properties, the specification should also hold. The weak condition is the plants' liveness property.      
### 24.Channel Compression: Rethinking Information Redundancy among Channels in CNN Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2007.01696.pdf)
>  Model compression and acceleration are attracting increasing attentions due to the demand for embedded devices and mobile applications. Research on efficient convolutional neural networks (CNNs) aims at removing feature redundancy by decomposing or optimizing the convolutional calculation. In this work, feature redundancy is assumed to exist among channels in CNN architectures, which provides some leeway to boost calculation efficiency. Aiming at channel compression, a novel convolutional construction named compact convolution is proposed to embrace the progress in spatial convolution, channel grouping and pooling operation. Specifically, the depth-wise separable convolution and the point-wise interchannel operation are utilized to efficiently extract features. Different from the existing channel compression method which usually introduces considerable learnable weights, the proposed compact convolution can reduce feature redundancy with no extra parameters. With the point-wise interchannel operation, compact convolutions implicitly squeeze the channel dimension of feature maps. To explore the rules on reducing channel redundancy in neural networks, the comparison is made among different point-wise interchannel operations. Moreover, compact convolutions are extended to tackle with multiple tasks, such as acoustic scene classification, sound event detection and image classification. The extensive experiments demonstrate that our compact convolution not only exhibits high effectiveness in several multimedia tasks, but also can be efficiently implemented by benefiting from parallel computation.      
### 25.Improving auto-encoder novelty detection using channel attention and entropy minimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.01682.pdf)
>  Novelty detection is a important research area which mainly solves the classification problem of inliers which usually consists of normal samples and outliers composed of abnormal samples. We focus on the role of auto-encoder in novelty detection and further improved the performance of such methods based on auto-encoder through two main contributions. Firstly, we introduce attention mechanism into novelty detection. Under the action of attention mechanism, auto-encoder can pay more attention to the representation of inlier samples through adversarial training. Secondly, we try to constrain the expression of the latent space by information entropy. Experimental results on three public datasets show that the proposed method has potential performance for novelty detection.      
### 26.An Edge Computing-based Photo Crowdsourcing Framework for Real-time 3D Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2007.01562.pdf)
>  Image-based three-dimensional (3D) reconstruction utilizes a set of photos to build 3D model and can be widely used in many emerging applications such as augmented reality (AR) and disaster recovery. Most of existing 3D reconstruction methods require a mobile user to walk around the target area and reconstruct objectives with a hand-held camera, which is inefficient and time-consuming. To meet the requirements of delay intensive and resource hungry applications in 5G, we propose an edge computing-based photo crowdsourcing (EC-PCS) framework in this paper. The main objective is to collect a set of representative photos from ubiquitous mobile and Internet of Things (IoT) devices at the network edge for real-time 3D model reconstruction, with network resource and monetary cost considerations. Specifically, we first propose a photo pricing mechanism by jointly considering their freshness, resolution and data size. Then, we design a novel photo selection scheme to dynamically select a set of photos with the required target coverage and the minimum monetary cost. We prove the NP-hardness of such problem, and develop an efficient greedy-based approximation algorithm to obtain a near-optimal solution. Moreover, an optimal network resource allocation scheme is presented, in order to minimize the maximum uploading delay of the selected photos to the edge server. Finally, a 3D reconstruction algorithm and a 3D model caching scheme are performed by the edge server in real time. Extensive experimental results based on real-world datasets demonstrate the superior performance of our EC-PCS system over the existing mechanisms.      
### 27.Domain Adaptation without Source Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.01524.pdf)
>  Domain adaptation assumes that samples from source and target domains are freely accessible during a training phase. However, such an assumption is rarely plausible in real cases and possibly causes data-privacy issues, especially when the label of the source domain can be a sensitive attribute as an identifier. To avoid accessing source data which may contain sensitive information, we introduce source data-free domain adaptation (SFDA). Our key idea is to leverage a pre-trained model from the source domain and progressively update the target model in a self-learning manner. We observe that target samples with lower self-entropy measured by the pre-trained source model are more likely to be classified correctly. From this, we select the reliable samples with the self-entropy criterion and define these as class prototypes. We then assign pseudo labels for every target sample based on the similarity score with class prototypes. Further, to reduce the uncertainty from the pseudo labeling process, we propose set-to-set distance-based filtering which does not require any tunable hyperparameters. Finally, we train the target model with the filtered pseudo labels with regularization from the pre-trained source model. Surprisingly, without direct usage of labeled source samples, our SFDA outperforms conventional domain adaptation methods on benchmark datasets. Our code is publicly available at <a class="link-external link-https" href="https://github.com/youngryan1993/SFDA-Domain-Adaptation-without-Source-Data" rel="external noopener nofollow">this https URL</a>.      
### 28.Joint Beam Training and Data Transmission Design for Covert Millimeter-Wave Communication  [ :arrow_down: ](https://arxiv.org/pdf/2007.01513.pdf)
>  Covert communication prevents legitimate transmission from being detected by a warden while maintaining certain covert rate at the intended user. Prior works have considered the design of covert communication over conventional low-frequency bands, but few works so far have explored the higher-frequency millimeter-wave (mmWave) spectrum. The directional nature of mmWave communication makes it attractive for covert transmission. However, how to establish such directional link in a covert manner in the first place remains as a significant challenge. In this paper, we consider a covert mmWave communication system, where legitimate parties Alice and Bob adopt beam training approach for directional link establishment. Accounting for the training overhead, we develop a new design framework that jointly optimizes beam training duration, training power and data transmission power to maximize the effective throughput of Alice-Bob link while ensuring the covertness constraint at warden Willie is met. We further propose a dual-decomposition successive convex approximation algorithm to solve the problem efficiently. Numerical studies demonstrate interesting tradeoff among the key design parameters considered and also the necessity of joint design of beam training and data transmission for covert mmWave communication.      
### 29.Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations  [ :arrow_down: ](https://arxiv.org/pdf/2007.01496.pdf)
>  Despite the great progress made by deep neural networks in the semantic segmentation task, traditional neural network-based methods typically suffer from a shortage of large amounts of pixel-level annotations. Recent progress in few-shot semantic segmentation tackles the issue by utilizing only a few pixel-level annotated examples. However, these few-shot approaches cannot easily be applied to utilize image-level weak annotations, which can easily be obtained and considerably improve performance in the semantic segmentation task. In this paper, we advance the few-shot segmentation paradigm towards a scenario where image-level annotations are available to help the training process of a few pixel-level annotations. Specifically, we propose a new framework to learn the class prototype representation in the metric space by integrating image-level annotations. Furthermore, a soft masked average pooling strategy is designed to handle distractions in image-level annotations. Extensive empirical results on PASCAL-5i show that our method can achieve 5.1% and 8.2% increases of mIoU score for one-shot settings with pixel-level and scribble annotations, respectively.      
### 30.Self-Supervised GAN Compression  [ :arrow_down: ](https://arxiv.org/pdf/2007.01491.pdf)
>  Deep learning's success has led to larger and larger models to handle more and more complex tasks; trained models can contain millions of parameters. These large models are compute- and memory-intensive, which makes it a challenge to deploy them with minimized latency, throughput, and storage requirements. Some model compression methods have been successfully applied to image classification and detection or language models, but there has been very little work compressing generative adversarial networks (GANs) performing complex tasks. In this paper, we show that a standard model compression technique, weight pruning, cannot be applied to GANs using existing methods. We then develop a self-supervised compression technique which uses the trained discriminator to supervise the training of a compressed generator. We show that this framework has a compelling performance to high degrees of sparsity, can be easily applied to new tasks and models, and enables meaningful comparisons between different pruning granularities.      
### 31.A generalized stochastic control problem of bounded noise process under ambiguity arising in biological management  [ :arrow_down: ](https://arxiv.org/pdf/2007.01457.pdf)
>  The objectives and contributions of this paper are mathematical and numerical analyses of a stochastic control problem of bounded population dynamics under ambiguity, an important but not well-studied problem, focusing on the optimality equation as a nonlinear degenerate parabolic partial integro-differential equation (PIDE). The ambiguity comes from lack of knowledge on the continuous and jump noises in the dynamics, and its optimization appears as nonlinear and nonlocal terms in the PIDE. Assuming a strong dynamic programming principle for continuous value functions, we characterize its solutions from both viscosity and distribution viewpoints. Numerical computation focusing on an ergodic case are presented as well to complement the mathematical analysis.      
### 32.Improved Preterm Prediction Based on Optimized Synthetic Sampling of EHG Signal  [ :arrow_down: ](https://arxiv.org/pdf/2007.01447.pdf)
>  Preterm labor is the leading cause of neonatal morbidity and mortality and has attracted research efforts from many scientific areas. The inter-relationship between uterine contraction and the underlying electrical activities makes uterine electrohysterogram (EHG) a promising direction for preterm detection and prediction. Due the scarcity of EHG signals, especially those of preterm patients, synthetic algorithms are applied to create artificial samples of preterm type in order to remove prediction bias towards term, at the expense of a reduction of the feature effectiveness in machine-learning based automatic preterm detecting. To address such problem, we quantify the effect of synthetic samples (balance coefficient) on features' effectiveness, and form a general performance metric by utilizing multiple feature scores with relevant weights that describe their contributions to class separation. Combined with the activation/inactivation functions that characterizes the effect of the abundance of training samples in term and preterm prediction precision, we obtain an optimal sample balance coefficient that compromise the effect of synthetic samples in removing bias towards the majority and the side-effect of reducing features' importance. Substantial improvement in prediction precision has been achieved through a set of numerical tests on public available TPEHG database, and it verifies the effectiveness of the proposed method.      
### 33.Joint Frequency- and Image-Space Learning for Fourier Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.01441.pdf)
>  We propose a neural network layer structure that combines frequency and image feature representations for robust Fourier image reconstruction. Our work is motivated by the challenges in magnetic resonance imaging (MRI) where the acquired signal is a corrupted Fourier transform of the desired image. The proposed layer structure enables both correction of artifacts native to the frequency-space and manipulation of image-space representations to reconstruct coherent image structures. This is in contrast to the current deep learning approaches for image reconstruction that manipulate data solely in the frequency-space or solely in the image-space. We demonstrate the advantages of the proposed joint learning on three diverse tasks including image reconstruction from undersampled acquisitions, motion correction, and image denoising in brain MRI. Unlike purely image based and purely frequency based architectures, the proposed joint model produces consistently high quality output images. The resulting joint frequency- and image-space feature representations promise to significantly improve modeling and reconstruction of images acquired in the frequency-space. Our code is available at <a class="link-external link-https" href="https://github.com/nalinimsingh/interlacer" rel="external noopener nofollow">this https URL</a>.      
### 34.Lecture Notes on Control System Theory and Design  [ :arrow_down: ](https://arxiv.org/pdf/2007.01367.pdf)
>  This is a collection of the lecture notes of the three authors for a first-year graduate course on control system theory and design (ECE 515 , formerly ECE 415) at the ECE Department of the University of Illinois at Urbana-Champaign. This is a fundamental course on the modern theory of dynamical systems and their control, and builds on a first-level course in control that emphasizes frequency-domain methods (such as the course ECE 486 , formerly ECE 386, at UIUC ). The emphasis in this graduate course is on state space techniques, and it encompasses modeling , analysis (of structural properties of systems, such as stability, controllability, and observability), synthesis (of observers/compensators and controllers) subject to design specifications, and optimization . Accordingly, this set of lecture notes is organized in four parts, with each part dealing with one of the issues identified above. Concentration is on linear systems , with nonlinear systems covered only in some specific contexts, such as stability and dynamic optimization. Both continuous-time and discrete-time systems are covered, with the former, however, in much greater depth than the latter. <br>The main objective of this course is to teach the student some fundamental principles within a solid conceptual framework, that will enable her/him to design feedback loops compatible with the information available on the "states" of the system to be controlled, and by taking into account considerations such as stability, performance, energy conservation, and even robustness. A second objective is to familiarize her/him with the available modern computational, simulation, and general software tools that facilitate the design of effective feedback loops      
### 35.Multi-agent Planning for thermalling gliders using multi level graph-search  [ :arrow_down: ](https://arxiv.org/pdf/2007.01334.pdf)
>  This paper solves a path planning problem for a group of gliders. The gliders are tasked with visiting a set of interest points. The gliders have limited range but are able to increase their range by visiting special points called thermals. The problem addressed in this paper is of path planning for the gliders such that, the total number of interest points visited by the gliders is maximized. This is referred to as the multi-agent problem. The problem is solved by first decomposing it into several single-agent problems. In a single-agent problem a set of interest points are allocated to a single glider. This problem is solved by planning a path which maximizes the number of visited interest points from the allocated set. This is achieved through a uniform cost graph search, as shown in our earlier work. The multi-agent problem now consists of determining the best allocation (of interest points) for each glider. Two ways are presented of solving this problem, a brute force search approach as shown in earlier work and a Branch\&amp;Bound type graph search. The Branch&amp;Bound approach is the main contribution of the paper. This approach is proven to be optimal and shown to be faster than the brute force search using simulations.      
