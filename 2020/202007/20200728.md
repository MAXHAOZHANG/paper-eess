# ArXiv eess --Tue, 28 Jul 2020
### 1.Investigating the effect of edge modifications on networked control systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.13713.pdf)
>  This paper investigates the impact of addition/removal of edges in a complex networked control system, for the purposes of improving its controllability, system performances or robustness to external disturbances. The transfer function formulation we obtain allows to quantify the impact of an edge modification with the $\hinf$ and $\htwo$ norms. For stable networks with positive edge weights, we show that the $\hinf$ norm can be computed exactly for each possible single edge modification, as well as the associated stability margin. For the $\htwo$ norm we instead obtain a lower bound. Since this bound is linked to the trace of the controllability Gramian, it can be used for instance to reduce the energy needed for control. When instead the dynamics is of Laplacian type, then the norms become unbounded. However, the associated displacement systems are stable and for them the effect of edge modifications can be quantified. In particular, in this case we provide an upper bound on the $ \hinf$ norm and compute the exact value of the $ \htwo $ norm for arbitrary edge additions.      
### 2.Resource Allocation via Model-Free Deep Learning in Free Space Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.13709.pdf)
>  This paper investigates the general problem of resource allocation for mitigating channel fading effects in Free Space Optical (FSO) networks. The resource allocation problem is modelled with a constrained stochastic optimization framework, which we exemplify with problems in power adaptation and relay selection. Under this framework, we develop two algorithms to solve FSO resource allocation problems. We first present the Stochastic Dual Gradient algorithm that solves the problem exactly by exploiting the null duality gap but whose implementation necessarily requires explicit and accurate system models. As an alternative we present the Primal-Dual Deep Learning algorithm, which parametrizes the resource allocation policy with Deep Neural Networks (DNNs) and optimizes via a primal-dual method. The parametrized resource allocation problem incurs only a small loss of optimality due to the strong representational power of DNNs, and can be moreover implemented in an unsupervised manner without knowledge of system models. Numerical experiments are performed to exhibit superior performance of proposed algorithms compared to baseline methods in a variety of resource allocation problems in FSO networks, including both continuous power allocation and binary relay selection.      
### 3.From Sound Representation to Model Robustness  [ :arrow_down: ](https://arxiv.org/pdf/2007.13703.pdf)
>  In this paper, we demonstrate the extreme vulnerability of a residual deep neural network architecture (ResNet-18) against adversarial attacks in time-frequency representations of audio signals. We evaluate MFCC, short time Fourier transform (STFT), and discrete wavelet transform (DWT) to modulate environmental sound signals in 2D representation spaces. ResNet-18 not only outperforms other dense deep learning classifiers (i.e., GoogLeNet and AlexNet) in terms of recognition accuracy, but also it considerably transfers adversarial examples to other victim classifiers. On the balance of average budgets allocated by adversaries and the cost of the attack, we notice an inverse relationship between high recognition accuracy and model robustness against six strong adversarial attacks. We investigated this relationship to the three 2D representation domains, which are commonly used to represent audio signals, on three benchmarking environmental sound datasets. The experimental results have shown that while the ResNet-18 classifier trained on DWT spectrograms achieves the highest recognition accuracy, attacking this model is relatively more costly for the adversary compared to the MFCC and STFT representations.      
### 4.MMDF: Mobile Microscopy Deep Framework  [ :arrow_down: ](https://arxiv.org/pdf/2007.13701.pdf)
>  In the last decade, a huge step was done in the field of mobile microscopes development as well as in the field of mobile microscopy application to real-life disease diagnostics and a lot of other important areas (air/water quality pollution, education, agriculture). In current study we applied image processing techniques from Deep Learning (in-focus/out-of-focus classification, image deblurring and denoising, multi-focus image fusion) to the data obtained from the mobile microscope. Overview of significant works for every task is presented, the most suitable approaches were highlighted. Chosen approaches were implemented as well as their performance were compared with classical computer vision techniques.      
### 5.Adaptive Height Optimisation for Cellular-Connected UAVs using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.13695.pdf)
>  With the increasing number of \acp{uav} as users of the cellular network, the research community faces particular challenges in providing reliable \ac{uav} connectivity. A challenge that has limited research is understanding how the local building and \ac{bs} density affects \ac{uav}'s connection to a cellular network, that in the physical layer is related to its spectrum efficiency. With more \acp{bs}, the \ac{uav} connectivity could be negatively affected as it has \ac{los} to most of them, decreasing its spectral efficiency. On the other hand, buildings could be blocking interference from undesirable \ac{bs}, improving the link of the \ac{uav} to the serving \ac{bs}. This paper proposes a \ac{rl}-based algorithm to optimise the height of a UAV, as it moves dynamically within a range of heights, with the focus of increasing the UAV spectral efficiency. We evaluate the solution for different \ac{bs} and building densities. Our results show that in most scenarios \ac{rl} outperforms the baselines achieving up to 125\% over naive constant baseline, and up to 20\% over greedy approach with up front knowledge of the best height of UAV in the next time step.      
### 6.Llums que no són només llums  [ :arrow_down: ](https://arxiv.org/pdf/2007.13679.pdf)
>  Visible Light Communications (VLC) is a new paradigm in wireless communications. The characteristics of this technology, which uses light-emitting diode-based lighting devices as transmitting elements, make it possible to be considered a complement to current wireless radio communication systems. <br>----- <br>Les comunicacions per llum visible o 'Visible Light Communications' (VLC) són un nou paradigma en comunicacions sense fils. Les característiques que presenta aquesta tecnologia, que utilitza els dispositius d'il{\lgem{}}luminació basats en díodes emissors de llum com elements transmissors, fa que es pugui considerar un complement dels actuals sistemes de comunicació inal`ambrics.      
### 7.Cloud Detection through Wavelet Transforms in Machine Learning and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.13678.pdf)
>  Cloud detection is a specialized application of image recognition and object detection using remotely sensed data. The task presents a number of challenges, including analyzing images obtained in visible, infrared and multi-spectral frequencies, usually without ground truth data for comparison. Moreover, machine learning and deep learning (MLDL) algorithms applied to this task are required to be computationally efficient, as they are typically deployed in low-power devices and called to operate in real-time. <br>This paper explains Wavelet Transform (WT) theory, comparing it to more widely used image and signal processing transforms, and explores the use of WT as a powerful signal compressor and feature extractor for MLDL classifiers.      
### 8.Resource Allocation for SWIPT in Multi-Service Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.13676.pdf)
>  The novel resource allocation for simultaneous wireless information and power transfer (SWIPT) is presented as a means of not only helping to communicate and access information with increasing efficiency in the next generation of mobile data networks but also contributing to minimizing a network's overall power consumption by providing a green energy source. First, a unique architecture is proposed that harvests energy from an access point (AP) without the receiver needing a splitter. In the proposed system model, a portion of the spectrum is used for information decoding (ID) while the remaining portion is exploited for energy harvesting (EH) in an orthogonal frequency division multiple access (OFDMA) network. To investigate the performance gain, an optimization problem is formulated that maximizes the harvested energy of a multi-user single-cell OFDMA downlink (DL) network with SWIPT and also satisfies a minimum data-rate requirement for all users. A locally optimal solution for the underlying problem, which is essentially non-convex due to the coupling of the integer variable, is obtained by using optimization tools. Second, the proposed system model is improved in order to investigate the resource allocation problem of needing to maximize throughput based on the separated receiver architecture in an OFDMA multi-user multi-cell system that uses SWIPT. The resulting problem, which jointly optimizes the subcarrier assignment and power allocation, is a mixed-integer non-linear problem (MINLP) that is difficult to solve. Third, a state-of-the-art harvesting technique at the receiver that is based on receiver antenna selection with a co-located architecture is explored to optimize the energy efficiency (EE) of a SWIPT-enabled multi-cell multi-user OFDMA network. This is referred to as a Generalized Antenna-Switching Technique.      
### 9.PowerGAN: Synthesizing Appliance Power Signatures Using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.13645.pdf)
>  Non-intrusive load monitoring (NILM) allows users and energy providers to gain insight into home appliance electricity consumption using only the building's smart meter. Most current techniques for NILM are trained using significant amounts of labeled appliances power data. The collection of such data is challenging, making data a major bottleneck in creating well generalizing NILM solutions. To help mitigate the data limitations, we present the first truly synthetic appliance power signature generator. Our solution, PowerGAN, is based on conditional, progressively growing, 1-D Wasserstein generative adversarial network (GAN). Using PowerGAN, we are able to synthesise truly random and realistic appliance power data signatures. We evaluate the samples generated by PowerGAN in a qualitative way as well as numerically by using traditional GAN evaluation methods such as the Inception score.      
### 10.Robust optimal control using dynamic programming and guaranteed Euler's method  [ :arrow_down: ](https://arxiv.org/pdf/2007.13644.pdf)
>  Set-based integration methods allow to prove properties of differential systems, which take into account bounded disturbances. The systems (either time-discrete, time-continuous or hybrid) satisfying such properties are said to be "robust". In the context of optimal control synthesis, the set-based methods are generally extensions of numerical optimal methods of two classes: first, methods based on convex optimization; second, methods based on the dynamic programming principle. Heymann et al. have recently shown that, for certain systems of low dimension, the second numerical method can give better solutions than the first one. They have built a solver (Bocop) that implements both numerical methods. We show in this paper that a set-based extension of a method of the second class which uses a guaranteed Euler integration method, allows us to find such good solutions. Besides, these solutions enjoy the property of robustness against uncertainties on initial conditions and bounded disturbances. We demonstrate the practical interest of our method on an example taken from the numerical Bocop solver. We also give a variant of our method, inspired by the method of Model Predictive Control, that allows us to find more efficiently an optimal control at the price of losing robustness.      
### 11.Fully Decentralized Federated Learning Based Beamforming Design for UAV Communications  [ :arrow_down: ](https://arxiv.org/pdf/2007.13614.pdf)
>  To handle the data explosion in the era of internet of things (IoT), it is of interest to investigate the decentralized network, with the aim at relaxing the burden to central server along with keeping data privacy. In this work, we develop a fully decentralized federated learning (FL) framework with an inexact stochastic parallel random walk alternating direction method of multipliers (ISPW-ADMM). Performing more communication efficient and enhanced privacy preservation compared with the current state-of-the-art, the proposed ISPW-ADMM can be partially immune to the impacts from time-varying dynamic network and stochastic data collection, while still in fast convergence. Benefits from the stochastic gradients and biased first-order moment estimation, the proposed framework can be applied to any decentralized FL tasks over time-varying graphs. Thus to further demonstrate the practicability of such framework in providing fast convergence, high communication efficiency, and system robustness, we study the extreme learning machine(ELM)-based FL model for robust beamforming (BF) design in UAV communications, as verified by the numerical simulations.      
### 12.Interlacing properties of system-poles, system-zeros and spectral-zeros in MIMO systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.13599.pdf)
>  SISO passive systems with just one type of memory/storage element (either only inductive or only capacitative) are known to have real poles and zeros, and further, with the zeros interlacing poles (ZIP). Due to a variety of definitions of the notion of a system zero, and due to other reasons described in the paper, results involving ZIP have not been extended to MIMO systems. This paper formulates conditions under which MIMO systems too have interlaced poles and zeros. <br>This paper next focusses on the notion of a `spectral zero' of a system, which has been well-studied in various contexts: for example, spectral factorization, optimal charging/discharging of a dissipative system, and even model order reduction. We formulate conditions under which the spectral zeros of a MIMO system are real, and further, conditions that guarantee that the system-zeros, spectral zeros, and the poles are all interlaced. <br>The techniques used in the proofs involve new results in Algebraic Riccati equations (ARE) and Hamiltonian matrices, and these results help in formulating new notions of positive-real balancing, and inter-relations with the existing notion of positive-real balancing; we also relate the positive-real singular values with the eigenvalues of the extremal ARE solutions in the proposed `quasi-balanced' forms.      
### 13.A minimalist approach to scenario-based forecasting of electric vehicle consumption  [ :arrow_down: ](https://arxiv.org/pdf/2007.13570.pdf)
>  Electrification of transport is a key strategy in reducing carbon emissions. Many countries have adopted policies of complete but gradual transformation to electric vehicles (EVs). However, mass EV adoption also means a spike in electricity demand, which in turn can disrupt existing electricity infrastructure. Good EV consumption forecasts are key for distribution network operators (DNOs) to effectively manage demand and capacity. In this paper, we consider a suite of models to forecast EV consumption. More specifically, we evaluate a nested modeling approach for scenario-based forecasting of EV consumption. Using the data collected as part of the Electric Nation trials, we studied statistical models (Time Series Regression and Regression with ARIMA Errors), scalable machine learning systems (Extreme Gradient Boosting or XGB), and artificial neural networks (Long Short-Term Memory Networks or LSTMs). We found that LSTMs delivered the best forecasting performance.      
### 14.Radio Access Technology Characterisation Through Object Detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.13561.pdf)
>  \ac{RAT} classification and monitoring are essential for efficient coexistence of different communication systems in shared spectrum. Shared spectrum, including operation in license-exempt bands, is envisioned in the \ac{5G} standards (e.g., 3GPP Rel. 16). In this paper, we propose a \ac{ML} approach to characterise the spectrum utilisation and facilitate the dynamic access to it. Recent advances in \acp{CNN} enable us to perform waveform classification by processing spectrograms as images. In contrast to other \ac{ML} methods that can only provide the class of the monitored \acp{RAT}, the solution we propose can recognise not only different \acp{RAT} in shared spectrum, but also identify critical parameters such as inter-frame duration, frame duration, centre frequency, and signal bandwidth by using object detection and a feature extraction module to extract features from spectrograms. We have implemented and evaluated our solution using a dataset of commercial transmissions, as well as in a \ac{SDR} testbed environment. The scenario evaluated was the coexistence of WiFi and LTE transmissions in shared spectrum. Our results show that our approach has an accuracy of 96\% in the classification of \acp{RAT} from a dataset that captures transmissions of regular user communications. It also shows that the extracted features can be precise within a margin of 2\%, %of the size of the image, and is capable of detect above 94\% of objects under a broad range of transmission power levels and interference conditions.      
### 15.Rate-Splitting Multiple Access: Unifying NOMA and SDMA in MISO VLC Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.13560.pdf)
>  The increased proliferation of connected devices requires a paradigm shift towards the development of innovative technologies for the next generation of wireless systems. One of the key challenges, however, is the spectrum scarcity, owing to the unprecedented broadband penetration rate in recent years. Visible light communications (VLC) has recently emerged as a possible solution to enable high-speed short-range communications. However, VLC systems suffer from several limitations, including the limited modulation bandwidth of light-emitting diodes. Consequently, several multiple access techniques (MA), e.g., space-division multiple access (SDMA) and non-orthogonal multiple access (NOMA), have been considered for VLC networks. Despite their promising multiplexing gains, their performance is somewhat limited. In this article, we first provide an overview of the key MA technologies used in VLC systems. Then, we introduce rate-splitting multiple access (RSMA), which was initially proposed for RF systems and discuss its potentials in VLC systems. Through system modeling and simulations of an RSMA-based two-use scenario, we illustrate the flexibility of RSMA in generalizing NOMA and SDMA, as well as its superiority in terms of weighted sum rate (WSR) in VLC. Finally, we discuss challenges, open issues, and research directions, which will enable the practical realization of RSMA in VLC.      
### 16.Evaluating the reliability of acoustic speech embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2007.13542.pdf)
>  Speech embeddings are fixed-size acoustic representations of variable-length speech sequences. They are increasingly used for a variety of tasks ranging from information retrieval to unsupervised term discovery and speech segmentation. However, there is currently no clear methodology to compare or optimise the quality of these embeddings in a task-neutral way. Here, we systematically compare two popular metrics, ABX discrimination and Mean Average Precision (MAP), on 5 languages across 17 embedding methods, ranging from supervised to fully unsupervised, and using different loss functions (autoencoders, correspondence autoencoders, siamese). Then we use the ABX and MAP to predict performances on a new downstream task: the unsupervised estimation of the frequencies of speech segments in a given corpus. We find that overall, ABX and MAP correlate with one another and with frequency estimation. However, substantial discrepancies appear in the fine-grained distinctions across languages and/or embedding methods. This makes it unrealistic at present to propose a task-independent silver bullet method for computing the intrinsic quality of speech embeddings. There is a need for more detailed analysis of the metrics currently used to evaluate such embeddings.      
### 17.Learning Common Harmonic Waves on Stiefel Manifold -- A New Mathematical Approach for Brain Network Analyses  [ :arrow_down: ](https://arxiv.org/pdf/2007.13533.pdf)
>  Converging evidence shows that disease-relevant brain alterations do not appear in random brain locations, instead, its spatial pattern follows large scale brain networks. In this context, a powerful network analysis approach with a mathematical foundation is indispensable to understand the mechanism of neuropathological events spreading throughout the brain. Indeed, the topology of each brain network is governed by its native harmonic waves, which are a set of orthogonal bases derived from the Eigen-system of the underlying Laplacian matrix. To that end, we propose a novel connectome harmonic analysis framework to provide enhanced mathematical insights by detecting frequency-based alterations relevant to brain disorders. The backbone of our framework is a novel manifold algebra appropriate for inference across harmonic waves that overcomes the limitations of using classic Euclidean operations on irregular data structures. The individual harmonic difference is measured by a set of common harmonic waves learned from a population of individual Eigen systems, where each native Eigen-system is regarded as a sample drawn from the Stiefel manifold. Specifically, a manifold optimization scheme is tailored to find the common harmonic waves which reside at the center of Stiefel manifold. To that end, the common harmonic waves constitute the new neuro-biological bases to understand disease progression. Each harmonic wave exhibits a unique propagation pattern of neuro-pathological burdens spreading across brain networks. The statistical power of our novel connectome harmonic analysis approach is evaluated by identifying frequency-based alterations relevant to Alzheimer's disease, where our learning-based manifold approach discovers more significant and reproducible network dysfunction patterns compared to Euclidian methods.      
### 18.A Gauss-Newton-Like Hessian Approximation for Economic NMPC  [ :arrow_down: ](https://arxiv.org/pdf/2007.13519.pdf)
>  Economic Model Predictive Control (EMPC) has recently become popular because of its ability to control constrained nonlinear systems while explicitly optimizing a prescribed performance criterion. Large performance gains have been reported for many applications and closed-loop stability has been recently investigated. However, computational performance still remains an open issue and only few contributions have proposed real-time algorithms tailored to EMPC. We perform a step towards computationally cheap algorithms for EMPC by proposing a new positive-definite Hessian approximation which does not hinder fast convergence and is suitable for being used within the real-time iteration (RTI) scheme. We provide two simulation examples to demonstrate the effectiveness of RTI-based EMPC relying on the proposed Hessian approximation.      
### 19.Evidence of Task-Independent Person-Specific Signatures in EEG using Subspace Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2007.13517.pdf)
>  Electroencephalography (EEG) signals are promising as a biometric owing to the increased protection they provide against spoofing. Previous studies have focused on capturing individual variability by analyzing task/condition-specific EEG. This work attempts to model biometric signatures independent of task/condition by normalizing the associated variance. Toward this goal, the paper extends ideas from subspace-based text-independent speaker recognition and proposes novel modification for modeling multi-channel EEG data. The proposed techniques assume that biometric information is present in entirety of the EEG signal. They accumulate statistics across time in a higher dimension space and then project it to a lower-dimensional space such that the biometric information is preserved. The embeddings obtained in the proposed approach are shown to encode task-independent biometric signatures by training and testing on different tasks or conditions. The best subspace system recognizes individuals with an equal error rate (EER) of 5.81% and 16.5% on datasets with 30 and 920 subjects using just nine EEG channels. The paper also provides insights into the scalability of the subspace model to unseen tasks and individuals during training and the number of channels needed for subspace modeling.      
### 20.Receptive-Field Regularized CNNs for Music Classification and Tagging  [ :arrow_down: ](https://arxiv.org/pdf/2007.13503.pdf)
>  Convolutional Neural Networks (CNNs) have been successfully used in various Music Information Retrieval (MIR) tasks, both as end-to-end models and as feature extractors for more complex systems. However, the MIR field is still dominated by the classical VGG-based CNN architecture variants, often in combination with more complex modules such as attention, and/or techniques such as pre-training on large datasets. Deeper models such as ResNet -- which surpassed VGG by a large margin in other domains -- are rarely used in MIR. One of the main reasons for this, as we will show, is the lack of generalization of deeper CNNs in the music domain. In this paper, we present a principled way to make deep architectures like ResNet competitive for music-related tasks, based on well-designed regularization strategies. In particular, we analyze the recently introduced Receptive-Field Regularization and Shake-Shake, and show that they significantly improve the generalization of deep CNNs on music-related tasks, and that the resulting deep CNNs can outperform current more complex models such as CNNs augmented with pre-training and attention. We demonstrate this on two different MIR tasks and two corresponding datasets, thus offering our deep regularized CNNs as a new baseline for these datasets, which can also be used as a feature-extracting module in future, more complex approaches.      
### 21.Attention-based Graph ResNet for Motor Intent Detection from Raw EEG signals  [ :arrow_down: ](https://arxiv.org/pdf/2007.13484.pdf)
>  In previous studies, decoding electroencephalography (EEG) signals has not considered the topological relationship of EEG electrodes. However, the latest neuroscience has suggested brain network connectivity. Thus, the exhibited interaction between EEG channels might not be appropriately measured via Euclidean distance. To fill the gap, an attention-based graph residual network, a novel structure of Graph Convolutional Neural Network (GCN), was presented to detect human motor intents from raw EEG signals, where the topological structure of EEG electrodes was built as a graph. Meanwhile, deep residual learning with a full-attention architecture was introduced to address the degradation problem concerning deeper networks in raw EEG motor imagery (MI) data. Individual variability, the critical and longstanding challenge underlying EEG signals, has been successfully handled with the state-of-the-art performance, 98.08% accuracy at the subject level, 94.28% for 20 subjects. Numerical results were promising that the implementation of the graph-structured topology was superior to decode raw EEG data. The innovative deep learning approach was expected to entail a universal method towards both neuroscience research and real-world EEG-based practical applications, e.g., seizure prediction.      
### 22.Review of the State-of-the-Art on Adaptive Protection for Microgrids based on Communications  [ :arrow_down: ](https://arxiv.org/pdf/2007.13479.pdf)
>  The dominance of distributed energy resources in microgrids and the associated weather dependency require flexible protection. They include devices capable of adapting their protective settings as a reaction to (potential) changes in system state. Communication technologies have a key role in this system since the reactions of the adaptive devices shall be coordinated. This coordination imposes strict requirements: communications must be available and ultra-reliable with bounded latency in the order of milliseconds. This paper reviews the state-of-the-art in the field and provides a thorough analysis of the main related communication technologies and optimization techniques. We also present our perspective on the future of communication deployments in microgrids, indicating the viability of 5G wireless systems and multi-connectivity to enable adaptive protection.      
### 23.Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.13465.pdf)
>  We propose a self-supervised representation learning model for the task of unsupervised phoneme boundary detection. The model is a convolutional neural network that operates directly on the raw waveform. It is optimized to identify spectral changes in the signal using the Noise-Contrastive Estimation principle. At test time, a peak detection algorithm is applied over the model outputs to produce the final boundaries. As such, the proposed model is trained in a fully unsupervised manner with no manual annotations in the form of target boundaries nor phonetic transcriptions. We compare the proposed approach to several unsupervised baselines using both TIMIT and Buckeye corpora. Results suggest that our approach surpasses the baseline models and reaches state-of-the-art performance on both data sets. Furthermore, we experimented with expanding the training set with additional examples from the Librispeech corpus. We evaluated the resulting model on distributions and languages that were not seen during the training phase (English, Hebrew and German) and showed that utilizing additional untranscribed data is beneficial for model performance.      
### 24.Intelligent Trajectory Planning in UAV-mounted Wireless Networks: A Quantum-Inspired Reinforcement Learning Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2007.13418.pdf)
>  In this paper, we consider a wireless uplink transmission scenario in which an unmanned aerial vehicle (UAV) serves as an aerial base station collecting data from ground users. To optimise the expected sum uplink transmit rate without any prior knowledge of ground users (e.g., locations, channel state information and transmit power), the trajectory planning problem is optimized via the quantum-inspired reinforcement learning (QiRL) approach. Specifically, the QiRL method adopts novel probabilistic action selection policy and new reinforcement strategy, which are inspired by the collapse phenomenon and amplitude amplification in quantum computation theory. Numerical results demonstrate that the proposed QiRL solution can offer natural balancing between exploration and exploitation via ranking collapse probabilities of possible actions, compared to the popularly-used reinforcement learning approaches which are highly dependent on tuned exploration parameters.      
### 25.XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on Anatomically Variable XCAT Phantoms  [ :arrow_down: ](https://arxiv.org/pdf/2007.13408.pdf)
>  Generative adversarial networks (GANs) have provided promising data enrichment solutions by synthesizing high-fidelity images. However, generating large sets of labeled images with new anatomical variations remains unexplored. We propose a novel method for synthesizing cardiac magnetic resonance (CMR) images on a population of virtual subjects with a large anatomical variation, introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human phantom. We investigate two conditional image synthesis approaches grounded on a semantically-consistent mask-guided image generation technique: 4-class and 8-class XCAT-GANs. The 4-class technique relies on only the annotations of the heart; while the 8-class technique employs a predicted multi-tissue label map of the heart-surrounding organs and provides better guidance for our conditional image synthesis. For both techniques, we train our conditional XCAT-GAN with real images paired with corresponding labels and subsequently at the inference time, we substitute the labels with the XCAT derived ones. Therefore, the trained network accurately transfers the tissue-specific textures to the new label maps. By creating 33 virtual subjects of synthetic CMR images at the end-diastolic and end-systolic phases, we evaluate the usefulness of such data in the downstream cardiac cavity segmentation task under different augmentation strategies. Results demonstrate that even with only 20% of real images (40 volumes) seen during training, segmentation performance is retained with the addition of synthetic CMR images. Moreover, the improvement in utilizing synthetic images for augmenting the real data is evident through the reduction of Hausdorff distance up to 28% and an increase in the Dice score up to 5%, indicating a higher similarity to the ground truth in all dimensions.      
### 26.IEEE 802.11be-Extremely High Throughput WLAN: New Challenges and Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2007.13401.pdf)
>  With the emergence of 4k/8k video, the throughput requirement of video delivery will keep grow to tens of Gbps. Other new high-throughput and low-latency video applications including augmented reality (AR), virtual reality (VR), and online gaming, are also proliferating. Due to the related stringent requirements, supporting these applications over wireless local area network (WLAN) is far beyond the capabilities of the new WLAN standard -- IEEE 802.11ax. To meet these emerging demands, the IEEE 802.11 will release a new amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known as Wireless-Fidelity (Wi-Fi) 7. This article provides the comprehensive survey on the key medium access control (MAC) layer techniques and physical layer (PHY) techniques being discussed in the EHT task group, including the channelization and tone plan, multiple resource units (multi-RU) support, 4096 quadrature amplitude modulation (4096-QAM), preamble designs, multiple link operations (e.g., multi-link aggregation and channel access), multiple input multiple output (MIMO) enhancement, multiple access point (multi-AP) coordination (e.g., multi-AP joint transmission), enhanced link adaptation and retransmission protocols (e.g., hybrid automatic repeat request (HARQ)). This survey covers both the critical technologies being discussed in EHT standard and the related latest progresses from worldwide research. Besides, the potential developments beyond EHT are discussed to provide some possible future research directions for WLAN.      
### 27.Self Attentive Multi Layer Aggregation with Feature Recalibration and Normalization for End to End Speaker Verification System  [ :arrow_down: ](https://arxiv.org/pdf/2007.13350.pdf)
>  One of the most important parts of an end-to-end speaker verification system is the speaker embedding generation. In our previous paper, we reported that shortcut connections-based multi-layer aggregation improves the representational power of the speaker embedding. However, the number of model parameters is relatively large and the unspecified variations increase in the multi-layer aggregation. Therefore, we propose a self-attentive multi-layer aggregation with feature recalibration and normalization for end-to-end speaker verification system. To reduce the number of model parameters, the ResNet, which scaled channel width and layer depth, is used as a baseline. To control the variability in the training, a self-attention mechanism is applied to perform the multi-layer aggregation with dropout regularizations and batch normalizations. Then, a feature recalibration layer is applied to the aggregated feature using fully-connected layers and nonlinear activation functions. Deep length normalization is also used on a recalibrated feature in the end-to-end training process. Experimental results using the VoxCeleb1 evaluation dataset showed that the performance of the proposed methods was comparable to that of state-of-the-art models (equal error rate of 4.95% and 2.86%, using the VoxCeleb1 and VoxCeleb2 training datasets, respectively).      
### 28.Analysis of Emotional Content in Indian Political Speeches  [ :arrow_down: ](https://arxiv.org/pdf/2007.13325.pdf)
>  Emotions play an essential role in public speaking. The emotional content of speech has the power to influence minds. As such, we present an analysis of the emotional content of politicians speech in the Indian political scenario. We investigate the emotional content present in the speeches of politicians using an Attention based CNN+LSTM network. Experimental evaluations on a dataset of eight Indian politicians shows how politicians incorporate emotions in their speeches to strike a chord with the masses. An analysis of the voting share received along with victory margin and their relation to emotional content in speech of the politicians is also presented.      
### 29.Enhanced Beam Alignment for Millimeter Wave MIMO Systems: A Kolmogorov Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.13299.pdf)
>  We present an enhancement to the problem of beam alignment in millimeter wave (mmWave) multiple-input multiple-output (MIMO) systems, based on a modification of the machine learning-based criterion, called Kolmogorov model (KM), previously applied to the beam alignment problem. Unlike the previous KM, whose computational complexity is not scalable with the size of the problem, a new approach, centered on discrete monotonic optimization (DMO), is proposed, leading to significantly reduced complexity. We also present a Kolmogorov-Smirnov (KS) criterion for the advanced hypothesis testing, which does not require any subjective threshold setting compared to the frequency estimation (FE) method developed for the conventional KM. Simulation results that demonstrate the efficacy of the proposed KM learning for mmWave beam alignment are presented.      
### 30.Aerial Intelligent Reflecting Surface: Joint Placement and Passive Beamforming Design with 3D Beam Flattening  [ :arrow_down: ](https://arxiv.org/pdf/2007.13295.pdf)
>  This paper proposes a new three-dimensional (3D) wireless passive relaying system enabled by aerial IRS (AIRS). Compared to the conventional terrestrial IRS, AIRS enjoys more deployment flexibility as well as wider-range signal reflection, thanks to its high altitude and thus more likelihood of establishing line-of-sight (LoS) links with ground source/destination nodes. Specifically, we aim to maximize the worst-case signal-to-noise ratio (SNR) over all locations in a target area by jointly optimizing the transmit beamforming for the source node and the placement as well as 3D passive beamforming for the AIRS. The formulated problem is non-convex and thus difficult to solve. To gain useful insights, we first consider the special case of maximizing the SNR at a given target location, for which the optimal solution is obtained in closed-form. The result shows that the optimal horizontal AIRS placement only depends on the ratio between the source-destination distance and the AIRS altitude. Then for the general case of AIRS-enabled area coverage, we propose an efficient solution by decoupling the AIRS passive beamforming design to maximize the worst-case array gain, from its placement optimization by balancing the resulting angular span and the cascaded channel path loss. Our proposed solution is based on a novel 3D beam broadening and flattening technique, where the passive array of the AIRS is divided into sub-arrays of appropriate size, and their phase shifts are designed to form a flattened beam pattern with adjustable beamwidth catering to the size of the coverage area. Both the uniform linear array (ULA)-based and uniform planar array (UPA)-based AIRSs are considered in our design, which enable two-dimensional (2D) and 3D passive beamforming, respectively. Numerical results show that the proposed designs achieve significant performance gains over the benchmark schemes.      
### 31.Deep Learning Methods for Solving Linear Inverse Problems: Research Directions and Paradigms  [ :arrow_down: ](https://arxiv.org/pdf/2007.13290.pdf)
>  The linear inverse problem is fundamental to the development of various scientific areas. Innumerable attempts have been carried out to solve different variants of the linear inverse problem in different applications. Nowadays, the rapid development of deep learning provides a fresh perspective for solving the linear inverse problem, which has various well-designed network architectures results in state-of-the-art performance in many applications. In this paper, we present a comprehensive survey of the recent progress in the development of deep learning for solving various linear inverse problems. We review how deep learning methods are used in solving different linear inverse problems, and explore the structured neural network architectures that incorporate knowledge used in traditional methods. Furthermore, we identify open challenges and potential future directions along this research line.      
### 32.Privacy-Preserving Resilience of Cyber-Physical Systems to Adversaries  [ :arrow_down: ](https://arxiv.org/pdf/2007.13272.pdf)
>  A cyber-physical system (CPS) is expected to be resilient to more than one type of adversary. In this paper, we consider a CPS that has to satisfy a linear temporal logic (LTL) objective in the presence of two kinds of adversaries. The first adversary has the ability to tamper with inputs to the CPS to influence satisfaction of the LTL objective. The interaction of the CPS with this adversary is modeled as a stochastic game. We synthesize a controller for the CPS to maximize the probability of satisfying the LTL objective under any policy of this adversary. The second adversary is an eavesdropper who can observe labeled trajectories of the CPS generated from the previous step. It could then use this information to launch other kinds of attacks. A labeled trajectory is a sequence of labels, where a label is associated to a state and is linked to the satisfaction of the LTL objective at that state. We use differential privacy to quantify the indistinguishability between states that are related to each other when the eavesdropper sees a labeled trajectory. Two trajectories of equal length will be differentially private if they are differentially private at each state along the respective trajectories. We use a skewed Kantorovich metric to compute distances between probability distributions over states resulting from actions chosen according to policies from related states in order to quantify differential privacy. Moreover, we do this in a manner that does not affect the satisfaction probability of the LTL objective. We validate our approach on a simulation of a UAV that has to satisfy an LTL objective in an adversarial environment.      
### 33.On the Use of Audio Fingerprinting Features for Speech Enhancement with Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.13258.pdf)
>  The advent of learning-based methods in speech enhancement has revived the need for robust and reliable training features that can compactly represent speech signals while preserving their vital information. Time-frequency domain features, such as the Short-Term Fourier Transform (STFT) and Mel-Frequency Cepstral Coefficients (MFCC), are preferred in many approaches. While the MFCC provide for a compact representation, they ignore the dynamics and distribution of energy in each mel-scale subband. In this work, a speech enhancement system based on Generative Adversarial Network (GAN) is implemented and tested with a combination of Audio FingerPrinting (AFP) features obtained from the MFCC and the Normalized Spectral Subband Centroids (NSSC). The NSSC capture the locations of speech formants and complement the MFCC in a crucial way. In experiments with diverse speakers and noise types, GAN-based speech enhancement with the proposed AFP feature combination achieves the best objective performance while reducing memory requirements and training time.      
### 34.Uniformizing Techniques to Process CT scans with 3D CNNs for Tuberculosis Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2007.13224.pdf)
>  A common approach to medical image analysis on volumetric data uses deep 2D convolutional neural networks (CNNs). This is largely attributed to the challenges imposed by the nature of the 3D data: variable volume size, GPU exhaustion during optimization. However, dealing with the individual slices independently in 2D CNNs deliberately discards the depth information which results in poor performance for the intended task. Therefore, it is important to develop methods that not only overcome the heavy memory and computation requirements but also leverage the 3D information. To this end, we evaluate a set of volume uniformizing methods to address the aforementioned issues. The first method involves sampling information evenly from a subset of the volume. Another method exploits the full geometry of the 3D volume by interpolating over the z-axis. We demonstrate performance improvements using controlled ablation studies as well as put this approach to the test on the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark. We report 73% area under curve (AUC) and binary classification accuracy (ACC) of 67.5% on the test set beating all methods which leveraged only image information (without using clinical meta-data) achieving 5-th position overall. All codes and models are made available at <a class="link-external link-https" href="https://github.com/hasibzunair/uniformizing-3D" rel="external noopener nofollow">this https URL</a>.      
### 35.Double Multi-Head Attention for Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2007.13199.pdf)
>  Most state-of-the-art Deep Learning systems for speaker verification are based on speaker embedding extractors. These architectures are commonly composed of a feature extractor front-end together with a pooling layer to encode variable-length utterances into fixed-length speaker vectors. In this paper we present Double Multi-Head Attention pooling, which extends our previous approach based on Self Multi-Head Attention. An additional self attention layer is added to the pooling layer that summarizes the context vectors produced by Multi-Head Attention into a unique speaker representation. This method enhances the pooling mechanism by giving weights to the information captured for each head and it results in creating more discriminative speaker embeddings. We have evaluated our approach with the VoxCeleb2 dataset. Our results show 9.19\% and 4.29\% relative improvement in terms of EER compared to Self Attention pooling and Self Multi-Head Attention, respectively. According to the obtained results, Double Multi-Head Attention has shown to be an excellent approach to efficiently select the most relevant features captured by the CNN-based front-ends from the speech signal.      
### 36.PDE Evolutions for M-Smoothers in One, Two, and Three Dimensions  [ :arrow_down: ](https://arxiv.org/pdf/2007.13191.pdf)
>  Local M-smoothers are interesting and important signal and image processing techniques with many connections to other methods. In our paper we derive a family of partial differential equations (PDEs) that result in one, two, and three dimensions as limiting processes from M-smoothers which are based on local order-$p$ means within a ball the radius of which tends to zero. The order $p$ may take any nonzero value $&gt;-1$, allowing also negative values. In contrast to results from the literature, we show in the space-continuous case that mode filtering does not arise for $p \to 0$, but for $p \to -1$. Extending our filter class to $p$-values smaller than $-1$ allows to include e.g. the classical image sharpening flow of Gabor. The PDEs we derive in 1D, 2D, and 3D show large structural similarities. Since our PDE class is highly anisotropic and may contain backward parabolic operators, designing adequate numerical methods is difficult. We present an $L^\infty$-stable explicit finite difference scheme that satisfies a discrete maximum--minimum principle, offers excellent rotation invariance, and employs a splitting into four fractional steps to allow larger time step sizes. Although it approximates parabolic PDEs, it consequently benefits from stabilisation concepts from the numerics of hyperbolic PDEs. Our 2D experiments show that the PDEs for $p&lt;1$ are of specific interest: Their backward parabolic term creates favourable sharpening properties, while they appear to maintain the strong shape simplification properties of mean curvature motion.      
### 37.Controller design for robust invariance from noisy data  [ :arrow_down: ](https://arxiv.org/pdf/2007.13181.pdf)
>  For an unknown linear system, starting from noisy open-loop input-state data collected during a finite-length experiment, we directly design a linear feedback controller that guarantees robust invariance of a given polyhedral set of the state in the presence of disturbances. The main result is a necessary and sufficient condition for the existence of such a controller, and amounts to the solution of a linear program. The benefits of large and rich data sets for the solution of the problem are discussed. A numerical example about a simplified platoon of two vehicles illustrates the method.      
### 38.Radio Resource Management in Joint Radar and Communication: A Comprehensive Survey  [ :arrow_down: ](https://arxiv.org/pdf/2007.13146.pdf)
>  Joint radar and communication (JRC) has recently attracted substantial attention. The first reason is that JRC allows individual radar and communication systems to share spectrum bands and thus improves the spectrum utilization. The second reason is that JRC enables a single hardware platform, e.g., an autonomous vehicle or a UAV, to simultaneously perform the communication function and the radar function. As a result, JRC is able to improve the efficiency of resources, i.e., spectrum and energy, reduce the system size, and minimize the system cost. However, there are several challenges to be solved for the JRC design. In particular, sharing the spectrum imposes the interference caused by the systems, and sharing the hardware platform and energy resource complicates the design of the JRC transmitter and compromises the performance of each function. To address the challenges, several resource management approaches have been recently proposed, and this paper presents a comprehensive literature review on resource management for JRC. First, we give fundamental concepts of JRC, important performance metrics used in JRC systems, and applications of the JRC systems. Then, we review and analyze resource management approaches, i.e., spectrum sharing, power allocation, and interference management, for JRC. In addition, we present security issues to JRC and provide a discussion of countermeasures to the security issues. Finally, we highlight important challenges in the JRC design and discuss future research directions related to JRC.      
### 39.Joint reconstruction and bias field correction for undersampled MR imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.13123.pdf)
>  Undersampling the k-space in MRI allows saving precious acquisition time, yet results in an ill-posed inversion problem. Recently, many deep learning techniques have been developed, addressing this issue of recovering the fully sampled MR image from the undersampled data. However, these learning based schemes are susceptible to differences between the training data and the image to be reconstructed at test time. One such difference can be attributed to the bias field present in MR images, caused by field inhomogeneities and coil sensitivities. In this work, we address the sensitivity of the reconstruction problem to the bias field and propose to model it explicitly in the reconstruction, in order to decrease this sensitivity. To this end, we use an unsupervised learning based reconstruction algorithm as our basis and combine it with a N4-based bias field estimation method, in a joint optimization scheme. We use the HCP dataset as well as in-house measured images for the evaluations. We show that the proposed method improves the reconstruction quality, both visually and in terms of RMSE.      
### 40.UIAI System for Short-Duration Speaker Verification Challenge 2020  [ :arrow_down: ](https://arxiv.org/pdf/2007.13118.pdf)
>  In this work, we present the system description of the UIAI entry for the short-duration speaker verification (SdSV) challenge 2020. Our focus is on Task 1 dedicated to text-dependent speaker verification. We investigate different feature extraction and modeling approaches for automatic speaker verification (ASV) and utterance verification (UV). We have also studied different fusion strategies for combining UV and ASV modules. Our primary submission to the challenge is the fusion of seven subsystems which yields a normalized minimum detection cost function (minDCF) of 0.072 and an equal error rate (EER) of 2.14% on the evaluation set. The single system consisting of a pass-phrase identification based model with phone-discriminative bottleneck features gives a normalized minDCF of 0.118 and achieves 19% relative improvement over the state-of-the-art challenge baseline.      
### 41.Deep CHORES: Estimating Hallmark Measures of Physical Activity Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.13114.pdf)
>  Wrist accelerometers for assessing hallmark measures of physical activity (PA) are rapidly growing with the advent of smartwatch technology. Given the growing popularity of wrist-worn accelerometers, there needs to be a rigorous evaluation for recognizing (PA) type and estimating energy expenditure (EE) across the lifespan. Participants (66% women, aged 20-89 yrs) performed a battery of 33 daily activities in a standardized laboratory setting while a tri-axial accelerometer collected data from the right wrist. A portable metabolic unit was worn to measure metabolic intensity. We built deep learning networks to extract spatial and temporal representations from the time-series data, and used them to recognize PA type and estimate EE. The deep learning models resulted in high performance; the F1 score was: 0.82, 0.81, and 95 for recognizing sedentary, locomotor, and lifestyle activities, respectively. The root mean square error was 1.1 (+/-0.13) for the estimation of EE.      
### 42.MACU-Net Semantic Segmentation from High-Resolution Remote Sensing Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.13083.pdf)
>  Semantic segmentation of remote sensing images plays an important role in land resource management, yield estimation, and economic assessment. U-Net is a sophisticated encoder-decoder architecture which has been frequently used in medical image segmentation and has attained prominent performance. And asymmetric convolution block can enhance the square convolution kernels using asymmetric convolutions. In this paper, based on U-Net and asymmetric convolution block, we incorporate multi-scale features generated by different layers of U-Net and design a multi-scale skip connected architecture, MACU-Net, for semantic segmentation using high-resolution remote sensing images. Our design has the following advantages: (1) The multi-scale skip connections combine and realign semantic features contained both in low-level and high-level feature maps with different scales; (2) the asymmetric convolution block strengthens the representational capacity of a standard convolution layer. Experiments conducted on two remote sensing image datasets captured by separate satellites demonstrate that the performance of our MACU-Net transcends the U-Net, SegNet, DeepLab V3+, and other baseline algorithms.      
### 43.End-to-end spoofing detection with raw waveform CLDNNs  [ :arrow_down: ](https://arxiv.org/pdf/2007.13060.pdf)
>  Albeit recent progress in speaker verification generates powerful models, malicious attacks in the form of spoofed speech, are generally not coped with. Recent results in ASVSpoof2015 and BTAS2016 challenges indicate that spoof-aware features are a possible solution to this problem. Most successful methods in both challenges focus on spoof-aware features, rather than focusing on a powerful classifier. In this paper we present a novel raw waveform based deep model for spoofing detection, which jointly acts as a feature extractor and classifier, thus allowing it to directly classify speech signals. This approach can be considered as an end-to-end classifier, which removes the need for any pre- or post-processing on the data, making training and evaluation a streamlined process, consuming less time than other neural-network based approaches. The experiments on the BTAS2016 dataset show that the system performance is significantly improved by the proposed raw waveform convolutional long short term neural network (CLDNN), from the previous best published 1.26\% half total error rate (HTER) to the current 0.82\% HTER. Moreover it shows that the proposed system also performs well under the unknown (RE-PH2-PH3,RE-LPPH2-PH3) conditions.      
### 44.Convex Decreasing Algorithms: Distributed Synthesis and Finite-time Termination in Higher Dimensio  [ :arrow_down: ](https://arxiv.org/pdf/2007.13050.pdf)
>  We introduce a general mathematical framework for distributed algorithms, and a monotonicity property frequently satisfied in application. These properties are leveraged to provide finite-time guarantees for converging algorithms, suited for use in the absence of a central authority. A central application is to consensus algorithms in higher dimension. These pursuits motivate a new peer to peer convex hull algorithm which we demonstrate to be an instantiation of the described theory. To address the diversity of convex sets and the potential computation and communication costs of knowing such sets in high dimension, a lightweight norm based stopping criteria is developed. More explicitly, we give a distributed algorithm that terminates in finite time when applied to consensus problems in higher dimensions and guarantees the convergence of the consensus algorithm in norm, within any given tolerance. Applications to consensus least squared estimation and distributed function determination are developed. The practical utility of the algorithm is illustrated through MATLAB simulations.      
### 45.Calibration-free quantitative phase imaging using data-driven aberration modeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.13038.pdf)
>  We present a data-driven approach to compensate for optical aberration in calibration-free quantitative phase imaging (QPI). Unlike existing methods that require additional measurements or a background region to correct aberrations, we exploit deep learning techniques to model the physics of aberration in an imaging system. We demonstrate the generation of a single-shot aberration-corrected field image by using a U-net-based deep neural network that learns a translation between an optical field with aberrations and an aberration-corrected field. The high fidelity of our method is demonstrated on 2D and 3D QPI measurements of various confluent eukaryotic cells, benchmarking against the conventional method using background subtractions.      
### 46.Cyclic Imaging for All-Sky Interference Forecasting with Array Radio Telescopes  [ :arrow_down: ](https://arxiv.org/pdf/2007.13035.pdf)
>  Radio Frequency Interference (RFI) is threatening modern radio astronomy. A classic approach to mitigate its impact on astronomical data involves discarding the corrupted time and frequency data samples through a process called flagging and blanking. We propose the exploitation of the cyclostationary properties of the RFI signals to reliably detect and predict their locations within an array radio telescope field-of-view, and dynamically schedule the astronomical observations such as to minimize the probability of RFI data corruption.      
### 47.Self-Expressing Autoencoders for Unsupervised Spoken Term Discovery  [ :arrow_down: ](https://arxiv.org/pdf/2007.13033.pdf)
>  Unsupervised spoken term discovery consists of two tasks: finding the acoustic segment boundaries and labeling acoustically similar segments with the same labels. We perform segmentation based on the assumption that the frame feature vectors are more similar within a segment than across the segments. Therefore, for strong segmentation performance, it is crucial that the features represent the phonetic properties of a frame more than other factors of variability. We achieve this via a self-expressing autoencoder framework. It consists of a single encoder and two decoders with shared weights. The encoder projects the input features into a latent representation. One of the decoders tries to reconstruct the input from these latent representations and the other from the self-expressed version of them. We use the obtained features to segment and cluster the speech data. We evaluate the performance of the proposed method in the Zero Resource 2020 challenge unit discovery task. The proposed system consistently outperforms the baseline, demonstrating the usefulness of the method in learning representations.      
### 48.Exploring Deep Hybrid Tensor-to-Vector Network Architectures for Regression Based Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2007.13024.pdf)
>  This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train (TT) output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network (CNN) based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32\% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44\% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes.      
### 49.Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.13002.pdf)
>  This study addresses unsupervised subword modeling, i.e., learning feature representations that can distinguish subword units of a language. The proposed approach adopts a two-stage bottleneck feature (BNF) learning framework, consisting of autoregressive predictive coding (APC) as a front-end and a DNN-BNF model as a back-end. APC pretrained features are set as input features to a DNN-BNF model. A language-mismatched ASR system is used to provide cross-lingual phone labels for DNN-BNF model training. Finally, BNFs are extracted as the subword-discriminative feature representation. A second aim of this work is to investigate the robustness of our approach's effectiveness to different amounts of training data. The results on Libri-light and the ZeroSpeech 2017 databases show that APC is effective in front-end feature pretraining. Our whole system outperforms the state of the art on both databases. Cross-lingual phone labels for English data by a Dutch ASR outperform those by a Mandarin ASR, possibly linked to the larger similarity of Dutch compared to Mandarin with English. Our system is less sensitive to training data amount when the training data is over 50 hours. APC pretraining leads to a reduction of needed training material from over 5,000 hours to around 200 hours with little performance degradation.      
### 50.Minimum Overhead Beamforming and Resource Allocation in D2D Edge Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.12963.pdf)
>  Device-to-device (D2D) communications is expected to be a critical enabler of distributed computing in edge networks at scale. A key challenge in providing this capability is the requirement for judicious management of the heterogeneous communication and computation resources that exist at the edge to meet processing needs. In this paper, we develop an optimization methodology that considers topology configuration jointly with device and network resource allocation to minimize total D2D overhead, which we quantify in terms of time and energy required for task processing. Variables in our model include task assignment, CPU allocation, subchannel selection, and beamforming design for multiple input multiple output (MIMO) wireless devices. We propose two methods to solve the resulting non-convex mixed integer program: semi-exhaustive search optimization, which represents a ``best-effort'' at obtaining the optimal solution, and efficient alternate optimization, which is more computationally efficient. As a component of these two methods, we develop a coordinated beamforming algorithm which we show obtains the optimal beamformer for a common receiver characteristic. Through numerical experiments, we find that our methodology yields substantial improvements in network overhead compared with local computation and partially optimized methods, which validates our joint optimization approach. Further, we find that the efficient alternate optimization scales well with the number of nodes, and thus can be a practical solution for D2D computing in large networks.      
### 51.Quasi-Periodic Parallel WaveGAN: A Non-autoregressive Raw Waveform Generative Model with Pitch-dependent Dilated Convolution Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.12955.pdf)
>  In this paper, we propose a quasi-periodic parallel WaveGAN (QPPWG) waveform generative model, which applies a quasi-periodic (QP) structure to a parallel WaveGAN (PWG) model using pitch-dependent dilated convolution networks (PDCNNs). PWG is a small-footprint GAN-based raw waveform generative model, whose generation time is much faster than real time because of its compact model and non-autoregressive (non-AR) and non-causal mechanisms. Although PWG achieves high-fidelity speech generation, the generic and simple network architecture lacks pitch controllability for an unseen auxiliary fundamental frequency ($F_{0}$) feature such as a scaled $F_{0}$. To improve the pitch controllability and speech modeling capability, we apply a QP structure with PDCNNs to PWG, which introduces pitch information to the network by dynamically changing the network architecture corresponding to the auxiliary $F_{0}$ feature. Both objective and subjective experimental results show that QPPWG outperforms PWG when the auxiliary $F_{0}$ feature is scaled. Moreover, analyses of the intermediate outputs of QPPWG also show better tractability and interpretability of QPPWG, which respectively models spectral and excitation-like signals using the cascaded fixed and adaptive blocks of the QP structure.      
### 52.Comparison of Machine Learning Methods for Predicting Karst Spring Discharge in North China  [ :arrow_down: ](https://arxiv.org/pdf/2007.12951.pdf)
>  The quantitative analyses of karst spring discharge typically rely on physical-based models, which are inherently uncertain. To improve the understanding of the mechanism of spring discharge fluctuation and the relationship between precipitation and spring discharge, three machine learning methods were developed to reduce the predictive errors of physical-based groundwater models, simulate the discharge of Longzici Spring's karst area, and predict changes in the spring on the basis of long time series precipitation monitoring and spring water flow data from 1987 to 2018. The three machine learning methods included two artificial neural networks (ANNs), namely, multilayer perceptron (MLP) and long short-term memory-recurrent neural network (LSTM-RNN), and support vector regression (SVR). A normalization method was introduced for data preprocessing to make the three methods robust and computationally efficient. To compare and evaluate the capability of the three machine learning methods, the mean squared error (MSE), mean absolute error (MAE), and root-mean-square error (RMSE) were selected as the performance metrics for these methods. Simulations showed that MLP reduced MSE, MAE, and RMSE to 0.0010, 0.0254, and 0.0318, respectively. Meanwhile, LSTM-RNN reduced MSE to 0.0010, MAE to 0.0272, and RMSE to 0.0329. Moreover, the decrease in MSE, MAE, and RMSE were 0.0910, 0.1852, and 0.3017, respectively, for SVR. Results indicated that MLP performed slightly better than LSTM-RNN, and MLP and LSTM-RNN performed considerably better than SVR. Furthermore, ANNs were demonstrated to be prior machine learning methods for simulating and predicting karst spring discharge.      
### 53.Nonlinear ISA with Auxiliary Variables for Learning Speech Representations  [ :arrow_down: ](https://arxiv.org/pdf/2007.12948.pdf)
>  This paper extends recent work on nonlinear Independent Component Analysis (ICA) by introducing a theoretical framework for nonlinear Independent Subspace Analysis (ISA) in the presence of auxiliary variables. Observed high dimensional acoustic features like log Mel spectrograms can be considered as surface level manifestations of nonlinear transformations over individual multivariate sources of information like speaker characteristics, phonological content etc. Under assumptions of energy based models we use the theory of nonlinear ISA to propose an algorithm that learns unsupervised speech representations whose subspaces are independent and potentially highly correlated with the original non-stationary multivariate sources. We show how nonlinear ICA with auxiliary variables can be extended to a generic identifiable model for subspaces as well while also providing sufficient conditions for the identifiability of these high dimensional subspaces. Our proposed methodology is generic and can be integrated with standard unsupervised approaches to learn speech representations with subspaces that can theoretically capture independent higher order speech signals. We evaluate the gains of our algorithm when integrated with the Autoregressive Predictive Decoding (APC) model by showing empirical results on the speaker verification and phoneme recognition tasks.      
### 54.Multi-speaker Emotion Conversion via Latent Variable Regularization and a Chained Encoder-Decoder-Predictor Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.12937.pdf)
>  We propose a novel method for emotion conversion in speech based on a chained encoder-decoder-predictor neural network architecture. The encoder constructs a latent embedding of the fundamental frequency (F0) contour and the spectrum, which we regularize using the Large Diffeomorphic Metric Mapping (LDDMM) registration framework. The decoder uses this embedding to predict the modified F0 contour in a target emotional class. Finally, the predictor uses the original spectrum and the modified F0 contour to generate a corresponding target spectrum. Our joint objective function simultaneously optimizes the parameters of three model blocks. We show that our method outperforms the existing state-of-the-art approaches on both, the saliency of emotion conversion and the quality of resynthesized speech. In addition, the LDDMM regularization allows our model to convert phrases that were not present in training, thus providing evidence for out-of-sample generalization.      
### 55.A deep learning based multiscale approach to segment cancer area in liver whole slide image  [ :arrow_down: ](https://arxiv.org/pdf/2007.12935.pdf)
>  This paper addresses the problem of liver cancer segmentation in Whole Slide Image (WSI). We propose a multi-scale image processing method based on automatic end-to-end deep neural network algorithm for segmentation of cancer area. A seven-levels gaussian pyramid representation of the histopathological image was built to provide the texture information in different scales. In this work, several neural architectures were compared using the original image level for the training procedure. The proposed method is based on U-Net applied to seven levels of various resolutions (pyramidal subsumpling). The predictions in different levels are combined through a voting mechanism. The final segmentation result is generated at the original image level. Partial color normalization and weighted overlapping method were applied in preprocessing and prediction separately. The results show the effectiveness of the proposed multi-scales approach achieving better scores compared to the state-of-the-art.      
### 56.Simulation Based Algorithms for Markov Decision Processes and Multi-Action Restless Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2007.12933.pdf)
>  We consider multi-dimensional Markov decision processes and formulate a long term discounted reward optimization problem. Two simulation based algorithms---Monte Carlo rollout policy and parallel rollout policy are studied, and various properties for these policies are discussed. We next consider a restless multi-armed bandit (RMAB) with multi-dimensional state space and multi-actions bandit model. A standard RMAB consists of two actions for each arms whereas in multi-actions RMAB, there are more that two actions for each arms. A popular approach for RMAB is Whittle index based heuristic policy. Indexability is an important requirement to use index based policy. Based on this, an RMAB is classified into indexable or non-indexable bandits. Our interest is in the study of Monte-Carlo rollout policy for both indexable and non-indexable restless bandits. We first analyze a standard indexable RMAB (two-action model) and discuss an index based policy approach. We present approximate index computation algorithm using Monte-Carlo rollout policy. This algorithm's convergence is shown using two-timescale stochastic approximation scheme. Later, we analyze multi-actions indexable RMAB, and discuss the index based policy approach. We also study non-indexable RMAB for both standard and multi-actions bandits using Monte-Carlo rollout policy.      
### 57.Non-parallel Emotion Conversion using a Deep-Generative Hybrid Network and an Adversarial Pair Discriminator  [ :arrow_down: ](https://arxiv.org/pdf/2007.12932.pdf)
>  We introduce a novel method for emotion conversion in speech that does not require parallel training data. Our approach loosely relies on a cycle-GAN schema to minimize the reconstruction error from converting back and forth between emotion pairs. However, unlike the conventional cycle-GAN, our discriminator classifies whether a pair of input real and generated samples corresponds to the desired emotion conversion (e.g., A to B) or to its inverse (B to A). We will show that this setup, which we refer to as a variational cycle-GAN (VC-GAN), is equivalent to minimizing the empirical KL divergence between the source features and their cyclic counterpart. In addition, our generator combines a trainable deep network with a fixed generative block to implement a smooth and invertible transformation on the input features, in our case, the fundamental frequency (F0) contour. This hybrid architecture regularizes our adversarial training procedure. We use crowd sourcing to evaluate both the emotional saliency and the quality of synthesized speech. Finally, we show that our model generalizes to new speakers by modifying speech produced by Wavenet.      
### 58.A Drone-Aided Blockchain-Based Smart Vehicular Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.12912.pdf)
>  The staggering growth of the number of vehicles worldwide has become a critical challenge resulting in tragic incidents, environment pollution, congestion, etc. Therefore, one of the promising approaches is to design a smart vehicular system as it is beneficial to drive safely. Present vehicular system lacks data reliability, security, and easy deployment. Motivated by these issues, this paper addresses a drone-enabled intelligent vehicular system, which is secure, easy to deploy and reliable in quality. Nevertheless, an increase in the number of operating drones in the communication networks makes them more vulnerable towards the cyber-attacks, which can completely sabotage the communication infrastructure. To tackle these problems, we propose a blockchain-based registration and authentication system for the entities such as drones, smart vehicles (SVs) and roadside units (RSUs). This paper is mainly focused on the blockchain-based secure system design and the optimal placement of drones to improve the spectral efficiency of the overall network. In particular, we investigate the association of RSUs with the drones by considering multiple communication-related factors such as available bandwidth, maximum number of links a drone can support, and backhaul limitations. We show that the proposed model can easily be overlaid on the current vehicular network reaping benefits of secure and reliable communications.      
### 59.3D Neural Network for Lung Cancer Risk Prediction on CT Volumes  [ :arrow_down: ](https://arxiv.org/pdf/2007.12898.pdf)
>  With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States. Lung cancer CT screening has been shown to reduce mortality by up to 40% and is now included in US screening guidelines. Reducing the high error rates in lung cancer screening is imperative because of the high clinical and financial costs caused by diagnosis mistakes. Despite the use of standards for radiological diagnosis, persistent inter-grader variability and incomplete characterization of comprehensive imaging findings remain as limitations of current methods. These limitations suggest opportunities for more sophisticated systems to improve performance and inter-reader consistency. In this report, we reproduce a state-of-the-art deep learning algorithm for lung cancer risk prediction. Our model predicts malignancy probability and risk bucket classification from lung CT studies. This allows for risk categorization of patients being screened and suggests the most appropriate surveillance and management. Combining our solution high accuracy, consistency and fully automated nature, our approach may enable highly efficient screening procedures and accelerate the adoption of lung cancer screening.      
### 60.Joint Design of Transmit Beamforming, IRS Platform, and Power Splitting SWIPT Receivers for Downlink Cellular Multiuser MISO  [ :arrow_down: ](https://arxiv.org/pdf/2007.12894.pdf)
>  A multiple antenna base station (BS) with an intelligent reflecting surface (IRS) platform, and several single antenna users are considered in the downlink mode. Simultaneous wireless information and power transfer (SWIPT) is utilized by the BS via transmit beamforming to convey information and power to all devices. Each device applies power splitting (PS) to dedicate separate parts of received power to information decoding and energy harvesting. We formulate a total transmit power minimization problem to jointly design the BS beamforming vectors, IRS phase shifts, and PS ratios at the receivers subject to minimum rate and harvested energy quality of service (QoS) constraints at all the receivers. First, we develop a block coordinate descent algorithm, also known as alternating optimization that can decrease the objective function with every iteration with guaranteed convergence. Afterwards, two low-complexity sub-optimal algorithms that rely on well-known maximum ratio transmission and zero-forcing beamforming techniques are introduced. These algorithms are beneficial when the number of BS antennas and/or number of users are large, or coherence times of channels are small. Simulations corroborate the expectation that by deploying a passive IRS, BS power can be reduced by $10-20$ dBw while maintaining similar guaranteed QoS. Furthermore, even the proposed sub-optimal algorithms outperform the globally optimal SWIPT solution without IRS for a modest number of IRS elements.      
### 61.MP3 Compression To Diminish Adversarial Noise in End-to-End Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.12892.pdf)
>  Audio Adversarial Examples (AAE) represent specially created inputs meant to trick Automatic Speech Recognition (ASR) systems into misclassification. The present work proposes MP3 compression as a means to decrease the impact of Adversarial Noise (AN) in audio samples transcribed by ASR systems. To this end, we generated AAEs with the Fast Gradient Sign Method for an end-to-end, hybrid CTC-attention ASR system. Our method is then validated by two objective indicators: (1) Character Error Rates (CER) that measure the speech decoding performance of four ASR models trained on uncompressed, as well as MP3-compressed data sets and (2) Signal-to-Noise Ratio (SNR) estimated for both uncompressed and MP3-compressed AAEs that are reconstructed in the time domain by feature inversion. We found that MP3 compression applied to AAEs indeed reduces the CER when compared to uncompressed AAEs. Moreover, feature-inverted (reconstructed) AAEs had significantly higher SNRs after MP3 compression, indicating that AN was reduced. In contrast to AN, MP3 compression applied to utterances augmented with regular noise resulted in more transcription errors, giving further evidence that MP3 encoding is effective in diminishing only AN.      
### 62.Preliminary Assessment of hands motor imagery in theta- and beta-bands for Brain-Machine-Interfaces using functional connectivity analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.12843.pdf)
>  The use of time- and frequency-based features has proven effective in the process of classifying mental tasks in Brain Computer Interfaces (BCIs). Still, most of those methods provide little insight about the underlying brain activity and functions. Thus, a better understanding of the mechanisms and dynamics of brain activity, is necessary in order to obtain useful and informative features for BCIs. In the present study, the objective is to investigate the differences in functional connectivity of two motor imagery tasks, through a partial directed coherence (PDC) analysis, which is a frequency-domain metric that provides information about directionality in the interaction between signals recorded at different channels. Four healthy subjects participated in this study, two mental tasks were evaluated: Imagination of the movement of the right hand or left hand. We carry out the differentiation of these tasks through two different approaches: on one hand, the traditional one based on spectral power; on the other hand, an approach based on PDC. The results showed that EEG-based PDC analysis provides additional information and it can potentially improve the feature selection mainly in the beta frequency band.      
### 63.All-Optical Information Processing Capacity of Diffractive Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2007.12813.pdf)
>  Precise engineering of materials and surfaces has been at the heart of some of the recent advances in optics and photonics. These advances around the engineering of materials with new functionalities have also opened up exciting avenues for designing trainable surfaces that can perform computation and machine learning tasks through light-matter interaction and diffraction. Here, we analyze the information processing capacity of coherent optical networks formed by diffractive surfaces that are trained to perform an all-optical computational task between a given input and output field-of-view. We prove that the dimensionality of the all-optical solution space covering the complex-valued transformations between the input and output fields-of-view is linearly proportional to the number of diffractive surfaces within the optical network, up to a limit that is dictated by the extent of the input and output fields-of-view. Deeper diffractive networks that are composed of larger numbers of trainable surfaces can cover a higher dimensional subspace of the complex-valued linear transformations between a larger input field-of-view and a larger output field-of-view, and exhibit depth advantages in terms of their statistical inference, learning and generalization capabilities for different image classification tasks, when compared with a single trainable diffractive surface. These analyses and conclusions are broadly applicable to various forms of diffractive surfaces, including e.g., plasmonic and/or dielectric-based metasurfaces and flat optics that can be used to form all-optical processors.      
### 64.Pilot Low-Cost Concentrating Solar Power Systems Deployment in Sub-Saharan Africa: A Case Study of Implementation Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2007.12785.pdf)
>  Electricity is one of the most crucial resources that drives any given nation's growth and development. The latest Sustainable Development Goals report indicates Africa still has a high deficit in electricity generation. Concentrating solar power seems to be a potential option to fill the deficit. That is because most of the components of concentrating solar power plants are readily available on the African market at affordable prices, and there are qualified local persons to build the plants. Pilot micro-concentrating solar power plants have been implemented in Sub-Saharan Africa and have shown promising results that could be expanded and leveraged for large-scale electricity generation. An assessment of a pilot concentrating solar power plant in the sub-region noticed one noteworthy obstacle that is the failure of the tracking system to reduce the operating energy cost of running the tracking control system and improve the multifaceted heliostat focusing behavior. This paper highlights the energy situation and the current development in concentrating solar power technology research in Africa. The paper also presents a comprehensive review of the state-of-the-art solar tracking systems for central receiver systems to illustrate the current direction of research regarding the design of low-cost tracking systems in terms of computational complexity, energy consumption, and heliostat alignment accuracy.      
### 65.Selection of Proper EEG Channels for Subject Intention Classification Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.12764.pdf)
>  Brain signals could be used to control devices to assist individuals with disabilities. Signals such as electroencephalograms are complicated and hard to interpret. A set of signals are collected and should be classified to identify the intention of the subject. Different approaches have tried to reduce the number of channels before sending them to a classifier. We are proposing a deep learning-based method for selecting an informative subset of channels that produce high classification accuracy. The proposed network could be trained for an individual subject for the selection of an appropriate set of channels. Reduction of the number of channels could reduce the complexity of brain-computer-interface devices. Our method could find a subset of channels. The accuracy of our approach is comparable with a model trained on all channels. Hence, our model's temporal and power costs are low, while its accuracy is kept high.      
### 66.Noisy Agents: Self-supervised Exploration by Predicting Auditory Events  [ :arrow_down: ](https://arxiv.org/pdf/2007.13729.pdf)
>  Humans integrate multiple sensory modalities (e.g. visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. Experimental results on Atari games show that our new intrinsic motivation significantly outperforms several state-of-the-art baselines. We further visualize our noisy agents' behavior in a physics environment and demonstrate that our newly designed intrinsic reward leads to the emergence of physical interaction behaviors (e.g. contact with objects).      
### 67.FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers &amp; Goods Transportation  [ :arrow_down: ](https://arxiv.org/pdf/2007.13699.pdf)
>  The growth in online goods delivery is causing a dramatic surge in urban vehicle traffic from last-mile deliveries. On the other hand, ride-sharing has been on the rise with the success of ride-sharing platforms and increased research on using autonomous vehicle technologies for routing and matching. The future of urban mobility for passengers and goods relies on leveraging new methods that minimize operational costs and environmental footprints of transportation systems. <br>This paper considers combining passenger transportation with goods delivery to improve vehicle-based transportation. Even though the problem has been studied with a defined dynamics model of the transportation system environment, this paper considers a model-free approach that has been demonstrated to be adaptable to new or erratic environment dynamics. We propose FlexPool, a distributed model-free deep reinforcement learning algorithm that jointly serves passengers &amp; goods workloads by learning optimal dispatch policies from its interaction with the environment. The proposed algorithm pools passengers for a ride-sharing service and delivers goods using a multi-hop transit method. These flexibilities decrease the fleet's operational cost and environmental footprint while maintaining service levels for passengers and goods. Through simulations on a realistic multi-agent urban mobility platform, we demonstrate that FlexPool outperforms other model-free settings in serving the demands from passengers &amp; goods. FlexPool achieves 30% higher fleet utilization and 35% higher fuel efficiency in comparison to (i) model-free approaches where vehicles transport a combination of passengers &amp; goods without the use of multi-hop transit, and (ii) model-free approaches where vehicles exclusively transport either passengers or goods.      
### 68.Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser  [ :arrow_down: ](https://arxiv.org/pdf/2007.13640.pdf)
>  Prior probability models are a central component of many image processing problems, but density estimation is notoriously difficult for high-dimensional signals such as photographic images. Deep neural networks have provided state-of-the-art solutions for problems such as denoising, which implicitly rely on a prior probability model of natural images. Here, we develop a robust and general methodology for making use of this implicit prior. We rely on a little-known statistical result due to Miyasawa (1961), who showed that the least-squares solution for removing additive Gaussian noise can be written directly in terms of the gradient of the log of the noisy signal density. We use this fact to develop a stochastic coarse-to-fine gradient ascent procedure for drawing high-probability samples from the implicit prior embedded within a CNN trained to perform blind (i.e., unknown noise level) least-squares denoising. A generalization of this algorithm to constrained sampling provides a method for using the implicit prior to solve any linear inverse problem, with no additional training. We demonstrate this general form of transfer learning in multiple applications, using the same algorithm to produce high-quality solutions for deblurring, super-resolution, inpainting, and compressive sensing.      
### 69.Memory-Latency-Accuracy Trade-offs for Continual Learning on a RISC-V Extreme-Edge Node  [ :arrow_down: ](https://arxiv.org/pdf/2007.13631.pdf)
>  AI-powered edge devices currently lack the ability to adapt their embedded inference models to the ever-changing environment. To tackle this issue, Continual Learning (CL) strategies aim at incrementally improving the decision capabilities based on newly acquired data. In this work, after quantifying memory and computational requirements of CL algorithms, we define a novel HW/SW extreme-edge platform featuring a low power RISC-V octa-core cluster tailored for on-demand incremental learning over locally sensed data. The presented multi-core HW/SW architecture achieves a peak performance of 2.21 and 1.70 MAC/cycle, respectively, when running forward and backward steps of the gradient descent. We report the trade-off between memory footprint, latency, and accuracy for learning a new class with Latent Replay CL when targeting an image classification task on the CORe50 dataset. For a CL setting that retrains all the layers, taking 5h to learn a new class and achieving up to 77.3% of precision, a more efficient solution retrains only part of the network, reaching an accuracy of 72.5% with a memory requirement of 300 MB and a computation latency of 1.5 hours. On the other side, retraining only the last layer results in the fastest (867 ms) and less memory hungry (20 MB) solution but scoring 58% on the CORe50 dataset. Thanks to the parallelism of the low-power cluster engine, our HW/SW platform results 25x faster than typical MCU device, on which CL is still impractical, and demonstrates an 11x gain in terms of energy consumption with respect to mobile-class solutions.      
### 70.3D diffractive imaging of nanoparticle ensembles using an X-ray laser  [ :arrow_down: ](https://arxiv.org/pdf/2007.13597.pdf)
>  We report the 3D structure determination of gold nanoparticles (AuNPs) by X-ray single particle imaging (SPI). Around 10 million diffraction patterns from gold nanoparticles were measured in less than 100 hours of beam time, more than 100 times the amount of data in any single prior SPI experiment, using the new capabilities of the European X-ray free electron laser which allow measurements of 1500 frames per second. A classification and structural sorting method was developed to disentangle the heterogeneity of the particles and to obtain a resolution of better than 3 nm. With these new experimental and analytical developments, we have entered a new era for the SPI method and the path towards close-to-atomic resolution imaging of biomolecules is apparent.      
### 71.MADGAN: unsupervised Medical Anomaly Detection GAN using multiple adjacent brain MRI slice reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2007.13559.pdf)
>  Unsupervised learning can discover various unseen diseases, relying on large-scale unannotated medical images of healthy subjects. Towards this, unsupervised methods reconstruct a 2D/3D single medical image to detect outliers either in the learned feature space or from high reconstruction loss. However, without considering continuity between multiple adjacent slices, they cannot directly discriminate diseases composed of the accumulation of subtle anatomical anomalies, such as Alzheimer's Disease (AD). Moreover, no study has shown how unsupervised anomaly detection is associated with either disease stages, various (i.e., more than two types of) diseases, or multi-sequence Magnetic Resonance Imaging (MRI) scans. Therefore, we propose unsupervised Medical Anomaly Detection Generative Adversarial Network (MADGAN), a novel two-step method using GAN-based multiple adjacent brain MRI slice reconstruction to detect various diseases at different stages on multi-sequence structural MRI: (Reconstruction) Wasserstein loss with Gradient Penalty + 100 L1 loss-trained on 3 healthy brain axial MRI slices to reconstruct the next 3 ones-reconstructs unseen healthy/abnormal scans; (Diagnosis) Average L2 loss per scan discriminates them, comparing the ground truth/reconstructed slices. For training, we use 1,133 healthy T1-weighted (T1) and 135 healthy contrast-enhanced T1 (T1c) brain MRI scans. Our Self-Attention MADGAN can detect AD on T1 scans at a very early stage, Mild Cognitive Impairment (MCI), with Area Under the Curve (AUC) 0.727, and AD at a late stage with AUC 0.894, while detecting brain metastases on T1c scans with AUC 0.921.      
### 72.A Novel adaptive optimization of Dual-Tree Complex Wavelet Transform for Medical Image Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2007.13538.pdf)
>  In recent years, many research achievements are made in the medical image fusion field. Fusion is basically extraction of best of inputs and conveying it to the output. Medical Image fusion means that several of various modality image information is comprehended together to form one image to express its information. The aim of image fusion is to integrate complementary and redundant information. In this paper, a multimodal image fusion algorithm based on the dual-tree complex wavelet transform (DT-CWT) and adaptive particle swarm optimization (APSO) is proposed. Fusion is achieved through the formation of a fused pyramid using the DTCWT coefficients from the decomposed pyramids of the source images. The coefficients are fused by the weighted average method based on pixels, and the weights are estimated by the APSO to gain optimal fused images. The fused image is obtained through conventional inverse dual-tree complex wavelet transform reconstruction process. Experiment results show that the proposed method based on adaptive particle swarm optimization algorithm is remarkably better than the method based on particle swarm optimization. The resulting fused images are compared visually and through benchmarks such as Entropy (E), Peak Signal to Noise Ratio, (PSNR), Root Mean Square Error (RMSE), Standard deviation (SD) and Structure Similarity Index Metric (SSIM) computations.      
### 73.Deep Multi-Task Learning for Cooperative NOMA: System Design and Principles  [ :arrow_down: ](https://arxiv.org/pdf/2007.13495.pdf)
>  Envisioned as a promising component of the future wireless Internet-of-Things (IoT) networks, the non-orthogonal multiple access (NOMA) technique can support massive connectivity with a significantly increased spectral efficiency. Cooperative NOMA is able to further improve the communication reliability of users under poor channel conditions. However, the conventional system design suffers from several inherent limitations and is not optimized from the bit error rate (BER) perspective. In this paper, we develop a novel deep cooperative NOMA scheme, drawing upon the recent advances in deep learning (DL). We develop a novel hybrid-cascaded deep neural network (DNN) architecture such that the entire system can be optimized in a holistic manner. On this basis, we construct multiple loss functions to quantify the BER performance and propose a novel multi-task oriented two-stage training method to solve the end-to-end training problem in a self-supervised manner. The learning mechanism of each DNN module is then analyzed based on information theory, offering insights into the proposed DNN architecture and its corresponding training method. We also adapt the proposed scheme to handle the power allocation (PA) mismatch between training and inference and incorporate it with channel coding to combat signal deterioration. Simulation results verify its advantages over orthogonal multiple access (OMA) and the conventional cooperative NOMA scheme in various scenarios.      
### 74.Self-Supervised Encoder for Fault Prediction in Electrochemical Cells  [ :arrow_down: ](https://arxiv.org/pdf/2007.13492.pdf)
>  Predicting faults before they occur helps to avoid potential safety hazards. Furthermore, planning the required maintenance actions in advance reduces operation costs. In this article, the focus is on electrochemical cells. In order to predict a cell's fault, the typical approach is to estimate the expected voltage that a healthy cell would present and compare it with the cell's measured voltage in real-time. This approach is possible because, when a fault is about to happen, the cell's measured voltage differs from the one expected for the same operating conditions. However, estimating the expected voltage is challenging, as the voltage of a healthy cell is also affected by its degradation -- an unknown parameter. Expert-defined parametric models are currently used for this estimation task. Instead, we propose the use of a neural network model based on an encoder-decoder architecture. The network receives the operating conditions as input. The encoder's task is to find a faithful representation of the cell's degradation and to pass it to the decoder, which in turn predicts the expected cell's voltage. As no labeled degradation data is given to the network, we consider our approach to be a self-supervised encoder. Results show that we were able to predict the voltage of multiple cells while diminishing the prediction error that was obtained by the parametric models by 53%. This improvement enabled our network to predict a fault 31 hours before it happened, a 64% increase in reaction time compared to the parametric model. Moreover, the output of the encoder can be plotted, adding interpretability to the neural network model.      
### 75.Automatic Parking in Smart Cities  [ :arrow_down: ](https://arxiv.org/pdf/2007.13491.pdf)
>  The objective behind this project is to maximize the efficiency of land space, to decrease the driver stress and frustration, along with a considerable reduction in air pollution. Our contribution is in the form of an automatic parking system that is controlled by cellular phones. The structure is a hexagon shape that uses conveyor belts, to transport the vehicles from the entrance into the parking spaces over an elevating platform. The entrance gate includes length-measuring sensors to determine whether the approaching vehicle is eligible to enter. Our system is controlled through a microcontroller, and using cellular communications to connect to the customer. The project can be applied to different locations and is capable of capacity extensions.      
### 76.Detection and Performance Analysis for Non-Coherent DF Relay Networks with Optimized Generalized Differential Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2007.13452.pdf)
>  This paper studies the detection and performance analysis problems for a relay network with $N$ parallel decode-and-forward (DF) relays. Due to the distributed nature of this network, it is practically very challenging to fulfill the requirement of instantaneous channel state information for coherent detection. To bypass this requirement, we consider the use of non-coherent DF relaying based on a generalized differential modulation (GDM) scheme, in which transmission power allocation over the $M$-ary phase shift keying symbols is exploited when performing differential encoding. In this paper, a novel detector at the destination of such a non-coherent DF relay network is proposed. It is an accurate approximation of the state-of-the-art detector, called the almost maximum likelihood detector (AMLD), but the detection complexity is considerably reduced from $\mathcal{O}(M^2N)$ to $\mathcal{O}(MN)$. By characterizing the dominant error terms, we derive an accurate approximate symbol error rate (SER) expression. An optimized power allocation scheme for GDM is further designed based on this SER expression. Our simulation demonstrates that the proposed non-coherent scheme can perform close to the coherent counterpart as the block length increases. Additionally, we prove that the diversity order of both the proposed detector and the AMLD is exactly $\lceil N/2 \rceil + 1$. Extensive simulation results further verify the accuracy of our results in various scenarios.      
### 77.Contact-Free Biosignal Acquisition via Capacitive and Ultrasonic Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2007.13439.pdf)
>  Contact-free detection of human vital signs like heart rate and respiration rate will improve the patients' comfort and enables long-term monitoring of newborns or bedridden patients. For that, reliable and safe measurement techniques are indispensable. The aim of this work is the development and comparison of novel ultrasonic and capacitive measurement setups, sharing a common hardware platform. Both measurement techniques that are implemented and compared are based on the detection of minor chest wall vibrations in millimeter ranges, due to geometrical thorax changes during respiration and heartbeat activities. After examining the physical measurement conditions and simulating the capacitive sensor, a problem-specific measurement setup is proposed. The system is characterized to be capable of detecting distance changes below 2 {\mu}m via the ultrasonic sensor and below 800 {\mu}m via the capacitive sensor. First subject measurements show that the detection of heart activities is possible under ideal conditions and exclusively with the proposed ultrasonic approach. However, the capacitive sensor works reliably for respiration monitoring, even when the subject is fully-clothed and covered with a blanket. The chosen ultrasonic approach is sensitive regarding minor changes of the reflecting surface and therefore has high uncertainty. In contrast, capacitive respiration detection is very reliable. It is conceivable that improvements in the capacitive sensor circuitry will also enable the detection of heart activities. The proposed ultrasonic approach presents current problems of this technique. In contrast to that, the unusual approach of capacitive sensing demonstrates a high potential regarding vital signs acquisition.      
### 78.Image-driven discriminative and generative machine learning algorithms for establishing microstructure-processing relationships  [ :arrow_down: ](https://arxiv.org/pdf/2007.13417.pdf)
>  We investigate methods of microstructure representation for the purpose of predicting processing condition from microstructure image data. A binary alloy (uranium-molybdenum) that is currently under development as a nuclear fuel was studied for the purpose of developing an improved machine learning approach to image recognition, characterization, and building predictive capabilities linking microstructure to processing conditions. Here, we test different microstructure representations and evaluate model performance based on the F1 score. A F1 score of 95.1% was achieved for distinguishing between micrographs corresponding to ten different thermo-mechanical material processing conditions. We find that our newly developed microstructure representation describes image data well, and the traditional approach of utilizing area fractions of different phases is insufficient for distinguishing between multiple classes using a relatively small, imbalanced original data set of 272 images. To explore the applicability of generative methods for supplementing such limited data sets, generative adversarial networks were trained to generate artificial microstructure images. Two different generative networks were trained and tested to assess performance. Challenges and best practices associated with applying machine learning to limited microstructure image data sets is also discussed. Our work has implications for quantitative microstructure analysis, and development of microstructure-processing relationships in limited data sets typical of metallurgical process design studies.      
### 79.Differentiable model-based adaptive optics with transmitted and reflected light  [ :arrow_down: ](https://arxiv.org/pdf/2007.13400.pdf)
>  Aberrations limit optical systems in many situations, for example when imaging in biological tissue. Machine learning offers novel ways to improve imaging under such conditions by learning inverse models of aberrations. Learning requires datasets that cover a wide range of possible aberrations, which however becomes limiting for more strongly scattering samples, and does not take advantage of prior information about the imaging process. Here, we show that combining model-based adaptive optics with the optimization techniques of machine learning frameworks can find aberration corrections with a small number of measurements. Corrections are determined in a transmission configuration through a single aberrating layer and in a reflection configuration through two different layers at the same time. Additionally, corrections are not limited by a predetermined model of aberrations (such as combinations of Zernike modes). Focusing in transmission can be achieved based only on reflected light, compatible with an epidetection imaging configuration.      
### 80.Split Computing for Complex Object Detectors: Challenges and Preliminary Results  [ :arrow_down: ](https://arxiv.org/pdf/2007.13312.pdf)
>  Following the trends of mobile and edge computing for DNN models, an intermediate option, split computing, has been attracting attentions from the research community. Previous studies empirically showed that while mobile and edge computing often would be the best options in terms of total inference time, there are some scenarios where split computing methods can achieve shorter inference time. All the proposed split computing approaches, however, focus on image classification tasks, and most are assessed with small datasets that are far from the practical scenarios. In this paper, we discuss the challenges in developing split computing methods for powerful R-CNN object detectors trained on a large dataset, COCO 2017. We extensively analyze the object detectors in terms of layer-wise tensor size and model size, and show that naive split computing methods would not reduce inference time. To the best of our knowledge, this is the first study to inject small bottlenecks to such object detectors and unveil the potential of a split computing approach. The source code and trained models' weights used in this study are available at <a class="link-external link-https" href="https://github.com/yoshitomo-matsubara/hnd-ghnd-object-detectors" rel="external noopener nofollow">this https URL</a> .      
### 81.Point-to-set distance functions for weakly supervised segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.13251.pdf)
>  When pixel-level masks or partial annotations are not available for training neural networks for semantic segmentation, it is possible to use higher-level information in the form of bounding boxes, or image tags. In the imaging sciences, many applications do not have an object-background structure and bounding boxes are not available. Any available annotation typically comes from ground truth or domain experts. A direct way to train without masks is using prior knowledge on the size of objects/classes in the segmentation. We present a new algorithm to include such information via constraints on the network output, implemented via projection-based point-to-set distance functions. This type of distance functions always has the same functional form of the derivative, and avoids the need to adapt penalty functions to different constraints, as well as issues related to constraining properties typically associated with non-differentiable functions. Whereas object size information is known to enable object segmentation from bounding boxes from datasets with many general and medical images, we show that the applications extend to the imaging sciences where data represents indirect measurements, even in the case of single examples. We illustrate the capabilities in case of a) one or more classes do not have any annotation; b) there is no annotation at all; c) there are bounding boxes. We use data for hyperspectral time-lapse imaging, object segmentation in corrupted images, and sub-surface aquifer mapping from airborne-geophysical remote-sensing data. The examples verify that the developed methodology alleviates difficulties with annotating non-visual imagery for a range of experimental settings.      
### 82.Deep Active Learning for Solvability Prediction in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.13250.pdf)
>  Traditional methods for solvability region analysis can only have inner approximations with inconclusive conservatism. Machine learning methods have been proposed to approach the real region. In this letter, we propose a deep active learning framework for power system solvability prediction. Compared with the passive learning methods where the training is performed after all instances are labeled, the active learning selects most informative instances to be label and therefore significantly reduce the size of labeled dataset for training. In the active learning framework, the acquisition functions, which correspond to different sampling strategies, are defined in terms of the on-the-fly posterior probability from the classifier. The IEEE 39-bus system is employed to validate the proposed framework, where a two-dimensional case is illustrated to visualize the effectiveness of the sampling method followed by the full-dimensional numerical experiments.      
### 83.Going Beyond the Debye Length: Overcoming Charge Screening Limitations in Next-Generation Bioelectronic Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2007.13201.pdf)
>  Electronic biosensors are a natural fit for field-deployable diagnostic devices, because they can be miniaturized, mass produced, and integrated with circuitry. Unfortunately, progress in the development of such platforms has been hindered by the fact that mobile ions present in biological samples screen charges from the target molecule, greatly reducing sensor sensitivity. Under physiological conditions, the thickness of the resulting electric double layer is less than 1 nm, and it has generally been assumed that electronic detection beyond this distance is virtually impossible. However, a few recently-described sensor design strategies seem to defy this conventional wisdom, exploiting the physics of electrical double layers in ways that traditional models do not capture. In the first strategy, charge screening is decreased by constraining the space in which double layers can form. The second strategy uses external stimuli to prevent double layers from reaching equilibrium, thereby effectively reducing charge screening. The goal of this article is to describe these relatively new concepts, and to offer theoretical insights into mechanisms that may enable electronic biosensing beyond the double-layer. If these concepts can be further developed and translated into practical electronic biosensors, we foresee exciting opportunities for the next generation of diagnostic technologies.      
### 84.Tag2Risk: Harnessing Social Music Tags for Characterizing Depression Risk  [ :arrow_down: ](https://arxiv.org/pdf/2007.13159.pdf)
>  Musical preferences have been considered a mirror of the self. In this age of Big Data, online music streaming services allow us to capture ecologically valid music listening behavior and provide a rich source of information to identify several user-specific aspects. Studies have shown musical engagement to be an indirect representation of internal states including internalized symptomatology and depression. The current study aims at unearthing patterns and trends in the individuals at risk for depression as it manifests in naturally occurring music listening behavior. Mental well-being scores, musical engagement measures, and listening histories of <a class="link-external link-http" href="http://Last.fm" rel="external noopener nofollow">this http URL</a> users (N=541) were acquired. Social tags associated with each listener's most popular tracks were analyzed to unearth the mood/emotions and genres associated with the users. Results revealed that social tags prevalent in the users at risk for depression were predominantly related to emotions depicting Sadness associated with genre tags representing neo-psychedelic-, avant garde-, dream-pop. This study will open up avenues for an MIR-based approach to characterizing and predicting risk for depression which can be helpful in early detection and additionally provide bases for designing music recommendations accordingly.      
### 85.Hover or Perch: Comparing Capacity of Airborne and Landed Millimeter-Wave UAV Cells  [ :arrow_down: ](https://arxiv.org/pdf/2007.13149.pdf)
>  On-demand deployments of millimeter-wave (mmWave) access points (APs) carried by unmanned aerial vehicles (UAVs) are considered today as a potential solution to enhance the performance of 5G+ networks. The battery lifetime of modern UAVs, though, limits the flight times in such systems. In this letter, we evaluate a feasible deployment alternative for temporary capacity boost in the areas with highly fluctuating user demands. The approach is to land UAV-based mmWave APs on the nearby buildings instead of hovering over the area. Within the developed mathematical framework, we compare the system-level performance of airborne and landed deployments by taking into account the full operation cycle of the employed drones. Our numerical results demonstrate that the choice of the UAV deployment option is determined by an interplay of the separation distance between the service area and the UAV charging station, drone battery lifetime, and the number of aerial APs in use. The presented methodology and results can support efficient on-demand deployments of UAV-based mmWave APs in prospective 5G+ networks.      
### 86.Virtual Multi-view Fusion for 3D Semantic Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.13138.pdf)
>  Semantic segmentation of 3D meshes is an important problem for 3D scene understanding. In this paper we revisit the classic multiview representation of 3D meshes and study several techniques that make them effective for 3D semantic segmentation of meshes. Given a 3D mesh reconstructed from RGBD sensors, our method effectively chooses different virtual views of the 3D mesh and renders multiple 2D channels for training an effective 2D semantic segmentation model. Features from multiple per view predictions are finally fused on 3D mesh vertices to predict mesh semantic segmentation labels. Using the large scale indoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual views enable more effective training of 2D semantic segmentation networks than previous multiview approaches. When the 2D per pixel predictions are aggregated on 3D surfaces, our virtual multiview fusion method is able to achieve significantly better 3D semantic segmentation results compared to all prior multiview approaches and competitive with recent 3D convolution approaches.      
### 87.Contrastive Visual-Linguistic Pretraining  [ :arrow_down: ](https://arxiv.org/pdf/2007.13135.pdf)
>  Several multi-modality representation learning approaches such as LXMERT and ViLBERT have been proposed recently. Such approaches can achieve superior performance due to the high-level semantic information captured during large-scale multimodal pretraining. However, as ViLBERT and LXMERT adopt visual region regression and classification loss, they often suffer from domain gap and noisy label problems, based on the visual features having been pretrained on the Visual Genome dataset. To overcome these issues, we propose unbiased Contrastive Visual-Linguistic Pretraining (CVLP), which constructs a visual self-supervised loss built upon contrastive learning. We evaluate CVLP on several down-stream tasks, including VQA, GQA and NLVR2 to validate the superiority of contrastive learning on multi-modality representation learning. Our code is available at: <a class="link-external link-https" href="https://github.com/ArcherYunDong/CVLP-" rel="external noopener nofollow">this https URL</a>.      
### 88.Performance Evaluation of Scheduling in 5G-mmWave Networks under Human Blockage  [ :arrow_down: ](https://arxiv.org/pdf/2007.13112.pdf)
>  The millimetre-wave spectrum provisions enormous enhancement to the achievable data rate of 5G networks. However, human blockages affecting the millimetre-wave signal can severely degrade the performance if proper resource allocation is not considered. In this paper, we assess how conventional schedulers, such as the Proportional Fair scheduler, react to the presence of blockage. Our results show that the resource allocation may disfavour users suffering from blockage, leading to low data rate for those users. To circumvent this problem, we show that the data rate of those users can be improved by using a scheduler adapted to react to upcoming blockage events. The adapted scheduler aims at proactively allocating the resources before a blockage happens, mitigating losses. Such adaptation is motivated by recent progress in blockage prediction for millimetre-wave signals in a dynamic human blockage scenario. Our extensive simulations indicate gains in the 1st percentile rate and fairness with respect to Proportional Fair scheduler when blockage conditions are severe.      
### 89.Geometry and control of the nonholonomic integrator: An electrodynamics analogy  [ :arrow_down: ](https://arxiv.org/pdf/2007.13074.pdf)
>  We consider some generalizations of the classical nonholonomic integrator and give a geometric approach to characterize controllability for these systems. We use Stokes' theorem and results from complex analysis to obtain necessary and sufficient conditions for controllability. Furthermore, we show that optimal trajectories of certain minimum energy optimal control problems defined on these systems can be identified with the trajectory of a charged particle in an electromagnetic field.      
### 90.A Preliminary Exploration into an Alternative CellLineNet: An Evolutionary Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.13044.pdf)
>  Within this paper, the exploration of an evolutionary approach to an alternative CellLineNet: a convolutional neural network adept at the classification of epithelial breast cancer cell lines, is presented. This evolutionary algorithm introduces control variables that guide the search of architectures in the search space of inverted residual blocks, bottleneck blocks, residual blocks and a basic 2x2 convolutional block. The promise of EvoCELL is predicting what combination or arrangement of the feature extracting blocks that produce the best model architecture for a given task. Therein, the performance of how the fittest model evolved after each generation is shown. The final evolved model CellLineNet V2 classifies 5 types of epithelial breast cell lines consisting of two human cancer lines, 2 normal immortalized lines, and 1 immortalized mouse line (MDA-MB-468, MCF7, 10A, 12A and HC11). The Multiclass Cell Line Classification Convolutional Neural Network extends our earlier work on a Binary Breast Cancer Cell Line Classification model. This paper presents an on-going exploratory approach to neural network architecture design and is presented for further study.      
### 91.Style is a Distribution of Features  [ :arrow_down: ](https://arxiv.org/pdf/2007.13010.pdf)
>  Neural style transfer (NST) is a powerful image generation technique that uses a convolutional neural network (CNN) to merge the content of one image with the style of another. Contemporary methods of NST use first or second order statistics of the CNN's features to achieve transfers with relatively little computational cost. However, these methods cannot fully extract the style from the CNN's features. We present a new algorithm for style transfer that fully extracts the style from the features by redefining the style loss as the Wasserstein distance between the distribution of features. Thus, we set a new standard in style transfer quality. In addition, we state two important interpretations of NST. The first is a re-emphasis from Li et al., which states that style is simply the distribution of features. The second states that NST is a type of generative adversarial network (GAN) problem.      
### 92.HATNet: An End-to-End Holistic Attention Network for Diagnosis of Breast Biopsy Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.13007.pdf)
>  Training end-to-end networks for classifying gigapixel size histopathological images is computationally intractable. Most approaches are patch-based and first learn local representations (patch-wise) before combining these local representations to produce image-level decisions. However, dividing large tissue structures into patches limits the context available to these networks, which may reduce their ability to learn representations from clinically relevant structures. In this paper, we introduce a novel attention-based network, the Holistic ATtention Network (HATNet) to classify breast biopsy images. We streamline the histopathological image classification pipeline and show how to learn representations from gigapixel size images end-to-end. HATNet extends the bag-of-words approach and uses self-attention to encode global information, allowing it to learn representations from clinically relevant tissue structures without any explicit supervision. It outperforms the previous best network Y-Net, which uses supervision in the form of tissue-level segmentation masks, by 8%. Importantly, our analysis reveals that HATNet learns representations from clinically relevant structures, and it matches the classification accuracy of human pathologists for this challenging test set. Our source code is available at \url{<a class="link-external link-https" href="https://github.com/sacmehta/HATNet" rel="external noopener nofollow">this https URL</a>}      
### 93.Object Handovers: a Review for Robotics  [ :arrow_down: ](https://arxiv.org/pdf/2007.12952.pdf)
>  This article surveys the literature on human-robot object handovers. A handover is a collaborative joint action where an agent, the giver, gives an object to another agent, the receiver. The physical exchange starts when the receiver first contacts the object held by the giver and ends when the giver fully releases the object to the receiver. However, important cognitive and physical processes begin before the physical exchange, including initiating implicit agreement with respect to the location and timing of the exchange. From this perspective, we structure our review into the two main phases delimited by the aforementioned events: 1) a pre-handover phase, and 2) the physical exchange. We focus our analysis on the two actors (giver and receiver) and report the state of the art of robotic givers (robot-to-human handovers) and the robotic receivers (human-to-robot handovers). We report a comprehensive list of qualitative and quantitative metrics commonly used to assess the interaction. While focusing our review on the cognitive level (e.g., prediction, perception, motion planning, learning) and the physical level (e.g., motion, grasping, grip release) of the handover, we briefly discuss also the concepts of safety, social context, and ergonomics. We compare the behaviours displayed during human-to-human handovers to the state of the art of robotic assistants, and identify the major areas of improvement for robotic assistants to reach performance comparable to human interactions. Finally, we propose a minimal set of metrics that should be used in order to enable a fair comparison among the approaches.      
### 94.Video Super Resolution Based on Deep Learning: A comprehensive survey  [ :arrow_down: ](https://arxiv.org/pdf/2007.12928.pdf)
>  In recent years, deep learning has made great progress in the fields of image recognition, video analysis, natural language processing and speech recognition, including video super-resolution tasks. In this survey, we comprehensively investigate 28 state-of-the-art video super-resolution methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Hence we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details (including input and output, loss function and learning rate) of all the methods are depicted in details. Finally, we summarize and compare their performance on some benchmark datasets under different magnification factors. We also discuss some challenges, which need to be further addressed by researchers in the community of video super-resolution. Therefore, this work is expected to make a contribution to the future development of research in video super-resolution, and alleviate understandability and transferability of existing and future techniques into practice.      
### 95.CNN Detection of GAN-Generated Face Images based on Cross-Band Co-occurrences Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.12909.pdf)
>  Last-generation GAN models allow to generate synthetic images which are visually indistinguishable from natural ones, raising the need to develop tools to distinguish fake and natural images thus contributing to preserve the trustworthiness of digital images. While modern GAN models can generate very high-quality images with no visible spatial artifacts, reconstruction of consistent relationships among colour channels is expectedly more difficult. In this paper, we propose a method for distinguishing GAN-generated from natural images by exploiting inconsistencies among spectral bands, with specific focus on the generation of synthetic face images. Specifically, we use cross-band co-occurrence matrices, in addition to spatial co-occurrence matrices, as input to a CNN model, which is trained to distinguish between real and synthetic faces. The results of our experiments confirm the goodness of our approach which outperforms a similar detection technique based on intra-band spatial co-occurrences only. The performance gain is particularly significant with regard to robustness against post-processing, like geometric transformations, filtering and contrast manipulations.      
### 96.Robust Front-End for Multi-Channel ASR using Flow-Based Density Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2007.12903.pdf)
>  For multi-channel speech recognition, speech enhancement techniques such as denoising or dereverberation are conventionally applied as a front-end processor. Deep learning-based front-ends using such techniques require aligned clean and noisy speech pairs which are generally obtained via data simulation. Recently, several joint optimization techniques have been proposed to train the front-end without parallel data within an end-to-end automatic speech recognition (ASR) scheme. However, the ASR objective is sub-optimal and insufficient for fully training the front-end, which still leaves room for improvement. In this paper, we propose a novel approach which incorporates flow-based density estimation for the robust front-end using non-parallel clean and noisy speech. Experimental results on the CHiME-4 dataset show that the proposed method outperforms the conventional techniques where the front-end is trained only with ASR objective.      
### 97.Unmanned Aerial Vehicles in Smart Agriculture: Applications, Requirements and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2007.12874.pdf)
>  In the next few years, smart farming will reach each and every nook of the world. The prospects of using unmanned aerial vehicles (UAV) for smart farming are immense. However, the cost and the ease in controlling UAVs for smart farming might play an important role for motivating farmers to use UAVs in farming. Mostly, UAVs are controlled by remote controllers using radio waves. There are several technologies such as WiFi or ZigBee that are also used for controlling UAVs. However, Smart Bluetooth (also referred to as Bluetooth Low Energy) is a wireless technology used to transfer data over short distances. Bluetooth smart is cheaper than other technologies and has the advantage of being available on every smart phone. Farmers can use any smart phone to operate their respective UAVs along with Bluetooth Smart enabled agricultural sensors in the future. However, certain requirements and challenges need to be addressed before UAVs can be operated for smart agriculture-related applications. Hence, in this article, an attempt has been made to explore the types of sensors suitable for smart farming, potential requirements and challenges for operating UAVs in smart agriculture. We have also identified the future applications of using UAVs in smart farming.      
### 98.DD-CNN: Depthwise Disout Convolutional Neural Network for Low-complexity Acoustic Scene Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.12864.pdf)
>  This paper presents a Depthwise Disout Convolutional Neural Network (DD-CNN) for the detection and classification of urban acoustic scenes. Specifically, we use log-mel as feature representations of acoustic signals for the inputs of our network. In the proposed DD-CNN, depthwise separable convolution is used to reduce the network complexity. Besides, SpecAugment and Disout are used for further performance boosting. Experimental results demonstrate that our DD-CNN can learn discriminative acoustic characteristics from audio fragments and effectively reduce the network complexity. Our DD-CNN was used for the low-complexity acoustic scene classification task of the DCASE2020 Challenge, which achieves 92.04% accuracy on the validation set.      
### 99.Physical Layer Security of Large Reflecting Surface Aided Communications with Phase Errors  [ :arrow_down: ](https://arxiv.org/pdf/2007.12859.pdf)
>  The physical layer security (PLS) performance of a wireless communication link through a large reflecting surface (LRS) with phase errors is analyzed. Leveraging recent results that express the \ac{LRS}-based composite channel as an equivalent scalar fading channel, we show that the eavesdropper's link is Rayleigh distributed and independent of the legitimate link. The different scaling laws of the legitimate and eavesdroppers signal-to-noise ratios with the number of reflecting elements, and the reasonably good performance even in the case of coarse phase quantization, show the great potential of LRS-aided communications to enhance PLS in practical wireless set-ups.      
### 100.Detecting Dynamic States of Temporal Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.12756.pdf)
>  Many temporal networks exhibit multiple system states, such as weekday and weekend patterns in social contact networks. The detection of such distinct states in temporal network data has recently been studied as it helps reveal underlying dynamical processes. A commonly used method is network aggregation over a certain time window, which aggregates a subsequence of multiple network snapshots into one static network. This method, however, necessarily discards temporal dynamics within the time window. Here we propose a new method for detecting dynamic states in temporal networks using connection series (i.e., time series of connection status) between nodes. Our method consists of the construction of connection series tensors over nonoverlapping time windows, similarity measurement between these tensors, and community detection in the similarity network of those time windows. Experiments with empirical temporal network data demonstrated that our method outperformed the conventional approach using simple network aggregation in revealing interpretable system states effectively. In addition, our method allows users to analyze hierarchical temporal structures and to uncover dynamic states at different spatial/temporal resolutions.      
### 101.Low Earth Orbit Satellites Provide Continuous Enterprise Data Connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2007.12748.pdf)
>  A critical problem in global telecommunication is the drastic increase in data volume transmitted and received across the world. To address the need for scalable telecommunication solutions in light of growing data volume, the BMW Group and Low Earth Orbit (LEO) satellite provider OneWeb pursue a proof of concept demo that assesses the potential of LEO networks for enterprise connectivity. Our results suggest that LEO satellite networks can enable the hybrid connectivity needed for continuous data transmission without interruption or loss of signal to enable the future of work and premium mobility. This is a proof of concept experiment to show throughput, latency, and key applications including handover to 4G and the use of a VPN while running cloud applications. Across three tests (i.e. entertainment and business productivity streaming services), the researchers demonstrate a 2-3x faster ping rate (ms), 4-5x faster download rates (Mb/s), and 30-60x (Mb/s) faster upload rates.      
