# ArXiv eess --Thu, 23 Jul 2020
### 1.Leveraging Synthetic Subject Invariant EEG Signals for Zero Calibration BCI  [ :arrow_down: ](https://arxiv.org/pdf/2007.11544.pdf)
>  Recently, substantial progress has been made in the area of Brain-Computer Interface (BCI) using modern machine learning techniques to decode and interpret brain signals. While Electroencephalography (EEG) has provided a non-invasive method of interfacing with a human brain, the acquired data is often heavily subject and session dependent. This makes seamless incorporation of such data into real-world applications intractable as the subject and session data variance can lead to long and tedious calibration requirements and cross-subject generalisation issues. Focusing on a Steady State Visual Evoked Potential (SSVEP) classification systems, we propose a novel means of generating highly-realistic synthetic EEG data invariant to any subject, session or other environmental conditions. Our approach, entitled the Subject Invariant SSVEP Generative Adversarial Network (SIS-GAN), produces synthetic EEG data from multiple SSVEP classes using a single network. Additionally, by taking advantage of a fixed-weight pre-trained subject classification network, we ensure that our generative model remains agnostic to subject-specific features and thus produces subject-invariant data that can be applied to new previously unseen subjects. Our extensive experimental evaluation demonstrates the efficacy of our synthetic data, leading to superior performance, with improvements of up to 16% in zero-calibration classification tasks when trained using our subject-invariant synthetic EEG signals.      
### 2.A Transfer Learning End-to-End ArabicText-To-Speech (TTS) Deep Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2007.11541.pdf)
>  Speech synthesis is the artificial production of human speech. A typical text-to-speech system converts a language text into a waveform. There exist many English TTS systems that produce mature, natural, and human-like speech synthesizers. In contrast, other languages, including Arabic, have not been considered until recently. Existing Arabic speech synthesis solutions are slow, of low quality, and the naturalness of synthesized speech is inferior to the English synthesizers. They also lack essential speech key factors such as intonation, stress, and rhythm. Different works were proposed to solve those issues, including the use of concatenative methods such as unit selection or parametric methods. However, they required a lot of laborious work and domain expertise. Another reason for such poor performance of Arabic speech synthesizers is the lack of speech corpora, unlike English that has many publicly available corpora and audiobooks. This work describes how to generate high quality, natural, and human-like Arabic speech using an end-to-end neural deep network architecture. This work uses just $\langle$ text, audio $\rangle$ pairs with a relatively small amount of recorded audio samples with a total of 2.41 hours. It illustrates how to use English character embedding despite using diacritic Arabic characters as input and how to preprocess these audio samples to achieve the best results.      
### 3.Cell-Free Satellite-UAV Networks for 6G Wide-Area Internet of Things  [ :arrow_down: ](https://arxiv.org/pdf/2007.11516.pdf)
>  In fifth generation (5G) and beyond Internet of Things (IoT), it becomes increasingly important to serve a massive number of IoT devices outside the coverage of terrestrial cellular networks. Due to their own limitations, unmanned aerial vehicles (UAVs) and satellites need to coordinate with each other in the coverage holes of 5G, leading to a cognitive satellite-UAV network (CSUN). In this paper, we investigate multi-domain resource allocation for CSUNs consisting of a satellite and a swarm of UAVs, so as to improve the efficiency of massive access in wide areas. Particularly, the cell-free on-demand coverage is established to overcome the cost-ineffectiveness of conventional cellular architecture. Opportunistic spectrum sharing is also implemented to cope with the spectrum scarcity problem. To this end, a process-oriented optimization framework is proposed for jointly allocating subchannels, transmit power and hovering times, which considers the whole flight process of UAVs and uses only the slowly-varying large-scale channel state information (CSI). Under the on-board energy constraints of UAVs and interference temperature constraints from UAV swarm to satellite users, we present iterative multi-domain resource allocation algorithms to improve network efficiency with guaranteed user fairness. Simulation results demonstrate the superiority of the proposed algorithms. Moreover, the adaptive cell-free coverage pattern is observed, which implies a promising way to efficiently serve wide-area IoT devices in the upcoming sixth generation (6G) era.      
### 4.Air-to-Ground Channel Characterization for Low-Height UAVs in Realistic Network Deployments  [ :arrow_down: ](https://arxiv.org/pdf/2007.11502.pdf)
>  Due to the decrease in cost, size and weight, \acp{UAV} are becoming more and more popular for general-purpose civil and commercial applications. Provision of communication services to \acp{UAV} both for user data and control messaging by using off-the-shelf terrestrial cellular deployments introduces several technical challenges. In this paper, an approach to the air-to-ground channel characterization for low-height \acp{UAV} based on an extensive measurement campaign is proposed, giving special attention to the comparison of the results when a typical directional antenna for network deployments is used and when a quasi-omnidirectional one is considered. Channel characteristics like path loss, shadow fading, root mean square delay and Doppler frequency spreads and the K-factor are statistically characterized for different suburban scenarios.      
### 5.Cache-enabling UAV Communications: Network Deployment and Resource Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2007.11501.pdf)
>  In this article, we investigate the content distribution in the hotspot area, whose traffic is offloaded by the combination of the unmanned aerial vehicle (UAV) communication and edge caching. In cache-enabling UAV-assisted cellular networks, the network deployment and resource allocation are vital for quality of experience (QoE) of users with content distribution applications. We formulate a joint optimization problem of UAV deployment, caching placement and user association for maximizing QoE of users, which is evaluated by mean opinion score (MOS). To solve this challenging problem, we decompose the optimization problem into three sub-problems. Specifically, we propose a swap matching based UAV deployment algorithm, then obtain the near-optimal caching placement and user association by greedy algorithm and Lagrange dual, respectively. Finally, we propose a low complexity iterative algorithm for the joint UAV deployment, caching placement and user association optimization, which achieves good computational complexity-optimality tradeoff. Simulation results reveal that: i) the MOS of the proposed algorithm approaches that of the exhaustive search method and converges within several iterations; and ii) compared with the benchmark algorithms, the proposed algorithm achieves better performance in terms of MOS, content access delay and backhaul traffic offloading.      
### 6.Resilient Secondary Voltage Control of Islanded Microgrids: An ESKBF-Based Distributed Fast Terminal Sliding Mode Control Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.11494.pdf)
>  This paper proposes a distributed secondary voltage control method based on extended state Kalman-Bucy filter (ESKBF) and fast terminal sliding mode (FTSM) control for the resilient operation of an islanded microgrid (MG) with inverter-based distributed generations (DGs). To tackle the co-existence of multiple uncertainties, a unified modelling framework is proposed to represent the set of different types of disturbances, including parameter perturbation, measurement noise, and immeasurably external variables, by an extended state method. Kalman-Bucy filter is then applied to accurately estimate the state information of the extended DG model. In addition, based on the accurate estimation, a fast terminal sliding mode (FTSM) surface with terminal attractors is designed to maintain the system stability and accelerate the convergence of consensus tracking, which significantly improves the performance of secondary voltage control under both normal and plug-and-play operation. Finally, case studies are conducted in both MATLAB/Simulink and an experimental testbed to demonstrate the effectiveness of the proposed method.      
### 7.Analysis and Comparison of Different Wavelet Transform Methods Using Benchmarks for Image Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2007.11488.pdf)
>  In recent years, many research achievements are made in the medical image fusion field. Medical Image fusion means that several of various modality image information is comprehended together to form one image to express its information. The aim of image fusion is to integrate complementary and redundant information. CT/MRI is one of the most common medical image fusion. These medical modalities give information about different diseases. Complementary information is offered by CT and MRI. CT provides the best information about denser tissue and MRI offers better information on soft tissue. There are two approaches to image fusion, namely Spatial Fusion and Transform fusion. Transform fusion uses transform for representing the source images at multi-scale. This paper presents a Wavelet Transform image fusion methodology based on the intensity magnitudes of the wavelet coefficients and compares five variations of the wavelet transform implemented separately in this fusion model. The image fusion model, using the Discrete Wavelet Transform (DWT), the Stationary Wavelet Transform (SWT), the Integer Lifting Wavelet Transform (ILFT) the dual-tree Complex Wavelet Transform (DT CWT) and dual-tree Q-shift dual-tree CWT, is applied to multi-modal images. The resulting fused images are compared visually and through benchmarks such as Entropy (E), Peak Signal to Noise Ratio, (PSNR), Root Mean Square Error (RMSE), Image Quality Index (IQI) and Standard deviation (SD) computations.      
### 8.Resource-Efficient Speech Mask Estimation for Multi-Channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2007.11477.pdf)
>  While machine learning techniques are traditionally resource intensive, we are currently witnessing an increased interest in hardware and energy efficient approaches. This need for resource-efficient machine learning is primarily driven by the demand for embedded systems and their usage in ubiquitous computing and IoT applications. In this article, we provide a resource-efficient approach for multi-channel speech enhancement based on Deep Neural Networks (DNNs). In particular, we use reduced-precision DNNs for estimating a speech mask from noisy, multi-channel microphone observations. This speech mask is used to obtain either the Minimum Variance Distortionless Response (MVDR) or Generalized Eigenvalue (GEV) beamformer. In the extreme case of binary weights and reduced precision activations, a significant reduction of execution time and memory footprint is possible while still obtaining an audio quality almost on par to single-precision DNNs and a slightly larger Word Error Rate (WER) for single speaker scenarios using the WSJ0 speech corpus.      
### 9.Sparse Signatures with Forward Error Correction Coding for Non-Orthogonal Massive Access  [ :arrow_down: ](https://arxiv.org/pdf/2007.11467.pdf)
>  Wireless communication systems providing massive access for Internet-of-Things (IoT) applications are of growing importance, especially in connection with 5G networks and beyond. Typical massive access scenarios are studied on the basis of a multiple access channel in which a randomly chosen subset of terminals (users) transmit short messages to a common receiver. The transmission of short messages leads to finite blocklength effects and therefore calls for approaches that jointly address the problem of multi-user interference and noise reduction. In this paper, we propose a coding and transmission scheme that combines sparse signature design and finite-size forward error correction (FEC) coding for non-orthogonal massive access that is suitable for low-complexity receiver processing. Our sparse signature construction method relies on the concept of Euler squares and we use graph theory tools to map a sparse allocation of users to resources in a non-orthogonal access model. We evaluate the system performance using an iterative receiver implementation with extrinsic information exchange between a multi-user detector (MUD) based on message passing algorithm (MPA) and user-specific FEC. The proposed construction can be explicitly characterized for a large number of combinations of system parameters, providing a framework for both, non-orthogonal scheduled - and grant-free (massive) random access.      
### 10.4S-DT: Self Supervised Super Sample Decomposition for Transfer learning with application to COVID-19 detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.11450.pdf)
>  Due to the high availability of large-scale annotated image datasets, knowledge transfer from pre-trained models showed outstanding performance in medical image classification. However, building a robust image classification model for datasets with data irregularity or imbalanced classes can be a very challenging task, especially in the medical imaging domain. In this paper, we propose a novel deep convolutional neural network, we called Self Supervised Super Sample Decomposition for Transfer learning (4S-DT) model.4S-DTencourages a coarse-to-fine transfer learning from large-scale image recognition tasks to a specific chest X-ray image classification task using a generic self-supervised sample decomposition approach. Our main contribution is a novel self-supervised learning mechanism guided by a super sample decomposition of unlabelled chest X-ray images. 4S-DT helps in improving the robustness of knowledge transformation via a downstream learning strategy with a class-decomposition layer to simplify the local structure of the data.4S-DT can deal with any irregularities in the image dataset by investigating its class boundaries using a downstream class-decomposition mechanism. We used 50,000 unlabelled chest X-ray images to achieve our coarse-to-fine transfer learning with an application to COVID-19 detection, as an exemplar. 4S-DT has achieved an accuracy of 97.54% (95% CI: 96.22%, 98.91%) in the detection of COVID-19 cases on an extended test set enriched by augmented images, out of which all real COVID-19 cases were detected, which was the highest accuracy obtained when compared to other methods.      
### 11.Learning generalized Nash equilibria in multi-agent dynamical systems via extremum seeking control  [ :arrow_down: ](https://arxiv.org/pdf/2007.11437.pdf)
>  In this paper, we consider the problem of learning a generalized Nash equilibrium (GNE) in strongly monotone games. First, we propose a novel continuous-time solution algorithm that uses regular projections and first-order information. As second main contribution, we design a data-driven variant of the former algorithm where each agent estimates their individual pseudo-gradient via zero-order information, namely, measurements of their individual cost function values, as typical of extremum seeking control. Third, we generalize our setup and results for multi-agent systems with nonlinear dynamics. Finally, we apply our algorithms to connectivity control in robotic sensor networks and distributed wind farm optimization.      
### 12.Joint Radio Resource Allocation and Cooperative Caching in PD-NOMA-Based HetNets  [ :arrow_down: ](https://arxiv.org/pdf/2007.11415.pdf)
>  In this paper, we propose a novel joint resource allocation and cooperative caching scheme for power-domain non-orthogonal multiple access (PD-NOMA)-based heterogeneous networks (HetNets). In our scheme, the requested content is fetched directly from the edge if it is cached in the storage of one of the base stations (BSs), and otherwise is fetched via the backhaul. Our scheme consists of two phases: 1. Caching phase where the contents are saved in the storage of the BSs, and 2. Delivery phase where the requested contents are delivered to users. We formulate a novel optimization problem over radio resources and content placement variables. We aim to minimize the network cost subject to quality-of-service (QoS), caching, subcarrier assignment, and power allocation constraints. By exploiting advanced optimization methods, such as alternative search method (ASM), Hungarian algorithm, successive convex approximation (SCA), we obtain an efficient sub-optimal solution of the optimization problem. Numerical results illustrate that our ergodic caching policy via the proposed resource management algorithm can achieve a considerable reduction on the total cost on average compared to the most popular caching and random caching policy. Moreover, our cooperative NOMA scheme outperforms orthogonal multiple access (OMA) in terms of the delivery cost in general with an acceptable complexity increase.      
### 13.Optimal Pacing of a Cyclist in a Time Trial Based on Experimentally Calibrated Models of Fatigue and Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2007.11393.pdf)
>  In this paper, we first use experimental data from six human subjects to validate and calibrate our proposed dynamic models for fatigue and recovery of cyclists in [1]. These models are used to formulate pacing strategy of a cyclist during a time trial as an optimal control problem. We first derive the optimal strategy in a time-trial using Pontryagin's Minimum Principle. We then resort to numerical solution via Dynamic Programming (DP) for simulating one of the subjects on four strategically different courses including stage 13 of the 2019 Tour de France. The DP simulation results confirm our analytical findings and also show reduced time over experimental results of the self-paced subject who is a competitive cyclist.      
### 14.Feature based Sequential Classifier with Attention Mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2007.11392.pdf)
>  Cervical cancer is one of the deadliest cancers affecting women globally. Cervical intraepithelial neoplasia (CIN) assessment using histopathological examination of cervical biopsy slides is subject to interobserver variability. Automated processing of digitized histopathology slides has the potential for more accurate classification for CIN grades from normal to increasing grades of pre-malignancy: CIN1, CIN2 and CIN3. Cervix disease is generally understood to progress from the bottom (basement membrane) to the top of the epithelium. To model this relationship of disease severity to spatial distribution of abnormalities, we propose a network pipeline, DeepCIN, to analyze high-resolution epithelium images (manually extracted from whole-slide images) hierarchically by focusing on localized vertical regions and fusing this local information for determining Normal/CIN classification. The pipeline contains two classifier networks: 1) a cross-sectional, vertical segment-level sequence generator (two-stage encoder model) is trained using weak supervision to generate feature sequences from the vertical segments to preserve the bottom-to-top feature relationships in the epithelium image data; 2) an attention-based fusion network image-level classifier predicting the final CIN grade by merging vertical segment sequences. The model produces the CIN classification results and also determines the vertical segment contributions to CIN grade prediction. Experiments show that DeepCIN achieves pathologist-level CIN classification accuracy.      
### 15.Blind hierarchical deconvolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.11391.pdf)
>  Deconvolution is a fundamental inverse problem in signal processing and the prototypical model for recovering a signal from its noisy measurement. Nevertheless, the majority of model-based inversion techniques require knowledge on the convolution kernel to recover an accurate reconstruction and additionally prior assumptions on the regularity of the signal are needed. To overcome these limitations, we parametrise the convolution kernel and prior length-scales, which are then jointly estimated in the inversion procedure. The proposed framework of blind hierarchical deconvolution enables accurate reconstructions of functions with varying regularity and unknown kernel size and can be solved efficiently with an empirical Bayes two-step procedure, where hyperparameters are first estimated by optimisation and other unknowns then by an analytical formula.      
### 16.Sensor-Based Continuous Hand Gesture Recognition by Long Short-Term Memory  [ :arrow_down: ](https://arxiv.org/pdf/2007.11268.pdf)
>  This article aims to present a novel sensor-based continuous hand gesture recognition algorithm by long short-term memory (LSTM). Only the basic accelerators and/or gyroscopes are required by the algorithm. Given a sequence of input sensory data, a many-to-many LSTM scheme is adopted to produce an output path. A maximum a posteriori estimation is then carried out based on the observed path to obtain the final classification results. A prototype system based on smartphones has been implemented for the performance evaluation. Experimental results show that the proposed algorithm is an effective alternative for robust and accurate hand-gesture recognition.      
### 17.Dynamic Independent Component/Vector Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.11241.pdf)
>  A novel extension of the popular FastICA algorithm for Independent Component Analysis is proposed in one-unit, symmetric and block-deflation variants. The methods introduced in this paper are capable of extracting/separating one or several sources from specific types of time-varying mixtures. The algorithms are derived within a unified framework so that they are applicable in the real-valued as well as complex-valued domains, and jointly to several mixtures, similar to Independent Vector Analysis. Performance analysis of the one-unit algorithm is provided; it shows its asymptotic efficiency under the given mixing and statistical models. Numerical simulations corroborate the validity of the analysis, confirm the usefulness of the algorithms in separation of moving sources, and show the superior speed of convergence and ability to separate super-Gaussian as well as sub-Gaussian signals.      
### 18.A Post-coder Feedback Approach to Overcome Training Asymmetry in MIMO-TDD  [ :arrow_down: ](https://arxiv.org/pdf/2007.11209.pdf)
>  Time Divison Duplex (TDD) wireless communication systems are inherently bidirectional, which facilitates exploiting channel reciprocity for pilot based channel estimation of both uplink and downlink. However, there exists a gross asymmetry in channel estimation complexity for the uplink and downlink, particularly for Multiple Input Multiple Output (MIMO) TDD systems. Usually, Base Stations (BS) with more antennas need to estimate fewer parameters from each antenna, whereas the estimation requirement is disproportionately higher at the User Equipment (UE). Unlike the UE, the BS has powerful hardware, computational resources and energy to accurately estimate and track channel profiles. To overcome this asymmetry, we propose a solution for MIMO-TDD downlink communication, wherein the post-coder part of the channel matrix is quantized at the BS, and is communicated to the UE via a low-rate channel. Using asymptotically tight lower bounds on the downlink achievable rates, we quantify the performance of the proposed scheme. Simulations reveal that a moderate number of quantization bits are sufficient to achieve rates close to the the link capacity. We further show that, when the BS has many more antennas at than the UE, the channel can be compensated by appropriate transmit domain precoding without post-coder knowledge at the UE.      
### 19.Secrecy Rate Maximization for Intelligent Reflecting Surface Aided SWIPT Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.11204.pdf)
>  Simultaneous wireless information and power transfer (SWIPT) and intelligent reflecting surface (IRS) are two promising techniques for providing enhanced wireless communication capability and sustainable energy supply to energy-constrained wireless devices. Moreover, the combination of the IRS and the SWIPT can create the "one plus one greater than two" effect. However, due to the broadcast nature of wireless media, the IRS-aided SWIPT systems are vulnerable to eavesdropping. In this paper, we study the security issue of the IRS-aided SWIPT systems. The objective is to maximize the secrecy rate by jointly designing the transmit beamforming and artificial noise (AN) covariance matrix at a base station (BS) and reflective beamforming at an IRS, under transmit power constraint at the BS and energy harvesting (EH) constraints at multiple energy receivers. To tackle the formulated non-convex problem, we first employ an alternating optimization (AO) algorithm to decouple the coupling variables. Then, reflective beamforming, transmit beamforming and AN covariance matrix can be optimized by using a penalty-based algorithm and semidefinite relaxation (SDR) method, respectively. Simulation results demonstrate the effectiveness of the proposed scheme over baseline schemes.      
### 20.Novel Mobility Model to Support the Routing of Mobile Energy Resources  [ :arrow_down: ](https://arxiv.org/pdf/2007.11191.pdf)
>  Mobile energy resources (MERs) have received increasing attention due to their effectiveness in boosting the power system resilience in a flexible way. In this letter, a novel mobility model for MERs is proposed, which can support the routing of MERs to provide various services for the power system. Two key points, the state transitions and travel time of MERs, are formulated by linear constraints. The feasibility of the proposed model, especially its advantages in model size and computational efficiency for routing problems with a small time span, is demonstrated by a series of tests.      
### 21.Energy Scheduling for Residential Distributed Energy Resources with Uncertainties Using Model-based Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2007.11182.pdf)
>  This paper proposes a reliable energy scheduling framework for distributed energy resources (DER) of a residential area to achieve an appropriate daily electricity consumption with the maximum affordable demand response. Renewable and non-renewable energy resources are available to respond to customers' demands using different classes of methods to manage energy during the time. The optimal operation problem is a mixed-integer-linear-programming (MILP) investigated using model-based predictive control (MPC) to determine which dispatchable unit should be operated at what time and at what power level while satisfying practical constraints. Renewable energy sources (RES), particularly solar and wind energies recently have expanded their role in electric power systems. Although they are environment friendly and accessible, there are challenging issues regarding their performance such as dealing with the variability and uncertainties concerned with them. This research investigates the energy management of these systems in three complementary scenarios. The first and second scenarios are respectively suitable for a market with a constant and inconstant price. Additionally, the third scenario is proposed to consider the role of uncertainties in RES and it is designed to recompense the power shortage using non-renewable resources. The validity of methods is explored in a residential area for 24 hours and the results thoroughly demonstrate the competence of the proposed approach for decreasing the operation cost.      
### 22.MI^2GAN: Generative Adversarial Network for Medical Image Domain Adaptation using Mutual Information Constraint  [ :arrow_down: ](https://arxiv.org/pdf/2007.11180.pdf)
>  Domain shift between medical images from multicentres is still an open question for the community, which degrades the generalization performance of deep learning models. Generative adversarial network (GAN), which synthesize plausible images, is one of the potential solutions to address the problem. However, the existing GAN-based approaches are prone to fail at preserving image-objects in image-to-image (I2I) translation, which reduces their practicality on domain adaptation tasks. In this paper, we propose a novel GAN (namely MI$^2$GAN) to maintain image-contents during cross-domain I2I translation. Particularly, we disentangle the content features from domain information for both the source and translated images, and then maximize the mutual information between the disentangled content features to preserve the image-objects. The proposed MI$^2$GAN is evaluated on two tasks---polyp segmentation using colonoscopic images and the segmentation of optic disc and cup in fundus images. The experimental results demonstrate that the proposed MI$^2$GAN can not only generate elegant translated images, but also significantly improve the generalization performance of widely used deep learning networks (e.g., U-Net).      
### 23.Output Based Adaptive Distributed Output Observer for Leader-follower Multiagent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.11153.pdf)
>  The adaptive distributed observer approach has been an effective tool for synthesizing a distributed control law for solving various control problems of leader-follower multiagent systems. However, the existing adaptive distributed observer needs to make use of the full state of the leader system. This assumption not only precludes many practical applications in which only the output of the leader system is available, but also leads to a high dimension observer. In this communique, we propose an adaptive distributed output observer which only makes use of the output of the leader system, and is thus more practical than the state based adaptive distributed observer. Moreover, the dimension and the information exchange among agents of the proposed adaptive distributed output observer can be significantly smaller than those of the state based adaptive distributed output observer.      
### 24.Invoking Deep Learning for Joint Estimation of Indoor LiFi User Position and Orientation  [ :arrow_down: ](https://arxiv.org/pdf/2007.11104.pdf)
>  Light-fidelity (LiFi) is a fully-networked bidirectional optical wireless communication (OWC) that is considered a promising solution for high-speed indoor connectivity. Unlike in conventional radio frequency wireless systems, the OWC channel is not isotropic, meaning that the device orientation affects the channel gain significantly. However, due to the lack of proper channel models for LiFi systems, many studies have assumed that the receiver is vertically upward and randomly located within the coverage area, which is not a realistic assumption from a practical point of view. In this paper, novel realistic and measurement-based channel models for indoor LiFi systems are proposed. Precisely, the statistics of the channel gain are derived for the case of randomly oriented stationary and mobile LiFi receivers. For stationary users, two channel models are proposed, namely, the modified truncated Laplace (MTL) model and the modified Beta (MB) model. For LiFi users, two channel models are proposed, namely, the sum of modified truncated Gaussian (SMTG) model and the sum of modified Beta (SMB) model. Based on the derived models, the impact of random orientation and spatial distribution of LiFi users is investigated, where we show that the aforementioned factors can strongly affect the channel gain and system performance.      
### 25.Fully Convolutional Neural Networks for Automotive Radar Interference Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2007.11102.pdf)
>  The interest of the automotive industry has progressively focused on subjects related to driver assistance systems as well as autonomous cars. Cars combine a variety of sensors to perceive their surroundings robustly. Among them, radar sensors are indispensable because of their independence of lighting conditions and the possibility to directly measure velocity. However, radar interference is an issue that becomes prevalent with the increasing amount of radar systems in automotive scenarios. In this paper, we address this issue for frequency modulated continuous wave (FMCW) radars with fully convolutional neural networks (FCNs), a state-of-the-art deep learning technique. We propose two FCNs that take spectrograms of the beat signals as input, and provide the corresponding clean range profiles as output. We propose two architectures for interference mitigation which outperform the classical zeroing technique. Moreover, considering the lack of databases for this task, we release as open source a large scale data set that closely replicates real world automotive scenarios for single-interference cases, allowing others to objectively compare their future work in this domain. The data set is available for download at: <a class="link-external link-http" href="http://github.com/ristea/arim" rel="external noopener nofollow">this http URL</a>.      
### 26.A Lite Distributed Semantic Communication System for Internet of Things  [ :arrow_down: ](https://arxiv.org/pdf/2007.11095.pdf)
>  The rapid development of deep learning (DL) and widespread applications of Internet-of-Things (IoT) have made the devices smarter than before, and enabled them to perform more intelligent tasks. However, it is challenging for any IoT device to train and run a DL model independently due to its limited computing capability. In this paper, we consider an IoT network where the cloud/edge platform performs the DL based semantic communication (DeepSC) model training and updating while IoT devices perform data collection and transmission based on the trained model. To make it affordable for IoT devices, we propose a lite distributed semantic communication system based on DL, named L-DeepSC, for text transmission with low complexity, where the data transmission from the IoT devices to the cloud/edge works at the semantic level to improve transmission efficiency. Particularly, by pruning the model redundancy and lowering the weight resolution, the L-DeepSC becomes affordable for IoT devices and the bandwidth required for model weight transmission between IoT devices and the cloud/edge is reduced significantly. Through analyzing the effects of fading channels in forward-propagation and back-propagation during the training of L-DeepSC, we develop a channel state information (CSI) aided training processing to decrease the effects of fading channels on transmission. Meanwhile, we tailor the semantic constellation by quantization for the current antenna design. Simulation demonstrates that the proposed L-DeepSC achieves competitive performance compared with traditional methods, especially in the low signal-to-noise (SNR) region. In particular, while it can reach as large as 20x compression ratio without performance degradation.      
### 27.One Click Lesion RECIST Measurement and Segmentation on CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2007.11087.pdf)
>  In clinical trials, one of the radiologists' routine work is to measure tumor sizes on medical images using the RECIST criteria (Response Evaluation Criteria In Solid Tumors). However, manual measurement is tedious and subject to inter-observer variability. We propose a unified framework named SEENet for semi-automatic lesion \textit{SE}gmentation and RECIST \textit{E}stimation on a variety of lesions over the entire human body. The user is only required to provide simple guidance by clicking once near the lesion. SEENet consists of two main parts. The first one extracts the lesion of interest with the one-click guidance, roughly segments the lesion, and estimates its RECIST measurement. Based on the results of the first network, the second one refines the lesion segmentation and RECIST estimation. SEENet achieves state-of-the-art performance in lesion segmentation and RECIST estimation on the large-scale public DeepLesion dataset. It offers a practical tool for radiologists to generate reliable lesion measurements (i.e. segmentation mask and RECIST) with minimal human effort and greatly reduced time.      
### 28.3D Localization of a Sound Source Using Mobile Microphone Arrays Referenced by SLAM  [ :arrow_down: ](https://arxiv.org/pdf/2007.11079.pdf)
>  A microphone array can provide a mobile robot with the capability of localizing, tracking and separating distant sound sources in 2D, i.e., estimating their relative elevation and azimuth. To combine acoustic data with visual information in real world settings, spatial correlation must be established. The approach explored in this paper consists of having two robots, each equipped with a microphone array, localizing themselves in a shared reference map using SLAM. Based on their locations, data from the microphone arrays are used to triangulate in 3D the location of a sound source in relation to the same map. This strategy results in a novel cooperative sound mapping approach using mobile microphone arrays. Trials are conducted using two mobile robots localizing a static or a moving sound source to examine in which conditions this is possible. Results suggest that errors under 0.3 m are observed when the relative angle between the two robots are above 30 degrees for a static sound source, while errors under 0.3 m for angles between 40 degrees and 140 degrees are observed with a moving sound source.      
### 29.Semantics-Empowered Communication for Networked Intelligent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.11579.pdf)
>  Wireless connectivity has traditionally been regarded as a content-agnostic data pipe; the impact upon receipt and the context- and goal-dependent significance of the conveyed messages have been deliberately ignored. Nevertheless, in emerging cyber-physical and autonomous intelligent networked systems, acquiring, processing, and sending excessive amounts of distributed real-time data, which ends up being stale, irrelevant, or useless to the end user, will cause communication bottlenecks, increased response time, and safety issues. We envision a communication paradigm shift that makes the semantics of information, i.e., the importance and the usefulness of information generated and transmitted for attaining a certain goal, the underpinning of the communication process. We advocate for a goal-oriented unification of data generation/active sampling, information transmission, and signal reconstruction, by taking into account process and source variability, signal sparsity and correlation, and semantic information attributes. We apply this structurally new joint approach to a communication scenario where the destination is tasked with real-time source reconstruction for the purpose of remote actuation. Capitalizing on semantics-aware metrics, we explore the optimal sampling policy, which significantly reduces the number of samples communicated and the reconstruction error in ways that are not possible by today's state-of-the-art approaches.      
### 30.Pushing the Physical Limits of IoT Devices with Programmable Metasurfaces  [ :arrow_down: ](https://arxiv.org/pdf/2007.11503.pdf)
>  Small, low-cost IoT devices are typically equipped with only a single, low-quality antenna, significantly limiting communication range and link quality. In particular, these antennas are typically linearly polarized and therefore susceptible to polarization mismatch, which can easily cause 10-15 dBm of link loss on communication to and from such devices. In this work, we highlight this under-appreciated issue and propose the augmentation of IoT deployment environments with programmable, RF-sensitive surfaces made of metamaterials. Our smart meta-surface mitigates polarization mismatch by rotating the polarization of signals that pass through or reflect off the surface. We integrate our metasurface into an IoT network as LAMA, a Low-power Lattice of Actuated Metasurface Antennas, designed for the pervasively used 2.4 GHz ISM band. We optimize LAMA's metasurface design for both low transmission loss and low cost, to facilitate deployment at scale. We then build an end-to-end system that actuates the metasurface structure to optimize for link performance in real time. Our experimental prototype-based evaluation demonstrates gains in link power of up to 15 dBm, and wireless capacity improvements of 100 and 180 Kbit/s/Hz in through-surface and surface-reflective scenarios, respectively, attributable to the polarization rotation properties of LAMA'S metasurface.      
### 31.Multi-reference alignment in high dimensions: sample complexity and phase transition  [ :arrow_down: ](https://arxiv.org/pdf/2007.11482.pdf)
>  Multi-reference alignment entails estimating a signal in $\mathbb{R}^L$ from its circularly-shifted and noisy copies. This problem has been studied thoroughly in recent years, focusing on the finite-dimensional setting (fixed $L$). Motivated by single-particle cryo-electron microscopy, we analyze the sample complexity of the problem in the high-dimensional regime $L\to\infty$. Our analysis uncovers a phase transition phenomenon governed by the parameter $\alpha = L/(\sigma^2\log L)$, where $\sigma^2$ is the variance of the noise. When $\alpha&gt;2$, the impact of the unknown circular shifts on the sample complexity is minor. Namely, the number of measurements required to achieve a desired accuracy $\varepsilon$ approaches $\sigma^2/\varepsilon$ for small $\varepsilon$; this is the sample complexity of estimating a signal in additive white Gaussian noise, which does not involve shifts. In sharp contrast, when $\alpha\leq 2$, the problem is significantly harder and the sample complexity grows substantially quicker with $\sigma^2$.      
### 32.Video-ception Network: Towards Multi-Scale Efficient Asymmetric Spatial-Temporal Interactions  [ :arrow_down: ](https://arxiv.org/pdf/2007.11460.pdf)
>  Previous video modeling methods leverage the cubic 3D convolution filters or its decomposed variants to exploit the motion cues for precise action recognition, which tend to be performed on the video features along the temporal and spatial axes symmetrically. This brings the hypothesis implicitly that the actions are recognized from the cubic voxel level and neglects the essential spatial-temporal shape diversity across different actions. In this paper, we propose a novel video representing method that fuses the features spatially and temporally in an asymmetric way to model action atomics spanning multi-scale spatial-temporal scales. To permit the feature fusion procedure efficiently and effectively, we also design the optimized feature interaction layer, which covers most feature fusion techniques as special case of it, e.g., channel shuffling and channel concatenating. We instantiate our method as a \textit{plug-and-play} block, termed Multi-Scale Efficient Asymmetric Spatial-Temporal Block. Our method can easily adapt the traditional 2D CNNs to the video understanding tasks such as action recognition. We verify our method on several most recent large-scale video datasets requiring strong temporal reasoning or appearance discriminating, e.g., Something-to-Something v1, Kinetics and Diving48, demonstrate the new state-of-the-art results without bells and whistles.      
### 33.Simplex-Structured Matrix Factorization: Sparsity-based Identifiability and Provably Correct Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2007.11446.pdf)
>  In this paper, we provide novel algorithms with identifiability guarantees for simplex-structured matrix factorization (SSMF), a generalization of nonnegative matrix factorization. Current state-of-the-art algorithms that provide identifiability results for SSMF rely on the sufficiently scattered condition (SSC) which requires the data points to be well spread within the convex hull of the basis vectors. The conditions under which our proposed algorithms recover the unique decomposition is in most cases much weaker than the SSC. We only require to have $d$ points on each facet of the convex hull of the basis vectors whose dimension is $d-1$. The key idea is based on extracting facets containing the largest number of points. We illustrate the effectiveness of our approach on synthetic data sets and hyperspectral images, showing that it outperforms state-of-the-art SSMF algorithms as it is able to handle higher noise levels, rank deficient matrices, outliers, and input data that highly violates the SSC.      
### 34.Deep Learning-Based FPGA Function Block Detection Method using an Image-Coded Representation of Bitstream  [ :arrow_down: ](https://arxiv.org/pdf/2007.11434.pdf)
>  Examining field-programmable gate array (FPGA) bitstream is found to help detect known function blocks, which offers assistance and insight to analyze the circuit's system function. Our goal is to detect one or more than one function block in FPGA design from a complete bitstream by utilizing the latest deep learning techniques, which do not require manually designing features. To this end, in this paper, we propose a deep learning-based FPGA function block detection method by transforming the bitstream into a three-channel color image. In specific, we first analyze the format of the bitstream to find the mapping relationship between the configuration bits and configurable logic blocks. Next, an image-coded representation of bitstream is proposed suitable for deep learning processing. This bitstream-to-image transformation takes into account of the adjacency nature of the programmable logic as well as high degree of redundancy of configuration information. With the color images transformed from bitstreams as the training dataset, a deep learning-based object detection algorithm is applied for generating the function block detection results. The effects of EDA tools, input size of the deep neural network, and the data arrangement of representation on the detection accuracy are explored. The Xilinx Zynq-7000 SoCs and Xilinx Zynq UltraScale+ MPSoCs are adopted to verify the proposed method, and the results show that the mean Average Precision (IoU=0.5) for 10 function blocks is as high as 97.72% for YOLOv3 detector.      
### 35.Learning Disentangled Feature Representation for Hybrid-distorted Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2007.11430.pdf)
>  Hybrid-distorted image restoration (HD-IR) is dedicated to restore real distorted image that is degraded by multiple distortions. Existing HD-IR approaches usually ignore the inherent interference among hybrid distortions which compromises the restoration performance. To decompose such interference, we introduce the concept of Disentangled Feature Learning to achieve the feature-level divide-and-conquer of hybrid distortions. Specifically, we propose the feature disentanglement module (FDM) to distribute feature representations of different distortions into different channels by revising gain-control-based normalization. We also propose a feature aggregation module (FAM) with channel-wise attention to adaptively filter out the distortion representations and aggregate useful content information from different channels for the construction of raw image. The effectiveness of the proposed scheme is verified by visualizing the correlation matrix of features and channel responses of different distortions. Extensive experimental results also prove superior performance of our approach compared with the latest HD-IR schemes.      
### 36.Sum-of-squares chordal decomposition of polynomial matrix inequalities  [ :arrow_down: ](https://arxiv.org/pdf/2007.11410.pdf)
>  We prove three decomposition results for sparse positive (semi-)definite polynomial matrices. First, we show that a polynomial matrix $P(x)$ with chordal sparsity is positive semidefinite for all $x\in \mathbb{R}^n$ if and only if there exists a sum-of-squares (SOS) polynomial $\sigma(x)$ such that $\sigma(x)P(x)$ can be decomposed into a sum of sparse SOS matrices, each of which is zero outside a small principal submatrix. Second, we establish that setting $\sigma(x)=(x_1^2 + \cdots + x_n^2)^\nu$ for some integer $\nu$ suffices if $P(x)$ is even, homogeneous, and positive definite. Third, we prove a sparse-matrix version of Putinar's Positivstellensatz: if $P(x)$ has chordal sparsity and is positive definite on a compact semialgebraic set $\mathcal{K}=\{x:g_1(x)\geq 0,\ldots,g_m(x)\geq 0\}$ satisfying the Archimedean condition, then $P(x) = S_0(x) + g_1(x)S_1(x) + \cdots + g_m(x)S_m(x)$ for matrices $S_i(x)$ that are sums of sparse SOS matrices, each of which is zero outside a small principal submatrix. Using these decomposition results, we obtain sparse SOS representation theorems for polynomials that are quadratic and correlatively sparse in a subset of variables. We also obtain new convergent hierarchies of sparsity-exploiting SOS reformulations to convex optimization problems with large and sparse polynomial matrix inequalities. Analytical examples illustrate all our decomposition results, while large-scale numerical examples demonstrate that the corresponding sparsity-exploiting SOS hierarchies have significantly lower computational complexity than traditional ones.      
### 37.Learning Centric Power Allocation for Edge Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2007.11399.pdf)
>  While machine-type communication (MTC) devices generate massive data, they often cannot process this data due to limited energy and computation power. To this end, edge intelligence has been proposed, which collects distributed data and performs machine learning at the edge. However, this paradigm needs to maximize the learning performance instead of the communication throughput, for which the celebrated water-filling and max-min fairness algorithms become inefficient since they allocate resources merely according to the quality of wireless channels. This paper proposes a learning centric power allocation (LCPA) method, which allocates radio resources based on an empirical classification error model. To get insights into LCPA, an asymptotic optimal solution is derived. The solution shows that the transmit powers are inversely proportional to the channel gain, and scale exponentially with the learning parameters. Experimental results show that the proposed LCPA algorithm significantly outperforms other power allocation algorithms.      
### 38.Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary Supervised Deep Adversarial Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.11319.pdf)
>  Robot-assisted surgery is an emerging technology which has undergone rapid growth with the development of robotics and imaging systems. Innovations in vision, haptics and accurate movements of robot arms have enabled surgeons to perform precise minimally invasive surgeries. Real-time semantic segmentation of the robotic instruments and tissues is a crucial step in robot-assisted surgery. Accurate and efficient segmentation of the surgical scene not only aids in the identification and tracking of instruments but also provided contextual information about the different tissues and instruments being operated with. For this purpose, we have developed a light-weight cascaded convolutional neural network (CNN) to segment the surgical instruments from high-resolution videos obtained from a commercial robotic system. We propose a multi-resolution feature fusion module (MFF) to fuse the feature maps of different dimensions and channels from the auxiliary and main branch. We also introduce a novel way of combining auxiliary loss and adversarial loss to regularize the segmentation model. Auxiliary loss helps the model to learn low-resolution features, and adversarial loss improves the segmentation prediction by learning higher order structural information. The model also consists of a light-weight spatial pyramid pooling (SPP) unit to aggregate rich contextual information in the intermediate stage. We show that our model surpasses existing algorithms for pixel-wise segmentation of surgical instruments in both prediction accuracy and segmentation time of high-resolution videos.      
### 39.Structure-Preserving Interpolation for Model Reduction of Parametric Bilinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.11269.pdf)
>  In this paper, we present an interpolation framework for structure-preserving model order reduction of parametric bilinear dynamical systems. We introduce a general setting, covering a broad variety of different structures for parametric bilinear systems, and then provide conditions on projection spaces for the interpolation of structured subsystem transfer functions such that the system structure and parameter dependencies are preserved in the reduced-order model. Two benchmark examples with different parameter dependencies are used to demonstrate the theoretical analysis.      
### 40.Deep-VFX: Deep Action Recognition Driven VFX for Short Video  [ :arrow_down: ](https://arxiv.org/pdf/2007.11257.pdf)
>  Human motion is a key function to communicate information. In the application, short-form mobile video is so popular all over the world such as Tik Tok. The users would like to add more VFX so as to pursue creativity and personlity. Many special effects are added on the short video platform. These gives the users more possibility to show off these personality. The common and traditional way is to create the template of VFX. However, in order to synthesis the perfect, the users have to tedious attempt to grasp the timing and rhythm of new templates. It is not easy-to-use especially for the mobile app. This paper aims to change the VFX synthesis by motion driven instead of the traditional template matching. We propose the AI method to improve this VFX synthesis. In detail, in order to add the special effect on the human body. The skeleton extraction is essential in this system. We also propose a novel form of LSTM to find out the user's intention by action recognition. The experiment shows that our system enables to generate VFX for short video more easier and efficient.      
### 41.Improving Monocular Depth Estimation by Leveraging Structural Awareness and Complementary Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2007.11256.pdf)
>  Monocular depth estimation plays a crucial role in 3D recognition and understanding. One key limitation of existing approaches lies in their lack of structural information exploitation, which leads to inaccurate spatial layout, discontinuous surface, and ambiguous boundaries. In this paper, we tackle this problem in three aspects. First, to exploit the spatial relationship of visual features, we propose a structure-aware neural network with spatial attention blocks. These blocks guide the network attention to global structures or local details across different feature layers. Second, we introduce a global focal relative loss for uniform point pairs to enhance spatial constraint in the prediction, and explicitly increase the penalty on errors in depth-wise discontinuous regions, which helps preserve the sharpness of estimation results. Finally, based on analysis of failure cases for prior methods, we collect a new Hard Case (HC) Depth dataset of challenging scenes, such as special lighting conditions, dynamic objects, and tilted camera angles. The new dataset is leveraged by an informed learning curriculum that mixes training examples incrementally to handle diverse data distributions. Experimental results show that our method outperforms state-of-the-art approaches by a large margin in terms of both prediction accuracy on NYUDv2 dataset and generalization performance on unseen datasets.      
### 42.A material decomposition method for dual-energy CT via dual interactive Wasserstein generative adversarial networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.11247.pdf)
>  Dual-energy computed tomography has great potential in material characterization and identification, whereas the reconstructed material-specific images always suffer from magnified noise and beam hardening artifacts. In this study, a data-driven approach using dual interactive Wasserstein generative adversarial networks is proposed to improve the material decomposition accuracy. Specifically, two interactive generators are used to synthesize the corresponding material images and different loss functions for training the decomposition model are incorporated to preserve texture and edges in the generated images. Besides, a selector is employed to ensure the modelling ability of two generators. The results from both the simulation phantoms and real data demonstrate the advantages of this method in suppressing the noise and beam hardening artifacts.      
### 43.Greenhouse Segmentation on High-Resolution Optical Satellite Imagery using Deep Learning Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2007.11222.pdf)
>  Greenhouse segmentation has pivotal importance for climate-smart agricultural land-use planning. Deep learning-based approaches provide state-of-the-art performance in natural image segmentation. However, semantic segmentation on high-resolution optical satellite imagery is a challenging task because of the complex environment. In this paper, a sound methodology is proposed for pixel-wise classification on images acquired by the Azersky (SPOT-7) optical satellite. In particular, customized variations of U-Net-like architectures are employed to identify greenhouses. Two models are proposed which uniquely incorporate dilated convolutions and skip connections, and the results are compared to that of the baseline U-Net model. The dataset used consists of pan-sharpened orthorectified Azersky images (red, green, blue,and near infrared channels) with 1.5-meter resolution and annotation masks, collected from 15 regions in Azerbaijan where the greenhouses are densely congested. The images cover the cumulative area of 1008 $km^2$ and annotation masks contain 47559 polygons in total. The $F_1, Kappa, AUC$, and $IOU$ scores are used for performance evaluation. It is observed that the use of the deconvolutional layers alone throughout the expansive path does not yield satisfactory results; therefore, they are either replaced or coupled with bilinear interpolation. All models benefit from the hard example mining (HEM) strategy. It is also reported that the best accuracy of $93.29\%$ ($F_1\,score$) is recorded when the weighted binary cross-entropy loss is coupled with the dice loss. Experimental results showed that both of the proposed models outperformed the baseline U-Net architecture such that the best model proposed scored $4.48\%$ higher in comparison to the baseline architecture.      
### 44.Regulating human control over autonomous systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.11218.pdf)
>  In recent years, many sectors have experienced significant progress in automation, associated with the growing advances in artificial intelligence and machine learning. There are already automated robotic weapons, which are able to evaluate and engage with targets on their own, and there are already autonomous vehicles that do not need a human driver. It is argued that the use of increasingly autonomous systems (AS) should be guided by the policy of human control, according to which humans should execute a certain significant level of judgment over AS. While in the military sector there is a fear that AS could mean that humans lose control over life and death decisions, in the transportation domain, on the contrary, there is a strongly held view that autonomy could bring significant operational benefits by removing the need for a human driver. This article explores the notion of human control in the United States in the two domains of defense and transportation. The operationalization of emerging policies of human control results in the typology of direct and indirect human controls exercised over the use of AS. The typology helps to steer the debate away from the linguistic complexities of the term autonomy. It identifies instead where human factors are undergoing important changes and ultimately informs about more detailed rules and standards formulation, which differ across domains, applications, and sectors.      
### 45.Rethinking CNN Models for Audio Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.11154.pdf)
>  In this paper, we show that ImageNet-Pretrained standard deep CNN models can be used as strong baseline networks for audio classification. Even though there is a significant difference between audio Spectrogram and standard ImageNet image samples, transfer learning assumptions still hold firmly. To understand what enables the ImageNet pretrained models to learn useful audio representations, we systematically study how much of pretrained weights is useful for learning spectrograms. We show (1) that for a given standard model using pretrained weights is better than using randomly initialized weights (2) qualitative results of what the CNNs learn from the spectrograms by visualizing the gradients. Besides, we show that even though we use the pretrained model weights for initialization, there is variance in performance in various output runs of the same model. This variance in performance is due to the random initialization of linear classification layer and random mini-batch orderings in multiple runs. This brings significant diversity to build stronger ensemble models with an overall improvement in accuracy. An ensemble of ImageNet pretrained DenseNet achieves 92.89% validation accuracy on the ESC-50 dataset and 87.42% validation accuracy on the UrbanSound8K dataset which is the current state-of-the-art on both of these datasets.      
### 46.Accelerating Deep Learning Applications in Space  [ :arrow_down: ](https://arxiv.org/pdf/2007.11089.pdf)
>  Computing at the edge offers intriguing possibilities for the development of autonomy and artificial intelligence. The advancements in autonomous technologies and the resurgence of computer vision have led to a rise in demand for fast and reliable deep learning applications. In recent years, the industry has introduced devices with impressive processing power to perform various object detection tasks. However, with real-time detection, devices are constrained in memory, computational capacity, and power, which may compromise the overall performance. This could be solved either by optimizing the object detector or modifying the images. In this paper, we investigate the performance of CNN-based object detectors on constrained devices when applying different image compression techniques. We examine the capabilities of a NVIDIA Jetson Nano; a low-power, high-performance computer, with an integrated GPU, small enough to fit on-board a CubeSat. We take a closer look at the Single Shot MultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN) that are pre-trained on DOTA - a Large Scale Dataset for Object Detection in Aerial Images. The performance is measured in terms of inference time, memory consumption, and accuracy. By applying image compression techniques, we are able to optimize performance. The two techniques applied, lossless compression and image scaling, improves speed and memory consumption with no or little change in accuracy. The image scaling technique achieves a 100% runnable dataset and we suggest combining both techniques in order to optimize the speed/memory/accuracy trade-off.      
### 47.Converse Barrier Functions via Lyapunov Functions  [ :arrow_down: ](https://arxiv.org/pdf/2007.11086.pdf)
>  We prove a robust converse barrier function theorem via the converse Lyapunov theory. While the use of a Lyapunov function as a barrier function is straightforward, the existence of a converse Lyapunov function as a barrier function for a given safety set is not. We establish this link by a robustness argument. We show that the closure of the forward reachable set of a robustly safe set must be robustly asymptotically stable under mild technical assumptions. As a result, all robustly safe dynamical systems must admit a robust barrier function in the form of a Lyapunov function for set stability. We present the results in both continuous-time and discrete-time settings and remark on connections with various barrier function conditions.      
### 48.Towards Quantum Belief Propagation for LDPC Decoding in Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.11069.pdf)
>  We present Quantum Belief Propagation (QBP), a Quantum Annealing (QA) based decoder design for Low Density Parity Check (LDPC) error control codes, which have found many useful applications in Wi-Fi, satellite communications, mobile cellular systems, and data storage systems. QBP reduces the LDPC decoding to a discrete optimization problem, then embeds that reduced design onto quantum annealing hardware. QBP's embedding design can support LDPC codes of block length up to 420 bits on real state-of-the-art QA hardware with 2,048 qubits. We evaluate performance on real quantum annealer hardware, performing sensitivity analyses on a variety of parameter settings. Our design achieves a bit error rate of $10^{-8}$ in 20 $\mu$s and a 1,500 byte frame error rate of $10^{-6}$ in 50 $\mu$s at SNR 9 dB over a Gaussian noise wireless channel. Further experiments measure performance over real-world wireless channels, requiring 30 $\mu$s to achieve a 1,500 byte 99.99$\%$ frame delivery rate at SNR 15-20 dB. QBP achieves a performance improvement over an FPGA based soft belief propagation LDPC decoder, by reaching a bit error rate of $10^{-8}$ and a frame error rate of $10^{-6}$ at an SNR 2.5--3.5 dB lower. In terms of limitations, QBP currently cannot realize practical protocol-sized ($\textit{e.g.,}$ Wi-Fi, WiMax) LDPC codes on current QA processors. Our further studies in this work present future cost, throughput, and QA hardware trend considerations.      
### 49.Spectral estimation from simulations via sketching  [ :arrow_down: ](https://arxiv.org/pdf/2007.11026.pdf)
>  Sketching is a stochastic dimension reduction method that preserves geometric structures of data and has applications in high-dimensional regression, low rank approximation and graph sparsification. In this work, we show that sketching can be used to compress simulation data and still accurately estimate time autocorrelation and power spectral density. For a given compression ratio, the accuracy is much higher than using previously known methods. In addition to providing theoretical guarantees, we apply sketching to a molecular dynamics simulation of methanol and find that the estimate of spectral density is 90% accurate using only 10% of the data.      
