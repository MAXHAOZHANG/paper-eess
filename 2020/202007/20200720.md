# ArXiv eess --Mon, 20 Jul 2020
### 1.SkipConvNet: Skip Convolutional Neural Network for Speech Dereverberation using Optimally Smoothed Spectral Mapping  [ :arrow_down: ](https://arxiv.org/pdf/2007.09131.pdf)
>  The reliability of using fully convolutional networks (FCNs) has been successfully demonstrated by recent studies in many speech applications. One of the most popular variants of these FCNs is the `U-Net', which is an encoder-decoder network with skip connections. In this study, we propose `SkipConvNet' where we replace each skip connection with multiple convolutional modules to provide decoder with intuitive feature maps rather than encoder's output to improve the learning capacity of the network. We also propose the use of optimal smoothing of power spectral density (PSD) as a pre-processing step, which helps to further enhance the efficiency of the network. To evaluate our proposed system, we use the REVERB challenge corpus to assess the performance of various enhancement approaches under the same conditions. We focus solely on monitoring improvements in speech quality and their contribution to improving the efficiency of back-end speech systems, such as speech recognition and speaker verification, trained on only clean speech. Experimental findings show that the proposed system consistently outperforms other approaches.      
### 2.CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.09127.pdf)
>  Recent end-to-end Automatic Speech Recognition (ASR) systems demonstrated the ability to outperform conventional hybrid DNN/ HMM ASR. Aside from architectural improvements in those systems, those models grew in terms of depth, parameters and model capacity. However, these models also require more training data to achieve comparable performance. <br>In this work, we combine freely available corpora for German speech recognition, including yet unlabeled speech data, to a big dataset of over $1700$h of speech data. For data preparation, we propose a two-stage approach that uses an ASR model pre-trained with Connectionist Temporal Classification (CTC) to boot-strap more training data from unsegmented or unlabeled training data. Utterances are then extracted from label probabilities obtained from the network trained with CTC to determine segment alignments. With this training data, we trained a hybrid CTC/attention Transformer model that achieves $12.8\%$ WER on the Tuda-DE test set, surpassing the previous baseline of $14.4\%$ of conventional hybrid DNN/HMM ASR.      
### 3.Breaking Moravec's Paradox: Visual-Based Distribution in Smart Fashion Retail  [ :arrow_down: ](https://arxiv.org/pdf/2007.09102.pdf)
>  In this paper, we report an industry-academia collaborative study on the distribution method of fashion products using an artificial intelligence (AI) technique combined with an optimization method. To meet the current fashion trend of short product lifetimes and an increasing variety of styles, the company produces limited volumes of a large variety of styles. However, due to the limited volume of each style, some styles may not be distributed to some off-line stores. As a result, this high-variety, low-volume strategy presents another challenge to distribution managers. We collaborated with KOLON F/C, one of the largest fashion business units in South Korea, to develop models and an algorithm to optimally distribute the products to the stores based on the visual images of the products. The team developed a deep learning model that effectively represents the styles of clothes based on their visual image. Moreover, the team created an optimization model that effectively determines the product mix for each store based on the image representation of clothes. In the past, computers were only considered to be useful for conducting logical calculations, and visual perception and cognition were considered to be difficult computational tasks. The proposed approach is significant in that it uses both AI (perception and cognition) and mathematical optimization (logical calculation) to address a practical supply chain problem, which is why the study was called "Breaking Moravec's Paradox."      
### 4.Reinforcement Learning-Enabled Decision-Making Strategies for a Vehicle-Cyber-Physical-System in Connected Environment  [ :arrow_down: ](https://arxiv.org/pdf/2007.09101.pdf)
>  As a typical vehicle-cyber-physical-system (V-CPS), connected automated vehicles attracted more and more attention in recent years. This paper focuses on discussing the decision-making (DM) strategy for autonomous vehicles in a connected environment. First, the highway DM problem is formulated, wherein the vehicles can exchange information via wireless networking. Then, two classical reinforcement learning (RL) algorithms, Q-learning and Dyna, are leveraged to derive the DM strategies in a predefined driving scenario. Finally, the control performance of the derived DM policies in safety and efficiency is analyzed. Furthermore, the inherent differences of the RL algorithms are embodied and discussed in DM strategies.      
### 5.Similarity quantification for linear stochastic systems as a set-theoretic control problem  [ :arrow_down: ](https://arxiv.org/pdf/2007.09052.pdf)
>  For the formal verification and design of control systems, abstractions with quantified accuracy are crucial. Such similarity quantification is hindered by the challenging computation of approximate stochastic simulation relations. This is especially the case when considering accurate deviation bounds between a stochastic continuous-state model and its finite-state abstraction. In this work, we give a comprehensive computational approach and analysis for linear stochastic systems. More precisely, we develop a computational method that characterizes the set of possible simulation relations and optimally trades off the error contributions on the system's output with deviations in the transition probability. To this end, we establish an optimal coupling between the models and simultaneously solve the approximate simulation relation problem as a set-theoretic control problem using the concept of invariant sets. We show the variation of the guaranteed satisfaction probability as a function of the error trade-off in a case study where a formal specification is given as a temporal logic formula.      
### 6.Estimation and uncertainty quantification for piecewise smooth signal recovery  [ :arrow_down: ](https://arxiv.org/pdf/2007.08989.pdf)
>  This paper presents a sparse Bayesian learning (SBL) algorithm for linear inverse problems with a high order total variation (HOTV) sparsity prior. For the problem of sparse signal recovery, SBL often produces more accurate estimates than maximum a posteriori estimates, including those that rely on l1 regularization. Moreover, rather than a single signal estimate, SBL yields a full posterior density estimate which can be used for uncertainty quantification. However, SBL is only immediately applicable to problems having a direct sparsity prior, or to those that can be formed via synthesis. This paper demonstrates how a problem with an HOTV sparsity prior can be formulated via synthesis, and then develops a corresponding Bayesian learning method. This expands the class of problems available to Bayesian learning to include, e.g., inverse problems dealing with the recovery of piecewise smooth functions or signals from data. Numerical examples are provided to demonstrate how this new technique is effectively employed.      
### 7.Low Complexity Reconfigurable Modified FRM Architecture with Full Spectral Utilization for Efficient Channelizers  [ :arrow_down: ](https://arxiv.org/pdf/2007.08928.pdf)
>  This paper proposes a design of low complexity, reconfigurable and narrow transition band (NTB) filter bank (FB). In our proposed Modified Frequency Response Masking (ModFRM) architecture, the modal filter and complementary filter in conventional FRM approach are replaced by a power complementary and linear phase FB. Additionally, a new masking strategy is proposed by which an M-channel FB can be designed by alternately masking even channels and odd channels. By using this masking strategy, it is only necessary to alternately mask even channels and odd channels by the two masking filters while modulating them over the multiple spectra replicas appearing in [0,2$\pi$] to generate uniform ModFRM FB. Also, reconfigurability of filter bank can be achieved by adjusting the interpolation values appropriately, to obtain more masking responses. To reduce the overall implementation complexity, masking filters are optimized using the interpolated FIR (IFIR) technique. The results indicate that the proposed method requires substantially less multipliers in comparison to the reconfigurable FB existing in literature. Finally, non-uniform FB are generated from the uniform FB by combining nearby channels. The proposed non-uniform ModFRM FB is used for the extraction of different communication standards in the software defined radio (SDR) channelizer. When more channels are to be extracted, the proposed scheme was able to achieve a reduced hardware complexity in comparison to other filter bank based SDR channelizers.      
### 8.Initialization of a Disease Transmission Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.08925.pdf)
>  Approaches to the calculation of the full state vector of a larger epidemiological model for the spread of COVID-19 in Sweden at the initial time instant from available data and with a simplified dynamical model are proposed and evaluated. The larger epidemiological model is based on a continuous Markov chain and captures the demographic composition of and the transport flows between the counties of Sweden. Its intended use is to predict the outbreak development in temporal and spatial coordinates as well as across the demographic groups. It can also support evaluating and comparing of prospective intervention strategies in terms of e.g. lockdown in certain areas or isolation of specific age groups. The simplified model is a discrete time-invariant linear system that has cumulative infectious incidence, infected population, asymptomatic population, exposed population, and infectious pressure as the state variables. Since the system matrix of the model depends on a number transition rates, structural properties of the model are investigated for suitable parameter ranges. It is concluded that the model becomes unobservable for some parameter values. Two contrasting approaches to the initial state estimation are considered. One is a version of Rauch-Tung-Striebel smoother and another is based on solving a batch nonlinear optimization problem. The benefits and shortcomings of the considered estimation techniques are analyzed and compared on synthetic data for several Swedish counties.      
### 9.Can Learned Frame-Prediction Compete with Block-Motion Compensation for Video Coding?  [ :arrow_down: ](https://arxiv.org/pdf/2007.08922.pdf)
>  Given recent advances in learned video prediction, we investigate whether a simple video codec using a pre-trained deep model for next frame prediction based on previously encoded/decoded frames without sending any motion side information can compete with standard video codecs based on block-motion compensation. Frame differences given learned frame predictions are encoded by a standard still-image (intra) codec. Experimental results show that the rate-distortion performance of the simple codec with symmetric complexity is on average better than that of x264 codec on 10 MPEG test videos, but does not yet reach the level of x265 codec. This result demonstrates the power of learned frame prediction (LFP), since unlike motion compensation, LFP does not use information from the current picture. The implications of training with L1, L2, or combined L2 and adversarial loss on prediction performance and compression efficiency are analyzed.      
### 10.Revisiting Rubik's Cube: Self-supervised Learning with Volume-wise Transformation for 3D Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.08826.pdf)
>  Deep learning highly relies on the quantity of annotated data. However, the annotations for 3D volumetric medical data require experienced physicians to spend hours or even days for investigation. Self-supervised learning is a potential solution to get rid of the strong requirement of training data by deeply exploiting raw data information. In this paper, we propose a novel self-supervised learning framework for volumetric medical images. Specifically, we propose a context restoration task, i.e., Rubik's cube++, to pre-train 3D neural networks. Different from the existing context-restoration-based approaches, we adopt a volume-wise transformation for context permutation, which encourages network to better exploit the inherent 3D anatomical information of organs. Compared to the strategy of training from scratch, fine-tuning from the Rubik's cube++ pre-trained weight can achieve better performance in various tasks such as pancreas segmentation and brain tissue segmentation. The experimental results show that our self-supervised learning method can significantly improve the accuracy of 3D deep learning networks on volumetric medical datasets without the use of extra data.      
### 11.Neural Architecture Search for Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.08818.pdf)
>  Deep neural networks (DNNs) based automatic speech recognition (ASR) systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search (NAS) techniques are used to automatically learn two hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network (TDNN-F) acoustic models: i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the standard DARTS method fully integrating the estimation of architecture weights and TDNN parameters in lattice-free MMI (LF-MMI) training; Gumbel-Softmax DARTS that reduces the confusion between candidate architectures; Pipelined DARTS that circumvents the overfitting of architecture weights using held-out data; and Penalized DARTS that further incorporates resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures was also used to facilitate efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on a 300-hour Switchboard conversational telephone speech recognition task suggest the NAS auto-configured TDNN-F systems consistently outperform the baseline LF-MMI trained TDNN-F systems using manual expert configurations. Absolute word error rate reductions up to 1.0% and relative model size reduction of 28% were obtained.      
### 12.Spatial Resolution Enhancement of Remote Sensing Mine Images using Deep Learning Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2007.08791.pdf)
>  Deep learning techniques are applied so as to increase the spatial resolution of Sentinel2 satellite imagery, depicting the Amynteo lignite mine in Ptolemaida, Greece. Resolution enhancement by factors 2 and 4 as well as by factors 2 and 6 using Very-Deep SuperResolution (VDSR) and DSen2 networks, respectively, provides fairly well results on Amynteo lignite mine images.      
### 13.High Altitude Platform Station based Super Macro Base Station (HAPS-SMBS) Constellations  [ :arrow_down: ](https://arxiv.org/pdf/2007.08747.pdf)
>  High altitude platform station (HAPS) systems have recently attracted renewed attention. While terrestrial and satellite technologies are well-established for providing connectivity services, they face certain shortcomings and challenges, which could be addressed by complementing them with HAPS systems. In this paper, we envision a HAPS as a super macro base station, which we refer to as HAPS-SMBS, to provide connectivity in a plethora of applications. Unlike conventional HAPS, which targets remote areas or disaster recovery, we envision next-generation HAPS-SMBS to have the necessary capabilities to address the high capacity, broad coverage, low latency, and computing requirements especially for highly populated metropolitan areas. This article focuses mainly on the potential opportunities, target use cases, and challenges that we expect to be associated with the design and implementation of the HAPS-SMBS based future wireless access architecture.      
### 14.Channel-wise Autoregressive Entropy Models for Learned Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2007.08739.pdf)
>  In learning-based approaches to image compression, codecs are developed by optimizing a computational model to minimize a rate-distortion objective. Currently, the most effective learned image codecs take the form of an entropy-constrained autoencoder with an entropy model that uses both forward and backward adaptation. Forward adaptation makes use of side information and can be efficiently integrated into a deep neural network. In contrast, backward adaptation typically makes predictions based on the causal context of each symbol, which requires serial processing that prevents efficient GPU / TPU utilization. We introduce two enhancements, channel-conditioning and latent residual prediction, that lead to network architectures with better rate-distortion performance than existing context-adaptive models while minimizing serial processing. Empirically, we see an average rate savings of 6.7% on the Kodak image set and 11.4% on the Tecnick image set compared to a context-adaptive baseline model. At low bit rates, where the improvements are most effective, our model saves up to 18% over the baseline and outperforms hand-engineered codecs like BPG by up to 25%.      
### 15.True-Time-Delay Arrays for Fast Beam Training in Wideband Millimeter-Wave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.08713.pdf)
>  The best beam steering directions are estimated through beam training, which is one of the most important and challenging tasks in millimeter-wave and sub-terahertz communications. Novel array architectures and signal processing techniques are required to avoid prohibitive beam training overhead associated with large antenna arrays and narrow beams. In this work, we leverage recent developments in true-time-delay (TTD) arrays with large delay-bandwidth products to accelerate beam training using frequency-dependent probing beams. We propose and study two TTD architecture candidates, including analog and hybrid analog-digital arrays, that can facilitate beam training with only one wideband pilot. We also propose a suitable algorithm that requires a single pilot to achieve high-accuracy estimation of angle of arrival. The proposed array architectures are compared in terms of beam training requirements and performance, robustness to practical hardware impairments, and power consumption. The findings suggest that the analog and hybrid TTD arrays achieve a sub-degree beam alignment precision with 66% and 25% lower power consumption than a fully digital array, respectively. Our results yield important design trade-offs among the basic system parameters, power consumption, and accuracy of angle of arrival estimation in fast TTD beam training.      
### 16.Decision-making Strategy on Highway for Autonomous Vehicles using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.08691.pdf)
>  Autonomous driving is a promising technology to reduce traffic accidents and improve driving efficiency. In this work, a deep reinforcement learning (DRL)-enabled decision-making policy is constructed for autonomous vehicles to address the overtaking behaviors on the highway. First, a highway driving environment is founded, wherein the ego vehicle aims to pass through the surrounding vehicles with an efficient and safe maneuver. A hierarchical control framework is presented to control these vehicles, which indicates the upper-level manages the driving decisions, and the lower-level cares about the supervision of vehicle speed and acceleration. Then, the particular DRL method named dueling deep Q-network (DDQN) algorithm is applied to derive the highway decision-making strategy. The exhaustive calculative procedures of deep Q-network and DDQN algorithms are discussed and compared. Finally, a series of estimation simulation experiments are conducted to evaluate the effectiveness of the proposed highway decision-making policy. The advantages of the proposed framework in convergence rate and control performance are illuminated. Simulation results reveal that the DDQN-based overtaking policy could accomplish highway driving tasks efficiently and safely.      
### 17.Transfer Deep Reinforcement Learning-enabled Energy Management Strategy for Hybrid Tracked Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2007.08690.pdf)
>  This paper proposes an adaptive energy management strategy for hybrid electric vehicles by combining deep reinforcement learning (DRL) and transfer learning (TL). This work aims to address the defect of DRL in tedious training time. First, an optimization control modeling of a hybrid tracked vehicle is built, wherein the elaborate powertrain components are introduced. Then, a bi-level control framework is constructed to derive the energy management strategies (EMSs). The upper-level is applying the particular deep deterministic policy gradient (DDPG) algorithms for EMS training at different speed intervals. The lower-level is employing the TL method to transform the pre-trained neural networks for a novel driving cycle. Finally, a series of experiments are executed to prove the effectiveness of the presented control framework. The optimality and adaptability of the formulated EMS are illuminated. The founded DRL and TL-enabled control policy is capable of enhancing energy efficiency and improving system performance.      
### 18.Deep Small Bowel Segmentation with Cylindrical Topological Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2007.08674.pdf)
>  We present a novel method for small bowel segmentation where a cylindrical topological constraint based on persistent homology is applied. To address the touching issue which could break the applied constraint, we propose to augment a network with an additional branch to predict an inner cylinder of the small bowel. Since the inner cylinder is free of the touching issue, a cylindrical shape constraint applied on this augmented branch guides the network to generate a topologically correct segmentation. For strict evaluation, we achieved an abdominal computed tomography dataset with dense segmentation ground-truths. The proposed method showed clear improvements in terms of four different metrics compared to the baseline method, and also showed the statistical significance from a paired t-test.      
### 19.Super-Resolution Remote Imaging using Time Encoded Remote Apertures  [ :arrow_down: ](https://arxiv.org/pdf/2007.08667.pdf)
>  Imaging of scenes using light or other wave phenomena is subject to the diffraction limit. The spatial profile of a wave propagating between a scene and the imaging system is distorted by diffraction resulting in a loss of resolution that is proportional with traveled distance. We show here that it is possible to reconstruct sparse scenes from the temporal profile of the wave-front using only one spatial pixel or a spatial average. The temporal profile of the wave is not affected by diffraction yielding an imaging method that can in theory achieve wavelength scale resolution independent of distance from the scene.      
### 20.On Mitigating the Uncertainty in Renewable Generation in Distribution Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2007.08641.pdf)
>  In this article, we focus on the problem of mitigating the risk of not being able to meet the power demand, due to the inherent uncertainty of renewable energy generation sources in microgrids. We consider three different demand scenarios, namely meeting short-time horizon power demand, a sustained energy demand and a scenario where the power demand at a prescribed future time has to be met with almost sure guarantee with power generation being stochastic and following dynamics governed by geometric Brownian motion. For each of these scenarios we provide solutions to meet the electrical demand. We present results of numerical experiments to demonstrate the applicability of our schemes.      
### 21.COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.08637.pdf)
>  Background and Objective: COVID-19 outbreak was declared as a pandemic on 11th March 2020. The rapid spread of this highly infectious virus has distinguished it from other classes of viral and respiratory diseases. The reverse transcription-polymerase chain reaction (RT-PCR) test is most commonly used for the qualitative assessment of the presence of SARS-CoV-2. Due to the high false-negative rate of RT-PCR tests reported worldwide, chest x-ray imaging has proved to be a feasible alternative for the detection of COVID-19. The COV-ELM classifier aims to classify COVID-19 cases from the chest x-ray images using extreme learning machine (ELM). The choice of ELM in this work is based on the fact that ELM significantly shortens the training time with the least interventions required to tune the networks as compared to other neural networks. Methods: The proposed work is experimented on the COVID-19 chest x-ray (CXR) image data collected from three publicly available sources. The image data is preprocessed and local patterns are extracted by exploiting the frequency and texture regions to generate a feature pool. This pool of features is provided as an input to the ELM and a 10-fold cross-validation method is employed to evaluate the proposed model. Results: The proposed method achieved a macro average of f1-score is 0.95 in a three-class classification scenario. The overall sensitivity of the COV-ELM classifier is ${0.94 \pm 0.03}$ at 95% confidence interval. Conclusions: The COV-ELM outperforms other competitive machine learning algorithms in a multi-class classification scenario. The results of COV-ELM are quite promising which increases its suitability to be applied to bigger and more diverse datasets.      
### 22.Dynamic Low-light Imaging with Quanta Image Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2007.08614.pdf)
>  Imaging in low light is difficult because the number of photons arriving at the sensor is low. Imaging dynamic scenes in low-light environments is even more difficult because as the scene moves, pixels in adjacent frames need to be aligned before they can be denoised. Conventional CMOS image sensors (CIS) are at a particular disadvantage in dynamic low-light settings because the exposure cannot be too short lest the read noise overwhelms the signal. We propose a solution using Quanta Image Sensors (QIS) and present a new image reconstruction algorithm. QIS are single-photon image sensors with photon counting capabilities. Studies over the past decade have confirmed the effectiveness of QIS for low-light imaging but reconstruction algorithms for dynamic scenes in low light remain an open problem. We fill the gap by proposing a student-teacher training protocol that transfers knowledge from a motion teacher and a denoising teacher to a student network. We show that dynamic scenes can be reconstructed from a burst of frames at a photon level of 1 photon per pixel per frame. Experimental results confirm the advantages of the proposed method compared to existing methods.      
### 23.Real-time Framework for Trust Monitoring in aNetwork of Unmanned Aerial Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2007.08590.pdf)
>  Unmanned aerial vehicles (UAVs) have been increasingly utilized in various civilian and military applications such as remote sensing, border patrolling, disaster monitoring, and communication coverage extension. However, there are still prone to several cyber attacks such as GPS spoofing attacks, distributed denial-of-service (DDoS) attacks, and man-in-the-middle attacks to obtain their collected information or to enforce the UAVs to perform their requested actions which may damage the UAVs or their surrounding environment or even endanger the safety of human in the operation field. In this paper, we propose a trust monitoring mechanism in which a centralized unit (e.g. the ground station) regularly observe the behavior of the UAVs in terms of their motion path, their consumed energy, as well as the number of their completed tasks and measure a relative trust score for the UAVs to detect any abnormal behaviors in a real-time manner. Our simulation results show that the trust model can detect malicious UAVs, which can be under various cyber-security attacks such as flooding attacks, man-in-the-middle attacks, GPS spoofing attack in real-time.      
### 24.Lyapunov Analysis of Least Squares Based Direct Adaptive Control  [ :arrow_down: ](https://arxiv.org/pdf/2007.08578.pdf)
>  Adaptive control studies mostly utilize gradient based parameter estimators for convenience in Lyapunov analysis based constructive design. However, simulations and real-time experiments reveal that, compared to gradient based online parameter identifiers, least squares (LS) based parameter identifiers, with proper selection of design parameters, exhibit better transient performance from the aspects of speed of convergence and robustness to measurement noise. The existing literature on LS based adaptive control mostly follow the indirect adaptive control approach as opposed to the direct one, because of the difficulty in integrating an LS based adaptive law within the direct approaches starting with a certain Lyapunov-like cost function to be driven to (a neighborhood) of zero. In this paper, a formal constructive analysis framework for integration of recursive LS (RLS) based estimation to direct adaptive control is proposed following the typical steps for gradient adaptive law based direct model reference adaptive control, but constructing a new Lyapunov-like function for the analysis. Adaptive cruise control simulation application is studied in Matlab/Simulink and CarSim.      
### 25.Quantum Entanglement and Cryptography for Automation and Control of Dynamic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.08567.pdf)
>  This paper addresses the application of quantum entanglement and cryptography for automation and control of dynamic systems. A dynamic system is a system where the rates of changes of its state variables are not negligible. Quantum entanglement is realized by the Spontaneous Parametric Down-conversion process. Two entangled autonomous systems exhibit correlated behavior without any classical communication in between them due to the quantum entanglement phenomenon. Specifically, the behavior of a system, Bob, at a distance, is correlated with a corresponding system, Alice. In an automation scenario, the 'Bob Robot' is entangled with the 'Alice Robot' in performing autonomous tasks without any classical connection between them. Quantum cryptography is a capability that allows guaranteed security. Such capabilities can be implemented in control of autonomous mechanical systems where, for instance, an 'Alice Autonomous System' can control a 'Bob Autonomous System' for applications of automation and robotics. The applications of quantum technologies to mechanical systems, at a scale larger than the atomistic scale, for control and automation, is a novel contribution of this paper. Notably, the feedback control transfer function of an integrated classical dynamic system and a quantum state is proposed.      
### 26.Analytical Derivation of Outage Correlation in Random Media Access with Application to Average Consensus in Wireless Networks: Extended Paper Version  [ :arrow_down: ](https://arxiv.org/pdf/2007.09120.pdf)
>  We study a finite and fixed relative formation of possibly mobile wireless networked nodes. The nodes apply average consensus to agree on a common value like the formation's center. %Performance of consensus in terms of convergence speed is affected by message losses due path loss and interference. We assume framed slotted ALOHA based broadcast communication. Our work has two contributions. First, we analyze outage correlation of random media access in wireless networks under Nakagami fading. Second, the correlation terms are applied to the so called L2-joint spectral and numerical radii to analyze convergence speed of average consensus under wireless broadcast communication. This yields a unified framework for studying joint optimization of control and network parameters for consensus subject to message losses in wireless communications. Exemplary we show in this work how far outage correlation in wireless broadcast communication positively affects convergence speed of average consensus compared to consensus in the uncorrelated case.      
### 27.Trajectory Design and Power Allocation for Drone-Assisted NR-V2X Network with Dynamic NOMA/OMA  [ :arrow_down: ](https://arxiv.org/pdf/2007.09097.pdf)
>  In this paper, we find trajectory planning and power allocation for a vehicular network in which an unmanned-aerial-vehicle (UAV) is considered as a relay to extend coverage for two disconnected far vehicles. We show that in a two-user network with an amplify-and-forward (AF) relay, non-orthogonal-multiple-access (NOMA) always has better or equal sum-rate in comparison to orthogonal-multiple-access (OMA) at high signal-to-noise-ratio (SNR) regime. However, for the cases where i) base station (BS)-to-relay link is weak, or ii) two users have similar links, or iii) BS-to-relay link is similar to relay-to-weak user link, applying NOMA has negligible sum-rate gain. Hence, due to the complexity of successive-interference-cancellation (SIC) decoding in NOMA, we propose a dynamic NOMA/OMA scheme in which OMA mode is selected for transmission when applying NOMA has only negligible gain. Also, we show that OMA always has better min-rate than NOMA at high SNR regime. Further, we formulate two optimization problems which maximize the sum-rate and min-rate of the two vehicles. These problems are non-convex, and hence we propose an iterative algorithm based on alternating-optimization (AO) method which solves trajectory and power allocation sub-problems by successive-convex-approximation (SCA) and difference-of-convex (DC) methods, respectively. Finally, the above-mentioned performance is confirmed by simulations.      
### 28.Standing on the Shoulders of Giants: Hardware and Neural Architecture Co-Search with Hot Start  [ :arrow_down: ](https://arxiv.org/pdf/2007.09087.pdf)
>  Hardware and neural architecture co-search that automatically generates Artificial Intelligence (AI) solutions from a given dataset is promising to promote AI democratization; however, the amount of time that is required by current co-search frameworks is in the order of hundreds of GPU hours for one target hardware. This inhibits the use of such frameworks on commodity hardware. The root cause of the low efficiency in existing co-search frameworks is the fact that they start from a "cold" state (i.e., search from scratch). In this paper, we propose a novel framework, namely HotNAS, that starts from a "hot" state based on a set of existing pre-trained models (a.k.a. model zoo) to avoid lengthy training time. As such, the search time can be reduced from 200 GPU hours to less than 3 GPU hours. In HotNAS, in addition to hardware design space and neural architecture search space, we further integrate a compression space to conduct model compressing during the co-search, which creates new opportunities to reduce latency but also brings challenges. One of the key challenges is that all of the above search spaces are coupled with each other, e.g., compression may not work without hardware design support. To tackle this issue, HotNAS builds a chain of tools to design hardware to support compression, based on which a global optimizer is developed to automatically co-search all the involved search spaces. Experiments on ImageNet dataset and Xilinx FPGA show that, within the timing constraint of 5ms, neural architectures generated by HotNAS can achieve up to 5.79% Top-1 and 3.97% Top-5 accuracy gain, compared with the existing ones.      
### 29.Generating Person Images with Appearance-aware Pose Stylizer  [ :arrow_down: ](https://arxiv.org/pdf/2007.09077.pdf)
>  Generation of high-quality person images is challenging, due to the sophisticated entanglements among image factors, e.g., appearance, pose, foreground, background, local details, global structures, etc. In this paper, we present a novel end-to-end framework to generate realistic person images based on given person poses and appearances. The core of our framework is a novel generator called Appearance-aware Pose Stylizer (APS) which generates human images by coupling the target pose with the conditioned person appearance progressively. The framework is highly flexible and controllable by effectively decoupling various complex person image factors in the encoding phase, followed by re-coupling them in the decoding phase. In addition, we present a new normalization method named adaptive patch normalization, which enables region-specific normalization and shows a good performance when adopted in person image generation model. Experiments on two benchmark datasets show that our method is capable of generating visually appealing and realistic-looking results using arbitrary image and pose inputs.      
### 30.GMNet: Graph Matching Network for Large Scale Part Semantic Segmentation in the Wild  [ :arrow_down: ](https://arxiv.org/pdf/2007.09073.pdf)
>  The semantic segmentation of parts of objects in the wild is a challenging task in which multiple instances of objects and multiple parts within those objects must be detected in the scene. This problem remains nowadays very marginally explored, despite its fundamental importance towards detailed object understanding. In this work, we propose a novel framework combining higher object-level context conditioning and part-level spatial relationships to address the task. To tackle object-level ambiguity, a class-conditioning module is introduced to retain class-level semantics when learning parts-level semantics. In this way, mid-level features carry also this information prior to the decoding stage. To tackle part-level ambiguity and localization we propose a novel adjacency graph-based module that aims at matching the relative spatial relationships between ground truth and predicted parts. The experimental evaluation on the Pascal-Part dataset shows that we achieve state-of-the-art results on this task.      
### 31.Joint Multi-User DNN Partitioning and Computational Resource Allocation for Collaborative Edge Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2007.09072.pdf)
>  Mobile Edge Computing (MEC) has emerged as a promising supporting architecture providing a variety of resources to the network edge, thus acting as an enabler for edge intelligence services empowering massive mobile and Internet of Things (IoT) devices with AI capability. With the assistance of edge servers, user equipments (UEs) are able to run deep neural network (DNN) based AI applications, which are generally resource-hungry and compute-intensive, such that an individual UE can hardly afford by itself in real time. However the resources in each individual edge server are typically limited. Therefore, any resource optimization involving edge servers is by nature a resource-constrained optimization problem and needs to be tackled in such realistic context. Motivated by this observation, we investigate the optimization problem of DNN partitioning (an emerging DNN offloading scheme) in a realistic multi-user resource-constrained condition that rarely considered in previous works. Despite the extremely large solution space, we reveal several properties of this specific optimization problem of joint multi-UE DNN partitioning and computational resource allocation. We propose an algorithm called Iterative Alternating Optimization (IAO) that can achieve the optimal solution in polynomial time. In addition, we present rigorous theoretic analysis of our algorithm in terms of time complexity and performance under realistic estimation error. Moreover, we build a prototype that implements our framework and conduct extensive experiments using realistic DNN models, whose results demonstrate its effectiveness and efficiency.      
### 32.Unsupervised Representation Learning For Context of Vocal Music  [ :arrow_down: ](https://arxiv.org/pdf/2007.09060.pdf)
>  In this paper we aim to learn meaningful representations of sung intonation patterns derived from surrounding data without supervision. We focus on two facets of context in which a vocal line is produced: 1) within the short-time context of contiguous vocalizations, and 2) within the larger context of a recording. We propose two unsupervised deep learning methods, pseudo-task learning and slot filling, to produce latent encodings of these con-textual representations. To evaluate the quality of these representations and their usefulness as meaningful feature space, we conduct classification tasks on recordings sung by both professional and amateur singers. Initial results indicate that the learned representations enhance the performance of downstream classification tasks by several points, as compared to learning directly from the intonation contours alone. Larger increases in performance on classification of technique and vocal phrase patterns suggest that the representations encode short-time temporal context learned directly from the original recordings. Additionally, their ability to improve singer and gender identification suggest the learning of more broad contextual pat-terns. The growing availability of large unlabeled datasets makes this idea of contextual representation learning additionally promising, with larger amounts of meaningful samples often yielding better performance      
### 33.Lightning optimizes: a threshold mechanism ensures minimum-path flow  [ :arrow_down: ](https://arxiv.org/pdf/2007.08980.pdf)
>  A well-known property of linear resistive electrical networks is that the current distribution minimizes the total dissipated energy. When the circuit includes resistors with nonlinear monotonic characteristic, the current distribution minimizes in general a different functional. We show that, if the nonlinear characteristic is a threshold-like function and the energy generator is concentrated in a single point, as in the case of lightning or dielectric discharge, then the current flow is concentrated along a single path, which is a minimum path to the ground with respect to the threshold. We also propose a dynamic model that explains and qualitatively reproduces the lightning transient behavior: initial generation of several plasma branches and subsequent dismissal of all branches but the one reaching the ground first, which is the optimal one.      
### 34.Always-On 674uW @ 4GOP/s Error Resilient Binary Neural Networks with Aggressive SRAM Voltage Scaling on a 22nm IoT End-Node  [ :arrow_down: ](https://arxiv.org/pdf/2007.08952.pdf)
>  Binary Neural Networks (BNNs) have been shown to be robust to random bit-level noise, making aggressive voltage scaling attractive as a power-saving technique for both logic and SRAMs. In this work, we introduce the first fully programmable IoT end-node system-on-chip (SoC) capable of executing software-defined, hardware-accelerated BNNs at ultra-low voltage. Our SoC exploits a hybrid memory scheme where error-vulnerable SRAMs are complemented by reliable standard-cell memories to safely store critical data under aggressive voltage scaling. On a prototype in 22nm FDX technology, we demonstrate that both the logic and SRAM voltage can be dropped to 0.5Vwithout any accuracy penalty on a BNN trained for the CIFAR-10 dataset, improving energy efficiency by 2.2X w.r.t. nominal conditions. Furthermore, we show that the supply voltage can be dropped to 0.42V (50% of nominal) while keeping more than99% of the nominal accuracy (with a bit error rate ~1/1000). In this operating point, our prototype performs 4Gop/s (15.4Inference/s on the CIFAR-10 dataset) by computing up to 13binary ops per pJ, achieving 22.8 Inference/s/mW while keeping within a peak power envelope of 674uW - low enough to enable always-on operation in ultra-low power smart cameras, long-lifetime environmental sensors, and insect-sized pico-drones.      
### 35.A Case Study on Video Color Transfer: Exploring User Motivations, Expectations, and Satisfaction  [ :arrow_down: ](https://arxiv.org/pdf/2007.08948.pdf)
>  Multimedia and creativity software products are being used to edit and control various elements of creative media practices. These days, the technical affordances of mobile multimedia devices and the advent of high-speed 5G internet access mean that these abilities are simpler and more readily available to be harnessed by mobile applications. In this paper, using a prototype application, we discuss how potential users of such technology are motivated to use a video recoloring application and explore the role that user expectation and satisfaction play in this process. By exploring this topic and focusing on the human-computer interaction, we found that color transfer interactions are driven by several intrinsic motivations and that user expectations and satisfaction ratings can be maintained via clear visualizations of the processes to be undertaken. Furthermore, we reveal the specific language that users use to communicate video recoloring when regarding user motivations, expectations, and satisfaction. This research provides important information for developers of state-of-art recoloring processes and contributes to dialogues surrounding the users of mobile multimedia technology in practice.      
### 36.Vision-based Estimation of MDS-UPDRS Gait Scores for Assessing Parkinson's Disease Motor Severity  [ :arrow_down: ](https://arxiv.org/pdf/2007.08920.pdf)
>  Parkinson's disease (PD) is a progressive neurological disorder primarily affecting motor function resulting in tremor at rest, rigidity, bradykinesia, and postural instability. The physical severity of PD impairments can be quantified through the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS), a widely used clinical rating scale. Accurate and quantitative assessment of disease progression is critical to developing a treatment that slows or stops further advancement of the disease. Prior work has mainly focused on dopamine transport neuroimaging for diagnosis or costly and intrusive wearables evaluating motor impairments. For the first time, we propose a computer vision-based model that observes non-intrusive video recordings of individuals, extracts their 3D body skeletons, tracks them through time, and classifies the movements according to the MDS-UPDRS gait scores. Experimental results show that our proposed method performs significantly better than chance and competing methods with an F1-score of 0.83 and a balanced accuracy of 81%. This is the first benchmark for classifying PD patients based on MDS-UPDRS gait severity and could be an objective biomarker for disease severity. Our work demonstrates how computer-assisted technologies can be used to non-intrusively monitor patients and their motor impairments. The code is available at <a class="link-external link-https" href="https://github.com/mlu355/PD-Motor-Severity-Estimation" rel="external noopener nofollow">this https URL</a>.      
### 37.Identification of Tree Species in Japanese Forests based on Aerial Photography and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.08907.pdf)
>  Natural forests are complex ecosystems whose tree species distribution and their ecosystem functions are still not well understood. Sustainable management of these forests is of high importance because of their significant role in climate regulation, biodiversity, soil erosion and disaster prevention among many other ecosystem services they provide. In Japan particularly, natural forests are mainly located in steep mountains, hence the use of aerial imagery in combination with computer vision are important modern tools that can be applied to forest research. Thus, this study constitutes a preliminary research in this field, aiming at classifying tree species in Japanese mixed forests using UAV images and deep learning in two different mixed forest types: a black pine (Pinus thunbergii)-black locust (Robinia pseudoacacia) and a larch (Larix kaempferi)-oak (Quercus mongolica) mixed forest. Our results indicate that it is possible to identify black locust trees with 62.6 % True Positives (TP) and 98.1% True Negatives (TN), while lower precision was reached for larch trees (37.4% TP and 97.7% TN).      
### 38.DVI: Depth Guided Video Inpainting for Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2007.08854.pdf)
>  To get clear street-view and photo-realistic simulation in autonomous driving, we present an automatic video inpainting algorithm that can remove traffic agents from videos and synthesize missing regions with the guidance of depth/point cloud. By building a dense 3D map from stitched point clouds, frames within a video are geometrically correlated via this common 3D map. In order to fill a target inpainting area in a frame, it is straightforward to transform pixels from other frames into the current one with correct occlusion. Furthermore, we are able to fuse multiple videos through 3D point cloud registration, making it possible to inpaint a target video with multiple source videos. The motivation is to solve the long-time occlusion problem where an occluded area has never been visible in the entire video. To our knowledge, we are the first to fuse multiple videos for video inpainting. To verify the effectiveness of our approach, we build a large inpainting dataset in the real urban road environment with synchronized images and Lidar data including many challenge scenes, e.g., long time occlusion. The experimental results show that the proposed approach outperforms the state-of-the-art approaches for all the criteria, especially the RMSE (Root Mean Squared Error) has been reduced by about 13%.      
### 39.Two-stream Fusion Model for Dynamic Hand Gesture Recognition using 3D-CNN and 2D-CNN Optical Flow guided Motion Template  [ :arrow_down: ](https://arxiv.org/pdf/2007.08847.pdf)
>  The use of hand gestures can be a useful tool for many applications in the human-computer interaction community. In a broad range of areas hand gesture techniques can be applied specifically in sign language recognition, robotic surgery, etc. In the process of hand gesture recognition, proper detection, and tracking of the moving hand become challenging due to the varied shape and size of the hand. Here the objective is to track the movement of the hand irrespective of the shape, size, and color of the hand. And, for this, a motion template guided by optical flow (OFMT) is proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In the experimentation, different datasets using bare hand with an open palm, and folded palm wearing green-glove are used, and in both cases, we could generate the OFMT images with equal precision. Recently, deep network-based techniques have shown impressive improvements as compared to conventional hand-crafted feature-based techniques. Moreover, in the literature, it is seen that the use of different streams with informative input data helps to increase the performance in the recognition accuracy. This work basically proposes a two-stream fusion model for hand gesture recognition and a compact yet efficient motion template based on optical flow. Specifically, the two-stream network consists of two layers: a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D has shown its efficiency in capturing spatio-temporal information of a video. Whereas OFMT helps to eliminate irrelevant gestures providing additional motion information. Though each stream can work independently, they are combined with a fusion scheme to boost the recognition results. We have shown the efficiency of the proposed two-stream network on two databases.      
### 40.Effect of Retransmissions on the Performance of C-V2X Communication for 5G  [ :arrow_down: ](https://arxiv.org/pdf/2007.08822.pdf)
>  In recent years, the next generation of wireless communication (5G) plays a significant role in both industry and academy societies. Cellular Vehicle-to-Everything (C-V2X) communication technology has been one of the prominent services for 5G. For C-V2X transmission mode, there is a newly defined communication channel (sidelink) that can support direct C-V2X communication. Direct C-V2X communication is a technology that allows vehicles to communicate and share safety-related information with other traffic participates directly without going through the cellular network. The C-V2X data packet will be delivered to all traffic Users (UE) in the proximity of the Transmitter (Tx). Some UEs might not successfully receive the data packets during one transmission but the sidelink Tx is not able to check whether the Receivers (Rxs) get the information or not due to the lack of feedback channel. For enabling the strict requirements in terms of reliability and latency for C-V2X communication, we propose and evaluate one retransmission scheme and retransmission with different traffic speed scheme. These schemes try to improve the reliability of the safety-related data by one blind retransmission without requiring feedback. Although this retransmission scheme is essential to C-V2X communication, the scheme has a limitation in the performance aspect because of its redundant retransmission. Since radio resources for CV2X communication are limited, we have to detect the effect of retransmission on the performance of the communication system. To the end, the simulator for evaluating the proposed schemes for the C-V2X communication has been implemented, and the performances of the different communication schemes are shown through the Packet Reception Ratio (PRR).      
### 41.INDRA: Intrusion Detection using Recurrent Autoencoders in Automotive Embedded Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.08795.pdf)
>  Today's vehicles are complex distributed embedded systems that are increasingly being connected to various external systems. Unfortunately, this increased connectivity makes the vehicles vulnerable to security attacks that can be catastrophic. In this work, we present a novel Intrusion Detection System (IDS) called INDRA that utilizes a Gated Recurrent Unit (GRU) based recurrent autoencoder to detect anomalies in Controller Area Network (CAN) bus-based automotive embedded systems. We evaluate our proposed framework under different attack scenarios and also compare it with the best known prior works in this area.      
### 42.Towards Enabling Critical mMTC: A Review of URLLC within mMTC  [ :arrow_down: ](https://arxiv.org/pdf/2007.08793.pdf)
>  Massive machine-type communication (mMTC) and ultra-reliable and low-latency communication (URLLC) are two key service types in the fifth-generation (5G) communication systems, pursuing scalability and reliability with low-latency, respectively. These two extreme services are envisaged to agglomerate together into \emph{critical mMTC} shortly with emerging use cases (e.g., wide-area disaster monitoring, wireless factory automation), creating new challenges to designing wireless systems beyond 5G. While conventional network slicing is effective in supporting a simple mixture of mMTC and URLLC, it is difficult to simultaneously guarantee the reliability, latency, and scalability requirements of critical mMTC (e.g., &lt; 4ms latency, $10^6$ devices/km$^2$ for factory automation) with limited radio resources. Furthermore, recently proposed solutions to scalable URLLC (e.g., machine learning aided URLLC for driverless vehicles) are ill-suited to critical mMTC whose machine type users have minimal energy budget and computing capability that should be (tightly) optimized for given tasks. To this end, our paper aims to characterize promising use cases of critical mMTC and search for their possible solutions. To this end, we first review the state-of-the-art (SOTA) technologies for separate mMTC and URLLC services and then identify key challenges from conflicting SOTA requirements, followed by potential approaches to prospective critical mMTC solutions at different layers.      
### 43.Proactive Network Maintenance using Fast, Accurate Anomaly Localization and Classification on 1-D Data Series  [ :arrow_down: ](https://arxiv.org/pdf/2007.08752.pdf)
>  Proactive network maintenance (PNM) is the concept of using data from a network to identify and locate network faults, many or all of which could worsen to become service failures. The separation between the network fault and the service failure affords early detection of problems in the network to allow PNM to take place. Consequently, PNM is a form of prognostics and health management (PHM). <br>The problem of localizing and classifying anomalies on 1-dimensional data series has been under research for years. We introduce a new algorithm that leverages Deep Convolutional Neural Networks to efficiently and accurately detect anomalies and events on data series, and it reaches 97.82% mean average precision (mAP) in our evaluation.      
### 44.Leveraging the Self-Transition Probability of Ordinal Pattern Transition Graph for Transportation Mode Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.08687.pdf)
>  The analysis of GPS trajectories is a well-studied problem in Urban Computing and has been used to track people. Analyzing people mobility and identifying the transportation mode used by them is essential for cities that want to reduce traffic jams and travel time between their points, thus helping to improve the quality of life of citizens. The trajectory data of a moving object is represented by a discrete collection of points through time, i.e., a time series. Regarding its interdisciplinary and broad scope of real-world applications, it is evident the need of extracting knowledge from time series data. Mining this type of data, however, faces several complexities due to its unique properties. Different representations of data may overcome this. In this work, we propose the use of a feature retained from the Ordinal Pattern Transition Graph, called the probability of self-transition for transportation mode classification. The proposed feature presents better accuracy results than Permutation Entropy and Statistical Complexity, even when these two are combined. This is the first work, to the best of our knowledge, that uses Information Theory quantifiers to transportation mode classification, showing that it is a feasible approach to this kind of problem.      
### 45.BRP-NAS: Prediction-based NAS using GCNs  [ :arrow_down: ](https://arxiv.org/pdf/2007.08668.pdf)
>  Neural architecture search (NAS) enables researchers to automatically explore broad design spaces in order to improve efficiency of neural networks. This efficiency is especially important in the case of on-device deployment, where improvements in accuracy should be balanced out with computational demands of a model. In practice, performance metrics of model are computationally expensive to obtain. Previous work uses a proxy (e.g. number of operations) or a layer-wise measurement of neural network layers to estimate end-to-end hardware performance but the imprecise prediction diminishes the quality of NAS. To address this problem, we propose BRP-NAS, an efficient hardware-aware NAS enabled by an accurate performance predictor-based on graph convolutional network (GCN). What is more, we investigate prediction quality on different metrics and show that sample-efficiency of the predictor-based NAS can be improved by considering binary relations of models and an iterative data selection strategy. We show that our proposed method outperforms all prior methods on both NAS-Bench-101 and NAS-Bench-201. Finally, to raise awareness of the fact that accurate latency estimation is not a trivial task, we release LatBench - a latency dataset of NAS-Bench-201 models running on a broad range of devices.      
### 46.Optimal Control of Port-Hamiltonian Systems: A Time-Continuous Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.08645.pdf)
>  Feedback controllers for port-Hamiltonian systems reveal an intrinsic inverse optimality property since each passivating state feedback controller is optimal with respect to some specific performance index. Due to the nonlinear port-Hamiltonian system structure, however, explicit (forward) methods for optimal control of port-Hamiltonian systems require the generally intractable analytical solution of the Hamilton-Jacobi-Bellman equation. Adaptive dynamic programming methods provide a means to circumvent this issue. However, the few existing approaches for port-Hamiltonian systems hinge on very specific sub-classes of either performance indices or system dynamics or require the intransparent guessing of stabilizing initial weights. In this paper, we contribute towards closing this largely unexplored research area by proposing a time-continuous adaptive feedback controller for the optimal control of general time-continuous input-state-output port-Hamiltonian systems with respect to general Lagrangian performance indices. Its control law implements an online learning procedure which uses the Hamiltonian of the system as an initial value function candidate. The time-continuous learning of the value function is achieved by means of a certain Lagrange multiplier that allows to evaluate the optimality of the current solution. In particular, constructive conditions for stabilizing initial weights are stated and asymptotic stability of the closed-loop equilibrium is proven. Our work is concluded by simulations for exemplary linear and nonlinear optimization problems which demonstrate asymptotic convergence of the controllers resulting from the proposed online adaptation procedure.      
### 47.Universal Model for Multi-Domain Medical Image Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2007.08628.pdf)
>  Medical Image Retrieval (MIR) helps doctors quickly find similar patients' data, which can considerably aid the diagnosis process. MIR is becoming increasingly helpful due to the wide use of digital imaging modalities and the growth of the medical image repositories. However, the popularity of various digital imaging modalities in hospitals also poses several challenges to MIR. Usually, one image retrieval model is only trained to handle images from one modality or one source. When there are needs to retrieve medical images from several sources or domains, multiple retrieval models need to be maintained, which is cost ineffective. In this paper, we study an important but unexplored task: how to train one MIR model that is applicable to medical images from multiple domains? Simply fusing the training data from multiple domains cannot solve this problem because some domains become over-fit sooner when trained together using existing methods. Therefore, we propose to distill the knowledge in multiple specialist MIR models into a single multi-domain MIR model via universal embedding to solve this problem. Using skin disease, x-ray, and retina image datasets, we validate that our proposed universal model can effectively accomplish multi-domain MIR.      
### 48.Optimization of Surface Plasmon Resonance Biosensor for Analysis of Lipid Molecules  [ :arrow_down: ](https://arxiv.org/pdf/2007.08607.pdf)
>  Surface Plasmon Resonance (SPR) is an important bio-sensing technique for real-time label-free detection. However, it is pivotal to optimize various parameters of the sensor configuration for efficient and highly sensitive sensing. To that effect, we focus on optimizing two different SPR structures -- the basic Kretschmann configuration and narrow groove grating. Our analysis aims to detect two different types of lipids known as phospholipid and eggyolk, which are used as analyte (sensing layer) and two different types of proteins namely tryptophan and bovine serum albumin (BSA) are used as ligand (binding site). For both the configurations, we investigate all possible lipid-protein combinations to understand the effect of various parameters on sensitivity, minimum reflectivity and full width half maximum (FWHM). Lipids are the structural building block of cell membranes and mutation of these layers by virus and bacteria is one the prime reasons of many diseases in our body. Hence, improving the performance of a SPR sensor to detect very small change in lipid holds immense significance. We use finite-difference time-domain (FDTD) technique to perform quantitative analysis to get an optimized structure. We find that sensitivity increases when lipid concentration is increased and it is the highest (21.95 degree/RIU) for phospholipid and tryptophan combination when metal and lipid layer thickness are 45 nm and 30 nm respectively. However, metal layer thickness does not cause any significant variation in sensitivity, but as it increases to 50 nm, minimum reflectivity and full width half maximum (FWHM) decreases to the lowest. In case of narrow groove grating structure, broad range of wavelengths can generate SPR and the sensitivity is highest (900nm/RIU) for a configuration of 10 nm groove width and 70 nm groove height at a resonance wavelength of 1411 nm.      
### 49.Advances in Deep Learning for Hyperspectral Image Analysis--Addressing Challenges Arising in Practical Imaging Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2007.08592.pdf)
>  Deep neural networks have proven to be very effective for computer vision tasks, such as image classification, object detection, and semantic segmentation -- these are primarily applied to color imagery and video. In recent years, there has been an emergence of deep learning algorithms being applied to hyperspectral and multispectral imagery for remote sensing and biomedicine tasks. These multi-channel images come with their own unique set of challenges that must be addressed for effective image analysis. Challenges include limited ground truth (annotation is expensive and extensive labeling is often not feasible), and high dimensional nature of the data (each pixel is represented by hundreds of spectral bands), despite being presented by a large amount of unlabeled data and the potential to leverage multiple sensors/sources that observe the same scene. In this chapter, we will review recent advances in the community that leverage deep learning for robust hyperspectral image analysis despite these unique challenges -- specifically, we will review unsupervised, semi-supervised and active learning approaches to image analysis, as well as transfer learning approaches for multi-source (e.g. multi-sensor, or multi-temporal) image analysis.      
### 50.FADACS: A Few-shot Adversarial Domain Adaptation Architecture for Context-Aware Parking Availability Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2007.08551.pdf)
>  The existing research on parking availability sensing mainly relies on extensive contextual and historical information. In practice, it is challenging to have such information available as it requires continuous collection of sensory signals. In this paper, we design an end-to-end transfer learning framework for parking availability sensing to predict the parking occupancy in areas where the parking data is insufficient to feed into data-hungry models. This framework overcomes two main challenges: 1) many real-world cases cannot provide enough data for most existing data-driven models. 2) it is difficult to merge sensor data and heterogeneous contextual information due to the differing urban fabric and spatial characteristics. Our work adopts a widely-used concept called adversarial domain adaptation to predict the parking occupancy in an area without abundant sensor data by leveraging data from other areas with similar features. In this paper, we utilise more than 35 million parking data records from sensors placed in two different cities, one is a city centre, and another one is a coastal tourist town. We also utilise heterogeneous spatio-temporal contextual information from external resources including weather and point of interests. We quantify the strength of our proposed framework in different cases and compare it to the existing data-driven approaches. The results show that the proposed framework outperforms existing methods and also provide a few valuable insights for parking availability prediction.      
### 51.Detecting Deepfake Videos: An Analysis of Three Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2007.08517.pdf)
>  Recent advances in deepfake generating algorithms that produce manipulated media have had dangerous implications in privacy, security and mass communication. Efforts to combat this issue have risen in the form of competitions and funding for research to detect deepfakes. This paper presents three techniques and algorithms: convolutional LSTM, eye blink detection and grayscale histograms-pursued while participating in the Deepfake Detection Challenge. We assessed the current knowledge about deepfake videos, a more severe version of manipulated media, and previous methods used, and found relevance in the grayscale histogram technique over others. We discussed the implications of each method developed and provided further steps to improve the given findings.      
### 52.Lyapunov Event-triggered Stabilization with a Known Convergence Rate  [ :arrow_down: ](https://arxiv.org/pdf/1803.08980.pdf)
>  A constructive tool of nonlinear control systems design, the method of Control Lyapunov Functions (CLF) has found numerous applications in stabilization problems for continuous time, discrete-time and hybrid systems. In this paper, we address the fundamental question: given a CLF, corresponding to the continuous-time controller with some predefined (e.g. exponential) convergence rate, can the same convergence rate be provided by an event-triggered controller? Under certain assumptions, we give an affirmative answer to this question and show that the corresponding event-based controllers provide positive dwelltimes between the consecutive events. Furthermore, we prove the existence of self-triggered and periodic event-triggered controllers, providing stabilization with a known convergence rate.      
