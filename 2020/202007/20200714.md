# ArXiv eess --Tue, 14 Jul 2020
### 1.Free-running SIMilarity-Based Angiography (SIMBA) for simplified anatomical MR imaging of the heart  [ :arrow_down: ](https://arxiv.org/pdf/2007.06544.pdf)
>  Purpose: Whole-heart MRA techniques typically target pre-determined motion states and address cardiac and respiratory dynamics independently. We propose a novel fast reconstruction algorithm, applicable to ungated free-running sequences, that leverages inherent similarities in the acquired data to avoid such physiological constraints. <br>Theory and Methods: The proposed SIMilarity-Based Angiography (SIMBA) method clusters the continuously acquired k-space data in order to find a motion-consistent subset that can be reconstructed into a motion-suppressed whole-heart MRA. Free-running 3D radial datasets from six ferumoxytol-enhanced scans of pediatric cardiac patients and twelve non-contrast scans of healthy volunteers were reconstructed with a non-motion-suppressed regridding of all the acquired data (All Data), our proposed SIMBA method, and a previously published free-running framework (FRF) that uses cardiac and respiratory self-gating and compressed sensing. Images were compared for blood-myocardium interface sharpness, contrast ratio, and visibility of coronary artery ostia. <br>Results: Both the fast SIMBA reconstruction (~20s) and the FRF provided significantly higher blood-myocardium sharpness than All Data (P&lt;0.001). No significant difference was observed among the former two. Significantly higher blood-myocardium contrast ratio was obtained with SIMBA compared to All Data and FRF (P&lt;0.01). More coronary ostia could be visualized with both SIMBA and FRF than with All Data (All Data: 4/36, SIMBA: 30/36, FRF: 33/36, both P&lt;0.001) but no significant difference was found between the first two. <br>Conclusion: The combination of free-running sequences and the fast SIMBA reconstruction, which operates without a priori assumptions related to physiological motion, forms a simple workflow for obtaining whole-heart MRA with sharp anatomical structures.      
### 2.Blockchain-Federated-Learning and Deep Learning Models for COVID-19 detection using CT Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.06537.pdf)
>  With the increase of COVID-19 cases worldwide, an effective way is required to diagnose COVID-19 patients. The primary problem in diagnosing COVID-19 patients is the shortage of testing kits, due to the quick spread of the virus, medical practitioners are facing difficulty identifying the positive cases. The second real-world problem is to share the data among the hospitals globally while keeping in view the privacy concern of the organizations. To address the problem of building a collaborative network model without leakage privacy of data are major concerns for training the deep learning model, this paper proposes a framework that collects a huge amount of data from different sources (various hospitals) and to train the deep learning model over a decentralized network for the newest information about COVID-19 patients. The main goal of this paper is to improve the recognition of a global deep learning model using, novel and up-to-date data, and learn itself from such data to improve recognition of COVID-19 patients based on computed tomography (CT) slices. Moreover, the integration of blockchain and federated-learning technology collects the data from different hospitals without leakage the privacy of the data. Firstly, we collect real-life COVID-19 patients data open to the research community. Secondly, we use various deep learning models (VGG, DenseNet, AlexNet, MobileNet, ResNet, and Capsule Network) to recognize the patterns via COVID-19 patients' lung screening. Thirdly, securely share the data among various hospitals with the integration of federated learning and blockchain. Finally, our results demonstrate a better performance to detect COVID-19 patients.      
### 3.Single Image Dehazing Algorithm Based on Sky Region Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.06492.pdf)
>  In this paper a hybrid image defogging approach based on region segmentation is proposed to address the dark channel priori algorithm's shortcomings in de-fogging the sky regions. The preliminary stage of the proposed approach focuses on the segmentation of sky and non-sky regions in a foggy image taking the advantageous of Meanshift and edge detection with embedded confidence. In the second stage, an improved dark channel priori algorithm is employed to defog the non-sky region. Ultimately, the sky area is processed by DehazeNet algorithm, which relies on deep learning Convolutional Neural Networks. The simulation results show that the proposed hybrid approach in this research addresses the problem of color distortion associated with sky regions in foggy images. The approach greatly improves the image quality indices including entropy information, visibility ratio of the edges, average gradient, and the saturation percentage with a very fast computation time, which is a good indication of the excellent performance of this model.      
### 4.Automatic Lyrics Transcription using Dilated Convolutional Neural Networks with Self-Attention  [ :arrow_down: ](https://arxiv.org/pdf/2007.06486.pdf)
>  Speech recognition is a well developed research field so that the current state of the art systems are being used in many applications in the software industry, yet as by today, there still does not exist such robust system for the recognition of words and sentences from singing voice. This paper proposes a complete pipeline for this task which may commonly be referred as automatic lyrics transcription (ALT). We have trained convolutional time-delay neural networks with self-attention on monophonic karaoke recordings using a sequence classification objective for building the acoustic model. The dataset used in this study, DAMP - Sing! 300x30x2 [1] is filtered to have songs with only English lyrics. Different language models are tested including MaxEnt and Recurrent Neural Networks based methods which are trained on the lyrics of pop songs in English. An in-depth analysis of the self-attention mechanism is held while tuning its context width and the number of attention heads. Using the best settings, our system achieves notable improvement to the state-of-the-art in ALT and provides a new baseline for the task.      
### 5.A Sampling Algorithm for Diffusion Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.06456.pdf)
>  In this paper, we propose a sampling mechanism for adaptive diffusion networks that adaptively changes the amount of sampled nodes based on mean-squared error in the neighborhood of each node. It presents fast convergence during transient and a significant reduction in the number of sampled nodes in steady state. Besides reducing the computational cost, the proposed mechanism can also be used as a censoring technique, thus saving energy by reducing the amount of communication between nodes. We also present a theoretical analysis to obtain lower and upper bounds for the number of network nodes sampled in steady state.      
### 6.Relative Entropy Regularised TDLAS Tomography for Robust Temperature Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.06416.pdf)
>  Tunable Diode Laser Absorption Spectroscopy (TDLAS) tomography has been widely used for in situ combustion diagnostics, yielding images of both species concentration and temperature. The temperature image is generally obtained from the reconstructed absorbance distributions for two spectral transitions, i.e. two-line thermometry. However, the inherently ill-posed nature of tomographic data inversion leads to noise in each of the reconstructed absorbance distributions. These noise effects propagate into the absorbance ratio and generate artefacts in the retrieved temperature image. To address this problem, we have developed a novel algorithm, which we call Relative Entropy Tomographic RecOnstruction (RETRO), for TDLAS tomography. A relative entropy regularisation is introduced for high-fidelity temperature image retrieval from jointly reconstructed two-line absorbance distributions. We have carried out numerical simulations and proof-of-concept experiments to validate the proposed algorithm. Compared with the well-established Simultaneous Algebraic Reconstruction Technique (SART), the RETRO algorithm significantly improves the quality of the tomographic temperature images, exhibiting excellent robustness against TDLAS tomographic measurement noise. RETRO offers great potential for industrial field applications of TDLAS tomography, where it is common for measurements to be performed in very harsh environments.      
### 7.Improving P300 Speller performance by means of optimization and machine learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.06411.pdf)
>  Brain-Computer Interfaces (BCIs) are systems allowing people to interact with the environment bypassing the natural neuromuscular and hormonal outputs of the peripheral nervous system (PNS). These interfaces record a user's brain activity and translate it into control commands for external devices, thus providing the PNS with additional artificial outputs. In this framework, the BCIs based on the P300 Event-Related Potentials (ERP), which represent the electrical responses recorded from the brain after specific events or stimuli, have proven to be particularly successful and robust. The presence or the absence of a P300 evoked potential within the EEG features is determined through a classification algorithm. Linear classifiers such as SWLDA and SVM are the most used for ERPs' classification. Due to the low signal-to-noise ratio of the EEG signals, multiple stimulation sequences (a.k.a. iterations) are carried out and then averaged before the signals being classified. However, while augmenting the number of iterations improves the Signal-to-Noise Ratio (SNR), it also slows down the process. In the early studies, the number of iterations was fixed (no stopping), but recently, several early stopping strategies have been proposed in the literature to dynamically interrupt the stimulation sequence when a certain criterion is met to enhance the communication rate. In this work, we explore how to improve the classification performances in P300 based BCIs by combining optimization and machine learning. First, we propose a new decision function that aims at improving classification performances in terms of accuracy and Information Transfer Rate both in a no stopping and early stopping environment. Then, we propose a new SVM training problem that aims to facilitate the target-detection process. Our approach proves to be effective on several publicly available datasets.      
### 8.Symmetric Dilated Convolution for Surgical Gesture Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.06373.pdf)
>  Automatic surgical gesture recognition is a prerequisite of intra-operative computer assistance and objective surgical skill assessment. Prior works either require additional sensors to collect kinematics data or have limitations on capturing temporal information from long and untrimmed surgical videos. To tackle these challenges, we propose a novel temporal convolutional architecture to automatically detect and segment surgical gestures with corresponding boundaries only using RGB videos. We devise our method with a symmetric dilation structure bridged by a self-attention module to encode and decode the long-term temporal patterns and establish the frame-to-frame relationship accordingly. We validate the effectiveness of our approach on a fundamental robotic suturing task from the JIGSAWS dataset. The experiment results demonstrate the ability of our method on capturing long-term frame dependencies, which largely outperform the state-of-the-art methods on the frame-wise accuracy up to ~6 points and the F1@50 score ~6 points.      
### 9.A novel random access scheme for M2M communication in crowded asynchronous massive MIMO systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.06370.pdf)
>  A new random access scheme is proposed to solve the intra-cell pilot collision for M2M communication in crowded asynchronous massive multiple-input multiple-output (MIMO) systems. The proposed scheme utilizes the proposed estimation of signal parameters via rotational invariance technique enhanced (ESPRIT-E) method to estimate the effective timing offsets, and then active UEs obtain their timing errors from the effective timing offsets for uplink message transmission. We analyze the mean squared error of the estimated effective timing offsets of UEs, and the uplink throughput. Simulation results show that, compared to the exiting random access scheme for the crowded asynchronous massive MIMO systems, the proposed scheme can improve the uplink throughput and estimate the effective timing offsets accurately at the same time.      
### 10.Vector-Quantized Timbre Representation  [ :arrow_down: ](https://arxiv.org/pdf/2007.06349.pdf)
>  Timbre is a set of perceptual attributes that identifies different types of sound sources. Although its definition is usually elusive, it can be seen from a signal processing viewpoint as all the spectral features that are perceived independently from pitch and loudness. Some works have studied high-level timbre synthesis by analyzing the feature relationships of different instruments, but acoustic properties remain entangled and generation bound to individual sounds. This paper targets a more flexible synthesis of an individual timbre by learning an approximate decomposition of its spectral properties with a set of generative features. We introduce an auto-encoder with a discrete latent space that is disentangled from loudness in order to learn a quantized representation of a given timbre distribution. Timbre transfer can be performed by encoding any variable-length input signals into the quantized latent features that are decoded according to the learned timbre. We detail results for translating audio between orchestral instruments and singing voice, as well as transfers from vocal imitations to instruments as an intuitive modality to drive sound synthesis. Furthermore, we can map the discrete latent space to acoustic descriptors and directly perform descriptor-based synthesis.      
### 11.DeU-Net: Deformable U-Net for 3D Cardiac MRI Video Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.06341.pdf)
>  Automatic segmentation of cardiac magnetic resonance imaging (MRI) facilitates efficient and accurate volume measurement in clinical applications. However, due to anisotropic resolution and ambiguous border (e.g., right ventricular endocardium), existing methods suffer from the degradation of accuracy and robustness in 3D cardiac MRI video segmentation. In this paper, we propose a novel Deformable U-Net (DeU-Net) to fully exploit spatio-temporal information from 3D cardiac MRI video, including a Temporal Deformable Aggregation Module (TDAM) and a Deformable Global Position Attention (DGPA) network. First, the TDAM takes a cardiac MRI video clip as input with temporal information extracted by an offset prediction network. Then we fuse extracted temporal information via a temporal aggregation deformable convolution to produce fused feature maps. Furthermore, to aggregate meaningful features, we devise the DGPA network by employing deformable attention U-Net, which can encode a wider range of multi-dimensional contextual information into global and local features. Experimental results show that our DeU-Net achieves the state-of-the-art performance on commonly used evaluation metrics, especially for cardiac marginal information (ASSD and HD).      
### 12.Stabilizing a spherical pendulum on a quadrotor  [ :arrow_down: ](https://arxiv.org/pdf/2007.06332.pdf)
>  In this article we design a backstepping control law based on geometric principles to swing up a spherical pendulum mounted on a moving quadrotor. The available degrees of freedom in the control vector also permit us to position the plane of the quadrotor parallel to the ground. The problem addressed here is, indeed, novel and has many practical applications which arise during the transport of a payload mounted on top of a quadrotor. The modeling and control law are coordinate-free and thus avoid singularity issues. The geometric treatment of the problem greatly simplifies both the modeling and control law for the system. The control action is verified and supported by numerical experiments for aggressive manoeuvres starting very close to the downward stable equilibrium position of the pendulum.      
### 13.Accelerated FBP for computed tomography image reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2007.06289.pdf)
>  Filtered back projection (FBP) is a commonly used technique in tomographic image reconstruction demonstrating acceptable quality. The classical direct implementations of this algorithm require the execution of $\Theta(N^3)$ operations, where $N$ is the linear size of the 2D slice. Recent approaches including reconstruction via the Fourier slice theorem require $\Theta(N^2\log N)$ multiplication operations. In this paper, we propose a novel approach that reduces the computational complexity of the algorithm to $\Theta(N^2\log N)$ addition operations avoiding Fourier space. For speeding up the convolution, ramp filter is approximated by a pair of causal and anticausal recursive filters, also known as Infinite Impulse Response filters. The back projection is performed with the fast discrete Hough transform. Experimental results on simulated data demonstrate the efficiency of the proposed approach.      
### 14.Drum Beats and Where To Find Them: Sampling Drum Patterns from a Latent Space  [ :arrow_down: ](https://arxiv.org/pdf/2007.06284.pdf)
>  This paper presents a large dataset of drum patterns and compares two different architectures of artificial neural networks that produce latent explorable spaces with some recognizable genre areas. Adversarially constrained autoencoder interpolations (ACAI) show better results in comparison with a standard variational autoencoder. To our knowledge, this is the first application of ACAI to drum-pattern generation.      
### 15.Complex Exponential Signal Recovery with Deep Hankel Matrix Factorization  [ :arrow_down: ](https://arxiv.org/pdf/2007.06246.pdf)
>  Exponential is a basic signal form and how to fast acquire these signals is one of the fundamental problems and frontiers in signal processing. To achieve this goal, partial data may be acquired but result in the serious artifacts in its spectrum, which is the Fourier transform of exponentials. Thus, reliable spectrum reconstruction is highly expected in the fast sampling in many applications, such as chemistry, biology, and medical imaging. In this work, we propose a deep learning method whose neural network structure is designed by unrolling the iterative process in the model-based state-of-the-art exponentials reconstruction method with low rank Hankel matrix factorization. With the experiments on synthetic and realistic biological signals, we demonstrate that the new method yields much lower reconstruction errors and more accuracy in spectrum parameter quantification than another state-of-the-art deep learning method, while costs much less time than the model-based reconstruction methods.      
### 16.Fast approximate reciprocal approximations for iterative algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2007.06241.pdf)
>  The reciprocal function, 1/x, is important for many real-time algorithms. It is used in a large variety of algorithms from areas ranging from iterative estimation to machine learning. Many of these algorithms are iterative in nature and require the online computation of the reciprocal. Such an iterative structure often prevents effective use of pipelining for implementation of the reciprocal. For this reason, a reciprocal algorithm requiring only a low amount of clock cycles is desired. Many real-time algorithms, often being of approximate nature, can tolerate the use of only an approximate solution of the reciprocal. <br>For this reason, we present a low complexity non-iterative approximation of the reciprocal function. This approximation can be calculated using only combinatorial logic. We present synthesis results showing that the proposed approach can be implemented with low area requirements at high clock frequencies. We analytically describe the error of the approximation and show that by optimizing a constant value used in the approximation, different variants with different error behaviors can be obtained. We furthermore present performance results of application examples that, when using our proposed method, show only negligible performance degradation compared to when using the exact reciprocal function, demonstrating the versatility of our proposed approach.      
### 17.CheXphoto: 10,000+ Smartphone Photos and Synthetic Photographic Transformations of Chest X-rays for Benchmarking Deep Learning Robustness  [ :arrow_down: ](https://arxiv.org/pdf/2007.06199.pdf)
>  Clinical deployment of deep learning algorithms for chest x-ray interpretation requires a solution that can integrate into the vast spectrum of clinical workflows across the world. An appealing solution to scaled deployment is to leverage the existing ubiquity of smartphones: in several parts of the world, clinicians and radiologists capture photos of chest x-rays to share with other experts or clinicians via smartphone using messaging services like WhatsApp. However, the application of chest x-ray algorithms to photos of chest x-rays requires reliable classification in the presence of smartphone photo artifacts such as screen glare and poor viewing angle not typically encountered on digital x-rays used to train machine learning models. We introduce CheXphoto, a dataset of smartphone photos and synthetic photographic transformations of chest x-rays sampled from the CheXpert dataset. To generate CheXphoto we (1) automatically and manually captured photos of digital x-rays under different settings, including various lighting conditions and locations, and, (2) generated synthetic transformations of digital x-rays targeted to make them look like photos of digital x-rays and x-ray films. We release this dataset as a resource for testing and improving the robustness of deep learning algorithms for automated chest x-ray interpretation on smartphone photos of chest x-rays.      
### 18.Bottom-up mechanism and improved contract net protocol for the dynamic task planning of heterogeneous Earth observation resources  [ :arrow_down: ](https://arxiv.org/pdf/2007.06172.pdf)
>  Earth observation resources are becoming increasingly indispensable in disaster relief, damage assessment and related domains. Many unpredicted factors, such as the change of observation task requirements, to the occurring of bad weather and resource failures, may cause the scheduled observation scheme to become infeasible. Therefore, it is crucial to be able to promptly and maybe frequently develop high-quality replanned observation schemes that minimize the effects on the scheduled tasks. A bottom-up distributed coordinated framework together with an improved contract net are proposed to facilitate the dynamic task replanning for heterogeneous Earth observation resources. This hierarchical framework consists of three levels, namely, neighboring resource coordination, single planning center coordination, and multiple planning center coordination. Observation tasks affected by unpredicted factors are assigned and treated along with a bottom-up route from resources to planning centers. This bottom-up distributed coordinated framework transfers part of the computing load to various nodes of the observation systems to allocate tasks more efficiently and robustly. To support the prompt assignment of large-scale tasks to proper Earth observation resources in dynamic environments, we propose a multiround combinatorial allocation (MCA) method. Moreover, a new float interval-based local search algorithm is proposed to obtain the promising planning scheme more quickly. The experiments demonstrate that the MCA method can achieve a better task completion rate for large-scale tasks with satisfactory time efficiency. It also demonstrates that this method can help to efficiently obtain replanning schemes based on original scheme in dynamic environments.      
### 19.MS-NAS: Multi-Scale Neural Architecture Search for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.06151.pdf)
>  The recent breakthroughs of Neural Architecture Search (NAS) have motivated various applications in medical image segmentation. However, most existing work either simply rely on hyper-parameter tuning or stick to a fixed network backbone, thereby limiting the underlying search space to identify more efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS) framework that is featured with multi-scale search space from network backbone to cell operation, and multi-scale fusion capability to fuse features with different sizes. To mitigate the computational overhead due to the larger search space, a partial channel connection scheme and a two-step decoding method are utilized to reduce computational overhead while maintaining optimization quality. Experimental results show that on various datasets for segmentation, MS-NAS outperforms the state-of-the-art methods and achieves 0.6-5.4% mIOU and 0.4-3.5% DSC improvements, while the computational resource consumption is reduced by 18.0-24.9%.      
### 20.Low-Complexity Set-Membership Normalized LMS Algorithm for Sparse System Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.06097.pdf)
>  In this work, we propose two low-complexity set-membership normalized least-mean-square (LCSM-NLMS1 and LCSM-NLMS2) algorithms to exploit the sparsity of an unknown system. For this purpose, in the LCSM-NLMS1 algorithm, we employ a function called the discard function to the adaptive coefficients in order to neglect the coefficients close to zero in the update process. Moreover, in the LCSM-NLMS2 algorithm, to decrease the overall number of computations needed even further, we substitute small coefficients with zero. Numerical results present similar performance of these algorithms when comparing them with some state-of-the-art sparsity-aware algorithms, whereas the proposed algorithms need lower computational cost.      
### 21.Fine-grained Language Identification with Multilingual CapsNet Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.06078.pdf)
>  Due to a drastic improvement in the quality of internet services worldwide, there is an explosion of multilingual content generation and consumption. This is especially prevalent in countries with large multilingual audience, who are increasingly consuming media outside their linguistic familiarity/preference. Hence, there is an increasing need for real-time and fine-grained content analysis services, including language identification, content transcription, and analysis. Accurate and fine-grained spoken language detection is an essential first step for all the subsequent content analysis algorithms. Current techniques in spoken language detection may lack on one of these fronts: accuracy, fine-grained detection, data requirements, manual effort in data collection \&amp; pre-processing. Hence in this work, a real-time language detection approach to detect spoken language from 5 seconds' audio clips with an accuracy of 91.8\% is presented with exiguous data requirements and minimal pre-processing. Novel architectures for Capsule Networks is proposed which operates on spectrogram images of the provided audio snippets. We use previous approaches based on Recurrent Neural Networks and iVectors to present the results. Finally we show a ``Non-Class'' analysis to further stress on why CapsNet architecture works for LID task.      
### 22.Adversarial jamming attacks and defense strategies via adaptive deep reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.06055.pdf)
>  As the applications of deep reinforcement learning (DRL) in wireless communications grow, sensitivity of DRL based wireless communication strategies against adversarial attacks has started to draw increasing attention. In order to address such sensitivity and alleviate the resulting security concerns, we in this paper consider a victim user that performs DRL-based dynamic channel access, and an attacker that executes DRLbased jamming attacks to disrupt the victim. Hence, both the victim and attacker are DRL agents and can interact with each other, retrain their models, and adapt to opponents' policies. In this setting, we initially develop an adversarial jamming attack policy that aims at minimizing the accuracy of victim's decision making on dynamic channel access. Subsequently, we devise defense strategies against such an attacker, and propose three defense strategies, namely diversified defense with proportional-integral-derivative (PID) control, diversified defense with an imitation attacker, and defense via orthogonal policies. We design these strategies to maximize the attacked victim's accuracy and evaluate their performances.      
### 23.Elevated LiDAR Placement under Energy and Throughput Capacity Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2007.06043.pdf)
>  Elevated LiDAR (ELiD) has the potential to hasten the deployment of Autonomous Vehicles (AV), as ELiD can reduce energy expenditures associated with AVs, and can also be utilized for other intelligent Transportation Systems applications such as urban 3D mapping. In this paper, we address the need for a planning framework in order for ITS operators to have an effective tool for determining what resources are required to achieve a desired level of coverage of urban roadways. To this end, we develop a mixed-integer nonlinear constrained optimization problem, with the aim of maximizing effective area coverage of a roadway, while satisfying energy and throughput capacity constraints. Due to the non-linearity of the problem, we utilize Particle Swarm Optimization (PSO) to solve the problem. After demonstrating its effectiveness in finding a solution for a realistic scenario, we perform a sensitivity analysis to test the model's general ability.      
### 24.Unified Virtual Oscillator Control for Grid-Forming and Grid-Following Converters  [ :arrow_down: ](https://arxiv.org/pdf/2007.06042.pdf)
>  A unified virtual oscillator controller (uVOC) is proposed, which enables a unified analysis, design, and implementation framework for both grid-forming (GFM) and grid-following (GFL) voltage source converters (VSCs). Oscillator based GFM controllers, such as dispatchable virtual oscillator control (dVOC), offer rigorous analytical framework with enhanced synchronization, but lack effective fault handling capability which severely limits practical application. The proposed uVOC facilitates synchronization with an arbitrarily low grid voltage and fast over-current limiting; this enables effective fault ride-through unlike existing GFM controllers which typically switch to a back-up controller during fault. GFM operation with uVOC is achieved in both grid connected and islanded modes with seamless transition between the two. In GFL converters, bidirectional power flow control and DC bus voltage regulation is achieved with uVOC. No phase-locked-loop (PLL) is required for either GFL or GFM operation circumventing the synchronization issues associated with PLLs in weak grid applications. Detail small signal models for GFM and GFL operation have been developed and systematic design guidelines for controller parameters are provided. The proposed controller is validated through hardware experiments in a hybrid AC-DC microgrid.      
### 25.TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech  [ :arrow_down: ](https://arxiv.org/pdf/2007.06028.pdf)
>  We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn through the formulation of a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous approaches, we use a multi-target auxiliary task to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from its altered counterpart, where we use a stochastic policy to alter along three dimensions: temporal, channel, and magnitude. TERA can be used to extract speech representations or fine-tune with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, speaker recognition, and speech recognition. TERA achieved strong performance on these tasks by improving upon surface features and outperforming previous methods. In our experiments, we show that through alteration along different dimensions, the model learns to encode distinct aspects of speech. We explore different knowledge transfer methods to incorporate the pre-trained model with downstream models. Furthermore, we show that the proposed method can be easily transferred to another dataset not used in pre-training.      
### 26.NISP: A Multi-lingual Multi-accent Dataset for Speaker Profiling  [ :arrow_down: ](https://arxiv.org/pdf/2007.06021.pdf)
>  Many commercial and forensic applications of speech demand the extraction of information about the speaker characteristics, which falls into the broad category of speaker profiling. The speaker characteristics needed for profiling include physical traits of the speaker like height, age, and gender of the speaker along with the native language of the speaker. Many of the datasets available have only partial information for speaker profiling. In this paper, we attempt to overcome this limitation by developing a new dataset which has speech data from five different Indian languages along with English. The metadata information for speaker profiling applications like linguistic information, regional information, and physical characteristics of a speaker are also collected. We call this dataset as NITK-IISc Multilingual Multi-accent Speaker Profiling (NISP) dataset. The description of the dataset, potential applications, and baseline results for speaker profiling on this dataset are provided in this paper.      
### 27.Standoff Through-the-Wall Sensing at Ka-Band Microwave  [ :arrow_down: ](https://arxiv.org/pdf/2007.06020.pdf)
>  Conventional microwave remote sensing/imaging of through-the-wall objects made of different materials is usually performed at frequencies below 3 GHz that provide relatively low spatial resolution. In this paper, we evaluate the ability and sensitivity of high-frequency microwave or millimeter wave standoff sensing of through-the-wall objects to achieve high spatial resolution. The target under study is a sandwich structure consisting of different object materials placed between two wall blocks. An Agilent PNA-X series (model N5245A) vector network analyzer is used to sweep over the entire Ka-band (26.5 GHz to 40 GHz). The beam is then directed to a standard rectangular horn antenna and collimated by a 6-inch-diameter Gaussian lens towards the sandwich structure (wall block/object/wall block). The reflected electromagnetic wave is picked up by the same system as the complex S-parameter S11. Both amplitude and phase of the reflected signal are used to recognize different materials sandwiched between the cement blocks. The experimental results are compared with the theoretical calculations, which show satisfactory agreement for the cases evaluated in this work.      
### 28.Deep Network Interpolation for Accelerated Parallel MR Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2007.05993.pdf)
>  We present a deep network interpolation strategy for accelerated parallel MR image reconstruction. In particular, we examine the network interpolation in parameter space between a source model that is formulated in an unrolled scheme with L1 and SSIM losses and its counterpart that is trained with an adversarial loss. We show that by interpolating between the two different models of the same network structure, the new interpolated network can model a trade-off between perceptual quality and fidelity.      
### 29.Tandem Assessment of Spoofing Countermeasures and Automatic Speaker Verification: Fundamentals  [ :arrow_down: ](https://arxiv.org/pdf/2007.05979.pdf)
>  Recent years have seen growing efforts to develop spoofing countermeasures (CMs) to protect automatic speaker verification (ASV) systems from being deceived by manipulated or artificial inputs. The reliability of spoofing CMs is typically gauged using the equal error rate (EER) metric. The primitive EER fails to reflect application requirements and the impact of spoofing and CMs upon ASV and its use as a primary metric in traditional ASV research has long been abandoned in favour of risk-based approaches to assessment. This paper presents several new extensions to the tandem detection cost function (t-DCF), a recent risk-based approach to assess the reliability of spoofing CMs deployed in tandem with an ASV system. Extensions include a simplified version of the t-DCF with fewer parameters, an analysis of a special case for a fixed ASV system, simulations which give original insights into its interpretation and new analyses using the ASVspoof 2019 database. It is hoped that adoption of the t-DCF for the CM assessment will help to foster closer collaboration between the anti-spoofing and ASV research communities.      
### 30.Deep Learning for Wireless Communications: An Emerging Interdisciplinary Paradigm  [ :arrow_down: ](https://arxiv.org/pdf/2007.05952.pdf)
>  Wireless communications are envisioned to bring about dramatic changes in the future, with a variety of emerging applications, such as virtual reality (VR), Internet of things (IoT), etc., becoming a reality. However, these compelling applications have imposed many new challenges, including unknown channel models, low-latency requirement in large-scale super-dense networks, etc. The amazing success of deep learning (DL) in various fields, particularly in computer science, has recently stimulated increasing interest in applying it to address those challenges. Hence, in this review, a pair of dominant methodologies of using DL for wireless communications are investigated. The first one is DL-based architecture design, which breaks the classical model-based block design rule of wireless communications in the past decades. The second one is DL-based algorithm design, which will be illustrated by several examples in a series of typical techniques conceived for 5G and beyond. Their principles, key features, and performance gains will be discussed. Furthermore, open problems and future research opportunities will also be pointed out, highlighting the interplay between DL and wireless communications. We expect that this review can stimulate more novel ideas and exciting contributions for intelligent wireless communications.      
### 31.The ASRU 2019 Mandarin-English Code-Switching Speech Recognition Challenge: Open Datasets, Tracks, Methods and Results  [ :arrow_down: ](https://arxiv.org/pdf/2007.05916.pdf)
>  Code-switching (CS) is a common phenomenon and recognizing CS speech is challenging. But CS speech data is scarce and there' s no common testbed in relevant research. This paper describes the design and main outcomes of the ASRU 2019 Mandarin-English code-switching speech recognition challenge, which aims to improve the ASR performance in Mandarin-English code-switching situation. 500 hours Mandarin speech data and 240 hours Mandarin-English intra-sentencial CS data are released to the participants. Three tracks were set for advancing the AM and LM part in traditional DNN-HMM ASR system, as well as exploring the E2E models' performance. The paper then presents an overview of the results and system performance in the three tracks. It turns out that traditional ASR system benefits from pronunciation lexicon, CS text generating and data augmentation. In E2E track, however, the results highlight the importance of using language identification, building-up a rational set of modeling units and spec-augment. The other details in model training and method comparsion are discussed.      
### 32.Multi-sensor Spatial Association using Joint Range-Doppler Features  [ :arrow_down: ](https://arxiv.org/pdf/2007.05907.pdf)
>  We investigate the problem of localizing multiple targets using a single set of measurements from a network of radar sensors. Such "single snapshot imaging" provides timely situational awareness, but can utilize neither platform motion, as in synthetic aperture radar, nor track targets across time, as in Kalman filtering and its variants. Associating measurements with targets becomes a fundamental bottleneck in this setting. In this paper, we present a computationally efficient method to extract 2D position and velocity of multiple targets using a linear array of FMCW radar sensors by identifying and exploiting inherent geometric features to drastically reduce the complexity of spatial association. The proposed framework is robust to detection anomalies, and achieves order of magnitude lower complexity compared to conventional methods. While our approach is compatible with conventional FFT-based range-Doppler processing, we show that more sophisticated techniques for range-Doppler estimation lead to reduced data association complexity as well as higher accuracy estimates of target positions and velocities.      
### 33.Recognition and evaluation of constellation diagram using deep learning based on underwater wireless optical communication  [ :arrow_down: ](https://arxiv.org/pdf/2007.05890.pdf)
>  Abstract. In this paper, we proposed a method of constellation diagram recognition and evaluation using deep learning based on underwater wireless optical communication (UWOC). More specifically, an constellation diagram analyzer for UWOC system based on convolutional neural network (CNN) is designed for modulation format recognition (MFR), optical signal noise ratio (OSNR) and phase error estimation. Besides, unsupervised learning is used to excavate a new optimization metric from various factors that affect the quality of underwater channel.The proposed new metric synthesizes several original indexes, which we termed it as multi noise spatial metric (MNSM). The proposed MNSM divides the quality of constellation from high to low into several levels and reflects the quality of UWOC channel. Through the simulation, the constellation diagrams of four widely used M-QAM modulation formats for 16 OSNR values (15dB~30dB) are obtained, with the phase error standard deviations ranging from 0° to 45°. The results show that the accuracy of MFR , the estimation of OSNR and phase noise are 100%, 95% and 98.6% accuracies are achieved respectively. The ablation studies are also carried out in order to analyze the performance of deep learning in the recognition of constellation diagrams.      
### 34.Deep Learning-based Resource Allocation for Infrastructure Resilience  [ :arrow_down: ](https://arxiv.org/pdf/2007.05880.pdf)
>  From an optimization point of view, resource allocation is one of the cornerstones of research for addressing limiting factors commonly arising in applications such as power outages and traffic jams. In this paper, we take a data-driven approach to estimate an optimal nodal restoration sequence for immediate recovery of the infrastructure networks after natural disasters such as earthquakes. We generate data from td-INDP, a high-fidelity simulator of optimal restoration strategies for interdependent networks, and employ deep neural networks to approximate those strategies. Despite the fact that the underlying problem is NP-complete, the restoration sequences obtained by our method are observed to be nearly optimal. In addition, by training multiple models---the so-called estimators---for a variety of resource availability levels, our proposed method balances a trade-off between resource utilization and restoration time. Decision-makers can use our trained models to allocate resources more efficiently after contingencies, and in turn, improve the community resilience. Besides their predictive power, such trained estimators unravel the effect of interdependencies among different nodal functionalities in the restoration strategies. We showcase our methodology by the real-world interdependent infrastructure of Shelby County, TN.      
### 35.1Reconfigurable Intelligent Surfaces AidedmmWave NOMA: Joint Power Allocation,Phase Shifts, and Hybrid Beamforming Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.05873.pdf)
>  In this paper, we investigate the application of reconfigurable intelligent surfaces (RISs) {}{to} millimeter wave (mmWave) non-orthogonal multiple access (NOMA) systems. In particular, we consider an RIS-aided mmWave-NOMA downlink system with a hybrid beamforming structure. To maximize the achievable sum-rate under {}{a} minimum rate constraint {}{for the users} and {}{a minimum} transmit power constraint, a joint RIS phase shifts, hybrid beamforming, and power allocation problem is formulated. To solve this non-convex optimization problem, we {}{introduce} an alternating optimization algorithm. First, the non-convex problem is transformed into three subproblems, i.e., power allocation, joint phase shifts and analog beamforming optimization}, and digital beamforming design. Then, we solve the power allocation problem under fixed phase shifts of the RIS and hybrid beamforming. Finally, given the power allocation matrix, {}{an} alternating manifold optimization (AMO)-based method and {}{a} successive convex approximation (SCA)-based method are utilized to design the phase shifts, analog beamforming, and transmit beamforming, respectively. Numerical results reveal that the proposed alternating optimization algorithm outperforms state-of-the-art schemes in terms of sum-rate. Moreover, compared to {}{a} conventional mmWave-NOMA system without RIS, the proposed RIS-aided mmWave-NOMA system {}{is capable of improving} the achievable sum-rate of the system.      
### 36.Recursive algorithm for the control of output remnant of Preisach hysteresis operator  [ :arrow_down: ](https://arxiv.org/pdf/2007.05866.pdf)
>  We study in this paper the control of hysteresis-based actuator systems where its remanence behavior (e.g., the remaining memory when the actuation signal is set to zero) must follow a desired reference point. We present a recursive algorithm for the output regulation of the hysteresis remnant behavior described by Preisach operators. Under some mild conditions, we prove that our proposed algorithm guarantees that the output remnant converges to a desired value. Simulation result shows the efficacy of our proposed algorithm.      
### 37.Model Properties for Efficient Synthesis of Nonblocking Modular Supervisors  [ :arrow_down: ](https://arxiv.org/pdf/2007.05795.pdf)
>  Supervisory control theory provides means to synthesize supervisors for cyber-physical systems from models of the uncontrolled plant and models of the control requirements. It has been shown that in general supervisory control synthesis is NP-hard. However, for several industrial systems supervisory control synthesis verifies that the provided control requirements are sufficient to act as a supervisor. In this paper, we propose model properties and a method to identify when no synthesis is needed for a given set of plant models and requirement models, i.e., the plant models and requirement models together form a maximally permissive, controllable, and nonblocking supervisor. The method consists of creating a control problem dependency graph and verifying whether it is acyclic to establish that synthesis can be skipped. In case of a cyclic graph, potential blocking issues can be localized, so that the original control problem can be reduced to only synthesizing supervisors for smaller partial control problems. The proposed method is illustrated in detail with a case study of a production line and applied on a case study of a roadway tunnel for which the method identifies a large part of the system that requires no synthesis.      
### 38.Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast Cancer Risk Models  [ :arrow_down: ](https://arxiv.org/pdf/2007.05791.pdf)
>  The ability to accurately estimate risk of developing breast cancer would be invaluable for clinical decision-making. One promising new approach is to integrate image-based risk models based on deep neural networks. However, one must take care when using such models, as selection of training data influences the patterns the network will learn to identify. With this in mind, we trained networks using three different criteria to select the positive training data (i.e. images from patients that will develop cancer): an inherent risk model trained on images with no visible signs of cancer, a cancer signs model trained on images containing cancer or early signs of cancer, and a conflated model trained on all images from patients with a cancer diagnosis. We find that these three models learn distinctive features that focus on different patterns, which translates to contrasts in performance. Short-term risk is best estimated by the cancer signs model, whilst long-term risk is best estimated by the inherent risk model. Carelessly training with all images conflates inherent risk with early cancer signs, and yields sub-optimal estimates in both regimes. As a consequence, conflated models may lead physicians to recommend preventative action when early cancer signs are already visible.      
### 39.Fast Griffin Lim based Waveform Generation Strategy for Text-to-Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2007.05764.pdf)
>  The performance of text-to-speech (TTS) systems heavily depends on spectrogram to waveform generation, also known as the speech reconstruction phase. The time required for the same is known as synthesis delay. In this paper, an approach to reduce speech synthesis delay has been proposed. It aims to enhance the TTS systems for real-time applications such as digital assistants, mobile phones, embedded devices, etc. The proposed approach applies Fast Griffin Lim Algorithm (FGLA) instead Griffin Lim algorithm (GLA) as vocoder in the speech synthesis phase. GLA and FGLA are both iterative, but the convergence rate of FGLA is faster than GLA. The proposed approach is tested on LJSpeech, Blizzard and Tatoeba datasets and the results for FGLA are compared against GLA and neural Generative Adversarial Network (GAN) based vocoder. The performance is evaluated based on synthesis delay and speech quality. A 36.58% reduction in speech synthesis delay has been observed. The quality of the output speech has improved, which is advocated by higher Mean opinion scores (MOS) and faster convergence with FGLA as opposed to GLA.      
### 40.Multi-functional Coexistence of Radar-Sensing and Communication Waveforms  [ :arrow_down: ](https://arxiv.org/pdf/2007.05753.pdf)
>  In this study, a novel transmission scheme is proposed to serve radar-sensing and communication objectives at the same time and allocated bandwidth. The proposed transmitted frame non-orthogonally superimposes two different waveforms, which are frequency modulated continuous-wave (FMCW) for radar-sensing and orthogonal frequency division multiplexing (OFDM) for communication. Also, the receiver scheme that performs channel estimation via radar-sensing functionality without degrading data rate of communication operation is introduced. As numerically evaluated, the proposed system achieves good sensing accuracy even if the signal-to-noise ratio (SNR) is low, and communication performance is only 0.6 dB less at the target bit-error rate (BER) of 1% compared to the assumption of perfect channel state information (CSI) without any pilot overhead over OFDM subcarriers.      
### 41.Graph Neural Networks for Massive MIMO Detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.05703.pdf)
>  In this paper, we innovately use graph neural networks (GNNs) to learn a message-passing solution for the inference task of massive multiple multiple-input multiple-output (MIMO) detection in wireless communication. We adopt a graphical model based on the Markov random field (MRF) where belief propagation (BP) yields poor results when it assumes a uniform prior over the transmitted symbols. Numerical simulations show that, under the uniform prior assumption, our GNN-based MIMO detection solution outperforms the minimum mean-squared error (MMSE) baseline detector, in contrast to BP. Furthermore, experiments demonstrate that the performance of the algorithm slightly improves by incorporating MMSE information into the prior.      
### 42.Event-based Signal Processing for Radioisotope Identification  [ :arrow_down: ](https://arxiv.org/pdf/2007.05686.pdf)
>  this paper identifies the problem of unnecessary high power overhead of the conventional frame-based radioisotope identification process and proposes an event-based signal processing process to address the problem established. It also presents the design flow of the neuromorphic processor.      
### 43.NeuralExplorer: State Space Exploration of Closed Loop Control Systems Using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.05685.pdf)
>  In this paper, we propose a framework for performing state space exploration of closed loop control systems. Our approach involves approximating sensitivity and a newly introduced notion of inverse sensitivity by a neural network. We show how the approximation of sensitivity and inverse sensitivity can be used for computing estimates of the reachable set. We then outline algorithms for performing state space exploration by generating trajectories that reach a neighborhood. We demonstrate the effectiveness of our approach by applying it not only to standard linear and nonlinear dynamical systems, but also to nonlinear hybrid systems and also neural network based feedback control systems.      
### 44.Quasi-Periodic WaveNet: An Autoregressive Raw Waveform Generative Model with Pitch-dependent Dilated Convolution Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.05663.pdf)
>  In this paper, a pitch-adaptive waveform generative model named Quasi-Periodic WaveNet (QPNet) is proposed to improve the pitch controllability of vanilla WaveNet (WN) using pitch-dependent dilated convolution neural networks (PDCNNs). Specifically, as a probabilistic autoregressive generation model with stacked dilated convolution layers, WN achieves high-fidelity audio waveform generation. However, the pure-data-driven nature and the lack of prior knowledge of audio signals degrade the pitch controllability of WN. For instance, it is difficult for WN to precisely generate the periodic components of audio signals when the given auxiliary fundamental frequency (F0) features are outside the F0 range observed in the training data. To address this problem, QPNet with two novel designs is proposed. First, the PDCNN component is applied to dynamically change the network architecture of WN according to the given auxiliary F0 features. Second, a cascaded network structure is utilized to simultaneously model the long- and short-term dependences of quasi-periodic signals such as speech. The performances of single-tone sinusoid and speech generations are evaluated. The experimental results show the effectiveness of the PDCNNs for unseen auxiliary F0 features and the effectiveness of the cascaded structure for speech generation.      
### 45.Decentralized Frequency Control using Packet-based Energy Coordination  [ :arrow_down: ](https://arxiv.org/pdf/2007.05624.pdf)
>  This paper presents a novel frequency-responsive control scheme for demand-side resources, such as electric water heaters. A frequency-dependent control law is designed to provide damping from distributed energy resources (DERs) in a fully decentralized fashion. This local control policy represents a frequency-dependent threshold for each DER that ensures that the aggregate response provides damping during frequency deviations. The proposed decentralized policy is based on an adaptation of a packet-based DER coordination scheme where each device send requests for energy access (also called an "energy packet") to an aggregator. The number of previously accepted active packets can then be used a-priori to form an online estimate of the aggregate damping capability of the DER fleet in a dynamic power system. A simple two-area power system is used to illustrate and validate performance of the decentralized control policy and the accuracy of the online damping estimating for a fleet of 400,000 DERs.      
### 46.A Distributed Model Predictive Wind Farm Controller for Active Power Control  [ :arrow_down: ](https://arxiv.org/pdf/2007.05621.pdf)
>  Due to the fluctuating nature of the wind and the increasing use of wind energy as a power source, wind power will have an increasing negative influence on the stability of the power grid. In this paper, a model predictive control strategy is introduced that not only stabilizes the power produced by wind farms, but also creates the possibility to perform power reference tracking with wind farms. With power reference tracking, it is possible for grid operators to adapt the power production to a change in the power demand and to counteract fluctuations that are introduced by other power generators. In this way, wind farms can actually contribute to the stabilization of the power grid when this is necessary instead of negatively influencing it. A low-fidelity control-oriented wind farm model is developed and employed in the developed distributed model predictive controller. In this control model, the wake dynamics are taken into account and consequently, the model's order is relatively large. This makes it, from a computational point of view, challenging for a centralized model predictive control to provide real-time control for large wind farms. Therefore, the controller proposed in this paper is a distributed model predictive control. Here, the central control problem is divided into smaller local control problems that are solved in parallel on local controllers, which significantly reduces the computational complexity and brings the application of model predictive control in a wind farm a step closer to practical implementation. The proposed control solution is tested in simulations on a 10 and 64 turbine wind farm.      
### 47.EMIXER: End-to-end Multimodal X-ray Generation via Self-supervision  [ :arrow_down: ](https://arxiv.org/pdf/2007.05597.pdf)
>  Deep generative models have enabled the automated synthesis of high-quality data for diverse applications. However, the most effective generative models are specialized to data from a single domain (e.g., images or text). Real-world applications such as healthcare require multi-modal data from multiple domains (e.g., both images and corresponding text), which are difficult to acquire due to limited availability and privacy concerns and are much harder to synthesize. To tackle this joint synthesis challenge, we propose an End-to-end MultImodal X-ray genERative model (EMIXER) for jointly synthesizing x-ray images and corresponding free-text reports, all conditional on diagnosis labels. EMIXER is an conditional generative adversarial model by 1) generating an image based on a label, 2) encoding the image to a hidden embedding, 3) producing the corresponding text via a hierarchical decoder from the image embedding, and 4) a joint discriminator for assessing both the image and the corresponding text. EMIXER also enables self-supervision to leverage vast amount of unlabeled data. Extensive experiments with real X-ray reports data illustrate how data augmentation using synthesized multimodal samples can improve the performance of a variety of supervised tasks including COVID-19 X-ray classification with very limited samples. The quality of generated images and reports are also confirmed by radiologists. We quantitatively show that EMIXER generated synthetic datasets can augment X-ray image classification, report generation models to achieve 5.94% and 6.9% improvement on models trained only on real data samples. Taken together, our results highlight the promise of state of generative models to advance clinical machine learning.      
### 48.Experiments of Federated Learning for COVID-19 Chest X-ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.05592.pdf)
>  AI plays an important role in COVID-19 identification. Computer vision and deep learning techniques can assist in determining COVID-19 infection with Chest X-ray Images. However, for the protection and respect of the privacy of patients, the hospital's specific medical-related data did not allow leakage and sharing without permission. Collecting such training data was a major challenge. To a certain extent, this has caused a lack of sufficient data samples when performing deep learning approaches to detect COVID-19. Federated Learning is an available way to address this issue. It can effectively address the issue of data silos and get a shared model without obtaining local data. In the work, we propose the use of federated learning for COVID-19 data training and deploy experiments to verify the effectiveness. And we also compare performances of four popular models (MobileNet, ResNet18, MoblieNet, and COVID-Net) with the federated learning framework and without the framework. This work aims to inspire more researches on federated learning about COVID-19.      
### 49.A New Frame Synchronization Algorithm for Linear Periodic Channels with Memory -- Full Version  [ :arrow_down: ](https://arxiv.org/pdf/2007.05571.pdf)
>  Identifying the start time of a sequence of symbols received at the receiver, commonly referred to as \emph{frame synchronization}, is a critical task for achieving good performance in digital communications systems employing time-multiplexed transmission. In this work we focus on \emph{frame synchronization} for linear channels with memory in which the channel impulse response is periodic and the additive Gaussian noise is correlated and cyclostationary. Such channels appear in many communications scenarios, including narrowband power line communications and interference-limited wireless communications. We derive frame synchronization algorithms based on simplifications of the optimal likelihood-ratio test, assuming the channel impulse response is unknown at the receiver, which is applicable to many practical scenarios. The computational complexity of each of the derived algorithms is characterized, and a procedure for selecting nearly optimal synchronization sequences is proposed. The algorithms derived in this work achieve better performance than the noncoherent correlation detector, and, in fact, facilitate a controlled tradeoff between complexity and performance.      
### 50.Mismatched Data Detection in Massive MU-MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2007.06491.pdf)
>  We investigate mismatched data detection for massive multi-user (MU) multiple-input multiple-output (MIMO) wireless systems in which the prior distribution of the transmit signal used in the data detector differs from the true prior. In order to minimize the performance loss caused by the prior mismatch, we include a tuning stage into the recently proposed large-MIMO approximate message passing (LAMA) algorithm, which enables the development of data detectors with optimal as well as sub-optimal parameter tuning. We show that carefully-designed priors enable the design of simpler and computationally more efficient data detection algorithms compared to LAMA that uses the optimal prior, while achieving near-optimal error-rate performance. In particular, we demonstrate that a hardware-friendly approximation of the exact prior enables the design of low-complexity data detectors that achieve near individually-optimal performance. Furthermore, for Gaussian priors and uniform priors within a hypercube covering the quadrature amplitude modulation (QAM) constellation, our performance analysis recovers classical and recent results on linear and non-linear massive MU-MIMO data detection, respectively.      
### 51.DeepHAZMAT: Hazardous Materials Sign Detection and Segmentation with Restricted Computational Resources  [ :arrow_down: ](https://arxiv.org/pdf/2007.06392.pdf)
>  One of the most challenging and non-trivial tasks in robotics-based rescue operations is Hazardous Materials or HAZMATs sign detection within the operation field, in order to prevent other unexpected disasters. Each Hazmat sign has a specific meaning that the rescue robot should detect and interpret it to take a safe action, accordingly. Accurate Hazmat detection and real-time processing are the two most important factors in such robotics applications. Furthermore, we also have to cope with some secondary challengers such as image distortion problems and restricted CPU and computational resources which are embedded in a rescue robot. In this paper, we propose a CNN-Based pipeline called DeepHAZMAT for detecting and segmenting Hazmats in four steps; 1) optimising the number of input images that are fed into the CNN network, 2) using the YOLOv3-tiny structure to collect the required visual information from the hazardous areas, 3) Hazmat sign segmentation and separation from the background using GrabCut technique, and 4) post-processing the result with morphological operators and convex hall algorithm. In spite of the utilisation of a very limited memory and CPU resources, the experimental results show the proposed method has successfully maintained a better performance in terms of detection-speed and detection-accuracy, compared with the state-of-the-art methods.      
### 52.Synthetic Aperture Radar Image Formation with Uncertainty Quantification  [ :arrow_down: ](https://arxiv.org/pdf/2007.06380.pdf)
>  Synthetic aperture radar (SAR) is a day or night any-weather imaging modality that is an important tool in remote sensing. Most existing SAR image formation methods result in a maximum a posteriori image which approximates the reflectivity of an unknown ground scene. This single image provides no quantification of the certainty with which the features in the estimate should be trusted. In addition, finding the mode is generally not the best way to interrogate a posterior. This paper addresses these issues by introducing a sampling framework to SAR image formation. A hierarchical Bayesian model is constructed using conjugate priors that directly incorporate coherent imaging and the problematic speckle phenomenon which is known to degrade image quality. Samples of the resulting posterior as well as parameters governing speckle and noise are obtained using a Gibbs sampler. These samples may then be used to compute estimates, and also to derive other statistics like variance which aid in uncertainty quantification. The latter information is particularly important in SAR, where ground truth images even for synthetically-created examples are typically unknown. An example result using real-world data shows that the sampling-based approach introduced here to SAR image formation provides parameter-free estimates with improved contrast and significantly reduced speckle, as well as unprecedented uncertainty quantification information.      
### 53.Seeing Eye-to-Eye? A Comparison of Object Recognition Performance in Humans and Deep Convolutional Neural Networks under Image Manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2007.06294.pdf)
>  For a considerable time, deep convolutional neural networks (DCNNs) have reached human benchmark performance in object recognition. On that account, computational neuroscience and the field of machine learning have started to attribute numerous similarities and differences to artificial and biological vision. This study aims towards a behavioral comparison of visual core object recognition between humans and feedforward neural networks in a classification learning paradigm on an ImageNet data set. For this purpose, human participants (n = 65) competed in an online experiment against different feedforward DCNNs. The designed approach based on a typical learning process of seven different monkey categories included a training and validation phase with natural examples, as well as a testing phase with novel shape and color manipulations. Analyses of accuracy revealed that humans not only outperform DCNNs on all conditions, but also display significantly greater robustness towards shape and most notably color alterations. Furthermore, a precise examination of behavioral patterns highlights these findings by revealing independent classification errors between the groups. The obtained results endorse an implementation of recurrent circuits similar to the primate ventral stream in artificial vision models as a way to achieve adequate object generalization abilities across unexperienced manipulations.      
### 54.Robotized Ultrasound Imaging of the Peripheral Arteries -- a Phantom Study  [ :arrow_down: ](https://arxiv.org/pdf/2007.06278.pdf)
>  The first choice in diagnostic imaging for patients suffering from peripheral arterial disease is 2D ultrasound (US). However, for a proper imaging process, a skilled and experienced sonographer is required. Additionally, it is a highly user-dependent operation. A robotized US system that autonomously scans the peripheral arteries has the potential to overcome these limitations. In this work, we extend a previously proposed system by a hierarchical image analysis pipeline based on convolutional neural networks in order to control the robot. The system was evaluated by checking its feasibility to keep the vessel lumen of a leg phantom within the US image while scanning along the artery. In 100 % of the images acquired during the scan process the whole vessel lumen was visible. While defining an insensitivity margin of 2.74 mm, the mean absolute distance between vessel center and the horizontal image center line was 2.47 mm and 3.90 mm for an easy and complex scenario, respectively. In conclusion, this system presents the basis for fully automatized peripheral artery imaging in humans using a radiation-free approach.      
### 55.OpenStreetMap: Challenges and Opportunities in Machine Learning and Remote Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2007.06277.pdf)
>  OpenStreetMap (OSM) is a community-based, freely available, editable map service that was created as an alternative to authoritative ones. Given that it is edited mainly by volunteers with different mapping skills, the completeness and quality of its annotations are heterogeneous across different geographical locations. Despite that, OSM has been widely used in several applications in {Geosciences}, Earth Observation and environmental sciences. In this work, we present a review of recent methods based on machine learning to improve and use OSM data. Such methods aim either 1) at improving the coverage and quality of OSM layers, typically using GIS and remote sensing technologies, or 2) at using the existing OSM layers to train models based on image data to serve applications like navigation or {land use} classification. We believe that OSM (as well as other sources of open land maps) can change the way we interpret remote sensing data and that the synergy with machine learning can scale participatory map making and its quality to the level needed to serve global and up-to-date land mapping.      
### 56.Neural Network Verification through Replication  [ :arrow_down: ](https://arxiv.org/pdf/2007.06226.pdf)
>  A system identification based approach to neural network model replication is presented and the application of model replication to verification of fundamental, single hidden layer, neural network systems is demonstrated. The presented approach serves as a means to partially address the problem of verifying that a neural network implementation meets a provided specification given only grey-box access to the implemented network. The procedure developed involves stimulating a neural network with a chosen signal, extracting a replicated model from the response, and systematically checking that the replicated model is output-equivalent to a specified model in order to verify that the grey-box system under test is implemented to specification without direct access to its hidden parameters. The replication step is introduced to provide an inherent guarantee that the stimulus signals employed yield sufficient test coverage. This method is investigated as a neural network focused nonlinear counterpart to the traditional verification of circuits through system identification. A strategy for choosing the stimulus is provided and an algorithm for verifying that the resulting response is indicative of a specification-compliant neural network system under test is derived. We find that the method can reliably detect defects in small neural networks or in small sub-circuits within larger neural networks.      
### 57.Sensor-Aided Learning for Wi-Fi Positioning with Beacon Channel State Information  [ :arrow_down: ](https://arxiv.org/pdf/2007.06204.pdf)
>  Because each indoor site has its own radio propagation characteristics, a site survey process is essential to optimize a Wi-Fi ranging strategy for range-based positioning solutions. This paper studies an unsupervised learning technique that autonomously investigates the characteristics of the surrounding environment using sensor data accumulated while users use a positioning application. Using the collected sensor data, the device trajectory can be regenerated, and a Wi-Fi ranging module is trained to make the shape of the estimated trajectory using Wi-Fi similar to that obtained from sensors. In this process, the ranging module learns the way to identify the channel conditions from each Wi-Fi access point (AP) and produce ranging results accordingly. Furthermore, we collect the channel state information (CSI) from beacon frames and evaluate the benefit of using CSI in addition to received signal strength (RSS) measurements. When CSI is available, the ranging module can identify more diverse channel conditions from each AP, and thus more precise positioning results can be achieved. The effectiveness of the proposed learning technique is verified using a real-time positioning application implemented on a PC platform.      
### 58.Data-driven geophysics: from dictionary learning to deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.06183.pdf)
>  Understanding the principles of geophysical phenomena is an essential and challenging task. Model-driven approaches have supported the development of geophysics for a long time; however, such methods suffer from the curse of dimensionality and may inaccurately model the subsurface. Data-driven techniques may overcome these issues with increasingly available geophysical data. In this article, we review the basic concepts of and recent advances in data-driven approaches from dictionary learning to deep learning in a variety of geophysical scenarios, including seismic and earthquake data processing, inversion, and interpretation. We present a coding tutorial and a summary of tips for beginners and interested geophysical readers to rapidly explore deep learning. Some promising directions are provided for future research involving deep learning in geophysics, such as unsupervised learning, transfer learning, multimodal deep learning, federated learning, uncertainty estimation, and activate learning.      
### 59.Approximations of the Reproducing Kernel Hilbert Space (RKHS) Embedding Method over Manifolds  [ :arrow_down: ](https://arxiv.org/pdf/2007.06163.pdf)
>  The reproducing kernel Hilbert space (RKHS) embedding method is a recently introduced estimation approach that seeks to identify the unknown or uncertain function in the governing equations of a nonlinear set of ordinary differential equations (ODEs). While the original state estimate evolves in Euclidean space, the function estimate is constructed in an infinite-dimensional RKHS that must be approximated in practice. When a finite-dimensional approximation is constructed using a basis defined in terms of shifted kernel functions centered at the observations along a trajectory, the RKHS embedding method can be understood as a data-driven approach. This paper derives sufficient conditions that ensure that approximations of the unknown function converge in a Sobolev norm over a submanifold that supports the dynamics. Moreover, the rate of convergence for the finite-dimensional approximations is derived in terms of the fill distance of the samples in the embedded manifold. Numerical simulation of an example problem is carried out to illustrate the qualitative nature of convergence results derived in the paper.      
### 60.OtoWorld: Towards Learning to Separate by Learning to Move  [ :arrow_down: ](https://arxiv.org/pdf/2007.06123.pdf)
>  We present OtoWorld, an interactive environment in which agents must learn to listen in order to solve navigational tasks. The purpose of OtoWorld is to facilitate reinforcement learning research in computer audition, where agents must learn to listen to the world around them to navigate. OtoWorld is built on three open source libraries: OpenAI Gym for environment and agent interaction, PyRoomAcoustics for ray-tracing and acoustics simulation, and nussl for training deep computer audition models. OtoWorld is the audio analogue of GridWorld, a simple navigation game. OtoWorld can be easily extended to more complex environments and games. To solve one episode of OtoWorld, an agent must move towards each sounding source in the auditory scene and "turn it off". The agent receives no other input than the current sound of the room. The sources are placed randomly within the room and can vary in number. The agent receives a reward for turning off a source. We present preliminary results on the ability of agents to win at OtoWorld. OtoWorld is open-source and available.      
### 61.Competition in Electric Autonomous Mobility on Demand Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.06051.pdf)
>  This paper investigates the impacts of competition in autonomous mobility-on-demand systems. By adopting a network-flow based formulation, we first determine the optimal strategies of profit-maximizing platform operators in monopoly and duopoly markets, including the optimal prices of rides. Furthermore, we characterize the platform operator's profits and the consumer surplus. We show that for the duopoly, the optimal equilibrium prices for rides have to be symmetric between the firms. Then, in order to study the benefits of introducing competition in the market, we derive universal theoretical bounds on the ratio of prices for rides, aggregate demand served, profits of the firms, and consumer surplus between the monopolistic and the duopolistic setting. We discuss how consumers' firm loyalty affects each of the aforementioned metrics. Finally, using the Manhattan network and demand data, we quantify the efficacy of static pricing and routing policies and compare it to real-time model predictive policies.      
### 62.Augmenting Differentiable Simulators with Neural Networks to Close the Sim2Real Gap  [ :arrow_down: ](https://arxiv.org/pdf/2007.06045.pdf)
>  We present a differentiable simulation architecture for articulated rigid-body dynamics that enables the augmentation of analytical models with neural networks at any point of the computation. Through gradient-based optimization, identification of the simulation parameters and network weights is performed efficiently in preliminary experiments on a real-world dataset and in sim2sim transfer applications, while poor local optima are overcome through a random search approach.      
### 63.MeDaS: An open-source platform as service to help break the walls between medicine and informatics  [ :arrow_down: ](https://arxiv.org/pdf/2007.06013.pdf)
>  In the past decade, deep learning (DL) has achieved unprecedented success in numerous fields including computer vision, natural language processing, and healthcare. In particular, DL is experiencing an increasing development in applications for advanced medical image analysis in terms of analysis, segmentation, classification, and furthermore. On the one hand, tremendous needs that leverage the power of DL for medical image analysis are arising from the research community of a medical, clinical, and informatics background to jointly share their expertise, knowledge, skills, and experience. On the other hand, barriers between disciplines are on the road for them often hampering a full and efficient collaboration. To this end, we propose our novel open-source platform, i.e., MeDaS -- the MeDical open-source platform as Service. To the best of our knowledge, MeDaS is the first open-source platform proving a collaborative and interactive service for researchers from a medical background easily using DL related toolkits, and at the same time for scientists or engineers from information sciences to understand the medical knowledge side. Based on a series of toolkits and utilities from the idea of RINV (Rapid Implementation aNd Verification), our proposed MeDaS platform can implement pre-processing, post-processing, augmentation, visualization, and other phases needed in medical image analysis. Five tasks including the subjects of lung, liver, brain, chest, and pathology, are validated and demonstrated to be efficiently realisable by using MeDaS.      
### 64.Universal Approximation Power of Deep Neural Networks via Nonlinear Control Theory  [ :arrow_down: ](https://arxiv.org/pdf/2007.06007.pdf)
>  In this paper, we explain the universal approximation capabilities of deep neural networks through geometric nonlinear control. Inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. Many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with n states to approximate arbitrarily well any continuous function defined on a compact subset of R^n. We further show this result to hold for very simple architectures, where the weights only need to assume two values. The key technical contribution consists of relating the universal approximation problem to controllability of an ensemble of control systems corresponding to a residual network, and to leverage classical Lie algebraic techniques to characterize controllability.      
### 65.Multi-Modality Information Fusion for Radiomics-based Neural Architecture Search  [ :arrow_down: ](https://arxiv.org/pdf/2007.06002.pdf)
>  'Radiomics' is a method that extracts mineable quantitative features from radiographic images. These features can then be used to determine prognosis, for example, predicting the development of distant metastases (DM). Existing radiomics methods, however, require complex manual effort including the design of hand-crafted radiomic features and their extraction and selection. Recent radiomics methods, based on convolutional neural networks (CNNs), also require manual input in network architecture design and hyper-parameter tuning. Radiomic complexity is further compounded when there are multiple imaging modalities, for example, combined positron emission tomography - computed tomography (PET-CT) where there is functional information from PET and complementary anatomical localization information from computed tomography (CT). Existing multi-modality radiomics methods manually fuse the data that are extracted separately. Reliance on manual fusion often results in sub-optimal fusion because they are dependent on an 'expert's' understanding of medical images. In this study, we propose a multi-modality neural architecture search method (MM-NAS) to automatically derive optimal multi-modality image features for radiomics and thus negate the dependence on a manual process. We evaluated our MM-NAS on the ability to predict DM using a public PET-CT dataset of patients with soft-tissue sarcomas (STSs). Our results show that our MM-NAS had a higher prediction accuracy when compared to state-of-the-art radiomics methods.      
### 66.Differentiable Programming for Hyperspectral Unmixing using a Physics-based Dispersion Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.05996.pdf)
>  Hyperspectral unmixing is an important remote sensing task with applications including material identification and analysis. Characteristic spectral features make many pure materials identifiable from their visible-to-infrared spectra, but quantifying their presence within a mixture is a challenging task due to nonlinearities and factors of variation. In this paper, spectral variation is considered from a physics-based approach and incorporated into an end-to-end spectral unmixing algorithm via differentiable programming. The dispersion model is introduced to simulate realistic spectral variation, and an efficient method to fit the parameters is presented. Then, this dispersion model is utilized as a generative model within an analysis-by-synthesis spectral unmixing algorithm. Further, a technique for inverse rendering using a convolutional neural network to predict parameters of the generative model is introduced to enhance performance and speed when training data is available. Results achieve state-of-the-art on both infrared and visible-to-near-infrared (VNIR) datasets, and show promise for the synergy between physics-based models and deep learning in hyperspectral unmixing in the future.      
### 67.CellEVAC: An adaptive guidance system for crowd evacuation through behavioral optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.05963.pdf)
>  A critical aspect of crowds' evacuation processes is the dynamism of individual decision making. Here, we investigate how to favor a coordinated group dynamic through optimal exit-choice instructions using behavioral strategy optimization. We propose and evaluate an adaptive guidance system (Cell-based Crowd Evacuation, CellEVAC) that dynamically allocates colors to cells in a cell-based pedestrian positioning infrastructure, to provide efficient exit-choice indications. The operational module of CellEVAC implements an optimized discrete-choice model that integrates the influential factors that would make evacuees adapt their exit choice. To optimize the model, we used a simulation-optimization modeling framework that integrates microscopic pedestrian simulation based on the classical Social Force Model. We paid particular attention to safety by using Pedestrian Fundamental Diagrams that model the dynamics of the exit gates. CellEVAC has been tested in a simulated real scenario (Madrid Arena) under different external pedestrian flow patterns that simulate complex pedestrian interactions. Results showed that CellEVAC outperforms evacuation processes in which the system is not used, with an exponential improvement as interactions become complex. We compared our system with an existing approach based on Cartesian Genetic Programming. Our system exhibited a better overall performance in terms of safety, evacuation time, and the number of revisions of exit-choice decisions. Further analyses also revealed that Cartesian Genetic Programming generates less natural pedestrian reactions and movements than CellEVAC. The fact that the decision logic module is built upon a behavioral model seems to favor a more natural and effective response. We also found that our proposal has a positive influence on evacuations even for a low compliance rate (40%).      
### 68.Dual Adversarial Network: Toward Real-world Noise Removal and Noise Generation  [ :arrow_down: ](https://arxiv.org/pdf/2007.05946.pdf)
>  Real-world image noise removal is a long-standing yet very challenging task in computer vision. The success of deep neural network in denoising stimulates the research of noise generation, aiming at synthesizing more clean-noisy image pairs to facilitate the training of deep denoisers. In this work, we propose a novel unified framework to simultaneously deal with the noise removal and noise generation tasks. Instead of only inferring the posteriori distribution of the latent clean image conditioned on the observed noisy image in traditional MAP framework, our proposed method learns the joint distribution of the clean-noisy image pairs. Specifically, we approximate the joint distribution with two different factorized forms, which can be formulated as a denoiser mapping the noisy image to the clean one and a generator mapping the clean image to the noisy one. The learned joint distribution implicitly contains all the information between the noisy and clean images, avoiding the necessity of manually designing the image priors and noise assumptions as traditional. Besides, the performance of our denoiser can be further improved by augmenting the original training dataset with the learned generator. Moreover, we propose two metrics to assess the quality of the generated noisy image, for which, to the best of our knowledge, such metrics are firstly proposed along this research line. Extensive experiments have been conducted to demonstrate the superiority of our method over the state-of-the-arts both in the real noise removal and generation tasks. The training and testing code is available at <a class="link-external link-https" href="https://github.com/zsyOAOA/DANet" rel="external noopener nofollow">this https URL</a>.      
### 69.A Three-limb Teleoperated Robotic System with Foot Control for Flexible Endoscopic Surgery  [ :arrow_down: ](https://arxiv.org/pdf/2007.05927.pdf)
>  Flexible endoscopy requires high skills to manipulate both the endoscope and associated instruments. In most robotic flexible endoscopic systems, the endoscope and instruments are controlled separately by two operators, which may result in communication errors and inefficient operation. We present a novel teleoperation robotic endoscopic system that can be commanded by a surgeon alone. This 13 degrees-of-freedom (DoF) system integrates a foot-controlled robotic flexible endoscope and two hand-controlled robotic endoscopic instruments (a robotic grasper and a robotic cauterizing hook). A foot-controlled human-machine interface maps the natural foot gestures to the 4-DoF movements of the endoscope, and two hand-controlled interfaces map the movements of the two hands to the two instruments individually. The proposed robotic system was validated in an ex-vivo experiment carried out by six subjects, where foot control was also compared with a sequential clutch-based hand control scheme. The participants could successfully teleoperate the endoscope and the two instruments to cut the tissues at scattered target areas in a porcine stomach. Foot control yielded 43.7% faster task completion and required less mental effort as compared to the clutch-based hand control scheme. The system introduced in this paper is intuitive for three-limb manipulation even for operators without experience of handling the endoscope and robotic instruments. This three-limb teleoperated robotic system enables one surgeon to intuitively control three endoscopic tools which normally require two operators, leading to reduced manpower, less communication errors, and improved efficiency.      
### 70.Lightweight Modules for Efficient Deep Learning based Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2007.05835.pdf)
>  Low level image restoration is an integral component of modern artificial intelligence (AI) driven camera pipelines. Most of these frameworks are based on deep neural networks which present a massive computational overhead on resource constrained platform like a mobile phone. In this paper, we propose several lightweight low-level modules which can be used to create a computationally low cost variant of a given baseline model. Recent works for efficient neural networks design have mainly focused on classification. However, low-level image processing falls under the image-to-image' translation genre which requires some additional computational modules not present in classification. This paper seeks to bridge this gap by designing generic efficient modules which can replace essential components used in contemporary deep learning based image restoration networks. We also present and analyse our results highlighting the drawbacks of applying depthwise separable convolutional kernel (a popular method for efficient classification network) for sub-pixel convolution based upsampling (a popular upsampling strategy for low-level vision applications). This shows that concepts from domain of classification cannot always be seamlessly integrated into image-to-image translation tasks. We extensively validate our findings on three popular tasks of image inpainting, denoising and super-resolution. Our results show that proposed networks consistently output visually similar reconstructions compared to full capacity baselines with significant reduction of parameters, memory footprint and execution speeds on contemporary mobile devices.      
### 71.A Tutorial on Graph Theory for Brain Signal Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.05800.pdf)
>  This tutorial paper refers to the use of graph-theoretic concepts for analyzing brain signals. For didactic purposes it splits into two parts: theory and application. In the first part, we commence by introducing some basic elements from graph theory and stemming algorithmic tools, which can be employed for data-analytic purposes. Next, we describe how these concepts are adapted for handling evolving connectivity and gaining insights into network reorganization. Finally, the notion of signals residing on a given graph is introduced and elements from the emerging field of graph signal processing (GSP) are provided. The second part serves as a pragmatic demonstration of the tools and techniques described earlier. It is based on analyzing a multi-trial dataset containing single-trial responses from a visual ERP paradigm. The paper ends with a brief outline of the most recent trends in graph theory that are about to shape brain signal processing in the near future and a more general discussion on the relevance of graph-theoretic methodologies for analyzing continuous-mode neural recordings.      
### 72.Robust Tracking and Model Following Controller Based on Higher Order Sliding Mode Control and Observation: With an Application to MagLev System  [ :arrow_down: ](https://arxiv.org/pdf/2007.05750.pdf)
>  This paper deals with the design of robust tracking and model following (RTMF) controller for linear time-invariant (LTI) systems with uncertainties. The controller is based on the second order sliding mode (SOSM) algorithm (super twisting) which is the most effective and popular in the family of higher order sliding modes (HOSM). The use of super twisting algorithm (STA) eliminates the chattering problem encountered in traditional sliding mode control while retaining its robustness properties. The proposed robust tracking controller can guarantee the asymptotic stability of tracking error in the presence of time varying uncertain parameter and exogenous disturbances. Finally, this strategy is implemented on a magnetic levitation system (MagLev) which is inherently unstable and nonlinear. While implementing this proposed RTMF controller for MagLev system, a super twisting observer (STO) is used to estimate the unknown state i.e the velocity of the ball which is not directly available for measurement. It has been observed that the RTMF controller based on STA-STO pair, is not good enough to achieve SOSM for a chosen sliding surface using continuous control. As a remedy, continuous RTMF controller based on STA is implemented with a higher order sliding mode observer (HOSMO). The simulated as well as the experimental results are provided to illustrate the effectiveness of the proposed controller-observers pair.      
### 73.Do We Need Sound for Sound Source Localization?  [ :arrow_down: ](https://arxiv.org/pdf/2007.05722.pdf)
>  During the performance of sound source localization which uses both visual and aural information, it presently remains unclear how much either image or sound modalities contribute to the result, i.e. do we need both image and sound for sound source localization? To address this question, we develop an unsupervised learning system that solves sound source localization by decomposing this task into two steps: (i) "potential sound source localization", a step that localizes possible sound sources using only visual information (ii) "object selection", a step that identifies which objects are actually sounding using aural information. Our overall system achieves state-of-the-art performance in sound source localization, and more importantly, we find that despite the constraint on available information, the results of (i) achieve similar performance. From this observation and further experiments, we show that visual information is dominant in "sound" source localization when evaluated with the currently adopted benchmark dataset. Moreover, we show that the majority of sound-producing objects within the samples in this dataset can be inherently identified using only visual information, and thus that the dataset is inadequate to evaluate a system's capability to leverage aural information. As an alternative, we present an evaluation protocol that enforces both visual and aural information to be leveraged, and verify this property through several experiments.      
### 74.Capacity Improvement in Wideband Reconfigurable Intelligent Surface-Aided Cell-Free Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.05680.pdf)
>  Thanks to the strong ability against the inter-cell interference, cell-free network has been considered as a promising technique to improve the network capacity of future wireless systems. However, for further capacity enhancement, it requires to deploy more base stations (BSs) with high cost and power consumption. To address the issue, inspired by the recently proposed technique called reconfigurable intelligent surface (RIS), we propose the concept of RIS-aided cell-free network to improve the network capacity with low cost and power consumption. Then, for the proposed RIS-aided cell-free network in the typical wideband scenario, we formulate the joint precoding design problem at the BSs and RISs to maximize the network capacity. Due to the non-convexity and high complexity of the formulated problem, we develop an alternating optimization algorithm to solve this challenging problem. Note that most of the considered scenarios in existing works are special cases of the general scenario in this paper, and the proposed joint precoding framework can also serve as a general solution to maximize the capacity in most of existing RIS-aided scenarios. Finally, simulation results verify that, compared with the conventional cell-free network, the network capacity of the proposed scheme can be improved significantly.      
### 75.Hardware Implementation of Deep Network Accelerators Towards Healthcare and Biomedical Applications  [ :arrow_down: ](https://arxiv.org/pdf/2007.05657.pdf)
>  With the advent of dedicated Deep Learning (DL) accelerators and neuromorphic processors, new opportunities are emerging for applying deep and Spiking Neural Network (SNN) algorithms to healthcare and biomedical applications at the edge. This can facilitate the advancement of the medical Internet of Things (IoT) systems and Point of Care (PoC) devices. In this paper, we provide a tutorial describing how various technologies ranging from emerging memristive devices, to established Field Programmable Gate Arrays (FPGAs), and mature Complementary Metal Oxide Semiconductor (CMOS) technology can be used to develop efficient DL accelerators to solve a wide variety of diagnostic, pattern recognition, and signal processing problems in healthcare. Furthermore, we explore how spiking neuromorphic processors can complement their DL counterparts for processing biomedical signals. After providing the required background, we unify the sparsely distributed research on neural network and neuromorphic hardware implementations as applied to the healthcare domain. In addition, we benchmark various hardware platforms by performing a biomedical electromyography (EMG) signal processing task and drawing comparisons among them in terms of inference delay and energy. Finally, we provide our analysis of the field and share a perspective on the advantages, disadvantages, challenges, and opportunities that different accelerators and neuromorphic processors introduce to healthcare and biomedical domains. This paper can serve a large audience, ranging from nanoelectronics researchers, to biomedical and healthcare practitioners in grasping the fundamental interplay between hardware, algorithms, and clinical adoption of these tools, as we shed light on the future of deep networks and spiking neuromorphic processing systems as proponents for driving biomedical circuits and systems forward.      
### 76.Adaptive Superresolution in Deconvolution of Sparse Peaks  [ :arrow_down: ](https://arxiv.org/pdf/2007.05636.pdf)
>  The aim of this paper is to investigate superresolution in deconvolution driven by sparsity priors. The observed signal is a convolution of an original signal with a continuous kernel.With the prior knowledge that the original signal can be considered as a sparse combination of Dirac delta peaks, we seek to estimate the positions and amplitudes of these peaks by solving a finite dimensional convex problem on a computational grid. Because, the support of the original signal may or may not be on this grid, by studying the discrete deconvolution of sparse peaks using L1-norm sparsity prior, we confirm recent observations that canonically the discrete reconstructions will result in multiple peaks at grid points adjacent to the location of the true peak. Owning to the complexity of this problem, we analyse carefully the de-convolution of single peaks on a grid and gain a strong insight about the dependence of the reconstructed magnitudes on the exact peak location. This in turn allows us to infer further information on recovering the location of the exact peaks i.e. to perform super-resolution. We analyze in detail the possible cases that can appear and based on our theoretical findings, we propose an self-driven adaptive grid approach that allows to perform superresolution in one-dimensional and multi-dimensional spaces. With the view that the current study can provide a further step in the development of more robust algorithms for the detection of single molecules in fluorescence microscopy or identification of characteristic frequencies in spectral analysis, we demonstrate how the proposed approach can recover sparse signals using simulated clusters of point sources (peaks) of low-resolution in one and two-dimensional spaces.      
### 77.Batch-Incremental Triplet Sampling for Training Triplet Networks Using Bayesian Updating Theorem  [ :arrow_down: ](https://arxiv.org/pdf/2007.05610.pdf)
>  Variants of Triplet networks are robust entities for learning a discriminative embedding subspace. There exist different triplet mining approaches for selecting the most suitable training triplets. Some of these mining methods rely on the extreme distances between instances, and some others make use of sampling. However, sampling from stochastic distributions of data rather than sampling merely from the existing embedding instances can provide more discriminative information. In this work, we sample triplets from distributions of data rather than from existing instances. We consider a multivariate normal distribution for the embedding of each class. Using Bayesian updating and conjugate priors, we update the distributions of classes dynamically by receiving the new mini-batches of training data. The proposed triplet mining with Bayesian updating can be used with any triplet-based loss function, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss. Accordingly, Our triplet mining approaches are called Bayesian Updating Triplet (BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is being used. Experimental results on two public datasets, namely MNIST and histopathology colorectal cancer (CRC), substantiate the effectiveness of the proposed triplet mining method.      
### 78.Partial Altruism is Worse than Complete Selfishness in Nonatomic Congestion Games  [ :arrow_down: ](https://arxiv.org/pdf/2007.05591.pdf)
>  We seek to understand the fundamental mathematics governing infrastructure-scale interactions between humans and machines, particularly when the machines' intended purpose is to influence and optimize the behavior of the humans. To that end, this paper investigates the worst-case congestion that can arise in nonatomic network congestion games when a fraction of the traffic is completely altruistic (e.g., benevolent self-driving cars) and the remainder is completely selfish (e.g., human commuters). We study the worst-case harm of altruism in such scenarios in terms of the perversity index, or the worst-case equilibrium congestion cost resulting from the presence of altruistic traffic, relative to the congestion cost which would result if all traffic were selfish. We derive a tight bound on the perversity index for the class of series-parallel network congestion games with convex latency functions, and show three facts: First, the harm of altruism is maximized when exactly half of the traffic is altruistic, but it gracefully vanishes when the fraction of altruistic traffic approaches either 0 or 1. Second, we show that the harm of altruism is linearly increasing in a natural measure of the "steepness" of network latency functions. Finally, we show that for any nontrivial fraction of altruistic traffic, the harm of altruism exceeds the price of anarchy associated with all-selfish traffic: in a sense, partial altruism is worse than complete selfishness.      
### 79.Multi-Domain Image Completion for Random Missing Input Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.05534.pdf)
>  Multi-domain data are widely leveraged in vision applications taking advantage of complementary information from different modalities, e.g., brain tumor segmentation from multi-parametric magnetic resonance imaging (MRI). However, due to possible data corruption and different imaging protocols, the availability of images for each domain could vary amongst multiple data sources in practice, which makes it challenging to build a universal model with a varied set of input data. To tackle this problem, we propose a general approach to complete the random missing domain(s) data in real applications. Specifically, we develop a novel multi-domain image completion method that utilizes a generative adversarial network (GAN) with a representational disentanglement scheme to extract shared skeleton encoding and separate flesh encoding across multiple domains. We further illustrate that the learned representation in multi-domain image completion could be leveraged for high-level tasks, e.g., segmentation, by introducing a unified framework consisting of image completion and segmentation with a shared content encoder. The experiments demonstrate consistent performance improvement on three datasets for brain tumor segmentation, prostate segmentation, and facial expression image completion respectively.      
### 80.Attention Guided Anomaly Localization in Images  [ :arrow_down: ](https://arxiv.org/pdf/1911.08616.pdf)
>  Anomaly localization is an important problem in computer vision which involves localizing anomalous regions within images with applications in industrial inspection, surveillance, and medical imaging. This task is challenging due to the small sample size and pixel coverage of the anomaly in real-world scenarios. Most prior works need to use anomalous training images to compute a class-specific threshold to localize anomalies. Without the need of anomalous training images, we propose Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), which localizes the anomaly with a convolutional latent variable to preserve the spatial information. In the unsupervised setting, we propose an attention expansion loss where we encourage CAVGA to focus on all normal regions in the image. Furthermore, in the weakly-supervised setting we propose a complementary guided attention loss, where we encourage the attention map to focus on all normal regions while minimizing the attention map corresponding to anomalous regions in the image. CAVGA outperforms the state-of-the-art (SOTA) anomaly localization methods on MVTec Anomaly Detection (MVTAD), modified ShanghaiTech Campus (mSTC) and Large-scale Attention based Glaucoma (LAG) datasets in the unsupervised setting and when using only 2% anomalous images in the weakly-supervised setting. CAVGA also outperforms SOTA anomaly detection methods on the MNIST, CIFAR-10, Fashion-MNIST, MVTAD, mSTC and LAG datasets.      
