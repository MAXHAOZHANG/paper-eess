# ArXiv eess --Tue, 21 Jul 2020
### 1.Convolutional Image Edge Detection Using Ultrafast Photonic Spiking VCSEL Neurons  [ :arrow_down: ](https://arxiv.org/pdf/2007.10309.pdf)
>  We report experimentally and in theory on the detection of edge information in digital images using ultrafast spiking optical artificial neurons towards convolutional neural networks (CNNs). In tandem with traditional convolution techniques, a photonic neuron model based on a Vertical-Cavity Surface Emitting Laser (VCSEL) is implemented experimentally to threshold and activate fast spiking responses upon the detection of target edge features in digital images. Edges of different directionalities are detected using individual kernel operators and complete image edge detection is achieved using gradient magnitude. Importantly, the neuromorphic (brain-like) image edge detection system of this work uses commercially sourced VCSELs exhibiting spiking responses at sub-nanosecond rates (many orders of magnitude faster than biological neurons) and operating at the telecom wavelength of 1300 nm; hence making our approach compatible with optical communication and data-center technologies. These results therefore have exciting prospects for ultrafast photonic implementations of neural networks towards computer vision and decision making systems for future artificial intelligence applications.      
### 2.Distributed Control of Charging for Electric Vehicle Fleets under Dynamic Transformer Ratings  [ :arrow_down: ](https://arxiv.org/pdf/2007.10304.pdf)
>  Due to their large power draws and increasing adoption rates, electric vehicles (EVs) will become a significant challenge for electric distribution grids. However, with proper charging control strategies, the challenge can be mitigated without the need for expensive grid reinforcements. This manuscript presents and analyzes new distributed charging control methods to coordinate EV charging under nonlinear transformer temperature ratings. Specifically, we assess the trade-offs between required data communications, computational efficiency, and optimality guarantees for different control strategies based on a convex relaxation of the underlying nonlinear transformer temperature dynamics. Classical distributed control methods such as those based on dual decomposition and alternating direction method of multipliers (ADMM) are compared against the new Augmented Lagrangian-based Alternating Direction Inexact Newton (ALADIN) method and a novel low-information, look-ahead version of packetized energy management (PEM). These algorithms are implemented and analyzed for two case studies on residential and commercial EV fleets. Simulation results validate the new methods and provide insights into key trade-offs.      
### 3.Across-domains transferability of Deep-RED in de-noising and compressive sensing recovery of seismic data  [ :arrow_down: ](https://arxiv.org/pdf/2007.10250.pdf)
>  In the past decade, deep learning algorithms gained a remarkable interest in the signal processing community. The availability of big datasets and advanced computational resources resulted in developing efficient algorithms. However, such algorithms are biased towards the training dataset. Thus, the transferability of deep-learning-based operators are challenging, especially when the goal is to apply the learned operator on a new dataset/domain. Lack of transferability of learned operator across domains hinders the applicability of deep learning algorithms in processing seismic data. Unlike camera images, the comprehensively labeled seismic datasets are not available. Moreover, from one task to another, the training parameters should be tuned. To remedy this shortcoming, we have developed a workflow that transfers the learned operator from the camera images to the seismic domain, without modifying its training parameters. The similarities in the algorithms and optimization methods in camera and seismic data processing allow us to do so. Accordingly, by incorporating feed-forward de-noising convolutional neural networks (DnCNN) in regularization by de-noising regularizer, we formulate two transferable optimization problems for de-noising and compressive sensing recovery of seismic data. Simulated and real-world data examples show the efficiency of our proposed workflow.      
### 4.Image reconstruction in dynamic inverse problems with temporal models  [ :arrow_down: ](https://arxiv.org/pdf/2007.10238.pdf)
>  The paper surveys variational approaches for image reconstruction in dynamic inverse problems. Emphasis is on methods that rely on parametrised temporal models. These are here encoded as diffeomorphic deformations with time dependent parameters, or as motion constrained reconstruction where the motion model is given by a partial differential equation. The survey also includes recent development in integrating deep learning for solving these computationally demanding variational methods. Examples are given for 2D dynamic tomography, but methods apply to general inverse problems.      
### 5.A Control Theoretical Adaptive Human Pilot Model: Theory and Experimental Validation  [ :arrow_down: ](https://arxiv.org/pdf/2007.10216.pdf)
>  This paper proposes an adaptive human pilot model that is able to mimic the crossover model in the presence of uncertainties. The proposed structure is based on the model reference adaptive control, and the adaptive laws are obtained using the Lyapunov-Krasovskii stability criteria. The model can be employed for human-in-the-loop stability and performance analyses incorporating different types of controllers and plant types. For validation purposes, an experimental setup is employed to collect data and a statistical analysis is conducted to measure the predictive power of the pilot model.      
### 6."Self-Wiener" Filtering: Non-Iterative Data-Driven Robust Deconvolution of Deterministic Signals  [ :arrow_down: ](https://arxiv.org/pdf/2007.10164.pdf)
>  We consider the fundamental problem of robust deconvolution, and particularly the recovery of an unknown deterministic signal convolved with a known filter and corrupted by additive noise. We present a novel, non-iterative data-driven approach. Specifically, our algorithm works in the frequency-domain, where it tries to mimic the optimal unrealizable Wiener-like filter as if the unknown deterministic signal were known. This leads to a threshold-type regularized estimator, where the threshold value at each frequency is found in a fully data-driven manner. We provide an analytical performance analysis, and derive approximate closed-form expressions for the residual Mean Squared Error (MSE) of our proposed estimator in the low and high Signal-to-Noise Ratio (SNR) regimes. We show analytically that in the low SNR regime our method provides enhanced noise suppression, and in the high SNR regime it approaches the performance of the optimal unrealizable solution. Further, as we demonstrate in simulations, our solution is highly suitable for (approximately) bandlimited or frequency-domain sparse signals, and provides a significant gain of several dBs relative to other methods in the resulting MSE.      
### 7.Inter-Mobile-Device Distance Estimation using Network Localization Algorithms for Digital Contact Logging Applications  [ :arrow_down: ](https://arxiv.org/pdf/2007.10162.pdf)
>  Mobile applications are being developed for automated logging of contacts via Bluetooth to help scale up digital contact tracing efforts in the context of the ongoing COVID-19 pandemic. A useful component of such applications is inter-device distance estimation, which can be formulated as a network localization problem. We survey several approaches and evaluate the performance of each on real and simulated Bluetooth Low Energy (BLE) measurement datasets with respect to both distance estimate accuracy and the proximity detection problem. We investigate the effects of obstructions like pockets, differences between device models, and the environment (i.e. indoors or outdoors) on performance. We conclude that while direct estimation can provide the best proximity detection when Received Signal Strength Indicator (RSSI) measurements are available, network localization algorithms like Isomap, Local Linear Embedding, and the spring model outperform direct estimation in the presence of missing or very noisy measurements. The spring model consistently achieves the best distance estimation accuracy.      
### 8.Nested Hybrid Cylindrical Array Design and DoA Estimation for Massive IoT Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.10155.pdf)
>  Reducing cost and power consumption while maintaining high network access capability is a key physical-layer requirement of massive Internet of Things (mIoT) networks. Deploying a hybrid array is a cost- and energy-efficient way to meet the requirement, but would penalize system degree of freedom (DoF) and channel estimation accuracy. This is because signals from multiple antennas are combined by a radio frequency (RF) network of the hybrid array. This paper presents a novel hybrid uniform circular cylindrical array (UCyA) for mIoT networks. We design a nested hybrid beamforming structure based on sparse array techniques and propose the corresponding channel estimation method based on the second-order channel statistics. As a result, only a small number of RF chains are required to preserve the DoF of the UCyA. We also propose a new tensor-based two-dimensional (2-D) direction-of-arrival (DoA) estimation algorithm tailored for the proposed hybrid array. The algorithm suppresses the noise components in all tensor modes and operates on the signal data model directly, hence improving estimation accuracy with an affordable computational complexity. Corroborated by a Cramer-Rao lower bound (CRLB) analysis, simulation results show that the proposed hybrid UCyA array and the DoA estimation algorithm can accurately estimate the 2-D DoAs of a large number of IoT devices.      
### 9.PAC Model Checking of Black-Box Continuous-Time Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.10141.pdf)
>  In this paper we present a novel model checking approach to finite-time safety verification of black-box continuous-time dynamical systems within the framework of probably approximately correct (PAC) learning. The black-box dynamical systems are the ones, for which no model is given but whose states changing continuously through time within a finite time interval can be observed at some discrete time instants for a given input. The new model checking approach is termed as PAC model checking due to incorporation of learned models with correctness guarantees expressed using the terms error probability and confidence. Based on the error probability and confidence level, our approach provides statistically formal guarantees that the time-evolving trajectories of the black-box dynamical system over finite time horizons fall within the range of the learned model plus a bounded interval, contributing to insights on the reachability of the black-box system and thus on the satisfiability of its safety requirements. The learned model together with the bounded interval is obtained by scenario optimization, which boils down to a linear programming problem. Three examples demonstrate the performance of our approach.      
### 10.Consensus-Based Current Sharing and Voltage Balancing in DC Microgrids with Exponential Loads  [ :arrow_down: ](https://arxiv.org/pdf/2007.10134.pdf)
>  In this work, we present a novel consensus-based secondary control scheme for current sharing and voltage balancing in DC microgrids, composed of distributed generation units, dynamic RLC lines, and nonlinear ZIE (constant impedance, constant current, and exponential) loads. Situated atop a primary voltage control layer, our secondary controllers have a distributed structure, and utilize information exchanged over a communication network to compute necessary control actions. Besides showing that the desired objectives are always attained in steady state, we deduce sufficient conditions for the existence and uniqueness of an equilibrium point for constant power loads -- E loads with zero exponent. Our control design hinges only on the local parameters of the generation units, facilitating plug-and-play operations. We provide a voltage stability analysis, and illustrate the performance and robustness of our designs via simulations. All results hold for arbitrary, albeit connected, microgrid and communication network topologies.      
### 11.Information Freshness-Aware Task Offloading in Air-Ground Integrated Edge Computing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.10129.pdf)
>  This paper studies the problem of information freshness-aware task offloading in an air-ground integrated multi-access edge computing system, which is deployed by an infrastructure provider (InP). A third-party real-time application service provider provides computing services to the subscribed mobile users (MUs) with the limited communication and computation resources from the InP based on a long-term business agreement. Due to the dynamic characteristics, the interactions among the MUs are modelled by a non-cooperative stochastic game, in which the control policies are coupled and each MU aims to selfishly maximize its own expected long-term payoff. To address the Nash equilibrium solutions, we propose that each MU behaves in accordance with the local system states and conjectures, based on which the stochastic game is transformed into a single-agent Markov decision process. Moreover, we derive a novel online deep reinforcement learning (RL) scheme that adopts two separate double deep Q-networks for each MU to approximate the Q-factor and the post-decision Q-factor. Using the proposed deep RL scheme, each MU in the system is able to make decisions without a priori statistical knowledge of dynamics. Numerical experiments examine the potentials of the proposed scheme in balancing the age of information and the energy consumption.      
### 12.Human-like Energy Management Based on Deep Reinforcement Learning and Historical Driving Experiences  [ :arrow_down: ](https://arxiv.org/pdf/2007.10126.pdf)
>  Development of hybrid electric vehicles depends on an advanced and efficient energy management strategy (EMS). With online and real-time requirements in mind, this article presents a human-like energy management framework for hybrid electric vehicles according to deep reinforcement learning methods and collected historical driving data. The hybrid powertrain studied has a series-parallel topology, and its control-oriented modeling is founded first. Then, the distinctive deep reinforcement learning (DRL) algorithm, named deep deterministic policy gradient (DDPG), is introduced. To enhance the derived power split controls in the DRL framework, the global optimal control trajectories obtained from dynamic programming (DP) are regarded as expert knowledge to train the DDPG model. This operation guarantees the optimality of the proposed control architecture. Moreover, the collected historical driving data based on experienced drivers are employed to replace the DP-based controls, and thus construct the human-like EMSs. Finally, different categories of experiments are executed to estimate the optimality and adaptability of the proposed human-like EMS. Improvements in fuel economy and convergence rate indicate the effectiveness of the constructed control structure.      
### 13.Towards robust sensing for Autonomous Vehicles: An adversarial perspective  [ :arrow_down: ](https://arxiv.org/pdf/2007.10115.pdf)
>  Autonomous Vehicles rely on accurate and robust sensor observations for safety critical decision-making in a variety of conditions. Fundamental building blocks of such systems are sensors and classifiers that process ultrasound, RADAR, GPS, LiDAR and camera signals~\cite{Khan2018}. It is of primary importance that the resulting decisions are robust to perturbations, which can take the form of different types of nuisances and data transformations, and can even be adversarial perturbations (APs). Adversarial perturbations are purposefully crafted alterations of the environment or of the sensory measurements, with the objective of attacking and defeating the autonomous systems. A careful evaluation of the vulnerabilities of their sensing system(s) is necessary in order to build and deploy safer systems in the fast-evolving domain of AVs. To this end, we survey the emerging field of sensing in adversarial settings: after reviewing adversarial attacks on sensing modalities for autonomous systems, we discuss countermeasures and present future research directions.      
### 14.On the Use of AI for Satellite Communications  [ :arrow_down: ](https://arxiv.org/pdf/2007.10110.pdf)
>  This document presents an initial approach to the investigation and development of artificial intelligence (AI) mechanisms in satellite communication (SatCom) systems. We first introduce the nowadays SatCom operations which are strongly dependent on the human intervention. Along with those use cases, we present an initial way of automatizing some of those tasks and we show the key AI tools capable of dealing with those challenges. Finally, the long term AI developments in the SatCom sector is discussed.      
### 15.A Machine Learning Approach for Task and Resource Allocation in Mobile Edge Computing Based Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.10102.pdf)
>  In this paper, a joint task, spectrum, and transmit power allocation problem is investigated for a wireless network in which the base stations (BSs) are equipped with mobile edge computing (MEC) servers to jointly provide computational and communication services to users. Each user can request one computational task from three types of computational tasks. Since the data size of each computational task is different, as the requested computational task varies, the BSs must adjust their resource (subcarrier and transmit power) and task allocation schemes to effectively serve the users. This problem is formulated as an optimization problem whose goal is to minimize the maximal computational and transmission delay among all users. A multi-stack reinforcement learning (RL) algorithm is developed to solve this problem. Using the proposed algorithm, each BS can record the historical resource allocation schemes and users' information in its multiple stacks to avoid learning the same resource allocation scheme and users' states, thus improving the convergence speed and learning efficiency. Simulation results illustrate that the proposed algorithm can reduce the number of iterations needed for convergence and the maximal delay among all users by up to 18% and 11.1% compared to the standard Q-learning algorithm.      
### 16.Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure  [ :arrow_down: ](https://arxiv.org/pdf/2007.10088.pdf)
>  Complex devices are connected daily and eagerly generate vast streams of multidimensional state measurements. These devices often operate in distinct modes based on external conditions (day/night, occupied/vacant, etc.), and to prevent complete or partial system outage, we would like to recognize as early as possible when these devices begin to operate outside the normal modes. Unfortunately, it is often impractical or impossible to predict failures using rules or supervised machine learning, because failure modes are too complex, devices are too new to adequately characterize in a specific environment, or environmental change puts the device into an unpredictable condition. We propose an unsupervised anomaly detection method that creates a negative sample from the positive, observed sample, and trains a classifier to distinguish between positive and negative samples. Using the Contraction Principle, we explain why such a classifier ought to establish suitable decision boundaries between normal and anomalous regions, and show how Integrated Gradients can attribute the anomaly to specific variables within the anomalous state vector. We have demonstrated that negative sampling with random forest or neural network classifiers yield significantly higher AUC scores than Isolation Forest, One Class SVM, and Deep SVDD, against (a) a synthetic dataset with dimensionality ranging between 2 and 128, with 1, 2, and 3 modes, and with and without noise dimensions; (b) four standard benchmark datasets; and (c) a multidimensional, multimodal dataset from real climate control devices. Finally, we describe how negative sampling with neural network classifiers have been successfully deployed at large scale to predict failures in real time in over 15,000 climate-control and power meter devices in 145 Google office buildings.      
### 17.Multispectral Pan-sharpening via Dual-Channel Convolutional Network with Convolutional LSTM Based Hierarchical Spatial-Spectral Feature Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2007.10060.pdf)
>  Multispectral pan-sharpening aims at producing a high resolution (HR) multispectral (MS) image in both spatial and spectral domains by fusing a panchromatic (PAN) image and a corresponding MS image. In this paper, we propose a novel dual-channel network (DCNet) framework for MS pan-sharpening. In our DCNet, the dual-channel backbone involves a spatial channel to capture spatial information with a 2D CNN, and a spectral channel to extract spectral information with a 3D CNN. This heterogeneous 2D/3D CNN architecture can minimize causing spectral information distortion, which typically happens in conventional 2D CNN models. In order to fully integrate the spatial and spectral features captured from different levels, we introduce a multi-level fusion strategy. Specifically, a spatial-spectral CLSTM (S$^2$-CLSTM) module is proposed for fusing the hierarchical spatial and spectral features, which can effectively capture correlations among multi-level features. The S$^2$-CLSTM module attaches two fusion ways: the intra-level fusion via bi-directional lateral connections and inter-level fusion via the cell state in the S$^2$-CLSTM. Finally, the ideal HR-MS image is recovered by a reconstruction module. Extensive experiments have been conducted at both simulated lower scale and the original scale of real-world datasets. Compared with the state-of-the-art methods, the proposed DCNet achieves superior or competitive performance.      
### 18.Universal Loss Reweighting to Balance Lesion Size Inequality in 3D Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.10033.pdf)
>  Target imbalance affects the performance of recent deep learning methods in many medical image segmentation tasks. It is a twofold problem: class imbalance - positive class (lesion) size compared to negative class (non-lesion) size; lesion size imbalance - large lesions overshadows small ones (in the case of multiple lesions per image). While the former was addressed in multiple works, the latter lacks investigation. We propose a loss reweighting approach to increase the ability of the network to detect small lesions. During the learning process, we assign a weight to every image voxel. The assigned weights are inversely proportional to the lesion volume, thus smaller lesions get larger weights. We report the benefit from our method for well-known loss functions, including Dice Loss, Focal Loss, and Asymmetric Similarity Loss. Additionally, we compare our results with other reweighting techniques: Weighted Cross-Entropy and Generalized Dice Loss. Our experiments show that inverse weighting considerably increases the detection quality, while preserves the delineation quality on a state-of-the-art level. We publish a complete experimental pipeline for two publicly available datasets of CT images: LiTS and LUNA16 (<a class="link-external link-https" href="https://github.com/neuro-ml/inverse_weighting" rel="external noopener nofollow">this https URL</a>). We also show results on a private database of MR images for the task of multiple brain metastases delineation.      
### 19.Revisiting Efficient Multi-Step Nonlinearity Compensation with Machine Learning: An Experimental Demonstration  [ :arrow_down: ](https://arxiv.org/pdf/2007.10025.pdf)
>  Efficient nonlinearity compensation in fiber-optic communication systems is considered a key element to go beyond the "capacity crunch''. One guiding principle for previous work on the design of practical nonlinearity compensation schemes is that fewer steps lead to better systems. In this paper, we challenge this assumption and show how to carefully design multi-step approaches that provide better performance--complexity trade-offs than their few-step counterparts. We consider the recently proposed learned digital backpropagation (LDBP) approach, where the linear steps in the split-step method are re-interpreted as general linear functions, similar to the weight matrices in a deep neural network. Our main contribution lies in an experimental demonstration of this approach for a 25 Gbaud single-channel optical transmission system. It is shown how LDBP can be integrated into a coherent receiver DSP chain and successfully trained in the presence of various hardware impairments. Our results show that LDBP with limited complexity can achieve better performance than standard DBP by using very short, but jointly optimized, finite-impulse response filters in each step. This paper also provides an overview of recently proposed extensions of LDBP and we comment on potentially interesting avenues for future work.      
### 20.Improving Memory Utilization in Convolutional Neural Network Accelerators  [ :arrow_down: ](https://arxiv.org/pdf/2007.09963.pdf)
>  While the accuracy of convolutional neural networks has achieved vast improvements by introducing larger and deeper network architectures, also the memory footprint for storing their parameters and activations has increased. This trend especially challenges power- and resource-limited accelerator designs, which are often restricted to store all network data in on-chip memory to avoid interfacing energy-hungry external memories. Maximizing the network size that fits on a given accelerator thus requires to maximize its memory utilization. While the traditionally used ping-pong buffering technique is mapping subsequent activation layers to disjunctive memory regions, we propose a mapping method that allows these regions to overlap and thus utilize the memory more efficiently. This work presents the mathematical model to compute the maximum activations memory overlap and thus the lower bound of on-chip memory needed to perform layer-by-layer processing of convolutional neural networks on memory-limited accelerators. Our experiments with various real-world object detector networks show that the proposed mapping technique can decrease the activations memory by up to 32.9%, reducing the overall memory for the entire network by up to 23.9% compared to traditional ping-pong buffering. For higher resolution de-noising networks, we achieve activation memory savings of 48.8%. Additionally, we implement a face detector network on an FPGA-based camera to validate these memory savings on a complete end-to-end system.      
### 21.A Do-It-Yourself (DIY) Light-Wave Sensing and Communication Project: Low-Cost, Portable, Effective, and Fun  [ :arrow_down: ](https://arxiv.org/pdf/2007.09891.pdf)
>  A do-it-yourself (DIY) light-wave sensing (LWS) and communication project was developed to generate interest and clarify basic electromagnetic (EM) and wireless communication concepts among students at different education levels from middle school to undergraduate. This paper demonstrates the nature of the project and its preliminary effectiveness. Wireless sensing/communication concepts are generally considered hard to comprehend being underpinned only by theoretical coursework and occasional simulations. Further, K-12 schools and small academic institutions may not have the resources necessary to produce tangible demonstrations for clarification. The consequent lack of affordable hands-on experiences fails to motivate and engage students. The DIY-LWS is intended to make wireless concepts more understandable and less esoteric by linking fundamental concepts with familiar technologies such as solar cells, visible lights, and smartphones. It is also intended to pique student interest by allowing them to personally assemble, operate, and explore a light-based wireless communication system. A preliminary assessment is used to determine the student base knowledge level and enthusiasm for wireless and related core topics. Students are instructed to assemble and test their own DIY-LWS hardware to provide a hands-on experience and stimulate further exploration. Short lectures are given to link conceptual ideas to the real-world phenomena. Finally, students are re-assessed to quantify any change in conceptual understanding. The DIY-LWS kits have been used in multiple events with students at different levels from secondary to high schools to college. Pre- and post-assessments revealed pronounced improvements (the number of correct answers doubled) in student understanding of EM concepts. Instructors observed tremendous interest and excitement among the students during and after the experiments.      
### 22.Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.09854.pdf)
>  Witnessing the success of deep learning neural networks in natural image processing, an increasing number of studies have been proposed to develop deep-learning-based frameworks for medical image segmentation. However, since the pixel-wise annotation of medical images is laborious and expensive, the amount of annotated data is usually deficient to well-train a neural network. In this paper, we propose a semi-supervised approach to train neural networks with limited labeled data and a large quantity of unlabeled images for medical image segmentation. A novel pseudo-label (namely self-loop uncertainty), generated by recurrently optimizing the neural network with a self-supervised task, is adopted as the ground-truth for the unlabeled images to augment the training set and boost the segmentation accuracy. The proposed self-loop uncertainty can be seen as an approximation of the uncertainty estimation yielded by ensembling multiple models with a significant reduction of inference time. Experimental results on two publicly available datasets demonstrate the effectiveness of our semi-supervied approach.      
### 23.Joint Transmit Power and Placement Optimization for URLLC-enabled UAV Relay Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.09850.pdf)
>  This letter considers an unmanned aerial vehicle (UAV)-enabled relay communication system for delivering latency-critical messages with ultra-high reliability, where the relay is operating under amplifier-and-forward (AF) mode. We aim to jointly optimize the UAV location and power to minimize decoding error probability while guaranteeing the latency constraints. Both the free-space channel model and three-dimensional (3-D) channel model are considered. For the first model, we propose a low-complexity iterative algorithm to solve the problem, while globally optimal solution is derived for the case when the signal-to-noise ratio (SNR) is extremely high. For the second model, we also propose a low-complexity iterative algorithm to solve the problem. Simulation results confirm the performance advantages of our proposed algorithms.      
### 24.Wireless Performance Evaluation of Building Layouts: Closed-Form Computation of Figures of Merit  [ :arrow_down: ](https://arxiv.org/pdf/2007.09829.pdf)
>  This paper presents a part of our ground-breaking work on evaluation of buildings in terms of wireless friendliness in the building-design stage. The main goal is to devise construction practices that provide for a good performance of wireless networks deployed in buildings. In this paper, the interference gain (IG) and power gain (PG) are defined as two figures of merit (FoM) of the wireless performance of buildings. The FoMs bridge the gap between building design and wireless communications industries. An approach to derive exact closed-form equations for these FoMs is proposed for the first time. The derived analytic expressions facilitate straightforward and more computationally efficient numerical evaluation of the proposed FoMs as compared to Monte Carlo simulations for well-known indoor propagation models. It is shown that the derived closed-form expression can be readily employed to evaluate the impact of building properties, such as the sizes and the aspect ratios (ARs) of rooms, on the wireless performance. The proposed approach sheds light to architects on evaluation and design of wireless-friendly building layouts.      
### 25.E$^2$Net: An Edge Enhanced Network for Accurate Liver and Tumor Segmentation on CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2007.09791.pdf)
>  Developing an effective liver and liver tumor segmentation model from CT scans is very important for the success of liver cancer diagnosis, surgical planning and cancer treatment. In this work, we propose a two-stage framework for 2D liver and tumor segmentation. The first stage is a coarse liver segmentation network, while the second stage is an edge enhanced network (E$^2$Net) for more accurate liver and tumor segmentation. E$^2$Net explicitly models complementary objects (liver and tumor) and their edge information within the network to preserve the organ and lesion boundaries. We introduce an edge prediction module in E$^2$Net and design an edge distance map between liver and tumor boundaries, which is used as an extra supervision signal to train the edge enhanced network. We also propose a deep cross feature fusion module to refine multi-scale features from both objects and their edges. E$^2$Net is more easily and efficiently trained with a small labeled dataset, and it can be trained/tested on the original 2D CT slices (resolve resampling error issue in 3D models). The proposed framework has shown superior performance on both liver and liver tumor segmentation compared to several state-of-the-art 2D, 3D and 2D/3D hybrid frameworks.      
### 26.Multi-stage Power Scheduling Framework for Data Center with Chilled Water Storage in Energy and Regulation Markets  [ :arrow_down: ](https://arxiv.org/pdf/2007.09770.pdf)
>  Leveraging electrochemical and thermal energy storage systems has been proposed as a strategy to reduce peak power in data centers. Thermal energy storage systems, such as chilled water tanks, have gained increasing attention in data centers for load shifting due to their relatively small capital and operational costs compared to electrochemical energy storage. However, there are few studies investigating the possibility of utilizing thermal energy storage system with resources to provide ancillary services (e.g., frequency regulation) to the grid. This paper proposes a synergistic control strategy for the data center with a chilled water storage providing frequency regulation service by adjusting the chiller capacity, storage charging rate, and IT server CPU frequency. Then, a three-stage multi-market scheduling framework based on a model predictive control scheme is developed to minimize operational costs of data centers participating in both energy and regulation markets. The framework solves a power baseline scheduling problem, a regulation reserve problem, and a real-time power signal tracking problem sequentially. Simulation results show that utilizing the thermal energy storage can increase the regulation capacity bid, reduce energy costs and demand charges, and also harvest frequency regulation revenues. The proposed multi-market scheduling framework in a span of two days can reduce the operational costs up to 8.8% ($1,606.4) compared to the baseline with 0.2% (\$38.7) energy cost reduction, 6.5% (\$1,179.4) from demand reduction, and 2.1% (\$338.3) from regulation revenues.      
### 27.Full Quaternion Representation of Color images: A Case Study on QSVD-based Color Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2007.09758.pdf)
>  For many years, channels of a color image have been processed individually, or the image has been converted to grayscale one with respect to color image processing. Pure quaternion representation of color images solves this issue as it allows images to be processed in a holistic space. Nevertheless, it brings additional costs due to the extra fourth dimension. In this paper, we propose an approach for representing color images with full quaternion numbers that enables us to process color images holistically without additional cost in time, space and computation. With taking auto- and cross-correlation of color channels into account, an autoencoder neural network is used to generate a global model for transforming a color image into a full quaternion matrix. To evaluate the model, we use UCID dataset, and the results indicate that the model has an acceptable performance on color images. Moreover, we propose a compression method based on the generated model and QSVD as a case study. The method is compared with the same compression method using pure quaternion representation and is assessed with UCID dataset. The results demonstrate that the compression method using the proposed full quaternion representation fares better than the other in terms of time, quality, and size of compressed files.      
### 28.Using Deep Convolutional Neural Networks to Diagnose COVID-19 From Chest X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.09695.pdf)
>  The COVID-19 epidemic has become a major safety and health threat worldwide. Imaging diagnosis is one of the most effective ways to screen COVID-19. This project utilizes several open-source or public datasets to present an open-source dataset of COVID-19 CXRs, named COVID-19-CXR-Dataset, and introduces a deep convolutional neural network model. The model validates on 740 test images and achieves 87.3% accuracy, 89.67 % precision, and 84.46% recall, and correctly classifies 98 out of 100 COVID-19 x-ray images in test set with more than 81% prediction probability under the condition of 95% confidence interval. This project may serve as a reference for other researchers aiming to advance the development of deep learning applications in medical imaging.      
### 29.Unified cross-modality feature disentangler for unsupervised multi-domain MRI abdomen organs segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.09669.pdf)
>  Our contribution is a unified cross-modality feature disentagling approach for multi-domain image translation and multiple organ segmentation. Using CT as the labeled source domain, our approach learns to segment multi-modal (T1-weighted and T2-weighted) MRI having no labeled data. Our approach uses a variational auto-encoder (VAE) to disentangle the image content from style. The VAE constrains the style feature encoding to match a universal prior (Gaussian) that is assumed to span the styles of all the source and target modalities. The extracted image style is converted into a latent style scaling code, which modulates the generator to produce multi-modality images according to the target domain code from the image content features. Finally, we introduce a joint distribution matching discriminator that combines the translated images with task-relevant segmentation probability maps to further constrain and regularize image-to-image (I2I) translations. We performed extensive comparisons to multiple state-of-the-art I2I translation and segmentation methods. Our approach resulted in the lowest average multi-domain image reconstruction error of 1.34$\pm$0.04. Our approach produced an average Dice similarity coefficient (DSC) of 0.85 for T1w and 0.90 for T2w MRI for multi-organ segmentation, which was highly comparable to a fully supervised MRI multi-organ segmentation network (DSC of 0.86 for T1w and 0.90 for T2w MRI).      
### 30.Meta-learning with Latent Space Clustering in Generative Adversarial Network for Speaker Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2007.09635.pdf)
>  The performance of most speaker diarization systems with x-vector embeddings is both vulnerable to noisy environments and lacks domain robustness. Earlier work on speaker diarization using generative adversarial network (GAN) with an encoder network (ClusterGAN) to project input x-vectors into a latent space has shown promising performance on meeting data. In this paper, we extend the ClusterGAN network to improve diarization robustness and enable rapid generalization across various challenging domains. To this end, we fetch the pre-trained encoder from the ClusterGAN and fine-tune it by using prototypical loss (meta-ClusterGAN or MCGAN) under the meta-learning paradigm. Experiments are conducted on CALLHOME telephonic conversations, AMI meeting data, DIHARD II (dev set) which includes challenging multi-domain corpus, and two child-clinician interaction corpora (ADOS, BOSCC) related to the autism spectrum disorder domain. Extensive analyses of the experimental data are done to investigate the effectiveness of the proposed ClusterGAN and MCGAN embeddings over x-vectors. The results show that the proposed embeddings with normalized maximum eigengap spectral clustering (NME-SC) back-end consistently outperform Kaldi state-of-the-art z-vector diarization system. Finally, we employ embedding fusion with x-vectors to provide further improvement in diarization performance. We achieve a relative diarization error rate (DER) improvement of 6.67% to 53.93% on the aforementioned datasets using the proposed fused embeddings over x-vectors. Besides, the MCGAN embeddings provide better performance in the number of speakers estimation and short speech segment diarization as compared to x-vectors and ClusterGAN in telephonic data.      
### 31.A geometric approach to separate the effects of magnetic susceptibility and chemical shift/exchange in a phantom with isotropic magnetic susceptibility  [ :arrow_down: ](https://arxiv.org/pdf/2007.09614.pdf)
>  Purpose: To separate the effects of magnetic susceptibility and chemical shift/exchange in a phantom with isotropic magnetic susceptibility. To generate a chemical shift/exchange-corrected quantitative susceptibility mapping (QSM) result. <br>Theory and Methods: Magnetic susceptibility and chemical shift/exchange are the properties of a material. Both are known to induce the resonance frequency shift in MRI. In current QSM, the susceptibility is reconstructed from the frequency shift, ignoring the contribution of the chemical shift/exchange. In this work, a simple geometric approach, which averages the frequency shift maps from three orthogonal B0 directions to generate a chemical shift/exchange map, is developed using the fact that the average nullifies the (isotropic) susceptibility effects. The resulting chemical shift/exchange map is subtracted from the total frequency shift, producing a frequency shift map solely from susceptibility. Finally, this frequency shift map is reconstructed to a susceptibility map using a QSM algorithm. The proposed method is validated in numerical simulations and applied to phantom experiments with olive oil, bovine serum albumin, ferritin, and iron oxide solutions. <br>Results: Both simulations and experiments confirm that the method successfully separates the contributions of the susceptibility and chemical shift/exchange, reporting the susceptibility and chemical shift/exchange of olive oil (susceptibility: 0.62 ppm, chemical shift: -3.60 ppm), bovine serum albumin (susceptibility: -0.059 ppm, chemical shift: 0.008 ppm), ferritin (susceptibility: 0.125 ppm, chemical shift: -0.005 ppm), and iron oxide (susceptibility: 0.30 ppm, chemical shift: -0.039 ppm) solutions. <br>Conclusion: The proposed method successfully separates the susceptibility and chemical shift/exchange in phantoms with isotropic magnetic susceptibility.      
### 32.Large Intelligent Surface Assisted Non-Orthogonal Multiple Access: Performance Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.09611.pdf)
>  Large intelligent surface (LIS) has recently emerged as a potential enabling technology for 6G networks, offering extended coverage and enhanced energy and spectral efficiency. In this work, motivated by its promising potentials, we investigate the error rate performance of LIS-assisted non-orthogonal multiple access (NOMA) networks. Specifically, we consider a downlink NOMA system, in which data transmission between a base station (BS) and $L$ NOMA users is assisted by an LIS comprising $M$ reflective elements. First, we derive the probability density function of the end-to-end wireless fading channels between the BS and NOMA users. Then, by leveraging the obtained results, we derive an approximate expression for the pairwise error probability (PEP) of NOMA users under the assumption of imperfect successive interference cancellation. Furthermore, accurate expressions for the PEP for $M = 1$ and large $M$ values ($M &gt; 10$) are presented in closed-form. To gain further insights into the system performance, an asymptotic expression for the PEP in high signal-to-noise ratio regime, the achievable diversity order, and a tight union bound on the bit error rate are provided. Finally, numerical and simulation results are presented to validate the derived mathematical results.      
### 33.DeepResp: Deep learning solution for respiration-induced B0 fluctuation artifacts in multi-slice GRE  [ :arrow_down: ](https://arxiv.org/pdf/2007.09597.pdf)
>  Respiration-induced B$_0$ fluctuation corrupts MRI images by inducing phase errors in k-space. A few approaches such as navigator have been proposed to correct for the artifacts at the expense of sequence modification. In this study, a new deep learning method, which is referred to as DeepResp, is proposed for reducing the respiration-artifacts in multi-slice gradient echo (GRE) images. DeepResp is designed to extract the respiration-induced phase errors from a complex image using deep neural networks. Then, the network-generated phase errors are applied to the k-space data, creating an artifact-corrected image. For network training, the computer-simulated images were generated using artifact-free images and respiration data. When evaluated, both simulated images and in-vivo images of two different breathing conditions (deep breathing and natural breathing) show improvements (simulation: normalized root-mean-square error (NRMSE) from 7.8% to 1.3%; structural similarity (SSIM) from 0.88 to 0.99; ghost-to-signal-ratio (GSR) from 7.9% to 0.6%; deep breathing: NRMSE from 13.9% to 5.8%; SSIM from 0.86 to 0.95; GSR 20.2% to 5.7%; natural breathing: NRMSE from 5.2% to 4.0%; SSIM from 0.94 to 0.97; GSR 5.7% to 2.8%). Our approach does not require any modification of the sequence or additional hardware, and may therefore find useful applications. Furthermore, the deep neural networks extract respiration-induced phase errors, which is more interpretable and reliable than results of end-to-end trained networks.      
### 34.A zero-carbon, reliable and affordable energy future in Australia  [ :arrow_down: ](https://arxiv.org/pdf/2007.09586.pdf)
>  Australia has one of the highest per capita consumption of energy and emissions of greenhouse gases in the world. It is also the global leader in rapid per capita annual deployment of new solar and wind energy, which is causing the country's emissions to decline. Australia is located at low-moderate latitudes along with three quarters of the global population. These factors make the Australian experience globally significant. In this study, we model a fully decarbonised electricity system together with complete electrification of heating, transport and industry in Australia leading to an 80% reduction in greenhouse gas emissions. An energy supply-demand balance is simulated based on long-term (10 years), high-resolution (half-hourly) meteorological and energy demand data. A significant feature of this model is that short-term off-river energy storage and distributed energy storage are utilised to support the large-scale integration of variable solar and wind energy. The results show that high levels of energy reliability and affordability can be effectively achieved through a synergy of flexible energy sources; interconnection of electricity grids over large areas; response from demand-side participation; and mass energy storage. This strategy represents a rapid and generic pathway towards zero-carbon energy futures within the Sunbelt.      
### 35.A Novel Nonlinear Leader-Follower Opinion Dynamics Model With Asynchronous Trust/Distrust Evolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.09561.pdf)
>  Trust and distrust are common in the opinion interactions among agents in social networks. It is believed in social psychology that opinion difference is usually an important factor affecting the trust/distrust level between neighboring agents. With that in mind, this paper proposes a nonlinear opinion dynamics model with asynchronous evolution of trust/distrust level based on opinion difference, in which the trust/distrust level between neighboring agents is portrayed as a nonlinear weight function of their opinion difference, and the asynchronous setting implies that each agent interacts with the neighbors to update the trust/distrust level and opinion at the times determined by its own will. The influence of an opinion leader with a firm stand on the formation of followers' opinions is considered. Based on infinite products of nonnegative matrices, a comprehensive theoretical analysis for the opinion dynamics is performed. Numerical simulations based on two well-known networks called the ``12 Angry Men" network and the Karate Club network in social psychology are provided to verify the correctness of the theoretical results.      
### 36.Progressive Multi-Scale Residual Network for Single Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.09552.pdf)
>  Super-resolution is a classical issue in image restoration field. In recent years, deep learning methods have achieved significant success in super-resolution topic, which concentrate on different elaborate network designs to exploit the image features more effectively. However, most of the networks focus on increasing the depth or width for superior capacities with a large number of parameters, which cause a high computation complexity cost and seldom focus on the inherent correlation of different features. This paper proposes a progressive multi-scale residual network (PMRN) for single image super-resolution problem by sequentially exploiting features with restricted parameters. Specifically, we design a progressive multi-scale residual block (PMRB) to progressively explore the multi-scale features with different layer combinations, aiming to consider the correlations of different scales. The combinations for feature exploitation are defined in a recursive fashion for introducing the non-linearity and better feature representation with limited parameters. Furthermore, we investigate a joint channel-wise and pixel-wise attention mechanism for comprehensive correlation exploration, termed as CPA, which is utilized in PMRB by considering both scale and bias factors for features in parallel. Experimental results show that proposed PMRN recovers structural textures more effectively with superior PSNR/SSIM results than other lightweight works. The extension model PMRN+ with self-ensemble achieves competitive or better results than large networks with much fewer parameters and lower computation complexity.      
### 37.Predicting risk of late age-related macular degeneration using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.09550.pdf)
>  By 2040, age-related macular degeneration (AMD) will affect approximately 288 million people worldwide. Identifying individuals at high risk of progression to late AMD, the sight-threatening stage, is critical for clinical actions, including medical interventions and timely monitoring. Although deep learning has shown promise in diagnosing/screening AMD using color fundus photographs, it remains difficult to predict individuals' risks of late AMD accurately. For both tasks, these initial deep learning attempts have remained largely unvalidated in independent cohorts. Here, we demonstrate how deep learning and survival analysis can predict the probability of progression to late AMD using 3,298 participants (over 80,000 images) from the Age-Related Eye Disease Studies AREDS and AREDS2, the largest longitudinal clinical trials in AMD. When validated against an independent test dataset of 601 participants, our model achieved high prognostic accuracy (five-year C-statistic 86.4 (95% confidence interval 86.2-86.6)) that substantially exceeded that of retinal specialists using two existing clinical standards (81.3 (81.1-81.5) and 82.0 (81.8-82.3), respectively). Interestingly, our approach offers additional strengths over the existing clinical standards in AMD prognosis (e.g., risk ascertainment above 50%) and is likely to be highly generalizable, given the breadth of training data from 82 US retinal specialty clinics. Indeed, during external validation through training on AREDS and testing on AREDS2 as an independent cohort, our model retained substantially higher prognostic accuracy than existing clinical standards. These results highlight the potential of deep learning systems to enhance clinical decision-making in AMD patients.      
### 38.Learning Geometry-Dependent and Physics-Based Inverse Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2007.09522.pdf)
>  Deep neural networks have shown great potential in image reconstruction problems in Euclidean space. However, many reconstruction problems involve imaging physics that are dependent on the underlying non-Euclidean geometry. In this paper, we present a new approach to learn inverse imaging that exploit the underlying geometry and physics. We first introduce a non-Euclidean encoding-decoding network that allows us to describe the unknown and measurement variables over their respective geometrical domains. We then learn the geometry-dependent physics in between the two domains by explicitly modeling it via a bipartite graph over the graphical embedding of the two geometry. We applied the presented network to reconstructing electrical activity on the heart surface from body-surface potential. In a series of generalization tasks with increasing difficulty, we demonstrated the improved ability of the presented network to generalize across geometrical changes underlying the data in comparison to its Euclidean alternatives.      
### 39.Deep Learning Based Brain Tumor Segmentation: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2007.09479.pdf)
>  Brain tumor segmentation is a challenging problem in medical image analysis. The goal of brain tumor segmentation is to generate accurate delineation of brain tumor regions with correctly located masks. In recent years, deep learning methods have shown very promising performance in solving various computer vision problems, such as image classification, object detection and semantic segmentation. A number of deep learning based methods have been applied to brain tumor segmentation and achieved impressive system performance. Considering state-of-the-art technologies and their performance, the purpose of this paper is to provide a comprehensive survey of recently developed deep learning based brain tumor segmentation techniques. The established works included in this survey extensively cover technical aspects such as the strengths and weaknesses of different approaches, pre- and post-processing frameworks, datasets and evaluation metrics. Finally, we conclude this survey by discussing the potential development in future research work.      
### 40.Automated Phenotyping via Cell Auto Training (CAT) on the Cell DIVE Platform  [ :arrow_down: ](https://arxiv.org/pdf/2007.09471.pdf)
>  We present a method for automatic cell classification in tissue samples using an automated training set from multiplexed immunofluorescence images. The method utilizes multiple markers stained in situ on a single tissue section on a robust hyperplex immunofluorescence platform (Cell DIVE, GE Healthcare) that provides multi-channel images allowing analysis at single cell/sub-cellular levels. The cell classification method consists of two steps: first, an automated training set from every image is generated using marker-to-cell staining information. This mimics how a pathologist would select samples from a very large cohort at the image level. In the second step, a probability model is inferred from the automated training set. The probabilistic model captures staining patterns in mutually exclusive cell types and builds a single probability model for the data cohort. We have evaluated the proposed approach to classify: i) immune cells in cancer and ii) brain cells in neurological degenerative diseased tissue with average accuracies above 95%.      
### 41.PSIGAN: Joint probabilistic segmentation and image distribution matching for unpaired cross-modality adaptation based MRI segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.09465.pdf)
>  We developed a new joint probabilistic segmentation and image distribution matching generative adversarial network (PSIGAN) for unsupervised domain adaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI) images. Our UDA approach models the co-dependency between images and their segmentation as a joint probability distribution using a new structure discriminator. The structure discriminator computes structure of interest focused adversarial loss by combining the generated pseudo MRI with probabilistic segmentations produced by a simultaneously trained segmentation sub-network. The segmentation sub-network is trained using the pseudo MRI produced by the generator sub-network. This leads to a cyclical optimization of both the generator and segmentation sub-networks that are jointly trained as part of an end-to-end network. Extensive experiments and comparisons against multiple state-of-the-art methods were done on four different MRI sequences totalling 257 scans for generating multi-organ and tumor segmentation. The experiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20 T2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right kidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for parotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our method achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the abdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for lung tumors.      
### 42.Controllability of reaction systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.09461.pdf)
>  Controlling a dynamical system is the ability of changing its configuration arbitrarily through a suitable choice of inputs. It is a very well studied concept in control theory, with wide ranging applications in medicine, biology, social sciences, engineering. We introduce in this article the concept of controllability of reaction systems as the ability of transitioning between any two states through a suitable choice of context sequences. We show that the problem is PSPACE-hard. We also introduce a model of oncogenic signalling based on reaction systems and use it to illustrate the intricacies of the controllability of reaction systems.      
### 43.ICA-UNet: ICA Inspired Statistical UNet for Real-time 3D Cardiac Cine MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.09455.pdf)
>  Real-time cine magnetic resonance imaging (MRI) plays an increasingly important role in various cardiac interventions. In order to enable fast and accurate visual assistance, the temporal frames need to be segmented on-the-fly. However, state-of-the-art MRI segmentation methods are used either offline because of their high computation complexity, or in real-time but with significant accuracy loss and latency increase (causing visually noticeable lag). As such, they can hardly be adopted to assist visual guidance. In this work, inspired by a new interpretation of Independent Component Analysis (ICA) for learning, we propose a novel ICA-UNet for real-time 3D cardiac cine MRI segmentation. Experiments using the MICCAI ACDC 2017 dataset show that, compared with the state-of-the-arts, ICA-UNet not only achieves higher Dice scores, but also meets the real-time requirements for both throughput and latency (up to 12.6X reduction), enabling real-time guidance for cardiac interventions without visual lag.      
### 44.Achieving Optimal Output Consensus for Discrete-time Linear Multi-agent Systems with Disturbances Rejection  [ :arrow_down: ](https://arxiv.org/pdf/2007.09452.pdf)
>  In this paper, an optimal output consensus problem is studied for discrete-time linear multi-agent systems subject to external disturbances. Each agent is assigned with a local cost function which is known only to itself. Distributed protocols are to be designed to guarantee an output consensus for these high-order agents and meanwhile minimize the aggregate cost as the sum of these local costs. To overcome the difficulties brought by high-order dynamics and external disturbances, we develop an embedded design and constructively present a distributed rule to solve this problem. The proposed control includes three terms: an optimal signal generator under a directed information graph, an observer-based compensator to reject these disturbances, and a reference tracking controller for these linear agents. It is shown to solve the formulated problem with some mild assumptions. Numerical examples are also provided to illustrate the effectiveness of our proposed distributed control laws.      
### 45.Optimal Consensus for Uncertain Multi-agent Systems by Output Feedbacks  [ :arrow_down: ](https://arxiv.org/pdf/2007.09441.pdf)
>  This paper investigates an optimal consensus problem for a group of uncertain linear multi-agent systems. All agents are allowed to possess parametric uncertainties that range over an arbitrarily large compact set. The goal is to collectively minimize a sum of local costs in a distributed fashion and finally achieve an output consensus on this optimal point using only output information of agents. By adding an optimal signal generator to generate the global optimal point, we convert this problem to several decentralized robust tracking problems. Output feedback integral control is constructively given to achieve an optimal consensus under a mild graph connectivity condition. The efficacy of this control is verified by a numerical example.      
### 46.3D Computational Cannula Fluorescence Microscopy enabled by Artificial Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.09430.pdf)
>  Computational Cannula Microscopy (CCM) is a high-resolution widefield fluorescence imaging approach deep inside tissue, which is minimally invasive. Rather than using conventional lenses, a surgical cannula acts as a lightpipe for both excitation and fluorescence emission, where computational methods are used for image visualization. Here, we enhance CCM with artificial neural networks to enable 3D imaging of cultured neurons and fluorescent beads, the latter inside a volumetric phantom. We experimentally demonstrate transverse resolution of ~6um, field of view ~200um and axial sectioning of ~50um for depths down to ~700um, all achieved with computation time of ~3ms/frame on a laptop computer.      
### 47.Simultaneous Detectability of Process and Sensor Faults: Application to Water Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.09401.pdf)
>  Detecting leaks in Water Distribution Networks (WDN) using sensors has become crucial towards an efficient management of water resources. The leak detection methods that use this data rely on the correctness of the acquired data. However, this assumption is often violated in practice. Consequently, leak detection under sensor faults is a problem of practical importance. This relates to the more general problem of simultaneous detectability in sensor and process faults for a class of systems modelled as a network, by exploiting the redundancies available through the topological relationship between the sensors. This paper hence aims at i) modeling WDN as graphs containing both systems and sensors faults ii) providing theoretical joint detectability results for such graphs and iii) applying these results to the scenario of leak identification under sensor faults conditions on real data issued from a rural WDN.      
### 48.Deep Multimodal Learning: Merging Sensory Data for Massive MIMO Channel Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2007.09366.pdf)
>  Existing work in intelligent communications has recently made preliminary attempts to utilize multi-source sensing information (MSI) to improve the system performance. However, the research on MSI aided intelligent communications has not yet explored how to integrate and fuse the multimodal sensory data, which motivates us to develop a systematic framework for wireless communications based on deep multimodal learning (DML). In this paper, we first present complete descriptions and heuristic understandings on the framework of DML based wireless communications, where core design choices are analyzed in the view of communications. Then, we develop several DML based architectures for channel prediction in massive multiple-input multiple-output (MIMO) systems that leverage various modality combinations and fusion levels. The case study of massive MIMO channel prediction offers an important example that can be followed in developing other DML based communication technologies. Simulations results demonstrate that the proposed DML framework can effectively exploit the constructive and complementary information of multimodal sensory data in various wireless communication scenarios.      
### 49.Identification and Stabilization of Critical Clusters in Inverter-Based Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2007.09347.pdf)
>  A new method for stability assessment of inverter-based microgrids is presented in this paper. It leverages the notion of critical clusters -- a localized group of inverters with parameters having the highest impact on the system stability. The spectrum of the weighted network admittance matrix is proposed to decompose a system into clusters and rank them based on their distances from the stability boundary. We show that each distinct eigenvalue of this matrix is associated with one cluster, and its eigenvectors reveal a set of inverters that participate most in the corresponding cluster. The least stable or unstable clusters correspond to higher values of respective eigenvalues of the weighted admittance matrix. We also establish an upper threshold for eigenvalues that determines the stability boundary of the entire system and demonstrate that this value depends only on the grid type (i.e. $R/X$ ratio of the network) and does not depend on the grid topology. Therefore, the proposed method provides the stability certificate based on this upper threshold and identifies the lines or inverter droop settings needed to be adjusted to restore or improve the stability.      
### 50.Multi-Task Neural Networks with Spatial Activation for Retinal Vessel Segmentation and Artery/Vein Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.09337.pdf)
>  Retinal artery/vein (A/V) classification plays a critical role in the clinical biomarker study of how various systemic and cardiovascular diseases affect the retinal vessels. Conventional methods of automated A/V classification are generally complicated and heavily depend on the accurate vessel segmentation. In this paper, we propose a multi-task deep neural network with spatial activation mechanism that is able to segment full retinal vessel, artery and vein simultaneously, without the pre-requirement of vessel segmentation. The input module of the network integrates the domain knowledge of widely used retinal preprocessing and vessel enhancement techniques. We specially customize the output block of the network with a spatial activation mechanism, which takes advantage of a relatively easier task of vessel segmentation and exploits it to boost the performance of A/V classification. In addition, deep supervision is introduced to the network to assist the low level layers to extract more semantic information. The proposed network achieves pixel-wise accuracy of 95.70% for vessel segmentation, and A/V classification accuracy of 94.50%, which is the state-of-the-art performance for both tasks on the AV-DRIVE dataset. Furthermore, we have also tested the model performance on INSPIRE-AVR dataset, which achieves a skeletal A/V classification accuracy of 91.6%.      
### 51.Visual Explanation for Identification of the Brain Bases for Dyslexia on fMRI Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.09260.pdf)
>  Brain imaging of mental health, neurodevelopmental and learning disorders has coupled with machine learning to identify patients based only on their brain activation, and ultimately identify features that generalize from smaller samples of data to larger ones. However, the success of machine learning classification algorithms on neurofunctional data has been limited to more homogeneous data sets of dozens of participants. More recently, larger brain imaging data sets have allowed for the application of deep learning techniques to classify brain states and clinical groups solely from neurofunctional features. Deep learning techniques provide helpful tools for classification in healthcare applications, including classification of structural 3D brain images. Recent approaches improved classification performance of larger functional brain imaging data sets, but they fail to provide diagnostic insights about the underlying conditions or provide an explanation from the neural features that informed the classification. We address this challenge by leveraging a number of network visualization techniques to show that, using such techniques in convolutional neural network layers responsible for learning high-level features, we are able to provide meaningful images for expert-backed insights into the condition being classified. Our results show not only accurate classification of developmental dyslexia from the brain imaging alone, but also provide automatic visualizations of the features involved that match contemporary neuroscientific knowledge, indicating that the visual explanations do help in unveiling the neurological bases of the disorder being classified.      
### 52.Wavelet Platform for Crowdsensed Modal Identification of Bridges  [ :arrow_down: ](https://arxiv.org/pdf/2007.09249.pdf)
>  This study presents a flexible approach for bridge modal identification using smartphone data collected by a large pool of passing vehicles. With each trip of a mobile sensor, the spatio-temporal response of the bridge is sampled, plus various sources of noise, e.g., vehicle dynamics, environmental effects, and road profile. This paper provides further evidence to support the hypothesis that through trip aggregation, such noise effects can be mitigated and the true bridge dynamics are exhibited. In this study, the continuous wavelet transform is applied to each trip, and the results are combined to estimate the structural modal response of the bridge. The Crowdsourced Modal Identification using Continuous Wavelets (CMICW) method is presented and validated in an experimental setting. In summary, the method successfully identifies natural frequencies and absolute mode shapes of a bridge with high accuracy. Notably, these results are the first to extract torsional mode shape information from mobile sensor data. Moreover, the influence of vehicle speed on the estimation accuracy is investigated. Finally, a hybrid simulation framework is proposed to account for the vehicle dynamics within the raw mobile sensing data. The proposed method is successful in removing vehicle dynamic effects and identifying modal properties. These results contribute to the growing body of knowledge on the practice of mobile crowdsensing for physical properties of transportation infrastructure.      
### 53.Fine Timing and Frequency Synchronization for MIMO-OFDM: An Extreme Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.09248.pdf)
>  Multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) is a key technology component in the evolution towards next-generation communication in which the accuracy of timing and frequency synchronization significantly impacts the overall system performance. In this paper, we propose a novel scheme leveraging extreme learning machine (ELM) to achieve high-precision timing and frequency synchronization. Specifically, two ELMs are incorporated into a traditional MIMO-OFDM system to estimate both the residual symbol timing offset (RSTO) and the residual carrier frequency offset (RCFO). The simulation results show that the performance of an ELM-based synchronization scheme is superior to the traditional method under both additive white Gaussian noise (AWGN) and frequency selective fading channels. Finally, the proposed method is robust in terms of choice of channel parameters (e.g., number of paths) and also in terms of "generalization ability" from a machine learning standpoint.      
### 54.Streaming ResLSTM with Causal Mean Aggregation for Device-Directed Utterance Detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.09245.pdf)
>  In this paper, we propose a streaming model to distinguish voice queries intended for a smart-home device from background speech. The proposed model consists of multiple CNN layers with residual connections, followed by a stacked LSTM architecture. The streaming capability is achieved by using unidirectional LSTM layers and a causal mean aggregation layer to form the final utterance-level prediction up to the current frame. In order to avoid redundant computation during online streaming inference, we use a caching mechanism for every convolution operation. Experimental results on a device-directed vs. non device-directed task show that the proposed model yields an equal error rate reduction of 41% compared to our previous best model on this task. Furthermore, we show that the proposed model is able to accurately predict earlier in time compared to the attention-based models.      
### 55.Initializing Successive Linear Programming Solver for ACOPF using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.09210.pdf)
>  A Successive linear programming (SLP) approach is one of the favorable approaches for solving large scale nonlinear optimization problems. Solving an alternating current optimal power flow (ACOPF) problem is no exception, particularly considering the large real-world transmission networks across the country. It is, however, essential to improve the computational performance of the SLP algorithm. One way to achieve this goal is through the efficient initialization of the algorithm with a near-optimal solution. This paper examines various machine learning (ML) algorithms available in the Scikit-Learn library to initialize an SLP-ACOPF solver, including examining linear and nonlinear ML algorithms. We evaluate the quality of each of these machine learning algorithms for predicting variables needed for a power flow solution. The solution is then used as an initialization for an SLP-ACOPF algorithm. The approach is tested on a congested and non-congested 3 bus systems. The results obtained from the best-performed ML algorithm in this work are compared with the results of a DCOPF solution for the initialization of an SLP-ACOPF solver.      
### 56.Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal Fundus Images using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.09191.pdf)
>  Fluorescein Angiography (FA) is a technique that employs the designated camera for Fundus photography incorporating excitation and barrier filters. FA also requires fluorescein dye that is injected intravenously, which might cause adverse effects ranging from nausea, vomiting to even fatal anaphylaxis. Currently, no other fast and non-invasive technique exists that can generate FA without coupling with Fundus photography. To eradicate the need for an invasive FA extraction procedure, we introduce an Attention-based Generative network that can synthesize Fluorescein Angiography from Fundus images. The proposed gan incorporates multiple attention based skip connections in generators and comprises novel residual blocks for both generators and discriminators. It utilizes reconstruction, feature-matching, and perceptual loss along with adversarial training to produces realistic Angiograms that is hard for experts to distinguish from real ones. Our experiments confirm that the proposed architecture surpasses recent state-of-the-art generative networks for fundus-to-angio translation task.      
### 57.Low Light Video Enhancement using Synthetic Data Produced with an Intermediate Domain Mapping  [ :arrow_down: ](https://arxiv.org/pdf/2007.09187.pdf)
>  Advances in low-light video RAW-to-RGB translation are opening up the possibility of fast low-light imaging on commodity devices (e.g. smartphone cameras) without the need for a tripod. However, it is challenging to collect the required paired short-long exposure frames to learn a supervised mapping. Current approaches require a specialised rig or the use of static videos with no subject or object motion, resulting in datasets that are limited in size, diversity, and motion. We address the data collection bottleneck for low-light video RAW-to-RGB by proposing a data synthesis mechanism, dubbed SIDGAN, that can generate abundant dynamic video training pairs. SIDGAN maps videos found 'in the wild' (e.g. internet videos) into a low-light (short, long exposure) domain. By generating dynamic video data synthetically, we enable a recently proposed state-of-the-art RAW-to-RGB model to attain higher image quality (improved colour, reduced artifacts) and improved temporal consistency, compared to the same model trained with only static real video data.      
### 58.A Novel Spectrally-Efficient Uplink Hybrid-Domain NOMA System  [ :arrow_down: ](https://arxiv.org/pdf/2007.09179.pdf)
>  This paper proposes a novel hybrid-domain (HD) non-orthogonal multiple access (NOMA) approach to support a larger number of uplink users than the recently proposed code-domain NOMA approach, i.e., sparse code multiple access (SCMA). HD-NOMA combines the code-domain and power-domain NOMA schemes by clustering the users in small path loss (strong) and large path loss (weak) groups. The two groups are decoded using successive interference cancellation while within the group users are decoded using the message passing algorithm. To further improve the performance of the system, a spectral-efficiency maximization problem is formulated under a user quality-of-service constraint, which dynamically assigns power and subcarrier to the users. The problem is non-convex and has sparsity constraints. The alternating optimization procedure is used to solve it iteratively. We apply successive convex approximation and reweighted $\ell_1$ minimization approaches to deal with the non-convexity and sparsity constraints, respectively. The performance of the proposed HD-NOMA is evaluated and compared with the conventional SCMA scheme through numerical simulation. The results show the potential of HD-NOMA in increasing the number of uplink users.      
### 59.An approach for auxiliary diagnosing and screening coronary disease based on machine learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.10316.pdf)
>  How to accurately classify and predict whether an individual has coronary disease and the degree of coronary stenosis without using invasive examination? This problem has not been solved satisfactorily. To this end, the three kinds of machine learning (ML) algorithms, i.e., Boost Tree (BT), Decision Tree (DT), Logistic Regression (LR), are employed in this paper. First, 11 features including basic information of an individual, symptoms and results of routine physical examination are selected, and one label is specified, indicating whether an individual suffers from coronary disease or different severity of coronary artery stenosis. On the basis of it, a sample set is constructed. Second, each of these three ML algorithms learns from the sample set to obtain the corresponding optimal predictive results, respectively. The experimental results show that: BT predicts whether an individual has coronary disease with an accuracy of 94%, and this algorithm predicts the degree of an individuals coronary artery stenosis with an accuracy of 90%.      
### 60.CoVoST 2: A Massively Multilingual Speech-to-Text Translation Corpus  [ :arrow_down: ](https://arxiv.org/pdf/2007.10310.pdf)
>  Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines.      
### 61.wav2shape: Hearing the Shape of a Drum Machine  [ :arrow_down: ](https://arxiv.org/pdf/2007.10299.pdf)
>  Disentangling and recovering physical attributes, such as shape and material, from a few waveform examples is a challenging inverse problem in audio signal processing, with numerous applications in musical acoustics as well as structural engineering. We propose to address this problem via a combination of time--frequency analysis and supervised machine learning. We start by synthesizing a dataset of sounds using the functional transformation method. Then, we represent each percussive sound in terms of its time-invariant scattering transform coefficients and formulate the parametric estimation of the resonator as multidimensional regression with a deep convolutional neural network. We interpolate scattering coefficients over the surface of the drum as a surrogate for potentially missing data, and study the response of the neural network to interpolated samples. Lastly, we resynthesize drum sounds from scattering coefficients, therefore paving the way towards a deep generative model of drum sounds whose latent variables are physically interpretable.      
### 62.DNN Speaker Tracking with Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2007.10248.pdf)
>  In multi-speaker applications is common to have pre-computed models from enrolled speakers. Using these models to identify the instances in which these speakers intervene in a recording is the task of speaker tracking. In this paper, we propose a novel embedding-based speaker tracking method. Specifically, our design is based on a convolutional neural network that mimics a typical speaker verification PLDA (probabilistic linear discriminant analysis) classifier and finds the regions uttered by the target speakers in an online fashion. The system was studied from two different perspectives: diarization and tracking; results on both show a significant improvement over the PLDA baseline under the same experimental conditions. Two standard public datasets, CALLHOME and DIHARD II single channel, were modified to create two-speaker subsets with overlapping and non-overlapping regions. We evaluate the robustness of our supervised approach with models generated from different segment lengths. A relative improvement of 17% in DER for DIHARD II single channel shows promising performance. Furthermore, to make the baseline system similar to speaker tracking, non-target speakers were added to the recordings. Even in these adverse conditions, our approach is robust enough to outperform the PLDA baseline.      
### 63.Model-Informed Machine Learning for Multi-component T2 Relaxometry  [ :arrow_down: ](https://arxiv.org/pdf/2007.10225.pdf)
>  Recovering the T2 distribution from multi-echo T2 magnetic resonance (MR) signals is challenging but has high potential as it provides biomarkers characterizing the tissue micro-structure, such as the myelin water fraction (MWF). In this work, we propose to combine machine learning and aspects of parametric (fitting from the MRI signal using biophysical models) and non-parametric (model-free fitting of the T2 distribution from the signal) approaches to T2 relaxometry in brain tissue by using a multi-layer perceptron (MLP) for the distribution reconstruction. For training our network, we construct an extensive synthetic dataset derived from biophysical models in order to constrain the outputs with \textit{a priori} knowledge of \textit{in vivo} distributions. The proposed approach, called Model-Informed Machine Learning (MIML), takes as input the MR signal and directly outputs the associated T2 distribution. We evaluate MIML in comparison to non-parametric and parametric approaches on synthetic data, an ex vivo scan, and high-resolution scans of healthy subjects and a subject with Multiple Sclerosis. In synthetic data, MIML provides more accurate and noise-robust distributions. In real data, MWF maps derived from MIML exhibit the greatest conformity to anatomical scans, have the highest correlation to a histological map of myelin volume, and the best unambiguous lesion visualization and localization, with superior contrast between lesions and normal appearing tissue. In whole-brain analysis, MIML is 22 to 4980 times faster than non-parametric and parametric methods, respectively.      
### 64.Sample, Quantize and Encode: Timely Estimation Over Noisy Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.10200.pdf)
>  The effects of quantization and coding on the estimation quality of Gauss-Markov processes are considered, with a special attention to the Ornstein-Uhlenbeck process. Samples are acquired from the process, quantized, and then encoded for transmission using either infinite incremental redundancy (IIR) or fixed redundancy (FR) coding schemes. A fixed processing time is consumed at the receiver for decoding and sending feedback to the transmitter. Decoded messages are used to construct a minimum mean square error (MMSE) estimate of the process as a function of time. This is shown to be an increasing functional of the age-of-information (AoI), defined as the time elapsed since the sampling time pertaining to the latest successfully decoded message. Such (age-penalty) functional depends on the quantization bits, codewords lengths and receiver processing time. The goal, for each coding scheme, is to optimize sampling times such that the long-term average MMSE is minimized. This is then characterized in the setting of general increasing age-penalty functionals, not necessarily corresponding to MMSE, which may be of independent interest in other contexts. <br>The solution is first shown to be a threshold policy for IIR, and a just-in-time policy for FR. Enhanced transmissions schemes are then developed in order to exploit the processing times to make new data available at the receiver sooner. For both IIR and FR, it is shown that there exists an optimal number of quantization bits that balances AoI and quantization errors. It is also shown that for longer receiver processing times, the relatively simpler FR scheme outperforms IIR.      
### 65.Look and Listen: A Multi-modality Late Fusion Approach to Scene Classification for Autonomous Machines  [ :arrow_down: ](https://arxiv.org/pdf/2007.10175.pdf)
>  The novelty of this study consists in a multi-modality approach to scene classification, where image and audio complement each other in a process of deep late fusion. The approach is demonstrated on a difficult classification problem, consisting of two synchronised and balanced datasets of 16,000 data objects, encompassing 4.4 hours of video of 8 environments with varying degrees of similarity. We first extract video frames and accompanying audio at one second intervals. The image and the audio datasets are first classified independently, using a fine-tuned VGG16 and an evolutionary optimised deep neural network, with accuracies of 89.27% and 93.72%, respectively. This is followed by late fusion of the two neural networks to enable a higher order function, leading to accuracy of 96.81% in this multi-modality classifier with synchronised video frames and audio clips. The tertiary neural network implemented for late fusion outperforms classical state-of-the-art classifiers by around 3% when the two primary networks are considered as feature generators. We show that situations where a single-modality may be confused by anomalous data points are now corrected through an emerging higher order integration. Prominent examples include a water feature in a city misclassified as a river by the audio classifier alone and a densely crowded street misclassified as a forest by the image classifier alone. Both are examples which are correctly classified by our multi-modality approach.      
### 66.Tracking the Untrackable  [ :arrow_down: ](https://arxiv.org/pdf/2007.10148.pdf)
>  Although short-term fully occlusion happens rare in visual object tracking, most trackers will fail under these circumstances. However, humans can still catch up the target by anticipating the trajectory of the target even the target is invisible. Recent psychology also has shown that humans build the mental image of the future. Inspired by that, we present a HAllucinating Features to Track (HAFT) model that enables to forecast the visual feature embedding of future frames. The anticipated future frames focus on the movement of the target while hallucinating the occluded part of the target. Jointly tracking on the hallucinated features and the real features improves the robustness of the tracker even when the target is highly occluded. Through extensive experimental evaluations, we achieve promising results on multiple datasets: OTB100, VOT2018, LaSOT, TrackingNet, and UAV123.      
### 67.An Accelerated-Decomposition Approach for Security-Constrained Unit Commitment with Corrective Network Reconfiguration- Part II: Results and Discussion  [ :arrow_down: ](https://arxiv.org/pdf/2007.10142.pdf)
>  This paper presents a novel approach to handle the computational complexity in security-constrained unit commitment (SCUC) with corrective network reconfiguration (CNR) to harness the flexibility in transmission networks. This is achieved with consideration of scalability through decomposing the SCUC/SCUC-CNR formulation and then fast screening non-critical sub-problems. This is compared against the extensive formulations of SCUC and SCUC-CNR to show the advantages of the proposed typical-decomposition and accelerated-decomposition approaches to SCUC and SCUC-CNR respectively. Simulation results on the IEEE 24-bus system show that the proposed methods are substantially faster without the loss in solution quality. The proposed accelerated-decomposition approaches can be implemented for large power systems as they have great performance in the scalability tests on the IEEE 73-bus system and the Polish system when compared against the respective extensive formulations and typical-decomposition approaches. Overall, a dynamic post-contingency network can substantially alleviate network congestion and lead to a lower optimal cost.      
### 68.Monte Carlo Dropout Ensembles for Robust Illumination Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2007.10114.pdf)
>  Computational color constancy is a preprocessing step used in many camera systems. The main aim is to discount the effect of the illumination on the colors in the scene and restore the original colors of the objects. Recently, several deep learning-based approaches have been proposed to solve this problem and they often led to state-of-the-art performance in terms of average errors. However, for extreme samples, these methods fail and lead to high errors. In this paper, we address this limitation by proposing to aggregate different deep learning methods according to their output uncertainty. We estimate the relative uncertainty of each approach using Monte Carlo dropout and the final illumination estimate is obtained as the sum of the different model estimates weighted by the log-inverse of their corresponding uncertainties. The proposed framework leads to state-of-the-art performance on INTEL-TAU dataset.      
### 69.Modeling Stochastic Microscopic Traffic Behaviors: a Physics Regularized Gaussian Process Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.10109.pdf)
>  Modeling stochastic traffic behaviors at the microscopic level, such as car-following and lane-changing, is a crucial task to understand the interactions between individual vehicles in traffic streams. Leveraging a recently developed theory named physics regularized Gaussian process (PRGP), this study presents a stochastic microscopic traffic model that can capture the randomness and measure errors in the real world. Physical knowledge from classical car-following models is converted as physics regularizers, in the form of shadow Gaussian process (GP), of a multivariate PRGP for improving the modeling accuracy. More specifically, a Bayesian inference algorithm is developed to estimate the mean and kernel of GPs, and an enhanced latent force model is formulated to encode physical knowledge into stochastic processes. Also, based on the posterior regularization inference framework, an efficient stochastic optimization algorithm is developed to maximize the evidence lower-bound of the system likelihood. To evaluate the performance of the proposed models, this study conducts empirical studies on real-world vehicle trajectories from the NGSIM dataset. Since one unique feature of the proposed framework is the capability of capturing both car-following and lane-changing behaviors with one single model, numerical tests are carried out with two separated datasets, one contains lane-changing maneuvers and the other doesn't. The results show the proposed method outperforms the previous influential methods in estimation precision.      
### 70.Learning Adaptive Sampling and Reconstruction for Volume Visualization  [ :arrow_down: ](https://arxiv.org/pdf/2007.10093.pdf)
>  A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the relevant information is encoded. In this work, we make a first step towards answering the question of whether an artificial neural network can predict where to sample the data with higher or lower density, by learning of correspondences between the data, the sampling patterns and the generated images. We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples. For the first time, to the best of our knowledge, we demonstrate that the selection of structures that are relevant for the final visual representation can be jointly learned together with the reconstruction of this representation from these structures. Therefore, we introduce differentiable sampling and reconstruction stages, which can leverage back-propagation based on supervised losses solely on the final image. We shed light on the adaptive sampling patterns generated by the network pipeline and analyze its use for volume visualization including isosurface and direct volume rendering.      
### 71.Resource Allocation in Virtualized CoMP-NOMA HetNets: Multi-Connectivity for Joint Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2007.10013.pdf)
>  In this work, we design a generalized joint transmission coordinated multi-point (JT-CoMP)-non-orthogonal multiple access (NOMA) model for a virtualized multi-infrastructure network. In this model, all users can benefit from multiple joint transmissions of CoMP thanks to the multi-connectivity opportunity provided by wireless network virtualization (WNV). We propose an unlimited NOMA clustering (UNC) scheme, where the order of NOMA clusters is the maximum possible value (called global NOMA cluster). We show that UNC provides the maximum overall spectral efficiency in CoMP-NOMA with maximum successful interference cancellation (SIC) complexity at users. To strike a balance between spectral efficiency and SIC complexity, we propose a limited NOMA clustering (LNC), where the SIC is performed to a subset of the global NOMA cluster sets. We formulate the problem of joint power allocation and user association in UNC and LNC such that CoMP scheduling and NOMA clustering are determined by the user association policy. Then, one globally and one locally optimal solutions are proposed for each problem based on mixed-integer monotonic optimization and sequential programming, respectively. Numerical assessments reveal that WNV and LNC improves users sum-rate and reduces users SIC complexity up to $65\%$ and $45\%$ compared to non-virtualized CoMP-NOMA and UNC, respectively.      
### 72.Power Minimization for Multi-cell Uplink NOMA with Imperfect SIC  [ :arrow_down: ](https://arxiv.org/pdf/2007.10001.pdf)
>  In this paper, we investigate a multi-cell uplink non-orthogonal multiple access (NOMA) system with imperfect successive interference cancellation (SIC). The objective of the formulated optimization problem is to minimize the total power consumption under users' quality-of-service constraints. The considered problem is first transformed into a linear programming problem, upon which centralized and distributed optimal solutions are proposed. Numerical results are presented to verify the performance of the proposed solutions and evaluate the impact of imperfect SIC on the system performance.      
### 73.Mathematical and computational approaches for stochastic control of river environment and ecology: from fisheries viewpoint  [ :arrow_down: ](https://arxiv.org/pdf/2007.09978.pdf)
>  We present a modern stochastic control framework for dynamic optimization of river environment and ecology. We focus on a fisheries problem in Japan, and show several examples of simplified optimal control problems of stochastic differential equations modeling fishery resource dynamics, reservoir water balance dynamics, benthic algae dynamics, and sediment storage dynamics. These problems concern different phenomena with each other, but they all reduce to solving degenerate parabolic or elliptic equations. Optimal controls and value functions of these problems are computed using finite difference schemes. Finally, we present a higher-dimensional problem of controlling a dam-reservoir system using a semi-Lagrangian discretization on sparse grids. Our contribution shows the state-of-art of modeling, analysis, and computation of stochastic control in environmental engineering and science, and related research areas.      
### 74.Interpretable Control by Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.09964.pdf)
>  In this paper, three recently introduced reinforcement learning (RL) methods are used to generate human-interpretable policies for the cart-pole balancing benchmark. The novel RL methods learn human-interpretable policies in the form of compact fuzzy controllers and simple algebraic equations. The representations as well as the achieved control performances are compared with two classical controller design methods and three non-interpretable RL methods. All eight methods utilize the same previously generated data batch and produce their controller offline - without interaction with the real benchmark dynamics. The experiments show that the novel RL methods are able to automatically generate well-performing policies which are at the same time human-interpretable. Furthermore, one of the methods is applied to automatically learn an equation-based policy for a hardware cart-pole demonstrator by using only human-player-generated batch data. The solution generated in the first attempt already represents a successful balancing policy, which demonstrates the methods applicability to real-world problems.      
### 75.Acting Selfish for the Good of All: Contextual Bandits for Resource-Efficient Transmission of Vehicular Sensor Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.09921.pdf)
>  as a novel client-based method for resource-efficient opportunistic transmission of delay-tolerant vehicular sensor data. BS-CB applies a hybrid approach which brings together all major machine learning disciplines - supervised, unsupervised, and reinforcement learning - in order to autonomously schedule vehicular sensor data transmissions with respect to the expected resource efficiency. Within a comprehensive real world performance evaluation in the public cellular networks of three Mobile Network Operators (MNOs), it is found that 1) The average uplink data rate is improved by 125%-195% 2) The apparently selfish goal of data rate optimization reduces the amount of occupied cell resources by 84%-89% 3) The average transmission-related power consumption can be reduced by 53%-75% 4) The price to pay is an additional buffering delay due to the opportunistic medium access strategy.      
### 76.Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2007.09902.pdf)
>  Stereophonic audio is an indispensable ingredient to enhance human auditory experience. Recent research has explored the usage of visual information as guidance to generate binaural or ambisonic audio from mono ones with stereo supervision. However, this fully supervised paradigm suffers from an inherent drawback: the recording of stereophonic audio usually requires delicate devices that are expensive for wide accessibility. To overcome this challenge, we propose to leverage the vastly available mono data to facilitate the generation of stereophonic audio. Our key observation is that the task of visually indicated audio separation also maps independent audios to their corresponding visual positions, which shares a similar objective with stereophonic audio generation. We integrate both stereo generation and source separation into a unified framework, Sep-Stereo, by considering source separation as a particular type of audio spatialization. Specifically, a novel associative pyramid network architecture is carefully designed for audio-visual feature fusion. Extensive experiments demonstrate that our framework can improve the stereophonic audio generation results while performing accurate sound separation with a shared backbone.      
### 77.A novel deep learning-based method for monochromatic image synthesis from spectral CT using photon-counting detectors  [ :arrow_down: ](https://arxiv.org/pdf/2007.09870.pdf)
>  With the growing technology of photon-counting detectors (PCD), spectral CT is a widely concerned topic which has the potential of material differentiation. However, due to some non-ideal factors such as cross talk and pulse pile-up of the detectors, direct reconstruction from detected spectrum without any corrections will get a wrong result. Conventional methods try to model these factors using calibration and make corrections accordingly, but depend on the preciseness of the model. To solve this problem, in this paper, we proposed a novel deep learning-based monochromatic image synthesis method working in sinogram domain. Different from previous deep learning-based methods aimed at this problem, we designed a novel network architecture according to the physical model of cross talk, and it can solve this problem better in an ingenious way. Our method was tested on a cone-beam CT (CBCT) system equipped with a PCD. After using FDK algorithm on the corrected projection, we got quite more accurate results with less noise, which showed the feasibility of monochromatic image synthesis by our method.      
### 78.A 3D Tractable Model for UAV-Enabled Cellular Networks With Multiple Antennas  [ :arrow_down: ](https://arxiv.org/pdf/2007.09866.pdf)
>  This paper aims to propose a three-dimensional (3D) point process model that can be employed to generally deploy unmanned aerial vehicles (UAVs) in a large-scale cellular network and tractably analyze the fundamental network-wide performances of the network. The proposed 3D point process is devised based on a 2D homogeneous marked Poisson point process (PPP) in which each point and its random mark uniquely correspond to the projection and the altitude of each point in the 3D point process, respectively. We study some of the important statistical properties of the proposed 3D point process and shed light on some crucial insights into these properties that facilitate the analyses of a UAV-enabled cellular network wherein all the UAVs equipped with multiple antennas are deployed by the proposed 3D point process to serve as aerial base stations. The salient features of the proposed 3D point process lie in its suitability in practical 3D channel modeling and tractability in analysis. The downlink coverage performances of the UAV-enabled cellular network are analyzed and found in neat expressions and their closed-form results for some special cases are also derived. Most importantly, their fundamental limits achieved by cell-free massive antenna array are characterized when coordinating all the UAVs to jointly perform non-coherent downlink transmission. Finally, numerical results are provided to validate some of the key findings in this paper.      
### 79.Achieving Real-Time Execution of 3D Convolutional Neural Networks on Mobile Devices  [ :arrow_down: ](https://arxiv.org/pdf/2007.09835.pdf)
>  Mobile devices are becoming an important carrier for deep learning tasks, as they are being equipped with powerful, high-end mobile CPUs and GPUs. However, it is still a challenging task to execute 3D Convolutional Neural Networks (CNNs) targeting for real-time performance, besides high inference accuracy. The reason is more complex model structure and higher model dimensionality overwhelm the available computation/storage resources on mobile devices. A natural way may be turning to deep learning weight pruning techniques. However, the direct generalization of existing 2D CNN weight pruning methods to 3D CNNs is not ideal for fully exploiting mobile parallelism while achieving high inference accuracy. <br>This paper proposes RT3D, a model compression and mobile acceleration framework for 3D CNNs, seamlessly integrating neural network weight pruning and compiler code generation techniques. We propose and investigate two structured sparsity schemes i.e., the vanilla structured sparsity and kernel group structured (KGS) sparsity that are mobile acceleration friendly. The vanilla sparsity removes whole kernel groups, while KGS sparsity is a more fine-grained structured sparsity that enjoys higher flexibility while exploiting full on-device parallelism. We propose a reweighted regularization pruning algorithm to achieve the proposed sparsity schemes. The inference time speedup due to sparsity is approaching the pruning rate of the whole model FLOPs (floating point operations). RT3D demonstrates up to 29.1$\times$ speedup in end-to-end inference time comparing with current mobile frameworks supporting 3D CNNs, with moderate 1%-1.5% accuracy loss. The end-to-end inference time for 16 video frames could be within 150 ms, when executing representative C3D and R(2+1)D models on a cellphone. For the first time, real-time execution of 3D CNNs is achieved on off-the-shelf mobiles.      
### 80.Coverage Analysis of a Thinned LiFi Optical Attocell Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.09724.pdf)
>  This work analyzes coverage in the downlink of a thinned LiFi attocell network of deterministically arranged LEDs. The network is thinned by a Bernoulli probability p over all the LEDs to decide whether each one of them acts as a LiFi source or not. Then we use the series approximation approach used in [1] to obtain closed form expressions for the probability of coverage in such thinned LiFi attocell networks and validate them using numerical simulations.      
### 81.Analyzing Optical TDMA to Mitigate Interference in Downlink LiFi Optical Attocell Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.09718.pdf)
>  Co-channel interference in the downlink of LiFi attocell networks significantly decreases the network performance in terms of rate. Analysis of multiple access schemes is essential to mitigate interference and improve rate. The light emitting diodes (LEDs) being centrally monitored, the time division multiple access (TDMA) scheme over the LEDs will be suitable to analyze. This work considers the interference characterization in [1] over M-PAM modulated signals to derive an exact expression for the goodput G of the time scheduled attocell network, which is arranged as a deterministic square lattice in two dimensions. Given this TDMA over the LEDs, numerical simulations show that the LEDs can be optimally time scheduled to maximize the goodput, which implies that the TDMA mitigates interference in an attocell network compared to the case when the LEDs are unscheduled.      
### 82.Phase-Noise Compensation for OFDM Systems Exploiting Coherence Bandwidth: Modeling, Algorithms, and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.09628.pdf)
>  Phase-noise (PN) estimation and compensation are crucial in millimeter-wave (mmWave) communication systems to achieve high reliability. The PN estimation, however, suffers from high computational complexity due to its fundamental characteristics, such as spectral spreading and fast-varying fluctuations. In this paper, we propose a new framework for low-complexity PN compensation in orthogonal frequency-division multiplexing systems. The proposed framework also includes a pilot allocation strategy to minimize its overhead. The key ideas are to exploit the coherence bandwidth of mmWave systems and to approximate the actual PN spectrum with its dominant components, resulting in a non-iterative solution by using linear minimum mean squared-error estimation. The proposed method obtains a reduction of more than 2.5 times in total complexity, as compared to the existing methods. Furthermore, we derive closed-form expressions for normalized mean squared-errors (NMSEs) as a function of critical system parameters, which help in understanding the NMSE behavior in low and high signal-to-noise ratio regimes.      
### 83.Massive Access in Cell-Free Massive MIMO Based Internet of Things: Cloud Computing and Edge Computing Paradigms  [ :arrow_down: ](https://arxiv.org/pdf/2007.09617.pdf)
>  This paper studies massive access in cell-free massive multi-input multi-output (MIMO) based Internet of Things and solves the challenging active user detection (AUD) and channel estimation (CE) problems. For the uplink transmission, we propose an advanced frame structure design to reduce the access latency. Moreover, by considering the cooperation of all access points (APs), we investigate two processing paradigms at the receiver for massive access: cloud computing and edge computing. For cloud computing, all APs are connected to a centralized processing unit (CPU), and the signals received at all APs are centrally processed at the CPU. While for edge computing, the central processing is offloaded to part of APs equipped with distributed processing units, so that the AUD and CE can be performed in a distributed processing strategy. Furthermore, by leveraging the structured sparsity of the channel matrix, we develop a structured sparsity-based generalized approximated message passing (SS-GAMP) algorithm for reliable joint AUD and CE, where the quantization accuracy of the processed signals is taken into account. Based on the SS-GAMP algorithm, a successive interference cancellation-based AUD and CE scheme is further developed under two paradigms for reduced access latency. Simulation results validate the superiority of the proposed approach over the state-of-the-art baseline schemes. Besides, the results reveal that the edge computing can achieve the similar massive access performance as the cloud computing, and the edge computing is capable of alleviating the burden on CPU, having a faster access response, and supporting more flexible AP cooperation.      
### 84.Quantitative phase-contrast imaging: a bridge between qualitative phase-contrast and phase retrieval algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2007.09602.pdf)
>  In the last five decades, iterative phase retrieval methods draw large amount of interest across the research community as a non-interferometric approach to recover quantitative phase distributions from one (or more) intensity measurement. However, in cases where a unique solution does exist, these methods often require oversampling and high computational resources, which limits the use of this approach in important applications. On the other hand, phase contrast methods are based on a single camera exposure but provides only a qualitative description of the phase, thus are not useful for applications in which the quantitative phase description is needed. In this study we adopt a combined approach of the two above-mentioned methods to overcome their respective drawbacks. We show that a modified phase retrieval algorithm easily converges to the correct solution by initializing the algorithm with a phase-induced intensity measurement, namely with a phase contrast image of the examined object. Accurate quantitative phase measurements for both binary and continuously varying phase objects are demonstrated to support the suggested system as a single-shot quantitative phase contrast microscope.      
### 85.Learning to Play Cup-and-Ball with Noisy Camera Observations  [ :arrow_down: ](https://arxiv.org/pdf/2007.09562.pdf)
>  Playing the cup-and-ball game is an intriguing task for robotics research since it abstracts important problem characteristics including system nonlinearity, contact forces and precise positioning as terminal goal. In this paper, we present a learning model based control strategy for the cup-and-ball game, where a Universal Robots UR5e manipulator arm learns to catch a ball in one of the cups on a Kendama. Our control problem is divided into two sub-tasks, namely $(i)$ swinging the ball up in a constrained motion, and $(ii)$ catching the free-falling ball. The swing-up trajectory is computed offline, and applied in open-loop to the arm. Subsequently, a convex optimization problem is solved online during the ball's free-fall to control the manipulator and catch the ball. The controller utilizes noisy position feedback of the ball from an Intel RealSense D435 depth camera. We propose a novel iterative framework, where data is used to learn the support of the camera noise distribution iteratively in order to update the control policy. The probability of a catch with a fixed policy is computed empirically with a user specified number of roll-outs. Our design guarantees that probability of the catch increases in the limit, as the learned support nears the true support of the camera noise distribution. High-fidelity Mujoco simulations and preliminary experimental results support our theoretical analysis.      
### 86.RISMA: Reconfigurable Intelligent Surfaces Enabling Beamforming for IoT Massive Access  [ :arrow_down: ](https://arxiv.org/pdf/2007.09386.pdf)
>  Massive access for Internet-of-Things (IoT) in beyond 5G networks represents a daunting challenge for conventional bandwidth-limited technologies. Millimeter-wave technologies (mmWave)---which provide large chunks of bandwidth at the cost of more complex wireless processors in harsher radio environments---is a promising alternative to accommodate massive IoT but its cost and power requirements are an obstacle for wide adoption in practice. In this context, meta-materials arise as a key innovation enabler to address this challenge by Re-configurable Intelligent Surfaces (RISs). In this paper we take on the challenge and study a beyond 5G scenario consisting of a multi-antenna base station (BS) serving a large set of single-antenna user equipments (UEs) with the aid of RISs to cope with non-line-of-sight paths. Specifically, we build a mathematical framework to jointly optimize the precoding strategy of the BS and the RIS parameters in order to minimize the system sum mean squared error (SMSE). This novel approach reveals convenient properties used to design two algorithms, RISMA and Lo-RISMA, which are able to either find simple and efficient solutions to our problem (the former) or accommodate practical constraints with low-resolution RISs (the latter). Numerical results show that our algorithms outperform conventional benchmarks that do not employ RIS (even with low-resolution meta-surfaces) with gains that span from 20% to 120% in sum rate performance.      
### 87.Learning based Predictive Error Estimation and Compensator Design for Autonomous Vehicle Path Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2007.09372.pdf)
>  Model predictive control (MPC) is widely used for path tracking of autonomous vehicles due to its ability to handle various types of constraints. However, a considerable predictive error exists because of the error of mathematics model or the model linearization. In this paper, we propose a framework combining the MPC with a learning-based error estimator and a feedforward compensator to improve the path tracking accuracy. An extreme learning machine is implemented to estimate the model based predictive error from vehicle state feedback information. Offline training data is collected from a vehicle controlled by a model-defective regular MPC for path tracking in several working conditions, respectively. The data include vehicle state and the spatial error between the current actual position and the corresponding predictive position. According to the estimated predictive error, we then design a PID-based feedforward compensator. Simulation results via Carsim show the estimation accuracy of the predictive error and the effectiveness of the proposed framework for path tracking of an autonomous vehicle.      
### 88.Superluminal Motion-Assisted 4-Dimensional Light-in-Flight Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.09308.pdf)
>  Advances in high speed imaging techniques have opened new possibilities for capturing ultrafast phenomena such as light propagation in air or through media. Capturing light-in-flight in 3-dimensional xyt-space has been reported based on various types of imaging systems, whereas reconstruction of light-in-flight information in the fourth dimension z has been a challenge. We demonstrate the first 4-dimensional light-in-flight imaging based on the observation of a superluminal motion captured by a new time-gated megapixel single-photon avalanche diode camera. A high resolution light-in-flight video is generated with no laser scanning, camera translation, interpolation, nor dark noise subtraction. A machine learning technique is applied to analyze the measured spatio-temporal data set. A theoretical formula is introduced to perform least-square regression, and extra-dimensional information is recovered without prior knowledge. The algorithm relies on the mathematical formulation equivalent to the superluminal motion in astrophysics, which is scaled by a factor of a quadrillionth. The reconstructed light-in-flight trajectory shows a good agreement with the actual geometry of the light path. Our approach could potentially provide novel functionalities to high speed imaging applications such as non-line-of-sight imaging and time-resolved optical tomography.      
### 89.Virtual Telescope for X-Ray Observations  [ :arrow_down: ](https://arxiv.org/pdf/2007.09289.pdf)
>  Selected by NASA for an Astrophysics Science SmallSat study, The Virtual Telescope for X-Ray Observations (VTXO) is a small satellite mission being developed by NASAs Goddard Space Flight Center (GSFC) and New Mexico State University (NMSU). VTXO will perform X-ray observations with an angular resolution around 50 milliarcseconds, an order of magnitude better than is achievable by current state of the art X-ray telescopes. VTXOs fine angular resolution enables measuring the environments closer to the central engines in compact X-ray sources. This resolution will be achieved by the use of Phased Fresnel Lenses (PFLs) optics which provide near diffraction-limited imaging in the X-ray band. However, PFLs require long focal lengths in order to realize their imaging performance, for VTXO this dictates that the telescopes optics and the camera will have a separation of 1 km. As it is not realistic to build a structure this large in space, the solution being adapted for VTXO is to place the camera, and the optics on two separate spacecraft and fly them in formation with the necessary spacing. This requires centimeter level control, and sub-millimeter level knowledge of the two spacecrafts relative transverse position. This paper will present VTXOs current baseline, with particular emphasis on the missions flight dynamics design.      
### 90.Efficient Iterative Solutions to Complex-Valued Nonlinear Least-Squares Problems with Mixed Linear and Antilinear Operators  [ :arrow_down: ](https://arxiv.org/pdf/2007.09281.pdf)
>  We consider a setting in which it is desired to find an optimal complex vector $\mathbf{x}\in\mathbb{C}^N$ that satisfies $\mathcal{A}(\mathbf{x}) \approx \mathbf{b}$ in a least-squares sense, where $\mathbf{b} \in \mathbb{C}^M$ is a data vector (possibly noise-corrupted), and $\mathcal{A}(\cdot): \mathbb{C}^N \rightarrow \mathbb{C}^M$ is a measurement operator. If $\mathcal{A}(\cdot)$ were linear, this reduces to the classical linear least-squares problem, which has a well-known analytic solution as well as powerful iterative solution algorithms. However, instead of linear least-squares, this work considers the more complicated scenario where $\mathcal{A}(\cdot)$ is nonlinear, but can be represented as the summation and/or composition of some operators that are linear and some operators that are antilinear. Some common nonlinear operations that have this structure include complex conjugation or taking the real-part or imaginary-part of a complex vector. Previous literature has shown that this kind of mixed linear/antilinear least-squares problem can be mapped into a linear least-squares problem by considering $\mathbf{x}$ as a vector in $\mathbb{R}^{2N}$ instead of $\mathbb{C}^N$. While this approach is valid, the replacement of the original complex-valued optimization problem with a real-valued optimization problem can be complicated to implement, and can also be associated with increased computational complexity. In this work, we describe theory and computational methods that enable mixed linear/antilinear least-squares problems to be solved iteratively using standard linear least-squares tools, while retaining all of the complex-valued structure of the original inverse problem. An illustration is provided to demonstrate that this approach can simplify the implementation and reduce the computational complexity of iterative solution algorithms.      
### 91.Optimal allocation of excitation and measurement for identification of dynamic networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.09263.pdf)
>  In this paper, the problem of choosing the best allocation of excitations and measurements for the identification of a dynamic network is formally stated and analyzed. The best choice will be one that achieves the most accurate identification with the least costly experiment. Accuracy is assessed by the trace of the asymptotic covariance matrix of the parameters estimates, whereas the cost criterion is the number of excitations and measurements. Analytical and numerical results are presented for two classes of dynamic networks in state space form: branches and cycles. From these results, a number of guidelines for the choice emerge, which are based either on the topology of the network or on the relative magnitude of the modules being identified. An example is given to illustrate that these guidelines can to some extent be applied to networks of more generic topology.      
### 92.Wavelet Channel Attention Module with a Fusion Network for Single Image Deraining  [ :arrow_down: ](https://arxiv.org/pdf/2007.09163.pdf)
>  Single image deraining is a crucial problem because rain severely degenerates the visibility of images and affects the performance of computer vision tasks like outdoor surveillance systems and intelligent vehicles. In this paper, we propose the new convolutional neural network (CNN) called the wavelet channel attention module with a fusion network. Wavelet transform and the inverse wavelet transform are substituted for down-sampling and up-sampling so feature maps from the wavelet transform and convolutions contain different frequencies and scales. Furthermore, feature maps are integrated by channel attention. Our proposed network learns confidence maps of four sub-band images derived from the wavelet transform of the original images. Finally, the clear image can be well restored via the wavelet reconstruction and fusion of the low-frequency part and high-frequency parts. Several experimental results on synthetic and real images present that the proposed algorithm outperforms state-of-the-art methods.      
