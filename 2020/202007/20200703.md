# ArXiv eess --Fri, 3 Jul 2020
### 1.An Implementation of Partial Transmit Sequences to Design Energy Efficient Underwater Acoustic OFDM Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2007.01273.pdf)
>  In this article we research about underwater acoustics transceivers. As Underwater acoustic transceivers consume more power than Radio frequency transceivers. The techniques which are being utilized in radio frequency cannot be implemented directly in underwater acoustic system it needs to be re investigated to design new methods. To achieve reliable acoustic data transmission new techniques should be achieved or the traditional Orthogonal frequency divisional multiplexing techniques should be revised. The power consumption also relies upon underwater acoustic signal propagation and transmission distances. Several underwater acoustic applications require long-term monitoring of the sea. For the battery powered modems, it becomes very serious problem. By designing an Energy efficient OFDM Communication system we can solve this problem. We study about peak to average power ratio in an Orthogonal frequency divisional multiplexing system by reducing the major draw-back of OFDM system. The PAPR reduction utilized in this paper is Partial Transmit Sequences for underwater acoustic OFDM communication system which has lesser complexity. The results have provided better performance in underwater acoustic OFDM communication system.      
### 2.Image Processing and Quality Control for Abdominal Magnetic Resonance Imaging in the UK Biobank  [ :arrow_down: ](https://arxiv.org/pdf/2007.01251.pdf)
>  An end-to-end image analysis pipeline is presented for the abdominal MRI protocol used in the UK Biobank on the first 38,971 participants. Emphasis is on the processing steps necessary to ensure a high-level of data quality and consistency is produced in order to prepare the datasets for downstream quantitative analysis, such as segmentation and parameter estimation. Quality control procedures have been incorporated to detect and, where possible, correct issues in the raw data. Detection of fat-water swaps in the Dixon series is performed by a deep learning model and corrected automatically. Bone joints are predicted using a hybrid atlas-based registration and deep learning model for the shoulders, hips and knees. Simultaneous estimation of proton density fat fraction and transverse relaxivity (R2*) is performed using both the magnitude and phase information for the single-slice multiecho series. Approximately 98.1% of the two-point Dixon acquisitions were successfully processed and passed quality control, with 99.98% of the high-resolution T1-weighted 3D volumes succeeding. Approximately 99.98% of the single-slice multiecho acquisitions covering the liver were successfully processed and passed quality control, with 97.6% of the single-slice multiecho acquisitions covering the pancreas succeeding. At least one fat-water swap was detected in 1.8% of participants. With respect to the bone joints, approximately 3.3% of participants were missing at least one knee joint and 0.8% were missing at least one shoulder joint. For the participants who received both single-slice multiecho acquisition protocols for the liver a systematic difference between the two protocols was identified and modeled using multiple linear regression. The findings presented here will be invaluable for scientists who seek to use image-derived phenotypes from the abdominal MRI protocol.      
### 3.Balancing Rates and Variance via Adaptive Batch-Size for Stochastic Optimization Problems  [ :arrow_down: ](https://arxiv.org/pdf/2007.01219.pdf)
>  Stochastic gradient descent is a canonical tool for addressing stochastic optimization problems, and forms the bedrock of modern machine learning and statistics. In this work, we seek to balance the fact that attenuating step-size is required for exact asymptotic convergence with the fact that constant step-size learns faster in finite time up to an error. To do so, rather than fixing the mini-batch and the step-size at the outset, we propose a strategy to allow parameters to evolve adaptively. Specifically, the batch-size is set to be a piecewise-constant increasing sequence where the increase occurs when a suitable error criterion is satisfied. Moreover, the step-size is selected as that which yields the fastest convergence. The overall algorithm, two scale adaptive (TSA) scheme, is developed for both convex and non-convex stochastic optimization problems. It inherits the exact asymptotic convergence of stochastic gradient method. More importantly, the optimal error decreasing rate is achieved theoretically, as well as an overall reduction in computational cost. Experimentally, we observe that TSA attains a favorable tradeoff relative to standard SGD that fixes the mini-batch and the step-size, or simply allowing one to increase or decrease respectively.      
### 4.Globally Optimal Surface Segmentation using Deep Learning with Learnable Smoothness Priors  [ :arrow_down: ](https://arxiv.org/pdf/2007.01217.pdf)
>  Automated surface segmentation is important and challenging in many medical image analysis applications. Recent deep learning based methods have been developed for various object segmentation tasks. Most of them are a classification based approach, e.g. U-net, which predicts the probability of being target object or background for each voxel. One problem of those methods is lacking of topology guarantee for segmented objects, and usually post processing is needed to infer the boundary surface of the object. In this paper, a novel model based on convolutional neural network (CNN) followed by a learnable surface smoothing block is proposed to tackle the surface segmentation problem with end-to-end training. To the best of our knowledge, this is the first study to learn smoothness priors end-to-end with CNN for direct surface segmentation with global optimality. Experiments carried out on Spectral Domain Optical Coherence Tomography (SD-OCT) retinal layer segmentation and Intravascular Ultrasound (IVUS) vessel wall segmentation demonstrated very promising results.      
### 5.Acoustic Source Localization with the Angular Spectrum Approach in Continuously Stratified Media  [ :arrow_down: ](https://arxiv.org/pdf/2007.01133.pdf)
>  The angular spectrum approach (ASA)---a fast, frequency domain method for calculation of the acoustic field---enables passive source localization and modeling forward propagation in homogeneous media with high computational efficiency. Here we show that, if the medium is continuously stratified, a first-order analytical solution may be obtained for the field at arbitrary depth. Our simulations show that the stratified ASA solution enables accurate source localization as compared to the uncorrected ASA (error from 1.2$\pm$0.3 to 0.49$\pm$0.3 wavelengths) at scalings relevant to biomedical ($kL \sim$ 500, where $L$ is the length of the measurement aperture), underwater ($kL \sim$ 800), and atmospheric ($kL \sim$ 10) acoustic applications. Overall the total computation was on the order milliseconds on standard hardware (225$\pm$84 ms, compared with $78\pm63$ ms for the homogeneous ASA formulation over all cases). Collectively, the results suggest the proposed ASA phase correction enables efficient and accurate method for source localization in continuously stratified environments.      
### 6.Multi-Objective Energy Efficient Resource Allocation and User Association for In-band Full Duplex Small-Cells  [ :arrow_down: ](https://arxiv.org/pdf/2007.01117.pdf)
>  In this paper, we develop a framework to maximize the network energy efficiency (EE) by optimizing joint user-base station~(BS) association,~subchannel assignment, and power control considering an in-band full-duplex (IBFD)-enabled small-cell network. We maximize EE (ratio of network aggregate throughput and power consumption) while guaranteeing a minimum data rate requirement in both the uplink and downlink. The considered problem belongs to the category of mixed-integer non-linear programming problem (MINLP), {\color{black} and thus is NP-hard}. To cope up with this complexity and to derive a trade-off between system throughput and energy utilization, we first restate the considered problem as a multi-objective optimization problem (MOOP) aiming at maximizing system's throughput and minimizing system's energy consumption, simultaneously. This MOOP is then tackled by using $\epsilon$-constraint method. To do so, we first transform the binary subchannel and BS assignment variables into continuous ones without altering the feasible region of the problem and then approximate the non-convex rate functions through majorization-minimization (MM) approach. Simulation results are presented to demonstrate the effectiveness of our proposed algorithm in improving network's EE compared to the existing literature.~Furthermore, simulation results unveil that by employing the IBFD capability in OFDMA networks, our proposed resource allocation algorithm achieves a $69\%$ improvement in the EE as compared to the half-duplex system for practical values of residual self-interference.      
### 7.Evaluation of Contemporary Convolutional Neural Network Architectures for Detecting COVID-19 from Chest Radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2007.01108.pdf)
>  Interpreting chest radiograph, a.ka. chest x-ray, images is a necessary and crucial diagnostic tool used by medical professionals to detect and identify many diseases that may plague a patient. Although the images themselves contain a wealth of valuable information, their usefulness may be limited by how well they are interpreted, especially when the reviewing radiologist may be fatigued or when or an experienced radiologist is unavailable. Research in the use of deep learning models to analyze chest radiographs yielded impressive results where, in some instances, the models outperformed practicing radiologists. Amidst the COVID-19 pandemic, researchers have explored and proposed the use of said deep models to detect COVID-19 infections from radiographs as a possible way to help ease the strain on medical resources. In this study, we train and evaluate three model architectures, proposed for chest radiograph analysis, under varying conditions, find issues that discount the impressive model performances proposed by contemporary studies on this subject, and propose methodologies to train models that yield more reliable results.. Code, scripts, pre-trained models, and visualizations are available at <a class="link-external link-https" href="https://github.com/nalbert/COVID-detection-from-radiographs" rel="external noopener nofollow">this https URL</a>.      
### 8.Joint Passive Beamforming and User Association Optimization for IRS-assisted mmWave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.01069.pdf)
>  In this paper, we investigate an intelligent reflect surface (IRS) assisted multi-user millimeter wave (mmWave) downlink communication system, exploiting IRS to alleviate the blockage effect and enhance the performance of the mmWave system. Considering the impact of IRS on user association, we formulate a sum rate maximization problem by jointly optimizing the passive beamforming at IRS and user association, which is an intractable non-convex problem. Then an alternating optimization algorithm is proposed to solve the problem efficiently. In the proposed algorithm, passive beamforming at IRS is optimized by utilizing the fractional programming method and user association is solved through the network optimization based auction algorithm. We provide numerical comparisons between the proposed algorithm and different reference algorithms. Simulation results demonstrate that the proposed algorithm can achieve significant gains in the sum rate of all users.      
### 9.Factorization over interpolation: A fast continuous orthogonal matching pursuit  [ :arrow_down: ](https://arxiv.org/pdf/2007.01060.pdf)
>  We propose a fast greedy algorithm to compute sparse representations of signals from continuous dictionaries that are factorizable, i.e., with atoms that can be separated as a product of sub-atoms. Existing algorithms strongly reduce the computational complexity of the sparse decomposition of signals in discrete factorizable dictionaries. On another flavour, existing greedy algorithms use interpolation strategies from a discretization of continuous dictionaries to perform off-the-grid decomposition. Our algorithm aims to combine the factorization and the interpolation concepts to enable low complexity computation of continuous sparse representation of signals. The efficiency of our algorithm is highlighted by simulations of its application to a radar system.      
### 10.Hyperspectral Image Denoising with Partially Orthogonal Matrix Vector Tensor Factorization  [ :arrow_down: ](https://arxiv.org/pdf/2007.01056.pdf)
>  Hyperspectral image (HSI) has some advantages over natural image for various applications due to the extra spectral information. During the acquisition, it is often contaminated by severe noises including Gaussian noise, impulse noise, deadlines, and stripes. The image quality degeneration would badly effect some applications. In this paper, we present a HSI restoration method named smooth and robust low rank tensor recovery. Specifically, we propose a structural tensor decomposition in accordance with the linear spectral mixture model of HSI. It decomposes a tensor into sums of outer matrix vector products, where the vectors are orthogonal due to the independence of endmember spectrums. Based on it, the global low rank tensor structure can be well exposited for HSI denoising. In addition, the 3D anisotropic total variation is used for spatial spectral piecewise smoothness of HSI. Meanwhile, the sparse noise including impulse noise, deadlines and stripes, is detected by the l1 norm regularization. The Frobenius norm is used for the heavy Gaussian noise in some real world scenarios. The alternating direction method of multipliers is adopted to solve the proposed optimization model, which simultaneously exploits the global low rank property and the spatial spectral smoothness of the HSI. Numerical experiments on both simulated and real data illustrate the superiority of the proposed method in comparison with the existing ones.      
### 11.Efficient Mining Cluster Selection for Blockchain-based Cellular V2X Communications  [ :arrow_down: ](https://arxiv.org/pdf/2007.01052.pdf)
>  Cellular vehicle-to-everything (V2X) communication is expected to herald the age of autonomous vehicles in the coming years. With the integration of blockchain in such networks, information of all granularity levels, from complete blocks to individual transactions, would be accessible to vehicles at any time. Specifically, the blockchain technology is expected to improve the security, immutability, and decentralization of cellular V2X communication through smart contract and distributed ledgers. Although blockchain-based cellular V2X networks hold promise, many challenges need to be addressed to enable the future interoperability and accessibility of such large-scale platforms. One such challenge is the offloading of mining tasks in cellular V2X networks. While transportation authorities may try to balance the network mining load, the vehicles may select the nearest mining clusters to offload a task. This may cause congestion and disproportionate use of vehicular network resources. To address this issue, we propose a game-theoretic approach for balancing the load at mining clusters while maintaining fairness among offloading vehicles. Keeping in mind the low-latency requirements of vehicles, we consider a finite channel blocklength transmission which is more practical compared to the use of infinite blocklength codes. The simulation results obtained with our proposed offloading framework show improved performance over the conventional nearest mining cluster selection technique.      
### 12.4D Spatio-Temporal Convolutional Networks for Object Position Estimation in OCT Volumes  [ :arrow_down: ](https://arxiv.org/pdf/2007.01044.pdf)
>  Tracking and localizing objects is a central problem in computer-assisted surgery. Optical coherence tomography (OCT) can be employed as an optical tracking system, due to its high spatial and temporal resolution. Recently, 3D convolutional neural networks (CNNs) have shown promising performance for pose estimation of a marker object using single volumetric OCT images. While this approach relied on spatial information only, OCT allows for a temporal stream of OCT image volumes capturing the motion of an object at high volumes rates. In this work, we systematically extend 3D CNNs to 4D spatio-temporal CNNs to evaluate the impact of additional temporal information for marker object tracking. Across various architectures, our results demonstrate that using a stream of OCT volumes and employing 4D spatio-temporal convolutions leads to a 30% lower mean absolute error compared to single volume processing with 3D CNNs.      
### 13.Spectral-Spatial Recurrent-Convolutional Networks for In-Vivo Hyperspectral Tumor Type Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.01042.pdf)
>  Early detection of cancerous tissue is crucial for long-term patient survival. In the head and neck region, a typical diagnostic procedure is an endoscopic intervention where a medical expert manually assesses tissue using RGB camera images. While healthy and tumor regions are generally easier to distinguish, differentiating benign and malignant tumors is very challenging. This requires an invasive biopsy, followed by histological evaluation for diagnosis. Also, during tumor resection, tumor margins need to be verified by histological analysis. To avoid unnecessary tissue resection, a non-invasive, image-based diagnostic tool would be very valuable. Recently, hyperspectral imaging paired with deep learning has been proposed for this task, demonstrating promising results on ex-vivo specimens. In this work, we demonstrate the feasibility of in-vivo tumor type classification using hyperspectral imaging and deep learning. We analyze the value of using multiple hyperspectral bands compared to conventional RGB images and we study several machine learning models' ability to make use of the additional spectral information. Based on our insights, we address spectral and spatial processing using recurrent-convolutional models for effective spectral aggregating and spatial feature learning. Our best model achieves an AUC of 76.3%, significantly outperforming previous conventional and deep learning methods.      
### 14.Robust and Low Complexity Beam Tracking with Monopulse Signal for UAV communication  [ :arrow_down: ](https://arxiv.org/pdf/2007.01006.pdf)
>  UAV communications based on an antenna array entail a beam tracking issue for reliable link acquisition. Unlike conventional cellular communication, beam tracking in UAV communication addresses new issues such as mobility and abrupt channel disconnection from UAV's perturbation. To deal with these issues, we propose a beam tracking scheme based on extended Kalman filter (EKF) using a monopulse signal, which can provide (1) higher robustness by offering a reliable link in the estimated spatial direction and (2) lower complexity compared with the existing codebook based beamforming scheme. We point out the limitations of using a beamformed signal as a measurement model for a Kalman filter (KF) based scheme and instead utilize the monopulse signal as a more plausible model. For the performance evaluation, we derive the upper bound of the mean square error for spatial angle estimation of the UAV and confirm that our proposed scheme is stable with a certain bounded error. We also show from our simulations that our proposed scheme can efficiently track UAV and detect beam disconnection every time frame using a beamformed signal.      
### 15.DeepOPF: A Feasibility-Optimized Deep Neural Network Approach for AC Optimal Power Flow Problems  [ :arrow_down: ](https://arxiv.org/pdf/2007.01002.pdf)
>  The AC-OPF problem is the key and challenging problem in the power system operation. When solving the AC-OPF problem, the feasibility issue is critical. In this paper, we develop an efficient Deep Neural Network (DNN) approach, DeepOPF, to ensure the feasibility of the generated solution. The idea is to train a DNN model to predict a set of independent operating variables, and then to directly compute the remaining dependable variables by solving the AC power flow equations. While this guarantees the power-flow balances, the principal difficulty lies in ensuring that the obtained solutions satisfy the operation limits of generations, voltages, and branch flow. We tackle this hurdle by employing a penalty approach in training the DNN. As the penalty gradients make the common first-order gradient-based algorithms prohibited due to the hardness of obtaining an explicit-form expression of the penalty gradients, we further apply a zero-order optimization technique to design the training algorithm to address the critical issue. The simulation results of the IEEE test case demonstrate the effectiveness of the penalty approach. Also, they show that DeepOPF can speed up the computing time by one order of magnitude compared to a state-of-the-art solver, at the expense of minor optimality loss.      
### 16.PGD-UNet: A Position-Guided Deformable Network for Simultaneous Segmentation of Organs and Tumors  [ :arrow_down: ](https://arxiv.org/pdf/2007.01001.pdf)
>  Precise segmentation of organs and tumors plays a crucial role in clinical applications. It is a challenging task due to the irregular shapes and various sizes of organs and tumors as well as the significant class imbalance between the anatomy of interest (AOI) and the background region. In addition, in most situation tumors and normal organs often overlap in medical images, but current approaches fail to delineate both tumors and organs accurately. To tackle such challenges, we propose a position-guided deformable UNet, namely PGD-UNet, which exploits the spatial deformation capabilities of deformable convolution to deal with the geometric transformation of both organs and tumors. Position information is explicitly encoded into the network to enhance the capabilities of deformation. Meanwhile, we introduce a new pooling module to preserve position information lost in conventional max-pooling operation. Besides, due to unclear boundaries between different structures as well as the subjectivity of annotations, labels are not necessarily accurate for medical image segmentation tasks. It may cause the overfitting of the trained network due to label noise. To address this issue, we formulate a novel loss function to suppress the influence of potential label noise on the training process. Our method was evaluated on two challenging segmentation tasks and achieved very promising segmentation accuracy in both tasks.      
### 17.Data Augmenting Contrastive Learning of Speech Representations in the Time Domain  [ :arrow_down: ](https://arxiv.org/pdf/2007.00991.pdf)
>  Contrastive Predictive Coding (CPC), based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC (relative improvement of 18-22%), beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15% relative.      
### 18.Energy Efficiency Optimization in IRS-Enhanced mmWave Systems with Lens Antenna Array  [ :arrow_down: ](https://arxiv.org/pdf/2007.00986.pdf)
>  In millimeter wave (mmWave) systems, the advanced lens antenna array can effectively reduce the radio frequency chains cost. However, the mmWave signal is still vulnerable to blocking obstacles and suffers from severe path loss. To address this problem, we propose an intelligent reflect surface (IRS) enhanced multi-user mmWave communication system with lens antenna array. Moreover, we attempt to optimize energy efficiency in the proposed system. An energy efficiency maximization problem is formulated where the transmit beamforming at base station and the reflect beamforming at IRSs are jointly considered. To solve this non-convex problem, we propose an algorithm based on the alternating optimization technique. In the proposed algorithm, the transmit beamforming is handled by the sequential convex approximation method and the reflect beamforming is optimized based on the quadratic transform method. Our simulation results show that the proposed algorithm can achieve significant energy efficiency improvement under various scenarios.      
### 19.Polyphonic sound event detection based on convolutional recurrent neural networks with semi-supervised loss function for DCASE challenge 2020 task 4  [ :arrow_down: ](https://arxiv.org/pdf/2007.00947.pdf)
>  This report proposes a polyphonic sound event detection (SED) method for the DCASE 2020 Challenge Task 4. The proposed SED method is based on semi-supervised learning to deal with the different combination of training datasets such as weakly labeled dataset, unlabeled dataset, and strongly labeled synthetic dataset. Especially, the target label of each audio clip from weakly labeled or unlabeled dataset is first predicted by using the mean teacher model that is the DCASE 2020 baseline. The data with predicted labels are used for training the proposed SED model, which consists of CNNs with skip connections and self-attention mechanism, followed by RNNs. In order to compensate for the erroneous prediction of weakly labeled and unlabeled data, a semi-supervised loss function is employed for the proposed SED model. In this work, several versions of the proposed SED model are implemented and evaluated on the validation set according to the different parameter setting for the semi-supervised loss function, and then an ensemble model that combines five-fold validation models is finally selected as our final model.      
### 20.Robust MPC for LTI Systems with Parametric and Additive Uncertainty: A Novel Constraint Tightening Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.00930.pdf)
>  We propose an approach to design a Model Predictive Controller (MPC) for constrained uncertain Linear Time Invariant systems. The uncertainty is modeled as an additive disturbance and an additive error on the system dynamics matrices. Set based bounds for each component of the model uncertainty are assumed to be known. We propose a novel optimization based constraint tightening strategy around a predicted nominal trajectory which utilizes these bounds. The resulting MPC controller guarantees robust satisfaction of state and input constraints in closed-loop with the uncertain system, while avoiding restrictive constraint tightenings around the optimal predicted nominal trajectory. With appropriately designed terminal cost function and constraint set, and an adaptive horizon strategy, we prove the recursive feasibility of the controller in closed-loop and Input to State Stability of the origin. We highlight the efficacy of our proposed approach via a detailed numerical example.      
### 21.Semi-Supervised NMF-CNN For Sound Event Detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.00908.pdf)
>  For the DCASE 2020 Challenge Task 4, this paper pro-posed a combinative approach using Nonnegative Matrix Factorization (NMF) and Convolutional Neural Network (CNN). The main idea begins with utilizing NMF to ap-proximate strong labels for the weakly labeled data. Sub-sequently, based on the approximated strongly labeled data, two different CNNs are trained using a semi-supervised framework where one CNN is used for clip-level prediction and the other for frame-level prediction. Using this idea, the best model trained can achieve an event-based F1-score of 45.7% on the validation dataset. Using an ensemble of models, the event-based F1-score can be increased to 48.6%. By comparing with the base-line model, the proposed model outperforms the baseline model by a margin of over 8%.      
### 22.Non-Gaussianity Detection of EEG Signals Based on a Multivariate Scale Mixture Model for Diagnosis of Epileptic Seizures  [ :arrow_down: ](https://arxiv.org/pdf/2007.00898.pdf)
>  Objective: The detection of epileptic seizures from scalp electroencephalogram (EEG) signals can facilitate early diagnosis and treatment. Previous studies suggested that the Gaussianity of EEG distributions changes depending on the presence or absence of seizures; however, no general EEG signal models can explain such changes in distributions within a unified scheme. Methods: This paper describes the formulation of a stochastic EEG model based on a multivariate scale mixture distribution that can represent changes in non-Gaussianity caused by stochastic fluctuations in EEG. In addition, we propose an EEG analysis method by combining the model with a filter bank and introduce a feature representing the non-Gaussianity latent in each EEG frequency band. Results: We applied the proposed method to multichannel EEG data from twenty patients with focal epilepsy. The results showed a significant increase in the proposed feature during epileptic seizures, particularly in the high-frequency band. The feature calculated in the high-frequency band allowed highly accurate classification of seizure and non-seizure segments [area under the receiver operating characteristic curve (AUC) = 0.881] using only a simple threshold. Conclusion: This paper proposed a multivariate scale mixture distribution-based stochastic EEG model capable of representing non-Gaussianity associated with epileptic seizures. Experiments using simulated and real EEG data demonstrated the validity of the model and its applicability to epileptic seizure detection. Significance: The stochastic fluctuations of EEG quantified by the proposed model can help detect epileptic seizures with high accuracy.      
### 23.On the Uniformly-Damped Binomial Filter  [ :arrow_down: ](https://arxiv.org/pdf/2007.00890.pdf)
>  The problem of approximating the response of the ideal frequency-selective transfer-function in both the time and frequency domain represents a fundamental limitation in linear systems theory. In this paper, we propose the uniformly-damped binomial filter (UDBF) transfer-function as a better and balanced compromise to this approximation problem in the time and frequency domain, than both the butterworth filter and the binomial filter. This class of filter can be viewed as a general approach to realize, in any integer order, a damped binomial filter transfer-function with a maximum complementary-sensitivity and transient response similar to the standard second-order butterworth filter. We further demonstrate that this uniformly-damped binomial response overcomes both the excessive ringing phenomena associated with the butterworth response, and the sluggish response associated with the binomial response for higher order transfer-functions. Finally, we conclude that in applications of interest, where both strong filtering and a smooth transient-response are desired, this uniformly-damped binomial standard form response is a viable replacement for both the butterworth and binomial forms.      
### 24.Multi-mode OAM Radio Waves: Generation, Angle of Arrival Estimation and Reception With UCAs  [ :arrow_down: ](https://arxiv.org/pdf/2007.00881.pdf)
>  Orbital angular momentum (OAM) at radio frequency (RF) provides a novel approach of multiplexing a set of orthogonal modes on the same frequency channel to achieve high spectrum efficiencies. However, there are still big challenges in the multi-mode OAM generation, OAM antenna alignment and OAM signal reception. To solve these problems, we propose an overall scheme of the line-of-sight multi-carrier and multi-mode OAM (LoS MCMM-OAM) communication based on uniform circular arrays (UCAs). First, we verify that UCA can generate multi-mode OAM radio beam with both the RF analog synthesis method and the baseband digital synthesis method. Then, for the considered UCA-based LoS MCMM-OAM communication system, a distance and AoA estimation method is proposed based on the two-dimensional ESPRIT (2-D ESPRIT) algorithm. A salient feature of the proposed LoS MCMM-OAM and LoS MCMM-OAM-MIMO systems is that the channel matrices are completely characterized by three parameters, namely, the azimuth angle, the elevation angle and the distance, independent of the numbers of subcarriers and antennas, which significantly reduces the burden by avoiding estimating large channel matrices, as traditional MIMO-OFDM systems. After that, we propose an OAM reception scheme including the beam steering with the estimated AoA and the amplitude detection with the estimated distance. At last, the proposed methods are extended to the LoS MCMM-OAM-MIMO system equipped with uniform concentric circular arrays (UCCAs). Both mathematical analysis and simulation results validate that the proposed OAM reception scheme can eliminate the effect of the misalignment error of a practical OAM channel and approaches the performance of an ideally aligned OAM channel.      
### 25.An encoder-decoder-based method for COVID-19 lung infection segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.00861.pdf)
>  The novelty of the COVID-19 disease and the speed of spread has created a colossal chaos, impulse among researchers worldwide to exploit all the resources and capabilities to understand and analyze characteristics of the coronavirus in term of the ways it spreads and virus incubation time. For that, the existing medical features like CT and X-ray images are used. For example, CT-scan images can be used for the detection of lung infection. But the challenges of these features such as the quality of the image and infection characteristics limitate the effectiveness of these features. Using artificial intelligence (AI) tools and computer vision algorithms, the accuracy of detection can be more accurate and can help to overcome these issues. This paper proposes a multi-task deep-learning-based method for lung infection segmentation using CT-scan images. Our proposed method starts by segmenting the lung regions that can be infected. Then, segmenting the infections in these regions. Also, to perform a multi-class segmentation the proposed model is trained using the two-stream inputs. The multi-task learning used in this paper allows us to overcome shortage of labeled data. Also, the multi-input stream allows the model to do the learning on many features that can improve the results. To evaluate the proposed method, many features have been used. Also, from the experiments, the proposed method can segment lung infections with a high degree performance even with shortage of data and labeled images. In addition, comparing with the state-of-the-art method our method achieves good performance results.      
### 26.Computationally Efficient Learning of Large Scale Dynamical Systems: A Koopman Theoretic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.00835.pdf)
>  In recent years there has been a considerable drive towards data-driven analysis, discovery and control of dynamical systems. To this end, operator theoretic methods, namely, Koopman operator methods have gained a lot of interest. In general, the Koopman operator is obtained as a solution to a least-squares problem, and as such, the Koopman operator can be expressed as a closed-form solution that involves the computation of Moore-Penrose inverse of a matrix. For high dimensional systems and also if the size of the obtained data-set is large, the computation of the Moore-Penrose inverse becomes computationally challenging. In this paper, we provide an algorithm for computing the Koopman operator for high dimensional systems in a time-efficient manner. We further demonstrate the efficacy of the proposed approach on two different systems, namely a network of coupled oscillators (with state-space dimension up to 2500) and IEEE 68 bus system (with state-space dimension 204 and up to 24,000 time-points).      
### 27.Uncertainty-Guided Efficient Interactive Refinement of Fetal Brain Segmentation from Stacks of MRI Slices  [ :arrow_down: ](https://arxiv.org/pdf/2007.00833.pdf)
>  Segmentation of the fetal brain from stacks of motion-corrupted fetal MRI slices is important for motion correction and high-resolution volume reconstruction. Although Convolutional Neural Networks (CNNs) have been widely used for automatic segmentation of the fetal brain, their results may still benefit from interactive refinement for challenging slices. To improve the efficiency of interactive refinement process, we propose an Uncertainty-Guided Interactive Refinement (UGIR) framework. We first propose a grouped convolution-based CNN to obtain multiple automatic segmentation predictions with uncertainty estimation in a single forward pass, then guide the user to provide interactions only in a subset of slices with the highest uncertainty. A novel interactive level set method is also proposed to obtain a refined result given the initial segmentation and user interactions. Experimental results show that: (1) our proposed CNN obtains uncertainty estimation in real time which correlates well with mis-segmentations, (2) the proposed interactive level set is effective and efficient for refinement, (3) UGIR obtains accurate refinement results with around 30% improvement of efficiency by using uncertainty to guide user interactions. Our code is available online.      
### 28.Automated Empathy Detection for Oncology Encounters  [ :arrow_down: ](https://arxiv.org/pdf/2007.00809.pdf)
>  Empathy involves understanding other people's situation, perspective, and feelings. In clinical interactions, it helps clinicians establish rapport with a patient and support patient-centered care and decision making. Understanding physician communication through observation of audio-recorded encounters is largely carried out with manual annotation and analysis. However, manual annotation has a prohibitively high cost. In this paper, a multimodal system is proposed for the first time to automatically detect empathic interactions in recordings of real-world face-to-face oncology encounters that might accelerate manual processes. An automatic speech and language processing pipeline is employed to segment and diarize the audio as well as for transcription of speech into text. Lexical and acoustic features are derived to help detect both empathic opportunities offered by the patient, and the expressed empathy by the oncologist. We make the empathy predictions using Support Vector Machines (SVMs) and evaluate the performance on different combinations of features in terms of average precision (AP).      
### 29.Fast DTW and Fuzzy Clustering for Scenario Generation in Power System Planning Problems  [ :arrow_down: ](https://arxiv.org/pdf/2007.00805.pdf)
>  Power system planning problems become computationally intractable if one accounts for all uncertain operating scenarios. Consequently, one selects a subset of scenarios that are representative of likely/extreme operating conditions, e.g. heavy summer, heavy winter, light summer, and so on. However, such an approach may not be able to accurately capture the dependencies that exist between renewable generation (RG) and system load in RG-rich power systems. This paper proposes the use of fast dynamic time warping (FDTW) and fuzzy c-means++ (FCM++) clustering to account for key statistical properties of load and RG for scenario generation for power system planning problems. Case studies using a U.S. power network, and comparison with existing scenario generation techniques demonstrate the benefits of the proposed approach.      
### 30.Learning a Distributed Control Scheme for Demand Flexibility in Thermostatically Controlled Loads  [ :arrow_down: ](https://arxiv.org/pdf/2007.00791.pdf)
>  Demand flexibility is increasingly important for power grids, in light of growing penetration of renewable generation. Careful coordination of thermostatically controlled loads (TCLs) can potentially modulate energy demand, decrease operating costs, and increase grid resiliency. However, it is challenging to control a heterogeneous population of TCLs: the control problem has a large state action space; each TCL has unique and complex dynamics; and multiple system-level objectives need to be optimized simultaneously. To address these challenges, we propose a distributed control solution, which consists of a central load aggregator that optimizes system-level objectives and building-level controllers that track the load profiles planned by the aggregator. To optimize our agents' policies, we draw inspirations from both reinforcement learning (RL) and model predictive control. Specifically, the aggregator is updated with an evolutionary strategy, which was recently demonstrated to be a competitive and scalable alternative to more sophisticated RL algorithms and enables policy updates independent of the building-level controllers. We evaluate our proposed approach across four climate zones in four nine-building clusters, using the newly-introduced CityLearn simulation environment. Our approach achieved an average reduction of 16.8% in the environment cost compared to the benchmark rule-based controller.      
### 31.Rapid tissue oxygenation mapping from snapshot structured-light images with adversarial deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.00760.pdf)
>  Spatial frequency domain imaging (SFDI) is a powerful technique for mapping tissue oxygen saturation over a wide field of view. However, current SFDI methods either require a sequence of several images with different illumination patterns or, in the case of single snapshot optical properties (SSOP), introduce artifacts and sacrifice accuracy. To avoid this tradeoff, we introduce OxyGAN: a data-driven, content-aware method to estimate tissue oxygenation directly from single structured light images using end-to-end generative adversarial networks. Conventional SFDI is used to obtain ground truth tissue oxygenation maps for ex vivo human esophagi, in vivo hands and feet, and an in vivo pig colon sample under 659 nm and 851 nm sinusoidal illumination. We benchmark OxyGAN by comparing to SSOP and to a two-step hybrid technique that uses a previously-developed deep learning model to predict optical properties followed by a physical model to calculate tissue oxygenation. When tested on human feet, a cross-validated OxyGAN maps tissue oxygenation with an accuracy of 96.5%. When applied to sample types not included in the training set, such as human hands and pig colon, OxyGAN achieves a 93.0% accuracy, demonstrating robustness to various tissue types. On average, OxyGAN outperforms SSOP and a hybrid model in estimating tissue oxygenation by 24.9% and 24.7%, respectively. Lastly, we optimize OxyGAN inference so that oxygenation maps are computed ~10 times faster than previous work, enabling video-rate, 25Hz imaging. Due to its rapid acquisition and processing speed, OxyGAN has the potential to enable real-time, high-fidelity tissue oxygenation mapping that may be useful for many clinical applications.      
### 32.Generating Series for Networks of Chen-Fliess Series  [ :arrow_down: ](https://arxiv.org/pdf/2007.00743.pdf)
>  Consider a set of single-input, single-output nonlinear systems whose input-output maps are described only in terms of convergent Chen-Fliess series without any assumption that finite dimensional state space models are available. It is shown that any additive or multiplicative interconnection of such systems always has a Chen-Fliess series representation that can be computed explicitly in terms of iterated formal Lie derivatives.      
### 33.From Spectrum Wavelet to Vertex Propagation: Graph Convolutional Networks Based on Taylor Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2007.00730.pdf)
>  Graph convolutional networks (GCN) have been recently applied to semi-supervised classification problems with fewer labeled data and higher-dimensional features. Existing GCNs mostly rely on a first-order Chebyshev approximation of the graph wavelet-kernels. Such a generic propagation model may not always be well suited for the datasets. This work revisits the fundamentals of graph wavelet and explores the utility of spectral wavelet-kernels to signal propagation in the vertex domain. We first derive the conditions for representing the graph wavelet-kernels via vertex propagation. We next propose alternative propagation models for GCN layers based on Taylor expansions. We further analyze the choices of detailed propagation models. We test the proposed Taylor-based GCN (TGCN) in citation networks and 3D point clouds to demonstrate its advantages over traditional GCN methods.      
### 34.Parkinson's Disease Detection Using Ensemble Architecture from MR Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.00682.pdf)
>  Parkinson's Disease(PD) is one of the major nervous system disorders that affect people over 60. PD can cause cognitive impairments. In this work, we explore various approaches to identify Parkinson's using Magnetic Resonance (MR) T1 images of the brain. We experiment with ensemble architectures combining some winning Convolutional Neural Network models of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and propose two architectures. We find that detection accuracy increases drastically when we focus on the Gray Matter (GM) and White Matter (WM) regions from the MR images instead of using whole MR images. We achieved an average accuracy of 94.7\% using smoothed GM and WM extracts and one of our proposed architectures. We also perform occlusion analysis and determine which brain areas are relevant in the architecture decision making process.      
### 35.Distributed Safe Learning using an Invariance-based Safety Framework  [ :arrow_down: ](https://arxiv.org/pdf/2007.00681.pdf)
>  In large-scale networks of uncertain dynamical systems, where communication is limited and there is a strong interaction among subsystems, learning local models and control policies offers great potential for designing high-performance controllers. At the same time, the lack of safety guarantees, here considered in the form of constraint satisfaction, prevents the use of data-driven techniques to safety-critical distributed systems. This paper presents a safety framework that guarantees constraint satisfaction for uncertain distributed systems while learning. The framework considers linear systems with coupling in the dynamics and subject to bounded parametric uncertainty, and makes use of robust invariance to guarantee safety. In particular, a robust non-convex invariant set, given by the union of multiple ellipsoidal invariant sets, and a nonlinear backup control law, given by the combination of multiple stabilizing linear feedbacks, are computed offline. In presence of unsafe inputs, the safety framework applies the backup control law, preventing the system to violate the constraints. As the robust invariant set and the backup stabilizing controller are computed offline, the online operations reduce to simple function evaluations, which enables the use of the proposed framework on systems with limited computational resources. The capabilities of the safety framework are illustrated by three numerical examples.      
### 36.LSTM and GPT-2 Synthetic Speech Transfer Learning for Speaker Recognition to Overcome Data Scarcity  [ :arrow_down: ](https://arxiv.org/pdf/2007.00659.pdf)
>  In speech recognition problems, data scarcity often poses an issue due to the willingness of humans to provide large amounts of data for learning and classification. In this work, we take a set of 5 spoken Harvard sentences from 7 subjects and consider their MFCC attributes. Using character level LSTMs (supervised learning) and OpenAI's attention-based GPT-2 models, synthetic MFCCs are generated by learning from the data provided on a per-subject basis. A neural network is trained to classify the data against a large dataset of Flickr8k speakers and is then compared to a transfer learning network performing the same task but with an initial weight distribution dictated by learning from the synthetic data generated by the two models. The best result for all of the 7 subjects were networks that had been exposed to synthetic data, the model pre-trained with LSTM-produced data achieved the best result 3 times and the GPT-2 equivalent 5 times (since one subject had their best result from both models at a draw). Through these results, we argue that speaker classification can be improved by utilising a small amount of user data but with exposure to synthetically-generated MFCCs which then allow the networks to achieve near maximum classification scores.      
### 37.Throughput Analysis for Virtual MIMO WSNs over Measured MIMO Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.00656.pdf)
>  A recently conducted indoor-to-outdoor measurement campaign for investigating the propagation characteristics of an $8 \times 8$ virtual multiple-input-multiple-output (MIMO) based wireless sensor network (WSN) is presented in this paper. The campaign is conducted in an instrumentation room devoid of windows, but filled with different noisy electrical and measuring units. The channel impulse responses are reported when a 20 MHz wide signal is transmitted at 2.53 GHz. Measurements are collected for 15 different spatial combinations of the transmit antennas. After analyzing the collected data, system capacity and achievable transmission rates are calculated for each measurement scenario. Using these values, we examined the best configuration for positioning the sensors that can maximize overall network throughput. Results demonstrated that distributing sensors on all 4 walls of the room can achieve the highest possible information rate.      
### 38.Image Classification by Reinforcement Learning with Two-State Q-Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.01298.pdf)
>  In this paper, a simple and efficient Hybrid Classifier is presented which is based on deep learning and reinforcement learning. Q-Learning has been used with two states and 'two or three' actions. Other techniques found in the literature use feature map extracted from Convolutional Neural Networks and use these in the Q-states along with past history. This leads to technical difficulties in these approaches because the number of states is high due to large dimensions of the feature map. Because our technique uses only two Q-states it is straightforward and consequently has much lesser number of optimization parameters, and thus also has a simple reward function. Also, the proposed technique uses novel actions for processing images as compared to other techniques found in literature. The performance of the proposed technique is compared with other recent algorithms like ResNet50, InceptionV3, etc. on popular databases including ImageNet, Cats and Dogs Dataset, and Caltech-101 Dataset. Our approach outperforms others techniques on all the datasets used.      
### 39.Deep Learning for Neuroimaging-based Diagnosis and Rehabilitation of Autism Spectrum Disorder: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2007.01285.pdf)
>  Accurate diagnosis of Autism Spectrum Disorder (ASD) is essential for management and rehabilitation. Neuro-imaging techniques that are non-invasive are disease markers and may be leveraged to aid ASD diagnosis. Structural and functional neural imaging techniques provide physicians substantial information about the structure (anatomy and structural communication) and function (activity and functional communication) of the brain. Due to the intricate structure and function of the brain, diagnosing ASD with neuro-imaging data without exploiting artificial intelligence (AI) techniques is extremely challenging. AI techniques comprise traditional machine learning (ML) approaches and deep learning (DL) techniques. Conventional ML methods employ various feature extraction and classification techniques, but in DL, the process of feature extraction and classification is accomplished intelligently and integrally. In this paper, studies conducted with the aid of DL networks to distinguish ASD were investigated. Rehabilitation tools provided by supporting ASD patients utilizing DL networks were also assessed. Finally, we presented important challenges in this automated detection and rehabilitation of ASD.      
### 40.Curriculum Manager for Source Selection in Multi-Source Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2007.01261.pdf)
>  The performance of Multi-Source Unsupervised Domain Adaptation depends significantly on the effectiveness of transfer from labeled source domain samples. In this paper, we proposed an adversarial agent that learns a dynamic curriculum for source samples, called Curriculum Manager for Source Selection (CMSS). The Curriculum Manager, an independent network module, constantly updates the curriculum during training, and iteratively learns which domains or samples are best suited for aligning to the target. The intuition behind this is to force the Curriculum Manager to constantly re-measure the transferability of latent domains over time to adversarially raise the error rate of the domain discriminator. CMSS does not require any knowledge of the domain labels, yet it outperforms other methods on four well-known benchmarks by significant margins. We also provide interpretable results that shed light on the proposed method.      
### 41.AutoBayes: Automated Inference via Bayesian Graph Exploration for Nuisance-Robust Biosignal Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.01255.pdf)
>  Learning data representations that capture task-related features, but are invariant to nuisance variations remains a key challenge in machine learning, in particular for biosignal processing. We introduce an automated Bayesian inference framework, called AutoBayes, that explores different graphical models linking classifier, encoder, decoder, estimator and adversary network blocks to optimize nuisance-invariant machine learning pipelines. AutoBayes also enables justifying disentangled representation, which splits the latent variable into multiple pieces to impose different relation with subject/session-variation and task labels. We benchmark the framework on a series of physiological datasets, where we have access to subject and class labels during training, and provide analysis of its capability for subject transfer learning with/without variational modeling and adversarial training. The framework can be effectively utilized in semi-supervised multi-class classification, and reconstruction tasks for datasets in different domains as well.      
### 42.Spot the conversation: speaker diarisation in the wild  [ :arrow_down: ](https://arxiv.org/pdf/2007.01216.pdf)
>  The goal of this paper is speaker diarisation of videos collected 'in the wild'. We make three key contributions. First, we propose an automatic audio-visual diarisation method for YouTube videos. Our method consists of active speaker detection using audio-visual methods and speaker verification using self-enrolled speaker models. Second, we integrate our method into a semi-automatic dataset creation pipeline which significantly reduces the number of hours required to annotate videos with diarisation labels. Finally, we use this pipeline to create a large-scale diarisation dataset called VoxConverse, collected from 'in the wild' videos, which we will release publicly to the research community. Our dataset consists of overlapping speech, a large and diverse speaker pool, and challenging background conditions.      
### 43.Reinforcement Learning Based Handwritten Digit Recognition with Two-State Q-Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.01193.pdf)
>  We present a simple yet efficient Hybrid Classifier based on Deep Learning and Reinforcement Learning. Q-Learning is used with two Q-states and four actions. Conventional techniques use feature maps extracted from Convolutional Neural Networks (CNNs) and include them in the Qstates along with past history. This leads to difficulties with these approaches as the number of states is very large number due to high dimensions of the feature maps. Since our method uses only two Q-states it is simple and has much lesser number of parameters to optimize and also thus has a straightforward reward function. Also, the approach uses unexplored actions for image processing vis-a-vis other contemporary techniques. Three datasets have been used for benchmarking of the approach. These are the MNIST Digit Image Dataset, the USPS Digit Image Dataset and the MATLAB Digit Image Dataset. The performance of the proposed hybrid classifier has been compared with other contemporary techniques like a well-established Reinforcement Learning Technique, AlexNet, CNN-Nearest Neighbor Classifier and CNNSupport Vector Machine Classifier. Our approach outperforms these contemporary hybrid classifiers on all the three datasets used.      
### 44.Multiclass Classification with an Ensemble of Binary Classification Deep Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.01192.pdf)
>  Deep neural network classifiers have been used frequently and are efficient. In multiclass deep network classifiers, the burden of classifying samples of different classes is put on a single classifier. As shown in this paper, the classification capability of deep networks can be further increased by using an ensemble of binary classification deep networks. In the proposed approach, a single (one-versus-all) deep network binary classifier is dedicated to each category classification. Subsequently, binary classification deep network ensembles have been investigated. Every network in an ensemble has been trained by a one-versus-all binary training technique using the Stochastic Gradient Descent with Momentum Algorithm. For classification of the test sample, the sample is presented to each network in the ensemble. After softmax-layer score voting, the network with the largest score is assumed to have classified the sample. Digit image recognition has been used for experimentation. Three datasets have been used for experimentation viz. the MATLAB Digit Image Dataset, the USPS+ Digit Image Dataset, and the MNIST Digit Image Dataset. The experiments demonstrate that given sufficient training, a Binary Classification Convolutional Neural Network (BCCNN) ensemble can outperform a conventional Multi-class Convolutional Neural Network (MCNN). In one of the experiments, it was noted that after training and testing of a BCCNN ensemble and an MCNN respectively on a subset of the MNIST Digit Image Dataset, the BCCNN ensemble gave a higher accuracy of 98.03% as compared to the MCNN which gave an accuracy of 97.90%. The architecture of the BCCNNs in an ensemble has also been modified in order to increase their recognition accuracy. On a large subset of the MNIST Digit Image Dataset, the modified BCCNN ensemble gave a higher accuracy of 98.50%, while as the MCNN gave an accuracy of 98.4875%.      
### 45.Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.01076.pdf)
>  This work explores the combination of free cloud computing, free open-source software, and deep learning methods to analyse a real, large-scale problem: the automatic country-wide identification and classification of surface mines and mining tailings dams in Brazil. Locations of officially registered mines and dams were obtained from the Brazilian government open data resource. Multispectral Sentinel-2 satellite imagery, obtained and processed at the Google Earth Engine platform, was used to train and test deep neural networks using the TensorFlow 2 API and Google Colab platform. Fully Convolutional Neural Networks were used in an innovative way, to search for unregistered ore mines and tailing dams in large areas of the Brazilian territory. The efficacy of the approach is demonstrated by the discovery of 263 mines that do not have an official mining concession. This exploratory work highlights the potential of a set of new technologies, freely available, for the construction of low cost data science tools that have high social impact. At the same time, it discusses and seeks to suggest practical solutions for the complex and serious problem of illegal mining and the proliferation of tailings dams, which pose high risks to the population and the environment, especially in developing countries. Code is made publicly available at: <a class="link-external link-https" href="https://github.com/remis/mining-discovery-with-deep-learning" rel="external noopener nofollow">this https URL</a>.      
### 46.Are there any 'object detectors' in the hidden layers of CNNs trained to identify objects or scenes?  [ :arrow_down: ](https://arxiv.org/pdf/2007.01062.pdf)
>  Various methods of measuring unit selectivity have been developed with the aim of better understanding how neural networks work. But the different measures provide divergent estimates of selectivity, and this has led to different conclusions regarding the conditions in which selective object representations are learned and the functional relevance of these representations. In an attempt to better characterize object selectivity, we undertake a comparison of various selectivity measures on a large set of units in AlexNet, including localist selectivity, precision, class-conditional mean activity selectivity (CCMAS), network dissection,the human interpretation of activation maximization (AM) images, and standard signal-detection measures. We find that the different measures provide different estimates of object selectivity, with precision and CCMAS measures providing misleadingly high estimates. Indeed, the most selective units had a poor hit-rate or a high false-alarm rate (or both) in object classification, making them poor object detectors. We fail to find any units that are even remotely as selective as the 'grandmother cell' units reported in recurrent neural networks. In order to generalize these results, we compared selectivity measures on units in VGG-16 and GoogLeNet trained on the ImageNet or Places-365 datasets that have been described as 'object detectors'. Again, we find poor hit-rates and high false-alarm rates for object classification. We conclude that signal-detection measures provide a better assessment of single-unit selectivity compared to common alternative approaches, and that deep convolutional networks of image classification do not learn object detectors in their hidden layers.      
### 47.On the inefficiency of ride-sourcing services towards urban congestion  [ :arrow_down: ](https://arxiv.org/pdf/2007.00980.pdf)
>  The advent of shared-economy and smartphones made on-demand transportation services possible, which created additional opportunities, but also more complexity to urban mobility. Companies that offer these services are called Transportation Network Companies (TNCs) due to their internet-based nature. Although ride-sourcing is the most notorious service TNCs provide, little is known about to what degree its operations can interfere in traffic conditions, while replacing other transportation modes, or when a large number of idle vehicles is cruising for passengers. We experimentally analyze the efficiency of TNCs using taxi trip data from a Chinese megacity and a agent-based simulation with a trip-based MFD model for determining the speed. We investigate the effect of expanding fleet sizes for TNCs, passengers' inclination towards sharing rides, and strategies to alleviate urban congestion. We show that the lack of coordination of objectives between TNCs and society can create 37% longer travel times and significant congestion. Moreover, allowing shared rides is not capable of decreasing total distance traveled due to higher empty kilometers traveled. Elegant parking management strategies can prevent idle vehicles from cruising without assigned passengers and lower to 7% the impacts of the absence of coordination.      
### 48.Decentralized Blockchain for Privacy-Preserving Large-Scale Contact Tracing  [ :arrow_down: ](https://arxiv.org/pdf/2007.00894.pdf)
>  Activity-tracking applications and location-based services using short-range communication (SRC) techniques have been abruptly demanded in the COVID-19 pandemic, especially for automated contact tracing. The attention from both public and policy keeps raising on related practical problems, including \textit{1) how to protect data security and location privacy? 2) how to efficiently and dynamically deploy SRC Internet of Thing (IoT) witnesses to monitor large areas?} To answer these questions, in this paper, we propose a decentralized and permissionless blockchain protocol, named \textit{Bychain}. Specifically, 1) a privacy-preserving SRC protocol for activity-tracking and corresponding generalized block structure is developed, by connecting an interactive zero-knowledge proof protocol and the key escrow mechanism. As a result, connections between personal identity and the ownership of on-chain location information are decoupled. Meanwhile, the owner of the on-chain location data can still claim its ownership without revealing the private key to anyone else. 2) An artificial potential field-based incentive allocation mechanism is proposed to incentivize IoT witnesses to pursue the maximum monitoring coverage deployment. We implemented and evaluated the proposed blockchain protocol in the real-world using the Bluetooth 5.0. The storage, CPU utilization, power consumption, time delay, and security of each procedure and performance of activities are analyzed. The experiment and security analysis is shown to provide a real-world performance evaluation.      
### 49.MSA-MIL: A deep residual multiple instance learning model based on multi-scale annotation for classification and visualization of glomerular spikes  [ :arrow_down: ](https://arxiv.org/pdf/2007.00858.pdf)
>  Membranous nephropathy (MN) is a frequent type of adult nephrotic syndrome, which has a high clinical incidence and can cause various complications. In the biopsy microscope slide of membranous nephropathy, spikelike projections on the glomerular basement membrane is a prominent feature of the MN. However, due to the whole biopsy slide contains large number of glomeruli, and each glomerulus includes many spike lesions, the pathological feature of the spikes is not obvious. It thus is time-consuming for doctors to diagnose glomerulus one by one and is difficult for pathologists with less experience to diagnose. In this paper, we establish a visualized classification model based on the multi-scale annotation multi-instance learning (MSA-MIL) to achieve glomerular classification and spikes visualization. The MSA-MIL model mainly involves three parts. Firstly, U-Net is used to extract the region of the glomeruli to ensure that the features learned by the succeeding algorithm are focused inside the glomeruli itself. Secondly, we use MIL to train an instance-level classifier combined with MSA method to enhance the learning ability of the network by adding a location-level labeled reinforced dataset, thereby obtaining an example-level feature representation with rich semantics. Lastly, the predicted scores of each tile in the image are summarized to obtain glomerular classification and visualization of the classification results of the spikes via the usage of sliding window method. The experimental results confirm that the proposed MSA-MIL model can effectively and accurately classify normal glomeruli and spiked glomerulus and visualize the position of spikes in the glomerulus. Therefore, the proposed model can provide a good foundation for assisting the clinical doctors to diagnose the glomerular membranous nephropathy.      
### 50.Deep Learning Methods for Universal MISO Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2007.00841.pdf)
>  This letter studies deep learning (DL) approaches to optimize beamforming vectors in downlink multi-user multi-antenna systems that can be universally applied to arbitrarily given transmit power limitation at a base station. We exploit the sum power budget as side information so that deep neural networks (DNNs) can effectively learn the impact of the power constraint in the beamforming optimization. Consequently, a single training process is sufficient for the proposed universal DL approach, whereas conventional methods need to train multiple DNNs for all possible power budget levels. Numerical results demonstrate the effectiveness of the proposed DL methods over existing schemes.      
### 51.OrchideaSOL: a dataset of extended instrumental techniques for computer-aided orchestration  [ :arrow_down: ](https://arxiv.org/pdf/2007.00763.pdf)
>  This paper introduces OrchideaSOL, a free dataset of samples of extended instrumental playing techniques, designed to be used as default dataset for the Orchidea framework for target-based computer-aided orchestration. OrchideaSOL is a reduced and modified subset of Studio On Line, or SOL for short, a dataset developed at Ircam between 1996 and 1998. We motivate the reasons behind OrchideaSOL and describe the differences between the original SOL and our dataset. We will also show the work done in improving the dynamic ranges of orchestral families and other aspects of the data.      
### 52.Deep learning-based holographic polarization microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2007.00741.pdf)
>  Polarized light microscopy provides high contrast to birefringent specimen and is widely used as a diagnostic tool in pathology. However, polarization microscopy systems typically operate by analyzing images collected from two or more light paths in different states of polarization, which lead to relatively complex optical designs, high system costs or experienced technicians being required. Here, we present a deep learning-based holographic polarization microscope that is capable of obtaining quantitative birefringence retardance and orientation information of specimen from a phase recovered hologram, while only requiring the addition of one polarizer/analyzer pair to an existing holographic imaging system. Using a deep neural network, the reconstructed holographic images from a single state of polarization can be transformed into images equivalent to those captured using a single-shot computational polarized light microscope (SCPLM). Our analysis shows that a trained deep neural network can extract the birefringence information using both the sample specific morphological features as well as the holographic amplitude and phase distribution. To demonstrate the efficacy of this method, we tested it by imaging various birefringent samples including e.g., monosodium urate (MSU) and triamcinolone acetonide (TCA) crystals. Our method achieves similar results to SCPLM both qualitatively and quantitatively, and due to its simpler optical design and significantly larger field-of-view, this method has the potential to expand the access to polarization microscopy and its use for medical diagnosis in resource limited settings.      
