# ArXiv eess --Fri, 10 Jul 2020
### 1.Assessing the impact of inertia and reactive power constraints in generation expansion planning  [ :arrow_down: ](https://arxiv.org/pdf/2007.04843.pdf)
>  On the path towards power systems with high renewable penetrations and ultimately carbon-neutral, more and more synchronous generation is being displaced by variable renewable generation that does not currently provide system inertia nor reactive power support. This could create serious issues of power system stability in the near future, and countries with high renewable penetrations such as Ireland are already facing these challenges. Therefore, this paper aims at answering the questions of whether and how explicitly including inertia and reactive power constraints in generation expansion planning would affect the optimal capacity mix of the power system of the future. Towards this end, we propose the novel Low-carbon Expansion Generation Optimization (LEGO) model, which explicitly accounts for: unit commitment constraints, Rate of Change of Frequency (RoCoF) inertia requirements and virtual inertia provision, and, a second-order cone programming (SOCP) approximation of the AC power flow, accounting for reactive power constraints. An illustrative case study underlines that disregarding inertia and reactive power constraints in generation expansion planning can result in additional system cost, system infeasibilities, a distortion of optimal resource allocation and inability to reach established policy goals.      
### 2.Medical Instrument Detection in Ultrasound-Guided Interventions: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2007.04807.pdf)
>  Medical instrument detection is essential for computer-assisted interventions since it would facilitate the surgeons to find the instrument efficiently with a better interpretation, which leads to a better outcome. This article reviews medical instrument detection methods in the ultrasound-guided intervention. First, we present a comprehensive review of instrument detection methodologies, which include traditional non-data-driven methods and data-driven methods. The non-data-driven methods were extensively studied prior to the era of machine learning, i.e. data-driven approaches. We discuss the main clinical applications of medical instrument detection in ultrasound, including anesthesia, biopsy, prostate brachytherapy, and cardiac catheterization, which were validated on clinical datasets. Finally, we selected several principal publications to summarize the key issues and potential research directions for the computer-assisted intervention community.      
### 3.A Novel Heap-based Pilot Assignment for Full Duplex Cell-Free Massive MIMO with Zero-Forcing  [ :arrow_down: ](https://arxiv.org/pdf/2007.04787.pdf)
>  This paper investigates the combined benefits of full-duplex (FD) and cell-free massive multiple-input multipleoutput (CF-mMIMO), where a large number of distributed access points (APs) having FD capability simultaneously serve numerous uplink and downlink user equipments (UEs) on the same time-frequency resources. To enable the incorporation of FD technology in CF-mMIMO systems, we propose a novel heapbased pilot assignment algorithm, which not only can mitigate the effects of pilot contamination but also reduce the involved computational complexity. Then, we formulate a robust design problem for spectral efficiency (SE) maximization in which the power control and AP-UE association are jointly optimized, resulting in a difficult mixed-integer nonconvex programming. To solve this problem, we derive a more tractable problem before developing a very simple iterative algorithm based on inner approximation method with polynomial computational complexity. Numerical results show that our proposed methods with realistic parameters significantly outperform the existing approaches in terms of the quality of channel estimate and SE.      
### 4.Modelling the Distribution of 3D Brain MRI using a 2D Slice VAE  [ :arrow_down: ](https://arxiv.org/pdf/2007.04780.pdf)
>  Probabilistic modelling has been an essential tool in medical image analysis, especially for analyzing brain Magnetic Resonance Images (MRI). Recent deep learning techniques for estimating high-dimensional distributions, in particular Variational Autoencoders (VAEs), opened up new avenues for probabilistic modeling. Modelling of volumetric data has remained a challenge, however, because constraints on available computation and training data make it difficult effectively leverage VAEs, which are well-developed for 2D images. We propose a method to model 3D MR brain volumes distribution by combining a 2D slice VAE with a Gaussian model that captures the relationships between slices. We do so by estimating the sample mean and covariance in the latent space of the 2D model over the slice direction. This combined model lets us sample new coherent stacks of latent variables to decode into slices of a volume. We also introduce a novel evaluation method for generated volumes that quantifies how well their segmentations match those of true brain anatomy. We demonstrate that our proposed model is competitive in generating high quality volumes at high resolutions according to both traditional metrics and our proposed evaluation.      
### 5.Self-supervised edge features for improved Graph Neural Network training  [ :arrow_down: ](https://arxiv.org/pdf/2007.04777.pdf)
>  Graph Neural Networks (GNN) have been extensively used to extract meaningful representations from graph structured data and to perform predictive tasks such as node classification and link prediction. In recent years, there has been a lot of work incorporating edge features along with node features for prediction tasks. One of the main difficulties in using edge features is that they are often handcrafted, hard to get, specific to a particular domain, and may contain redundant information. In this work, we present a framework for creating new edge features, applicable to any domain, via a combination of self-supervised and unsupervised learning. In addition to this, we use Forman-Ricci curvature as an additional edge feature to encapsulate the local geometry of the graph. We then encode our edge features via a Set Transformer and combine them with node features extracted from popular GNN architectures for node classification in an end-to-end training scheme. We validate our work on three biological datasets comprising of single-cell RNA sequencing data of neurological disease, \textit{in vitro} SARS-CoV-2 infection, and human COVID-19 patients. We demonstrate that our method achieves better performance on node classification tasks over baseline Graph Attention Network (GAT) and Graph Convolutional Network (GCN) models. Furthermore, given the attention mechanism on edge and node features, we are able to interpret the cell types and genes that determine the course and severity of COVID-19, contributing to a growing list of potential disease biomarkers and therapeutic targets.      
### 6.Automated Chest CT Image Segmentation of COVID-19 Lung Infection based on 3D U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2007.04774.pdf)
>  The coronavirus disease 2019 (COVID-19) affects billions of lives around the world and has a significant impact on public healthcare. Due to rising skepticism towards the sensitivity of RT-PCR as screening method, medical imaging like computed tomography offers great potential as alternative. For this reason, automated image segmentation is highly desired as clinical decision support for quantitative assessment and disease monitoring. However, publicly available COVID-19 imaging data is limited which leads to overfitting of traditional approaches. To address this problem, we propose an innovative automated segmentation pipeline for COVID-19 infected regions, which is able to handle small datasets by utilization as variant databases. Our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation. For further reduction of the overfitting risk, we implemented a standard 3D U-Net architecture instead of new or computational complex neural network architectures. Through a 5-fold cross-validation on 20 CT scans of COVID-19 patients, we were able to develop a highly accurate as well as robust segmentation model for lungs and COVID-19 infected regions without overfitting on the limited data. Our method achieved Dice similarity coefficients of 0.956 for lungs and 0.761 for infection. We demonstrated that the proposed method outperforms related approaches, advances the state-of-the-art for COVID-19 segmentation and improves medical image analysis with limited data. The code and model are available under the following link: <a class="link-external link-https" href="https://github.com/frankkramer-lab/covid19.MIScnn" rel="external noopener nofollow">this https URL</a>      
### 7.Low Dose CT Denoising via Joint Bilateral Filtering and Intelligent Parameter Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.04768.pdf)
>  Denoising of clinical CT images is an active area for deep learning research. Current clinically approved methods use iterative reconstruction methods to reduce the noise in CT images. Iterative reconstruction techniques require multiple forward and backward projections, which are time-consuming and computationally expensive. Recently, deep learning methods have been successfully used to denoise CT images. However, conventional deep learning methods suffer from the 'black box' problem. They have low accountability, which is necessary for use in clinical imaging situations. In this paper, we use a Joint Bilateral Filter (JBF) to denoise our CT images. The guidance image of the JBF is estimated using a deep residual convolutional neural network (CNN). The range smoothing and spatial smoothing parameters of the JBF are tuned by a deep reinforcement learning task. Our actor first chooses a parameter, and subsequently chooses an action to tune the value of the parameter. A reward network is designed to direct the reinforcement learning task. Our denoising method demonstrates good denoising performance, while retaining structural information. Our method significantly outperforms state of the art deep neural networks. Moreover, our method has only two parameters, which makes it significantly more interpretable and reduces the 'black box' problem. We experimentally measure the impact of our intelligent parameter optimization and our reward network. Our studies show that our current setup yields the best results in terms of structural preservation.      
### 8.JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2007.04754.pdf)
>  Deep neural networks have shown great success in low dose CT denoising. However, most of these deep neural networks have several hundred thousand trainable parameters. This, combined with the inherent non-linearity of the neural network, makes the deep neural network diffcult to understand with low accountability. In this study we introduce JBFnet, a neural network for low dose CT denoising. The architecture of JBFnet implements iterative bilateral filtering. The filter functions of the Joint Bilateral Filter (JBF) are learned via shallow convolutional networks. The guidance image is estimated by a deep neural network. JBFnet is split into four filtering blocks, each of which performs Joint Bilateral Filtering. Each JBF block consists of 112 trainable parameters, making the noise removal process comprehendable. The Noise Map (NM) is added after filtering to preserve high level features. We train JBFnet with the data from the body scans of 10 patients, and test it on the AAPM low dose CT Grand Challenge dataset. We compare JBFnet with state-of-the-art deep learning networks. JBFnet outperforms CPCE3D, GAN and deep GFnet on the test dataset in terms of noise removal while preserving structures. We conduct several ablation studies to test the performance of our network architecture and training method. Our current setup achieves the best performance, while still maintaining behavioural accountability.      
### 9.Brain Tumor Anomaly Detection via Latent Regularized Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.04734.pdf)
>  With the development of medical imaging technology, medical images have become an important basis for doctors to diagnose patients. The brain structure in the collected data is complicated, thence, doctors are required to spend plentiful energy when diagnosing brain abnormalities. Aiming at the imbalance of brain tumor data and the rare amount of labeled data, we propose an innovative brain tumor abnormality detection algorithm. The semi-supervised anomaly detection model is proposed in which only healthy (normal) brain images are trained. Model capture the common pattern of the normal images in the training process and detect anomalies based on the reconstruction error of latent space. Furthermore, the method first uses singular value to constrain the latent space and jointly optimizes the image space through multiple loss functions, which make normal samples and abnormal samples more separable in the feature-level. This paper utilizes BraTS, HCP, MNIST, and CIFAR-10 datasets to comprehensively evaluate the effectiveness and practicability. Extensive experiments on intra- and cross-dataset tests prove that our semi-supervised method achieves outperforms or comparable results to state-of-the-art supervised techniques.      
### 10.Graph Signal Processing: Vertex Multiplication  [ :arrow_down: ](https://arxiv.org/pdf/2007.04723.pdf)
>  On the Euclidean domains of classical signal processing, linking of signal samples to the underlying coordinate structure is straightforward. While graph adjacency matrices totally define the quantitative associations among the underlying graph vertices, a major problem in graph signal processing is the lack of explicit association of vertices with an underlying quantitative coordinate structure. To make this link, we propose an operation, called the vertex multiplication, which is defined for graphs and can operate on graph signals. Vertex multiplication, which generalizes the coordinate multiplication operation in time series signals, can be interpreted as an operator which assigns a coordinate structure to a graph. By using the graph domain extension of differentiation and graph Fourier transform (GFT), vertex multiplication is defined such that it shows Fourier duality, which states that differentiation and coordinate multiplication operations are duals of each other under Fourier transformation (FT). The proposed definition is shown to reduce to coordinate multiplication for graphs corresponding to time series. Numerical examples are also presented.      
### 11.End-to-end Learned Image Compression with Fixed Point Weight Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2007.04684.pdf)
>  Learned image compression (LIC) has reached the traditional hand-crafted methods such as JPEG2000 and BPG in terms of the coding gain. However, the large model size of the network prohibits the usage of LIC on resource-limited embedded systems. This paper presents a LIC with 8-bit fixed-point weights. First, we quantize the weights in groups and propose a non-linear memory-free codebook. Second, we explore the optimal grouping and quantization scheme. Finally, we develop a novel weight clipping fine tuning scheme. Experimental results illustrate that the coding loss caused by the quantization is small, while around 75% model size can be reduced compared with the 32-bit floating-point anchor. As far as we know, this is the first work to explore and evaluate the LIC fully with fixed-point weights, and our proposed quantized LIC is able to outperform BPG in terms of MS-SSIM.      
### 12.Capturing scattered discriminative information using a deep architecture in acoustic scene classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.04631.pdf)
>  Frequently misclassified pairs of classes that share many common acoustic properties exist in acoustic scene classification (ASC). To distinguish such pairs of classes, trivial details scattered throughout the data could be vital clues. However, these details are less noticeable and are easily removed using conventional non-linear activations (e.g. ReLU). Furthermore, making design choices to emphasize trivial details can easily lead to overfitting if the system is not sufficiently generalized. In this study, based on the analysis of the ASC task's characteristics, we investigate various methods to capture discriminative information and simultaneously mitigate the overfitting problem. We adopt a max feature map method to replace conventional non-linear activations in a deep neural network, and therefore, we apply an element-wise comparison between different filters of a convolution layer's output. Two data augment methods and two deep architecture modules are further explored to reduce overfitting and sustain the system's discriminative power. Various experiments are conducted using the detection and classification of acoustic scenes and events 2020 task1-a dataset to validate the proposed methods. Our results show that the proposed system consistently outperforms the baseline, where the single best performing system has an accuracy of 70.4% compared to 65.1% of the baseline.      
### 13.DeepSinger: Singing Voice Synthesis with Data Mined From the Web  [ :arrow_down: ](https://arxiv.org/pdf/2007.04590.pdf)
>  In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages (Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness (footnote: Our audio samples are shown in <a class="link-external link-https" href="https://speechresearch.github.io/deepsinger/" rel="external noopener nofollow">this https URL</a>.)      
### 14.Learning Hidden Markov Models for Linear Gaussian Systems with Applications to Event-based State Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2007.04587.pdf)
>  This work attempts to approximate a linear Gaussian system with a finite-state hidden Markov model (HMM), which is found useful in solving sophisticated event-based state estimation problems. An indirect modeling approach is developed, wherein a state space model (SSM) is firstly identified for a Gaussian system and the SSM is then used as an emulator for learning an HMM. In the proposed method, the training data for the HMM are obtained from the data generated by the SSM through building a quantization mapping. Parameter learning algorithms are designed to learn the parameters of the HMM, through exploiting the periodical structural characteristics of the HMM. The convergence and asymptotic properties of the proposed algorithms are analyzed. The HMM learned using the proposed algorithms is applied to event-triggered state estimation, and numerical results on model learning and state estimation demonstrate the validity of the proposed algorithms.      
### 15.Neural Video Coding using Multiscale Motion Compensation and Spatiotemporal Context Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.04574.pdf)
>  Over the past two decades, traditional block-based video coding has made remarkable progress and spawned a series of well-known standards such as MPEG-4, H.264/AVC and H.265/HEVC. On the other hand, deep neural networks (DNNs) have shown their powerful capacity for visual content understanding, feature extraction and compact representation. Some previous works have explored the learnt video coding algorithms in an end-to-end manner, which show the great potential compared with traditional methods. In this paper, we propose an end-to-end deep neural video coding framework (NVC), which uses variational autoencoders (VAEs) with joint spatial and temporal prior aggregation (PA) to exploit the correlations in intra-frame pixels, inter-frame motions and inter-frame compensation residuals, respectively. Novel features of NVC include: 1) To estimate and compensate motion over a large range of magnitudes, we propose an unsupervised multiscale motion compensation network (MS-MCN) together with a pyramid decoder in the VAE for coding motion features that generates multiscale flow fields, 2) we design a novel adaptive spatiotemporal context model for efficient entropy coding for motion information, 3) we adopt nonlocal attention modules (NLAM) at the bottlenecks of the VAEs for implicit adaptive feature extraction and activation, leveraging its high transformation capacity and unequal weighting with joint global and local information, and 4) we introduce multi-module optimization and a multi-frame training strategy to minimize the temporal error propagation among P-frames. NVC is evaluated for the low-delay causal settings and compared with H.265/HEVC, H.264/AVC and the other learnt video compression methods following the common test conditions, demonstrating consistent gains across all popular test sequences for both PSNR and MS-SSIM distortion metrics.      
### 16.Efficient detection of adversarial images  [ :arrow_down: ](https://arxiv.org/pdf/2007.04564.pdf)
>  In this paper, detection of deception attack on deep neural network (DNN) based image classification in autonomous and cyber-physical systems is considered. Several studies have shown the vulnerability of DNN to malicious deception attacks. In such attacks, some or all pixel values of an image are modified by an external attacker, so that the change is almost invisible to the human eye but significant enough for a DNN-based classifier to misclassify it. This paper first proposes a novel pre-processing technique that facilitates the detection of such modified images under any DNN-based image classifier as well as the attacker model. The proposed pre-processing algorithm involves a certain combination of principal component analysis (PCA)-based decomposition of the image, and random perturbation based detection to reduce computational complexity. Next, an adaptive version of this algorithm is proposed where a random number of perturbations are chosen adaptively using a doubly-threshold policy, and the threshold values are learnt via stochastic approximation in order to minimize the expected number of perturbations subject to constraints on the false alarm and missed detection probabilities. Numerical experiments show that the proposed detection scheme outperforms a competing algorithm while achieving reasonably low computational complexity.      
### 17.Distributed Energy Trading and Scheduling among Microgrids via Multiagent Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.04517.pdf)
>  The development of renewable energy generation empowers microgrids to generate electricity to supply itself and to trade the surplus on energy markets. To minimize the overall cost, a microgrid must determine how to schedule its energy resources and electrical loads and how to trade with others. The control decisions are influenced by various factors, such as energy storage, renewable energy yield, electrical load, and competition from other microgrids. Making the optimal control decision is challenging, due to the complexity of the interconnected microgrids, the uncertainty of renewable energy generation and consumption, and the interplay among microgrids. The previous works mainly adopted the modeling-based approaches for deriving the control decision, yet they relied on the precise information of future system dynamics, which can be hard to obtain in a complex environment. This work provides a new perspective of obtaining the optimal control policy for distributed energy trading and scheduling by directly interacting with the environment, and proposes a multiagent deep reinforcement learning approach for learning the optimal control policy. Each microgrid is modeled as an agent, and different agents learn collaboratively for maximizing their rewards. The agent of each microgrid can make the local scheduling decision without knowing others' information, which can well maintain the autonomy of each microgrid. We evaluate the performances of our proposed method using real-world datasets. The experimental results show that our method can significantly reduce the cost of the microgrids compared with the baseline methods.      
### 18.Automatic Probe Movement Guidance for Freehand Obstetric Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2007.04480.pdf)
>  We present the first system that provides real-time probe movement guidance for acquiring standard planes in routine freehand obstetric ultrasound scanning. Such a system can contribute to the worldwide deployment of obstetric ultrasound scanning by lowering the required level of operator expertise. The system employs an artificial neural network that receives the ultrasound video signal and the motion signal of an inertial measurement unit (IMU) that is attached to the probe, and predicts a guidance signal. The network termed US-GuideNet predicts either the movement towards the standard plane position (goal prediction), or the next movement that an expert sonographer would perform (action prediction). While existing models for other ultrasound applications are trained with simulations or phantoms, we train our model with real-world ultrasound video and probe motion data from 464 routine clinical scans by 17 accredited sonographers. Evaluations for 3 standard plane types show that the model provides a useful guidance signal with an accuracy of 88.8% for goal prediction and 90.9% for action prediction.      
### 19.An Efficient Data Imputation Technique for Human Activity Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.04456.pdf)
>  The tremendous applications of human activity recognition are surging its span from health monitoring systems to virtual reality applications. Thus, the automatic recognition of daily life activities has become significant for numerous applications. In recent years, many datasets have been proposed to train the machine learning models for efficient monitoring and recognition of human daily living activities. However, the performance of machine learning models in activity recognition is crucially affected when there are incomplete activities in a dataset, i.e., having missing samples in dataset captures. Therefore, in this work, we propose a methodology for extrapolating the missing samples of a dataset to better recognize the human daily living activities. The proposed method efficiently pre-processes the data captures and utilizes the k-Nearest Neighbors (KNN) imputation technique to extrapolate the missing samples in dataset captures. The proposed methodology elegantly extrapolated a similar pattern of activities as they were in the real dataset.      
### 20.Herding an Adversarial Swarm in Three-dimensional Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2007.04406.pdf)
>  This paper presents a defense approach to safeguard a protected area against an attack by a swarm of adversarial agents in three-dimensional (3D) space. We extend our 2D `StringNet Herding' approach, in which a closed formation of string-barriers is established around the adversarial swarm to confine their motion and herd them to a safe area, to 3D spaces by introducing 3D-StringNet. 3D-StringNet is a closed 3D formation of triangular net-like barriers. We provide a systematic approach to generate three types of 3D formations that are used in the 3D herding process and modifications to the finite-time convergent control laws developed in \cite{chipade2020swarmherding} that are required for a 3D environment. Furthermore, for given initial positions of the defenders, we provide conditions on the initial positions of the attackers for which the defenders are guaranteed to gather as a specified formation at a position on the shortest path of the attackers to the protected area before attackers reach there.      
### 21.Achievable Rates of Opportunistic Cognitive Radio Systems Using Reconfigurable Antennas with Imperfect Sensing and Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2007.04390.pdf)
>  We consider an opportunistic cognitive radio (CR) system in which secondary transmitter (SUtx) is equipped with a reconfigurable antenna (RA). Utilizing the beam steering capability of the RA, we regard a design framework for integrated sector-based spectrum sensing and data communication. In this framework, SUtx senses the spectrum and detects the beam corresponding to active primary user's (PU) location. SUtx also sends training symbols (prior to data symbols), to enable channel estimation at secondary receiver (SUrx) and selection of the strongest beam between SUtx-SUrx for data transmission. We establish a lower bound on the achievable rates of SUtx-SUrx link, in the presence of spectrum sensing and channel estimation errors, and errors due to incorrect detection of the beam corresponding to PU's location and incorrect selection of the strongest beam for data transmission. We formulate a novel constrained optimization problem, aiming at maximizing the derived achievable rate lower bound subject to average transmit and interference power constraints. We optimize the durations of spatial spectrum sensing and channel training as well as data symbol transmission power. Our numerical results demonstrate that between optimizing spectrum sensing and channel training durations, the latter is more important for providing higher achievable rates.      
### 22.Modular Medium Voltage AC to Low Voltage DC Converter for Extreme Fast Charging Applications  [ :arrow_down: ](https://arxiv.org/pdf/2007.04369.pdf)
>  A modular and scalable converter for medium voltage (MV) AC to low voltage (LV) DC power conversion is proposed; single-phase-modules (SPMs), each consisting of an active-front-end (AFE) stage and an isolated DC-DC stage, are connected in input-series-output-parallel (ISOP) configuration to reach desired voltage and power capacity. In prior art, high-speed bidirectional communication among modules and a centralized controller is required to ensure module-level voltage and power balancing, which severely limits the scalability and practical realization of higher voltage and higher power systems. Moreover, large capacitors are used to suppress double-line-frequency voltage variations on the common MV DC bus shared by the AFE and the DC-DC stage originating from AC power pulsations through the SPMs. We propose a comprehensive controller which achieves voltage and power balancing using complete decentralized control of the DC-DC stages based on only local sensor feedback and the AFE stages are controlled using feedback of only the LV DC output. Furthermore, reduced capacitor requirement on the MV DC bus is achieved through design and control. The proposed method is validated through simulation and experimental results.      
### 23.Journey Towards Tiny Perceptual Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.04356.pdf)
>  Recent works in single-image perceptual super-resolution (SR) have demonstrated unprecedented performance in generating realistic textures by means of deep convolutional networks. However, these convolutional models are excessively large and expensive, hindering their effective deployment to end devices. In this work, we propose a neural architecture search (NAS) approach that integrates NAS and generative adversarial networks (GANs) with recent advances in perceptual SR and pushes the efficiency of small perceptual SR models to facilitate on-device execution. Specifically, we search over the architectures of both the generator and the discriminator sequentially, highlighting the unique challenges and key observations of searching for an SR-optimized discriminator and comparing them with existing discriminator architectures in the literature. Our tiny perceptual SR (TPSR) models outperform SRGAN and EnhanceNet on both full-reference perceptual metric (LPIPS) and distortion metric (PSNR) while being up to 26.4$\times$ more memory efficient and 33.6$\times$ more compute efficient respectively.      
### 24.Lightweight Image Super-Resolution with Enhanced CNN  [ :arrow_down: ](https://arxiv.org/pdf/2007.04344.pdf)
>  Deep convolutional neural networks (CNNs) with strong expressive ability have achieved impressive performances on single image super-resolution (SISR). However, their excessive amounts of convolutions and parameters usually consume high computational cost and more memory storage for training a SR model, which limits their applications to SR with resource-constrained devices in real world. To resolve these problems, we propose a lightweight enhanced SR CNN (LESRCN-N) with three successive sub-blocks, an information extraction and enhancement block (IEEB), a reconstruction block (RB) and an information refinement block (IRB). Specifically, the IEEB extracts hierarchical low-resolution (LR) features and aggregates the obtained features step-by-step to increase the memory ability of the shallow layers on deep layers for SISR. To remove redundant information obtained, a heterogeneous architecture is adopted in the IEEB. After that, the RB converts low-frequency features into high-frequency features by fusing global and local features, which is complementary with the IEEB in tackling the long-term dependency problem. Finally, the IRB uses coarse high-frequency features from the RB to learn more accurate SR features and construct a SR image. The proposed LESRCNN can obtain a high-quality image by a model for different scales. Extensive experiments demonstrate that the proposed LESRCNN outperforms state-of-the-arts on SISR in terms of qualitative and quantitative evaluation.      
### 25.Prostate motion modelling using biomechanically-trained deep neural networks on unstructured nodes  [ :arrow_down: ](https://arxiv.org/pdf/2007.04972.pdf)
>  In this paper, we propose to train deep neural networks with biomechanical simulations, to predict the prostate motion encountered during ultrasound-guided interventions. In this application, unstructured points are sampled from segmented pre-operative MR images to represent the anatomical regions of interest. The point sets are then assigned with point-specific material properties and displacement loads, forming the un-ordered input feature vectors. An adapted PointNet can be trained to predict the nodal displacements, using finite element (FE) simulations as ground-truth data. Furthermore, a versatile bootstrap aggregating mechanism is validated to accommodate the variable number of feature vectors due to different patient geometries, comprised of a training-time bootstrap sampling and a model averaging inference. This results in a fast and accurate approximation to the FE solutions without requiring subject-specific solid meshing. Based on 160,000 nonlinear FE simulations on clinical imaging data from 320 patients, we demonstrate that the trained networks generalise to unstructured point sets sampled directly from holdout patient segmentation, yielding a near real-time inference and an expected error of 0.017 mm in predicted nodal displacement.      
### 26.Subject-Aware Contrastive Learning for Biosignals  [ :arrow_down: ](https://arxiv.org/pdf/2007.04871.pdf)
>  Datasets for biosignals, such as electroencephalogram (EEG) and electrocardiogram (ECG), often have noisy labels and have limited number of subjects (&lt;100). To handle these challenges, we propose a self-supervised approach based on contrastive learning to model biosignals with a reduced reliance on labeled data and with fewer subjects. In this regime of limited labels and subjects, intersubject variability negatively impacts model performance. Thus, we introduce subject-aware learning through (1) a subject-specific contrastive loss, and (2) an adversarial training to promote subject-invariance during the self-supervised learning. We also develop a number of time-series data augmentation techniques to be used with the contrastive loss for biosignals. Our method is evaluated on publicly available datasets of two different biosignals with different tasks: EEG decoding and ECG anomaly detection. The embeddings learned using self-supervision yield competitive classification results compared to entirely supervised methods. We show that subject-invariance improves representation quality for these tasks, and observe that subject-specific loss increases performance when fine-tuning with supervised labels.      
### 27.A Deep Joint Sparse Non-negative Matrix Factorization Framework for Identifying the Common and Subject-specific Functional Units of Tongue Motion During Speech  [ :arrow_down: ](https://arxiv.org/pdf/2007.04865.pdf)
>  Intelligible speech is produced by creating varying internal local muscle groupings---i.e., functional units---that are generated in a systematic and coordinated manner. There are two major challenges in characterizing and analyzing functional units. First, due to the complex and convoluted nature of tongue structure and function, it is of great importance to develop a method that can accurately decode complex muscle coordination patterns during speech. Second, it is challenging to keep identified functional units across subjects comparable due to their substantial variability. In this work, to address these challenges, we develop a new deep learning framework to identify common and subject-specific functional units of tongue motion during speech. Our framework hinges on joint deep graph-regularized sparse non-negative matrix factorization (NMF) using motion quantities derived from displacements by tagged Magnetic Resonance Imaging. More specifically, we transform NMF with sparse and manifold regularizations into modular architectures akin to deep neural networks by means of unfolding the Iterative Shrinkage-Thresholding Algorithm to learn interpretable building blocks and associated weighting map. We then apply spectral clustering to common and subject-specific functional units. Experiments carried out with simulated datasets show that the proposed method surpasses the comparison methods. Experiments carried out with in vivo tongue motion datasets show that the proposed method can determine the common and subject-specific functional units with increased interpretability and decreased size variability.      
### 28.Identifying efficient controls of complex interaction networks using genetic algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2007.04853.pdf)
>  Control theory has seen recently impactful applications in network science, especially in connections with applications in network medicine. A key topic of research is that of finding minimal external interventions that offer control over the dynamics of a given network, a problem known as network controllability. We propose in this article a new solution for this problem based on genetic algorithms. We tailor our solution for applications in computational drug repurposing, seeking to maximise its use of FDA-approved drug targets in a given disease-specific protein-protein interaction network. We show how our algorithm identifies a number of potentially efficient drugs for breast, ovarian, and pancreatic cancer. We demonstrate our algorithm on several benchmark networks from cancer medicine, social networks, electronic circuits, and several random networks with their edges distributed according to the Erdős-Rényi, the small-world, and the scale-free properties. Overall, we show that our new algorithm is more efficient in identifying relevant drug targets in a disease network, advancing the computational solutions needed for new therapeutic and drug repurposing approaches.      
### 29.RWCP-SSD-Onomatopoeia: Onomatopoeic Word Dataset for Environmental Sound Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2007.04719.pdf)
>  Environmental sound synthesis is a technique for generating a natural environmental sound. Conventional work on environmental sound synthesis using sound event labels cannot finely control synthesized sounds, for example, the pitch and timbre. We consider that onomatopoeic words can be used for environmental sound synthesis. Onomatopoeic words are effective for explaining the feature of sounds. We believe that using onomatopoeic words will enable us to control the fine time-frequency structure of synthesized sounds. However, there is no dataset available for environmental sound synthesis using onomatopoeic words. In this paper, we thus present RWCP-SSD-Onomatopoeia, a dataset consisting of 155,568 onomatopoeic words paired with audio samples for environmental sound synthesis. We also collected self-reported confidence scores and others-reported acceptance scores of onomatopoeic words, to help us investigate the difficulty in the transcription and selection of a suitable word for environmental sound synthesis.      
### 30.Green Lighting ML: Confidentiality, Integrity, and Availability of Machine Learning Systems in Deployment  [ :arrow_down: ](https://arxiv.org/pdf/2007.04693.pdf)
>  Security and ethics are both core to ensuring that a machine learning system can be trusted. In production machine learning, there is generally a hand-off from those who build a model to those who deploy a model. In this hand-off, the engineers responsible for model deployment are often not privy to the details of the model and thus, the potential vulnerabilities associated with its usage, exposure, or compromise. Techniques such as model theft, model inversion, or model misuse may not be considered in model deployment, and so it is incumbent upon data scientists and machine learning engineers to understand these potential risks so they can communicate them to the engineers deploying and hosting their models. This is an open problem in the machine learning community and in order to help alleviate this issue, automated systems for validating privacy and security of models need to be developed, which will help to lower the burden of implementing these hand-offs and increasing the ubiquity of their adoption.      
### 31.Multi-task Regularization Based on Infrequent Classes for Audio Captioning  [ :arrow_down: ](https://arxiv.org/pdf/2007.04660.pdf)
>  Audio captioning is a multi-modal task, focusing on using natural language for describing the contents of general audio. Most audio captioning methods are based on deep neural networks, employing an encoder-decoder scheme and a dataset with audio clips and corresponding natural language descriptions (i.e. captions). A significant challenge for audio captioning is the distribution of words in the captions: some words are very frequent but acoustically non-informative, i.e. the function words (e.g. "a", "the"), and other words are infrequent but informative, i.e. the content words (e.g. adjectives, nouns). In this paper we propose two methods to mitigate this class imbalance problem. First, in an autoencoder setting for audio captioning, we weigh each word's contribution to the training loss inversely proportional to its number of occurrences in the whole dataset. Secondly, in addition to multi-class, word-level audio captioning task, we define a multi-label side task based on clip-level content word detection by training a separate decoder. We use the loss from the second task to regularize the jointly trained encoder for the audio captioning task. We evaluate our method using Clotho, a recently published, wide-scale audio captioning dataset, and our results show an increase of 37\% relative improvement with SPIDEr metric over the baseline method.      
### 32.Attention Neural Network for Trash Detection on Water Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.04639.pdf)
>  Rivers and canals flowing through cities are often used illegally for dumping the trash. This contaminates freshwater channels as well as causes blockage in sewerage resulting in urban flooding. When this contaminated water reaches agricultural fields, it results in degradation of soil and poses critical environmental as well as economic threats. The dumped trash is often found floating on the water surface. The trash could be disfigured, partially submerged, decomposed into smaller pieces, clumped together with other objects which obscure its shape and creates a challenging detection problem. This paper proposes a method for the detection of visible trash floating on the water surface of the canals in urban areas. We also provide a large dataset, first of its kind, trash in water channels that contains object-level annotations. A novel attention layer is proposed that improves the detection of smaller objects. Towards the end of this paper, we provide a detailed comparison of our method with state-of-the-art object detectors and show that our method significantly improves the detection of smaller objects. The dataset will be made publicly available.      
### 33.Ultra-sensitive Parity-Time Symmetry based Graphene FET (PTS-GFET) Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2007.04567.pdf)
>  A novel ultra-sensitive Parity-Time symmetry based Graphene FET (PTS-GFET) sensor is studied for gas concentration detection. The PTS-GFET sensor effectively integrates the sensitivity of the PT symmetry around its Exceptional Point (EP) and the tunability of the GFET conductance. The change of GFET conductance with the gas concentration can be brought back to the EP of the PTS-GFET by tuning the gate voltage on the GFET. Thus, the applied gate voltage indicates the gas concentration. The minimum detectable gas concentration has been derived and estimated based on the experimental data, which shows that PTS-GFET can detect gas concentration below 50 ppb.      
### 34.Attention-based Residual Speech Portrait Model for Speech to Face Generation  [ :arrow_down: ](https://arxiv.org/pdf/2007.04536.pdf)
>  Given a speaker's speech, it is interesting to see if it is possible to generate this speaker's face. One main challenge in this task is to alleviate the natural mismatch between face and speech. To this end, in this paper, we propose a novel Attention-based Residual Speech Portrait Model (AR-SPM) by introducing the ideal of the residual into a hybrid encoder-decoder architecture, where face prior features are merged with the output of speech encoder to form the final face feature. In particular, we innovatively establish a tri-item loss function, which is a weighted linear combination of the L2-norm, L1-norm and negative cosine loss, to train our model by comparing the final face feature and true face feature. Evaluation on AVSpeech dataset shows that our proposed model accelerates the convergence of training, outperforms the state-of-the-art in terms of quality of the generated face, and achieves superior recognition accuracy of gender and age compared with the ground truth.      
### 35.Symbolic Reachability Analysis of High Dimensional Max-Plus Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.04510.pdf)
>  This work discusses the reachability analysis (RA) of Max-Plus Linear (MPL) systems, a class of continuous-space, discrete-event models defined over the max-plus algebra. Given the initial and target sets, we develop algorithms to verify whether there exist trajectories of the MPL system that, starting from the initial set, eventually reach the target set. We show that RA can be solved symbolically by encoding the MPL system, as well as initial and target sets into difference logic, and then checking the satisfaction of the resulting logical formula via an off-the-shelf satisfiability modulo theories (SMT) solver. The performance and scalability of the developed SMT-based algorithms are shown to clearly outperform state-of-the-art RA algorithms for MPL systems, newly allowing to investigate RA of high-dimensional MPL systems: the verification of models with more than 100 continuous variables shows the applicability of these techniques to MPL systems of industrial relevance.      
### 36.Searching for Efficient Architecture for Instrument Segmentation in Robotic Surgery  [ :arrow_down: ](https://arxiv.org/pdf/2007.04449.pdf)
>  Segmentation of surgical instruments is an important problem in robot-assisted surgery: it is a crucial step towards full instrument pose estimation and is directly used for masking of augmented reality overlays during surgical procedures. Most applications rely on accurate real-time segmentation of high-resolution surgical images. While previous research focused primarily on methods that deliver high accuracy segmentation masks, majority of them can not be used for real-time applications due to their computational cost. In this work, we design a light-weight and highly-efficient deep residual architecture which is tuned to perform real-time inference of high-resolution images. To account for reduced accuracy of the discovered light-weight deep residual network and avoid adding any additional computational burden, we perform a differentiable search over dilation rates for residual units of our network. We test our discovered architecture on the EndoVis 2017 Robotic Instruments dataset and verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff with a speed of up to 125 FPS on high resolution images.      
### 37.SmaAt-UNet: Precipitation Nowcasting using a Small Attention-UNet Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2007.04417.pdf)
>  Weather forecasting is dominated by numerical weather prediction that tries to model accurately the physical properties of the atmosphere. A downside of numerical weather prediction is that it is lacking the ability for short-term forecasts using the latest available information. By using a data-driven neural network approach we show that it is possible to produce an accurate precipitation nowcast. To this end, we propose \textit{SmaAt-UNet}, an efficient convolutional neural networks based on the well known UNet architecture equipped with attention modules and depthwise-separable convolutions. We evaluate our approach on a real-life dataset using precipitation maps from the region of the Netherlands. The experimental results show that in terms of accuracy the proposed model is comparable to other examined models while only using a quarter of the trainable parameters.      
### 38.Multi-Swarm Herding: Protecting against Adversarial Swarms  [ :arrow_down: ](https://arxiv.org/pdf/2007.04407.pdf)
>  This paper studies a defense approach against one or more swarms of adversarial agents. In our earlier work, we employ a closed formation (`StringNet') of defending agents (defenders) around a swarm of adversarial agents (attackers) to confine their motion within given bounds, and guide them to a safe area. The control design relies on the assumption that the adversarial agents remain close enough to each other, i.e., within a prescribed connectivity region. To handle situations when the attackers no longer stay within such a connectivity region, but rather split into smaller swarms (clusters) to maximize the chance or impact of attack, this paper proposes an approach to learn the attacking sub-swarms and reassign defenders towards the attackers. We use a `Density-based Spatial Clustering of Application with Noise (DBSCAN)' algorithm to identify the spatially distributed swarms of the attackers. Then, the defenders are assigned to each identified swarm of attackers by solving a constrained generalized assignment problem. Simulations are provided to demonstrate the effectiveness of the approach.      
### 39.Words as Art Materials: Generating Paintings with Sequential GANs  [ :arrow_down: ](https://arxiv.org/pdf/2007.04383.pdf)
>  Converting text descriptions into images using Generative Adversarial Networks has become a popular research area. Visually appealing images have been generated successfully in recent years. Inspired by these studies, we investigated the generation of artistic images on a large variance dataset. This dataset includes images with variations, for example, in shape, color, and content. These variations in images provide originality which is an important factor for artistic essence. One major characteristic of our work is that we used keywords as image descriptions, instead of sentences. As the network architecture, we proposed a sequential Generative Adversarial Network model. The first stage of this sequential model processes the word vectors and creates a base image whereas the next stages focus on creating high-resolution artistic-style images without working on word vectors. To deal with the unstable nature of GANs, we proposed a mixture of techniques like Wasserstein loss, spectral normalization, and minibatch discrimination. Ultimately, we were able to generate painting images, which have a variety of styles. We evaluated our results by using the Fréchet Inception Distance score and conducted a user study with 186 participants.      
### 40.Information, communication and music: Recognition of musical dissonance and consonance in a simple reservoir computing system  [ :arrow_down: ](https://arxiv.org/pdf/2007.04360.pdf)
>  Reservoir computing is an emerging, but very successful approach towards processing and classification of various signals. It can be described as a model of a transient computation, where influence of input changes internal dynamics of chosen computational reservoir. Trajectory of these changes represents computation performed by the system. The selection of a suitable computational substrate capable of non-linear response and rich internal dynamics ensures the implementation of simple readout protocols. Signal evolution based on the rich dynamics of the reservoir layer helps to emphasize differences between given signals thus enabling their easier classification. Here we present a simple reservoir computing system (single node echo-state machine) implemented on Multisim platform as a tool for classification of musical intervals according to their consonant or dissonant character. The result of this classification closely resembled sensory dissonance curve, with some significant differences. A deeper analysis of the received signals indicates the geometric relationships between the consonant and dissonant intervals, enabling their classification.      
### 41.Deep Placental Vessel Segmentation for Fetoscopic Mosaicking  [ :arrow_down: ](https://arxiv.org/pdf/2007.04349.pdf)
>  During fetoscopic laser photocoagulation, a treatment for twin-to-twin transfusion syndrome (TTTS), the clinician first identifies abnormal placental vascular connections and laser ablates them to regulate blood flow in both fetuses. The procedure is challenging due to the mobility of the environment, poor visibility in amniotic fluid, occasional bleeding, and limitations in the fetoscopic field-of-view and image quality. Ideally, anastomotic placental vessels would be automatically identified, segmented and registered to create expanded vessel maps to guide laser ablation, however, such methods have yet to be clinically adopted. We propose a solution utilising the U-Net architecture for performing placental vessel segmentation in fetoscopic videos. The obtained vessel probability maps provide sufficient cues for mosaicking alignment by registering consecutive vessel maps using the direct intensity-based technique. Experiments on 6 different in vivo fetoscopic videos demonstrate that the vessel intensity-based registration outperformed image intensity-based registration approaches showing better robustness in qualitative and quantitative comparison. We additionally reduce drift accumulation to negligible even for sequences with up to 400 frames and we incorporate a scheme for quantifying drift error in the absence of the ground-truth. Our paper provides a benchmark for fetoscopy placental vessel segmentation and registration by contributing the first in vivo vessel segmentation and fetoscopic videos dataset.      
