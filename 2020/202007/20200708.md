# ArXiv eess --Wed, 8 Jul 2020
### 1.Online Topology Inference from Streaming Stationary Graph Signals with Partial Connectivity Information  [ :arrow_down: ](https://arxiv.org/pdf/2007.03653.pdf)
>  We develop online graph learning algorithms from streaming network data. Our goal is to track the (possibly) time-varying network topology, and effect memory and computational savings by processing the data on-the-fly as they are acquired. The setup entails observations modeled as stationary graph signals generated by local diffusion dynamics on the unknown network. Moreover, we may have a priori information on the presence or absence of a few edges as in the link prediction problem. The stationarity assumption implies that the observations' covariance matrix and the so-called graph shift operator (GSO -- a matrix encoding the graph topology) commute under mild requirements. This motivates formulating the topology inference task as an inverse problem, whereby one searches for a sparse GSO that is structurally admissible and approximately commutes with the observations' empirical covariance matrix. For streaming data said covariance can be updated recursively, and we show online proximal gradient iterations can be brought to bear to efficiently track the time-varying solution of the inverse problem with quantifiable guarantees. Specifically, we derive conditions under which the GSO recovery cost is strongly convex and use this property to prove that the online algorithm converges to within a neighborhood of the optimal time-varying batch solution. Numerical tests illustrate the effectiveness of the proposed graph learning approach in adapting to streaming information and tracking changes in the sought dynamic network.      
### 2.Remote Estimation in Decentralized Random Access Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.03652.pdf)
>  Efficient sampling and remote estimation is critical for a plethora of wireless-empowered applications in the Internet of Things and cyber-physical systems. Motivated by such applications, this work proposes decentralized policies for the real-time monitoring and estimation of autoregressive processes over random access channels. Two classes of policies are investigated: (i) oblivious schemes in which sampling and transmission policies are independent of the processes that are monitored, and (ii) non-oblivious schemes in which transmitters causally observe their corresponding processes for decision making. In the class of oblivious policies, we show that minimizing the expected time-average estimation error is equivalent to minimizing the expected age of information. Consequently, we prove lower and upper bounds on the minimum achievable estimation error in this class. Next, we consider non-oblivious policies and design a threshold policy, called error-based thinning, in which each source node becomes active if its instantaneous error has crossed a fixed threshold (which we optimize). Active nodes then transmit stochastically following a slotted ALOHA policy. A closed-form, approximately optimal, solution is found for the threshold as well as the resulting estimation error. It is shown that non-oblivious policies offer a multiplicative gain close to $3$ compared to oblivious policies. Moreover, it is shown that oblivious policies that use the age of information for decision making improve the state-of-the-art at least by the multiplicative factor $2$. The performance of all discussed policies is compared using simulations. Numerical comparison shows that the performance of the proposed decentralized policy is very close to that of centralized greedy scheduling.      
### 3.Segmentation of Pulmonary Opacification in Chest CT Scans of COVID-19 Patients  [ :arrow_down: ](https://arxiv.org/pdf/2007.03643.pdf)
>  The Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has rapidly spread into a global pandemic. A form of pneumonia, presenting as opacities with in a patient's lungs, is the most common presentation associated with this virus, and great attention has gone into how these changes relate to patient morbidity and mortality. In this work we provide open source models for the segmentation of patterns of pulmonary opacification on chest Computed Tomography (CT) scans which have been correlated with various stages and severities of infection. We have collected 663 chest CT scans of COVID-19 patients from healthcare centers around the world, and created pixel wise segmentation labels for nearly 25,000 slices that segment 6 different patterns of pulmonary opacification. We provide open source implementations and pre-trained weights for multiple segmentation models trained on our dataset. Our best model achieves an opacity Intersection-Over-Union score of 0.76 on our test set, demonstrates successful domain adaptation, and predicts the volume of opacification within 1.7\% of expert radiologists. Additionally, we present an analysis of the inter-observer variability inherent to this task, and propose methods for appropriate probabilistic approaches.      
### 4.Monitoring Browsing Behavior of Customers in Retail Stores via RFID Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.03600.pdf)
>  In this paper, we propose to use commercial off-the-shelf (COTS) monostatic RFID devices (i.e. which use a single antenna at a time for both transmitting and receiving RFID signals to and from the tags) to monitor browsing activity of customers in front of display items in places such as retail stores. To this end, we propose TagSee, a multi-person imaging system based on monostatic RFID imaging. TagSee is based on the insight that when customers are browsing the items on a shelf, they stand between the tags deployed along the boundaries of the shelf and the reader, which changes the multi-paths that the RFID signals travel along, and both the RSS and phase values of the RFID signals that the reader receives change. Based on these variations observed by the reader, TagSee constructs a coarse grained image of the customers. Afterwards, TagSee identifies the items that are being browsed by the customers by analyzing the constructed images. The key novelty of this paper is on achieving browsing behavior monitoring of multiple customers in front of display items by constructing coarse grained images via robust, analytical model-driven deep learning based, RFID imaging. To achieve this, we first mathematically formulate the problem of imaging humans using monostatic RFID devices and derive an approximate analytical imaging model that correlates the variations caused by human obstructions in the RFID signals. Based on this model, we then develop a deep learning framework to robustly image customers with high accuracy. We implement TagSee scheme using a Impinj Speedway R420 reader and SMARTRAC DogBone RFID tags. TagSee can achieve a TPR of more than ~90% and a FPR of less than ~10% in multi-person scenarios using training data from just 3-4 users.      
### 5.X-vectors: New Quantitative Biomarkers for Early Parkinson's Disease Detection from Speech  [ :arrow_down: ](https://arxiv.org/pdf/2007.03599.pdf)
>  Many articles have used voice analysis to detect Parkinson's disease (PD), but few have focused on the early stages of the disease and the gender effect. In this article, we have adapted the latest speaker recognition system, called x-vectors, in order to detect an early stage of PD from voice analysis. X-vectors are embeddings extracted from a deep neural network, which provide robust speaker representations and improve speaker recognition when large amounts of training data are used. Our goal was to assess whether, in the context of early PD detection, this technique would outperform the more standard classifier MFCC-GMM (Mel-Frequency Cepstral Coefficients - Gaussian Mixture Model) and, if so, under which conditions. We recorded 221 French speakers (including recently diagnosed PD subjects and healthy controls) with a high-quality microphone and with their own telephone. Men and women were analyzed separately in order to have more precise models and to assess a possible gender effect. Several experimental and methodological aspects were tested in order to analyze their impacts on classification performance. We assessed the impact of audio segment duration, data augmentation, type of dataset used for the neural network training, kind of speech tasks, and back-end analyses. X-vectors technique provided better classification performances than MFCC-GMM for text-independent tasks, and seemed to be particularly suited for the early detection of PD in women (7 to 15% improvement). This result was observed for both recording types (high-quality microphone and telephone).      
### 6.Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment  [ :arrow_down: ](https://arxiv.org/pdf/2007.03593.pdf)
>  Automatic instance segmentation of glomeruli within kidney Whole Slide Imaging (WSI) is essential for clinical research in renal pathology. In computer vision, the end-to-end instance segmentation methods (e.g., Mask-RCNN) have shown their advantages relative to detect-then-segment approaches by performing complementary detection and segmentation tasks simultaneously. As a result, the end-to-end Mask-RCNN approach has been the de facto standard method in recent glomerular segmentation studies, where downsampling and patch-based techniques are used to properly evaluate the high resolution images from WSI (e.g., &gt;10,000x10,000 pixels on 40x). However, in high resolution WSI, a single glomerulus itself can be more than 1,000x1,000 pixels in original resolution which yields significant information loss when the corresponding features maps are downsampled via the Mask-RCNN pipeline. In this paper, we assess if the end-to-end instance segmentation framework is optimal for high-resolution WSI objects by comparing Mask-RCNN with our proposed detect-then-segment framework. Beyond such a comparison, we also comprehensively evaluate the performance of our detect-then-segment pipeline through: 1) two of the most prevalent segmentation backbones (U-Net and DeepLab_v3); 2) six different image resolutions (from 512x512 to 28x28); and 3) two different color spaces (RGB and LAB). Our detect-then-segment pipeline, with the DeepLab_v3 segmentation framework operating on previously detected glomeruli of 512x512 resolution, achieved a 0.953 dice similarity coefficient (DSC), compared with a 0.902 DSC from the end-to-end Mask-RCNN pipeline. Further, we found that neither RGB nor LAB color spaces yield better performance when compared against each other in the context of a detect-then-segment framework. Detect-then-segment pipeline achieved better segmentation performance compared with End-to-end method.      
### 7.A Vision-based Social Distance and Critical Density Detection System for COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2007.03578.pdf)
>  Social distancing has been proven as an effective measure against the spread of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are not used to tracking the required 6-feet (2-meters) distance between themselves and their surroundings. An active surveillance system capable of detecting distances between individuals and warning them can slow down the spread of the deadly disease. Furthermore, measuring social density in a region of interest (ROI) and modulating inflow can decrease social distance violation occurrence chance. On the other hand, recording data and labeling individuals who do not follow the measures will breach individuals' rights in free-societies. Here we propose an Artificial Intelligence (AI) based real-time social distance detection and warning system considering four important ethical factors: (1) the system should never record/cache data, (2) the warnings should not target the individuals, (3) no human supervisor should be in the detection/warning loop, and (4) the code should be open-source and accessible to the public. Against this backdrop, we propose using a monocular camera and deep learning-based real-time object detectors to measure social distancing. If a violation is detected, a non-intrusive audio-visual warning signal is emitted without targeting the individual who breached the social distance measure. Also, if the social density is over a critical value, the system sends a control signal to modulate inflow into the ROI. We tested the proposed method across real-world datasets to measure its generality and performance. The proposed method is ready for deployment, and our code is open-sourced.      
### 8.Crossterm-Free Time-Frequency Representation Exploiting Deep Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.03570.pdf)
>  Bilinear time-frequency representations (TFRs) provide high-resolution time-varying frequency characteristics of nonstationary signals. However, they suffer from crossterms due to the bilinear nature. Existing crossterm-reduced TFRs focus on optimized kernel design which amounts to low-pass weighting or masking in the ambiguity function domain. Optimization of fixed and adaptive kernels are difficult, particularly for complicated signals whose autoterms and crossterms overlap in the ambiguity function. In this letter, we develop a new method to offer high-resolution TFRs of nonstationary signals with crossterms effectively suppressed. The proposed method exploits a deep convolutional neural network which is trained to construct crossterm-free TFRs. The effectiveness of the proposed method is verified by simulation results which clearly show desirable autoterm preservation and crossterm mitigation capabilities. The proposed technique significantly outperforms state-of-the-art time-frequency analysis algorithms based on adaptive kernels and compressive sensing techniques.      
### 9.Light Field Image Super-Resolution Using Deformable Convolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.03535.pdf)
>  Light field (LF) cameras can record scenes from multiple perspectives, and thus introduce beneficial angular information for image super-resolution (SR). However, it is challenging to incorporate angular information due to disparities among LF images. In this paper, we propose a deformable convolution network (i.e., LF-DFnet) to handle the disparity problem for LF image SR. Specifically, we design an angular deformable alignment module (ADAM) for feature-level alignment. Based on ADAM, we further propose a collect-and-distribute approach to perform bidirectional alignment between the center-view feature and each side-view feature. Using our approach, angular information can be well incorporated and encoded into features of each view, which benefits the SR reconstruction of all LF images. Moreover, we develop a baseline-adjustable LF dataset to evaluate SR performance under different disparities. Experiments on both public and our self-developed datasets have demonstrated the superiority of our method. Our LF-DFnet can generate high-resolution images with more faithful details and achieve state-of-the-art reconstruction accuracy. Besides, our LF-DFnet is more robust to disparity variations, which has not been well addressed in literature.      
### 10.Automatic Plane Adjustment of Orthopedic Intraoperative Flat Panel Detector CT-Volumes  [ :arrow_down: ](https://arxiv.org/pdf/2007.03525.pdf)
>  Flat panel computed tomography is used intraoperatively to assess the result of surgery. Due to workflow issues, the acquisition typically cannot be carried out in such a way that the axis aligned multiplanar reconstructions (MPR) of the volume match the anatomically aligned MPRs. This needs to be performed manually, adding additional effort during viewing the datasets. A PoseNet convolutional neural network (CNN) is trained such that parameters of anatomically aligned MPR planes are regressed. Different mathematical approaches to describe plane rotation are compared, as well as a cost function is optimized to incorporate orientation constraints. The CNN is evaluated on two anatomical regions. For one of these regions, one plane is not orthogonal to the other two planes. The plane's normal can be estimated with a median accuracy of 5°, the in-plane rotation with an accuracy of 6°, and the position with an accuracy of 6 mm. Compared to state-of-the-art algorithms the labeling effort for this method is much lower as no segmentation is required. The computation time during inference is less than 0.05 s.      
### 11.STBC-Aided Cooperative NOMA with Timing Offsets, Imperfect Successive Interference Cancellation, and Imperfect Channel State Information  [ :arrow_down: ](https://arxiv.org/pdf/2007.03497.pdf)
>  The combination of non-orthogonal multiple access(NOMA) and cooperative communications can be a suitable solution for fifth generation (5G) and beyond 5G (B5G) wireless systems with massive connectivity, because it can provide higher spectral efficiency, lower energy consumption, and improved fairness compared to the non-cooperative NOMA. However,the receiver complexity in the conventional cooperative NOMA increases with increasing number of users owing to successive interference cancellation (SIC) at each user. Space time block code-aided cooperative NOMA (STBC-CNOMA) offers less numbers of SIC as compared to that of conventional cooperative NOMA. In this paper, we evaluate the performance of STBC-CNOMA under practical challenges such as imperfect SIC, imperfect timing synchronization between distributed cooperating users, and imperfect channel state information (CSI). We derive closed-form expressions of the received signals in the presence of such realistic impairments and then use them to evaluate outage probability. Further, we provide intuitive insights into the impact of each impairment on the outage performance through asymptotic analysis at high transmit signal-to-noise ratio. We also compare the complexity of STBC-CNOMA with existing cooperative NOMA protocols for a given number of users. In addition, through analysis and simulation, we observe that the impact of the imperfect SIC on the outage performance of STBC-CNOMA is more significant compared to the other two imperfections. Therefore, considering the smaller number of SIC in STBC-CNOMA compared to the other cooperative NOMA protocols, STBC-CNOMA is an effective solution to achieve high reliability for the same SIC imperfection condition.      
### 12.Directional Modulation-Enabled Secure Transmission with Intelligent Reflecting Surface  [ :arrow_down: ](https://arxiv.org/pdf/2007.03482.pdf)
>  We propose a new secure transmission scheme in this paper which uses directional modulation (DM) with artificial noise and is aided by the intelligent reflecting surface (IRS). Specifically, the direct path and IRS-enabled reflect path carry the same confidential signal and thus can be coherently added at the desired position to maximize the total received power, while the received signals at other positions are distorted. We derive the closed-form expression for the secrecy rate achieved by the proposed scheme. The simulation results show that the proposed scheme can achieve two-dimensional secure transmission at a specific position and its performance advantage over the conventional DM scheme increases when the number of reflecting elements at the IRS increases.      
### 13.Unsupervised CT Metal Artifact Learning using Attention-guided beta-CycleGAN  [ :arrow_down: ](https://arxiv.org/pdf/2007.03480.pdf)
>  Metal artifact reduction (MAR) is one of the most important research topics in computed tomography (CT). With the advance of deep learning technology for image reconstruction,various deep learning methods have been also suggested for metal artifact removal, among which supervised learning methods are most popular. However, matched non-metal and metal image pairs are difficult to obtain in real CT acquisition. Recently, a promising unsupervised learning for MAR was proposed using feature disentanglement, but the resulting network architecture is complication and difficult to handle large size clinical images. To address this, here we propose a much simpler and much effective unsupervised MAR method for CT. The proposed method is based on a novel beta-cycleGAN architecture derived from the optimal transport theory for appropriate feature space disentanglement. Another important contribution is to show that attention mechanism is the key element to effectively remove the metal artifacts. Specifically, by adding the convolutional block attention module (CBAM) layers with a proper disentanglement parameter, experimental results confirm that we can get more improved MAR that preserves the detailed texture of the original image.      
### 14.Scale-free Design for Delayed Regulated Synchronization of Homogeneous and Heterogeneous Discrete-time Multi-agent Systems Subject to Unknown Non-uniform and Arbitrarily Large Communication Delays  [ :arrow_down: ](https://arxiv.org/pdf/2007.03478.pdf)
>  In this paper, we study delayed regulated state/output synchronization for discrete-time homogeneous and heterogeneous networks of multi-agent systems (MAS) subject to unknown, non-uniform and arbitrarily large communication delays. A delay transformation is utilized to transform the original MAS to a new system without delayed states. The proposed scale-free dynamic protocols are developed solely based on agent models and localized information exchange with neighbors such that we do not need any information about the communication networks and the number of agents.      
### 15.Delay Minimization for Federated Learning Over Wireless Communication Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.03462.pdf)
>  In this paper, the problem of delay minimization for federated learning (FL) over wireless communication networks is investigated. In the considered model, each user exploits limited local computational resources to train a local FL model with its collected data and, then, sends the trained FL model parameters to a base station (BS) which aggregates the local FL models and broadcasts the aggregated FL model back to all the users. Since FL involves learning model exchanges between the users and the BS, both computation and communication latencies are determined by the required learning accuracy level, which affects the convergence rate of the FL algorithm. This joint learning and communication problem is formulated as a delay minimization problem, where it is proved that the objective function is a convex function of the learning accuracy. Then, a bisection search algorithm is proposed to obtain the optimal solution. Simulation results show that the proposed algorithm can reduce delay by up to 27.3% compared to conventional FL methods.      
### 16.Noise-Powered Disentangled Representation for Unsupervised Speckle Reduction of Optical Coherence Tomography Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.03446.pdf)
>  Due to its noninvasive character, optical coherence tomography (OCT) has become a popular diagnostic method in clinical settings. However, the low-coherence interferometric imaging procedure is inevitably contaminated by heavy speckle noise, which impairs both visual quality and diagnosis of various ocular diseases. Although deep learning has been applied for image denoising and achieved promising results, the lack of well-registered clean and noisy image pairs makes it impractical for supervised learning-based approaches to achieve satisfactory OCT image denoising results. In this paper, we propose an unsupervised OCT image speckle reduction algorithm that does not rely on well-registered image pairs. Specifically, by employing the ideas of disentangled representation and generative adversarial network, the proposed method first disentangles the noisy image into content and noise spaces by corresponding encoders. Then, the generator is used to predict the denoised OCT image with the extracted content features. In addition, the noise patches cropped from the noisy image are utilized to facilitate more accurate disentanglement. Extensive experiments have been conducted, and the results suggest that our proposed method is superior to the classic methods and demonstrates competitive performance to several recently proposed learning-based approaches in both quantitative and qualitative aspects.      
### 17.Reconfigurable Intelligent Surfaces: Principles and Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2007.03435.pdf)
>  Reconfigurable intelligent surfaces (RISs), also known as intelligent reflecting surfaces (IRSs), have received significant attention for their potential to enhance the capacity and coverage of wireless networks by smartly reconfiguring the wireless propagation environment. Therefore, RISs are considered a promising technology for the sixth-generation (6G) communication networks. In this context, we provide a comprehensive overview of the state-of-the-art on RISs, with focus on their operating principles, performance evaluation, beamforming design and resource management, applications of machine learning to RIS-enhanced wireless networks, as well as the integration of RISs with other emerging technologies. We describe the basic principles of RISs both from physics and communications perspectives, based on which we present performance evaluation of multi-antenna assisted RIS systems. In addition, we systematically survey existing designs for RIS-enhanced wireless networks encompassing performance analysis, information theory, and performance optimization perspectives. Furthermore, we survey existing research contributions that apply machine learning for tackling challenges in dynamic scenarios, such as random fluctuations of wireless channels and user mobility in RIS-enhanced wireless networks. Last but not least, we identify major issues and research opportunities associated with the integration of RISs and other emerging technologies for application to next-generation networks.      
### 18.Off-grid Multi-Source Passive Localization Using a Moving Array  [ :arrow_down: ](https://arxiv.org/pdf/2007.03420.pdf)
>  A novel direct passive localization technique through a single moving array is proposed in this paper using the sparse representation of the array covariance matrix in spatial domain. The measurement is constructed by stacking the vectorized version of all the array covariance matrices at different observing positions. First, an on-grid compressive sensing (CS) based method is developed, where the dictionary is composed of the steering vectors from the searching grids to the observing positions. Convex optimization is applied to solve the `1-norm minimization problem. Second, to get much finer target positions, we develop an on-grid CS based method, where the majorization-minimization technique replaces the atan-sum objective function in each iteration by a quadratic convex function which can be easily minimized. The objective function,atan-sum, is more similar to `0-norm, and more sparsity encouraging than the log-sum function.This method also works more robustly at conditions of low SNR, and fewer observing positions are needed than in the traditional ones. The simulation experiments verify the promises of the proposed algorithm.      
### 19.Relay Assisted OFDM with Subcarrier Number Modulation in Multi-Hop Cooperative Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.03416.pdf)
>  Orthogonal frequency-division multiplexing (OFDM) with subcarrier number modulation (OFDM-SNM) manifests its superior nature of high spectral efficiency (SE) and low complexity for signal estimation. To exploit the spatial gain of OFDM-SNM, we propose a relay assisted OFDM-SNM scheme for multi-hop cooperative systems in this letter. It is stipulated that a relay operated by decode-and-forward (DF) and half-duplex (HD) protocols exists in each hop. We analyze the outage performance of the relay assisted OFDM-SNM system. The average outage probability is approximated in closed form. Moreover, to reveal the diversity and coding gains of relay assisted OFDM-SNM, we explore the asymptotic outage performance at high signal-to-noise ratio (SNR) by power series expansion. To verify the improvement on outage performance and energy efficiency, we carry out the comparison among different multi-hops systems with traditional OFDM-SNM fixing a certain distance from source to destination. Simulation results corroborate the derived outage probabilities and provide insight into the proposed system in this letter.      
### 20.Modeling, Simulation and Implementation of a Bird-Inspired Morphing Wing Aircraft  [ :arrow_down: ](https://arxiv.org/pdf/2007.03352.pdf)
>  We present a design of a bird-inspired morphing wing aircraft, including bionic research, modeling, simulation and flight experiments. Inspired by birds and activated by a planar linkage, our proposed aircraft has three key states: gliding, descending and high-maneuverability. We build the aerodynamic model of the aircraft and analyze its mechanisms to find out a group of optimized parameters. Furthermore, we validate our design by Computational Fluid Dynamics (CFD) simulation based on Lattice-Boltzmann technology and determine three phases of the planar linkage for the three states. Lastly, we manufacture a prototype and conduct flight experiments to test the performance of the aircraft.      
### 21.On the Construction of Polar Codes in the Middleton Class-A Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.03312.pdf)
>  Although power line communication (PLC) systems are available everywhere, unfortunately these systems are not suitable for information transmission due to the effects of the impulsive noise. Therefore, many previous studies on channel codes have been carried out for the purpose of reducing the impulsive noise in such channels. This paper investigates some methods for the construction of polar codes under PLC systems in the presence of Middleton class-A noise. We discuss here the most feasible construction methods which already have been adopted with other channels. In addition, we present an illustrative example for the construction in these methods and also we discuss a comparison between the methods in terms of performance.      
### 22.Divide-and-Rule: Self-Supervised Learning for Survival Analysis in Colorectal Cancer  [ :arrow_down: ](https://arxiv.org/pdf/2007.03292.pdf)
>  With the long-term rapid increase in incidences of colorectal cancer (CRC), there is an urgent clinical need to improve risk stratification. The conventional pathology report is usually limited to only a few histopathological features. However, most of the tumor microenvironments used to describe patterns of aggressive tumor behavior are ignored. In this work, we aim to learn histopathological patterns within cancerous tissue regions that can be used to improve prognostic stratification for colorectal cancer. To do so, we propose a self-supervised learning method that jointly learns a representation of tissue regions as well as a metric of the clustering to obtain their underlying patterns. These histopathological patterns are then used to represent the interaction between complex tissues and predict clinical outcomes directly. We furthermore show that the proposed approach can benefit from linear predictors to avoid overfitting in patient outcomes predictions. To this end, we introduce a new well-characterized clinicopathological dataset, including a retrospective collective of 374 patients, with their survival time and treatment information. Histomorphological clusters obtained by our method are evaluated by training survival models. The experimental results demonstrate statistically significant patient stratification, and our approach outperformed the state-of-the-art deep clustering methods.      
### 23.Multi-Tones' Phase Coding (MTPC) of Interaural Time Difference by Spiking Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.03274.pdf)
>  Inspired by the mammal's auditory localization pathway, in this paper we propose a pure spiking neural network (SNN) based computational model for precise sound localization in the noisy real-world environment, and implement this algorithm in a real-time robotic system with a microphone array. The key of this model relies on the MTPC scheme, which encodes the interaural time difference (ITD) cues into spike patterns. This scheme naturally follows the functional structures of the human auditory localization system, rather than artificially computing of time difference of arrival. Besides, it highlights the advantages of SNN, such as event-driven and power efficiency. The MTPC is pipelined with two different SNN architectures, the convolutional SNN and recurrent SNN, by which it shows the applicability to various SNNs. This proposal is evaluated by the microphone collected location-dependent acoustic data, in a real-world environment with noise, obstruction, reflection, or other affects. The experiment results show a mean error azimuth of 1~3 degrees, which surpasses the accuracy of the other biologically plausible neuromorphic approach for sound source localization.      
### 24.Full Duplex Integrated Access and Backhaul for 5G NR: Analyses and Prototype Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2007.03272.pdf)
>  Integrated access and backhaul (IAB) frameworks for 5G new radio (NR) as a cost-effective alternative to the wired backhaul have been investigated by 3GPP. A promising solution for this framework is the integration of full duplex (FD) technologies to enhance the spectral efficiency and make an efficient use of the network resources-termed FD IAB. However, FD IAB presents significant technical challenges, such as self-interference (SI) in the IAB framework, which may cast doubt over the feasibility of FD IAB. In this article, we present a brief tutorial of the FD IAB framework and its enabling technologies. We then numerically evaluate and discuss the link-level SI reduction and the system-level downlink throughput performance of the FD IAB. Finally, we validate the feasibility of FD IAB using 28 GHz hardware prototype measurements of propagation-domain SI suppression. Our numerical evaluations and prototype measurements confirm that the FD IAB serves as a promising framework for 5G NR.      
### 25.Topology-Aware Joint Graph Filter and Edge Weight Identification for Network Processes  [ :arrow_down: ](https://arxiv.org/pdf/2007.03266.pdf)
>  Data defined over a network have been successfully modelled by means of graph filters. However, although in many scenarios the connectivity of the network is known, e.g., smart grids, social networks, etc., the lack of well-defined interaction weights hinders the ability to model the observed networked data using graph filters. Therefore, in this paper, we focus on the joint identification of coefficients and graph weights defining the graph filter that best models the observed input/output network data. While these two problems have been mostly addressed separately, we here propose an iterative method that exploits the knowledge of the support of the graph for the joint identification of graph filter coefficients and edge weights. We further show that our iterative scheme guarantees a non-increasing cost at every iteration, ensuring a globally-convergent behavior. Numerical experiments confirm the applicability of our proposed approach.      
### 26.Automated Formal Synthesis of Neural Barrier Certificates for Dynamical Models  [ :arrow_down: ](https://arxiv.org/pdf/2007.03251.pdf)
>  We introduce an automated, formal, counterexample-based approach to synthesise Barrier Certificates (BC) for the safety verification of continuous and hybrid dynamical models. The approach is underpinned by an inductive framework: this is structured as a sequential loop between a learner, which manipulates a candidate BC as a neural network, and a sound verifier, which either certifies through algorithmic proofs the candidate's validity or generates counter-examples to further guide the learner. We compare the approach against state-of-the-art techniques, over polynomial and non-polynomial dynamical models: the outcomes show that we can synthesise sound BCs up to two orders of magnitude faster, with in particular a stark speedup on the verification engine (up to five orders less), whilst needing a far smaller data set (up to three orders less) for the learning part. Beyond the state of the art, we further challenge the (verification side of the) approach on a hybrid dynamical model.      
### 27.Demo: iJam with Channel Randomization  [ :arrow_down: ](https://arxiv.org/pdf/2007.03201.pdf)
>  Physical-layer key generation methods utilize the variations of the communication channel to achieve a secure key agreement between two parties with no prior security association. Their secrecy rate (bit generation rate) depends heavily on the randomness of the channel, which may reduce significantly in a stable environment. Existing methods seek to improve the secrecy rate by injecting artificial noise into the channel. Unfortunately, noise injection cannot alter the underlying channel state, which depends on the multipath environment between the transmitter and receiver. Consequently, these methods are known to leak key bits toward multi-antenna eavesdroppers, which is capable of filtering the noise through the differential of multiple signal receptions. This work demonstrates an improved approach to reinforce physical-layer key generation schemes, e.g., channel randomization. The channel randomization approach leverages a reconfigurable antenna to rapidly change the channel state during transmission, and an angle-of-departure (AoD) based channel estimation algorithm to cancel the changing effects for the intended receiver. The combined result is a communication channel stable in the eyes of the intended receiver but randomly changing from the viewpoint of the eavesdropper. We augmented an existing physical-layer key generation protocol, iJam, with the proposed approach and developed a full-fledged remote instrumentation platform to demonstrate its performance. Our evaluations show that augmentation does not affect the bit error rate (BER) of the intended receiver during key establishment but reduces the eavesdropper's BER to the level of random guessing, regardless of the number of antennas it equips.      
### 28.Automatic lesion detection, segmentation and characterization via 3D multiscale morphological sifting in breast MRI  [ :arrow_down: ](https://arxiv.org/pdf/2007.03199.pdf)
>  Previous studies on computer aided detection/diagnosis (CAD) in 4D breast magnetic resonance imaging (MRI) regard lesion detection, segmentation and characterization as separate tasks, and typically require users to manually select 2D MRI slices or regions of interest as the input. In this work, we present a breast MRI CAD system that can handle 4D multimodal breast MRI data, and integrate lesion detection, segmentation and characterization with no user intervention. The proposed CAD system consists of three major stages: region candidate generation, feature extraction and region candidate classification. Breast lesions are firstly extracted as region candidates using the novel 3D multiscale morphological sifting (MMS). The 3D MMS, which uses linear structuring elements to extract lesion-like patterns, can segment lesions from breast images accurately and efficiently. Analytical features are then extracted from all available 4D multimodal breast MRI sequences, including T1-, T2-weighted and DCE sequences, to represent the signal intensity, texture, morphological and enhancement kinetic characteristics of the region candidates. The region candidates are lastly classified as lesion or normal tissue by the random under-sampling boost (RUSboost), and as malignant or benign lesion by the random forest. Evaluated on a breast MRI dataset which contains a total of 117 cases with 95 malignant and 46 benign lesions, the proposed system achieves a true positive rate (TPR) of 0.90 at 3.19 false positives per patient (FPP) for lesion detection and a TPR of 0.91 at a FPP of 2.95 for identifying malignant lesions without any user intervention. The average dice similarity index (DSI) is 0.72 for lesion segmentation. Compared with previously proposed systems evaluated on the same breast MRI dataset, the proposed CAD system achieves a favourable performance in breast lesion detection and characterization.      
### 29.A Joint Design of MIMO-OFDM Dual-Function Radar Communication System Using Generalized Spatial Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2007.03164.pdf)
>  A novel dual-function radar communication (DFRC) system is proposed, that achieves high target resolution and high communication rate. It consists of a multiple-input multiple-output (MIMO) radar, where only a small number of antennas are active in each channel use. The probing waveforms are orthogonal frequency division multiplexing (OFDM) type. The OFDM carriers are divided into two groups, one that is used by the active antennas in a shared fashion, and another one, where each subcarrier is assigned to an active antenna in an exclusive fashion (private subcarriers). Target estimation is carried out based on the received and transmitted symbols. The system communicates information via the transmitted OFDM data symbols and the pattern of active antennas in a generalized spatial modulation (GSM) fashion. A multi-antenna communication receiver can identify the indices of active antennas via sparse signal recovery methods. The use of shared subcarriers enables high communication rate. The private subcarriers are used to synthesize a virtual array for high angular resolution, and also for improved estimation on the active antenna indices. The OFDM waveforms allow the communication receiver to easily mitigate the effect of frequency selective fading, while the use of a sparse array at the transmitter reduces the hardware cost of the system. The radar performance of the proposed DFRC system is evaluated via simulations, and bit error rate (BER) results for the communication system are provided.      
### 30.Speed-of-sound imaging by differential phase contrast with angular compounding  [ :arrow_down: ](https://arxiv.org/pdf/2007.03156.pdf)
>  We describe a technique to reveal speed-of-sound (SoS) variations within an echogenic sample. The technique uses the same receive data as standard pulse-echo imaging based on plane-wave compounding, and can be operated in parallel. Point-like scatterers randomly distributed throughout the sample serve as local probes of the downstream transmit-beam phase shifts caused by aberrating structures within the sample. Phase shifts are monitored in a differential manner, providing signatures of transverse gradients of the local sample SoS. The contrast of the signatures is augmented by a method of angular compounding, which provides ``focus" control of the image sharpness, which, in turn, enables a visual localization of aberrating inclusions within the sample on the fly. The localization can be performed in 2D when operated with standard B-mode imaging, or in 3D when operated with C-mode imaging. Finally, we present a wave-acoustic forward model that provides insight into the principle of differential phase contrast (DPC) imaging, and roughly recapitulates experimental results obtained with an elastography phantom. In particular, we demonstrate that our technique easily reveals relative SoS variations as small as 0.5\% in real time. Such imaging may ultimately be useful for clinical diagnosis of pathologies in soft tissue.      
### 31.High-speed Millimeter-wave 5G/6G Image Transmission via Artificial Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2007.03153.pdf)
>  Artificial Intelligence (AI) has been used to jointly optimize a mmWave Compressed Sensing (CS) for high-speed 5G/6G image transmission. Specifically, we have developed a Dictionary Learning Compressed Sensing neural Network (DL-CSNet) to realize three key functionalities: 1) to learn the dictionary basis of the images for transmission; 2) to optimize the Hadamard measurement matrix; and 3) to reconstruct the lossless images with the learned dictionary basis. A 94-GHz prototype has been built and up to one order of image transmission speed increase has been realized for letters ``A" to ``Z".      
### 32.An Energy-Saving Home Energy Management Supporting Selling Operation and User Comfort  [ :arrow_down: ](https://arxiv.org/pdf/2007.03142.pdf)
>  In this study, we investigate the operation of a home energy management system with integrated renewable energy system (RES) and energy storage system (ESS) in a smart home. In its operation, there is a focus on selling operation and user comfort, which are analyzed in detail. A multi-objective mixed integer nonlinear programming model is proposed to optimize different and conflicting objectives. Through incorporation of these varied objectives, our system not only reduces energy cost but also maintains user comfort and peak-to-average ratio (PAR) for residents. The effect of selling price on user comfort and PAR is also considered. Moreover, by applying the weight method of multi-objective optimization, we have more flexibility in setting trade-offs between the values of these objectives. A formula for the lower bound of energy cost is developed. This formula helps residents or engineers quickly choose best parameters of RES and ESS for their homes during the decision-making process. Performance of our system is verified through a number of simulations under different scenarios using real data, and simulation results are compared in terms of energy cost per day, PAR, user's convenience and waiting time to use appliances.      
### 33.Electromyogram (EMG) Removal by Adding Sources of EMG (ERASE) -- A novel ICA-based algorithm for removing myoelectric artifacts from EEG -- Part 2  [ :arrow_down: ](https://arxiv.org/pdf/2007.03136.pdf)
>  Extraction of the movement-related high-gamma (80 - 160 Hz) in electroencephalogram (EEG) from traumatic brain injury (TBI) patients who have had hemicraniectomies, remains challenging due to a confounding bandwidth overlap with surface electromyogram (EMG) artifacts related to facial and head movements. In part 1, we described an augmented independent component analysis (ICA) approach for removal of EMG artifacts from EEG, and referred to as EMG Reduction by Adding Sources of EMG (ERASE). Here, we tested ERASE on EEG recorded from six TBI patients with hemicraniectomies while they performed a thumb flexion task. ERASE removed a mean of 52 +/- 12% (mean +/- S.E.M) (maximum 73%) of EMG artifacts. In contrast, conventional ICA removed a mean of 27 +/- 19\% (mean +/- S.E.M) of EMG artifacts from EEG. In particular, high-gamma synchronization was significantly improved in the contralateral hand motor cortex area within the hemicraniectomy site after ERASE was applied. We computed fractal dimension (FD) of EEG high-gamma on each channel. We found relative FD of high-gamma over hemicraniectomy after applying ERASE were strongly correlated to the amplitude of finger flexion force. Results showed that significant correlation coefficients across the electrodes related to thumb flexion averaged 0.76, while the coefficients across the homologous electrodes in non-hemicraniectomy areas were nearly 0. Across all subjects, an average of 83% of electrodes significantly correlated with force was located in the hemicraniectomy areas after applying ERASE. After conventional ICA, only 19% of electrodes with significant correlations were located in the hemicraniectomy. These results indicated that the new approach isolated electrophysiological features during finger motor activation while selectively removing confounding EMG artifacts.      
### 34.Electromyogram (EMG) Removal by Adding Sources of EMG (ERASE) -- A novel ICA-based algorithm for removing myoelectric artifacts from EEG -- Part 1  [ :arrow_down: ](https://arxiv.org/pdf/2007.03130.pdf)
>  Electroencephalographic (EEG) recordings are often contaminated by electromyographic (EMG) artifacts, especially when recording during movement. Existing methods to remove EMG artifacts include independent component analysis (ICA), and other high-order statistical methods. However, these methods can not effectively remove most of EMG artifacts. Here, we proposed a modified ICA model for EMG artifacts removal in the EEG, which is called EMG Removal by Adding Sources of EMG (ERASE). In this new approach, additional channels of real EMG from neck and head muscles (reference artifacts) were added as inputs to ICA in order to "force" the most power from EMG artifacts into a few independent components (ICs). The ICs containing EMG artifacts (the "artifact ICs") were identified and rejected using an automated procedure. Simulation results showed ERASE removed EMG artifacts from EEG significantly more effectively than conventional ICA. Subsequently, EEG was collected from 8 healthy participants while they moved their hands to test the realistic efficacy of this approach. Results showed that ERASE successfully removed EMG artifacts (on average, about 75% of EMG artifacts were removed when using real EMGs as reference artifacts) while preserving the expected EEG features related to movement. We also tested the ERASE procedure using simulated EMGs as reference artifacts (about 63% of EMG artifacts removed). Compared to conventional ICA, ERASE removed on average 26% more EMG artifacts from EEG. These results indicate that using additional real or simulated EMG sources can increase the effectiveness of ICA in removing EMG artifacts from EEG. Combined with automated artifact IC rejection, ERASE also minimizes potential user bias.      
### 35.Multi-image Super Resolution of Remotely Sensed Images using Residual Feature Attention Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.03107.pdf)
>  Convolutional Neural Networks (CNNs) have been consistently proved state-of-the-art results in image Super-Resolution (SR), representing an exceptional opportunity for the remote sensing field to extract further information and knowledge from captured data. However, most of the works published in the literature have been focusing on the Single-Image Super-Resolution problem so far. At present, satellite based remote sensing platforms offer huge data availability with high temporal resolution and low spatial <a class="link-external link-http" href="http://resolution.In" rel="external noopener nofollow">this http URL</a> this context, the presented research proposes a novel residual attention model (RAMS) that efficiently tackles the multi-image super-resolution task, simultaneously exploiting spatial and temporal correlations to combine multiple images. We introduce the mechanism of visual feature attention with 3D convolutions in order to obtain an aware data fusion and information extraction of the multiple low-resolution images, transcending limitations of the local region of convolutional operations. Moreover, having multiple inputs with the same scene, our representation learning network makes extensive use of nestled residual connections to let flow redundant low-frequency signals and focus the computation on more important high-frequency components. Extensive experimentation and evaluations against other available solutions, either for single or multi-image super-resolution, have demonstrated that the proposed deep learning-based solution can be considered state-of-the-art for Multi-Image Super-Resolution for remote sensing applications.      
### 36.Domain Adaptation for Ultrasound Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2007.03096.pdf)
>  Ultrasound B-Mode images are created from data obtained from each element in the transducer array in a process called beamforming. The beamforming goal is to enhance signals from specified spatial locations, while reducing signal from all other locations. On clinical systems, beamforming is accomplished with the delay-and-sum (DAS) algorithm. DAS is efficient but fails in patients with high noise levels, so various adaptive beamformers have been proposed. Recently, deep learning methods have been developed for this task. With deep learning methods, beamforming is typically framed as a regression problem, where clean, ground-truth data is known, and usually simulated. For in vivo data, however, it is extremely difficult to collect ground truth information, and deep networks trained on simulated data underperform when applied to in vivo data, due to domain shift between simulated and in vivo data. In this work, we show how to correct for domain shift by learning deep network beamformers that leverage both simulated data, and unlabeled in vivo data, via a novel domain adaption scheme. A challenge in our scenario is that domain shift exists both for noisy input, and clean output. We address this challenge by extending cycle-consistent generative adversarial networks, where we leverage maps between synthetic simulation and real in vivo domains to ensure that the learned beamformers capture the distribution of both noisy and clean in vivo data. We obtain consistent in vivo image quality improvements compared to existing beamforming techniques, when applying our approach to simulated anechoic cysts and in vivo liver data.      
### 37.A Boolean Control Network Approach to the Formal Verification of Feedback Context-Aware Pervasive Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.03065.pdf)
>  The emergence of Context-aware systems in the domains of autonomic, monitoring, and safety-critical applications asks for the definition of methods to formally assess their correctness and dependability properties. Many of these properties are common to Automatic Control systems, a field that developed well established analysis and design techniques to formalize and investigate them. In this paper, we use Boolean Control Networks, to discuss some properties of a feedback Context-aware system in a case study based on a healthcare management example.      
### 38.Benefitting from Bicubically Down-Sampled Images for Learning Real-World Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.03053.pdf)
>  Super-resolution (SR) has traditionally been based on pairs of high-resolution images (HR) and their low-resolution (LR) counterparts obtained artificially with bicubic downsampling. However, in real-world SR, there is a large variety of realistic image degradations and analytically modeling these realistic degradations can prove quite difficult. In this work, we propose to handle real-world SR by splitting this ill-posed problem into two comparatively more well-posed steps. First, we train a network to transform real LR images to the space of bicubically downsampled images in a supervised manner, by using both real LR/HR pairs and synthetic pairs. Second, we take a generic SR network trained on bicubically downsampled images to super-resolve the transformed LR image. The first step of the pipeline addresses the problem by registering the large variety of degraded images to a common, well understood space of images. The second step then leverages the already impressive performance of SR on bicubically downsampled images, sidestepping the issues of end-to-end training on datasets with many different image degradations. We demonstrate the effectiveness of our proposed method by comparing it to recent methods in real-world SR and show that our proposed approach outperforms the state-of-the-art works in terms of both qualitative and quantitative results, as well as results of an extensive user study conducted on several real image datasets.      
### 39.PHELP: Pixel Heating Experiment Learning Platform for Education and Research on IAI-based Smart Control Engineering  [ :arrow_down: ](https://arxiv.org/pdf/2007.03048.pdf)
>  Thermal processes are one of the most common systems in the industry, making its understanding a mandatory skill for control engineers. So, multiple efforts are focused on developing low-cost and portable experimental training rigs recreating the thermal process dynamics and controls, usually limited to SISO or low order 2x2 MIMO systems. This paper presents PHELP, a low-cost, portable, and high order MIMO educational platform for uniformity temperature control training. The platform is composed of an array of 16 Peltier modules as heating elements, with a lower heating and cooling times, resulting in a 16x16 high order MIMO system. A low-cost real-time infrared thermal camera is employed as a temperature feedback sensor instead of a standard thermal sensor, ideal for high order MIMO system sensing and temperature distribution tracking. The control algorithm is developed in Matlab/Simulink and employs an Arduino board in hardware in the loop configuration to apply the control action to each Peltier module in the array. A temperature control experiment is performed, showing that the platform is suitable for teaching and training experiences not only in the classroom but also for engineers in the industry. Furthermore, various abnormal conditions can be introduced so that smart control engineering features can be tested.      
### 40.Deep Reinforcement Learning for Cybersecurity Assessment of Wind Integrated Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.03025.pdf)
>  The integration of renewable energy sources (RES), and specifically wind and solar PV systems, is rapidly increasing in electric power systems (EPS). While the inclusion of these intermittent RES coupled with the wide-scale deployment of communication and sensing devices is important towards a fully smart and modern grid, it has also expanded the cyberthreat landscape, effectively making power systems vulnerable to cyberattacks. This paper proposes a cybersecurity assessment approach designed to assess the cyberphysical security of EPS. The work takes into consideration the intermittent generation of RES, vulnerabilities introduced by microprocessor-based electronic information and operational technology (IT/OT) devices, and contingency analysis results. The proposed approach utilizes deep reinforcement learning (DRL) and an adapted Common Vulnerability Scoring System (CVSS) score tailored to assess vulnerabilities in EPS, in order to identify the optimal attack transition policy based on N-2 contingency results, i.e., the simultaneous failure of two system elements. The effectiveness of the approach is validated via numerical and real-time simulation experiments, which in turn, demonstrate how the proposed process successfully identifies potential threats that can be utilized by attackers to cause critical EPS disruptions.      
### 41.Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2007.03001.pdf)
>  We study training a single acoustic model for multiple languages with the aim of improving automatic speech recognition (ASR) performance on low-resource languages, and over-all simplifying deployment of ASR systems that support diverse languages. We perform an extensive benchmark on 51 languages, with varying amount of training data by language(from 100 hours to 1100 hours). We compare three variants of multilingual training from a single joint model without knowing the input language, to using this information, to multiple heads (one per language cluster). We show that multilingual training of ASR models on several languages can improve recognition performance, in particular, on low resource languages. We see 20.9%, 23% and 28.8% average WER relative reduction compared to monolingual baselines on joint model, joint model with language input and multi head model respectively. To our knowledge, this is the first work studying multilingual ASR at massive scale, with more than 50 languages and more than 16,000 hours of audio across them.      
### 42.Consensus Multi-Agent Reinforcement Learning for Volt-VAR Control in Power Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.02991.pdf)
>  Volt-VAR control (VVC) is a critical application in active distribution network management system to reduce network losses and improve voltage profile. To remove dependency on inaccurate and incomplete network models and enhance resiliency against communication or controller failure, we propose consensus multi-agent deep reinforcement learning algorithm to solve the VVC problem. The VVC problem is formulated as a networked multi-agent Markov decision process, which is solved using the maximum entropy reinforcement learning framework and a novel communication-efficient consensus strategy. The proposed algorithm allows individual agents to learn a group control policy using local rewards. Numerical studies on IEEE distribution test feeders show that our proposed algorithm matches the performance of single-agent reinforcement learning benchmark. In addition, the proposed algorithm is shown to be communication efficient and resilient.      
### 43.Smartphone-based Wellness Assessment Using Mobile Environmental Sensor  [ :arrow_down: ](https://arxiv.org/pdf/2007.03617.pdf)
>  Mental health and general wellness are becoming a growing concern in our society. Environmental factors contribute to mental illness and have the power to affect a person's wellness. This work presents a smartphone-based wellness assessment system and examines if there is any correlation with one's environment and their wellness. The introduced system was initiated in response to a growing need for individualized and independent mental health care and evaluated through experimentation. The participants were given an Android smartphone and a mobile sensor board and they were asked to complete a brief psychological survey three times per day. During the survey completion, the board in their possession is reading environmental data. The five environmental variables collected are temperature, humidity, air pressure, luminosity, and noise level. Upon submission of the survey, the results of the survey and the environmental data are sent to a server for further processing. Three experiments with 62 participants in total have been completed. The correlation most regularly deemed statistically significant was that of light and audio and stress.      
### 44.Detecting Signatures of Early-stage Dementia with Behavioural Models Derived from Sensor Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.03615.pdf)
>  There is a pressing need to automatically understand the state and progression of chronic neurological diseases such as dementia. The emergence of state-of-the-art sensing platforms offers unprecedented opportunities for indirect and automatic evaluation of disease state through the lens of behavioural monitoring. This paper specifically seeks to characterise behavioural signatures of mild cognitive impairment (MCI) and Alzheimer's disease (AD) in the \textit{early} stages of the disease. We introduce bespoke behavioural models and analyses of key symptoms and deploy these on a novel dataset of longitudinal sensor data from persons with MCI and AD. We present preliminary findings that show the relationship between levels of sleep quality and wandering can be subtly different between patients in the early stages of dementia and healthy cohabiting controls.      
### 45.Superiority of Simplicity: A Lightweight Model for Network Device Workload Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2007.03568.pdf)
>  The rapid growth and distribution of IT systems increases their complexity and aggravates operation and maintenance. To sustain control over large sets of hosts and the connecting networks, monitoring solutions are employed and constantly enhanced. They collect diverse key performance indicators (KPIs) (e.g. CPU utilization, allocated memory, etc.) and provide detailed information about the system state. Storing such metrics over a period of time naturally raises the motivation of predicting future KPI progress based on past observations. Although, a variety of time series forecasting methods exist, forecasting the progress of IT system KPIs is very hard. First, KPI types like CPU utilization or allocated memory are very different and hard to be expressed by the same model. Second, system components are interconnected and constantly changing due to soft- or firmware updates and hardware modernization. Thus a frequent model retraining or fine-tuning must be expected. Therefore, we propose a lightweight solution for KPI series prediction based on historic observations. It consists of a weighted heterogeneous ensemble method composed of two models - a neural network and a mean predictor. As ensemble method a weighted summation is used, whereby a heuristic is employed to set the weights. The modelling approach is evaluated on the available FedCSIS 2020 challenge dataset and achieves an overall $R^2$ score of 0.10 on the preliminary 10% test data and 0.15 on the complete test data. We publish our code on the following github repository: <a class="link-external link-https" href="https://github.com/citlab/fed_challenge" rel="external noopener nofollow">this https URL</a>      
### 46.3D Topology Transformation with Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.03532.pdf)
>  Generation and transformation of images and videos using artificial intelligence have flourished over the past few years. Yet, there are only a few works aiming to produce creative 3D shapes, such as sculptures. Here we show a novel 3D-to-3D topology transformation method using Generative Adversarial Networks (GAN). We use a modified pix2pix GAN, which we call Vox2Vox, to transform the volumetric style of a 3D object while retaining the original object shape. In particular, we show how to transform 3D models into two new volumetric topologies - the 3D Network and the Ghirigoro. We describe how to use our approach to construct customized 3D representations. We believe that the generated 3D shapes are novel and inspirational. Finally, we compare the results between our approach and a baseline algorithm that directly convert the 3D shapes, without using our GAN.      
### 47.Inverse Reinforcement Learning for Sequential Hypothesis Testing and Search  [ :arrow_down: ](https://arxiv.org/pdf/2007.03481.pdf)
>  This paper considers a novel formulation of inverse reinforcement learning~(IRL) with behavioral economics constraints to address inverse sequential hypothesis testing (SHT) and inverse search in Bayesian agents. We first estimate the stopping and search costs by observing the actions of these agents using Bayesian revealed preference from microeconomics and rational inattention from behavioral economics. We also solve the inverse problem of the more general rationally inattentive SHT where the agent incorporates controlled sensing by optimally choosing from various sensing modes. Second, we design statistical hypothesis tests with bounded Type-I and Type-II error probabilities to detect if the agents are Bayesian utility maximizers when their actions are measured in noise. By dynamically tuning the prior specified to the agents, we formulate an {\em active} IRL framework which enhances these detection tests and minimizes their Type-II and Type-I error probabilities of utility maximization detection. Finally, we give a finite sample complexity result which provides finite sample bounds on the error probabilities of the detection tests.      
### 48.Decentralized Deep Reinforcement Learning for Network Level Traffic Signal Control  [ :arrow_down: ](https://arxiv.org/pdf/2007.03433.pdf)
>  In this thesis, I propose a family of fully decentralized deep multi-agent reinforcement learning (MARL) algorithms to achieve high, real-time performance in network-level traffic signal control. In this approach, each intersection is modeled as an agent that plays a Markovian Game against the other intersection nodes in a traffic signal network modeled as an undirected graph, to approach the optimal reduction in delay. Following Partially Observable Markov Decision Processes (POMDPs), there are 3 levels of communication schemes between adjacent learning agents: independent deep Q-leaning (IDQL), shared states reinforcement learning (S2RL) and a shared states &amp; rewards version of S2RL--S2R2L. In these 3 variants of decentralized MARL schemes, individual agent trains its local deep Q network (DQN) separately, enhanced by convergence-guaranteed techniques like double DQN, prioritized experience replay, multi-step bootstrapping, etc. To test the performance of the proposed three MARL algorithms, a SUMO-based simulation platform is developed to mimic the traffic evolution of the real world. Fed with random traffic demand between permitted OD pairs, a 4x4 Manhattan-style grid network is set up as the testbed, two different vehicle arrival rates are generated for model training and testing. The experiment results show that S2R2L has a quicker convergence rate and better convergent performance than IDQL and S2RL in the training process. Moreover, three MARL schemes all reveal exceptional generalization abilities. Their testing results surpass the benchmark Max Pressure (MP) algorithm, under the criteria of average vehicle delay, network-level queue length and fuel consumption rate. Notably, S2R2L has the best testing performance of reducing 34.55% traffic delay and dissipating 10.91% queue length compared with MP.      
### 49.Impasse Surface of Differential-Algebraic Power System Models: An Interpretation Based on Admittance Matrices  [ :arrow_down: ](https://arxiv.org/pdf/2007.03418.pdf)
>  The impasse surface is an important concept in the differential-algebraic equation (DAE) model of power systems, which is associated with short-term voltage collapse. This paper establishes a necessary condition for a system trajectory hitting the impasse surface. The condition is in terms of admittance matrices regarding the power network, generators and loads, which specifies the pattern of interaction between those system components that can induce voltage collapse. It applies to generic DAE models featuring high-order synchronous generators, static load components, induction motors and a lossy power network. We also identify a class of static load parameters that prevents power systems from hitting the impasse surface; this proves a conjecture made by Hiskens that has been unsolved for decades. Moreover, the obtained results lead to an early indicator of voltage collapse and a novel viewpoint that inductive compensation has a positive effect on preventing short-term voltage collapse, which are verified via numerical simulations.      
### 50.Learning and Reasoning with the Graph Structure Representation in Robotic Surgery  [ :arrow_down: ](https://arxiv.org/pdf/2007.03357.pdf)
>  Learning to infer graph representations and performing spatial reasoning in a complex surgical environment can play a vital role in surgical scene understanding in robotic surgery. For this purpose, we develop an approach to generate the scene graph and predict surgical interactions between instruments and surgical region of interest (ROI) during robot-assisted surgery. We design an attention link function and integrate with a graph parsing network to recognize the surgical interactions. To embed each node with corresponding neighbouring node features, we further incorporate SageConv into the network. The scene graph generation and active edge classification mostly depend on the embedding or feature extraction of node and edge features from complex image representation. Here, we empirically demonstrate the feature extraction methods by employing label smoothing weighted loss. Smoothing the hard label can avoid the over-confident prediction of the model and enhances the feature representation learned by the penultimate layer. To obtain the graph scene label, we annotate the bounding box and the instrument-ROI interactions on the robotic scene segmentation challenge 2018 dataset with an experienced clinical expert in robotic surgery and employ it to evaluate our propositions.      
### 51.Safety Controller Synthesis for Collaborative Robots  [ :arrow_down: ](https://arxiv.org/pdf/2007.03340.pdf)
>  In human-robot collaboration (HRC), software-based automatic safety controllers (ASCs) are used in various forms (e.g. shutdown mechanisms, emergency brakes, interlocks) to improve operational safety. Complex robotic tasks and increasingly close human-robot interaction pose new challenges to ASC developers and certification authorities. Key among these challenges is the need to assure the correctness of ASCs under reasonably weak assumptions. To address this need, we introduce and evaluate a tool-supported ASC synthesis method for HRC in manufacturing. Our ASC synthesis is: (i) informed by the manufacturing process, risk analysis, and regulations; (ii) formally verified against correctness criteria; and (iii) selected from a design space of feasible controllers according to a set of optimality criteria. The synthesised ASC can detect the occurrence of hazards, move the process into a safe state, and, in certain circumstances, return the process to an operational state from which it can resume its original task.      
### 52.Automatic Ischemic Stroke Lesion Segmentation from Computed Tomography Perfusion Images by Image Synthesis and Attention-Based Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.03294.pdf)
>  Ischemic stroke lesion segmentation from Computed Tomography Perfusion (CTP) images is important for accurate diagnosis of stroke in acute care units. However, it is challenged by low image contrast and resolution of the perfusion parameter maps, in addition to the complex appearance of the lesion. To deal with this problem, we propose a novel framework based on synthesized pseudo Diffusion-Weighted Imaging (DWI) from perfusion parameter maps to obtain better image quality for more accurate segmentation. Our framework consists of three components based on Convolutional Neural Networks (CNNs) and is trained end-to-end. First, a feature extractor is used to obtain both a low-level and high-level compact representation of the raw spatiotemporal Computed Tomography Angiography (CTA) images. Second, a pseudo DWI generator takes as input the concatenation of CTP perfusion parameter maps and our extracted features to obtain the synthesized pseudo DWI. To achieve better synthesis quality, we propose a hybrid loss function that pays more attention to lesion regions and encourages high-level contextual consistency. Finally, we segment the lesion region from the synthesized pseudo DWI, where the segmentation network is based on switchable normalization and channel calibration for better performance. Experimental results showed that our framework achieved the top performance on ISLES 2018 challenge and: 1) our method using synthesized pseudo DWI outperformed methods segmenting the lesion from perfusion parameter maps directly; 2) the feature extractor exploiting additional spatiotemporal CTA images led to better synthesized pseudo DWI quality and higher segmentation accuracy; and 3) the proposed loss functions and network structure improved the pseudo DWI synthesis and lesion segmentation performance.      
### 53.Lossless CNN Channel Pruning via Gradient Resetting and Convolutional Re-parameterization  [ :arrow_down: ](https://arxiv.org/pdf/2007.03260.pdf)
>  Channel pruning (a.k.a. filter pruning) aims to slim down a convolutional neural network (CNN) by reducing the width (i.e., numbers of output channels) of convolutional layers. However, as CNN's representational capacity depends on the width, doing so tends to degrade the performance. A traditional learning-based channel pruning paradigm applies a penalty on parameters to improve the robustness to pruning, but such a penalty may degrade the performance even before pruning. Inspired by the neurobiology research about the independence of remembering and forgetting, we propose to re-parameterize a CNN into the remembering parts and forgetting parts, where the former learn to maintain the performance and the latter learn for efficiency. By training the re-parameterized model using regular SGD on the former but a novel update rule with penalty gradients on the latter, we achieve structured sparsity, enabling us to equivalently convert the re-parameterized model into the original architecture with narrower layers. With our method, we can slim down a standard ResNet-50 with 76.15\% top-1 accuracy on ImageNet to a narrower one with only 43.9\% FLOPs and no accuracy drop. Code and models are released at <a class="link-external link-https" href="https://github.com/DingXiaoH/ResRep" rel="external noopener nofollow">this https URL</a>.      
### 54.Extracting the fundamental diagram from aerial footage  [ :arrow_down: ](https://arxiv.org/pdf/2007.03227.pdf)
>  Efficient traffic monitoring is playing a fundamental role in successfully tackling congestion in transportation networks. Congestion is strongly correlated with two measurable characteristics, the demand and the network density that impact the overall system behavior. At large, this system behavior is characterized through the fundamental diagram of a road segment, a region or the <a class="link-external link-http" href="http://network.In" rel="external noopener nofollow">this http URL</a> this paper we devise an innovative way to obtain the fundamental diagram through aerial footage obtained from drone platforms. The derived methodology consists of 3 phases: vehicle detection, vehicle tracking and traffic state estimation. We elaborate on the algorithms developed for each of the 3 phases and demonstrate the applicability of the results in a real-world setting.      
### 55.Accelerated Insulation Aging Due to Fast, Repetitive Voltages: A Review Identifying Challenges and Future Research Needs  [ :arrow_down: ](https://arxiv.org/pdf/2007.03194.pdf)
>  Although the adverse effects of using power electronic conversion on the insulation systems used in different apparatuses have been investigated, they are limited to low slew rates and repetitions. These results cannot be used for next-generation wide bandgap (WBG) based conversion systems targeted to be fast (with a slew rate up to 100 kV/us) and operate at a high switching frequency up to 500 kHz. Frequency and slew rate are two of the most important factors of a voltage pulse, influencing the level of degradation of the insulation systems that are exposed to such voltage pulses. The paper reviews challenges concerning insulation degradation when benefitting from WBG-based conversion systems with the mentioned slew rate and switching frequency values and identifies technical gaps and future research needs. The paper provides a framework for future research in dielectrics and electrical insulation design for systems under fast, repetitive voltage pluses originated by WBG-based conversion systems.      
### 56.CrossCount: A Deep Learning System for Device-free Human Counting using WiFi  [ :arrow_down: ](https://arxiv.org/pdf/2007.03175.pdf)
>  Counting humans is an essential part of many people-centric applications. In this paper, we propose CrossCount: an accurate deep-learning-based human count estimator that uses a single WiFi link to estimate the human count in an area of interest. The main idea is to depend on the temporal link-blockage pattern as a discriminant feature that is more robust to wireless channel noise than the signal strength, hence delivering a ubiquitous and accurate human counting system. As part of its design, CrossCount addresses a number of deep learning challenges such as class imbalance and training data augmentation for enhancing the model generalizability. Implementation and evaluation of CrossCount in multiple testbeds show that it can achieve a human counting accuracy to within a maximum of 2 persons 100% of the time. This highlights the promise of CrossCount as a ubiquitous crowd estimator with non-labour-intensive data collection from off-the-shelf devices.      
### 57.Cognitive Radio Network Throughput Maximization with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.03165.pdf)
>  Radio Frequency powered Cognitive Radio Networks (RF-CRN) are likely to be the eyes and ears of upcoming modern networks such as Internet of Things (IoT), requiring increased decentralization and autonomous operation. To be considered autonomous, the RF-powered network entities need to make decisions locally to maximize the network throughput under the uncertainty of any network environment. However, in complex and large-scale networks, the state and action spaces are usually large, and existing Tabular Reinforcement Learning technique is unable to find the optimal state-action policy quickly. In this paper, deep reinforcement learning is proposed to overcome the mentioned shortcomings and allow a wireless gateway to derive an optimal policy to maximize network throughput. When benchmarked against advanced DQN techniques, our proposed DQN configuration offers performance speedup of up to 1.8x with good overall performance.      
### 58.Self domain adapted network  [ :arrow_down: ](https://arxiv.org/pdf/2007.03162.pdf)
>  Domain shift is a major problem for deploying deep networks in clinical practice. Network performance drops significantly with (target) images obtained differently than its (source) training data. Due to a lack of target label data, most work has focused on unsupervised domain adaptation (UDA). Current UDA methods need both source and target data to train models which perform image translation (harmonization) or learn domain-invariant features. However, training a model for each target domain is time consuming and computationally expensive, even infeasible when target domain data are scarce or source data are unavailable due to data privacy. In this paper, we propose a novel self domain adapted network (SDA-Net) that can rapidly adapt itself to a single test subject at the testing stage, without using extra data or training a UDA model. The SDA-Net consists of three parts: adaptors, task model, and auto-encoders. The latter two are pre-trained offline on labeled source images. The task model performs tasks like synthesis, segmentation, or classification, which may suffer from the domain shift problem. At the testing stage, the adaptors are trained to transform the input test image and features to reduce the domain shift as measured by the auto-encoders, and thus perform domain adaptation. We validated our method on retinal layer segmentation from different OCT scanners and T1 to T2 synthesis with T1 from different MRI scanners and with different imaging parameters. Results show that our SDA-Net, with a single test subject and a short amount of time for self adaptation at the testing stage, can achieve significant improvements.      
### 59.A Stochastic-Robust Approach for Resilient Microgrid Investment Planning Under Static and Transient Islanding Security Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2007.03149.pdf)
>  When planning the investment in Microgrids (MGs), usually static security constraints are included to ensure their resilience and ability to operate in islanded mode. However, unscheduled islanding events may trigger cascading disconnections of Distributed Energy Resources (DERs) inside the MG due to the transient response, leading to a partial or full loss of load. In this paper, a min-max-min, hybrid, stochastic-robust investment planning model is proposed to obtain a resilient MG considering both High-Impact-Low-Frequency (HILF) and Low-Impact-High-Frequency (LIHF) uncertainties. The HILF uncertainty pertains to the unscheduled islanding of the MG after a disastrous event, and the LIHF uncertainty relates to correlated loads and DER generation, characterized by a set of scenarios. The MG resilience under both types of uncertainty is ensured by incorporating static and transient islanding constraints into the proposed investment model. The inclusion of transient response constraints leads to a min-max-min problem with a non-linear dynamic frequency response model that cannot be solved directly by available optimization tools. Thus, in this paper, a three-stage solution approach is proposed to find the optimal investment plan. The performance of the proposed algorithm is tested on the CIGRE 18-node distribution network.      
### 60.Predicting Afrobeats Hit Songs Using Spotify Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.03137.pdf)
>  This study approached the Hit Song Science problem with the aim of predicting which songs in the Afrobeats genre will become popular among Spotify listeners. A dataset of 2063 songs was generated through the Spotify Web API, with the provided audio features. Random Forest and Gradient Boosting algorithms proved to be successful with approximately F1 scores of 86%.      
### 61.X-ray Photon-Counting Data Correction through Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.03119.pdf)
>  X-ray photon-counting detectors (PCDs) are drawing an increasing attention in recent years due to their low noise and energy discrimination capabilities. The energy/spectral dimension associated with PCDs potentially brings great benefits such as for material decomposition, beam hardening and metal artifact reduction, as well as low-dose CT imaging. However, X-ray PCDs are currently limited by several technical issues, particularly charge splitting (including charge sharing and K-shell fluorescence re-absorption or escaping) and pulse pile-up effects which distort the energy spectrum and compromise the data quality. Correction of raw PCD measurements with hardware improvement and analytic modeling is rather expensive and complicated. Hence, here we proposed a deep neural network based PCD data correction approach which directly maps imperfect data to the ideal data in the supervised learning mode. In this work, we first establish a complete simulation model incorporating the charge splitting and pulse pile-up effects. The simulated PCD data and the ground truth counterparts are then fed to a specially designed deep adversarial network for PCD data correction. Next, the trained network is used to correct separately generated PCD data. The test results demonstrate that the trained network successfully recovers the ideal spectrum from the distorted measurement within $\pm6\%$ relative error. Significant data and image fidelity improvements are clearly observed in both projection and reconstruction domains.      
### 62.ARC-Net: Activity Recognition Through Capsules  [ :arrow_down: ](https://arxiv.org/pdf/2007.03063.pdf)
>  Human Activity Recognition (HAR) is a challenging problem that needs advanced solutions than using handcrafted features to achieve a desirable performance. Deep learning has been proposed as a solution to obtain more accurate HAR systems being robust against noise. In this paper, we introduce ARC-Net and propose the utilization of capsules to fuse the information from multiple inertial measurement units (IMUs) to predict the activity performed by the subject. We hypothesize that this network will be able to tune out the unnecessary information and will be able to make more accurate decisions through the iterative mechanism embedded in capsule networks. We provide heatmaps of the priors, learned by the network, to visualize the utilization of each of the data sources by the trained network. By using the proposed network, we were able to increase the accuracy of the state-of-the-art approaches by 2%. Furthermore, we investigate the directionality of the confusion matrices of our results and discuss the specificity of the activities based on the provided data.      
### 63.Revealing the limits of single-particle imaging with orientation disconcurrence  [ :arrow_down: ](https://arxiv.org/pdf/2007.03054.pdf)
>  We propose a simple cryptographic scheme for validating diffraction intensity volumes reconstructed using single-particle imaging (SPI) with x-ray free-electron lasers (XFELs) when the ground truth volume is absent. This scheme is based on each reconstructed volumes' ability to decipher the orientations of unseen sentinel diffraction patterns. Here we quantify measures of orientation disconcurrence, inconsistency, and disagreement of this decipherment between two independently reconstructed volumes. We also study how these measures can be used to define data sufficiency and its relation to spatial resolution. This scheme further demonstrates that focusing XFEL pulses into smaller areas yields better reconstruction results when the total number of measured photons in all patterns is constrained, and if the number of patterns sufficiently cover the possible rotations. Importantly, this scheme overcomes critical ambiguities in using Fourier Shell Correlation (FSC) as a validation measure for SPI. Ultimately, we hope our work can inspire information-theoretic reformulations of the resolving power of XFEL-SPI, which will in turn lead to principled frameworks for experiment and instrument design.      
### 64.Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models  [ :arrow_down: ](https://arxiv.org/pdf/2007.03051.pdf)
>  Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.      
### 65.Nonlinear Transform Coding  [ :arrow_down: ](https://arxiv.org/pdf/2007.03034.pdf)
>  We review a class of methods that can be collected under the name nonlinear transform coding (NTC), which over the past few years have become competitive with the best linear transform codecs for images, and have superseded them in terms of rate--distortion performance under established perceptual quality metrics such as MS-SSIM. We assess the empirical rate--distortion performance of NTC with the help of simple example sources, for which the optimal performance of a vector quantizer is easier to estimate than with natural data sources. To this end, we introduce a novel variant of entropy-constrained vector quantization. We provide an analysis of various forms of stochastic optimization techniques for NTC models; review architectures of transforms based on artificial neural networks, as well as learned entropy models; and provide a direct comparison of a number of methods to parameterize the rate--distortion trade-off of nonlinear transforms, introducing a new one.      
### 66.Optimal Dynamic Mechanism Design with Stochastic Supply and Flexible Consumers  [ :arrow_down: ](https://arxiv.org/pdf/2007.03007.pdf)
>  We consider the problem of designing an expected-revenue maximizing mechanism for allocating multiple non-perishable goods of $k$ varieties to flexible consumers over $T$ time steps. In our model, a random number of goods of each variety may become available to the seller at each time and a random number of consumers may enter the market at each time. Each consumer is present in the market for one time step and wants to consume one good of one of its desired varieties. Each consumer is associated with a flexibility level that indicates the varieties of the goods it is equally interested in. A consumer's flexibility level and the utility it gets from consuming a good of its desired varieties are its private information. We characterize the allocation rule for a Bayesian incentive compatible, individually rational and expected revenue maximizing mechanism in terms of the solution to a dynamic program. The corresponding payment function is also specified in terms of the optimal allocation function. We leverage the structure of the consumers' flexibility model to simplify the dynamic program and provide an alternative description of the optimal mechanism in terms of thresholds computed by the dynamic program.      
### 67.Spatiotemporal Flexible Sparse Reconstruction for Rapid Dynamic Contrast-enhanced MRI  [ :arrow_down: ](https://arxiv.org/pdf/2007.02937.pdf)
>  Dynamic Contrast-enhanced magnetic resonance imaging (DCE-MRI) is a tissue perfusion imaging technique. Some versatile free-breathing DCE-MRI techniques combining compressed sensing (CS) and parallel imaging with golden-angle radial sampling have been developed to improve motion robustness with high spatial and temporal resolution. These methods have demonstrated good diagnostic performance in clinical setting, but the reconstruction quality will degrade at high acceleration rates and overall reconstruction time remains long. In this paper, we proposed a new parallel CS reconstruction model for DCE-MRI that enforces flexible weighted sparse constraint along both spatial and temporal dimensions. Weights were introduced to flexibly adjust the importance of time and space sparsity, and we derived a fast thresholding algorithm which was proven to be simple and efficient for solving the proposed reconstruction model. Results on in vivo liver DCE datasets show that the proposed method outperforms the state-of-the-art methods in terms of visual image quality assessment and reconstruction speed without introducing significant temporal blurring.      
### 68.Optimizing Information Freshness in Two-Hop Status Update Systems under a Resource Constraint  [ :arrow_down: ](https://arxiv.org/pdf/2007.02531.pdf)
>  This paper considers a two-hop status update system, in which an information source aims for the timely delivery of status updates to the destination with the aid of a relay. The relay is assumed to be an energy-constraint device and our goal is to devise scheduling policies that adaptively switch between the information decoding and information forwarding to minimize the long-term average Age-of-Information (AoI) at the destination, under a resource constraint on the average number of forwarding operations at the relay. We first identify an optimal scheduling policy by modelling the considered scheduling problem as a constrained Markov decision process (CMDP) problem. We resolve the CMDP problem by transforming it into an unconstrained Markov decision process (MDP) using a Lagrangian method. The structural properties of the optimal scheduling policy is analyzed, which is shown to have a multiple threshold structure. For implementation simplicity, based on the structural properties of the CMDP-based policy, we then propose a low-complexity double threshold relaying (DTR) policy with only two thresholds, one for relay's age and the other one for the age gain between destination and relay. We manage to derive approximate closed-form expressions of the average AoI at the destination, and the average number of forwarding operations at the relay for the DTR policy, by modelling the tangled evolution of age at the relay and destination as a Markov chain (MC). Numerical results are provided to verify all the theoretical analysis, and show that the low-complexity DTR policy can achieve near optimal performance compared with the optimal scheduling policy derived from the CMDP problem. The simulation results also unveil that only one threshold for the relay's age is needed in the DTR policy when there is no resource constraint or the resource constraint is loose.      
