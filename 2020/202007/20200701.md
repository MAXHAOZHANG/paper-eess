# ArXiv eess --Wed, 1 Jul 2020
### 1.Coupled Tensor Decomposition for Hyperspectral and Multispectral Image Fusion with Variability  [ :arrow_down: ](https://arxiv.org/pdf/2006.16968.pdf)
>  Coupled tensor approximation has recently emerged as a promising approach for the fusion of hyperspectral and multispectral images, reconciling state of the art performance with strong theoretical guarantees. However, tensor-based approaches previously proposed assume that the different observed images are acquired under exactly the same conditions. A recent work proposed to accommodate spectral variability in the image fusion problem using a matrix factorization-based formulation, but did not account for spatially-localized variations. Moreover, it lacks theoretical guarantees and has a high associated computational complexity. In this paper, we consider the image fusion problem while accounting for both spatially and spectrally localized changes in an additive model. We first study how the general identifiability of the model is impacted by the presence of such changes. Then, assuming that the high-resolution image and the variation factors admit a Tucker decomposition, two new algorithms are proposed -- one purely algebraic, and another based on an optimization procedure. Theoretical guarantees for the exact recovery of the high-resolution image are provided for both algorithms. Experimental results show that the proposed method outperforms state-of-the-art methods in the presence of spectral and spatial variations between the images, at a smaller computational cost.      
### 2.User Selection in Millimeter Wave Massive MIMO System using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2006.16854.pdf)
>  A hybrid architecture for millimeter wave (mmW) massive MIMO systems is considered practically implementable due to low power consumption and high energy efficiency. However, due to the limited number of RF chains, user selection becomes necessary for such architecture. Traditional user selection algorithms suffer from high computational complexity and, therefore, may not be scalable in 5G and beyond wireless mobile communications. To address this issue, in this letter we propose a low complexity CNN framework for user selection. The proposed CNN accepts as input the channel matrix and gives as output the selected users. Simulation results show that the proposed CNN performs close to optimal exhaustive search in terms of achievable rate, with negligible computational complexity. In addition, CNN based user selection outperforms the evolutionary algorithm and the greedy algorithm in terms of both achievable rate and computational complexity. Finally, simulation results also show that the proposed CNN based user selection scheme is robust to channel imperfections.      
### 3.Application of statistical analysis to working memory problem  [ :arrow_down: ](https://arxiv.org/pdf/2006.16850.pdf)
>  This article is devoted to EEG studying of connectivity cortical areas involved in keeping vision information in working memory. VAR-modeling was used for describing signals got from connected with working memory brain zones. Brain connections were estimated by based in Granger Causality Partial Directed Coherence (PDC) and then compared by Wilcoxon signed-rank test. In paper connection intensity dependence on executing task was found.      
### 4.Modeling and Uncertainty Analysis of Groundwater Level Using Six Evolutionary Optimization Algorithms Hybridized with ANFIS, SVM, and ANN  [ :arrow_down: ](https://arxiv.org/pdf/2006.16848.pdf)
>  In the present study, six meta-heuristic schemes are hybridized with artificial neural network (ANN), adaptive neuro-fuzzy interface system (ANFIS), and support vector machine (SVM), to predict monthly groundwater level (GWL), evaluate uncertainty analysis of predictions and spatial variation analysis. The six schemes, including grasshopper optimization algorithm (GOA), cat swarm optimization (CSO), weed algorithm (WA), genetic algorithm (GA), krill algorithm (KA), and particle swarm optimization (PSO), were used to hybridize for improving the performance of ANN, SVM, and ANFIS models. Groundwater level (GWL) data of Ardebil plain (Iran) for a period of 144 months were selected to evaluate the hybrid models. The pre-processing technique of principal component analysis (PCA) was applied to reduce input combinations from monthly time series up to 12-month prediction intervals. The results showed that the ANFIS-GOA was superior to the other hybrid models for predicting GWL in the first piezometer and third piezometer in the testing stage. The performance of hybrid models with optimization algorithms was far better than that of classical ANN, ANFIS, and SVM models without hybridization. The percent of improvements in the ANFIS-GOA versus standalone ANFIS in piezometer 10 were 14.4%, 3%, 17.8%, and 181% for RMSE, MAE, NSE, and PBIAS in the training stage and 40.7%, 55%, 25%, and 132% in testing stage, respectively. The improvements for piezometer 6 in train step were 15%, 4%, 13%, and 208% and in the test step were 33%, 44.6%, 16.3%, and 173%, respectively, that clearly confirm the superiority of developed hybridization schemes in GWL modeling. Uncertainty analysis showed that ANFIS-GOA and SVM had, respectively, the best and worst performances among other models. In general, GOA enhanced the accuracy of the ANFIS, ANN, and SVM models.      
### 5.A GRU-based Mixture Density Network for Data-Driven Dynamic Stochastic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2006.16845.pdf)
>  The conventional deep learning approaches for solving time-series problem such as long-short term memory (LSTM) and gated recurrent unit (GRU) both consider the time-series data sequence as the input with one single unit as the output (predicted time-series result). Those deep learning approaches have made tremendous success in many time-series related problems, however, this cannot be applied in data-driven stochastic programming problems since the output of either LSTM or GRU is a scalar rather than probability distribution which is required by stochastic programming model. To fill the gap, in this work, we propose an innovative data-driven dynamic stochastic programming (DD-DSP) framework for time-series decision-making problem, which involves three components: GRU, Gaussian Mixture Model (GMM) and SP. Specifically, we devise the deep neural network that integrates GRU and GMM which is called GRU-based Mixture Density Network (MDN), where GRU is used to predict the time-series outcomes based on the recent historical data, and GMM is used to extract the corresponding probability distribution of predicted outcomes, then the results will be input as the parameters for SP. To validate our approach, we apply the framework on the car-sharing relocation problem. The experiment validations show that our framework is superior to data-driven optimization based on LSTM with the vehicle average moving lower than LSTM.      
### 6.Machine learning in problems of automation of ultrasound diagnostics of railway tracks  [ :arrow_down: ](https://arxiv.org/pdf/2006.16844.pdf)
>  The article presents the system architecture for automatic decoding of railway track defectograms in real time. The system includes an ultrasound data preprocessing module, a set of neutral network classifiers, a decision block. Preprocessing of data includes affine transformations of measurement information into a format suitable for the operation of a neural network, as well as a combination of information on measurement channels, depending on the type of defect being defined. The classifier is built on a convolutional neural network. The proposed solution can be effectively implemented on a modern elemental basis for performing parallel computing, including tensor processor and GPUs.      
### 7.Siting thousands of radio transmitter towers on terrains with billions of points  [ :arrow_down: ](https://arxiv.org/pdf/2006.16783.pdf)
>  This paper presents a system that sites (finds optimal locations for) thousands of radio transmitter towers on terrains of up to two billion elevation posts. Applications include cellphone towers, camera systems, or even mitigating environmental visual nuisances. The transmitters and receivers may be situated above the terrain. The system has been parallelized with OpenMP to run on a multicore CPU.      
### 8.Image-level Harmonization of Multi-Site Data using Image-and-Spatial Transformer Networks  [ :arrow_down: ](https://arxiv.org/pdf/2006.16741.pdf)
>  We investigate the use of image-and-spatial transformer networks (ISTNs) to tackle domain shift in multi-site medical imaging data. Commonly, domain adaptation (DA) is performed with little regard for explainability of the inter-domain transformation and is often conducted at the feature-level in the latent space. We employ ISTNs for DA at the image-level which constrains transformations to explainable appearance and shape changes. As proof-of-concept we demonstrate that ISTNs can be trained adversarially on a classification problem with simulated 2D data. For real-data validation, we construct two 3D brain MRI datasets from the Cam-CAN and UK Biobank studies to investigate domain shift due to acquisition and population differences. We show that age regression and sex classification models trained on ISTN output improve generalization when training on data from one and testing on the other site.      
### 9.Deep Learning Based Load Balancing for improved QoS towards 6G  [ :arrow_down: ](https://arxiv.org/pdf/2006.16733.pdf)
>  Deep learning has made great strides lately with the availability of powerful computing machines and the advent of user-friendly programming environments. It is anticipated that the deep learning algorithms will entirely provision the majority of operations in 6G. One such environment where deep learning can be the right solution is load balancing in future 6G intelligent wireless networks. Load balancing presents an efficient, cost-effective method to improve the data process capability, throughput, and expand the bandwidth, thus enhancing the adaptability and availability of networks. Hence a load balancing algorithm based on Long Short Term Memory(LSTM) deep neural network is proposed through which the coverage area of base station changes according to geographic traffic distribution, catering the requirement for future generation 6G heterogeneous network. The LSTM model performance is evaluated by considering three different scenarios, and the results were presented. Load variance coefficient(LVC) and load factor(LF) are introduced and validated over two wireless network layouts(WNL) to study the Quality of Service(QoS) and load distribution. The proposed method shows a decrease of LVC by 98.311% and 99.21% for WNL1, WNL2 respectively.      
### 10.Complex Sparse Signal Recovery with Adaptive Laplace Priors  [ :arrow_down: ](https://arxiv.org/pdf/2006.16720.pdf)
>  Because of its self-regularizing nature and uncertainty estimation, the Bayesian approach has achieved excellent recovery performance across a wide range of sparse signal recovery applications. However, most methods are based on the real-value signal model, with the complex-value signal model rarely considered. Typically, the complex signal model is adopted so that phase information can be utilized. Therefore, it is non-trivial to develop Bayesian models for the complex-value signal model. Motivated by the adaptive least absolute shrinkage and selection operator (LASSO) and the sparse Bayesian learning (SBL) framework, a hierarchical model with adaptive Laplace priors is proposed for applications of complex sparse signal recovery in this paper. The proposed hierarchical Bayesian framework is easy to extend for the case of multiple measurement vectors. Moreover, the space alternating principle is integrated into the algorithm to avoid using the matrix inverse operation. In the experimental section of this work, the proposed algorithm is concerned with both complex Gaussian random dictionaries and directions of arrival (DOA) estimations. The experimental results show that the proposed algorithm offers better sparsity recovery performance than the state-of-the-art methods for different types of complex signals.      
### 11.Gaussian Process Repetitive Control for Suppressing Spatial Disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2006.16719.pdf)
>  Motion systems are often subject to disturbances such as cogging, commutation errors, and imbalances, that vary with velocity and appear periodic in time for constant operating velocities. The aim of this paper is to develop a repetitive controller (RC) for disturbances that are not periodic in the time domain, yet occur due to an identical position-domain disturbance. A new spatial RC framework is developed, allowing to attenuate disturbances that are periodic in the position domain but manifest a-periodic in the time domain. A Gaussian process (GP) based memory is employed with a suitable periodic kernel that can effectively deal with the intermittent observations inherent to the position domain. A mechatronic example confirms the potential of the method.      
### 12.A Speech Enhancement Algorithm based on Non-negative Hidden Markov Model and Kullback-Leibler Divergence  [ :arrow_down: ](https://arxiv.org/pdf/2006.16689.pdf)
>  In this paper, we propose a novel supervised single-channel speech enhancement method combing the the Kullback-Leibler divergence-based non-negative matrix factorization (NMF) and hidden Markov model (NMF-HMM). With the application of HMM, the temporal dynamics information of speech signals can be taken into account. In the training stage, the sum of Poisson, leading to the KL divergence measure, is used as the observation model for each state of HMM. This ensures that a computationally efficient multiplicative update can be used for the parameter update of the proposed model. In the online enhancement stage, we propose a novel minimum mean-square error (MMSE) estimator for the proposed NMF-HMM. This estimator can be implemented using parallel computing, saving the time complexity. The performance of the proposed algorithm is verified by objective measures. The experimental results show that the proposed strategy achieves better speech enhancement performance than state-of-the-art speech enhancement methods. More specifically, compared with the traditional NMF-based speech enhancement methods, our proposed algorithm achieves a 5\% improvement for short-time objective intelligibility (STOI) and 0.18 improvement for perceptual evaluation of speech quality (PESQ).      
### 13.Needle tip force estimation by deep learning from raw spectral OCT data  [ :arrow_down: ](https://arxiv.org/pdf/2006.16675.pdf)
>  Purpose. Needle placement is a challenging problem for applications such as biopsy or brachytherapy. Tip force sensing can provide valuable feedback for needle navigation inside the tissue. For this purpose, fiber-optical sensors can be directly integrated into the needle tip. Optical coherence tomography (OCT) can be used to image tissue. Here, we study how to calibrate OCT to sense forces, e.g. during robotic needle placement. <br>Methods. We investigate whether using raw spectral OCT data without a typical image reconstruction can improve a deep learning-based calibration between optical signal and forces. For this purpose, we consider three different needles with a new, more robust design which are calibrated using convolutional neural networks (CNNs). We compare training the CNNs with the raw OCT signal and the reconstructed depth profiles. <br>Results. We find that using raw data as an input for the largest CNN model outperforms the use of reconstructed data with a mean absolute error of 5.81 mN compared to 8.04 mN. <br>Conclusions. We find that deep learning with raw spectral OCT data can improve learning for the task of force estimation. Our needle design and calibration approach constitute a very accurate fiber-optical sensor for measuring forces at the needle tip.      
### 14.Real Elliptically Skewed Distributions and Their Application to Robust Cluster Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2006.16671.pdf)
>  This article proposes a new class of Real Elliptically Skewed (RESK) distributions and associated clustering algorithms that allow for integrating robustness and skewness into a single unified cluster analysis framework. Non-symmetrically distributed and heavy-tailed data clusters have been reported in a variety of real-world applications. Robustness is essential because a few outlying observations can severely obscure the cluster structure. The RESK distributions are a generalization of the Real Elliptically Symmetric (RES) distributions. To estimate the cluster parameters and memberships, we derive an expectation maximization (EM) algorithm for arbitrary RESK distributions. Special attention is given to a new robust skew-Huber M-estimator, which is also the maximum likelihood estimator (MLE) for the skew-Huber distribution that belongs to the RESK class. Numerical experiments on simulated and real-world data confirm the usefulness of the proposed methods for skewed and heavy-tailed data sets.      
### 15.Verification of Initial-State Opacity for Switched Systems: A Compositional Approach  [ :arrow_down: ](https://arxiv.org/pdf/2006.16661.pdf)
>  The security in information-flow has become a major concern for cyber-physical systems (CPSs). In this work, we focus on the analysis of an information-flow security property, called opacity. Opacity characterizes the plausible deniability of a system's secret in the presence of a malicious outside intruder. We propose a methodology of checking a notion of opacity, called approximate initial-state opacity, for networks of discrete-time switched systems. Our framework relies on compositional constructions of finite abstractions for networks of switched systems and their so-called approximate initial-state opacity-preserving simulation functions (InitSOPSFs). Those functions characterize how close concrete networks and their finite abstractions are in terms of the satisfaction of approximate initial-state opacity. We show that such InitSOPSFs can be obtained compositionally by assuming some small-gain type conditions and composing so-called local InitSOPSFs constructed for each subsystem separately. Additionally, assuming certain stability property of switched systems, we also provide a technique on constructing their finite abstractions together with the corresponding local InitSOPSFs. Finally, we illustrate the effectiveness of our results through an example.      
### 16.Delayed Q-update: A novel credit assignment technique for deriving an optimal operation policy for the Grid-Connected Microgrid  [ :arrow_down: ](https://arxiv.org/pdf/2006.16659.pdf)
>  A microgrid is an innovative system that integrates distributed energy resources to supply electricity demand within electrical boundaries. This study proposes an approach for deriving a desirable microgrid operation policy that enables sophisticated controls in the microgrid system using the proposed novel credit assignment technique, delayed-Q update. The technique employs novel features such as the ability to tackle and resolve the delayed effective property of the microgrid, which prevents learning agents from deriving a well-fitted policy under sophisticated controls. The proposed technique tracks the history of the charging period and retroactively assigns an adjusted value to the ESS charging control. The operation policy derived using the proposed approach is well-fitted for the real effects of ESS operation because of the process of the technique. Therefore, it supports the search for a near-optimal operation policy under a sophisticatedly controlled microgrid environment. To validate our technique, we simulate the operation policy under a real-world grid-connected microgrid system and demonstrate the convergence to a near-optimal policy by comparing performance measures of our policy with benchmark policy and optimal policy.      
### 17.Beamspace Channel Estimation in Terahertz Communications: A Model-Driven Unsupervised Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2006.16628.pdf)
>  Terahertz (THz)-band communications have been one of the promising technologies for future wireless networks that integrate a wide range of data-demanding applications. To compensate for the large channel attenuation in THz band and avoid high hardware cost, a lens-based beamspace massive multiple-input multiple-output (MIMO) system is considered. However, the beam squint effect appeared in wideband THz systems, making channel estimation very challenging, especially when the receiver is equipped with a limited number of radio-frequency (RF) chains. Furthermore, the real channel data cannot be obtained before the THz system is used in a new environment, which makes it impossible to train a deep learning (DL)-based channel estimator using real data set beforehand. To solve the problem, we propose a model-driven unsupervised learning network, named learned denoising-based generalized expectation consistent (LDGEC) signal recovery network. By utilizing the Steins unbiased risk estimator loss, the LDGEC network can be trained only with limited measurements corresponding to the pilot symbols, instead of the real channel data. Even if designed for unsupervised learning, the LDGEC network can be supervisingly trained with the real channel via the denoiser-by-denoiser way. The numerical results demonstrate that the LDGEC-based channel estimator significantly outperforms state-of-the-art compressive sensing-based algorithms when the receiver is equipped with a small number of RF chains and low-resolution ADCs.      
### 18.BitMix: Data Augmentation for Image Steganalysis  [ :arrow_down: ](https://arxiv.org/pdf/2006.16625.pdf)
>  Convolutional neural networks (CNN) for image steganalysis demonstrate better performances with employing concepts from high-level vision tasks. The major employed concept is to use data augmentation to avoid overfitting due to limited data. To augment data without damaging the message embedding, only rotating multiples of 90 degrees or horizontally flipping are used in steganalysis, which generates eight fixed results from one sample. To overcome this limitation, we propose BitMix, a data augmentation method for spatial image steganalysis. BitMix mixes a cover and stego image pair by swapping the random patch and generates an embedding adaptive label with the ratio of the number of pixels modified in the swapped patch to those in the cover-stego pair. We explore optimal hyperparameters, the ratio of applying BitMix in the mini-batch, and the size of the bounding box for swapping patch. The results reveal that using BitMix improves the performance of spatial image steganalysis and better than other data augmentation methods.      
### 19.A Novel Bistatic Joint Radar-Communication System in Multi-path Environments  [ :arrow_down: ](https://arxiv.org/pdf/2006.16591.pdf)
>  Radar detection and communication can be operated simultaneously in joint radar-communication (JRC) system. In this paper, we propose a bistatic JRC system which is applicable in multi-path environments. Basing on a novel joint waveform, a joint detection process is designed for both target detection and channel estimation. Meanwhile, a low-cost channel equalization method that utilizes the channel state information acquired from the detection process is proposed. The numerical results show that the symbol error rate (SER) of the proposed system is similar to that of the binary frequency shift keying system, and the signal to noise ratio requirement in multi-path environments is less than 2 dB higher compared with that in single-path environment to reach a SER of 10-5. Besides, the knowledge of the embedded information is not required for the joint detection process and the detection performance is robust to unknown information.      
### 20.Pan-Sharpening with Color-Aware Perceptual Loss and Guided Re-Colorization  [ :arrow_down: ](https://arxiv.org/pdf/2006.16583.pdf)
>  We present a novel color-aware perceptual (CAP) loss for learning the task of pan-sharpening. Our CAP loss is designed to focus on the deep features of a pre-trained VGG network that are more sensitive to spatial details and ignore color information to allow the network to extract the structural information from the PAN image while keeping the color from the lower resolution MS image. Additionally, we propose "guided re-colorization", which generates a pan-sharpened image with real colors from the MS input by "picking" the closest MS pixel color for each pan-sharpened pixel, as a human operator would do in manual colorization. Such a re-colorized (RC) image is completely aligned with the pan-sharpened (PS) network output and can be used as a self-supervision signal during training, or to enhance the colors in the PS image during test. We present several experiments where our network trained with our CAP loss generates naturally looking pan-sharpened images with fewer artifacts and outperforms the state-of-the-arts on the WorldView3 dataset in terms of ERGAS, SCC, and QNR metrics.      
### 21.Early Exit Or Not: Resource-Efficient Blind Quality Enhancement for Compressed Images  [ :arrow_down: ](https://arxiv.org/pdf/2006.16581.pdf)
>  Lossy image compression is pervasively conducted to save communication bandwidth, resulting in undesirable compression artifacts. Recently, extensive approaches have been proposed to reduce image compression artifacts at the decoder side; however, they require a series of architecture-identical models to process images with different quality, which are inefficient and resource-consuming. Besides, it is common in practice that compressed images are with unknown quality and it is intractable for existing approaches to select a suitable model for blind quality enhancement. In this paper, we propose a resource-efficient blind quality enhancement (RBQE) approach for compressed images. Specifically, our approach blindly and progressively enhances the quality of compressed images through a dynamic deep neural network (DNN), in which an early-exit strategy is embedded. Then, our approach can automatically decide to terminate or continue enhancement according to the assessed quality of enhanced images. Consequently, slight artifacts can be removed in a simpler and faster process, while the severe artifacts can be further removed in a more elaborate process. Extensive experiments demonstrate that our RBQE approach achieves state-of-the-art performance in terms of both blind quality enhancement and resource efficiency.      
### 22.Hand-drawn Symbol Recognition of Surgical Flowsheet Graphs with Deep Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2006.16546.pdf)
>  Perioperative data are essential to investigating the causes of adverse surgical outcomes. In some low to middle income countries, these data are computationally inaccessible due to a lack of digitization of surgical flowsheets. In this paper, we present a deep image segmentation approach using a U-Net architecture that can detect hand-drawn symbols on a flowsheet graph. The segmentation mask outputs are post-processed with techniques unique to each symbol to convert into numeric values. The U-Net method can detect, at the appropriate time intervals, the symbols for heart rate and blood pressure with over 99 percent accuracy. Over 95 percent of the predictions fall within an absolute error of five when compared to the actual value. The deep learning model outperformed template matching even with a small size of annotated images available for the training set.      
### 23.Design and Analysis of LoS MIMO Systems with Uniform Circular Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2006.16515.pdf)
>  We consider the design of a uniform circular array (UCA) based multiple-input multiple-output (MIMO) system over line-of-sight (LoS) environments in which array misalignment exists. In particular, optimal antenna placement in UCAs and transceiver architectures to achieve the maximum channel capacity without the knowledge of misalignment components are presented. To this end, we first derive a generic channel model of UCA-based LoS MIMO systems in which three misalignment factors including relative array rotation, tilting and center-shift are reflected concurrently. By factorizing the channel matrix into the singular value decomposition (SVD) form, we demonstrate that the singular values of UCA-based LoS MIMO systems are \textit{independent} of tilting and center-shift. Rather, they can be expressed as a function of the \textit{radii product-to-distance ratio} (RPDR) and the angle of relative array rotation. Numerical analyses of singular values show that the RPDR is a key design parameter of UCA systems. Based on this result, we propose an optimal design method for UCA systems which performs a one-dimensional search of RPDR to maximize channel capacity. It is observed that the channel matrix of the optimally designed UCA system is close to an orthogonal matrix; this fact allows channel capacity to be achieved by a simple zero-forcing (ZF) receiver. Additionally, we propose a low-complexity precoding scheme for UCA systems in which the optimal design criteria cannot be fulfilled because of limits on array size. The simulation results demonstrate the validity of the proposed design method and transceiver architectures.      
### 24.Hierarchical Temporal and Spatial Clustering of Uncertain and Time-varying Load Models  [ :arrow_down: ](https://arxiv.org/pdf/2006.16493.pdf)
>  Load modeling is difficult due to its uncertain and time-varying properties. Through the recently proposed ambient signals load modeling approach, these properties can be more frequently tracked. However, the large dataset of load modeling results becomes a new problem. In this paper, a hierarchical temporal and spatial clustering method of load models is proposed, after which the large size load model dataset can be represented by several representative load models (RLMs). In the temporal clustering stage, the RLMs of one load bus are picked up through clustering to represent all the load models of the load bus at different time. In the spatial clustering stage, the RLMs of all the load buses form a new set and the RLMs of the system are picked up through spatial clustering. In this way, the large sets of load models are represented by a small number of RLMs, through which the storage space of the load models is significantly reduced. The validation results in IEEE 39 bus system have shown that the simulation accuracy can still be maintained after replacing the load models with the RLMs. In this way, the effectiveness of the proposed hierarchical clustering framework is validated.      
### 25.Dynamic Voltage Restorer (DVR) For Protecting Hybrid Grids  [ :arrow_down: ](https://arxiv.org/pdf/2006.16452.pdf)
>  Dynamic Voltage Restorer (DVR), for some important reasons, is utilized in Power Distribution for protecting loads especially critical loads against some power quality issues like sag, swell, harmonics and faults. Fast transient response for compensating voltage quality is an unavoidable factor. Different control topologies have been applied to this device. In this project, a DVR is deployed to preserve a critical load that is connected in parallel with the grid and A wind turbine. Several situations of power quality issues have been considered. The simulation results of a proposed system that has been implemented in MATLAB/Simulink show how DVR is effective for protecting loads.      
### 26.Cyclical Electromechanical Error Denial System Using Matrix Profile  [ :arrow_down: ](https://arxiv.org/pdf/2006.16418.pdf)
>  We propose that the Matrix Profile data structure, conventionally applied to large scale time-series data mining, is applicable to the analysis and suppression of cyclical error in electromechanical systems, paving the way for an intelligent family of adaptable control systems which respond to environmental error at a computational cost low enough to be practical in embedded applications. We construct and evaluate the efficacy of a control algorithm utilizing the Matrix Profile, which we call Cyclical Electromechanical Error Denial System (CEEDS).      
### 27.Transition control of a tail-sitter UAV using recurrent neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2006.16401.pdf)
>  This paper presents the implementation of a Recurrent Neural Network (RNN) based-controller for the stabilization of the flight transition maneuver (hover-cruise and vice versa) of a tail-sitter UAV. The control strategy is based on attitude and velocity stabilization. For that aim, the RNN is used for the estimation of high nonlinear aerodynamic terms during the transition stage. Then, this estimate is used together with a feedback linearization technique for stabilizing the entire system. Results show convergence of linear velocities and the pitch angle during the transition maneuver. To analyze the performance of our proposed control strategy, we present simulations for the transition from hover to cruise and vice versa.      
### 28.Ultra2Speech -- A Deep Learning Framework for Formant Frequency Estimation and Tracking from Ultrasound Tongue Images  [ :arrow_down: ](https://arxiv.org/pdf/2006.16367.pdf)
>  Thousands of individuals need surgical removal of their larynx due to critical diseases every year and therefore, require an alternative form of communication to articulate speech sounds after the loss of their voice box. This work addresses the articulatory-to-acoustic mapping problem based on ultrasound (US) tongue images for the development of a silent-speech interface (SSI) that can provide them with an assistance in their daily interactions. Our approach targets automatically extracting tongue movement information by selecting an optimal feature set from US images and mapping these features to the acoustic space. We use a novel deep learning architecture to map US tongue images from the US probe placed beneath a subject's chin to formants that we call, Ultrasound2Formant (U2F) Net. It uses hybrid spatio-temporal 3D convolutions followed by feature shuffling, for the estimation and tracking of vowel formants from US images. The formant values are then utilized to synthesize continuous time-varying vowel trajectories, via Klatt Synthesizer. Our best model achieves R-squared (R^2) measure of 99.96% for the regression task. Our network lays the foundation for an SSI as it successfully tracks the tongue contour automatically as an internal representation without any explicit annotation.      
### 29.High-Fidelity Machine Learning Approximations of Large-Scale Optimal Power Flow  [ :arrow_down: ](https://arxiv.org/pdf/2006.16356.pdf)
>  The AC Optimal Power Flow (AC-OPF) is a key building block in many power system applications. It determines generator setpoints at minimal cost that meet the power demands while satisfying the underlying physical and operational constraints. It is non-convex and NP-hard, and computationally challenging for large-scale power systems. Motivated by the increased stochasticity in generation schedules and increasing penetration of renewable sources, this paper explores a deep learning approach to deliver highly efficient and accurate approximations to the AC-OPF. In particular, the paper proposes an integration of deep neural networks and Lagrangian duality to capture the physical and operational constraints. The resulting model, called OPF-DNN, is evaluated on real case studies from the French transmission system, with up to 3,400 buses and 4,500 lines. Computational results show that OPF-DNN produces highly accurate AC-OPF approximations whose costs are within 0.01% of optimality. OPF-DNN generates, in milliseconds, solutions that capture the problem constraints with high fidelity.      
### 30.Fourier DiffuserScope: Single-shot 3D Fourier light field microscopy with a diffuser  [ :arrow_down: ](https://arxiv.org/pdf/2006.16343.pdf)
>  Light field microscopy (LFM) uses a microlens array (MLA) near the sensor plane of a microscope to achieve single-shot 3D imaging of a sample, without any moving parts. Unfortunately, the 3D capability of LFM comes with a significant loss of lateral resolution at the focal plane, which is highly undesirable in microscopy. Placing the MLA near the pupil plane of the microscope, rather than the image plane, can mitigate the artifacts near focus and provide an efficient shift-invariant model at the expense of field-of-view. Here, we show that our Fourier DiffuserScope achieves significantly better performance than Fourier LFM. Fourier DiffuserScope uses a diffuser in the pupil plane to encode depth information, then reconstructs volumetric information computationally by solving a sparsity-constrained inverse problem. Our diffuser consists of randomly placed microlenses with varying focal lengths. We show that by randomizing the microlens positions, a larger lateral field-of-view can be achieved compared to a conventional MLA; furthermore, by adding diversity to the focal lengths, the axial depth range is increased. To predict system performance based on diffuser parameters, we for the first time establish a theoretical framework as a design guideline, followed by numerical simulations to verify. Under both theoretical and numerical analysis, we demonstrate that our diffuser design provides more uniform resolution over a larger volume, both laterally and axially, outperforming the MLA used in LFM. We build an experiment system with an optimized lens set and achieve &lt; 3 um lateral and 4 um axial resolution over a 1000 x 1000 x 280 um^3 volume.      
### 31.Estimation and Decomposition of Rack Force for Driving on Uneven Roads  [ :arrow_down: ](https://arxiv.org/pdf/2006.16319.pdf)
>  The force transmitted from the front tires to the steering rack of a vehicle, called the rack force, plays an important role in the function of electric power steering (EPS) systems. Estimates of rack force can be used by EPS to attenuate road feedback and reduce driver effort. Further, estimates of the components of rack force (arising, for example, due to steering angle and road profile) can be used to separately compensate for each component and thereby enhance steering feel. In this paper, we present three vehicle and tire model-based rack force estimators that utilize sensed steering angle and road profile to estimate total rack force and individual components of rack force. We test and compare the real-time performance of the estimators by performing driving experiments with non-aggressive and aggressive steering maneuvers on roads with low and high frequency profile variations. The results indicate that for aggressive maneuvers the estimators using non-linear tire models produce more accurate rack force estimates. Moreover, only the estimator that incorporates a semi-empirical Rigid Ring tire model is able to capture rack force variation for driving on a road with high frequency profile variation. Finally, we present results from a simulation study to validate the component-wise estimates of rack force.      
### 32.Rapid Transitions with Robust Accelerated Delayed Self Reinforcement for Consensus-based Multi Agent Networks  [ :arrow_down: ](https://arxiv.org/pdf/2006.16295.pdf)
>  Rapid transitions are important for quick response of consensus-based, multi-agent networks to external stimuli. While high-gain can increase response speed, potential instability tends to limit the maximum possible gain, and therefore, limits the maximum convergence rate to consensus during transitions. Since the update law for multi-agent networks with symmetric graphs can be considered as the gradient of its Laplacian-potential function, Nesterov-type accelerated gradient approaches from optimization theory, can further improve the convergence rate of such networks. An advantage of the accelerated-gradient approach is that it can be implemented using accelerated delayed-self-reinforcement (A-DSR), which does not require new information from the network nor modifications in the network connectivity. However, the accelerated-gradient approach is not directly applicable to non-symmetric graphs since the update law is not the gradient of the Laplacian-potential function. The main contributions of this work are to (i) extend the accelerated-gradient approach to general graph networks (whose Laplacians have real spectrum) using DSR, and (ii) develop analytical design criteria for a Robust A-DSR approach that maximizes both structural robustness and transition speed. Simulation results are presented to illustrate the performance improvement with the proposed Robust A-DSR of 40% in structural robustness and 50% in convergence rate to consensus, when compared to the case without the A-DSR. Moreover, experimental results are presented that show a similar 37% faster convergence with the Robust A-DSR when compared to the case without the A-DSR.      
### 33.TinyRadarNN: Combining Spatial and Temporal Convolutional Neural Networks for Embedded Gesture Recognition with Short Range Radars  [ :arrow_down: ](https://arxiv.org/pdf/2006.16281.pdf)
>  This work proposes a low-power high-accuracy embedded hand-gesture recognition algorithm targeting battery-operated wearable devices using low power short-range RADAR sensors. A 2D Convolutional Neural Network (CNN) using range frequency Doppler features is combined with a Temporal Convolutional Neural Network (TCN) for time sequence prediction. The final algorithm has a model size of only 46 thousand parameters, yielding a memory footprint of only 92 KB. Two datasets containing 11 challenging hand gestures performed by 26 different people have been recorded containing a total of 20,210 gesture instances. On the 11 hand gesture dataset, accuracies of 86.6% (26 users) and 92.4% (single user) have been achieved, which are comparable to the state-of-the-art, which achieves 87% (10 users) and 94% (single user), while using a TCN-based network that is 7500x smaller than the state-of-the-art. Furthermore, the gesture recognition classifier has been implemented on Parallel Ultra-Low Power Processor, demonstrating that real-time prediction is feasible with only 21 mW of power consumption for the full TCN sequence prediction network.      
### 34.PriorGAN: Real Data Prior for Generative Adversarial Nets  [ :arrow_down: ](https://arxiv.org/pdf/2006.16990.pdf)
>  Generative adversarial networks (GANs) have achieved rapid progress in learning rich data distributions. However, we argue about two main issues in existing techniques. First, the low quality problem where the learned distribution has massive low quality samples. Second, the missing modes problem where the learned distribution misses some certain regions of the real data distribution. To address these two issues, we propose a novel prior that captures the whole real data distribution for GANs, which are called PriorGANs. To be specific, we adopt a simple yet elegant Gaussian Mixture Model (GMM) to build an explicit probability distribution on the feature level for the whole real data. By maximizing the probability of generated data, we can push the low quality samples to high quality. Meanwhile, equipped with the prior, we can estimate the missing modes in the learned distribution and design a sampling strategy on the real data to solve the problem. The proposed real data prior can generalize to various training settings of GANs, such as LSGAN, WGAN-GP, SNGAN, and even the StyleGAN. Our experiments demonstrate that PriorGANs outperform the state-of-the-art on the CIFAR-10, FFHQ, LSUN-cat, and LSUN-bird datasets by large margins.      
### 35.Recovering Joint Probability of Discrete Random Variables from Pairwise Marginals  [ :arrow_down: ](https://arxiv.org/pdf/2006.16912.pdf)
>  Learning the joint probability of random variables (RVs) lies at the heart of statistical signal processing and machine learning. However, direct nonparametric estimation for high-dimensional joint probability is in general impossible, due to the curse of dimensionality. Recent work has proposed to recover the joint probability mass function (PMF) of an arbitrary number of RVs from three-dimensional marginals, leveraging the algebraic properties of low-rank tensor decomposition and the (unknown) dependence among the RVs. Nonetheless, accurately estimating three-dimensional marginals can still be costly in terms of sample complexity, affecting the performance of this line of work in practice in the sample-starved regime. Using three-dimensional marginals also involves challenging tensor decomposition problems whose tractability is unclear. This work puts forth a new framework for learning the joint PMF using only pairwise marginals, which naturally enjoys a lower sample complexity relative to the third-order ones. A coupled nonnegative matrix factorization (CNMF) framework is developed, and its joint PMF recovery guarantees under various conditions are analyzed. Our method also features a Gram-Schmidt (GS)-like algorithm that exhibits competitive runtime performance. The algorithm is shown to provably recover the joint PMF up to bounded error in finite iterations, under reasonable conditions. It is also shown that a recently proposed economical expectation maximization (EM) algorithm guarantees to improve upon the GS-like algorithm's output, thereby further lifting up the accuracy and efficiency. Real-data experiments are employed to showcase the effectiveness.      
### 36.Dragoon: Advanced Modelling of IP Geolocation by use of Latency Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2006.16895.pdf)
>  IP Geolocation is a key enabler for many areas of application like determination of an attack origin, targeted advertisement, and Content Delivery Networks. Although IP Geolocation is an ongoing field of research for over one decade, it is still a challenging task, whereas good results are only achieved by the use of active latency measurements. Nevertheless, an increased accuracy is needed to improve service quality. This paper presents an novel approach to find optimized Landmark positions which are used for active probing. Since a reasonable Landmark selection is important for a highly accurate localization service, the goal is to find Landmarks close to the target with respect to the infrastructure and hop count. Furthermore, we introduce a new approach of an adaptable and more accurate mathematical modelling of an improved geographical location estimation process. Current techniques provide less information about solving the Landmark problem as well as are using imprecise models. We demonstrate the usability of our approach in a real-world environment and analyse Geolocation for the first time in Europe. The combination of an optimized Landmark selection and advanced modulation results in an improved accuracy of IP Geolocation.      
### 37.QoE Based Revenue Maximizing Dynamic Resource Allocation and Pricing for Fog-Enabled Mission-Critical IoT Applications  [ :arrow_down: ](https://arxiv.org/pdf/2006.16894.pdf)
>  Fog computing is becoming a vital component for Internet of things (IoT) applications, acting as its computational engine. Mission-critical IoT applications are highly sensitive to latency, which depends on the physical location of the cloud server. Fog nodes of varying response rates are available to the cloud service provider (CSP) and it is faced with a challenge of forwarding the sequentially received IoT data to one of the fog nodes for processing. Since the arrival times and nature of requests is random, it is important to optimally classify the requests in real-time and allocate available virtual machine instances (VMIs) at the fog nodes to provide a high QoE to the users and consequently generate higher revenues for the CSP. In this paper, we use a pricing policy based on the QoE of the applications as a result of the allocation and obtain an optimal dynamic allocation rule based on the statistical information of the computational requests. The developed solution is statistically optimal, dynamic, and implementable in real-time as opposed to other static matching schemes in the literature. The performance of the proposed framework has been evaluated using simulations and the results show significant improvement as compared with benchmark schemes.      
### 38.Rapid Response Crop Maps in Data Sparse Regions  [ :arrow_down: ](https://arxiv.org/pdf/2006.16866.pdf)
>  Spatial information on cropland distribution, often called cropland or crop maps, are critical inputs for a wide range of agriculture and food security analyses and decisions. However, high-resolution cropland maps are not readily available for most countries, especially in regions dominated by smallholder farming (e.g., sub-Saharan Africa). These maps are especially critical in times of crisis when decision makers need to rapidly design and enact agriculture-related policies and mitigation strategies, including providing humanitarian assistance, dispersing targeted aid, or boosting productivity for farmers. A major challenge for developing crop maps is that many regions do not have readily accessible ground truth data on croplands necessary for training and validating predictive models, and field campaigns are not feasible for collecting labels for rapid response. We present a method for rapid mapping of croplands in regions where little to no ground data is available. We present results for this method in Togo, where we delivered a high-resolution (10 m) cropland map in under 10 days to facilitate rapid response to the COVID-19 pandemic by the Togolese government. This demonstrated a successful transition of machine learning applications research to operational rapid response in a real humanitarian crisis. All maps, data, and code are publicly available to enable future research and operational systems in data-sparse regions.      
### 39.Makespan minimization of Time-Triggered traffic on a TTEthernet network  [ :arrow_down: ](https://arxiv.org/pdf/2006.16863.pdf)
>  The reliability of the increasing number of modern applications and systems strongly depends on interconnecting technology. Complex systems which usually need to exchange, among other things, multimedia data together with safety-related information, as in the automotive or avionic industry, for example, make demands on both the high bandwidth and the deterministic behavior of the communication. TTEthernet is a protocol that has been developed to face these requirements while providing the generous bandwidth of Ethernet up to 1\,Gbit/s and enhancing its determinism by the Time-Triggered message transmission which follows the predetermined schedule. Therefore, synthesizing a good schedule which meets all the real-time requirements is essential for the performance of the whole system. <br>In this paper, we study the concept of creating the communication schedules for the Time-Triggered traffic while minimizing its makespan. The aim is to maximize the uninterrupted gap for remaining traffic classes in each integration cycle. The provided scheduling algorithm, based on the Resource-Constrained Project Scheduling Problem formulation and the load balancing heuristic, obtains near-optimal (within 15\% of non-tight lower bound) solutions in 5 minutes even for industrial sized instances. The universality of the provided method allows easily modify or extend the problem statement according to particular industrial demands. Finally, the studied concept of makespan minimization is justified through the concept of scheduling with porosity according to the worst-case delay analysis of Event-Triggered traffic.      
### 40.EasyQuant: Post-training Quantization via Scale Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2006.16669.pdf)
>  The 8 bits quantization has been widely applied to accelerate network inference in various deep learning applications. There are two kinds of quantization methods, training-based quantization and post-training quantization. Training-based approach suffers from a cumbersome training process, while post-training quantization may lead to unacceptable accuracy drop. In this paper, we present an efficient and simple post-training method via scale optimization, named EasyQuant (EQ),that could obtain comparable accuracy with the training-based method.Specifically, we first alternately optimize scales of weights and activations for all layers target at convolutional outputs to further obtain the high quantization precision. Then, we lower down bit width to INT7 both for weights and activations, and adopt INT16 intermediate storage and integer Winograd convolution implementation to accelerate inference.Experimental results on various computer vision tasks show that EQ outperforms the TensorRT method and can achieve near INT8 accuracy in 7 bits width post-training.      
### 41.Deep reinforcement learning approach to MIMO precoding problem: Optimality and Robustness  [ :arrow_down: ](https://arxiv.org/pdf/2006.16646.pdf)
>  In this paper, we propose a deep reinforcement learning (RL)-based precoding framework that can be used to learn an optimal precoding policy for complex multiple-input multiple-output (MIMO) precoding problems. We model the precoding problem for a single-user MIMO system as an RL problem in which a learning agent sequentially selects the precoders to serve the environment of MIMO system based on contextual information about the environmental conditions, while simultaneously adapting the precoder selection policy based on the reward feedback from the environment to maximize a numerical reward signal. We develop the RL agent with two canonical deep RL (DRL) algorithms, namely deep Q-network (DQN) and deep deterministic policy gradient (DDPG). To demonstrate the optimality of the proposed DRL-based precoding framework, we explicitly consider a simple MIMO environment for which the optimal solution can be obtained analytically and show that DQN- and DDPG-based agents can learn the near-optimal policy to map the environment state of MIMO system to a precoder that maximizes the reward function, respectively, in the codebook-based and non-codebook based MIMO precoding systems. Furthermore, to investigate the robustness of DRL-based precoding framework, we examine the performance of the two DRL algorithms in a complex MIMO environment, for which the optimal solution is not known. The numerical results confirm the effectiveness of the DRL-based precoding framework and show that the proposed DRL-based framework can outperform the conventional approximation algorithm in the complex MIMO environment.      
### 42.Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic Images via GANs  [ :arrow_down: ](https://arxiv.org/pdf/2006.16644.pdf)
>  Convolutional Neural Networks (CNN)-based approaches have shown promising results in pansharpening of satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared to existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas CNN-based methods provide a reduced resolution panchromatic image as input to their model along with reduced resolution multispectral images, hence learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as input, and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization Generative Adversarial Networks (PanColorGAN) framework, help overcome the spatial detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods as demonstrated in our experiments.      
### 43.Primary Tumor Origin Classification of Lung Nodules in Spectral CT using Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2006.16633.pdf)
>  Early detection of lung cancer has been proven to decrease mortality significantly. A recent development in computed tomography (CT), spectral CT, can potentially improve diagnostic accuracy, as it yields more information per scan than regular CT. However, the shear workload involved with analyzing a large number of scans drives the need for automated diagnosis methods. Therefore, we propose a detection and classification system for lung nodules in CT scans. Furthermore, we want to observe whether spectral images can increase classifier performance. For the detection of nodules we trained a VGG-like 3D convolutional neural net (CNN). To obtain a primary tumor classifier for our dataset we pre-trained a 3D CNN with similar architecture on nodule malignancies of a large publicly available dataset, the LIDC-IDRI dataset. Subsequently we used this pre-trained network as feature extractor for the nodules in our dataset. The resulting feature vectors were classified into two (benign/malignant) and three (benign/primary lung cancer/metastases) classes using support vector machine (SVM). This classification was performed both on nodule- and scan-level. We obtained state-of-the art performance for detection and malignancy regression on the LIDC-IDRI database. Classification performance on our own dataset was higher for scan- than for nodule-level predictions. For the three-class scan-level classification we obtained an accuracy of 78\%. Spectral features did increase classifier performance, but not significantly. Our work suggests that a pre-trained feature extractor can be used as primary tumor origin classifier for lung nodules, eliminating the need for elaborate fine-tuning of a new network and large datasets. Code is available at \url{<a class="link-external link-https" href="https://github.com/tueimage/lung-nodule-msc-2018" rel="external noopener nofollow">this https URL</a>}.      
### 44.Tackling Occlusion in Siamese Tracking with Structured Dropouts  [ :arrow_down: ](https://arxiv.org/pdf/2006.16571.pdf)
>  Occlusion is one of the most difficult challenges in object tracking to model. This is because unlike other challenges, where data augmentation can be of help, occlusion is hard to simulate as the occluding object can be anything in any shape. In this paper, we propose a simple solution to simulate the effects of occlusion in the latent space. Specifically, we present structured dropout to mimick the change in latent codes under occlusion. We present three forms of dropout (channel dropout, segment dropout and slice dropout) with the various forms of occlusion in mind. To demonstrate its effectiveness, the dropouts are incorporated into two modern Siamese trackers (SiamFC and SiamRPN++). The outputs from multiple dropouts are combined using an encoder network to obtain the final prediction. Experiments on several tracking benchmarks show the benefits of structured dropouts, while due to their simplicity requiring only small changes to the existing tracker models.      
### 45.Parameter Estimation of Fire Propagation Models Using Level Set Methods  [ :arrow_down: ](https://arxiv.org/pdf/2006.16550.pdf)
>  The availability of wildland fire propagation models with parameters estimated in an accurate way starting from measurements of fire fronts is crucial to predict the evolution of fire and allocate resources for firefighting. Thus, we propose an approach to estimate the parameters of a wildland fire propagation model combining an empirical fire spread rate and level set methods to describe the evolution of the fire front over time and space. After validating the model, the estimation of parameters in the spread rate is performed by using fire front shapes measured at different time instants as well as wind velocity and direction, landscape elevation, and vegetation distribution. Parameter estimation is performed by solving an optimization problem, where the objective function to be minimized is the symmetric difference between predicted and measured fronts. Numerical results obtained by the application of the proposed method are reported in two simulated scenarios and in an application case study using real data of the 2002 Troy fire in Southern California, thus showing the effectiveness of the proposed approach.      
### 46.Vehicle Re-ID for Surround-view Camera System  [ :arrow_down: ](https://arxiv.org/pdf/2006.16503.pdf)
>  The vehicle re-identification (ReID) plays a critical role in the perception system of autonomous driving, which attracts more and more attention in recent years. However, to our best knowledge, there is no existing complete solution for the surround-view system mounted on the vehicle. In this paper, we argue two main challenges in above scenario: i) In single camera view, it is difficult to recognize the same vehicle from the past image frames due to the fisheye distortion, occlusion, truncation, etc. ii) In multi-camera view, the appearance of the same vehicle varies greatly from different camera's viewpoints. Thus, we present an integral vehicle Re-ID solution to address these problems. Specifically, we propose a novel quality evaluation mechanism to balance the effect of tracking box's drift and target's consistency. Besides, we take advantage of the Re-ID network based on attention mechanism, then combined with a spatial constraint strategy to further boost the performance between different cameras. The experiments demonstrate that our solution achieves state-of-the-art accuracy while being real-time in practice. Besides, we will release the code and annotated fisheye dataset for the benefit of community.      
### 47.An input-output inspired method for permissible perturbation amplitude of transitional wall-bounded shear flows  [ :arrow_down: ](https://arxiv.org/pdf/2006.16484.pdf)
>  The precise set of parameters governing transition to turbulence in wall-bounded shear flows remains an open question; many theoretical bounds have been obtained, but there is not yet a consensus between these bounds and experimental/simulation results. In this work, we focus on a method to provide a provable Reynolds number dependent bound on the amplitude of perturbations a flow can sustain while maintaining the laminar state. Our analysis relies on an input-output approach that partitions the dynamics into a feedback interconnection of the linear and nonlinear dynamics (i.e., a Luré system that represents the nonlinearity as static feedback). We then construct quadratic constraints of the nonlinear term that is restricted by system physics to be energy conserving (lossless) and to have bounded input-output energy. Computing the region of attraction of the laminar state (set of safe perturbations) and permissible perturbation amplitude are then reformulated as Linear Matrix Inequalities (LMI), which provides a more computationally efficient solution than prevailing nonlinear approaches based on sum of squares programming. The proposed framework can also be used for energy method computations and linear stability analysis. We apply our approach to low dimensional nonlinear shear flow models for a range of Reynolds numbers. The results from our analytically derived bounds are consistent with the bounds identified through exhaustive simulations. However, they have the added benefit of being achieved at much lower computational cost and providing a provable guarantee that a certain level of perturbation is permissible.      
### 48.Dose Prediction with Deep Learning for Prostate Cancer Radiation Therapy: Model Adaptation to Different Treatment Planning Practices  [ :arrow_down: ](https://arxiv.org/pdf/2006.16481.pdf)
>  This work aims to study the generalizability of a pre-developed deep learning (DL) dose prediction model for volumetric modulated arc therapy (VMAT) for prostate cancer and to adapt the model to three different internal treatment planning styles and one external institution planning style. We built the source model with planning data from 108 patients previously treated with VMAT for prostate cancer. For the transfer learning, we selected patient cases planned with three different styles from the same institution and one style from a different institution to adapt the source model to four target models. We compared the dose distributions predicted by the source model and the target models with the clinical dose predictions and quantified the improvement in the prediction quality for the target models over the source model using the Dice similarity coefficients (DSC) of 10% to 100% isodose volumes and the dose-volume-histogram (DVH) parameters of the planning target volume and the organs-at-risk. The source model accurately predicts dose distributions for plans generated in the same source style but performs sub-optimally for the three internal and one external target styles, with the mean DSC ranging between 0.81-0.94 and 0.82-0.91 for the internal and the external styles, respectively. With transfer learning, the target model predictions improved the mean DSC to 0.88-0.95 and 0.92-0.96 for the internal and the external styles, respectively. Target model predictions significantly improved the accuracy of the DVH parameter predictions to within 1.6%. We demonstrated model generalizability for DL-based dose prediction and the feasibility of using transfer learning to solve this problem. With 14-29 cases per style, we successfully adapted the source model into several different practice styles. This indicates a realistic way to widespread clinical implementation of DL-based dose prediction.      
### 49.Neural Network Middle-Term Probabilistic Forecasting of Daily Power Consumption  [ :arrow_down: ](https://arxiv.org/pdf/2006.16388.pdf)
>  Middle-term horizon (months to a year) power consumption prediction is a main challenge in the energy sector, in particular when probabilistic forecasting is considered. We propose a new modelling approach that incorporates trend, seasonality and weather conditions, as explicative variables in a shallow Neural Network with an autoregressive feature. We obtain excellent results for density forecast on the one-year test set applying it to the daily power consumption in New England U.S.A.. The quality of the achieved power consumption probabilistic forecasting has been verified, on the one hand, comparing the results to other standard models for density forecasting and, on the other hand, considering measures that are frequently used in the energy sector as pinball loss and CI backtesting.      
### 50.Human Trust-based Feedback Control: Dynamically varying automation transparency to optimize human-machine interactions  [ :arrow_down: ](https://arxiv.org/pdf/2006.16353.pdf)
>  Human trust in automation plays an essential role in interactions between humans and automation. While a lack of trust can lead to a human's disuse of automation, over-trust can result in a human trusting a faulty autonomous system which could have negative consequences for the human. Therefore, human trust should be calibrated to optimize human-machine interactions with respect to context-specific performance objectives. In this article, we present a probabilistic framework to model and calibrate a human's trust and workload dynamics during his/her interaction with an intelligent decision-aid system. This calibration is achieved by varying the automation's transparency---the amount and utility of information provided to the human. The parameterization of the model is conducted using behavioral data collected through human-subject experiments, and three feedback control policies are experimentally validated and compared against a non-adaptive decision-aid system. The results show that human-automation team performance can be optimized when the transparency is dynamically updated based on the proposed control policy. This framework is a first step toward widespread design and implementation of real-time adaptive automation for use in human-machine interactions.      
### 51.Dynamic Knapsack Optimization Towards Efficient Multi-Channel Sequential Advertising  [ :arrow_down: ](https://arxiv.org/pdf/2006.16312.pdf)
>  In E-commerce, advertising is essential for merchants to reach their target users. The typical objective is to maximize the advertiser's cumulative revenue over a period of time under a budget constraint. In real applications, an advertisement (ad) usually needs to be exposed to the same user multiple times until the user finally contributes revenue (e.g., places an order). However, existing advertising systems mainly focus on the immediate revenue with single ad exposures, ignoring the contribution of each exposure to the final conversion, thus usually falls into suboptimal solutions. In this paper, we formulate the sequential advertising strategy optimization as a dynamic knapsack problem. We propose a theoretically guaranteed bilevel optimization framework, which significantly reduces the solution space of the original optimization space while ensuring the solution quality. To improve the exploration efficiency of reinforcement learning, we also devise an effective action space reduction approach. Extensive offline and online experiments show the superior performance of our approaches over state-of-the-art baselines in terms of cumulative revenue.      
