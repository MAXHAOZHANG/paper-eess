# ArXiv eess --Thu, 12 Nov 2020
### 1.On End-to-end Multi-channel Time Domain Speech Separation in Reverberant Environments  [ :arrow_down: ](https://arxiv.org/pdf/2011.05958.pdf)
>  This paper introduces a new method for multi-channel time domain speech separation in reverberant environments. A fully-convolutional neural network structure has been used to directly separate speech from multiple microphone recordings, with no need of conventional spatial feature extraction. To reduce the influence of reverberation on spatial feature extraction, a dereverberation pre-processing method has been applied to further improve the separation performance. A spatialized version of wsj0-2mix dataset has been simulated to evaluate the proposed system. Both source separation and speech recognition performance of the separated signals have been evaluated objectively. Experiments show that the proposed fully-convolutional network improves the source separation metric and the word error rate (WER) by more than 13% and 50% relative, respectively, over a reference system with conventional features. Applying dereverberation as pre-processing to the proposed system can further reduce the WER by 29% relative using an acoustic model trained on clean and reverberated data.      
### 2.Optimal Signal Processing for Common Randomness Generation over MIMO Gaussian Channels with Applications in Identification  [ :arrow_down: ](https://arxiv.org/pdf/2011.05929.pdf)
>  Common randomness (CR), as a resource, is not commonly used in existing practical communication systems. In the common randomness framework, both sender and receiver aim to generate a common random variable observable to both, ideally with low error probability. The knowledge of this CR allows us to implement correlated random protocols that can lead to faster and more efficient algorithms. Previous work on CR focused on Single-Input Single-Output (SISO) channels with finite input and output alphabets. As far as we know, no results on continuous and Multiple-Input Multiple-Output (MIMO) channels have yet been established. We characterize CR over Gaussian channels for their practical relevance in many communication situations. We first derive the CR capacity for Gaussian SISO channels. This result is then used for the derivation of the CR capacity in the MIMO case. CR plays a key role in the identification scheme. In contrast to the classical transmission, the decoder is now interested in knowing \emph{whether} a specific message was sent or not, rather than knowing \emph{what} the received message is. In many new applications such as several machine-to-machine and human-to-machine systems and the tactile internet, the identification, also called the post-Shannon scheme, is more efficient than the classical transmission. It has been proved that through CR generation, the post-Shannon communication task allows us to achieve an enormous performance gain that we can take advantage of in several applications. <br>We consider a correlation-assisted secure identification scheme and develop a lower bound on the corresponding secure identification capacity.      
### 3.Improvement of plant performance using Closed loop Reference Model Simple Adaptive Control for Micro Air Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2011.05924.pdf)
>  In this paper, we present a novel idea to improve the transient performance of the existing Simple Adaptive Control architecture, without requiring high adaptation gains. Improvement in performance is achieved by incorporating the closed loop reference model based on the output feedback to the Simple Adaptive Control architecture. In this proposed scheme, the reference model dynamics is driven by the desired command as well as the error signal between the plant output and the reference model output. It is shown that the modified control architecture improves the system performance without any additional control efforts, which is then validated through simulations of the lateral model dynamics of Micro Air Vehicle.      
### 4.Application of Compromising Evolution in Multi-objective Image Error Concealment  [ :arrow_down: ](https://arxiv.org/pdf/2011.05844.pdf)
>  Numerous multi-objective optimization problems encounter with a number of fitness functions to be simultaneously optimized of which their mutual preferences are not inherently known. Suffering from the lack of underlying generative models, the existing convex optimization approaches may fail to derive the Pareto optimal solution for those problems in complicated domains such as image enhancement. In order to obviate such shortcomings, the Compromising Evolution Method is proposed in this report to modify the Simple Genetic Algorithm by utilizing the notion of compromise. The simulation results show the power of the proposed method solving multi-objective optimizations in a case study of image error concealment.      
### 5.Power Injection Attacks in Smart Distribution Grids with Photovoltaics  [ :arrow_down: ](https://arxiv.org/pdf/2011.05829.pdf)
>  In order to protect smart distribution grids from intrusions, it is important to understand possible risks and impacts of attacks. We study the optimal attack strategy of a power injection attack against the physical layer of a smart distribution grid with a high penetration of photovoltaic resources. We derive both the optimal attack signal and optimal attack location: the optimal attack signal is a step function which switches its sign at the final stage, and the optimal attack location is the node with the largest impedance to the grid substation. Numerical examples on a European benchmark model verify the developed results. Finally, both theoretical and numerical results are used to discuss feasible defense strategies against power injection attacks.      
### 6.Competition between simultaneous demand-side flexibility options: The case of community electricity storage systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.05809.pdf)
>  Community electricity storage systems for multiple applications promise benefits over household electricity storage systems. More economical flexibility options such as demand response and sector coupling might reduce the market size for storage facilities. This paper assesses the economic performance of community electricity storage systems by taking competitive flexibility options into account. For this purpose, an actor-related, scenario-based optimization framework is applied. The results are in line with the literature and show that community storage systems are economically more efficient than household storage systems. Relative storage capacity reductions of community storage systems over household storage systems are possible, as the demand and generation profiles are balanced out among end users. On average, storage capacity reductions of 9% per household are possible in the base case, resulting in lower specific investments. The simultaneous application of demand-side flexibility options such as sector coupling and demand response enable a further capacity reduction of the community storage size by up to 23%. At the same time, the competition between flexibility options leads to smaller benefits regarding the community storage flexibility potential, which reduces the market viability for these applications. In the worst case, the cannibalization effects reach up to 38% between the flexibility measures. The losses of the flexibility benefits outweigh the savings of the capacity reduction whereby sector coupling constitutes a far greater influencing factor than demand response. Overall, in consideration of the stated cost trends, the economies of scale, and the reduction possibilities, a profitable community storage model might be reached between 2025 and 2035. Future work should focus on the analysis of policy frameworks.      
### 7.A Full-Stack Open-Source Framework for Antenna and Beamforming Evaluation in mmWave 5G NR  [ :arrow_down: ](https://arxiv.org/pdf/2011.05800.pdf)
>  Millimeter wave (mmWave) communication represents one of the main innovations of the next generation of wireless technologies, allowing users to reach unprecedented data rates. To overcome the high path loss at mmWave frequencies, these systems make use of directional antennas able to focus the transmit power into narrow beams using BeamForming (BF) techniques, thus making the communication directional. This new paradigm opens up a set of challenges for the design of efficient wireless systems, in which antenna and BF components play an important role also at the higher layer of the protocol stack. For this reason, accurate modeling of these components in a full-stack simulation is of primary importance to understand the overall system behavior. This paper proposes a novel framework for the end-to-end simulation of 5G mmWave cellular networks, including a ray-tracing based channel model and accurate models for antenna arrays and BF schemes. We showcase this framework by evaluating the performance of different antenna and BF configurations considering both link-level and end-to-end metrics and present the obtained results.      
### 8.Model-Dependent Prosthesis Control with Interaction Force Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2011.05793.pdf)
>  Current prosthesis control methods are primarily model-independent - lacking formal guarantees of stability, relying largely on heuristic tuning parameters for good performance, and neglecting use of the natural dynamics of the system. Model-dependence for prosthesis controllers is difficult to achieve due to the unknown human dynamics. We build upon previous work which synthesized provably stable prosthesis walking through the use of rapidly exponentially stabilizing control Lyapunov functions (RES-CLFs). This paper utilizes RES-CLFs together with force estimation to construct model-based optimization-based controllers for the prosthesis. These are experimentally realized on hardware with onboard sensing and computation. This hardware demonstration has formal guarantees of stability, utilizes the natural dynamics of the system, and achieves superior tracking to other prosthesis trajectory tracking control methods.      
### 9.Objective Diagnosis for Histopathological Images Based on Machine Learning Techniques: Classical Approaches and New Trends  [ :arrow_down: ](https://arxiv.org/pdf/2011.05790.pdf)
>  Histopathology refers to the examination by a pathologist of biopsy samples. Histopathology images are captured by a microscope to locate, examine, and classify many diseases, such as different cancer types. They provide a detailed view of different types of diseases and their tissue status. These images are an essential resource with which to define biological compositions or analyze cell and tissue structures. This imaging modality is very important for diagnostic applications. The analysis of histopathology images is a prolific and relevant research area supporting disease diagnosis. In this paper, the challenges of histopathology image analysis are evaluated. An extensive review of conventional and deep learning techniques which have been applied in histological image analyses is presented. This review summarizes many current datasets and highlights important challenges and constraints with recent deep learning techniques, alongside possible future research avenues. Despite the progress made in this research area so far, it is still a significant area of open research because of the variety of imaging techniques and disease-specific characteristics.      
### 10.Distorted image restoration using stacked adversarial network  [ :arrow_down: ](https://arxiv.org/pdf/2011.05784.pdf)
>  Liquify is a common technique for distortion. Due to the uncertainty in the distortion variation, restoring distorted images caused by liquify filter is a challenging task. Unlike existing methods mainly designed for specific single deformation, this paper aims at automatic distorted image restoration, which is characterized by seeking the appropriate warping of multitype and multi-scale distorted images. In this work, we propose a stacked adversarial framework with a novel coherent skip connection to directly predict the reconstruction mappings and represent high-dimensional feature. Since there is no available benchmark which hinders the exploration, we contribute a distorted face dataset by reconstructing distortion mappings based on CelebA dataset. We also introduce a novel method for generating synthesized data. We evaluate our method on proposed benchmark quantitatively and qualitatively, and apply it to the real world for validation.      
### 11.Constraints on Nonlinear Finite Dimensional Flat Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.05775.pdf)
>  This chapter presents an approach to embed the input/state/output constraints in a unified manner into the trajectory design for differentially flat systems. To that purpose, we specialize the flat outputs (or the reference trajectories) as Bezier curves. Using the flatness property, the system's inputs/states can be expressed as a combination of Bezier curved flat outputs and their derivatives. Consequently, we explicitly obtain the expressions of the control points of the inputs/states Bezier curves as a combination of the control points of the flat outputs. By applying desired constraints to the latter control points, we find the feasible regions for the output Bezier control points i.e. a set of feasible reference trajectories.      
### 12.On Performance of Sparse Fast Fourier Transform Algorithms Using the Aliasing Filter  [ :arrow_down: ](https://arxiv.org/pdf/2011.05749.pdf)
>  Computing the Sparse Fast Fourier Transform(sFFT) of a K-sparse signal of size N has emerged as a critical topic for a long time. There are mainly two stages in the sFFT: frequency bucketization and spectrum reconstruction. Frequency bucketization is equivalent to hashing the frequency coefficients into B buckets through one of these filters: Dirichlet kernel filter, flat filter, aliasing filter, etc. The spectrum reconstruction is equivalent to identifying frequencies that are isolated in their buckets. More than forty different sFFT algorithms compute Discrete Fourier Transform(DFT) by their unique methods so far. In order to use them properly, the urgent topic of great concern is how to analyze and evaluate the performance of these algorithms in theory and practice. The paper mainly discusses the sFFT Algorithms using the aliasing filter. In the first part, the paper introduces the technique of three frameworks: the one-shot framework based on the compressed sensing(CS) solver, the peeling framework based on the bipartite graph and the iterative framework based on the binary tree search. Then, we get the conclusion of the performance of six corresponding algorithms: sFFT-DT1.0, sFFT-DT2.0, sFFT-DT3.0, FFAST, R-FFAST and DSFFT algorithm in theory. In the second part, we make two categories of experiments for computing the signals of different SNR, different N, different K by a standard testing platform and record the run time, percentage of the signal sampled and L0, L1, L2 error both in the exactly sparse case and general sparse case. The result of experiments satisfies the inferences obtained in theory.      
### 13.Classification of COVID-19 in Chest CT Images using Convolutional Support Vector Machines  [ :arrow_down: ](https://arxiv.org/pdf/2011.05746.pdf)
>  Purpose: Coronavirus 2019 (COVID-19), which emerged in Wuhan, China and affected the whole world, has cost the lives of thousands of people. Manual diagnosis is inefficient due to the rapid spread of this virus. For this reason, automatic COVID-19 detection studies are carried out with the support of artificial intelligence algorithms. Methods: In this study, a deep learning model that detects COVID-19 cases with high performance is presented. The proposed method is defined as Convolutional Support Vector Machine (CSVM) and can automatically classify Computed Tomography (CT) images. Unlike the pre-trained Convolutional Neural Networks (CNN) trained with the transfer learning method, the CSVM model is trained as a scratch. To evaluate the performance of the CSVM method, the dataset is divided into two parts as training (%75) and testing (%25). The CSVM model consists of blocks containing three different numbers of SVM kernels. Results: When the performance of pre-trained CNN networks and CSVM models is assessed, CSVM (7x7, 3x3, 1x1) model shows the highest performance with 94.03% ACC, 96.09% SEN, 92.01% SPE, 92.19% PRE, 94.10% F1-Score, 88.15% MCC and 88.07% Kappa metric values. Conclusion: The proposed method is more effective than other methods. It has proven in experiments performed to be an inspiration for combating COVID and for future studies.      
### 14.FastSVC: Fast Cross-Domain Singing Voice Conversion with Feature-wise Linear Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2011.05731.pdf)
>  This paper presents FastSVC, a light-weight cross-domain sing voice conversion (SVC) system, which is able to achieve high conversion performance, with inference speed 4x faster than real time on CPUs. FastSVC uses Conformer based phoneme recognizer to extract singer-agnostic linguistic features from singing signals. A feature-wise linear modulation based generator is used to synthesize waveform directly from linguistic features, leveraging information from sine-excitation signals and loudness features. The waveform generator can be trained conveniently using a multi-resolution spectral loss and an adversarial loss. Experimental results show that the proposed FastSVC system, compared with a computationally heavy baseline system, can achieve comparable conversion performance in some scenarios and significantly better conversion performance in other scenarios. Moreover, the proposed FastSVC system achieves desirable cross-lingual singing conversion performance. Inference speed of the FastSVC system is 3x and 70x faster than the baseline system on GPUs and CPUs, respectively.      
### 15.Low-resource expressive text-to-speech using data augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2011.05707.pdf)
>  While recent neural text-to-speech (TTS) systems perform remarkably well, they typically require a substantial amount of recordings from the target speaker reading in the desired speaking style. In this work, we present a novel 3-step methodology to circumvent the costly operation of recording large amounts of target data in order to build expressive style voices with as little as 15 minutes of such recordings. First, we augment data via voice conversion by leveraging recordings in the desired speaking style from other speakers. Next, we use that synthetic data on top of the available recordings to train a TTS model. Finally, we fine-tune that model to further increase quality. Our evaluations show that the proposed changes bring significant improvements over non-augmented models across many perceived aspects of synthesised speech. We demonstrate the proposed approach on 2 styles (newscaster and conversational), on various speakers, and on both single and multi-speaker models, illustrating the robustness of our approach.      
### 16.On Performance of Multiscale Sparse Fast Fourier Transform Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2011.05698.pdf)
>  Computing the Sparse Fast Fourier Transform(sFFT) of a K-sparse signal of size N has emerged as a critical topic for a long time. The sFFT algorithms decrease the runtime and sampling complexity by taking advantage of the signal inherent characteristics that a large number of signals are sparse in the frequency domain(e.g., sensors, video data, audio, medical image, etc.). The first stage of sFFT is frequency bucketization through one of these filters: Dirichlet kernel filter, flat filter, aliasing filter, etc. Compared to other sFFT algorithms, the sFFT algorithms using the flat filter is more convenient and efficient because the filtered signal is concentrated both in the time domain and frequency domain. Up to now, three sFFT algorithms sFFT1.0, sFFT2.0, sFFT3.0 algorithm have been proposed by the Massachusetts Institute of Technology(MIT) in 2013. Still, the sFFT4.0 algorithm using the multiscale approach method has not been implemented yet. This paper will discuss this algorithm comprehensively in theory and implement it in practice. It is proved that the performance of the sFFT4.0 algorithm depends on two parameters. The runtime and sampling complexity are in direct ratio to the multiscale parameter and in inverse ratio to the extension parameter. The robustness is in direct ratio to the extension parameter and in inverse ratio to the multiscale parameter. Compared with three similar algorithms or other four types of algorithms, the sFFT4.0 algorithm has excellent runtime and sampling complexity that ten to one hundred times better than the fftw algorithm, although the robustness of the algorithm is medium.      
### 17.Bayesian model of electrical heating disaggregation  [ :arrow_down: ](https://arxiv.org/pdf/2011.05674.pdf)
>  Adoption of smart meters is a major milestone on the path of European transition to smart energy. The residential sector in France represents $\approx$35\% of electricity consumption with $\approx$40\% (INSEE) of households using electrical heating. The number of deployed smart meters Linky is expected to reach 35M in 2021. In this manuscript we present an analysis of 676 households with an observation period of at least 6 months, for which we have metadata, such as the year of construction and the type of heating and propose a Bayesian model of the electrical consumption conditioned on temperature that allows to disaggregate the heating component from the electrical load curve in an unsupervised manner. In essence the model is a mixture of piece-wise linear models, characterised by a temperature threshold, below which we allow a mixture of two modes to represent the latent state home/away.      
### 18.Optimal Synchronization Control for Heterogeneous Multi-Agent Systems: Online Adaptive Learning Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2011.05663.pdf)
>  This paper presents an online adaptive learning solution to optimal synchronization control problem of heterogeneous multi-agent systems via a novel distributed policy iteration approach.      
### 19.Efficient Neural Architecture Search for End-to-end Speech Recognition via Straight-Through Gradients  [ :arrow_down: ](https://arxiv.org/pdf/2011.05649.pdf)
>  Neural Architecture Search (NAS), the process of automating architecture engineering, is an appealing next step to advancing end-to-end Automatic Speech Recognition (ASR), replacing expert-designed networks with learned, task-specific architectures. In contrast to early computational-demanding NAS methods, recent gradient-based NAS methods, e.g., DARTS (Differentiable ARchiTecture Search), SNAS (Stochastic NAS) and ProxylessNAS, significantly improve the NAS efficiency. In this paper, we make two contributions. First, we rigorously develop an efficient NAS method via Straight-Through (ST) gradients, called ST-NAS. Basically, ST-NAS uses the loss from SNAS but uses ST to back-propagate gradients through discrete variables to optimize the loss, which is not revealed in ProxylessNAS. Using ST gradients to support sub-graph sampling is a core element to achieve efficient NAS beyond DARTS and SNAS. Second, we successfully apply ST-NAS to end-to-end ASR. Experiments over the widely benchmarked 80-hour WSJ and 300-hour Switchboard datasets show that the ST-NAS induced architectures significantly outperform the human-designed architecture across the two datasets. Strengths of ST-NAS such as architecture transferability and low computation cost in memory and time are also reported.      
### 20.From Aircraft Tracking Data to Network Delay Model: A Data-Driven Approach Considering En-Route Congestion  [ :arrow_down: ](https://arxiv.org/pdf/2011.05645.pdf)
>  En-route congestion causes delays in air traffic networks and will become more prominent as air traffic demand will continue to increase yet airspace volume cannot grow. However, most existing studies on flight delay modeling do not consider en-route congestion explicitly. In this study, we propose a new flight delay model, Multi-layer Air Traffic Network Delay (MATND) model, to capture the impact of en-route congestion on flight delays over an air traffic network. This model is developed by a data-driven approach, taking aircraft tracking data and flight schedules as inputs to characterize a national air traffic network, as well as a system-level model approach, modeling the delay process based on queueing theory. The two approaches combined make the network delay model a close representation of reality and easy-to-implement for what-if scenario analysis. The proposed MATND model includes 1) a data-driven method to learn a network composed of airports, en-route congestion points, and air corridors from aircraft tracking data, 2) a stochastic and dynamic queuing network model to calculate flight delays and track their propagation at both airports and in en-route congestion areas, in which the delays are computed via a space-time decomposition method. Using one month of historical aircraft tracking data over China's air traffic network, MATND is tested and shown to give an accurate quantification of delays of the national air traffic network. "What-if" scenario analysis is conducted to demonstrate how the proposed model can be used for the evaluation of air traffic network improvement strategies, where the manipulation of reality at such a scale is impossible. Results show that MATND is computationally efficient, well suited for evaluating the impact of policy alternatives on system-wide delay at a macroscopic level.      
### 21.Skin disease diagnosis with deep learning: a review  [ :arrow_down: ](https://arxiv.org/pdf/2011.05627.pdf)
>  Skin cancer is one of the most threatening diseases worldwide. However, diagnosing a skin cancer correctly is challenging. Recently, deep learning algorithms have achieved excellent performance on various tasks. Particularly, they have been also implemented for the tasks of skin disease diagnosis. In this paper, we present a review on deep learning methods and their applications in skin disease diagnosis. We first introduce skin diseases and image acquisition methods in dermatology, and list several publicly available datasets for training and testing algorithms for skin disease diagnosis. Then, we introduce the conception of deep learning and review popular deep learning architectures. Thereafter, popular deep learning frameworks that facilitate the implementation of deep learning algorithms and performance evaluation metrics are presented. As an important part of this article, we then review the literatures involving deep learning methods for skin disease diagnosis from several aspects according to the specific tasks. Additionally, we discuss the challenges faced in the area of skin disease diagnosis with deep learning and suggest possible future research directions. Finally, we summarize the article. The major purpose of this article is to provide a conceptual and systematically review of the recent works on skin disease diagnosis with deep learning. Given the popularity of deep learning, there remains great challenges in the area, as well as opportunities that we can explore in the future.      
### 22.Voltage Estimation in Low-Voltage Distribution Grids with Distributed Energy Resources  [ :arrow_down: ](https://arxiv.org/pdf/2011.05598.pdf)
>  The present distribution grids generally have limited sensing capabilities and are therefore characterized by low observability. Improved observability is a prerequisite for increasing the hosting capacity of distributed energy resources such as solar photovoltaics (PV) in distribution grids. In this context, this paper presents learning-aided low-voltage estimation using untapped but readily available and widely distributed sensors from cable television (CATV) networks. The CATV sensors offer timely local voltage magnitude sensing with 5-minute resolution and can provide an order of magnitude more data on the state of a distribution system than currently deployed utility sensors. The proposed solution incorporates voltage readings from neighboring CATV sensors, taking into account spatio-temporal aspects of the observations, and estimates single-phase voltage magnitudes at all non-monitored buses using random forest. The effectiveness of the proposed approach was demonstrated using a 1572-bus feeder from the SMART-DS data set for two case studies - passive distribution feeder (without PV) and active distribution feeder (with PV). The analysis was conducted on simulated data, and the results show voltage estimates with a high degree of accuracy, even at extremely low percentages of observable nodes.      
### 23.Invertible CNN-Based Super Resolution with Downsampling Awareness  [ :arrow_down: ](https://arxiv.org/pdf/2011.05586.pdf)
>  Single image super resolution involves artificially increasing the resolution of an image. Recently, convolutional neural networks have been demonstrated as very powerful tools for this problem. These networks are typically trained by artificially degrading high resolution images and training the neural network to reproduce the original. Because these neural networks are learning an inverse function for an image downsampling scheme, their high-resolution outputs should ideally re-produce the corresponding low-resolution input when the same downsampling scheme is applied. This constraint has not historically been explicitly and strictly imposed during training however. Here, a method for "downsampling aware" super resolution networks is proposed. A differentiable operator is applied as the final output layer of the neural network that forces the downsampled output to match the low resolution input data under 2D-average downsampling. It is demonstrated that appending this operator to a selection of state-of-the-art deep-learning-based super resolution schemes improves training time and overall performance on most of the common image super resolution benchmark datasets. In addition to this performance improvement for images, this method has potentially broad and significant impacts in the physical sciences. This scheme can be applied to data produced by medical scans, precipitation radars, gridded numerical simulations, satellite imagers, and many other sources. In such applications, the proposed method's guarantee of strict adherence to physical conservation laws is of critical importance.      
### 24.Deep Transfer Learning-Assisted Signal Detection for Ambient Backscatter Communications  [ :arrow_down: ](https://arxiv.org/pdf/2011.05574.pdf)
>  Existing tag signal detection algorithms inevitably suffer from a high bit error rate (BER) due to the difficulties in estimating the channel state information (CSI). To eliminate the requirement of channel estimation and to improve the system performance, in this paper, we adopt a deep transfer learning (DTL) approach to implicitly extract the features of communication channel and directly recover tag symbols. Inspired by the powerful capability of convolutional neural networks (CNN) in exploring the features of data in a matrix form, we design a novel covariance matrix aware neural network (CMNet)-based detection scheme to facilitate DTL for tag signal detection, which consists of offline learning, transfer learning, and online detection. Specifically, a CMNet-based likelihood ratio test (CMNet-LRT) is derived based on the minimum error probability (MEP) criterion. Taking advantage of the outstanding performance of DTL in transferring knowledge with only a few training data, the proposed scheme can adaptively fine-tune the detector for different channel environments to further improve the detection performance. Finally, extensive simulation results demonstrate that the BER performance of the proposed method is comparable to that of the optimal detection method with perfect CSI.      
### 25.An ensemble-based approach by fine-tuning the deep transfer learning models to classify pneumonia from chest X-ray images  [ :arrow_down: ](https://arxiv.org/pdf/2011.05543.pdf)
>  Pneumonia is caused by viruses, bacteria, or fungi that infect the lungs, which, if not diagnosed, can be fatal and lead to respiratory failure. More than 250,000 individuals in the United States, mainly adults, are diagnosed with pneumonia each year, and 50,000 die from the disease. Chest Radiography (X-ray) is widely used by radiologists to detect pneumonia. It is not uncommon to overlook pneumonia detection for a well-trained radiologist, which triggers the need for improvement in the diagnosis's accuracy. In this work, we propose using transfer learning, which can reduce the neural network's training time and minimize the generalization error. We trained, fine-tuned the state-of-the-art deep learning models such as InceptionResNet, MobileNetV2, Xception, DenseNet201, and ResNet152V2 to classify pneumonia accurately. Later, we created a weighted average ensemble of these models and achieved a test accuracy of 98.46%, precision of 98.38%, recall of 99.53%, and f1 score of 98.96%. These performance metrics of accuracy, precision, and f1 score are at their highest levels ever reported in the literature, which can be considered a benchmark for the accurate pneumonia classification.      
### 26.Surrogate Source Model Learning for Determined Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2011.05540.pdf)
>  We propose to learn surrogate functions of universal speech priors for determined blind speech separation. Deep speech priors are highly desirable due to their high modelling power, but are not compatible with state-of-the-art independent vector analysis based on majorization-minimization (AuxIVA), since deriving the required surrogate function is not easy, nor always possible. Instead, we do away with exact majorization and directly approximate the surrogate. Taking advantage of iterative source steering (ISS) updates, we back propagate the permutation invariant separation loss through multiple iterations of AuxIVA. ISS lends itself well to this task due to its lower complexity and lack of matrix inversion. Experiments show large improvements in terms of scale invariant signal-to-distortion (SDR) ratio and word error rate compared to baseline methods. Training is done on two speakers mixtures and we experiment with two losses, SDR and coherence. We find that the learnt approximate surrogate generalizes well on mixtures of three and four speakers without any modification. We also demonstrate generalization to a different variation of the AuxIVA update equations. The SDR loss leads to fastest convergence in iterations, while coherence leads to the lowest word error rate (WER). We obtain as much as 36 % reduction in WER.      
### 27.A Unified Framework for Compressive Video Recovery from Coded Exposure Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2011.05532.pdf)
>  Several coded exposure techniques have been proposed for acquiring high frame rate videos at low bandwidth. Most recently, a Coded-2-Bucket camera has been proposed that can acquire two compressed measurements in a single exposure, unlike previously proposed coded exposure techniques, which can acquire only a single measurement. Although two measurements are better than one for an effective video recovery, we are yet unaware of the clear advantage of two measurements, either quantitatively or qualitatively. Here, we propose a unified learning-based framework to make such a qualitative and quantitative comparison between those which capture only a single coded image (Flutter Shutter, Pixel-wise coded exposure) and those that capture two measurements per exposure (C2B). Our learning-based framework consists of a shift-variant convolutional layer followed by a fully convolutional deep neural network. Our proposed unified framework achieves the state of the art reconstructions in all three sensing techniques. Further analysis shows that when most scene points are static, the C2B sensor has a significant advantage over acquiring a single pixel-wise coded measurement. However, when most scene points undergo motion, the C2B sensor has only a marginal benefit over the single pixel-wise coded exposure measurement.      
### 28.On-The-Fly Control of Unknown Systems: From Side Information to Performance Guarantees through Reachability  [ :arrow_down: ](https://arxiv.org/pdf/2011.05524.pdf)
>  We develop data-driven algorithms for reachability analysis and control of systems with a priori unknown nonlinear dynamics. The resulting algorithms not only are suitable for settings with real-time requirements but also provide provable performance guarantees. To this end, they merge data from only a single finite-horizon trajectory and, if available, various forms of side information. Such side information may include knowledge of the regularity of the dynamics, algebraic constraints on the states, monotonicity, or decoupling in the dynamics between the states. Specifically, we develop two algorithms, $\texttt{DaTaReach}$ and $\texttt{DaTaControl}$, to over-approximate the reachable set and design control signals for the system on the fly. $\texttt{DaTaReach}$ constructs a differential inclusion that contains the unknown dynamics. Then, in a discrete-time setting, it over-approximates the reachable set through interval Taylor-based methods applied to systems with dynamics described as differential inclusions. We provide a bound on the time step size that ensures the correctness and termination of $\texttt{DaTaReach}$. $\texttt{DaTaControl}$ enables convex-optimization-based control using the computed over-approximation and the receding-horizon control framework. Besides, $\texttt{DaTaControl}$ achieves near-optimal control and is suitable for real-time control of such systems. We establish a bound on its suboptimality and the number of primitive operations it requires to compute control values. Then, we theoretically show that $\texttt{DaTaControl}$ achieves tighter suboptimality bounds with an increasing amount of data and richer side information. Finally, experiments on a unicycle, quadrotor, and aircraft systems demonstrate the efficacy of both algorithms over existing approaches.      
### 29.Three Dimensional Velocity Measurement Using a Dual Axis Millimeter-Wave Interferometric Radar  [ :arrow_down: ](https://arxiv.org/pdf/2011.05512.pdf)
>  In this work, a method for directly measuring target velocity in three dimensions using a dual axis correlation interferometric radar is presented. Recent advances have shown that the measurement of the angular velocity of a target is possible by correlating, or mixing, the signals measured at spatially diverse aperture locations. By utilizing multiple orthogonal baselines, and using conventional Doppler velocity methods to obtain radial velocity, a full three-dimensional velocity vector can be obtained using only three receive antennas and a single transmitter, without the need for tracking. A 40.5 GHz dual axis interferometric radar with a $7\lambda$ antenna baseline is presented along with measurements of a target moving fully tangentially to the radar, and of a target with a component of both radial and tangential velocity. These experiments obtain total velocity root-mean-square errors (RMSEs) of $\text{15.404 mm}\cdot\text{s}^{-1}$ for a target moving purely tangentially to the array, and $\text{39.22 mm}\cdot\text{s}^{-1}$ for a target moving with a radial component up to 30° off of tangent to the array, and estimated trajectory angle RMSEs of 2.33° and 2.35° for each experiment respectively.      
### 30.Dense U-net for super-resolution with shuffle pooling layer  [ :arrow_down: ](https://arxiv.org/pdf/2011.05490.pdf)
>  Single image super-resolution (SISR) in unconstrained environments is challenging because of various illuminations, occlusion and complex environments. Recent researches have achieved great progress on super-resolution due to the development of deep learning in the field of computer vision. In this letter, a Dense U-net with shuffle pooling method is proposed. First, a modified U-net with dense blocks, called dense U-net, is proposed for SISR. Second, a novel pooling strategy called shuffle pooling is designed, which is applied to the dense U-Net for super-resolution task. Third, a mix loss function, which combined with Mean Square Error(MSE), Structural Similarity Index (SSIM) and Mean Gradient Error (MGE), is proposed to solve the perception loss and high-frequency information loss. The proposed method achieves superior accuracy over previous state-of-the-arts on the three benchmark datasets: SET14, BSD300, ICDAR2003. Code is available online.      
### 31.Transient Simulation of Grid-Feeding Converter System for Stability Studies Using Frequency Response Optimized Integrators  [ :arrow_down: ](https://arxiv.org/pdf/2011.05439.pdf)
>  A grid-feeding converter system is added to a novel power system transient simulation scheme based on frequency response optimized integrators considering second order derivative. The converter system and its implementation in the simulation scheme are detailed. Case studies verify the accuracy and efficiency of the simulation scheme. Furthermore, this paper proposes and justifies extending the simulation scheme by integrating commonly used numerical integrators considering first order derivative for part of the studied system. The proposed extension has an insignificant impact on the accuracy of the simulation scheme while significantly enhancing its efficiency. It also reduces the development burden in adding new devices.      
### 32.Self-Supervised Out-of-Distribution Detection in Brain CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2011.05428.pdf)
>  Medical imaging data suffers from the limited availability of annotation because annotating 3D medical data is a time-consuming and expensive task. Moreover, even if the annotation is available, supervised learning-based approaches suffer highly imbalanced data. Most of the scans during the screening are from normal subjects, but there are also large variations in abnormal cases. To address these issues, recently, unsupervised deep anomaly detection methods that train the model on large-sized normal scans and detect abnormal scans by calculating reconstruction error have been reported. In this paper, we propose a novel self-supervised learning technique for anomaly detection. Our architecture largely consists of two parts: 1) Reconstruction and 2) predicting geometric transformations. By training the network to predict geometric transformations, the model could learn better image features and distribution of normal scans. In the test time, the geometric transformation predictor can assign the anomaly score by calculating the error between geometric transformation and prediction. Moreover, we further use self-supervised learning with context restoration for pretraining our model. By comparative experiments on clinical brain CT scans, the effectiveness of the proposed method has been verified.      
### 33.Modeling and Passivity Properties of District Heating Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.05419.pdf)
>  We propose a comprehensive nonlinear ODE-based thermo-hydraulic model of a district heating system featuring several heat producers, consumers and storage devices which are interconnected through a distribution network of meshed topology whose temperature dynamics are explicitly considered. Moreover, we present conditions under which the hydraulic and thermal subsystems of the model exhibit shifted passivity properties and discuss some of the beneficial implications for decentralized control design and stability analysis. For the former subsystem, our results draw on the monotonicity attributes manifested by the mappings involved. For the latter, we propose a storage function based on the {\em ectropy function} of a thermodynamic system, recently used in the passivity analysis of heat exchanger networks. Our formal analysis is supported with numerical simulations on a case study using realistic system parameters.      
### 34.Glioma Classification Using Multimodal Radiology and Histology Data  [ :arrow_down: ](https://arxiv.org/pdf/2011.05410.pdf)
>  Gliomas are brain tumours with a high mortality rate. There are various grades and sub-types of this tumour, and the treatment procedure varies accordingly. Clinicians and oncologists diagnose and categorise these tumours based on visual inspection of radiology and histology data. However, this process can be time-consuming and subjective. The computer-assisted methods can help clinicians to make better and faster decisions. In this paper, we propose a pipeline for automatic classification of gliomas into three sub-types: oligodendroglioma, astrocytoma, and glioblastoma, using both radiology and histopathology images. The proposed approach implements distinct classification models for radiographic and histologic modalities and combines them through an ensemble method. The classification algorithm initially carries out tile-level (for histology) and slice-level (for radiology) classification via a deep learning method, then tile/slice-level latent features are combined for a whole-slide and whole-volume sub-type prediction. The classification algorithm was evaluated using the data set provided in the CPM-RadPath 2020 challenge. The proposed pipeline achieved the F1-Score of 0.886, Cohen's Kappa score of 0.811 and Balance accuracy of 0.860. The ability of the proposed model for end-to-end learning of diverse features enables it to give a comparable prediction of glioma tumour sub-types.      
### 35.Deep Learning Derived Histopathology Image Score for Increasing Phase 3 Clinical Trial Probability of Success  [ :arrow_down: ](https://arxiv.org/pdf/2011.05406.pdf)
>  Failures in Phase 3 clinical trials contribute to expensive cost of drug development in oncology. To drastically reduce such cost, responders to an oncology treatment need to be identified early on in the drug development process with limited amount of patient data before the planning of Phase 3 clinical trials. Despite the challenge of small sample size, we pioneered the use of deep-learning derived digital pathology scores to identify responders based on the immunohistochemistry images of the target antigen expressed in tumor biopsy samples from a Phase 1 Non-small Cell Lung Cancer clinical trial. Based on repeated 10-fold cross validations, the deep-learning derived score on average achieved 4% higher AUC of ROC curve and 6% higher AUC of Precision-Recall curve comparing to the tumor proportion score (TPS) based clinical benchmark. In a small independent testing set of patients, we also demonstrated that the deep-learning derived score achieved numerically at least 25% higher responder rate in the enriched population than the TPS clinical benchmark.      
### 36.Accelerated Probabilistic State Estimation in Distribution Grids via Model Order Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2011.05397.pdf)
>  This paper applies a custom model order reduction technique to the distribution grid state estimation problem. Specifically, the method targets the situation where, due to pseudo-measurement uncertainty, it is advantageous to run the state estimation solver potentially thousands of times over sampled input perturbations in order to compute probabilistic bounds on the underlying system state. This routine, termed the Accelerated Probabilistic State Estimator (APSE), efficiently searches for the solutions of sequential state estimation problems in a low dimensional subspace with a reduced order model (ROM). When a sufficiently accurate solution is not found, the APSE reverts to a conventional QR factorization-based Gauss-Newton solver. It then uses the resulting solution to preform a simple basis expansion of the low-dimensional subspace, thus improving the reduced model solver. Simulated test results, collected from the unbalanced three-phase 8500-node distribution grid, show the resulting algorithm to be almost an order of magnitude faster than a comparable full-order Gauss-Newton solver and thus potentially fast enough for real-time use.      
### 37.Gaussian Compression Stream: Principle and Preliminary Results  [ :arrow_down: ](https://arxiv.org/pdf/2011.05390.pdf)
>  Random projections became popular tools to process big data. In particular, when applied to Nonnegative Matrix Factorization (NMF), it was shown that structured random projections were far more efficient than classical strategies based on gaussian compression. However, they remain costly and might not fully benefit from recent fast random projection <a class="link-external link-http" href="http://techniques.In" rel="external noopener nofollow">this http URL</a> this paper, we thus investigate an alternative to structured random projections named Gaussian compression stream, which (i)is based on Gaussian compressions only, (ii) can benefit from theabove fast techniques, and (iii) is shown to be well-suited to NMF.      
### 38.Rare-Event Chance-Constrained Flight Control Optimization Using Surrogate-Based Subset Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2011.05361.pdf)
>  A probabilistic performance-oriented control gain optimization approach with rare-event chance constraints is introduced for flight systems. Aiming at estimating rare probabilities accurately and efficiently, subset simulation is combined with a global surrogate to improve efficiency. At each level of subset simulation, the samples that are close to the failure domain are employed to construct a local surrogate model, and the global surrogate is thereby refined progressively. In return, seed candidates are screened by the global surrogate, thus saving a large number of calls to the true model and reducing computational expense. Then, control parameters are optimized under chance constraints to directly guarantee system performance. Simulations are conducted on an aircraft longitudinal model subjecting to parametric uncertainties to demonstrate the efficiency and accuracy of this method.      
### 39.Graph Neural Networks for Decentralized Linear-Quadratic Control  [ :arrow_down: ](https://arxiv.org/pdf/2011.05360.pdf)
>  The linear-quadratic controller is one of the fundamental problems in control theory. The optimal solution is a linear controller that requires access to the state of the entire system at any given time. When considering a network system, this renders the optimal controller a \emph{centralized} one. The interconnected nature of a network system oftentimes forces the need for a \emph{decentralized} controller, i.e. one that can be computed separately by each component, relying only on local information. Interestingly enough, even in the linear-quadratic problem, the optimal decentralized controller may be nonlinear. Thus, in this paper, we adopt a graph neural network (GNN) as the parametrization for a decentralized controller. GNNs are naturally local and distributed architectures making them perfectly suited for learning nonlinear decentralized controllers. We further cast the linear-quadratic problem as a self-supervised learning problem, and we derive sufficient conditions for the resulting system to be stable. We run extensive simulations to study the performance of GNN-based decentralized controllers.      
### 40.Polynomial Chaos-Based Flight Control Optimization with Guaranteed Probabilistic Performance  [ :arrow_down: ](https://arxiv.org/pdf/2011.05352.pdf)
>  A probabilistic performance-oriented controller design approach based on polynomial chaos expansion and optimization is proposed for flight dynamic systems. Unlike robust control techniques where uncertainties are conservatively handled, the proposed method aims at propagating uncertainties effectively and optimizing control parameters to satisfy the probabilistic requirements directly. To achieve this, the sensitivities of violation probabilities are evaluated by the expansion coefficients and the fourth moment method for reliability analysis, after which an optimization that minimizes failure probability under chance constraints is conducted. Afterward, a time-dependent polynomial chaos expansion is performed to validate the results. With this approach, the failure probability is reduced while guaranteeing the closed-loop performance, thus increasing the safety margin. Simulations are carried out on a longitudinal model subject to uncertain parameters to demonstrate the effectiveness of this approach.      
### 41.LittleYOLO-SPP: A Delicate Real-Time Vehicle Detection Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2011.05940.pdf)
>  Vehicle detection in real-time is a challenging and important task. The existing real-time vehicle detection lacks accuracy and speed. Real-time systems must detect and locate vehicles during criminal activities like theft of vehicle and road traffic violations with high accuracy. Detection of vehicles in complex scenes with occlusion is also extremely difficult. In this study, a lightweight model of deep neural network LittleYOLO-SPP based on the YOLOv3-tiny network is proposed to detect vehicles effectively in real-time. The YOLOv3-tiny object detection network is improved by modifying its feature extraction network to increase the speed and accuracy of vehicle detection. The proposed network incorporated Spatial pyramid pooling into the network, which consists of different scales of pooling layers for concatenation of features to enhance network learning capability. The Mean square error (MSE) and Generalized IoU (GIoU) loss function for bounding box regression is used to increase the performance of the network. The network training includes vehicle-based classes from PASCAL VOC 2007,2012 and MS COCO 2014 datasets such as car, bus, and truck. LittleYOLO-SPP network detects the vehicle in real-time with high accuracy regardless of video frame and weather conditions. The improved network achieves a higher mAP of 77.44% on PASCAL VOC and 52.95% mAP on MS COCO datasets.      
### 42.Hamiltonian Q-Learning: Leveraging Importance-sampling for Data Efficient RL  [ :arrow_down: ](https://arxiv.org/pdf/2011.05927.pdf)
>  Model-free reinforcement learning (RL), in particular Q-learning is widely used to learn optimal policies for a variety of planning and control problems. However, when the underlying state-transition dynamics are stochastic and high-dimensional, Q-learning requires a large amount of data and incurs a prohibitively high computational cost. In this paper, we introduce Hamiltonian Q-Learning, a data efficient modification of the Q-learning approach, which adopts an importance-sampling based technique for computing the Q function. To exploit stochastic structure of the state-transition dynamics, we employ Hamiltonian Monte Carlo to update Q function estimates by approximating the expected future rewards using Q values associated with a subset of next states. Further, to exploit the latent low-rank structure of the dynamic system, Hamiltonian Q-Learning uses a matrix completion algorithm to reconstruct the updated Q function from Q value updates over a much smaller subset of state-action pairs. By providing an efficient way to apply Q-learning in stochastic, high-dimensional problems, the proposed approach broadens the scope of RL algorithms for real-world applications, including classical control tasks and environmental monitoring.      
### 43.Multi-Frequency Canonical Correlation Analysis (MFCCA): An Extended Decoding Algorithm for Multi-Frequency SSVEP  [ :arrow_down: ](https://arxiv.org/pdf/2011.05861.pdf)
>  Stimulation methods that utilise more than one stimulation frequency have been developed in steady-state visual evoked potential (SSVEP) brain-computer interfaces (BCIs) for the purpose of increasing the number of targets that can be presented simultaneously. However, there is no unified decoding algorithm that can be applied to a large class of multi-frequency stimulated SSVEP settings. This paper extends the widely used canonical correlation analysis (CCA) decoder to explicitly accommodate multi-frequency SSVEP by exploiting the interactions between the multiple stimulation frequencies. A concept "order" was defined as the sum of absolute values of the coefficients in the linear interaction. The probability distribution of the order in the resulting SSVEP response was then used to improve decoding accuracy. Results show that, compared to the standard CCA formulation, the proposed multi-frequency CCA (MFCCA) has a 20% improvement in decoding accuracy on average at order 2. Although the proposed methods were only tested with two input frequencies, the technique is capable of handling more than two simultaneous input frequencies.      
### 44.Cryo-RALib -- a modular library for accelerating alignment in cryo-EM  [ :arrow_down: ](https://arxiv.org/pdf/2011.05755.pdf)
>  With the enhancement of algorithms, cryo-EM has become the most efficient technique to solve structures of molecules. Take a recent event for example, after the outbreak of COVID-19 in January, the first structure of 2019-nCoV Spike trimer was published in March using cryo-EM, which has provided crucial medical insight for developing vaccines. The enabler behind this efficiency is the GPU-accelerated computation which shortens the whole analysis process to 12 days. However, the data characteristics include strong noise, huge dimension, large sample size and high heterogeneity with unknown orientations have made analysis very challenging. Though, the popular GPU-accelerated Bayesian approach has been shown to be successful in 3D refinement. It is noted that the traditional method based on multireference alignment may better differentiate subtle structure differences under low signal to noise ratio (SNR). However, a modular GPU-acceleration package for multireference alignment is still lacking in the field. In this work, a modular GPU-accelerated alignment library called Cryo-RALib is proposed. The library contains both reference-free alignment and multireference alignment that can be widely used to accelerate state-of-the-art classification algorithms. In addition, we connect the cryo-EM image analysis with the python data science stack which enables users to perform data analysis, visualization and inference more easily. Benchmark on the TaiWan Computing Cloud container, our implementation can accelerate the computation by one order of magnitude. The library has been made publicly available at <a class="link-external link-https" href="https://github.com/phonchi/Cryo-RAlib" rel="external noopener nofollow">this https URL</a>.      
### 45.Noise Conscious Training of Non Local Neural Network powered by Self Attentive Spectral Normalized Markovian Patch GAN for Low Dose CT Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2011.05684.pdf)
>  The explosive rise of the use of Computer tomography (CT) imaging in medical practice has heightened public concern over the patient's associated radiation dose. However, reducing the radiation dose leads to increased noise and artifacts, which adversely degrades the scan's interpretability. Consequently, an advanced image reconstruction algorithm to improve the diagnostic performance of low dose ct arose as the primary concern among the researchers, which is challenging due to the ill-posedness of the problem. In recent times, the deep learning-based technique has emerged as a dominant method for low dose CT(LDCT) denoising. However, some common bottleneck still exists, which hinders deep learning-based techniques from furnishing the best performance. In this study, we attempted to mitigate these problems with three novel accretions. First, we propose a novel convolutional module as the first attempt to utilize neighborhood similarity of CT images for denoising tasks. Our proposed module assisted in boosting the denoising by a significant margin. Next, we moved towards the problem of non-stationarity of CT noise and introduced a new noise aware mean square error loss for LDCT denoising. Moreover, the loss mentioned above also assisted to alleviate the laborious effort required while training CT denoising network using image patches. Lastly, we propose a novel discriminator function for CT denoising tasks. The conventional vanilla discriminator tends to overlook the fine structural details and focus on the global agreement. Our proposed discriminator leverage self-attention and pixel-wise GANs for restoring the diagnostic quality of LDCT images. Our method validated on a publicly available dataset of the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge performed remarkably better than the existing state of the art method.      
### 46.FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2011.05670.pdf)
>  Deep learning techniques have provided significant improvements in hyperspectral image (HSI) classification. The current deep learning based HSI classifiers follow a patch-based learning framework by dividing the image into overlapping patches. As such, these methods are local learning methods, which have a high computational cost. In this paper, a fast patch-free global learning (FPGA) framework is proposed for HSI classification. In FPGA, an encoder-decoder based FCN is utilized to consider the global spatial information by processing the whole image, which results in fast inference. However, it is difficult to directly utilize the encoder-decoder based FCN for HSI classification as it always fails to converge due to the insufficiently diverse gradients caused by the limited training samples. To solve the divergence problem and maintain the abilities of FCN of fast inference and global spatial information mining, a global stochastic stratified sampling strategy is first proposed by transforming all the training samples into a stochastic sequence of stratified samples. This strategy can obtain diverse gradients to guarantee the convergence of the FCN in the FPGA framework. For a better design of FCN architecture, FreeNet, which is a fully end-to-end network for HSI classification, is proposed to maximize the exploitation of the global spatial information and boost the performance via a spectral attention based encoder and a lightweight decoder. A lateral connection module is also designed to connect the encoder and decoder, fusing the spatial details in the encoder and the semantic features in the decoder. The experimental results obtained using three public benchmark datasets suggest that the FPGA framework is superior to the patch-based framework in both speed and accuracy for HSI classification. Code has been made available at: <a class="link-external link-https" href="https://github.com/Z-Zheng/FreeNet" rel="external noopener nofollow">this https URL</a>.      
### 47.LMB Filter Based Tracking Allowing for Multiple Hypotheses in Object Reference Point Association*  [ :arrow_down: ](https://arxiv.org/pdf/2011.05657.pdf)
>  Autonomous vehicles need precise knowledge on dynamic objects in their surroundings. Especially in urban areas with many objects and possible occlusions, an infrastructure system based on a multi-sensor setup can provide the required environment model for the vehicles. Previously, we have published a concept of object reference points (e.g. the corners of an object), which allows for generic sensor "plug and play" interfaces and relatively cheap sensors. This paper describes a novel method to additionally incorporate multiple hypotheses for fusing the measurements of the object reference points using an extension to the previously presented Labeled Multi-Bernoulli (LMB) filter. In contrast to the previous work, this approach improves the tracking quality in the cases where the correct association of the measurement and the object reference point is unknown. Furthermore, this paper identifies options based on physical models to sort out inconsistent and unfeasible associations at an early stage in order to keep the method computationally tractable for real-time applications. The method is evaluated on simulations as well as on real scenarios. In comparison to comparable methods, the proposed approach shows a considerable performance increase, especially the number of non-continuous tracks is decreased significantly.      
### 48.Adversarial images for the primate brain  [ :arrow_down: ](https://arxiv.org/pdf/2011.05623.pdf)
>  Deep artificial neural networks have been proposed as a model of primate vision. However, these networks are vulnerable to adversarial attacks, whereby introducing minimal noise can fool networks into misclassifying images. Primate vision is thought to be robust to such adversarial images. We evaluated this assumption by designing adversarial images to fool primate vision. To do so, we first trained a model to predict responses of face-selective neurons in macaque inferior temporal cortex. Next, we modified images, such as human faces, to match their model-predicted neuronal responses to a target category, such as monkey faces. These adversarial images elicited neuronal responses similar to the target category. Remarkably, the same images fooled monkeys and humans at the behavioral level. These results challenge fundamental assumptions about the similarity between computer and primate vision and show that a model of neuronal activity can selectively direct primate visual behavior.      
### 49.Interference Impact on Decode-and-Forward Relay Networks with RIS-Assisted Source and Relays  [ :arrow_down: ](https://arxiv.org/pdf/2011.05619.pdf)
>  In this letter, we consider the scenario of decode-and-forward relay network with reconfigurable intelligent surface (RIS)-assisted source and relays in the presence of interference. We derive approximate closed-form expression for the system outage probability assuming Rayleigh fading channels and opportunistic relaying scheme. In addition, we study the system behavior at the high signal-to-noise ratio (SNR) regime, where the diversity order and coding gain are obtained and analyzed. The results show that the system can achieve a diversity order of Gd = min(N1,N2)K, where N1 and N2 are the numbers of reflecting elements at the source and relays, respectively, and K is the number of relays. In addition, findings illustrate that for the same diversity order, utilizing one relay with multiple reflecting elements gives better performance than utilizing multiple relays with a single reflecting element. Furthermore, findings illustrate that the interference at the destination is more severe on the system performance than the interference at the relays. Therefore, under the same interference powers and for a fixed number of relays K, results show that the case where the first hop is dominating the performance N1 &lt; N2 gives better results in terms of coding gain than the case where N2 &lt; N1.      
### 50.Performance Analysis of RIS-Assisted Source Mixed RF/FSO Relay Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.05612.pdf)
>  This letter proposes and evaluates the performance of reconfigurable intelligent surface (RIS)-assisted source multiuser mixed radio frequency (RF)/free space optical (FSO) relay network with opportunistic user scheduling. Closed-form analytical approximations are derived for the outage probability and average symbol error probability (ASEP) assuming Rayleigh and Gamma-Gamma fading models for the RF and FSO channels, respectively. Moreover, to gain more insight into the system behavior, the system is studied at the high signal-to-noise ratio (SNR) regime whereby the diversity order and coding gain are derived and analyzed. The results illustrate that the system performance is dominated by the worst hop and that the diversity order is equal to Gd = min(KN,alpha,beta,zeta^2), where K is the number of RF users, N is the number of reflecting elements at each user, alpha and beta are the atmospheric turbulence parameters of the FSO link, and zeta^2 is a measure for the alignment quality of the FSO link. In addition, findings show that for the same diversity order, N is more impactful on the system performance than K through the coding gain Gc.      
### 51.Deep Time Delay Neural Network for Speech Enhancement with Full Data Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.05591.pdf)
>  Recurrent neural networks (RNNs) have shown significant improvements in recent years for speech enhancement. However, the model complexity and inference time cost of RNNs are much higher than deep feed-forward neural networks (DNNs). Therefore, these limit the applications of speech enhancement. This paper proposes a deep time delay neural network (TDNN) for speech enhancement with full data learning. The TDNN has excellent potential for capturing long range temporal contexts, which utilizes a modular and incremental design. Besides, the TDNN preserves the feed-forward structure so that its inference cost is comparable to standard DNN. To make full use of the training data, we propose a full data learning method for speech enhancement. More specifically, we not only use the noisy-to-clean (input-to-target) to train the enhanced model, but also the clean-to-clean and noise-to-silence data. Therefore, all of the training data can be used to train the enhanced model. Our experiments are conducted on TIMIT dataset. Experimental results show that our proposed method could achieve a better performance than DNN and comparable even better performance than BLSTM. Meanwhile, compared with the BLSTM, the proposed method drastically reduce the inference time.      
### 52.Optimized Power Control for Over-the-Air Federated Edge Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.05587.pdf)
>  Over-the-air federated edge learning (Air-FEEL) is a communication-efficient solution for privacy-preserving distributed learning over wireless networks. Air-FEEL allows "one-shot" over-the-air aggregation of gradient/model-updates by exploiting the waveform superposition property of wireless channels, and thus promises an extremely low aggregation latency that is independent of the network size. However, such communication efficiency may come at a cost of learning performance degradation due to the aggregation error caused by the non-uniform channel fading over devices and noise perturbation. Prior work adopted channel inversion power control (or its variants) to reduce the aggregation error by aligning the channel gains, which, however, could be highly suboptimal in deep fading scenarios due to the noise amplification. To overcome this issue, we investigate the power control optimization for enhancing the learning performance of Air-FEEL. Towards this end, we first analyze the convergence behavior of the Air-FEEL by deriving the optimality gap of the loss-function under any given power control policy. Then we optimize the power control to minimize the optimality gap for accelerating convergence, subject to a set of average and maximum power constraints at edge devices. The problem is generally non-convex and challenging to solve due to the coupling of power control variables over different devices and iterations. To tackle this challenge, we develop an efficient algorithm by jointly exploiting the successive convex approximation (SCA) and trust region methods. Numerical results show that the optimized power control policy achieves significantly faster convergence than the benchmark policies such as channel inversion and uniform power transmission.      
### 53.Recognizing More Emotions with Less Data Using Self-supervised Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.05585.pdf)
>  We propose a novel transfer learning method for speech emotion recognition allowing us to obtain promising results when only few training data is available. With as low as 125 examples per emotion class, we were able to reach a higher accuracy than a strong baseline trained on 8 times more data. Our method leverages knowledge contained in pre-trained speech representations extracted from models trained on a more general self-supervised task which doesn't require human annotations, such as the wav2vec model. We provide detailed insights on the benefits of our approach by varying the training data size, which can help labeling teams to work more efficiently. We compare performance with other popular methods on the IEMOCAP dataset, a well-benchmarked dataset among the Speech Emotion Recognition (SER) research community. Furthermore, we demonstrate that results can be greatly improved by combining acoustic and linguistic knowledge from transfer learning. We align acoustic pre-trained representations with semantic representations from the BERT model through an attention-based recurrent neural network. Performance improves significantly when combining both modalities and scales with the amount of data. When trained on the full IEMOCAP dataset, we reach a new state-of-the-art of 73.9% unweighted accuracy (UA).      
### 54.Docking two multirotors in midair using relative vision measurements  [ :arrow_down: ](https://arxiv.org/pdf/2011.05565.pdf)
>  Modular robots have been rising in popularity for a variety of applications, and autonomous midair docking is a necessary task for real world deployment of these robots. We present a state estimator based on the extended Kalman filter for relative localization of one multirotor with respect to another using only onboard sensors, specifically an inertial measurement unit and a camera-marker pair. Acceleration and angular velocity measurements along with relative pose measurements from a camera on the first multirotor looking at a marker on the second multirotor are used to estimate the relative position and velocity of the first multirotor with respect to the second, and the absolute attitude of the first multirotor. We also present a control architecture to use these onboard state estimates to control the first multirotor at a desired setpoint with respect to the second. The performance of the estimator and control architecture are experimentally validated by successfully and repeatably performing midair docking -- a task that requires relative position precision on the order of a centimeter.      
### 55.Stability of Gradient Learning Dynamics in Continuous Games: Vector Action Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2011.05562.pdf)
>  Towards characterizing the optimization landscape of games, this paper analyzes the stability and spectrum of gradient-based dynamics near fixed points of two-player continuous games. We introduce the quadratic numerical range as a method to bound the spectrum of game dynamics linearized about local equilibria. We also analyze the stability of differential Nash equilibria and their robustness to variation in agent's learning rates. Our results show that by decomposing the game Jacobian into symmetric and anti-symmetric components, we can assess the contribution of vector field's potential and rotational components to the stability of the equilibrium. In zero-sum games, all differential Nash equilibria are stable; in potential games, all stable points are Nash. Furthermore, zero-sum Nash equilibria are robust in the sense that they are stable for all learning rates. For continuous games with general costs, we provide a sufficient condition for instability. We conclude with a numerical example that investigates how players with different learning rates can take advantage of rotational components of the game to converge faster.      
### 56.End-to-End Chinese Landscape Painting Creation Using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.05552.pdf)
>  Current GAN-based art generation methods produce unoriginal artwork due to their dependence on conditional input. Here, we propose Sketch-And-Paint GAN (SAPGAN), the first model which generates Chinese landscape paintings from end to end, without conditional input. SAPGAN is composed of two GANs: SketchGAN for generation of edge maps, and PaintGAN for subsequent edge-to-painting translation. Our model is trained on a new dataset of traditional Chinese landscape paintings never before used for generative research. A 242-person Visual Turing Test study reveals that SAPGAN paintings are mistaken as human artwork with 55% frequency, significantly outperforming paintings from baseline GANs. Our work lays a groundwork for truly machine-original art generation.      
### 57.Automatic Open-World Reliability Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2011.05506.pdf)
>  Image classification in the open-world must handle out-of-distribution (OOD) images. Systems should ideally reject OOD images, or they will map atop of known classes and reduce reliability. Using open-set classifiers that can reject OOD inputs can help. However, optimal accuracy of open-set classifiers depend on the frequency of OOD data. Thus, for either standard or open-set classifiers, it is important to be able to determine when the world changes and increasing OOD inputs will result in reduced system reliability. However, during operations, we cannot directly assess accuracy as there are no labels. Thus, the reliability assessment of these classifiers must be done by human operators, made more complex because networks are not 100% accurate, so some failures are to be expected. To automate this process, herein, we formalize the open-world recognition reliability problem and propose multiple automatic reliability assessment policies to address this new problem using only the distribution of reported scores/probability data. The distributional algorithms can be applied to both classic classifiers with SoftMax as well as the open-world Extreme Value Machine (EVM) to provide automated reliability assessment. We show that all of the new algorithms significantly outperform detection using the mean of SoftMax.      
### 58.ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2011.05479.pdf)
>  Characterizing the processes leading to deforestation is critical to the development and implementation of targeted forest conservation and management policies. In this work, we develop a deep learning model called ForestNet to classify the drivers of primary forest loss in Indonesia, a country with one of the highest deforestation rates in the world. Using satellite imagery, ForestNet identifies the direct drivers of deforestation in forest loss patches of any size. We curate a dataset of Landsat 8 satellite images of known forest loss events paired with driver annotations from expert interpreters. We use the dataset to train and validate the models and demonstrate that ForestNet substantially outperforms other standard driver classification approaches. In order to support future research on automated approaches to deforestation driver classification, the dataset curated in this study is publicly available at <a class="link-external link-https" href="https://stanfordmlgroup.github.io/projects/forestnet" rel="external noopener nofollow">this https URL</a> .      
### 59.Artificial sound change: Language change and deep convolutional neural networks in iterative learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.05463.pdf)
>  This paper proposes a framework for modeling sound change that combines deep convolutional neural networks and iterative learning. Acquisition and transmission of speech across generations is modeled by training generations of Generative Adversarial Networks (Goodfellow et al. <a class="link-https" data-arxiv-id="1406.2661" href="https://arxiv.org/abs/1406.2661">arXiv:1406.2661</a>,Donahue et al. <a class="link-https" data-arxiv-id="1705.07904" href="https://arxiv.org/abs/1705.07904">arXiv:1705.07904</a>) on unannotated raw speech data. The paper argues that several properties of sound change emerge from the proposed architecture. Four generations of Generative Adversarial Networks were trained on an allophonic distribution in English where voiceless stops are aspirated word-initially before stressed vowels except if preceded by [s]. The first generation of networks is trained on the relevant sequences in human speech from the TIMIT database. The subsequent generations are not trained on TIMIT, but on generated outputs from the previous generation and thus start learning from each other in an iterative learning task. The initial allophonic distribution is progressively being lost with each generation, likely due to pressures from the global distribution of aspiration in the training data that resembles phonological pressures in natural language. The networks show signs of a gradual shift in phonetic targets characteristic of a gradual phonetic sound change. At endpoints, the networks' outputs superficially resemble a phonological change -- rule loss -- driven by imperfect learning. The model features signs of stability, one of the more challenging aspects of computational models of sound change. The results suggest that the proposed Generative Adversarial models of phonetic and phonological acquisition have the potential to yield new insights into the long-standing question of how to model language change.      
### 60.Minimum Variance and Covariance Steering Based on Affine Disturbance Feedback Control Parameterization  [ :arrow_down: ](https://arxiv.org/pdf/2011.05394.pdf)
>  The goal of this paper is to address finite-horizon minimum variance and covariance steering problems for discrete-time stochastic (Gaussian) linear systems. On the one hand, the minimum variance problem seeks for a control policy that will steer the state mean of an uncertain system to a prescribed quantity while minimizing the trace of its terminal state covariance (or variance). On the other hand, the covariance steering problem seeks for a control policy that will steer the covariance of the terminal state to a prescribed positive definite matrix. We propose a solution approach that relies on the stochastic version of the affine disturbance feedback control parametrization according to which the control input at each stage can be expressed as an affine function of the history of disturbances that have acted upon the system. Our analysis reveals that this particular parametrization allows one to reduce the stochastic optimal control problems considered herein into tractable convex programs with essentially the same decision variables. This is in contrast with other control policy parametrizations, such as the state feedback parametrization, in which the decision variables of the convex program do not coincide with the controller's parameters of the stochastic optimal control problem. In addition, we propose a variation of the control parametrization which relies on truncated histories of past disturbances. We show that by selecting the length of the truncated sequences appropriately, we can design suboptimal controllers which can strike the desired balance between performance and computational cost.      
### 61.Smoking effect on the circadian rhythm of blood pressure in hypertensive subjects  [ :arrow_down: ](https://arxiv.org/pdf/2011.05392.pdf)
>  The use of office measurement of Blood Pressure (BP) as well as of the mean on day-time, on night-time or on 24h does not accurately describe the changes of the BP circadian rhythm. Moreover, several risk factors affect this rhythm but until now possible alterations, due to the presence of such risk factors considered separately, were not been yet studied. Cigarette smoking is one of the most relevant risk factors increasing cardiovascular morbidity and mortality. The aim of this study is to evaluate quantitatively and with a suitable temporal detail how the smoking influences the BP circadian rhythm in normotensive and hypertensive subjects excluding those who presented other risk factors like obesity, dyslipidemia and diabetes mellitus. Holter BP monitoring coming from 618 subjects was used and the behaviour on 24h was examined separately in normotensive and hypertensive subjects either smokers or non-smokers. Four intervals with alternate different characteristics were found in the BP rhythm and regression lines approximated them in order to evaluate the changing rate of BP in each period. Results showed higher values from 10:00 to 02:00 in hypertensive smokers than non-smokers and significant differences between normotensive smokers and non-smokers between 10:00 and 19:00. The changing rate between 10:00 and 14:30 was higher in non-smokers than in smokers for both normotensive and hypertensive subjects while the opposite was found in the other three periods. The different velocity rates of BP changes during 24h, could be associated with different risk levels of cardiovascular disease.      
### 62.Stochastic generalized Nash equilibrium seeking under partial-decision information  [ :arrow_down: ](https://arxiv.org/pdf/2011.05357.pdf)
>  We consider for the first time a stochastic generalized Nash equilibrium problem, i.e., with expected-value cost functions and joint feasibility constraints, under partial-decision information, meaning that the agents communicate only with some trusted neighbours. We propose several distributed algorithms for network games and aggregative games that we show being special instances of a preconditioned forward-backward splitting method. We prove that the algorithms converge to a generalized Nash equilibrium when the forward operator is restricted cocoercive by using the stochastic approximation scheme with variance reduction to estimate the expected value of the pseudogradient.      
