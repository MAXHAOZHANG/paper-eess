# ArXiv eess --Tue, 24 Nov 2020
### 1.A framework for constrained static state estimation in unbalanced distribution networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.11614.pdf)
>  State estimation plays a key role in the transition from the passive to the active operation of distribution systems, as it allows to monitor these networks and, successively, to perform control actions. However, designing state estimators for distribution systems carries a significant amount of challenges. This is due to the physical complexity of the networks, e.g., phase unbalance, and limited measurements. Furthermore, the features of the distribution system present significant local variations, e.g., voltage level and number and type of customers, which makes it hard to design a "one-size-fits-all" state estimator. The present paper introduces a unifying framework that allows to easily implement and compare diverse unbalanced static state estimation models. This is achieved by formulating state estimation as a general constrained optimization problem. The advantages of this approach are described and supported by numerical illustration on a large set of distribution feeders. The framework is also implemented and made available open-source.      
### 2.Extremely Sparse Co-Prime Sensing: Low Latency Estimation is not a Dream but a Reality  [ :arrow_down: ](https://arxiv.org/pdf/2011.11608.pdf)
>  Co-prime sensing is a sub-Nyquist technique for signal acquisition. Several modifications to the prototype co-prime array have been proposed in the literature. Researchers have also demonstrated low latency estimation. This paper describes the functioning of a Family of Adjustable Pivot Co-Prime Arrays. However, the main focus is to introduce the reader to a concept called Extremely Sparse Co-Prime Arrays and Samplers. Adjustable pivot co-prime arrays are a special case of the extremely sparse arrays. The closed-form expressions for the weight function and the correlogram bias window are derived. Low latency estimation is demonstrated using the extremely sparse co-prime scheme. Furthermore, a multidimensional and a hybrid extremely sparse co-prime array is proposed as a straightforward extension of the 1D-theory. Finally, a generalized extremely sparse structure is developed with several design parameters. Most existing structures may be viewed as a special case of the generalized scheme.      
### 3.Urgency-aware Optimal Routing in Repeated Games through Artificial Currencies  [ :arrow_down: ](https://arxiv.org/pdf/2011.11595.pdf)
>  When people choose routes minimizing their individual delay, the aggregate congestion can be much higher compared to that experienced by a centrally-imposed routing. Yet centralized routing is incompatible with the presence of self-interested agents. How can we reconcile the two? In this paper we address this question within a repeated game framework and propose a fair incentive mechanism based on artificial currencies that routes selfish agents in a system-optimal fashion, while accounting for their temporal preferences. We instantiate the framework in a parallel-network whereby agents commute repeatedly (e.g., daily) from a common start node to the end node. Thereafter, we focus on the specific two-arcs case whereby, based on an artificial currency, the agents are charged when traveling on the first, fast arc, whilst they are rewarded when traveling on the second, slower arc. We assume the agents to be rational and model their choices through a game where each agent aims at minimizing a combination of today's discomfort, weighted by their urgency, and the average discomfort encountered for the rest of the period (e.g., a week). We show that, if prices of artificial currencies are judiciously chosen, the routing pattern converges to a system-optimal solution, while accommodating the agents' urgency. We complement our study through numerical simulations. Our results show that it is possible to achieve a system-optimal solution whilst reducing the agents' perceived discomfort by 14-20% when compared to a centralized optimal but urgency-unaware policy.      
### 4.Power Market Tool (POMATO) for the Analysis of Zonal Electricity Markets  [ :arrow_down: ](https://arxiv.org/pdf/2011.11594.pdf)
>  The proposed open-source Power Market Tool (POMATO) aims to enable research on interconnected modern and future electricity markets in the context of the physical transmission system and its secure operation. POMATO has been designed to study capacity allocation and congestion management (CACM) policies of European zonal electricity markets, especially flow-based market coupling (FBMC). For this purpose, POMATO implements methods for the analysis of simultaneous zonal market clearing, nodal (N-k secure) power flow computation for capacity allocation, and multi-stage market clearing with adaptive grid representation and redispatch. The computationally demanding N-k secure power flow is enabled via an efficient constraint reduction algorithm. POMATO provides an integrated environment for data read-in, pre- and post-processing and interactive result visualization. Comprehensive data sets of European electricity systems compiled from Open Power System Data and Matpower Cases are part of the distribution. POMATO is implemented in Python and Julia, leveraging Python's easily maintainable data processing and user interaction features and Julia's well readable algebraic modeling language, superior computational performance and interfaces to open-source and commercial solvers.      
### 5.Using Synthetic Audio to Improve The Recognition of Out-Of-Vocabulary Words in End-To-End ASR Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.11564.pdf)
>  Today, many state-of-the-art automatic speech recognition (ASR) systems apply all-neural models that map audio to word sequences trained end-to-end along one global optimisation criterion in a fully data driven fashion. These models allow high precision ASR for domains and words represented in the training material but have difficulties recognising words that are rarely or not at all represented during training, i.e. trending words and new named entities. In this paper, we use a text-to-speech (TTS) engine to provide synthetic audio for out-of-vocabulary (OOV) words. We aim to boost the recognition accuracy of a recurrent neural network transducer (RNN-T) on OOV words by using those extra audio-text pairs, while maintaining the performance on the non-OOV words. Different regularisation techniques are explored and the best performance is achieved by fine-tuning the RNN-T on both original training data and extra synthetic data with elastic weight consolidation (EWC) applied on encoder. This yields 57% relative word error rate (WER) reduction on utterances containing OOV words without any degradation on the whole test set.      
### 6.Influence of some cardiovascular risk factors on the rela-tionship between age and blood pressure  [ :arrow_down: ](https://arxiv.org/pdf/2011.11530.pdf)
>  Blood Pressure (BP) is a biological signal related to the cardiovascular system that inevitably is affected by ageing. Moreover, it is also influenced by the presence of cardiovascular risk factors. To evaluate how the relationship be-tween BP and age changes with the presence of risk factors in hypertensive and normotensive subjects, we analyzed 880 subjects with and without smoking, obe-sity, diabetes mellitus and dyslipidemia. A regression line fitted each BP/Age relation calculated separately for normotensive and hypertensive subjects with and without risk factors. For each of the four conditions the office and the 24-hour ambulatory BP monitoring (ABPM) were considered. In subjects with and without risk factors, the slopes of the Systolic BP/Age relation were higher in hypertensive than in normotensive subjects in both office and ABPM conditions. Moreover, the presence of risk factors modified the Systolic BP/Age relation in hypertensive subjects by using either office or ABPM measurements. Finally, we confirmed that the difference between the two modalities depends on age too.      
### 7.Influence of the gender on the relationship between heart rate and blood pressure  [ :arrow_down: ](https://arxiv.org/pdf/2011.11519.pdf)
>  Blood Pressure (BP) and Heart Rate (HR) provide information on clin-ical condition along 24h. Both signals present circadian changes due to sympa-thetic/parasympathetic control system that influence the relationship between them. Moreover, also the gender could modify this relation, acting on both con-trol systems. Some studies, using office measurements examined the BP/HR re-lation, highlighting a direct association between the two variables, linked to sus-pected coronary heart disease. Nevertheless, till now such relation has not been studied yet using ambulatory technique that is known to lead to additional prog-nostic information about cardiovascular risks. In order to examine in a more ac-curate way this relation, in this work we evaluate the influence of gender on the BP/HR relationship by using hour-to-hour 24h ambulatory measurements. Data coming from 122 female and 50 male normotensive subjects were recorded using a Holter Blood Pressure Monitor and the parameters of the linear regression fit-ting BP/HR were calculated. Results confirmed those obtained in previous stud-ies using punctual office measures in males and underlined a significant relation between Diastolic BP and HR during each hour of the day in females; a different trend in the BP/HR relation between genders was found only during night-time. Moreover, the circadian rhythm of BP/HR is similar in both genders but with different values of HR and BP at different times of the day.      
### 8.A Two-Layer Framework with Battery Temperature Optimal Control and Network Optimal Power Flow  [ :arrow_down: ](https://arxiv.org/pdf/2011.11496.pdf)
>  Battery energy storage is an essential component of a microgrid. The working temperature of the battery is an important factor as a high-temperature condition generally increases losses, reduces useful life, and can even lead to fire hazards. Hence, it is indispensable to regulate the temperature profile of the battery modules/packs properly in the battery energy storage during the operation. In view of this, a two-layer optimal control and operation scheme is proposed for a microgrid with energy storage. In the first layer, an optimal control model is formed to derive the optimal control policy that minimizes the control efforts, consisting of the fan speed and battery current magnitude, in order to achieve a temperature distribution reference over the battery modules. In the second layer, the system operator of the microgrid performs an optimal power flow to search for the optimal temperature distribution reference used in the first stage and the corresponding operating current of the battery that minimize the operation cost of the entire microgrid system. This two-layer scheme offers a great computational benefit that allows for large-scale integration of batteries. A case study is performed on the proposed two-layer model to illustrate its performance.      
### 9.A hybrid barrier certificate approach to satisfy linear temporal logic specifications  [ :arrow_down: ](https://arxiv.org/pdf/2011.11464.pdf)
>  In this work we formulate the satisfaction of a (syntactically co-safe) linear temporal logic specification on a physical plant through a recent hybrid dynamical systems formalism. In order to solve this problem, we introduce an extension to such a hybrid system framework of the so-called eventuality property, which matches suitably the condition for the satisfaction of such a temporal logic specification. The eventuality property can be established through barrier certificates, which we derive for the considered hybrid system framework. Using a hybrid barrier certificate, we propose a solution to the original problem. Simulations illustrate the effectiveness of the proposed method.      
### 10.Automatic Detection and Classification of Tick-borne Skin Lesions using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.11459.pdf)
>  Around the globe, ticks are the culprit of transmitting a variety of bacterial, viral and parasitic diseases. The incidence of tick-borne diseases has drastically increased within the last decade, with annual cases of Lyme disease soaring to an estimated 300,000 in the United States alone. As a result, more efforts in improving lesion identification approaches and diagnostics for tick-borne illnesses is critical. The objective for this study is to build upon the approach used by Burlina et al. by using a variety of convolutional neural network models to detect tick-borne skin lesions. We expanded the data inputs by acquiring images from Google in seven different languages to test if this would diversify training data and improve the accuracy of skin lesion detection. The final dataset included nearly 6,080 images and was trained on a combination of architectures (ResNet 34, ResNet 50, VGG 19, and Dense Net 121). We obtained an accuracy of 80.72% with our model trained on the DenseNet 121 architecture.      
### 11.Robust super-resolution depth imaging via a multi-feature fusion deep network  [ :arrow_down: ](https://arxiv.org/pdf/2011.11444.pdf)
>  Three-dimensional imaging plays an important role in imaging applications where it is necessary to record depth. The number of applications that use depth imaging is increasing rapidly, and examples include self-driving autonomous vehicles and auto-focus assist on smartphone cameras. Light detection and ranging (LIDAR) via single-photon sensitive detector (SPAD) arrays is an emerging technology that enables the acquisition of depth images at high frame rates. However, the spatial resolution of this technology is typically low in comparison to the intensity images recorded by conventional cameras. To increase the native resolution of depth images from a SPAD camera, we develop a deep network built specifically to take advantage of the multiple features that can be extracted from a camera's histogram data. The network is designed for a SPAD camera operating in a dual-mode such that it captures alternate low resolution depth and high resolution intensity images at high frame rates, thus the system does not require any additional sensor to provide intensity images. The network then uses the intensity images and multiple features extracted from downsampled histograms to guide the upsampling of the depth. Our network provides significant image resolution enhancement and image denoising across a wide range of signal-to-noise ratios and photon levels. We apply the network to a range of 3D data, demonstrating denoising and a four-fold resolution enhancement of depth.      
### 12.Analysis of Empirical Mode Decomposition-based Load and Renewable Time Series Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2011.11410.pdf)
>  The empirical mode decomposition (EMD) method and its variants have been extensively employed in the load and renewable forecasting literature. Using this multiresolution decomposition, time series (TS) related to the historical load and renewable generation are decomposed into several intrinsic mode functions (IMFs), which are less non-stationary and non-linear. As such, the prediction of the components can theoretically be carried out with notably higher precision. The EMD method is prone to several issues, including modal aliasing and boundary effect problems, but the TS decomposition-based load and renewable generation forecasting literature primarily focuses on comparing the performance of different decomposition approaches from the forecast accuracy standpoint; as a result, these problems have rarely been scrutinized. Underestimating these issues can lead to poor performance of the forecast model in real-time applications. This paper examines these issues and their importance in the model development stage. Using real-world data, EMD-based models are presented, and the impact of the boundary effect is illustrated.      
### 13.Optimal Power Control for DoS Attack over Fading Channel: A Game-Theoretic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2011.11373.pdf)
>  In this paper, we investigate remote state estimation against an intelligent denial-of-service (DoS) attack over a vulnerable wireless network whose channel undergoes attenuation and distortion caused by fading. We use the sensor to observe system states and transmit its local state estimates to the remote center. Meanwhile, the attacker injects a jamming signal to destroy the packet accepted by the remote center and causes the performance degradation. Most of the existing works are built on a time-invariant channel state information (CSI) model in which the channel fading is stationary. However, the wireless communication channels are more prone to dynamic changes. To capture this time-variant property in the channel quality of the real-world networks, we study the fading channel network whose channel model is characterized by a generalized finite-state Markov chain. With the goals of two players in infinite-time horizon, we describe the conflicting characteristic between the attacker and the sensor with a general-sum stochastic game. Moreover, the Q-learning techniques are applied to obtain an optimal strategy pair at a Nash equilibrium. Also the monotone structure of the optimal stationary strategy is constructed under a sufficient condition. Besides, when channel gain is known a priori, except for the full Channel State Information (CSI), we also investigate the partial CSI, where Bayesian games are employed. Based on the player's own channel information and the belief on the channel distribution of other players, the energy strategy at a Nash equilibrium is obtained.      
### 14.Modeling and Architecture Design of Intelligent Reflecting Surfaces using Scattering Parameter Network Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2011.11362.pdf)
>  Intelligent reflecting surfaces (IRSs) are an emerging technology for future wireless communication. The vast majority of recent research on IRS has focused on system level optimizations. However, developing straightforward and tractable electromagnetic (EM) models that are suitable for IRS aided communication modeling remains an open issue. In this paper, we address this issue and derive communication models by using rigorous scattering parameter network analysis. We also propose new IRS architectures based on group and fully connected reconfigurable impedance networks, which are more general and more efficient than conventional single connected reconfigurable impedance network. In addition, the scaling law of the received signal power of an IRS aided system with reconfigurable impedance networks is also derived. Compared with the single connected reconfigurable impedance network, our group and fully connected reconfigurable impedance network can increase the received signal power by up to 62%, or maintain the same received signal power with a number of IRS elements reduced by up to 21%. We also investigate the proposed architecture in deployments with distance-dependent pathloss and Rician fading channel, and show that the proposed group and fully connected reconfigurable impedance networks outperform the single connected case by up to 34% and 48%, respectively.      
### 15.Data-Driven Stabilization of Nonlinear Systems with Rational Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2011.11355.pdf)
>  In this paper, we present a data-driven controller design method for continuous-time nonlinear systems with rational system dynamics, using no model knowledge but only measured data affected by noise. We first extend recent results on data-driven control for linear time-invariant systems by presenting a purely data-driven representation of unknown nonlinear systems with rational dynamics. By applying robust control techniques to this parametrization, we obtain sum-of-squares based criteria for designing controllers with closed-loop robust stability guarantees for all continuous-time systems with rational system dynamics which are consistent with the measured data and the assumed noise bound. Finally, we apply the developed approach to a numerical example.      
### 16.MIMO Radar Waveform-Filter Design for Extended Target Detection from a View of Games  [ :arrow_down: ](https://arxiv.org/pdf/2011.11346.pdf)
>  This paper studies the Two-Person Zero Sum(TPZS) game between a Multiple-Input Multiple-Output(MIMO) radar and an extended target with payoff function being the output Signal-to-Interference-pulse-Noise Ratio(SINR) at the radar receiver. The radar player wants to maximize SINR by adjusting its transmit waveform and receive filter. Conversely, the target player wants to minimize SINR by changing its Target Impulse Response(TIR) from a scaled sphere centered around a certain TIR. The interaction between them forms a Stackelberg game where the radar player acts as a leader. The Stackelberg equilibrium strategy of radar, namely robust or minimax waveform-filter pair, for three different cases are taken into consideration. In the first case, Energy Constraint(EC) on transmit waveform is introduced, where we theoretically prove that the Stackelberg equilibrium is also the Nash equilibrium of the game, and propose Algorithm 1 to solve the optimal waveform-filter pair through convex optimization. Note that the EC can't meet the demands of radar transmitter due to high Peak Average to power Ratio(PAR) of the transmit waveform, thus Constant Modulus and Similarity Constraint(CM-SC) on waveform is considered in the second case, and Algorithm 2 is proposed to solve this problem, where we theoretically prove the existence of Nash equilibrium for its Semi-Definite Programming(SDP) relaxation form. And the optimal waveform-filter pair is solved by calculating the Nash equilibrium followed by the randomization schemes. In the third case,...      
### 17.Space-based Global Maritime Surveillance. Part II: Artificial Intelligence and Data Fusion Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2011.11338.pdf)
>  Maritime surveillance (MS) is of paramount importance for search and rescue operations, fishery monitoring, pollution control, law enforcement, migration monitoring, and national security policies. Since ground-based radars and automatic identification system (AIS) do not always provide a comprehensive and seamless coverage of the entire maritime domain, the use of space-based sensors is crucial to complement them. We reviewed space-based technologies for MS in the first part of this work, titled "Space-based Global Maritime Surveillance. Part I: Satellite Technologies" [1]. However, future MS systems combining multiple terrestrial and space-based sensors with additional information sources will require dedicated artificial intelligence and data fusion techniques for the processing of raw satellite images and fuse heterogeneous information. The second part of our work focuses on the most promising artificial intelligence and data fusion techniques for MS using space-based sensors.      
### 18.DemodNet: Learning Soft Demodulation from Hard Information Using Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2011.11337.pdf)
>  Soft demodulation is a basic module of traditional communication receivers. It converts received symbols into soft bits, that is, log likelihood ratios (LLRs). However, in the nonideal additive white Gaussian noise (AWGN) channel, it is difficult to accurately calculate the LLR. In this letter, we propose a demodulator, DemodNet, based on a fully convolutional neural network with variable input and output length. We use hard bit information to train the DemodNet, and we propose log probability ratio (LPR) based on the output layer of the trained DemodNet to realize soft demodulation. The simulation results show that under the AWGN channel, the performance of both hard demodulation and soft demodulation of DemodNet is very close to the traditional methods. In three non-ideal channel scenarios, i.e., the presence of frequency deviation, additive generalized Gaussian noise (AGGN) channel, and Rayleigh fading channel, the performance of channel decoding using the soft information LPR obtained by DemodNet is better than the performance of decoding using the exact LLR calculated under the ideal AWGN assumption.      
### 19.Causality Graph of Vehicular Traffic Flow  [ :arrow_down: ](https://arxiv.org/pdf/2011.11323.pdf)
>  In an intelligent transportation system, the effects and relations of traffic flow at different points in a network are valuable features which can be exploited for control system design and traffic forecasting. In this paper, we define the notion of causality based on the directed information, a well-established data-driven measure, to represent the effective connectivity among nodes of a vehicular traffic network. This notion indicates whether the traffic flow at any given point affects another point's flow in the future and, more importantly, reveals the extent of this effect. In contrast with conventional methods to express connections in a network, it is not limited to linear models and normality conditions. In this work, directed information is used to determine the underlying graph structure of a network, denoted directed information graph, which expresses the causal relations among nodes in the network. We devise an algorithm to estimate the extent of the effects in each link and build the graph. The performance of the algorithm is then analyzed with synthetic data and real aggregated data of vehicular traffic.      
### 20.End-to-end Silent Speech Recognition with Acoustic Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2011.11315.pdf)
>  Silent speech interfaces (SSI) has been an exciting area of recent interest. In this paper, we present a non-invasive silent speech interface that uses inaudible acoustic signals to capture people's lip movements when they speak. We exploit the speaker and microphone of the smartphone to emit signals and listen to their reflections, respectively. The extracted phase features of these reflections are fed into the deep learning networks to recognize speech. And we also propose an end-to-end recognition framework, which combines the CNN and attention-based encoder-decoder network. Evaluation results on a limited vocabulary (54 sentences) yield word error rates of 8.4% in speaker-independent and environment-independent settings, and 8.1% for unseen sentence testing.      
### 21.Space-based Global Maritime Surveillance. Part I: Satellite Technologies  [ :arrow_down: ](https://arxiv.org/pdf/2011.11304.pdf)
>  Maritime surveillance (MS) is crucial for search and rescue operations, fishery monitoring, pollution control, law enforcement, migration monitoring, and national security policies. Since the early days of seafaring, MS has been a critical task for providing security in human coexistence. Several generations of sensors providing detailed maritime information have become available for large offshore areas in real time: maritime radar sensors in the 1950s and the automatic identification system (AIS) in the 1990s among them. However, ground-based maritime radars and AIS data do not always provide a comprehensive and seamless coverage of the entire maritime space. Therefore, the exploitation of space-based sensor technologies installed on satellites orbiting around the Earth, such as satellite AIS data, synthetic aperture radar, optical sensors, and global navigation satellite systems reflectometry, becomes crucial for MS and to complement the existing terrestrial technologies. In the first part of this work, we provide an overview of the main available space-based sensors technologies and present the advantages and limitations of each technology in the scope of MS. The second part, related to artificial intelligence, signal processing and data fusion techniques, is provided in a companion paper, titled: "Space-based Global Maritime Surveillance. Part II: Artificial Intelligence and Data Fusion Techniques" [1].      
### 22.KPC: Learning-Based Model Predictive Control with Deterministic Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2011.11303.pdf)
>  We propose Kernel Predictive Control (KPC), a learning-based predictive control strategy that enjoys deterministic guarantees of safety. Noise-corrupted samples of the unknown system dynamics are used to learn several models through the formalism of non-parametric kernel regression. By treating each prediction step individually, we dispense with the need of propagating sets through highly non-linear maps, a procedure that often involves multiple conservative approximation steps. Finite-sample error bounds are then used to enforce state-feasibility by employing an efficient robust formulation. We then present a relaxation strategy that exploits on-line data to weaken the optimization problem constraints while preserving safety. Two numerical examples are provided to illustrate the applicability of the proposed control method.      
### 23.Sparse Inpainting with Smoothed Particle Hydrodynamics  [ :arrow_down: ](https://arxiv.org/pdf/2011.11289.pdf)
>  Digital image inpainting refers to techniques used to reconstruct a damaged or incomplete image by exploiting available image information. The main goal of this work is to perform the image inpainting process from a set of sparsely distributed image samples with the Smoothed Particle Hydrodynamics (SPH) technique. As, in its naive formulation, the SPH technique is not even capable of reproducing constant functions, we modify the approach to obtain an approximation which can reproduce constant and linear functions. Furthermore, we examine the use of Voronoi tessellation for defining the necessary parameters in the SPH method as well as selecting optimally located image samples. In addition to this spatial optimization, optimization of data values is also implemented in order to further improve the results. Apart from a traditional Gaussian smoothing kernel, we assess the performance of other kernels on both random and spatially optimized masks. Since the use of isotropic smoothing kernels is not optimal in the presence of objects with a clear preferred orientation in the image, we also examine anisotropic smoothing kernels. Our final algorithm can compete with well-performing sparse inpainting techniques based on homogeneous or anisotropic diffusion processes as well as with exemplar-based approaches.      
### 24.Multi-Intelligent-Reflecting-Surfaces-Aided Secure Precise Wireless Transmission in 3D Scenario  [ :arrow_down: ](https://arxiv.org/pdf/2011.11255.pdf)
>  In this paper, intelligent-reflecting-surface(IRS)-aided secure precise wireless transmission (SPWT) schemes are proposed in the three dimension (3D) wireless communication scenario. Unavailable direct path channels from transmitter to receivers are considered when the direct pathes are obstructed by obstacles. Then, multiple IRSs are utilized to achieve SPWT through the reflection path among transmitter, IRS and receivers in order to enhance the communication performance and energy efficiency simultaneously. First, a maximum-signal-to-interference-and-noise ratio (MSINR) scheme is proposed in a single user scenario. Then, the multi-user scenario is considered where the illegitimate users are regarded as eavesdroppers. A maximum-secrecy-rate (MSR) scheme and a maximum-signal-to-leakage-and-noise ratio (MSLNR) are proposed, respectively. The former has a better performance in secrecy rate (SR), however it has a high complexity. The latter has a lower complexity than MSR scheme with the SR performance loss. Simulation results show that both single-user scheme and multi-user scheme can achieve SPWT which transmits confidential message precisely to location of desired users. Moreover, MSLNR scheme has a lower complexity than the MSR scheme, while the SR performance is close to that of the MSR scheme.      
### 25.Indias Rise in Nanoelectronics Research  [ :arrow_down: ](https://arxiv.org/pdf/2011.11251.pdf)
>  Modern semiconductors innovation has a strong relation to scale and skill. While India has a significant demand for semiconductors, it has a daunting challenge to create a semiconductor ecosystem. Yet, India has quietly come a long way. Starting with Centers of Excellence in Nanoelectronics (CENs) initiated in 2006 and broad science and technology funding, India has transformed its nanoelectronics research ecosystem. From negligible contributions as late as 2011, India has risen to be a top contributor to IEEE Electron Devices journals today. Our study presents important observations in terms of ecosystem development. First, there is a 6 year incubation time from infrastructure initiation to first papers. Then, 4 more years to become globally competitive. Second, growth in experimental research is essential along with modeling and simulations. Finally, the aspirational goals of translational research to contribute to the global technology roadmap requires cutting edge manufacturing infrastructure and ecosystem access, which still needs development. The learning informs a call to action for the research ecosystem i.e. academia, industry, and policy-makers. First, sustain and amplify successful strategies of national research infrastructure and funding growth. Second, enhance international collaborations to add further scale and infrastructure to RnD. Finally, strengthen the industry-academia-policy consortium approach to transform to an innovation-based economy. Ultimately, the electron devices community is entering an exciting phase where Beyond Moore offers open opportunities in materials, devices to systems, and algorithms. India must build on its success to play a significant role in this new world of disruptive innovation.      
### 26.Restricted Airspace Protection using Multi-UAV Spatio-TemporalMulti-Task Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2011.11247.pdf)
>  This paper addresses the problem of restricted airspace protection from invaders using the cooperative multi-UAV system. The objective is to detect and capture the invaders cooperatively by a team of homogeneous UAVs (called evaders)before invaders enter the restricted airspace. The problem of restricted airspace protection problem is formulated as a Multi-UAV Spatio-Temporal Multi-Task Allocation problem and is referred as MUST-MTA. The MUST-MTA problem is solved using a modified consensus-based bundled auction method. Here, the spatial and time constraints are handled by combining both spatial and temporal loss component. The solution identifies the sequence of spatial locations to be reached by the evader at specific time instants to neutralize the invaders. The performance of MUST-MTA with the consensus approach is evaluated in a simulated environment. The Monte-Carlo simulation results clearly indicate the efficacy of the proposed approach in restricted airspace protection against intruders      
### 27.Domain Adaptation based COVID-19 CT Lung Infections Segmentation Network  [ :arrow_down: ](https://arxiv.org/pdf/2011.11242.pdf)
>  Coronavirus disease (COVID-19 pneumonia) has spread rapidly and become a global epidemic, which has had a great impact on public health and the economy. The automatic segmentation of lung infections from computed tomography (CT) has become an effective method for diagnosis. In order to realize the full potential of deep learning models in COVID-19 pneumonia infections segmentation, a great deal of annotated CT data is needed for training. The data is difficult to collect due to the high risk of infection, and it is laborious to annotate. Recent advances in image synthesis make it possible to train deep learning models on realistic synthetic data with computer-generated annotations. However, the domain shift between real data and synthetic data significantly reduces segmentation performance. In order to solve this issue, we propose a novel domain adaptation based COVID-19 CT lung infections segmentation network. In this work, we use limited real data without annotations and a large amount of annotated synthetic data to train the U-Net segmentation network jointly. To overcome the domain mismatch, we introduce conditional GAN for adversarial training. We update the segmentation network with the cross-domain adversarial loss. This makes the embedding distribution learned by segmentation network from real data and synthetic data closer, thus greatly improving the representation ability of the segmentation network. The experiment results demonstrate that our proposed network significantly outperforms the baseline and state-of-the-art methods.      
### 28.Risk-Sensitive Motion Planning using Entropic Value-at-Risk  [ :arrow_down: ](https://arxiv.org/pdf/2011.11211.pdf)
>  We consider the problem of risk-sensitive motion planning in the presence of randomly moving obstacles. To this end, we adopt a model predictive control (MPC) scheme and pose the obstacle avoidance constraint in the MPC problem as a distributionally robust constraint with a KL divergence ambiguity set. This constraint is the dual representation of the Entropic Value-at-Risk (EVaR). Building upon this viewpoint, we propose an algorithm to follow waypoints and discuss its feasibility and completion in finite time. We compare the policies obtained using EVaR with those obtained using another common coherent risk measure, Conditional Value-at-Risk (CVaR), via numerical experiments for a 2D system. We also implement the waypoint following algorithm on a 3D quadcopter simulation.      
### 29.Fast Decentralized Linear Functions Over Edge Fluctuating Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2011.11162.pdf)
>  Implementing linear transformations is a key task in the decentralized signal processing framework, which performs learning tasks on data sets distributed over multi-node networks. That kind of network can be represented by a graph. Recently, some decentralized methods have been proposed to compute linear transformations by leveraging the notion of graph shift operator, which captures the local structure of the graph. However, existing approaches have some drawbacks such as considering some special instances of linear transformations, or reducing the family of transformations by assuming that a shift matrix is given such that a subset of its eigenvectors spans the subspace of interest. In contrast, this paper develops a framework for computing a wide class of linear transformations in a decentralized fashion by relying on the notion of graph shift operator. The main goal of the proposed method is to compute the desired linear transformation in a small number of iterations. To this end, a set of successive graph shift operators is employed, then, a new optimization problem is proposed whose goal is to compute the desired transformation as fast as possible. In addition, usually, the topology of the networks, especially the wireless sensor networks, change randomly because of node failures or random links. In this paper, the effect of edge fluctuations on the performance of the proposed method is studied. To deal with the negative effect of edge fluctuations, an online kernel-based method is proposed which enables nodes to estimate the missed values with their at hand information. The proposed method can also be employed to sparsify the network graph or reduce the number of local exchanges between nodes, which saves sensors power in the wireless sensor networks.      
### 30.Deep Learning in EEG: Advance of the Last Ten-Year Critical Period  [ :arrow_down: ](https://arxiv.org/pdf/2011.11128.pdf)
>  Deep learning has achieved excellent performance in a wide range of domains, especially in speech recognition and computer vision. Relatively less work has been done for EEG, but there is still significant progress attained in the last decade. Due to the lack of a comprehensive survey for deep learning in EEG, we attempt to summarize recent progress to provide an overview, as well as perspectives for future developments. We first briefly mention the artifacts removal for EEG signal and then introduce deep learning models that have been utilized in EEG processing and classification. Subsequently, the applications of deep learning in EEG are reviewed by categorizing them into groups such as brain-computer interface, disease detection, and emotion recognition. They are followed by the discussion, in which the pros and cons of deep learning are presented and future directions and challenges for deep learning in EEG are proposed. We hope that this paper could serve as a summary of past work for deep learning in EEG and the beginning of further developments and achievements of EEG studies based on deep learning.      
### 31.Identifying Critical Fleet Sizes Using a Novel Agent-Based Modelling Framework for Autonomous Ride-Sourcing  [ :arrow_down: ](https://arxiv.org/pdf/2011.11085.pdf)
>  Ride-sourcing platforms enable an on-demand shared transport service by solving decision problems often related to customer matching, pricing and vehicle routing. These problems have been frequently represented using aggregated mathematical models and solved via algorithmic approaches designed by researchers. The increasing complexity of ride-sourcing environments compromises the accuracy of aggregated methods. It, therefore, signals the need for alternative practices such as agent-based models which capture the level of complex dynamics in ride-sourcing systems. The use of these agent-based models to simulate ride-sourcing fleets has been a focal point of many studies; however, this occurred in the absence of a prescribed approach on how to build the models to mimic fleet operations realistically. To bridge this research gap, we provide a framework for building bespoke agent-based models for ride-sourcing fleets, derived from the fundamentals of agent-based modelling theory. We also introduce a model building sequence of the different modules necessary to structure a simulator based on our framework. To showcase the strength of our framework, we use it to tackle the highly non-linear problem of minimum fleet size estimation for autonomous ride-sourcing fleets. We do so by investigating the relationship of system parameters based on queuing theory principles and by deriving and validating a novel model for pickup wait times. By modelling the ride-sourcing fleet function in the urban areas of Manhattan, San Francisco, Paris and Barcelona, we find that ride-sourcing fleets operate queues with zero assignment times above the critical fleet size. We also show that pickup wait times have a pivotal role in the estimation of the minimum fleet size in ride-sourcing operations, with agent-based modelling to be a more reliable route for their identification given the system parameters.      
### 32.Cryo-ZSSR: multiple-image super-resolution based on deep internal learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.11020.pdf)
>  Single-particle cryo-electron microscopy (cryo-EM) is an emerging imaging modality capable of visualizing proteins and macro-molecular complexes at near-atomic resolution. The low electron-doses used to prevent sample radiation damage, result in images where the power of the noise is 100 times greater than the power of the signal. To overcome the low-SNRs, hundreds of thousands of particle projections acquired over several days of data collection are averaged in 3D to determine the structure of interest. Meanwhile, recent image super-resolution (SR) techniques based on neural networks have shown state of the art performance on natural images. Building on these advances, we present a multiple-image SR algorithm based on deep internal learning designed specifically to work under low-SNR conditions. Our approach leverages the internal image statistics of cryo-EM movies and does not require training on ground-truth data. When applied to a single-particle dataset of apoferritin, we show that the resolution of 3D structures obtained from SR micrographs can surpass the limits imposed by the imaging system. Our results indicate that the combination of low magnification imaging with image SR has the potential to accelerate cryo-EM data collection without sacrificing resolution.      
### 33.Compressive coded rotating mirror camera for high-speed imaging  [ :arrow_down: ](https://arxiv.org/pdf/2011.11000.pdf)
>  We develop novel compressive coded rotating mirror (CCRM) camera to capture events at high frame rates in passive mode with a compact instrument design at the fraction of the cost compared to other high-speed imaging cameras. Operation of CCRM camera is based on the amplitude optical encoding (grey scale) and a continuous frame sweep across a low-cost detector using a motorized rotating mirror system which can achieve single pixel shift between adjacent frames. Amplitude encoding and continuous frame overlapping enable the CCRM camera to achieve high number of captured frames and high temporal resolution without making sacrifices in the spatial resolution. Two sets of dynamic scenes have been captured at up to 120 Kfps frame rate in both monochrome and colored scales in the experimental demonstrations. The obtained heavily compressed data from the experiment are reconstructed using the optimization algorithm under the compressive sensing (CS) paradigm and the highest sequence depth of 1400 captured frames in single exposure has been achieved with the highest compression ratio of 368 compared to other CS-based high-speed imaging technologies. Under similar conditions CCRM camera is 700$\times$ faster than conventional rotating mirror based imaging devices and could reach frame rate of up to 20 Gfps.      
### 34.A Novel NOMA Solution with RIS Partitioning  [ :arrow_down: ](https://arxiv.org/pdf/2011.10977.pdf)
>  Reconfigurable intelligent surface (RIS) empowered communications with non-orthogonal multiple access (NOMA) has recently become as an appealing research direction for the next-generation wireless communications. In this paper, we propose a novel NOMA solution with RIS partitioning, where we aim to enhance the spectrum efficiency by improving the ergodic rate of all users, and to maximize the user fairness. In the proposed system, we distribute the physical resources among users such that the base station (BS) and RIS are dedicated to serve different clusters of users. Furthermore, we formulate an RIS partitioning optimization problem to slice the RIS elements between the users such that the user fairness is maximized. The formulated problem is a non-convex and non-linear integer programming (NLIP) problem with a combinatorial feasible set, which is very challenging to solve. Therefore, we exploit the structure of the problem to bound its feasible set and obtain a sub-optimal solution by sequentially applying three efficient search algorithms. Furthermore, we derive exact and asymptotic expressions for the outage probability. Simulation results clearly indicate the superiority of the proposed system over the considered benchmark systems in terms of ergodic sum-rate, outage probability, and user fairness performance.      
### 35.Waveform Optimization with Multiple Performance Metrics for Broadband Joint Communication and Radar Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2011.10943.pdf)
>  Joint communication and radar sensing (JCAS) integrates communication and radar/radio sensing into one system, sharing one transmitted signal. In this paper, we investigate JCAS waveform optimization underlying communication signals, where a base station detects radar targets and communicates with mobile users simultaneously. We first develop individual novel waveform optimization problems for communications and sensing, respectively. For communications, we propose a novel lower bound of sum rate by integrating multi-user interference and effective channel gain into one metric that simplifies the optimization of the sum rate. For radar sensing, we consider optimizing one of two metrics, the mutual information or the Cramer-Rao bound. Then, we formulate the JCAS problem by optimizing the communication metric under different constraints of the radar metric, and we obtain both closed-form solutions and iterative solutions to the non-convex JCAS optimization problem. Numerical results are provided and verify the proposed optimization solutions.      
### 36.Primal-dual Learning for the Model-free Risk-constrained Linear Quadratic Regulator  [ :arrow_down: ](https://arxiv.org/pdf/2011.10931.pdf)
>  Risk-aware control, though with promise to tackle unexpected events, requires a known exact dynamical model. In this work, we propose a model-free framework to learn a risk-aware controller with a focus on the linear system. We formulate it as a discrete-time infinite-horizon LQR problem with a state predictive variance constraint. To solve it, we parameterize the policy with a feedback gain pair and leverage primal-dual methods to optimize it by solely using data. We first study the optimization landscape of the Lagrangian function and establish the strong duality in spite of its non-convex nature. Alongside, we find that the Lagrangian function enjoys an important local gradient dominance property, which is then exploited to develop a convergent random search algorithm to learn the dual function. Furthermore, we propose a primal-dual algorithm with global convergence to learn the optimal policy-multiplier pair. Finally, we validate our results via simulations.      
### 37.A Workbench for Testing and Simulation Faults in Three-phase Electric Motors with Intelligent Electronic Device and Microcontrolled System  [ :arrow_down: ](https://arxiv.org/pdf/2011.10910.pdf)
>  Electric motors can be damaged or operate improperly from a possible set of failures. Such failures are related to high or very low voltage and current levels, phase loss or blocked rotor. Therefore, it is important to protect these equipments through appropriate mechanisms. Alternatively, a workbench can simulate detectable failures related to the engines, allowing to change parameters, in which maintenance operators are able to identify the results of these changes. This work presents the development of a workbench as a tool for testing electrical machines and drives. The workbench is based on the Arduino programming platform (microcontroller system), in which it checks the functioning of electric motors under the condition of failures that may occur in this engine. Motor protections are carried out through an Intelligent Electronic Device (IED), which are popularly known as intelligent relays. The results show the development of a workbench that can test and identify several faults in a small three-phase motor.      
### 38.A Comprehensive Survey on Real-Time Voltage Stability Assessment for Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.10885.pdf)
>  Accurate real-time assessment of power systems voltage stability has been an active area of research in the past few decades. In the past decade, after the development of phasor measurement units (PMU), a lot of discussions has been going on phasor measurement techniques for real-time voltage stability. The fundamental idea behind these methods is to find the Thevenin equivalents of the system, and then determine the voltage stability margin based on the equivalent circuits. Some approaches also include the use of Artificial Neural Networks (ANN), for online monitoring of voltage stability margins. These methods are really fast as compared to the other methods. It has been shown that if we can obtain the phase angles and voltage magnitude in real-time from the phasor measurement units (PMU), then the voltage stability margins can be obtained in real-time and we can initiate voltage stability control methods. We are going to discuss Thevenin's equivalent methods and Artificial Intelligence methods in detail in this paper. We will also introduce the traditional methods which were earlier used for power systems stability assessment such as Time Domain methods, Static Methods, and Sensitivity methods. We are going to finally compare these methods and try to give general guidance on choosing a power stability method.      
### 39.Comprehensive Assessment of COVID-19 Impact on Saskatchewan Power System Operations  [ :arrow_down: ](https://arxiv.org/pdf/2011.10844.pdf)
>  This paper presents lessons learned to date during the Coronavirus Disease 2019 (COVID-19) pandemic from the viewpoint of Saskatchewan power system operations. A load estimation approach is developed to identify how the closures affecting businesses, schools, and other non-critical businesses due to COVID-19 changed the electricity consumption. Furthermore, the impacts of COVID-19 containment measures and re-opening phases on load uncertainty are examined. Changes in CO2 emissions resulting from an increased proportion of renewable energy generation and the change in load pattern are discussed. In addition, the influence of COVID-19 on the balancing authority's power control performance is investigated. Analyses conducted in the paper are based upon data from SaskPower corporation, which is the principal electric utility in Saskatchewan, Canada. Some recommendations for future power system operation and planning are developed.      
### 40.Integrating Structure, Information Architecture and Control Design for Tensegrity Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.10838.pdf)
>  A novel unified approach to jointly optimize structural design parameters, actuator and sensor precision and controller parameters is presented in this paper. The joint optimization problem is posed as a covariance control problem, where feasibility is achieved by bounding the covariance of the output as well as that of the control signals. The formulation is used to design a tensegrity system, where the initial prestress parameters, sensor and actuator precisions, and the control law are jointly optimized. Tensegrity system dynamics models linearized about an equilibrium point are used for system design, where minimality is ensured by constraint projection. The feedback loop is assumed to have a full-order dynamic compensator with its characteristic matrices chosen as optimization variables. The suboptimal solution of this non-convex system design problem is found by iterating over an approximated convex problem through the use of a convexifying potential function that enables the convergence to a stationary point. It is shown that for a linear dynamical system, the approximated joint optimization problem can be formulated using Linear Matrix Inequalities (LMIs).      
### 41.A System for Automatic Rice Disease Detectionfrom Rice Paddy Images Serviced via a Chatbot  [ :arrow_down: ](https://arxiv.org/pdf/2011.10823.pdf)
>  A rice disease diagnosis LINE Bot System from paddy field images was presented in this paper. An easy-to-use automatic rice disease diagnosis system was necessary to help rice farmers improve yield and quality. We targeted on the images took from the paddy environment without special sample preparation. We used a deep learning neural networks technique to detect rice disease in the images. We purposed object detection model training and refinement process to improve the performance of our previous rice leaf diseases detection research. The process was based on analyzing the model's predictive results and could be repeatedly used to improve the quality of the database in the next training of the model. The deployment model for our LINE Bot system was created from the selected best performance technique in our previous paper, YOLOv3, trained by refined training data set. The performance of deployment model was measured on 5 target classes by average mAP improved from 82.74% in previous paper to 89.10%. We purposed Rice Disease LINE Bot system used this deployment model. Our system worked automatically real-time to suggest primary rice disease diagnosis results to the users in the LINE group. Our group included of rice farmers and rice disease experts, and they could communicate freely via chat. In the real LINE Bot deployment, the model's performance measured by our own defined measurement Average True Positive Point was 78.86%. It took approximately 2-3 seconds for detection process in our system servers.      
### 42.Monotonicity of the Trace-Inverse of Covariance Submatrices and Two-Sided Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2011.10810.pdf)
>  It is common to assess the "memory strength" of a stationary process looking at how fast the normalized log-determinant of its covariance submatrices (i.e., entropy rate) decreases. In this work, we propose an alternative characterization in terms of the normalized trace-inverse of the covariance submatrices. We show that this sequence is monotonically non-decreasing and is constant if and only if the process is white. Furthermore, while the entropy rate is associated with one-sided prediction errors (present from past), the new measure is associated with two-sided prediction errors (present from past and future). This measure can be used as an alternative to Burg's maximum-entropy principle for spectral estimation. We also propose a counterpart for non-stationary processes, by looking at the average trace-inverse of subsets.      
### 43.A Better and Faster End-to-End Model for Streaming ASR  [ :arrow_down: ](https://arxiv.org/pdf/2011.10798.pdf)
>  End-to-end (E2E) models have shown to outperform state-of-the-art conventional models for streaming speech recognition [1] across many dimensions, including quality (as measured by word error rate (WER)) and endpointer latency [2]. However, the model still tends to delay the predictions towards the end and thus has much higher partial latency compared to a conventional ASR model. To address this issue, we look at encouraging the E2E model to emit words early, through an algorithm called FastEmit [3]. Naturally, improving on latency results in a quality degradation. To address this, we explore replacing the LSTM layers in the encoder of our E2E model with Conformer layers [4], which has shown good improvements for ASR. Secondly, we also explore running a 2nd-pass beam search to improve quality. In order to ensure the 2nd-pass completes quickly, we explore non-causal Conformer layers that feed into the same 1st-pass RNN-T decoder, an algorithm we called Cascaded Encoders. Overall, we find that the Conformer RNN-T with Cascaded Encoders offers a better quality and latency tradeoff for streaming ASR.      
### 44.Co-Design of Autonomous Systems: From Hardware Selection to Control Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2011.10758.pdf)
>  Designing cyber-physical systems is a complex task which requires insights at multiple abstraction levels. The choices of single components are deeply interconnected and need to be jointly studied. In this work, we consider the problem of co-designing the control algorithm as well as the platform around it. In particular, we leverage a monotone theory of co-design to formalize variations of the LQG control problem as monotone feasibility relations. We then show how this enables the embedding of control co-design problems in the higher level co-design problem of a robotic platform. We illustrate the properties of our formalization by analyzing the co-design of an autonomous drone performing search-and-rescue tasks and show how, given a set of desired robot behaviors, we can compute Pareto efficient design solutions.      
### 45.Multi Time-scale Imputation aided State Estimation in Distribution System  [ :arrow_down: ](https://arxiv.org/pdf/2011.10738.pdf)
>  With the transition to a smart grid, we are witnessing a significant growth in sensor deployments and smart metering infrastructure in the distribution system. However, information from these sensors and meters are typically unevenly sampled at different time-scales and are incomplete. It is critical to effectively aggregate these information sources for situational awareness. In order to reconcile the heterogeneous multi-scale time-series data, we present a multi-task Gaussian process framework. This framework exploits the spatio-temporal correlation across the time-series data to impute data at any desired time-scale while providing confidence bounds on the imputations. The value of the imputed data for distribution system operation is illustrated via a matrix completion based state estimation strategy. Results on the IEEE 37 bus distribution system reveals the superior performance of the proposed approach relative to linear interpolation approaches.      
### 46.Towards Robust Data-Driven Control Synthesis for Nonlinear Systems with Actuation Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2011.10730.pdf)
>  Modern nonlinear control theory seeks to endow systems with properties such as stability and safety, and has been deployed successfully across various domains. Despite this success, model uncertainty remains a significant challenge in ensuring that model-based controllers transfer to real world systems. This paper develops a data-driven approach to robust control synthesis in the presence of model uncertainty using Control Certificate Functions (CCFs), resulting in a convex optimization based controller for achieving properties like stability and safety. An important benefit of our framework is nuanced data-dependent guarantees, which in principle can yield sample-efficient data collection approaches that need not fully determine the input-to-state relationship. This work serves as a starting point for addressing important questions at the intersection of nonlinear control theory and non-parametric learning, both theoretical and in application. We validate the proposed method in simulation with an inverted pendulum in multiple experimental configurations.      
### 47.Learning Control Barrier Functions with High Relative Degree for Safety-Critical Control  [ :arrow_down: ](https://arxiv.org/pdf/2011.10721.pdf)
>  Control barrier functions have shown great success in addressing control problems with safety guarantees. These methods usually find the next safe control input by solving an online quadratic programming problem. However, model uncertainty is a big challenge in synthesizing controllers. This may lead to the generation of unsafe control actions, resulting in severe consequences. In this paper, we develop a learning framework to deal with system uncertainty. Our method mainly focuses on learning the dynamics of the control barrier function, especially for high relative degree with respect to a system. We show that for each order, the time derivative of the control barrier function can be separated into the time derivative of the nominal control barrier function and a remainder. This implies that we can use a neural network to learn the remainder so that we can approximate the dynamics of the real control barrier function. We show by simulation that our method can generate safe trajectories under parametric uncertainty using a differential drive robot model.      
### 48.Learning-based attacks in Cyber-Physical Systems: Exploration, Detection, and Control Cost trade-offs  [ :arrow_down: ](https://arxiv.org/pdf/2011.10718.pdf)
>  We study the problem of learning-based attacks in linear systems, where the communication channel between the controller and the plant can be hijacked by a malicious attacker. We assume the attacker learns the dynamics of the system from observations, then overrides the controller's actuation signal, while mimicking legitimate operation by providing fictitious sensor readings to the controller. On the other hand, the controller is on a lookout to detect the presence of the attacker and tries to enhance the detection performance by carefully crafting its control signals. We study the trade-offs between the information acquired by the attacker from observations, the detection capabilities of the controller, and the control cost. Specifically, we provide tight upper and lower bounds on the expected $\epsilon$-deception time, namely the time required by the controller to make a decision regarding the presence of an attacker with confidence at least $(1-\epsilon\log(1/\epsilon))$. We then show a probabilistic lower bound on the time that must be spent by the attacker learning the system, in order for the controller to have a given expected $\epsilon$-deception time. We show that this bound is also order optimal, in the sense that if the attacker satisfies it, then there exists a learning algorithm with the given order expected deception time. Finally, we show a lower bound on the expected energy expenditure required to guarantee detection with confidence at least $1-\epsilon \log(1/\epsilon)$.      
### 49.SymAR: Symmetry Abstractions and Refinement for Accelerating Scenarios with Neural Network Controllers Verification  [ :arrow_down: ](https://arxiv.org/pdf/2011.10713.pdf)
>  We present a Symmetry-based abstraction refinement algorithm SymAR that is directed towards safety verification of large-scale scenarios with complex dynamical systems. The abstraction maps modes with symmetric dynamics to a single abstract mode and refinements recursively split the modes when safety checks fail. We show how symmetry abstractions can be applied effectively to closed-loop control systems, including non-symmetric deep neural network (DNN) controllers. For such controllers, we transform their inputs and outputs to enforce symmetry and make the closed loop system amenable for abstraction. We implemented SymAR in Python and used it to verify paths with 100s of segments in 2D and 3D scenarios followed by a six dimensional DNN-controlled quadrotor, and also a ground vehicle. Our experiments show significant savings, up to 10x in some cases, in verification time over existing methods.      
### 50.Histology to 3D In Vivo MR Registration for Volumetric Evaluation of MRgFUS Treatment Assessment Biomarkers  [ :arrow_down: ](https://arxiv.org/pdf/2011.10708.pdf)
>  Advances in imaging and early cancer detection have increased interest in magnetic resonance (MR) guided focused ultrasound (MRgFUS) technologies for cancer treatment. MRgFUS ablation treatments could reduce surgical risks, preserve organ tissue/function, and improve patient quality of life. However, surgical resection and histological analysis remain the gold standard to assess cancer treatment response. For non-invasive ablation therapies such as MRgFUS, the treatment response must be determined through MR imaging biomarkers. However, current MR biomarkers are inconclusive and have not been rigorously evaluated against histology via accurate registration. Existing registration methods rely on anatomical features to directly register in vivo MR and histology. For MRgFUS applications in anatomies such as liver, kidney, or breast, anatomical features independent from treatment features are often insufficient to perform direct registration. We present a novel MR to histology registration workflow that utilizes intermediate imaging and does not rely on these independent features. The presented workflow yields an overall registration accuracy of 1.00 +/- 0.13 mm. The developed registration pipeline is used to evaluate a common MRgFUS treatment assessment biomarker against histology. Evaluating MR biomarkers against histology using this registration pipeline will facilitate validating novel MRgFUS biomarkers to improve treatment assessment without surgical intervention.      
### 51.Deep Network Perceptual Losses for Speech Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2011.10706.pdf)
>  Contemporary speech enhancement predominantly relies on audio transforms that are trained to reconstruct a clean speech waveform. Here we investigate whether deep feature representations learned for audio classification tasks can be used to improve denoising. We first trained deep neural networks to classify either spoken words or environmental sounds from audio. We then trained an audio transform to map noisy speech to an audio waveform that minimized 'perceptual' losses derived from the recognition network. When the transform was trained to minimize the difference in the deep feature representations between the output audio and the corresponding clean audio, it removed noise substantially better than baseline methods trained to reconstruct clean waveforms. The learned deep features were essential for this improvement, as features from untrained networks with random weights did not provide the same benefit. The results suggest the use of deep features as perceptual metrics to guide speech enhancement.      
### 52.Analysis and Evaluation of Baseline Manipulation in Demand Response Programs  [ :arrow_down: ](https://arxiv.org/pdf/2011.10681.pdf)
>  The customer baseline is required to assign rebates to participants in baseline-based demand response (DR) programs. The average baseline method has been widely accepted in practice due to its simplicity and reliability. However, the customer's baseline manipulation is little-known in the literature. We start from a customer's perspective and establish a Markov decision process to model the customer's payoff-maximizing problem. The behavior of a rational customer's underconsumption on DR days and overconsumption on non-DR days are revealed. Furthermore, we propose an approximated baseline method and show how the consumption distribution and program parameters affect the results. Due to the curse of dimensionality, a linear policy-based rollout algorithm is introduced to obtain a practical approximate solution. Finally, a case study is carried out to illustrate the baseline manipulation, where the simulation results confirm the effectiveness of the proposed methods and shed light on how to properly design baseline methods.      
### 53.Cost-Effective Quasi-Parallel Sensing Instrumentation for Industrial Chemical Species Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2011.10679.pdf)
>  Chemical Species Tomography (CST) has been widely applied for imaging of critical gas-phase parameters in industrial processes. To acquire high-fidelity images, CST is typically implemented by line-of-sight Wavelength Modulation Spectroscopy (WMS) measurements from multiple laser beams. The modulated transmission signal on each laser beam needs to be a) digitised by a high-speed analogue-to-digital converter (ADC); b) demodulated by a digital lock-in (DLI) module; and c) transferred to high-level processor for image reconstruction. Although a fully parallel data acquisition (DAQ) and signal processing system can achieve these functionalities with maximised temporal response, it leads to a highly complex, expensive and power-consuming instrumentation system with high potential for inconsistency between the sampled beams due to the electronics alone. In addition, the huge amount of spectral data sampled in parallel significantly burdens the communication process in industrial applications where in situ signal digitisation is distanced from the high-level data processing. To address these issues, a quasi-parallel sensing technique and electronic circuits were developed for industrial CST, in which the digitisation and demodulation of the multi-beam transmission signals are multiplexed over the high-frequency modulation within a wavelength scan. Our development not only maintains the temporal response of the fully parallel sensing scheme, but also facilitates the cost-effective implementation of industrial CST with very low complexity and reduced load on data transfer. The proposed technique is analytically proven, numerically examined by noise-contaminated CST simulations, and experimentally validated using a lab-scale CST system with 32 laser beams.      
### 54.Learning How to Solve Bubble Ball  [ :arrow_down: ](https://arxiv.org/pdf/2011.10668.pdf)
>  "Bubble Ball" is a game built on a 2D physics engine, where a finite set of objects can modify the motion of a bubble-like ball. The objective is to choose the set and the initial configuration of the objects, in order to get the ball to reach a target flag. The presence of obstacles, friction, contact forces and combinatorial object choices make the game hard to solve. In this paper, we propose a hierarchical predictive framework which solves Bubble Ball. Geometric, kinematic and dynamic models are used at different levels of the hierarchy. At each level of the game, data collected during failed iterations are used to update models at all hierarchical level and converge to a feasible solution to the game. The proposed approach successfully solves a large set of Bubble Ball levels within reasonable number of trials. This proposed framework can also be used to solve other physics-based games, especially with limited training data from human demonstrations.      
### 55.Upgraded W-Net with Attention Gates and its Application in Unsupervised 3D Liver Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2011.10654.pdf)
>  Segmentation of biomedical images can assist radiologists to make a better diagnosis and take decisions faster by helping in the detection of abnormalities, such as tumors. Manual or semi-automated segmentation, however, can be a time-consuming task. Most deep learning based automated segmentation methods are supervised and rely on manually segmented ground-truth. A possible solution for the problem would be an unsupervised deep learning based approach for automated segmentation, which this research work tries to address. We use a W-Net architecture and modified it, such that it can be applied to 3D volumes. In addition, to suppress noise in the segmentation we added attention gates to the skip connections. The loss for the segmentation output was calculated using soft N-Cuts and for the reconstruction output using SSIM. Conditional Random Fields were used as a post-processing step to fine-tune the results. The proposed method has shown promising results, with a dice coefficient of 0.88 for the liver segmentation compared against manual segmentation.      
### 56.Linearization for High-Speed Current-Steering DACs Using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.10642.pdf)
>  This paper proposes a novel foreground linearization scheme for a high-speed CS-DAC. The technique leverages neural networks (NNs) to derive a LUT that maps the inverse of the DAC transfer characteristic onto the input codes. The algorithm is shown to improve conventional methods by at least 6dB in terms of intermodulation (IM) performance for frequencies up to 9GHz on a state-of-the-art 10-bit CS-DAC operating at 40.96GS/s (gigasamples-per-second) in 14nm CMOS.      
### 57.Landmark and IMU Data Fusion: Systematic Convergence Geometric Nonlinear Observer for SLAM and Velocity Bias  [ :arrow_down: ](https://arxiv.org/pdf/2011.10635.pdf)
>  Navigation solutions suitable for cases when both autonomous robot's pose (\textit{i.e}., attitude and position) and its environment are unknown are in great demand. Simultaneous Localization and Mapping (SLAM) fulfills this need by concurrently mapping the environment and observing robot's pose with respect to the map. This work proposes a nonlinear observer for SLAM posed on the manifold of the Lie group of $\mathbb{SLAM}_{n}\left(3\right)$, characterized by systematic convergence, and designed to mimic the nonlinear motion dynamics of the true SLAM problem. The system error is constrained to start within a known large set and decay systematically to settle within a known small set. The proposed estimator is guaranteed to achieve predefined transient and steady-state performance and eliminate the unknown bias inevitably present in velocity measurements by directly using measurements of angular and translational velocity, landmarks, and information collected by an inertial measurement unit (IMU). Experimental results obtained by testing the proposed solution on a real-world dataset collected by a quadrotor demonstrate the observer's ability to estimate the six-degrees-of-freedom (6 DoF) robot pose and to position unknown landmarks in three-dimensional (3D) space. Keywords: Simultaneous Localization and Mapping, Nonlinear filter for SLAM, Nonlinear filter for SLAM on Matrix Lie group, pose, asymptotic stability, prescribed performance, adaptive estimate, feature, inertial measurement unit, inertial vision unit, IMU, SE(3), SO(3), noise.      
### 58.Distributed Robust State Estimation for Hybrid AC/DC Distribution Systems using Multi-Source Data  [ :arrow_down: ](https://arxiv.org/pdf/2011.10634.pdf)
>  Hybrid AC/DC distribution systems are becoming a popular means to accommodate the increasing penetration of distributed energy resources and flexible loads. This paper proposes a distributed and robust state estimation (DRSE) method for hybrid AC/DC distribution systems using multiple sources of data. In the proposed distributed implementation framework, a unified robust linear state estimation model is derived for each AC and DC regions, where the regions are connected via AC/DC converters and only limited information exchange is needed. To enhance the estimation accuracy of the areas with low measurement coverage, a deep neural network (DNN) is used to extract hidden system statistical information and allow deriving nodal power injections that keep up with the real-time measurement update rate. This provides the way of integrating smart meter data, SCADA measurements and zero injections together for state estimation. Simulations on two hybrid AC/DC distribution systems show that the proposed DRSE has only slight accuracy loss by the linearization formulation but offers robustness of suppressing bad data automatically, as well as benefits of improving computational efficiency.      
### 59.Finite Horizon Discrete Models for Multi-Agent Control Systems with Coupled Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2011.10619.pdf)
>  The goal of this paper is to obtain online abstractions for coupled multi-agent systems in a decentralized manner. A discrete model which captures the motion capabilities of each agent is derived over a bounded time-horizon, by discretizing a corresponding overapproximation of the agent's reachable states. The individual abstractions' composition provides a correct representation of the coupled continuous system over the horizon and renders the approach appropriate for control synthesis under high-level specifications which are assigned to the agents over this time window. Sufficient conditions are also provided for the space and time discretization to guarantee the derivation of deterministic abstractions with tunable transition capabilities.      
### 60.Power Adaptation for Vector Parameter Estimation according to Fisher Information based Optimality Criteria  [ :arrow_down: ](https://arxiv.org/pdf/2011.10609.pdf)
>  The optimal power adaptation problem is investigated for vector parameter estimation according to various Fisher information based optimality criteria. By considering a generic observation model that involves a linear/nonlinear transformation of the parameter vector and an additive noise component with an arbitrary probability distribution, six different optimal power allocation problems are formulated based on Fisher information based objective functions. Via optimization theoretic approaches, various closed-form solutions are derived for the proposed problems. Also, the results are extended to cases in which nuisance parameters exist in the system model. Numerical examples are presented to investigate performance of the proposed power allocation strategies.      
### 61.The Value of Data in Learning-Based Control for Training Subset Selection  [ :arrow_down: ](https://arxiv.org/pdf/2011.10596.pdf)
>  Despite the existence of formal guarantees for learning-based control approaches, the relationship between data and control performance is still poorly understood. In this paper, we present a measure to quantify the value of data within the context of a predefined control task. Our approach is applicable to a wide variety of unknown nonlinear systems that are to be controlled by a generic learning-based control law. We model the unknown component of the system using Gaussian processes, which in turn allows us to directly assess the impact of model uncertainty on control. Results obtained in numerical simulations indicate the efficacy of the proposed measure.      
### 62.Design Approach for Additive Manufacturing in Spare Part Supply Chains  [ :arrow_down: ](https://arxiv.org/pdf/2011.10572.pdf)
>  In the current industrial revolution, additive manufacturing (AM) embodies a promising technology that can enhance the effectiveness, adaptability, and competitiveness of supply chains (SCs). Moreover, it facilitates the development of distributed SCs, thereby enhancing product availability, inventory levels, and lead time. However, the wide adoption of AM in industrial SCs creates various challenges, leading to new difficulties for SC design. In this context, this paper proposes a new design approach to AM SCs using optimization methods. More specifically, the proposed approach, comprising the p-median and mixed-integer linear programming models, considers the decision of deploying productive resources (3D printers) in specific locations of generic spare part SCs. The approach was evaluated in a real-world use case of an elevator maintenance service provider. The obtained results demonstrated the promising capabilities of the proposed design approach in managing the challenges arising from the forthcoming widespread use of 3D printers in manufacturing SCs.      
### 63.Transfer Learning for Oral Cancer Detection using Microscopic Images  [ :arrow_down: ](https://arxiv.org/pdf/2011.11610.pdf)
>  Oral cancer has more than 83% survival rate if detected in its early stages, however, only 29% of cases are currently detected early. Deep learning techniques can detect patterns of oral cancer cells and can aid in its early detection. In this work, we present the first results of neural networks for oral cancer detection using microscopic images. We compare numerous state-of-the-art models via transfer learning approach and collect and release an augmented dataset of high-quality microscopic images of oral cancer. We present a comprehensive study of different models and report their performance on this type of data. Overall, we obtain a 10-15% absolute improvement with transfer learning methods compared to a simple Convolutional Neural Network baseline. Ablation studies show the added benefit of data augmentation techniques with finetuning for this task.      
### 64.High Fidelity Interactive Video Segmentation Using Tensor Decomposition Boundary Loss Convolutional Tessellations and Context Aware Skip Connections  [ :arrow_down: ](https://arxiv.org/pdf/2011.11602.pdf)
>  We provide a high fidelity deep learning algorithm (HyperSeg) for interactive video segmentation tasks using a convolutional network with context-aware skip connections, and compressed, hypercolumn image features combined with a convolutional tessellation procedure. In order to maintain high output fidelity, our model crucially processes and renders all image features in high resolution, without utilizing downsampling or pooling procedures. We maintain this consistent, high grade fidelity efficiently in our model chiefly through two means: (1) We use a statistically-principled tensor decomposition procedure to modulate the number of hypercolumn features and (2) We render these features in their native resolution using a convolutional tessellation technique. For improved pixel level segmentation results, we introduce a boundary loss function; for improved temporal coherence in video data, we include temporal image information in our model. Through experiments, we demonstrate the improved accuracy of our model against baseline models for interactive segmentation tasks using high resolution video data. We also introduce a benchmark video segmentation dataset, the VFX Segmentation Dataset, which contains over 27,046 high resolution video frames, including greenscreen and various composited scenes with corresponding, hand crafted, pixel level segmentations. Our work presents an extension to improvement to state of the art segmentation fidelity with high resolution data and can be used across a broad range of application domains, including VFX pipelines and medical imaging disciplines.      
### 65.The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling  [ :arrow_down: ](https://arxiv.org/pdf/2011.11588.pdf)
>  We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.      
### 66.Direct Transcription for Dynamic Optimization: A Tutorial with a Case Study on Dual-Patient Ventilation During the COVID-19 Pandemic  [ :arrow_down: ](https://arxiv.org/pdf/2011.11570.pdf)
>  A variety of optimal control, estimation, system identification and design problems can be formulated as functional optimization problems with differential equality and inequality constraints. Since these problems are infinite-dimensional and often do not have a known analytical solution, one has to resort to numerical methods to compute an approximate solution. This paper uses a unifying notation to outline some of the techniques used in the transcription step of simultaneous direct methods (which discretize-then-optimize) for solving continuous-time dynamic optimization problems. We focus on collocation, integrated residual and Runge-Kutta schemes. These transcription methods are then applied to a simulation case study to answer a question that arose during the COVID-19 pandemic, namely: If there are not enough ventilators, is it possible to ventilate more than one patient on a single ventilator? The results suggest that it is possible, in principle, to estimate individual patient parameters sufficiently accurately, using a relatively small number of flow rate measurements, without needing to disconnect a patient from the system or needing more than one flow rate sensor. We also show that it is possible to ensure that two different patients can indeed receive their desired tidal volume, by modifying the resistance experienced by the air flow to each patient and controlling the ventilator pressure.      
### 67.Exploring Contrastive Learning in Human Activity Recognition for Healthcare  [ :arrow_down: ](https://arxiv.org/pdf/2011.11542.pdf)
>  Human Activity Recognition (HAR) constitutes one of the most important tasks for wearable and mobile sensing given its implications in human well-being and health monitoring. Motivated by the limitations of labeled datasets in HAR, particularly when employed in healthcare-related applications, this work explores the adoption and adaptation of SimCLR, a contrastive learning technique for visual representations, to HAR. The use of contrastive learning objectives causes the representations of corresponding views to be more similar, and those of non-corresponding views to be more different. After an extensive evaluation exploring 81 combinations of different signal transformations for augmenting the data, we observed significant performance differences owing to the order and the function thereof. In particular, preliminary results indicated an improvement over supervised and unsupervised learning methods when using fine-tuning and random rotation for augmentation, however, future work should explore under which conditions SimCLR is beneficial for HAR systems and other healthcare-related applications.      
### 68.Multi-task Learning for Human Settlement Extent Regression and Local Climate Zone Classification  [ :arrow_down: ](https://arxiv.org/pdf/2011.11452.pdf)
>  Human Settlement Extent (HSE) and Local Climate Zone (LCZ) maps are both essential sources, e.g., for sustainable urban development and Urban Heat Island (UHI) studies. Remote sensing (RS)- and deep learning (DL)-based classification approaches play a significant role by providing the potential for global mapping. However, most of the efforts only focus on one of the two schemes, usually on a specific scale. This leads to unnecessary redundancies, since the learned features could be leveraged for both of these related tasks. In this letter, the concept of multi-task learning (MTL) is introduced to HSE regression and LCZ classification for the first time. We propose a MTL framework and develop an end-to-end Convolutional Neural Network (CNN), which consists of a backbone network for shared feature learning, attention modules for task-specific feature learning, and a weighting strategy for balancing the two tasks. We additionally propose to exploit HSE predictions as a prior for LCZ classification to enhance the accuracy. The MTL approach was extensively tested with Sentinel-2 data of 13 cities across the world. The results demonstrate that the framework is able to provide a competitive solution for both tasks.      
### 69.Speech Command Recognition in Computationally Constrained Environments with a Quadratic Self-organized Operational Layer  [ :arrow_down: ](https://arxiv.org/pdf/2011.11436.pdf)
>  Automatic classification of speech commands has revolutionized human computer interactions in robotic applications. However, employed recognition models usually follow the methodology of deep learning with complicated networks which are memory and energy hungry. So, there is a need to either squeeze these complicated models or use more efficient light-weight models in order to be able to implement the resulting classifiers on embedded devices. In this paper, we pick the second approach and propose a network layer to enhance the speech command recognition capability of a lightweight network and demonstrate the result via experiments. The employed method borrows the ideas of Taylor expansion and quadratic forms to construct a better representation of features in both input and hidden layers. This richer representation results in recognition accuracy improvement as shown by extensive experiments on Google speech commands (GSC) and synthetic speech commands (SSC) datasets.      
### 70.Deep Directed Information-Based Learning for Privacy-Preserving Smart Meter Data Release  [ :arrow_down: ](https://arxiv.org/pdf/2011.11421.pdf)
>  The explosion of data collection has raised serious privacy concerns in users due to the possibility that sharing data may also reveal sensitive information. The main goal of a privacy-preserving mechanism is to prevent a malicious third party from inferring sensitive information while keeping the shared data useful. In this paper, we study this problem in the context of time series data and smart meters (SMs) power consumption measurements in particular. Although Mutual Information (MI) between private and released variables has been used as a common information-theoretic privacy measure, it fails to capture the causal time dependencies present in the power consumption time series data. To overcome this limitation, we introduce the Directed Information (DI) as a more meaningful measure of privacy in the considered setting and propose a novel loss function. The optimization is then performed using an adversarial framework where two Recurrent Neural Networks (RNNs), referred to as the releaser and the adversary, are trained with opposite goals. Our empirical studies on real-world data sets from SMs measurements in the worst-case scenario where an attacker has access to all the training data set used by the releaser, validate the proposed method and show the existing trade-offs between privacy and utility.      
### 71.Industrial object, machine part and defect recognition towards fully automated industrial monitoring employing deep learning. The case of multilevel VGG19  [ :arrow_down: ](https://arxiv.org/pdf/2011.11305.pdf)
>  Modern industry requires modern solutions for monitoring the automatic production of goods. Smart monitoring of the functionality of the mechanical parts of technology systems or machines is mandatory for a fully automatic production process. Although Deep Learning has been advancing, allowing for real-time object detection and other tasks, little has been investigated about the effectiveness of specially designed Convolutional Neural Networks for defect detection and industrial object recognition. In the particular study, we employed six publically available industrial-related datasets containing defect materials and industrial tools or engine parts, aiming to develop a specialized model for pattern recognition. Motivated by the recent success of the Virtual Geometry Group (VGG) network, we propose a modified version of it, called Multipath VGG19, which allows for more local and global feature extraction, while the extra features are fused via concatenation. The experiments verified the effectiveness of MVGG19 over the traditional VGG19. Specifically, top classification performance was achieved in five of the six image datasets, while the average classification improvement was 6.95%.      
### 72.Learning Hidden Markov Models from Aggregate Observations  [ :arrow_down: ](https://arxiv.org/pdf/2011.11236.pdf)
>  In this paper, we propose an algorithm for estimating the parameters of a time-homogeneous hidden Markov model from aggregate observations. This problem arises when only the population level counts of the number of individuals at each time step are available, from which one seeks to learn the individual hidden Markov model. Our algorithm is built upon expectation-maximization and the recently proposed aggregate inference algorithm, the Sinkhorn belief propagation. As compared with existing methods such as expectation-maximization with non-linear belief propagation, our algorithm exhibits convergence guarantees. Moreover, our learning framework naturally reduces to the standard Baum-Welch learning algorithm when observations corresponding to a single individual are recorded. We further extend our learning algorithm to handle HMMs with continuous observations. The efficacy of our algorithm is demonstrated on a variety of datasets.      
### 73.MAC for Machine Type Communications in Industrial IoT -- Part II: Scheduling and Numerical Results  [ :arrow_down: ](https://arxiv.org/pdf/2011.11139.pdf)
>  In the second part of this paper, we develop a centralized packet transmission scheduling scheme to pair with the protocol designed in Part I and complete our medium access control (MAC) design for machine-type communications in the industrial internet of things. For the networking scenario, fine-grained scheduling that attends to each device becomes necessary, given stringent quality of service (QoS) requirements and diversified service types, but prohibitively complex for a large number of devices. To address this challenge, we propose a scheduling solution in two steps. First, we develop algorithms for device assignment based on the analytical results from Part I, when parameters of the proposed protocol are given. Then, we train a deep neural network for assisting in the determination of the protocol parameters. The two-step approach ensures the accuracy and granularity necessary for satisfying the QoS requirements and avoids excessive complexity from handling a large number of devices. Integrating the distributed coordination in the protocol design from Part I and the centralized scheduling from this part, the proposed MAC protocol achieves high performance, demonstrated through extensive simulations. For example, the results show that the proposed MAC can support 1000 devices under an aggregated traffic load of 3000 packets per second with a single channel and achieve &lt;0.5ms average delay and &lt;1% average collision probability among 50 high priority devices.      
### 74.MAC for Machine Type Communications in Industrial IoT -- Part I: Protocol Design and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2011.11138.pdf)
>  In this two-part paper, we propose a novel medium access control (MAC) protocol for machine-type communications in the industrial internet of things. The considered use case features a limited geographical area and a massive number of devices with sporadic data traffic and different priority types. We target at supporting the devices while satisfying their quality of service (QoS) requirements with a single access point and a single channel, which necessitates a customized design that can significantly improve the MAC performance. In Part I of this paper, we present the MAC protocol that comprises a new slot structure, corresponding channel access procedure, and mechanisms for supporting high device density and providing differentiated QoS. A key idea behind this protocol is sensing-based distributed coordination for significantly improving channel utilization. To characterize the proposed protocol, we analyze its delay performance based on the packet arrival rates of devices. The analytical results provide insights and lay the groundwork for the fine-grained scheduling with QoS guarantee as presented in Part II.      
### 75.Turnpike in Lipschitz-nonlinear optimal control  [ :arrow_down: ](https://arxiv.org/pdf/2011.11091.pdf)
>  We present a new proof of the turnpike property for nonlinear optimal control problems, when the running target is a steady control-state pair of the underlying dynamics. Our strategy combines the construction of suboptimal quasi-turnpike trajectories via controllability, and a bootstrap argument, and does not rely on analyzing the optimality system or linearization techniques. This in turn allows us to address several optimal control problems for finite-dimensional, control-affine systems with globally Lipschitz (possibly nonsmooth) nonlinearities, without any smallness conditions on the initial data or the running target. These results are motivated by the large-layer regime of residual neural networks, commonly used in deep learning applications. We show that our methodology is applicable to controlled PDEs as well, such as the semilinear wave and heat equation with a globally Lipschitz nonlinearity, once again without any smallness assumptions.      
### 76.Data Mining Techniques in Predicting Breast Cancer  [ :arrow_down: ](https://arxiv.org/pdf/2011.11088.pdf)
>  Background and Objective: Breast cancer, which accounts for 23% of all cancers, is threatening the communities of developing countries because of poor awareness and treatment. Early diagnosis helps a lot in the treatment of the disease. The present study conducted in order to improve the prediction process and extract the main causes impacted the breast cancer. Materials and Methods: Data were collected based on eight attributes for 130 Libyan women in the clinical stages infected with this disease. Data mining was used by applying six algorithms to predict disease based on clinical stages. All the algorithms gain high accuracy, but the decision tree provides the highest accuracy-diagram of decision tree utilized to build rules from each leafnode. Ranking variables applied to extract significant variables and support final rules to predict disease. Results: All applied algorithms were gained a high prediction with different accuracies. Rules 1, 3, 4, 5 and 9 provided a pure subset to be confirmed as significant rules. Only five input variables contributed to building rules, but not all variables have a significant impact. Conclusion: Tumor size plays a vital role in constructing all rules with a significant impact. Variables of inheritance, breast side and menopausal status have an insignificant impact in analysis, but they may consider remarkable findings using a different strategy of data analysis.      
### 77.Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.11005.pdf)
>  Small area change detection from synthetic aperture radar (SAR) is a highly challenging task. In this paper, a robust unsupervised approach is proposed for small area change detection from multi-temporal SAR images using deep learning. First, a multi-scale superpixel reconstruction method is developed to generate a difference image (DI), which can suppress the speckle noise effectively and enhance edges by exploiting local, spatially homogeneous information. Second, a two-stage centre-constrained fuzzy c-means clustering algorithm is proposed to divide the pixels of the DI into changed, unchanged and intermediate classes with a parallel clustering strategy. Image patches belonging to the first two classes are then constructed as pseudo-label training samples, and image patches of the intermediate class are treated as testing samples. Finally, a convolutional wavelet neural network (CWNN) is designed and trained to classify testing samples into changed or unchanged classes, coupled with a deep convolutional generative adversarial network (DCGAN) to increase the number of changed class within the pseudo-label training samples. Numerical experiments on four real SAR datasets demonstrate the validity and robustness of the proposed approach, achieving up to 99.61% accuracy for small area change detection.      
### 78.Who is in Control? Practical Physical Layer Attack and Defense for mmWave based Sensing in Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2011.10947.pdf)
>  With the wide bandwidths in millimeter wave (mmWave) frequency band that results in unprecedented accuracy, mmWave sensing has become vital for many applications, especially in autonomous vehicles (AVs). In addition, mmWave sensing has superior reliability compared to other sensing counterparts such as camera and LiDAR, which is essential for safety-critical driving. Therefore, it is critical to understand the security vulnerabilities and improve the security and reliability of mmWave sensing in AVs. To this end, we perform the end-to-end security analysis of a mmWave-based sensing system in AVs, by designing and implementing practical physical layer attack and defense strategies in a state-of-the-art mmWave testbed and an AV testbed in real-world settings. Various strategies are developed to take control of the victim AV by spoofing its mmWave sensing module, including adding fake obstacles at arbitrary locations and faking the locations of existing obstacles. Five real-world attack scenarios are constructed to spoof the victim AV and force it to make dangerous driving decisions leading to a fatal crash. Field experiments are conducted to study the impact of the various attack scenarios using a Lincoln MKZ-based AV testbed, which validate that the attacker can indeed assume control of the victim AV to compromise its security and safety. To defend the attacks, we design and implement a challenge-response authentication scheme and a RF fingerprinting scheme to reliably detect aforementioned spoofing attacks.      
### 79.Spatio-Temporal Visualization of Interdependent Battery Bus Transit and Power Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.10917.pdf)
>  The high penetration of transportation electrification and its associated charging requirements magnify the interdependency of the transportation and power distribution systems. The emergent interdependency requires that system operators fully understand the status of both systems. To this end, a visualization tool is presented to illustrate the interdependency of battery bus transit and power distribution systems and the associated components. The tool aims at monitoring components from both systems, such as the locations of electric buses, the state of charge of batteries, the price of electricity, voltage, current, and active/reactive power flow. The results showcase the success of the visualization tool in monitoring the bus transit and power distribution components to determine a reliable cost-effective scheme for spatio-temporal charging of electric buses.      
### 80.Reinforcement learning with distance-based incentive/penalty (DIP) updates for highly constrained industrial control systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.10897.pdf)
>  Typical reinforcement learning (RL) methods show limited applicability for real-world industrial control problems because industrial systems involve various constraints and simultaneously require continuous and discrete control. To overcome these challenges, we devise a novel RL algorithm that enables an agent to handle a highly constrained action space. This algorithm has two main features. First, we devise two distance-based Q-value update schemes, incentive update and penalty update, in a distance-based incentive/penalty update technique to enable the agent to decide discrete and continuous actions in the feasible region and to update the value of these types of actions. Second, we propose a method for defining the penalty cost as a shadow price-weighted penalty. This approach affords two advantages compared to previous methods to efficiently induce the agent to not select an infeasible action. We apply our algorithm to an industrial control problem, microgrid system operation, and the experimental results demonstrate its superiority.      
### 81.Multi-experiment parameter identifiability of ODEs and model theory  [ :arrow_down: ](https://arxiv.org/pdf/2011.10868.pdf)
>  Structural identifiability is a property of an ODE model with parameters that allows for the parameters to be determined from continuous noise-free data. This is natural prerequisite for practical identifiability. Conducting multiple independent experiments could make more parameters or functions of parameters identifiable, which is a desirable property to have. How many experiments are sufficient? In the present paper, we provide an algorithm to determine the exact number of experiments for multi-experiment local identifiability and obtain an upper bound that is off at most by one for the number of experiments for multi-experiment global identifiability. <br>Interestingly, the main theoretical ingredient of the algorithm has been discovered and proved using model theory (in the sense of mathematical logic). We hope that this unexpected connection will stimulate interactions between applied algebra and model theory, and we provide a short introduction to model theory in the context of parameter identifiability. As another related application of model theory in this area, we construct a nonlinear ODE system with one output such that single-experiment and mutiple-experiment identifiability are different for the system. This contrasts with recent results about single-output linear systems. <br>We also present a Monte Carlo randomized version of the algorithm with a polynomial arithmetic complexity. Implementation of the algorithm is provided and its performance is demonstrated on several examples. The source code is available at <a class="link-external link-https" href="https://github.com/pogudingleb/ExperimentsBound" rel="external noopener nofollow">this https URL</a>.      
### 82.Efficiently Estimating a Sparse Delay-Doppler Channel  [ :arrow_down: ](https://arxiv.org/pdf/2011.10849.pdf)
>  Multiple wireless sensing tasks, e.g., radar detection for driver safety, involve estimating the "channel" or relationship between signal transmitted and received. In this work, we focus on a certain channel model known as the delay-doppler channel. This model begins to be useful in the high frequency carrier setting, which is increasingly common with developments in millimeter-wave technology. Moreover, the delay-doppler model then continues to be applicable even when using signals of large bandwidth, which is a standard approach to achieving high resolution channel estimation. However, when high resolution is desirable, this standard approach results in a tension with the desire for efficiency because, in particular, it immediately implies that the signals in play live in a space of very high dimension $N$ (e.g., ~$10^6$ in some applications), as per the Shannon-Nyquist sampling theorem. <br>To address this difficulty, we propose a novel randomized estimation scheme called Sparse Channel Estimation, or SCE for short, for channel estimation in the $k$-sparse setting (e.g., $k$ objects in radar detection). This scheme involves an estimation procedure with sampling and space complexity both on the order of $k(logN)^3$, and arithmetic complexity on the order of $k(log N)^3 + k^2$, for $N$ sufficiently large. <br>To the best of our knowledge, Sparse Channel Estimation (SCE) is the first of its kind to achieve these complexities simultaneously -- it seems to be extremely efficient! As an added advantage, it is a simple combination of three ingredients, two of which are well-known and widely used, namely digital chirp signals and discrete Gaussian filter functions, and the third being recent developments in sparse fast fourier transform algorithms.      
### 83.On the Convergence of Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.10829.pdf)
>  We consider the problem of Reinforcement Learning for nonlinear stochastic dynamical systems. We show that in the RL setting, there is an inherent "Curse of Variance" in addition to Bellman's infamous "Curse of Dimensionality", in particular, we show that the variance in the solution grows factorial-exponentially in the order of the approximation. A fundamental consequence is that this precludes the search for anything other than "local" feedback solutions in RL, in order to control the explosive variance growth, and thus, ensure accuracy. We further show that the deterministic optimal control has a perturbation structure, in that the higher order terms do not affect the calculation of lower order terms, which can be utilized in RL to get accurate local solutions.      
### 84.A Formal Approach to the Co-Design of Embodied Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2011.10756.pdf)
>  We consider the problem of formally co-designing embodied intelligence as a whole, from hardware components such as chassis and sensors to software modules such as control and perception pipelines. We propose a principled approach to formulate and solve complex embodied intelligence co-design problems, leveraging a monotone co-design theory. The methods we propose are intuitive and integrate heterogeneous engineering disciplines, allowing analytical and simulation-based modeling techniques and enabling interdisciplinarity. We illustrate through a case study how, given a set of desired behaviors, our framework is able to compute Pareto efficient solutions for the entire hardware and software stack of a self-driving vehicle.      
### 85.Exploring Voice Conversion based Data Augmentation in Text-Dependent Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2011.10710.pdf)
>  In this paper, we focus on improving the performance of the text-dependent speaker verification system in the scenario of limited training data. The speaker verification system deep learning based text-dependent generally needs a large scale text-dependent training data set which could be labor and cost expensive, especially for customized new wake-up words. In recent studies, voice conversion systems that can generate high quality synthesized speech of seen and unseen speakers have been proposed. Inspired by those works, we adopt two different voice conversion methods as well as the very simple re-sampling approach to generate new text-dependent speech samples for data augmentation purposes. Experimental results show that the proposed method significantly improves the Equal Error Rare performance from 6.51% to 4.51% in the scenario of limited training data.      
### 86.Deep Learning Approach to Channel Sensing and Hybrid Precoding for TDD Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.10709.pdf)
>  This paper proposes a deep learning approach to channel sensing and downlink hybrid analog and digital beamforming for massive multiple-input multiple-output systems with a limited number of radio-frequency chains operating in the time-division duplex mode at millimeter frequency. The conventional downlink precoding design hinges on the two-step process of first estimating the high-dimensional channel based on the uplink pilots received through the channel sensing matrices, then designing the precoding matrices based on the estimated channel. This two-step process is, however, not necessarily optimal, especially when the pilot length is short. This paper shows that by designing the analog sensing and the downlink precoding matrices directly from the received pilots without the intermediate channel estimation step, the overall system performance can be significantly improved. Specifically, we propose a channel sensing and hybrid precoding methodology that divides the pilot phase into an analog and a digital training phase. A deep neural network is utilized in the first phase to design the uplink channel sensing and the downlink analog beamformer. Subsequently, we fix the analog beamformers and design the digital precoder based on the equivalent low-dimensional channel. A key feature of the proposed deep learning architecture is that it decomposes into parallel independent single-user DNNs so that the overall design is generalizable to systems with an arbitrary number of users. Numerical comparisons reveal that the proposed methodology requires significantly less training overhead than the channel recovery based counterparts, and can approach the performance of systems with full channel state information with relatively few pilots.      
### 87.Continuous-Time Convergence Rates in Potential and Monotone Games  [ :arrow_down: ](https://arxiv.org/pdf/2011.10682.pdf)
>  In this paper, we provide exponential rates of convergence to the Nash equilibrium of continuous-time game dynamics such as mirror descent (MD) and actor-critic (AC) in $N$-player continuous games that are either potential games or monotone games but possibly potential-free. In the first part of this paper, under the assumption the game admits a relatively strongly concave potential, we show that MD and AC converge in $\mathcal{O}(e^{-\beta t})$. In the second part of this paper, using relative concavity, we provide a novel relative characterization of monotone games and show that MD and its discounted version converge with $\mathcal{O}(e^{-\beta t})$ in relatively strongly and relatively hypo-monotone games. Moreover, these rates extend their known convergence conditions and also improve the results in the potential game setup. Simulations are performed which empirically back up our results.      
### 88.Data-Driven System Level Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2011.10674.pdf)
>  We establish data-driven versions of the System Level Synthesis (SLS) parameterization of stabilizing controllers for linear-time-invariant systems. Inspired by recent work in data-driven control that leverages tools from behavioral theory, we show that optimization problems over system-responses can be posed using only libraries of past system trajectories, without explicitly identifying a system model. We first consider the idealized setting of noise free trajectories, and show an exact equivalence between traditional and data-driven SLS. We then show that in the case of a system driven by process noise, tools from robust SLS can be used to characterize the effects of noise on closed-loop performance, and further draw on tools from matrix concentration to show that a simple trajectory averaging technique can be used to mitigate these effects. We end with numerical experiments showing the soundness of our methods.      
### 89.Single Microhole per Pixel in CMOS Image Sensor with Enhanced Optical Sensitivity in Near-Infrared  [ :arrow_down: ](https://arxiv.org/pdf/2011.10624.pdf)
>  Silicon photodiode based CMOS sensors with backside-illumination for 300 to 1000 nm wavelength range were studied. We showed that a single hole in the photodiode increases the optical efficiency of the pixel. In near-infrared wavelengths, the enhancement allows 70% absorption in a 3 microns thick Si. It is 4x better than for the flat pixel. We compared different shapes and sizes of single holes and holes arrays. We have shown that a certain size and shape in single holes pronounce better optical efficiency enhancement. The crosstalk was successfully reduced with trenches between pixels. We optimized the trenches to achieve minimal pixel separation for 1.12 microns pixel.      
### 90.SReachTools Kernel Module: Data-Driven Stochastic Reachability Using Hilbert Space Embeddings of Distributions  [ :arrow_down: ](https://arxiv.org/pdf/2011.10610.pdf)
>  We present algorithms for performing data-driven stochastic reachability as an addition to SReachTools, an open-source stochastic reachability toolbox. Our method leverages a class of machine learning techniques known as kernel embeddings of distributions to approximate the safety probabilities for a wide variety of stochastic reachability problems. By representing the probability distributions of the system state as elements in a reproducing kernel Hilbert space, we can learn the "best fit" distribution via a simple regularized least-squares problem, and then compute the stochastic reachability safety probabilities as simple linear operations. This technique admits finite sample bounds and has known convergence in probability. We implement these methods as part of SReachTools, and demonstrate their use on a double integrator system, on a million-dimensional repeated planar quadrotor system, and a cart-pole system with a black-box neural network controller.      
### 91.LSTM-based Traffic Load Balancing and Resource Allocation for an Edge System  [ :arrow_down: ](https://arxiv.org/pdf/2011.10602.pdf)
>  The massive deployment of small cell Base Stations (SBSs) empowered with computing capabilities presents one of the most ingenious solutions adopted for 5G cellular networks towards meeting the foreseen data explosion and the ultra-low latency demanded by mobile applications. This empowerment of SBSs with Multi-access Edge Computing (MEC) has emerged as a tentative solution to overcome the latency demands and bandwidth consumption required by mobile applications at the network edge. The MEC paradigm offers a limited amount of resources to support computation, thus mandating the use of intelligence mechanisms for resource allocation. The use of green energy for powering the network apparatuses (e.g., Base Stations (BSs), MEC servers) has attracted attention towards minimizing the carbon footprint and network operational costs. However, due to their high intermittency and unpredictability, the adoption of learning methods is a requisite. Towards intelligent edge system management, this paper proposes a Green-based Edge Network Management (GENM) algorithm, which is a online edge system management algorithm for enabling green-based load balancing in BSs and energy savings within the MEC server. The main goal is to minimize the overall energy consumption and guarantee the Quality of Service (QoS) within the network. To achieve this, the GENM algorithm performs dynamic management of BSs, autoscaling and reconfiguration of the computing resources, and on/off switching of the fast tunable laser drivers coupled with location-aware traffic scheduling in the MEC server. The obtained simulation results validate our analysis and demonstrate the superior performance of GENM compared to a benchmark algorithm.      
### 92.SalSum: Saliency-based Video Summarization using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.10432.pdf)
>  The huge amount of video data produced daily by camera-based systems, such as surveilance, medical and telecommunication systems, emerges the need for effective video summarization (VS) methods. These methods should be capable of creating an overview of the video content. In this paper, we propose a novel VS method based on a Generative Adversarial Network (GAN) model pre-trained with human eye fixations. The main contribution of the proposed method is that it can provide perceptually compatible video summaries by combining both perceived color and spatiotemporal visual attention cues in a unsupervised scheme. Several fusion approaches are considered for robustness under uncertainty, and personalization. The proposed method is evaluated in comparison to state-of-the-art VS approaches on the benchmark dataset VSUMM. The experimental results conclude that SalSum outperforms the state-of-the-art approaches by providing the highest f-measure score on the VSUMM benchmark.      
### 93.No-Regret Prediction in Marginally Stable Systems  [ :arrow_down: ](https://arxiv.org/pdf/2002.02064.pdf)
>  We consider the problem of online prediction in a marginally stable linear dynamical system subject to bounded adversarial or (non-isotropic) stochastic perturbations. This poses two challenges. Firstly, the system is in general unidentifiable, so recent and classical results on parameter recovery do not apply. Secondly, because we allow the system to be marginally stable, the state can grow polynomially with time; this causes standard regret bounds in online convex optimization to be vacuous. In spite of these challenges, we show that the online least-squares algorithm achieves sublinear regret (improvable to polylogarithmic in the stochastic setting), with polynomial dependence on the system's parameters. This requires a refined regret analysis, including a structural lemma showing the current state of the system to be a small linear combination of past states, even if the state grows polynomially. By applying our techniques to learning an autoregressive filter, we also achieve logarithmic regret in the partially observed setting under Gaussian noise, with polynomial dependence on the memory of the associated Kalman filter.      
