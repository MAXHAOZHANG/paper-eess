# ArXiv eess --Thu, 5 Nov 2020
### 1.Robust Super-Resolution of Real Faces using Smooth Features  [ :arrow_down: ](https://arxiv.org/pdf/2011.02427.pdf)
>  Real low-resolution (LR) face images contain degradations which are too varied and complex to be captured by known downsampling kernels and signal-independent noises. So, in order to successfully super-resolve real faces, a method needs to be robust to a wide range of noise, blur, compression artifacts etc. Some of the recent works attempt to model these degradations from a dataset of real images using a Generative Adversarial Network (GAN). They generate synthetically degraded LR images and use them with corresponding real high-resolution(HR) image to train a super-resolution (SR) network using a combination of a pixel-wise loss and an adversarial loss. In this paper, we propose a two module super-resolution network where the feature extractor module extracts robust features from the LR image, and the SR module generates an HR estimate using only these robust features. We train a degradation GAN to convert bicubically downsampled clean images to real degraded images, and interpolate between the obtained degraded LR image and its clean LR counterpart. This interpolated LR image is then used along with it's corresponding HR counterpart to train the super-resolution network from end to end. Entropy Regularized Wasserstein Divergence is used to force the encoded features learnt from the clean and degraded images to closely resemble those extracted from the interpolated image to ensure robustness.      
### 2.One-shot conditional audio filtering of arbitrary sounds  [ :arrow_down: ](https://arxiv.org/pdf/2011.02421.pdf)
>  We consider the problem of separating a particular sound source from a single-channel mixture, based on only a short sample of the target source. Using SoundFilter, a wave-to-wave neural network architecture, we can train a model without using any sound class labels. Using a conditioning encoder model which is learned jointly with the source separation network, the trained model can be "configured" to filter arbitrary sound sources, even ones that it has not seen during training. Evaluated on the FSD50k dataset, our model obtains an SI-SDR improvement of 9.6 dB for mixtures of two sounds. When trained on Librispeech, our model achieves an SI-SDR improvement of 14.0 dB when separating one voice from a mixture of two speakers. Moreover, we show that the representation learned by the conditioning encoder clusters acoustically similar sounds together in the embedding space, even though it is trained without using any labels.      
### 3.SISO RIS-Enabled Joint 3D Downlink Localization and Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2011.02391.pdf)
>  We consider the problem of joint three-dimensional localization and synchronization for a single-input single-output (SISO) system in the presence of a reconfigurable intelligent surface (RIS), equipped with a uniform planar array. First, we derive the CramÃ©r-Rao bounds (CRBs) on the estimation error of the channel parameters, namely, the angle-of-departure (AOD), composed of azimuth and elevation, from RIS to the user equipment (UE) and times-of-arrival (TOAs) for the path from the base station (BS) to UE and BS-RIS-UE reflection. In order to avoid high-dimensional search over the parameter space, we devise a low-complexity estimation algorithm that performs two 1D searches over the TOAs and one 2D search over the AODs. Simulation results demonstrate that the considered RIS-aided wireless system can provide submeter-level positioning and synchronization accuracy, materializing the positioning capability of Beyond 5G networks even with single-antenna BS and UE. Furthermore, the proposed estimator is shown to attain the CRB at a wide interval of distances between UE and RIS. Finally, we also investigate the scaling of the position error bound with the number of RIS elements.      
### 4.Noise Reduction to Compute Tissue Mineral Density and Trabecular Bone Volume Fraction from Low Resolution QCT  [ :arrow_down: ](https://arxiv.org/pdf/2011.02382.pdf)
>  We propose a 3D neural network with specific loss functions for quantitative computed tomography (QCT) noise reduction to compute micro-structural parameters such as tissue mineral density (TMD) and bone volume ratio (BV/TV) with significantly higher accuracy than using no or standard noise reduction filters. The vertebra-phantom study contained high resolution peripheral and clinical CT scans with simulated in vivo CT noise and nine repetitions of three different tube currents (100, 250 and 360 mAs). Five-fold cross validation was performed on 20466 purely spongy pairs of noisy and ground-truth patches. Comparison of training and test errors revealed high robustness against over-fitting. While not showing effects for the assessment of BMD and voxel-wise densities, the filter improved thoroughly the computation of TMD and BV/TV with respect to the unfiltered data. Root-mean-square and accuracy errors of low resolution TMD and BV/TV decreased to less than 17% of the initial values. Furthermore filtered low resolution scans revealed still more TMD- and BV/TV-relevant information than high resolution CT scans, either unfiltered or filtered with two state-of-the-art standard denoising methods. The proposed architecture is threshold and rotational invariant, applicable on a wide range of image resolutions at once, and likely serves for an accurate computation of further micro-structural parameters. Furthermore, it is less prone for over-fitting than neural networks that compute structural parameters directly. In conclusion, the method is potentially important for the diagnosis of osteoporosis and other bone diseases since it allows to assess relevant 3D micro-structural information from standard low exposure CT protocols such as 100 mAs and 120 kVp.      
### 5.Switched Optimal Control and Dwell Time Constraints: A Preliminary Study  [ :arrow_down: ](https://arxiv.org/pdf/2011.02351.pdf)
>  Most modern control systems are switched, meaning they have continuous as well as discrete decision variables. Switched systems often have constraints called dwell-time constraints (e.g., cycling constraints in a heat pump) on the switching rate. This paper introduces an embedding-based-method to solve optimal control problems that have both discrete and continuous decision variables. Unlike existing methods, the developed technique can heuristically incorporate dwell-time constraints via an auxiliary cost, while also preserving other state and control constrains of the problem. Simulations results for a switched optimal control problem with and without the auxiliary cost showcase the utility of the developed method.      
### 6.Deep Reinforcement Learning in Electricity Generation Investment for the Minimization of Long-Term Carbon Emissions and Electricity Costs  [ :arrow_down: ](https://arxiv.org/pdf/2011.02342.pdf)
>  A change from a high-carbon emitting electricity power system to one based on renewables would aid in the mitigation of climate change. Decarbonization of the electricity grid would allow for low-carbon heating, cooling and transport. Investments in renewable energy must be made over a long time horizon to maximise return of investment of these long life power generators. Over these long time horizons, there exist multiple uncertainties, for example in future electricity demand and costs to consumers and investors. <br>To mitigate for imperfect information of the future, we use the deep deterministic policy gradient (DDPG) deep reinforcement learning approach to optimize for a low-cost, low-carbon electricity supply using a modified version of the FTT:Power model. In this work, we model the UK and Ireland electricity markets. The DDPG algorithm is able to learn the optimum electricity mix through experience and achieves this between the years of 2017 and 2050. We find that a change from fossil fuels and nuclear power to renewables, based upon wind, solar and wave would provide a cheap and low-carbon alternative to fossil fuels.      
### 7.Soft Attention Convolutional Neural Networks for Rare Event Detection in Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2011.02338.pdf)
>  Automated event detection in the sequences is an important aspect of temporal data analytics. The events can be in the form of peaks, changes in data distribution, changes of spectral characteristics etc. In this work, we propose a Soft-Attention Convolutional Neural Network (CNN) based approach for rare event detection in sequences. For the purpose of demonstration, we experiment with well logs where we aim to detect events depicting the changes in the geological layers (a.k.a. well tops/markers). Well logs (single or multivariate) are inputted to a soft attention CNN and a model is trained to locate the marker position. Attention mechanism enables the machine to relatively scale the relevant log features for the task. Experimental results show that our approach is able to locate the rare events with high precision.      
### 8.Fault Detection for Covered Conductors With High-Frequency Voltage Signals: From Local Patterns to Global Features  [ :arrow_down: ](https://arxiv.org/pdf/2011.02336.pdf)
>  The detection and characterization of partial discharge (PD) are crucial for the insulation diagnosis of overhead lines with covered conductors. With the release of a large dataset containing thousands of naturally obtained high-frequency voltage signals, data-driven analysis of fault-related PD patterns on an unprecedented scale becomes viable. The high diversity of PD patterns and background noise interferences motivates us to design an innovative pulse shape characterization method based on clustering techniques, which can dynamically identify a set of representative PD-related pulses. Capitalizing on those pulses as referential patterns, we construct insightful features and develop a novel machine learning model with a superior detection performance for early-stage covered conductor faults. The presented model outperforms the winning model in a Kaggle competition and provides the state-of-the-art solution to detect real-time disturbances in the field.      
### 9.Localization in Terahertz-Operating Energy Harvesting Software-Defined Metamaterials  [ :arrow_down: ](https://arxiv.org/pdf/2011.02335.pdf)
>  Software-Defined Metamaterials (SDMs) show a strong potential for advancing the engineered control of electromagnetic waves. As such, they are envisioned to enable a variety of exciting applications, among others in the domains of smart textiles, high-resolution structural monitoring, and sensing in challenging environments. Many of the applications envisage deformations of the SDM structure, such as its bending, stretching or rolling, which implies that the locations of metamaterial elements will be changing relative to one another. In this paper, we argue that, if this change of relative locations would be quantifiable, i.e., if the metamaterial elements would be accurately localizable, this location information could potentially be utilized for enabling novel SDM applications, as well as for optimizing the control of the metamaterial elements themselves. The question if it is possible, as well as how to perform such localization is, however, yet to spark in the community. In this work, we assume that the metamaterial elements are controlled wirelessly through a Terahertz (THz)-operating nanonetwork. Moreover, we consider the elements to be energy-constrained, with their sole powering option being to harvest environmental energy. For such a setup, we demonstrate sub-millimeter accuracy of the two-way Time of Flight (ToF)-based localization, as well as high availability of the service (i.e., consistently more than 80% of the time), which is a result of the low energy consumed in the localization. We do that for an exhaustive set of system parameters, among other the operational frequency, bandwidth, harvesting rate, as well as their energy harvesting and consumption patterns. Finally, we qualitatively characterize the effects of the mentioned system parameters on the latency of the proposed localization service, as well as outline several challenges and future research directions.      
### 10.Deep Learning Assisted mmWave Beam Prediction with Prior Low-frequency Information  [ :arrow_down: ](https://arxiv.org/pdf/2011.02332.pdf)
>  Huge overhead of beam training poses a significant challenge to mmWave communications. To address this issue, beam tracking has been widely investigated whereas existing methods are hard to handle serious multipath interference and non-stationary scenarios. Inspired by the spatial similarity between low-frequency and mmWave channels in non-standalone architectures, this paper proposes to utilize prior low-frequency information to predict the optimal mmWave beam, where deep learning is adopted to enhance the prediction accuracy. Specifically, periodically estimated low-frequency channel state information (CSI) is applied to track the movement of user equipment, and timing offset indicator is proposed to indicate the instant of mmWave beam training relative to low-frequency CSI estimation. Meanwhile, long-short term memory networks based dedicated models are designed to implement the prediction. Simulation results show that our proposed scheme can predict the optimal mmWave beam more accurately than the conventional methods while requiring little overhead of mmWave beam training.      
### 11.Fast Data-Driven Learning of MRI Sampling Pattern for Large Scale Problems  [ :arrow_down: ](https://arxiv.org/pdf/2011.02322.pdf)
>  Purpose: A fast data-driven optimization approach, named bias-accelerated subset selection (BASS), is proposed for learning efficacious sampling patterns (SPs) with the purpose of reducing scan time in large-dimensional parallel MRI. Methods: BASS is applicable when Cartesian fully-sampled k-space data of specific anatomy is available for training and the reconstruction method is specified, learning which k-space points are more relevant for the specific anatomy and reconstruction in recovering the non-sampled points. BASS was tested with four reconstruction methods for parallel MRI based on low-rankness and sparsity that allow a free choice of the SP. Two datasets were tested, one of the brain images for high-resolution imaging and another of knee images for quantitative mapping of the cartilage. Results: BASS, with its low computational cost and fast convergence, obtained SPs 100 times faster than the current best greedy approaches. Reconstruction quality increased up to 45\% with our learned SP over that provided by variable density and Poisson disk SPs, considering the same scan time. Optionally, the scan time can be nearly halved without loss of reconstruction quality. Conclusion: Compared with current approaches, BASS can be used to rapidly learn effective SPs for various reconstruction methods, using larger SP and larger datasets. This enables a better selection of efficacious sampling-reconstruction pairs for specific MRI problems.      
### 12.Widely-distributed Radar Imaging Based on Consensus ADMM  [ :arrow_down: ](https://arxiv.org/pdf/2011.02319.pdf)
>  A widely-distributed radar system is a promising architecture to enhance radar imaging performance. However, most existing algorithms rely on isotropic scattering assumption, which is only satisfied in collocated radar systems. Moreover, due to noise and imaging model imperfections, artifacts such as layovers are common in radar images. In this paper, a novel $l_1$-regularized, consensus alternating direction method of multipliers (CADMM) based algorithm is proposed to mitigate artifacts by exploiting a widely-distributed radar system's spatial diversity. By imposing the consensus constraints on the local images formed by distributed antenna clusters and solving the resulting distributed optimization problem, the scenario's spatial-invariant common features are retained. Simultaneously, the spatial-variant artifacts are mitigated, and it will finally converge to a high-quality global image in the consensus of all distributed measurements. The proposed algorithm outperforms the joint sparsity-based composite imaging (JSC) algorithm in terms of artifacts mitigation. It can also reduce the computation and storage burden of large-scale imaging problems through its distributed and parallelizable optimization scheme.      
### 13.Prosodic Representation Learning and Contextual Sampling for Neural Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2011.02252.pdf)
>  In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage I, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage II, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use BERT on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of $13.2\%$ in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique, and show a statistically significant improvement over the baseline in each case.      
### 14.Correlation based Multi-phasal models for improved imagined speech EEG recognition  [ :arrow_down: ](https://arxiv.org/pdf/2011.02195.pdf)
>  Translation of imagined speech electroencephalogram(EEG) into human understandable commands greatly facilitates the design of naturalistic brain computer interfaces. To achieve improved imagined speech unit classification, this work aims to profit from the parallel information contained in multi-phasal EEG data recorded while speaking, imagining and performing articulatory movements corresponding to specific speech units. A bi-phase common representation learning module using neural networks is designed to model the correlation and reproducibility between an analysis phase and a support phase. The trained Correlation Network is then employed to extract discriminative features of the analysis phase. These features are further classified into five binary phonological categories using machine learning models such as Gaussian mixture based hidden Markov model and deep neural networks. The proposed approach further handles the non-availability of multi-phasal data during decoding. Topographic visualizations along with result-based inferences suggest that the multi-phasal correlation modelling approach proposed in the paper enhances imagined-speech EEG recognition performance.      
### 15.Learning in your voice: Non-parallel voice conversion based on speaker consistency loss  [ :arrow_down: ](https://arxiv.org/pdf/2011.02168.pdf)
>  In this paper, we propose a novel voice conversion strategy to resolve the mismatch between the training and conversion scenarios when parallel speech corpus is unavailable for training. Based on auto-encoder and disentanglement frameworks, we design the proposed model to extract identity and content representations while reconstructing the input speech signal itself. Since we use other speaker's identity information in the training process, the training philosophy is naturally matched with the objective of voice conversion process. In addition, we effectively design the disentanglement framework to reliably preserve linguistic information and to enhance the quality of converted speech signals. The superiority of the proposed method is shown in subjective listening tests as well as objective measures.      
### 16.Do Noises Bother Human and Neural Networks In the Same Way? A Medical Image Analysis Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2011.02155.pdf)
>  Deep learning had already demonstrated its power in medical images, including denoising, classification, segmentation, etc. All these applications are proposed to automatically analyze medical images beforehand, which brings more information to radiologists during clinical assessment for accuracy improvement. Recently, many medical denoising methods had shown their significant artifact reduction result and noise removal both quantitatively and qualitatively. However, those existing methods are developed around human-vision, i.e., they are designed to minimize the noise effect that can be perceived by human eyes. In this paper, we introduce an application-guided denoising framework, which focuses on denoising for the following neural networks. In our experiments, we apply the proposed framework to different datasets, models, and use cases. Experimental results show that our proposed framework can achieve a better result than human-vision denoising network.      
### 17.Interpretable Representation Learning for Speech and Audio Signals Based on Relevance Weighting  [ :arrow_down: ](https://arxiv.org/pdf/2011.02136.pdf)
>  The learning of interpretable representations from raw data presents significant challenges for time series data like speech. In this work, we propose a relevance weighting scheme that allows the interpretation of the speech representations during the forward propagation of the model itself. The relevance weighting is achieved using a sub-network approach that performs the task of feature selection. A relevance sub-network, applied on the output of first layer of a convolutional neural network model operating on raw speech signals, acts as an acoustic filterbank (FB) layer with relevance weighting. A similar relevance sub-network applied on the second convolutional layer performs modulation filterbank learning with relevance weighting. The full acoustic model consisting of relevance sub-networks, convolutional layers and feed-forward layers is trained for a speech recognition task on noisy and reverberant speech in the Aurora-4, CHiME-3 and VOiCES datasets. The proposed representation learning framework is also applied for the task of sound classification in the UrbanSound8K dataset. A detailed analysis of the relevance weights learned by the model reveals that the relevance weights capture information regarding the underlying speech/audio content. In addition, speech recognition and sound classification experiments reveal that the incorporation of relevance weighting in the neural network architecture improves the performance significantly.      
### 18.Multi-Modal Transformers Utterance-Level Code-Switching Detection  [ :arrow_down: ](https://arxiv.org/pdf/2011.02132.pdf)
>  An utterance that contains speech from multiple languages is known as a code-switched sentence. In this work, we propose a novel technique to predict whether given audio is mono-lingual or code-switched. We propose a multi-modal learning approach by utilising the phoneme information along with audio features for code-switch detection. Our model consists of a Phoneme Network that processes phoneme sequence and Audio Network(AN), which processes the mfcc features. We fuse representation learned from both the Networks to predict if the utterance is code-switched or not. The Audio Network and Phonetic Network consist of initial convolution, Bi-LSTM, and transformer encoder layers. The transformer encoder layer helps in selecting important and relevant features for better classification by using self-attention. We show that utilising the phoneme sequence of the utterance along with the mfcc features improves the performance of code-switch detection significantly. We train and evaluate our model on Microsoft code-switching challenge datasets for Telugu, Tamil, and Gujarati languages. Our experiments show that the multi-modal learning approach significantly improved accuracy over the uni-modal approaches for Telugu-English, Gujarati-English, and Tamil-English datasets. We also study the system performance using different neural layers and show that the transformers help obtain better performance.      
### 19.Sparse Array Beamforming Design for Wideband Signal Models  [ :arrow_down: ](https://arxiv.org/pdf/2011.02115.pdf)
>  We develop sparse array receive beamformer design methods achieving maximum signal-to-interference plus noise ratio (MaxSINR) for wideband sources and jammers. Both tapped delay line (TDL) filtering and the DFT realizations to wideband array processing are considered. The array sparsity stems from the limited number of available RF transmission chains that switch between the sensors, thereby configuring different arrays at different times. The sparse array configuration design problem is formulated as a quadratically constraint quadratic program (QCQP) and solved by using SDR (semidefinite relaxation). A computationally viable approach through SCA (successive convex relaxation) is also pursued. In order to realize an implementable design, in presence of missing autocorrelation lags, we propose parameter-free block Toeplitz matrix completion to estimate the received data correlation matrix across the entire array aperture. It is shown that the optimum wideband sparse array effectively utilizes the array aperture and provides considerable performance improvement over suboptimal array topologies.      
### 20.Deep Multi-task Network for Delay Estimation and Echo Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2011.02109.pdf)
>  Echo path delay (or ref-delay) estimation is a big challenge in acoustic echo cancellation. Different devices may introduce various ref-delay in practice. Ref-delay inconsistency slows down the convergence of adaptive filters, and also degrades the performance of deep learning models due to 'unseen' ref-delays in the training set. In this paper, a multi-task network is proposed to address both ref-delay estimation and echo cancellation tasks. The proposed architecture consists of two convolutional recurrent networks (CRNNs) to estimate the echo and enhanced signals separately, as well as a fully-connected (FC) network to estimate the echo path delay. Echo signal is first predicted, and then is combined with reference signal together for delay estimation. At the end, delay compensated reference and microphone signals are used to predict the enhanced target signal. Experimental results suggest that the proposed method makes reliable delay estimation and outperforms the existing state-of-the-art solutions in inconsistent echo path delay scenarios, in terms of echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ). Furthermore, a data augmentation method is studied to evaluate the model performance on different portion of synthetical data with artificially introduced ref-delay.      
### 21.Robust Speaker Extraction Network Based on Iterative Refined Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2011.02102.pdf)
>  Speaker extraction aims to extract target speech signal from a multi-talker environment with interference speakers and surrounding noise, given the target speaker's reference information. Most speaker extraction systems achieve satisfactory performance on the premise that the test speakers have been encountered during training time. Such systems suffer from performance degradation given unseen target speakers and/or mismatched reference voiceprint information. In this paper we propose a novel strategy named Iterative Refined Adaptation (IRA) to improve the robustness and generalization capability of speaker extraction systems in the aforementioned scenarios. Given an initial speaker embedding encoded by an auxiliary network, the extraction network can obtain a latent representation of the target speaker, which is fed back to the auxiliary network to get a refined embedding to provide more accurate guidance for the extraction network. Experiments on WSJ0-2mix-extr and WHAM! dataset confirm the superior performance of the proposed method over the network without IRA in terms of SI-SDR and PESQ improvement.      
### 22.Frustratingly Easy Noise-aware Training of Acoustic Models  [ :arrow_down: ](https://arxiv.org/pdf/2011.02090.pdf)
>  Environmental noises and reverberation have a detrimental effect on the performance of automatic speech recognition (ASR) systems. Multi-condition training of neural network-based acoustic models is used to deal with this problem, but it requires many-folds data augmentation, resulting in increased training time. In this paper, we propose utterance-level noise vectors for noise-aware training of acoustic models in hybrid ASR. Our noise vectors are obtained by combining the means of speech frames and silence frames in the utterance, where the speech/silence labels may be obtained from a GMM-HMM model trained for ASR alignments, such that no extra computation is required beyond averaging of feature vectors. We show through experiments on AMI and Aurora-4 that this simple adaptation technique can result in 6-7% relative WER improvement. We implement several embedding-based adaptation baselines proposed in literature, and show that our method outperforms them on both the datasets. Finally, we extend our method to the online ASR setting by using frame-level maximum likelihood for the mean estimation.      
### 23.Direction of Arrival Estimation for Non-Coherent Sub-Arrays via Joint Sparse and Low-Rank Signal Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2011.02083.pdf)
>  Estimating the directions of arrival (DOAs) of multiple sources from a single snapshot obtained by a coherent antenna array is a well-known problem, which can be addressed by sparse signal reconstruction methods, where the DOAs are estimated from the peaks of the recovered high-dimensional signal. In this paper, we consider a more challenging DOA estimation task where the array is composed of non-coherent sub-arrays (i.e., sub-arrays that observe different unknown phase shifts due to using low-cost unsynchronized local oscillators). We formulate this problem as the reconstruction of a joint sparse and low-rank matrix and solve its convex relaxation. While the DOAs can be estimated from the solution of the convex problem, we further show how an improvement is obtained if instead one estimates from this solution the phase shifts, creates "phase-corrected" observations and applies another final (plain, coherent) sparsity-based DOA estimation. Numerical experiments show that the proposed approach outperforms strategies that are based on non-coherent processing of the sub-arrays as well as other sparsity-based methods.      
### 24.Uncertainty Estimation in Medical Image Localization: Towards Robust Anterior Thalamus Targeting for Deep Brain Stimulation  [ :arrow_down: ](https://arxiv.org/pdf/2011.02067.pdf)
>  Atlas-based methods are the standard approaches for automatic targeting of the Anterior Nucleus of the Thalamus (ANT) for Deep Brain Stimulation (DBS), but these are known to lack robustness when anatomic differences between atlases and subjects are large. To improve the localization robustness, we propose a novel two-stage deep learning (DL) framework, where the first stage identifies and crops the thalamus regions from the whole brain MRI and the second stage performs per-voxel regression on the cropped volume to localize the targets at the finest resolution scale. To address the issue of data scarcity, we train the models with the pseudo labels which are created based on the available labeled data using multi-atlas registration. To assess the performance of the proposed framework, we validate two sampling-based uncertainty estimation techniques namely Monte Carlo Dropout (MCDO) and Test-Time Augmentation (TTA) on the second-stage localization network. Moreover, we propose a novel uncertainty estimation metric called maximum activation dispersion (MAD) to estimate the image-wise uncertainty for localization tasks. Our results show that the proposed method achieved more robust localization performance than the traditional multi-atlas method and TTA could further improve the robustness. Moreover, the epistemic and hybrid uncertainty estimated by MAD could be used to detect the unreliable localizations and the magnitude of the uncertainty estimated by MAD could reflect the degree of unreliability for the rejected predictions.      
### 25.Online Observer-Based Inverse Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.02057.pdf)
>  In this paper, a novel approach to the output-feedback inverse reinforcement learning (IRL) problem is developed by casting the IRL problem, for linear systems with quadratic cost functions, as a state estimation problem. Two observer-based techniques for IRL are developed, including a novel observer method that re-uses previous state estimates via history stacks. Theoretical guarantees for convergence and robustness are established under appropriate excitation conditions. Simulations demonstrate the performance of the developed observers and filters under noisy and noise-free measurements.      
### 26.Self-Adaptively Learning to Demoire from Focused and Defocused Image Pairs  [ :arrow_down: ](https://arxiv.org/pdf/2011.02055.pdf)
>  Moire artifacts are common in digital photography, resulting from the interference between high-frequency scene content and the color filter array of the camera. Existing deep learning-based demoireing methods trained on large scale datasets are limited in handling various complex moire patterns, and mainly focus on demoireing of photos taken of digital displays. Moreover, obtaining moire-free ground-truth in natural scenes is difficult but needed for training. In this paper, we propose a self-adaptive learning method for demoireing a high-frequency image, with the help of an additional defocused moire-free blur image. Given an image degraded with moire artifacts and a moire-free blur image, our network predicts a moire-free clean image and a blur kernel with a self-adaptive strategy that does not require an explicit training stage, instead performing test-time adaptation. Our model has two sub-networks and works iteratively. During each iteration, one sub-network takes the moire image as input, removing moire patterns and restoring image details, and the other sub-network estimates the blur kernel from the blur image. The two sub-networks are jointly optimized. Extensive experiments demonstrate that our method outperforms state-of-the-art methods and can produce high-quality demoired results. It can generalize well to the task of removing moire artifacts caused by display screens. In addition, we build a new moire dataset, including images with screen and texture moire artifacts. As far as we know, this is the first dataset with real texture moire patterns.      
### 27.Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis  [ :arrow_down: ](https://arxiv.org/pdf/2011.02014.pdf)
>  Multi-speaker speech recognition of unsegmented recordings has diverse applications such as meeting transcription and automatic subtitle generation. With technical advances in systems dealing with speech separation, speaker diarization, and automatic speech recognition (ASR) in the last decade, it has become possible to build pipelines that achieve reasonable error rates on this task. In this paper, we propose an end-to-end modular system for the LibriCSS meeting data, which combines independently trained separation, diarization, and recognition components, in that order. We study the effect of different state-of-the-art methods at each stage of the pipeline, and report results using task-specific metrics like SDR and DER, as well as downstream WER. Experiments indicate that the problem of overlapping speech for diarization and ASR can be effectively mitigated with the presence of a well-trained separation module. Our best system achieves a speaker-attributed WER of 12.7%, which is close to that of a non-overlapping ASR.      
### 28.Complex ratio masking for singing voice separation  [ :arrow_down: ](https://arxiv.org/pdf/2011.02008.pdf)
>  Music source separation is important for applications such as karaoke and remixing. Much of previous research focuses on estimating short-time Fourier transform (STFT) magnitude and discarding phase information. We observe that, for singing voice separation, phase can make considerable improvement in separation quality. This paper proposes a complex ratio masking method for voice and accompaniment separation. The proposed method employs DenseUNet with self attention to estimate the real and imaginary components of STFT for each sound source. A simple ensemble technique is introduced to further improve separation performance. Evaluation results demonstrate that the proposed method outperforms recent state-of-the-art models for both separated voice and accompaniment.      
### 29.DOVER-Lap: A Method for Combining Overlap-aware Diarization Outputs  [ :arrow_down: ](https://arxiv.org/pdf/2011.01997.pdf)
>  Several advances have been made recently towards handling overlapping speech for speaker diarization. Since speech and natural language tasks often benefit from ensemble techniques, we propose an algorithm for combining outputs from such diarization systems through majority voting. Our method, DOVER-Lap, is inspired from the recently proposed DOVER algorithm, but is designed to handle overlapping segments in diarization outputs. We also modify the pair-wise incremental label mapping strategy used in DOVER, and propose an approximation algorithm based on weighted k-partite graph matching, which performs this mapping using a global cost tensor. We demonstrate the strength of our method by combining outputs from diverse systems -- clustering-based, region proposal networks, and target-speaker voice activity detection -- on AMI and LibriCSS datasets, where it consistently outperforms the single best system. Additionally, we show that DOVER-Lap can be used for late fusion in multichannel diarization, and compares favorably with early fusion methods like beamforming.      
### 30.Internal Language Model Estimation for Domain-Adaptive End-to-End Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2011.01991.pdf)
>  The external language models (LM) integration remains a challenging task for end-to-end (E2E) automatic speech recognition (ASR) which has no clear division between acoustic and language models. In this work, we propose an internal LM estimation (ILME) method to facilitate a more effective integration of the external LM with all pre-existing E2E models with no additional model training, including the most popular recurrent neural network transducer (RNN-T) and attention-based encoder-decoder (AED) models. Trained with audio-transcript pairs, an E2E model implicitly learns an internal LM that characterizes the training data in the source domain. With ILME, the internal LM scores of an E2E model are estimated and subtracted from the log-linear interpolation between the scores of the E2E model and the external LM. The internal LM scores are approximated as the output of an E2E model when eliminating its acoustic components. ILME can alleviate the domain mismatch between training and testing, or improve the multi-domain E2E ASR. Experimented with 30K-hour trained RNN-T and AED models, ILME achieves up to 15.5% and 6.8% relative word error rate reductions from Shallow Fusion on out-of-domain LibriSpeech and in-domain Microsoft production test sets, respectively.      
### 31.Unsupervised Pattern Discovery from Thematic Speech Archives Based on Multilingual Bottleneck Features  [ :arrow_down: ](https://arxiv.org/pdf/2011.01986.pdf)
>  The present study tackles the problem of automatically discovering spoken keywords from untranscribed audio archives without requiring word-by-word speech transcription by automatic speech recognition (ASR) technology. The problem is of practical significance in many applications of speech analytics, including those concerning low-resource languages, and large amount of multilingual and multi-genre data. We propose a two-stage approach, which comprises unsupervised acoustic modeling and decoding, followed by pattern mining in acoustic unit sequences. The whole process starts by deriving and modeling a set of subword-level speech units with untranscribed data. With the unsupervisedly trained acoustic models, a given audio archive is represented by a pseudo transcription, from which spoken keywords can be discovered by string mining algorithms. For unsupervised acoustic modeling, a deep neural network trained by multilingual speech corpora is used to generate speech segmentation and compute bottleneck features for segment clustering. Experimental results show that the proposed system is able to effectively extract topic-related words and phrases from the lecture recordings on MIT OpenCourseWare.      
### 32.Short-time deep-learning based source separation for speech enhancement in reverberant environments with beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2011.01965.pdf)
>  The source separation-based speech enhancement problem with multiple beamforming in reverberant indoor environments is addressed in this paper. We propose that more generic solutions should cope with time-varying dynamic scenarios with moving microphone array or sources such as those found in voice-based human-robot interaction or smart speaker applications. The effectiveness of ordinary source separation methods based on statistical models such as ICA and NMF depends on the analysis window size and cannot handle reverberation environments. To address these limitations, a short-term source separation method based on a temporal convolutional network in combination with compact bilinear pooling is presented. The proposed scheme is virtually independent of the analysis window size and does not lose effectiveness when the analysis window is shortened to 1.6s, which in turn is very interesting to tackle the source separation problem in time-varying scenarios. Also, improvements in WER as high as 80% were obtained when compared to ICA and NMF with multi-condition reverberant training and testing, and with time-varying SNR experiments to simulate a moving target speech source. Finally, the experiment with the estimation of the clean signal employing the proposed scheme and a clean trained ASR provided a WER 13% lower than the one obtained with the corrupted signal and a multi-condition trained ASR. This surprising result contradicts the widely adopted practice of using multi-condition trained ASR systems and reinforce the use of speech enhancement methods for user profiling in HRI environments.      
### 33.Geometric Solution of Image Degradation by Diffraction in Lensless Sensing and Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2011.02468.pdf)
>  This letter proposes a non-computational method of counteracting the effect of image degradation introduced by the diffraction phenomenon in lensless microscopy. The method builds upon the certainty that all the optical images (whether focused by lenses or not) are diffraction patterns and vice versa, which preserve the visual information upto a certain extent determined by the size of the point spread functions, like airy disks in some cases. A highly diverging beam can be exploited to reduce the spatial extent of these point spread functions relatively in the transformed projective space, which can aid us in the spatial unmixing of the visual information. The principle has been experimentally validated by the lensless imaging of red blood cells of diameter ~6-9 micrometers and a photolithography mask with features in micrometer scale. The important advantages of the proposed approach of shadow imaging is the improved depth of field and a drastic increase in the sensor to sample working distance.      
### 34.Single channel voice separation for unknown number of speakers under reverberant and noisy settings  [ :arrow_down: ](https://arxiv.org/pdf/2011.02329.pdf)
>  We present a unified network for voice separation of an unknown number of speakers. The proposed approach is composed of several separation heads optimized together with a speaker classification branch. The separation is carried out in the time domain, together with parameter sharing between all separation heads. The classification branch estimates the number of speakers while each head is specialized in separating a different number of speakers. We evaluate the proposed model under both clean and noisy reverberant set-tings. Results suggest that the proposed approach is superior to the baseline model by a significant margin. Additionally, we present a new noisy and reverberant dataset of up to five different speakers speaking simultaneously.      
### 35.VAW-GAN for Disentanglement and Recomposition of Emotional Elements in Speech  [ :arrow_down: ](https://arxiv.org/pdf/2011.02314.pdf)
>  Emotional voice conversion (EVC) aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity. In this paper, we study the disentanglement and recomposition of emotional elements in speech through variational autoencoding Wasserstein generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for spectrum conversion, and another for prosody conversion. We train a spectral encoder that disentangles emotion and prosody (F0) information from spectral features; we also train a prosodic encoder that disentangles emotion modulation of prosody (affective prosody) from linguistic prosody. At run-time, the decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN. The vocoder takes the converted spectral and prosodic features to generate the target emotional speech. Experiments validate the effectiveness of our proposed method in both objective and subjective evaluations.      
### 36.FDRN: A Fast Deformable Registration Network for Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2011.02307.pdf)
>  Deformable image registration is a fundamental task in medical imaging. Due to the large computational complexity of deformable registration of volumetric images, conventional iterative methods usually face the tradeoff between the registration accuracy and the computation time in practice. In order to boost the registration performance in both accuracy and runtime, we propose a fast unsupervised convolutional neural network for deformable image registration. Specially, the proposed FDRN possesses a compact encoder-decoder structure and exploits deep supervision, additive forwarding and residual learning. We conducted comparison with the existing state-of-the-art registration methods on the LPBA40 brain MRI dataset. Experimental results demonstrate that our FDRN performs better than the investigated methods qualitatively and quantitatively in Dice score and normalized cross correlation (NCC). Besides, FDRN is a generalized framework for image registration which is not confined to a particular type of medical images or anatomy. It can also be applied to other anatomical structures or CT images.      
### 37.Surgical Data Science -- from Concepts to Clinical Translation  [ :arrow_down: ](https://arxiv.org/pdf/2011.02284.pdf)
>  Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical data science is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of surgical data science, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) technical infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. Drawing from this extensive review, we present current challenges for technology development and (4) describe a roadmap for faster clinical translation and exploitation of the full potential of surgical data science.      
### 38.Video Generative Adversarial Networks: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2011.02250.pdf)
>  With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increasing trend in the papers that uses AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) in one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models, in general, have been covered to some extent in several survey papers, to the best of our knowledge, this is among the first survey papers that reviews the state-of-the-art video GANs models. This paper first categorized GANs review papers into general GANs review papers, image GANs review papers, and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs frameworks that are not initially developed for the video domain but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models is provided under two main divisions according to the presence or non-presence of a condition. The conditional models then further grouped according to the type of condition into audio, text, video, and image. The paper is concluded by highlighting the main challenges and limitations of the current video GANs models. A comprehensive list of datasets, applied loss functions, and evaluation metrics is provided in the supplementary material.      
### 39.BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering Realistic Bokeh  [ :arrow_down: ](https://arxiv.org/pdf/2011.02242.pdf)
>  A photo captured with bokeh effect often means objects in focus are sharp while the out-of-focus areas are all blurred. DSLR can easily render this kind of effect naturally. However, due to the limitation of sensors, smartphones cannot capture images with depth-of-field effects directly. In this paper, we propose a novel generator called Glass-Net, which generates bokeh images not relying on complex hardware. Meanwhile, the GAN-based method and perceptual loss are combined for rendering a realistic bokeh effect in the stage of finetuning the model. Moreover, Instance Normalization(IN) is reimplemented in our network, which ensures our tflite model with IN can be accelerated on smartphone GPU. Experiments show that our method is able to render a high-quality bokeh effect and process one $1024 \times 1536$ pixel image in 1.9 seconds on all smartphone chipsets. This approach ranked First in AIM 2020 Rendering Realistic Bokeh Challenge Track 1 \&amp; Track 2.      
### 40.Two-timescale Beamforming Optimization for Intelligent Reflecting Surface Aided Multiuser Communication with QoS Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2011.02237.pdf)
>  Intelligent reflecting surface (IRS) is an emerging technology that is able to reconfigure the wireless channel via tunable passive signal reflection and thereby enhance the spectral and energy efficiency of wireless networks cost-effectively. In this paper, we study an IRS-aided multiuser multiple-input single-output (MISO) wireless system and adopt the two-timescale (TTS) transmission to reduce the signal processing complexity and channel training overhead as compared to the existing schemes based on the instantaneous channel state information (I-CSI), and at the same time, exploit the multiuser channel diversity in transmission scheduling. Specifically, the long-term passive beamforming is designed based on the statistical CSI (S-CSI) of all links, while the short-term active beamforming is designed to cater to the I-CSI of all users' reconfigured channels with optimized IRS phase shifts. We aim to minimize the average transmit power at the access point (AP), subject to the users' individual quality of service (QoS) constraints. The formulated stochastic optimization problem is non-convex and difficult to solve since the long-term and short-term design variables are complicatedly coupled in the QoS constraints. To tackle this problem, we propose an efficient algorithm, called the primal-dual decomposition based TTS joint active and passive beamforming (PDD-TJAPB), where the original problem is decomposed into a long-term problem and a family of short-term problems, and the deep unfolding technique is employed to extract gradient information from the short-term problems to construct a convex surrogate problem for the long-term problem. The proposed algorithm is proved to converge to a stationary solution of the original problem almost surely. Simulation results are presented which demonstrate the advantages and effectiveness of the proposed algorithm as compared to benchmark schemes.      
### 41.Continuous PPG-Based Blood Pressure Monitoring Using Multi-Linear Regression  [ :arrow_down: ](https://arxiv.org/pdf/2011.02231.pdf)
>  In this work, we present the Senbiosys blood pressure monitoring algorithm (SB-BPM) that solely requires a photoplethysmography (PPG) signal. The technology is based on pulse wave analysis (PWA) of PPG signals retrieved from different body locations to continuously estimate the systolic blood pressure (SBP) and the diastolic blood pressure (DBP).      
### 42.IEEE SLT 2021 Alpha-mini Speech Challenge: Open Datasets, Tracks, Rules and Baselines  [ :arrow_down: ](https://arxiv.org/pdf/2011.02198.pdf)
>  The IEEE Spoken Language Technology Workshop (SLT) 2021 Alpha-mini Speech Challenge (ASC) is intended to improve research on keyword spotting (KWS) and sound source location (SSL) on humanoid robots. Many publications report significant improvements in deep learning based KWS and SSL on open source datasets in recent years. For deep learning model training, it is necessary to expand the data coverage to improve the robustness of model. Thus, simulating multi-channel noisy and reverberant data from single-channel speech, noise, echo and room impulsive response (RIR) is widely adopted. However, this approach may generate mismatch between simulated data and recorded data in real application scenarios, especially echo data. In this challenge, we open source a sizable speech, keyword, echo and noise corpus for promoting data-driven methods, particularly deep-learning approaches on KWS and SSL. We also choose Alpha-mini, a humanoid robot produced by UBTECH equipped with a built-in four-microphone array on its head, to record development and evaluation sets under the actual Alpha-mini robot application scenario, including noise as well as echo and mechanical noise generated by the robot itself for model evaluation. Furthermore, we illustrate the rules, evaluation methods and baselines for researchers to quickly assess their achievements and optimize their models.      
### 43.Data Augmentation for End-to-end Code-switching Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2011.02160.pdf)
>  Training a code-switching end-to-end automatic speech recognition (ASR) model normally requires a large amount of data, while code-switching data is often limited. In this paper, three novel approaches are proposed for code-switching data augmentation. Specifically, they are audio splicing with the existing code-switching data, and TTS with new code-switching texts generated by word translation or word insertion. Our experiments on 200 hours Mandarin-English code-switching dataset show that all the three proposed approaches yield significant improvements on code-switching ASR individually. Moreover, all the proposed approaches can be combined with recent popular SpecAugment, and an addition gain can be obtained. WER is significantly reduced by relative 24.0% compared to the system without any data augmentation, and still relative 13.0% gain compared to the system with only SpecAugment      
### 44.DESNet: A Multi-channel Network for Simultaneous Speech Dereverberation, Enhancement and Separation  [ :arrow_down: ](https://arxiv.org/pdf/2011.02131.pdf)
>  In this paper, we propose a multi-channel network for simultaneous speech dereverberation, enhancement and separation (DESNet). To enable gradient propagation and joint optimization, we adopt the attentional selection mechanism of the multi-channel features, which is originally proposed in end-to-end unmixing, fixed-beamforming and extraction (E2E-UFE) structure. Furthermore, the novel deep complex convolutional recurrent network (DCCRN) is used as the structure of the speech unmixing and the neural network based weighted prediction error (WPE) is cascaded beforehand for speech dereverberation. We also introduce the staged SNR strategy and symphonic loss for the training of the network to further improve the final performance. Experiments show that in non-dereverberated case, the proposed DESNet outperforms DCCRN and most state-of-the-art structures in speech enhancement and separation, while in dereverberated scenario, DESNet also shows improvements over the cascaded WPE-DCCRN networks.      
### 45.Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese, and Bataks Speech Recognition and Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2011.02128.pdf)
>  Even though over seven hundred ethnic languages are spoken in Indonesia, the available technology remains limited that could support communication within indigenous communities as well as with people outside the villages. As a result, indigenous communities still face isolation due to cultural barriers; languages continue to disappear. To accelerate communication, speech-to-speech translation (S2ST) technology is one approach that can overcome language barriers. However, S2ST systems require machine translation (MT), speech recognition (ASR), and synthesis (TTS) that rely heavily on supervised training and a broad set of language resources that can be difficult to collect from ethnic communities. Recently, a machine speech chain mechanism was proposed to enable ASR and TTS to assist each other in semi-supervised learning. The framework was initially implemented only for monolingual languages. In this study, we focus on developing speech recognition and synthesis for these Indonesian ethnic languages: Javanese, Sundanese, Balinese, and Bataks. We first separately train ASR and TTS of standard Indonesian in supervised training. We then develop ASR and TTS of ethnic languages by utilizing Indonesian ASR and TTS in a cross-lingual machine speech chain framework with only text or only speech data removing the need for paired speech-text data of those ethnic languages.      
### 46.Sequence-to-Sequence Learning via Attention Transfer for Incremental Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2011.02127.pdf)
>  Attention-based sequence-to-sequence automatic speech recognition (ASR) requires a significant delay to recognize long utterances because the output is generated after receiving entire input sequences. Although several studies recently proposed sequence mechanisms for incremental speech recognition (ISR), using different frameworks and learning algorithms is more complicated than the standard ASR model. One main reason is because the model needs to decide the incremental steps and learn the transcription that aligns with the current short speech segment. In this work, we investigate whether it is possible to employ the original architecture of attention-based ASR for ISR tasks by treating a full-utterance ASR as the teacher model and the ISR as the student model. We design an alternative student network that, instead of using a thinner or a shallower model, keeps the original architecture of the teacher model but with shorter sequences (few encoder and decoder states). Using attention transfer, the student network learns to mimic the same alignment between the current input short speech segments and the transcription. Our experiments show that by delaying the starting time of recognition process with about 1.7 sec, we can achieve comparable performance to one that needs to wait until the end.      
### 47.Incremental Machine Speech Chain Towards Enabling Listening while Speaking in Real-time  [ :arrow_down: ](https://arxiv.org/pdf/2011.02126.pdf)
>  Inspired by a human speech chain mechanism, a machine speech chain framework based on deep learning was recently proposed for the semi-supervised development of automatic speech recognition (ASR) and text-to-speech synthesis TTS) systems. However, the mechanism to listen while speaking can be done only after receiving entire input sequences. Thus, there is a significant delay when encountering long utterances. By contrast, humans can listen to what hey speak in real-time, and if there is a delay in hearing, they won't be able to continue speaking. In this work, we propose an incremental machine speech chain towards enabling machine to listen while speaking in real-time. Specifically, we construct incremental ASR (ISR) and incremental TTS (ITTS) by letting both systems improve together through a short-term loop. Our experimental results reveal that our proposed framework is able to reduce delays due to long utterances while keeping a comparable performance to the non-incremental basic machine speech chain.      
### 48.Can We Trust Deep Speech Prior?  [ :arrow_down: ](https://arxiv.org/pdf/2011.02110.pdf)
>  Recently, speech enhancement (SE) based on deep speech prior has attracted much attention, such as the variational auto-encoder with non-negative matrix factorization (VAE-NMF) architecture. Compared to conventional approaches that represent clean speech by shallow models such as Gaussians with a low-rank covariance, the new approach employs deep generative models to represent the clean speech, which often provides a better prior. Despite the clear advantage in theory, we argue that deep priors must be used with much caution, since the likelihood produced by a deep generative model does not always coincide with the speech quality. We designed a comprehensive study on this issue and demonstrated that based on deep speech priors, a reasonable SE performance can be achieved, but the results might be suboptimal. A careful analysis showed that this problem is deeply rooted in the disharmony between the flexibility of deep generative models and the nature of the maximum-likelihood (ML) training.      
### 49.Augmenting Images for ASR and TTS through Single-loop and Dual-loop Multimodal Chain Framework  [ :arrow_down: ](https://arxiv.org/pdf/2011.02099.pdf)
>  Previous research has proposed a machine speech chain to enable automatic speech recognition (ASR) and text-to-speech synthesis (TTS) to assist each other in semi-supervised learning and to avoid the need for a large amount of paired speech and text data. However, that framework still requires a large amount of unpaired (speech or text) data. A prototype multimodal machine chain was then explored to further reduce the need for a large amount of unpaired data, which could improve ASR or TTS even when no more speech or text data were available. Unfortunately, this framework relied on the image retrieval (IR) model, and thus it was limited to handling only those images that were already known during training. Furthermore, the performance of this framework was only investigated with single-speaker artificial speech data. In this study, we revamp the multimodal machine chain framework with image generation (IG) and investigate the possibility of augmenting image data for ASR and TTS using single-loop and dual-loop architectures on multispeaker natural speech data. Experimental results revealed that both single-loop and dual-loop multimodal chain frameworks enabled ASR and TTS to improve their performance using an image-only dataset.      
### 50.Non contrast Doppler Microvessel Image Reconstruction by a semi Nonrigid Motion Compensation and Localized Clutter Filtering; a Qualitative and Quantitative Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2011.02096.pdf)
>  Vascular networks can provide invaluable information about tumor angiogenesis. Ultrafast Doppler imaging enables ultrasound to image micro-vessels by applying tissue clutter filtering methods on the Spatio-temporal data obtained from plane-wave imaging. However, motion is an intrinsic part of microvasculature imaging due to various reasons e.g breathing and vessel pulsation. Part of such a motion is taken care of by using Spatio-temporal cluttering filtering. Nonetheless, the remaining part of the motion who manifests itself as blurring or generating ghost vessels should be corrected using another level of motion compensation. We proposed a robust and computationally efficient motion compensation algorithm for Ultrasound micro-vessel imaging. We successfully evaluated the performance of the algorithm by a simulation study. Finally, we tested the proposed motion compensation method on the in vivo data of microvasculature in different organs including breast and thyroid. Results show blurring and ghost vessel problems are significantly reduced using the proposed algorithm. Moreover, our quantitative assessment demonstrated that image correlation among different frames in Ultrafast Doppler imaging is significantly improved utilizing the proposed motion correction method.      
### 51.DeepReach: A Deep Learning Approach to High-Dimensional Reachability  [ :arrow_down: ](https://arxiv.org/pdf/2011.02082.pdf)
>  Hamilton-Jacobi (HJ) reachability analysis is an important formal verification method for guaranteeing performance and safety properties of dynamical control systems. Its advantages include compatibility with general nonlinear system dynamics, formal treatment of bounded disturbances, and the ability to deal with state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the number of state variables, limiting its direct use to small-scale systems. We propose DeepReach, a method that leverages new developments in sinusoidal networks to develop a neural PDE solver for high-dimensional reachability problems. The computational requirements of DeepReach do not scale directly with the state dimension, but rather with the complexity of the underlying reachable tube. DeepReach achieves comparable results to the state-of-the-art reachability methods, does not require any explicit supervision for the PDE solution, can easily handle external disturbances, adversarial inputs, and system constraints, and also provides a safety controller for the system. We demonstrate DeepReach on a 9D multi-vehicle collision problem, and a 10D narrow passage problem, motivated by autonomous driving applications.      
### 52.MBVI: Model-Based Value Initialization for Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.02073.pdf)
>  Model-free reinforcement learning (RL) is capable of learning control policies for high-dimensional, complex robotic tasks, but tends to be data inefficient. Model-based RL and optimal control have been proven to be much more data-efficient if an accurate model of the system and environment is known, but can be difficult to scale to expressive models for high-dimensional problems. In this paper, we propose a novel approach to alleviate data inefficiency of model-free RL by warm-starting the learning process using model-based solutions. We do so by initializing a high-dimensional value function via supervision from a low-dimensional value function obtained by applying model-based techniques on a low-dimensional problem featuring an approximate system model. Therefore, our approach exploits the model priors from a simplified problem space implicitly and avoids the direct use of high-dimensional, expressive models. We demonstrate our approach on two representative robotic learning tasks and observe significant improvements in performance and efficiency, and analyze our method empirically with a third task.      
### 53.The upgrade of EAST Safety and Interlock system  [ :arrow_down: ](https://arxiv.org/pdf/2011.02028.pdf)
>  The Experimental Advanced Superconducting Tokamak (EAST), a nation-level large-scale scientific project of China, plays a key role for the research of peaceful utilizations of fusion energy. The safety and interlock system (SIS) is in charge of the supervision and control of all the EAST components involved in the protection of human and tokamak from potential accidents. With the development of physical experiment, the SIS had come close to reaching its limits for expandability. Therefore, a prototype for upgrading EAST SIS has been designed, and a fast architecture based on COTS FPGA is absorbed into the new SIS. This paper presents EAST machine and human protection mechanism and the architecture of the upgrading safety and interlock system.      
### 54.Analysis and Reliability of Separable Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.02027.pdf)
>  The operation of a system, such as a vehicle, communication network or automatic process, heavily depends on the correct operation of its components. A Stochastic Binary System (SBS) mathematically models the behavior of on-off systems, where the components are subject to probabilistic failures. Our goal is to understand the reliability of the global system. <br>The reliability evaluation of an SBS belongs to the class of NP-Hard problems, and the combinatorics of SBS imposes several challenges. In a previous work by the same authors, a special sub-class of SBSs called "separable systems" was introduced. These systems accept an efficient representation by a linear inequality on the binary states of the components. However, the reliability evaluation of separable systems is still hard. <br>A theoretical contribution in the understanding of separable systems is given. We fully characterize separable systems under the all-terminal reliability model, finding that they admit efficient reliability evaluation in this relevant context.      
### 55.Quasi Monte Carlo Time-Frequency Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2011.02025.pdf)
>  We study signal processing tasks in which the signal is mapped via some generalized time-frequency transform to a higher dimensional time-frequency space, processed there, and synthesized to an output signal. We show how to approximate such methods using a quasi-Monte Carlo (QMC) approach. The QMC method speeds up computations, since the number of samples required for a certain accuracy is log-linear in the resolution of the signal space, and depends only weakly on the resolution of the time-frequency space, which is typically higher. We focus on signal processing based on the localizing time-frequency transform (LTFT). In the LTFT, the time-frequency plane is enhanced by adding a third axis. This higher dimensional time-frequency space improves the quality of some time-frequency signal processing tasks, like phase vocoder (an audio signal processing effect). Since the computational complexity of the QMC is log-linear in the resolution of the signal space, this higher dimensional time-frequency space does not degrade the computation complexity of the QMC method. This is in contrast to more standard grid based discretization methods, that increase exponentially in the dimension of the time-frequency space. The QMC method is also more efficient than standard Monte Carlo methods, since the deterministic QMC sample points are optimally spread in the time-frequency space, while random samples are not.      
### 56.Arbitrary Order Fixed-Time Differentiators  [ :arrow_down: ](https://arxiv.org/pdf/2011.02012.pdf)
>  Differentiation is an important task in control, observation and fault detection. Levant's differentiator is unique, since it is able to estimate exactly and robustly the derivatives of a signal with a bounded high-order derivative. However, the convergence time, although finite, grows unboundedly with the norm of the initial differentiation error, making it uncertain when the estimated derivative is exact. In this paper we propose an extension of Levant's differentiator so that the worst case convergence time can be arbitrarily assigned independently of the initial condition, i.e. the estimation converges in \emph{Fixed-Time}. We propose also a family of continuous differentiators and provide a unified Lyapunov framework for analysis and design.      
### 57.Detection of Maternal and Fetal Stress from ECG with Self-supervised Representation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.02000.pdf)
>  In a pregnant mother and her fetus, chronic prenatal stress results in entrainment of the fetal heartbeat by the maternal heartbeat, quantified by the fetal stress index (FSI). Deep learning (DL) is capable of pattern detection in complex medical data with high accuracy in noisy real-life environments, but little is known about the utility of DL in non-invasive biometrics during pregnancy. We hypothesized that a recently established self-supervised learning (SSL) approach that provides emotional recognition from ECG will identify chronically stressed mother-fetus dyads from the raw maternal abdominal electrocardiograms (aECG), containing fetal and maternal ECG. Chronically stressed mothers and controls matched on enrollment at 32 weeks of gestation were studied. We validated the chronic stress exposure by psychological inventory, maternal hair cortisol, and FSI. We tested two variants of SSL architecture, one trained on the generic ECG features for emotional recognition obtained from public datasets and another transfer-learned on a subset of our data. Our DL models predict excellently the chronic stress exposure, the individual psychological score, and FSI values at 34 weeks of gestation and maternal cortisol at birth. The best performance was achieved with the DL model trained on the public dataset and using maternal ECG alone. The present DL approach provides a novel source of physiological insights into complex multi-modal relationships between different regulatory systems exposed to chronic stress. The developed DL model can be deployed in low-cost regular ECG biosensors as a simple ubiquitous early stress detection and exposure tool during pregnancy. This discovery should enable early behavioral interventions.      
