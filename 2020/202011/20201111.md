# ArXiv eess --Wed, 11 Nov 2020
### 1.Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.05317.pdf)
>  This paper explores how well deep learning models trained on chest CT images can diagnose COVID-19 infected people in a fast and automated process. To this end, we adopt advanced deep network architectures and propose a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conduct extensive sets of experiments on two CT image datasets, namely the SARS-CoV-2 CT-scan and the COVID19-CT. The obtained results show superior performances for our models compared with previous studies, where our best models achieve average accuracy, precision, sensitivity, specificity and F1 score of 99.4%, 99.6%, 99.8%, 99.6% and 99.4% on the SARS-CoV-2 dataset; and 92.9%, 91.3%, 93.7%, 92.2% and 92.5% on the COVID19-CT dataset, respectively. Furthermore, we apply two visualization techniques to provide visual explanations for the models' predictions. The visualizations show well-separated clusters for CT images of COVID-19 from other lung diseases, and accurate localizations of the COVID-19 associated regions.      
### 2.EPSR: Edge Profile Super resolution  [ :arrow_down: ](https://arxiv.org/pdf/2011.05308.pdf)
>  Recently numerous deep convolutional neural networks(CNNs) have been explored in single image super-resolution(SISR) and they achieved significant performance. However, most deep CNN-based SR mainly focuses on designing wider or deeper architecture and it is hard to find methods that utilize image properties in SISR. In this paper, by developing an edge-profile approach based on end-to-end CNN model to SISR problem, we propose an edge profile super resolution(EPSR). Specifically, we construct a residual edge enhance block(REEB), which consists of residual efficient channel attention block(RECAB), edge profile(EP) module, and context network(CN) module. RE-CAB extracts adaptively rescale channel-wise features by considering interdependencies among channels efficiently.From the features, EP module generates edge-guided features by extracting edge profile itself, and then CN module enhances details by exploiting contextual information of the features. To utilize various information from low to high frequency components, we design a fractal skip connection(FSC) structure. Since self-similarity of the architecture, FSC structure allows our EPSR to bypass abundant information into each REEB block. Experimental results present that our EPSR achieves competitive performance against state-of-the-art methods.      
### 3.A Stochastic Optimal Control Model with Internal Feedback and Velocity Tracking for Saccades  [ :arrow_down: ](https://arxiv.org/pdf/2011.05292.pdf)
>  A stochastic optimal control based model with velocity tracking and internal feedback for saccadic eye movements is presented in this paper. Recent evidence from neurophysiological studies of superior colliculus suggests the presence of a dynamic input to the saccade generation system that encodes saccade velocity, rather than just the saccade amplitude and direction. The new evidence makes it imperative to test if saccade control can use a desired velocity input which is the basis for the proposed velocity tracking model. The model is validated using behavioral data of saccades generated by healthy human subjects. It generates trajectories of horizontal saccades made to different amplitudes as well as predicts vertical and oblique saccade behavior. This paper presents the first-ever model of the saccadic system in an optimal control framework using an alternate interpretation of velocity-based control, contrary to the dominant end-point based models available in the literature.      
### 4.Control Protocol Design and Analysis for Unmanned Aircraft System Traffic Management  [ :arrow_down: ](https://arxiv.org/pdf/2011.05274.pdf)
>  Due to the rapid development technologies for small unmanned aircraft systems (sUAS), the supply and demand market for sUAS is expanding globally. With the great number of sUAS ready to fly in civilian airspace, an sUAS aircraft traffic management system that can guarantee the safe and efficient operation of sUAS is still at absence. In this paper, we propose a control protocol design and analysis method for sUAS traffic management (UTM) which can safely manage a large number of sUAS. The benefits of our approach are two folds: at the top level, the effort for monitoring sUAS traffic (authorities) and control/planning for each sUAS (operator/pilot) are both greatly reduced under our framework; and at the low level, the behavior of individual sUAS is guaranteed to follow the restrictions. Mathematical proofs and numerical simulations are presented to demonstrate the proposed method.      
### 5.Probabilistic Hosting Capacity Analysis via Bayesian Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2011.05193.pdf)
>  This paper studies the probabilistic hosting capacity analysis (PHCA) problem in distribution networks considering uncertainties from distributed energy resources (DERs) and residential loads. PHCA aims to compute the hosting capacity, which is defined as the maximal level of DERs that can be securely integrated into a distribution network while satisfying operational constraints with high probability. We formulate PHCA as a chance-constrained optimization problem, and model the uncertainties from DERs and loads using historical data. Due to non-convexities and a substantial number of historical scenarios being used, PHCA is often formulated as large-scale nonlinear optimization problem, thus computationally intractable to solve. To address the core computational challenges, we propose a fast and extensible framework to solve PHCA based on Bayesian Optimization (BayesOpt). Comparing with state-of-the-art algorithms such as interior point and active set, numerical results show that the proposed BayesOpt approach is able to find better solutions (25% higher hosting capacity) with 70% savings in computation time on average.      
### 6.Pristine annotations-based multi-modal trained artificial intelligence solution to triage chest X-ray for COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2011.05186.pdf)
>  The COVID-19 pandemic continues to spread and impact the well-being of the global population. The front-line modalities including computed tomography (CT) and X-ray play an important role for triaging COVID patients. Considering the limited access of resources (both hardware and trained personnel) and decontamination considerations, CT may not be ideal for triaging suspected subjects. Artificial intelligence (AI) assisted X-ray based applications for triaging and monitoring require experienced radiologists to identify COVID patients in a timely manner and to further delineate the disease region boundary are seen as a promising solution. Our proposed solution differs from existing solutions by industry and academic communities, and demonstrates a functional AI model to triage by inferencing using a single x-ray image, while the deep-learning model is trained using both X-ray and CT data. We report on how such a multi-modal training improves the solution compared to X-ray only training. The multi-modal solution increases the AUC (area under the receiver operating characteristic curve) from 0.89 to 0.93 and also positively impacts the Dice coefficient (0.59 to 0.62) for localizing the pathology. To the best our knowledge, it is the first X-ray solution by leveraging multi-modal information for the development.      
### 7.Improving Prosody Modelling with Cross-Utterance BERT Embeddings for End-to-end Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2011.05161.pdf)
>  Despite prosody is related to the linguistic information up to the discourse structure, most text-to-speech (TTS) systems only take into account that within each sentence, which makes it challenging when converting a paragraph of texts into natural and expressive speech. In this paper, we propose to use the text embeddings of the neighboring sentences to improve the prosody generation for each utterance of a paragraph in an end-to-end fashion without using any explicit prosody features. More specifically, cross-utterance (CU) context vectors, which are produced by an additional CU encoder based on the sentence embeddings extracted by a pre-trained BERT model, are used to augment the input of the Tacotron2 decoder. Two types of BERT embeddings are investigated, which leads to the use of different CU encoder structures. Experimental results on a Mandarin audiobook dataset and the LJ-Speech English audiobook dataset demonstrate the use of CU information can improve the naturalness and expressiveness of the synthesized speech. Subjective listening testing shows most of the participants prefer the voice generated using the CU encoder over that generated using standard Tacotron2. It is also found that the prosody can be controlled indirectly by changing the neighbouring sentences.      
### 8.Machine learning based luminance analysis of a $Î¼$LED array  [ :arrow_down: ](https://arxiv.org/pdf/2011.05155.pdf)
>  In the past years, the development of $\mu$LED arrays gained momentum since they combine the advantages of $\mu$LEDs, such as high brightness and longevity, with a high resolution of a micro-scaled structure. For the development, spatially resolved measurements of luminance and color of single $\mu$LEDs and the entire light-emitting surface are analyzed as they are crucial for the visual perception. However, the former is time intense in measurement and evaluation, and the latter suffers from interference caused by nonfunctional $\mu$LEDs. This paper presents a method to perform both analyzes with a single measurement using unsupervised machine learning. The results suggest that a precious reconstruction of the $\mu$LEDs and a more accurate characterization $\mu$LED arrays can be achieved.      
### 9.X-ray phase-contrast imaging: a broad overview of some fundamentals  [ :arrow_down: ](https://arxiv.org/pdf/2011.05146.pdf)
>  We outline some basics of imaging using both fully-coherent and partially-coherent X-ray beams, with an emphasis on phase-contrast imaging. We open with some of the basic notions of X-ray imaging, including the vacuum wave equations and the physical meaning of the intensity and phase of complex scalar fields. The projection approximation is introduced, together with the concepts of attenuation contrast and phase contrast. We also outline the multi-slice approach to X-ray propagation through thick samples or optical elements, together with the Fresnel scaling theorem. Having introduced the fundamentals, we then consider several aspects of the forward problem, of modelling the formation of phase-contrast X-ray images. Several topics related to this forward problem are considered, including the transport-of-intensity equation, arbitrary linear imaging systems, shift-invariant linear imaging systems, the transfer-function formalism, blurring induced by finite source size, the space-frequency model for partially-coherent fields, and the Fokker-Planck equation for paraxial X-ray imaging. Having considered these means for modelling the formation of X-ray phase-contrast images, we then consider aspects of the associated inverse problem of phase retrieval. This concerns how one may decode phase-contrast images to gain information regarding the sample-induced attenuation and phase shift.      
### 10.Image transmission through a flexible multimode fiber by deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.05144.pdf)
>  When multimode optical fibers are perturbed, the data that is transmitted through them is scrambled. This presents a major difficulty for many possible applications, such as multimode fiber based telecommunication and endoscopy. To overcome this challenge we present a deep learning approach that generalizes over mechanical perturbations. Using our approach, we are able to successfully reconstruct the input images from intensity-only measurements of speckle patterns at the output of a 1.5 meter-long randomly perturbed multimode fiber. We explain the model's success by hidden correlations in the speckle of random fiber conformations.      
### 11.Classification of optics-free images with deep neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.05132.pdf)
>  The thinnest possible camera is achieved by removing all optics, leaving only the image sensor. We train deep neural networks to perform multi-class detection and binary classification (with accuracy of 92%) on optics-free images without the need for anthropocentric image reconstructions. Inferencing from optics-free images has the potential for enhanced privacy and power efficiency.      
### 12.Scannerless non-line-of-sight three dimensional imaging with a 32x32 SPAD array  [ :arrow_down: ](https://arxiv.org/pdf/2011.05122.pdf)
>  We develop a scannerless non-line-of-sight three dimensional imaging system based on a commercial 32x32 SPAD camera combined with a 70 ps pulsed laser. In our experiment, 1024 time histograms can be achieved synchronously in 3s with an average time resolution of about 165 ps. The result with filtered back projection shows a discernable reconstruction while the result using virtual wave field demonstrates a better quality similar to the ones created by earlier scanning imaging systems with single pixel SPAD. Comparatively, our system has large potential advantages in frame frequency, power requirements, compactness and robustness. The research results will pave a path for scannerless non-line-of-sight three dimensional imaging application.      
### 13.PUPILS pipeline: A flexible Matlab toolbox for eyetracking and pupillometry data processing  [ :arrow_down: ](https://arxiv.org/pdf/2011.05118.pdf)
>  With the development of widely available commercial eye trackers, the use of eye tracking and pupillometry have become prevalent tools in cognitive research, both in industry and academia. However, dealing with pupil recordings often proves challenging, as the raw data produced by the eye tracker is subject to noise and artifacts. With the PUPILS toolbox the authors wish to simplify the processing of pupil data for researchers. The toolbox receives the raw data extracted from the eye tracker output files and provides an analysis of the relevant pupillary events (e.g., blinks and saccades), interpolates missing data and denoises the recorded traces. The final output of the toolbox is a cleaned up recording that can be easily exported for use in further analysis.      
### 14.Blind SNR Estimation and Nonparametric Channel Denoising in Multi-Antenna mmWave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.05113.pdf)
>  We propose blind estimators for the average noise power, receive signal power, signal-to-noise ratio (SNR), and mean-square error (MSE) suitable for multi-antenna millimeter wave (mmWave) wireless systems. The proposed estimators can be computed at low complexity and solely rely on beamspace sparsity, i.e., the fact that only a small number of dominant propagation paths exist in typical mmWave channels. Our estimators can be used (i) to quickly track some of the key quantities in multi-antenna mmWave systems while avoiding additional pilot overhead and (ii) to design efficient nonparametric algorithms that require such quantities. We provide a theoretical analysis of the proposed estimators, and we demonstrate their efficacy via synthetic experiments and using a nonparametric channel-vector denoising task with realistic multi-antenna mmWave channels.      
### 15.Noise2Stack: Improving Image Restoration by Learning from Volumetric Data  [ :arrow_down: ](https://arxiv.org/pdf/2011.05105.pdf)
>  Biomedical images are noisy. The imaging equipment itself has physical limitations, and the consequent experimental trade-offs between signal-to-noise ratio, acquisition speed, and imaging depth exacerbate the problem. Denoising is, therefore, an essential part of any image processing pipeline, and convolutional neural networks are currently the method of choice for this task. One popular approach, Noise2Noise, does not require clean ground truth, and instead, uses a second noisy copy as a training target. Self-supervised methods, like Noise2Self and Noise2Void, relax data requirements by learning the signal without an explicit target but are limited by the lack of information in a single image. Here, we introduce Noise2Stack, an extension of the Noise2Noise method to image stacks that takes advantage of a shared signal between spatially neighboring planes. Our experiments on magnetic resonance brain scans and newly acquired multiplane microscopy data show that learning only from image neighbors in a stack is sufficient to outperform Noise2Noise and Noise2Void and close the gap to supervised denoising methods. Our findings point towards low-cost, high-reward improvement in the denoising pipeline of multiplane biomedical images. As a part of this work, we release a microscopy dataset to establish a benchmark for the multiplane image denoising.      
### 16.A Low-Complexity Approach for Max-Min Fairness in Uplink Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2011.05076.pdf)
>  We consider the problem of max-min fairness for uplink cell-free massive multiple-input multiple-output which is a potential technology for beyond 5G networks. More specifically, we aim to maximize the minimum spectral efficiency of all users subject to the per-user power constraint, assuming linear receive combining technique at access points. The considered problem can be further divided into two subproblems: the receiver filter coefficient design and the power control problem. While the receiver coefficient design turns out to be a generalized eigenvalue problem, and thus, admits a closed-form solution, the power control problem is numerically troublesome. To solve the power control problem, existing approaches rely on geometric programming (GP) which is not suitable for large-scale systems. To overcome the high-complexity issue of the GP method, we first reformulate the power control problem intro a convex program, and then apply a smoothing technique in combination with an accelerated projected gradient method to solve it. The simulation results demonstrate that the proposed solution can achieve almost the same objective but in much lesser time than the existing GP-based method.      
### 17.Impact of Interference on the Performance of RIS-Assisted Source DF Relaying Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.05070.pdf)
>  This letter investigates the impact of co-channel interference (CCI) on the performance of a decode-and-forward (DF) relaying network with a reconfigurable intelligent surface (RIS)-assisted source. We consider one source, multiple DF relays, and one destination with CCI at both the relays and destination. We derive closed-form accurate approximations for the system outage probability assuming Rayleigh fading channels. In addition, we study the system performance at the high signal-to-noise ratio (SNR) regime, where closed-form expressions are derived for the asymptotic outage probability, diversity order, and the coding gain. The results show that the number of reflecting elements N at the source has a small effect on the coding gain of the system and not on the diversity order. Furthermore, findings illustrate that the number of relays K is affecting the diversity order and is more impactful on the performance than N. Finally, results show that utilizing RIS at the source node mitigates the interference effect at the relay nodes.      
### 18.Enhancing Low-Quality Voice Recordings Using Disentangled Channel Factor and Neural Waveform Model  [ :arrow_down: ](https://arxiv.org/pdf/2011.05038.pdf)
>  High-quality speech corpora are essential foundations for most speech applications. However, such speech data are expensive and limited since they are collected in professional recording environments. In this work, we propose an encoder-decoder neural network to automatically enhance low-quality recordings to professional high-quality recordings. To address channel variability, we first filter out the channel characteristics from the original input audio using the encoder network with adversarial training. Next, we disentangle the channel factor from a reference audio. Conditioned on this factor, an auto-regressive decoder is then used to predict the target-environment Mel spectrogram. Finally, we apply a neural vocoder to synthesize the speech waveform. Experimental results show that the proposed system can generate a professional high-quality speech waveform when setting high-quality audio as the reference. It also improves speech enhancement performance compared with several state-of-the-art baseline systems.      
### 19.Going Below and Beyond, Off-the-Grid Velocity Estimation from 1-bit Radar Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2011.05034.pdf)
>  In this paper we propose to bridge the gap between using extremely low resolution 1-bit measurements and estimating targets' parameters, such as their velocities, that exist in a continuum, i.e., by performing Off-the-Grid estimation. To that end, a Continuous version of Orthogonal Matching Pursuit (COMP) is modified in order to leverage the 1-bit measurements coming from a simple Doppler Radar. Using Monte-Carlo simulations, we show that although the resolution of the acquisition is dramatically reduced, velocity estimation of multiple targets can still be achieved and reaches performances beyond classic On-the-Grid methods. Furthermore, we show empirically that adding a random and uniform dithering before the quantization is necessary when estimating more than one target.      
### 20.Correlated Age-of-Information Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2011.05032.pdf)
>  We consider a system composed of a sensor node tracking a time varying quantity. In every discretized time slot, the node attempts to send an update to a central monitoring station through one of K communication channels. We consider the setting where channel realizations are correlated across channels. This is motivated by mmWave based 5G systems where line-of-sight which is critical for successful communication is common across all frequency channels while the effect of other factors like humidity is frequency dependent. The metric of interest is the Age-of-Information (AoI) which is a measure of the freshness of the data available at the monitoring station. In the setting where channel statistics are unknown but stationary across time and correlated across channels, the algorithmic challenge is to determine which channel to use in each time-slot for communication. We model the problem as a Multi-Armed bandit (MAB) with channels as arms. We characterize the fundamental limits on the performance of any policy. In addition, via analysis and simulations, we characterize the performance of variants of the UCB and Thompson Sampling policies that exploit correlation.      
### 21.Formation Path Following Control of Underactuated USVs -- With Proofs  [ :arrow_down: ](https://arxiv.org/pdf/2011.05021.pdf)
>  This paper proposes a formation control method for two underactuated unmanned surface vessels (USVs) to follow curved paths in the presence of ocean currents. By uniting a line-of-sight (LOS) guidance law and the null-spacebased behavioral control (NSB) framework, we achieve curved path following of the barycenter, while maintaining the desired vessel formation. The closed-loop dynamics are investigated using cascaded systems theory, and it is shown that the closed-loop system is USGES and UGAS, while the underactuated sway dynamics remains bounded. Both simulation and experimental results are presented to verify the theoretical results.      
### 22.AIM 2020 Challenge on Rendering Realistic Bokeh  [ :arrow_down: ](https://arxiv.org/pdf/2011.04988.pdf)
>  This paper reviews the second AIM realistic bokeh effect rendering challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world bokeh simulation problem, where the goal was to learn a realistic shallow focus technique using a large-scale EBB! bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs captured using the Canon 7D DSLR camera. The participants had to render bokeh effect based on only one single frame without any additional data from other cameras or sensors. The target metric used in this challenge combined the runtime and the perceptual quality of the solutions measured in the user study. To ensure the efficiency of the submitted models, we measured their runtime on standard desktop CPUs as well as were running the models on smartphone GPUs. The proposed solutions significantly improved the baseline results, defining the state-of-the-art for practical bokeh effect rendering problem.      
### 23.A Scenario-oriented Approach for Energy-Reserve Joint Procurement and Pricing  [ :arrow_down: ](https://arxiv.org/pdf/2011.04933.pdf)
>  We propose a scenario-oriented approach for energy-reserve joint procurement and pricing for electricity market. In this model, without the empirical reserve requirements, reserve is procured according to all possible contingencies and load/renewable generation fluctuations with the minimum expected system total cost. The innovative locational marginal pricing approach for loads, generations and reserve and the associated settlement process are proposed. We show that payments from loads, payments to generators and congestion rent will reach their balance in the basecase as well as in all scenarios, so that revenue adequacy can be guaranteed for the system operator.      
### 24.Generalized LSTM-based End-to-End Text-Independent Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2011.04896.pdf)
>  The increasing amount of available data and more affordable hardware solutions have opened a gate to the realm of Deep Learning (DL). Due to the rapid advancements and ever-growing popularity of DL, it has begun to invade almost every field, where machine learning is applicable, by altering the traditional state-of-the-art methods. While many researchers in the speaker recognition area have also started to replace the former state-of-the-art methods with DL techniques, some of the traditional i-vector-based methods are still state-of-the-art in the context of text-independent speaker verification (TI-SV). In this paper, we discuss the most recent generalized end-to-end (GE2E) DL technique based on Long Short-term Memory (LSTM) units for TI-SV by Google and compare different scenarios and aspects including utterance duration, training time, and accuracy to prove that our method outperforms the traditional methods.      
### 25.Dynamic Relay Selection and Power Allocation for Minimizing Outage Probability: A Hierarchical Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2011.04891.pdf)
>  Cooperative communication is an effective approach to improve spectrum utilization. When considering relay selection and power allocation in cooperative communication, most of the existing studies require the assumption of channel state information (CSI). However, it is difficult to get an accurate CSI in practice. In this paper, we consider an outage-based method subjected to a total transmission power constraint in the two-hop cooperative communication scenario. We use reinforcement learning (RL) methods to learn strategies, and complete the optimal relay selection and power allocation, which do not need any prior knowledge of CSI but simply rely on the interaction with the communication environment. It is noted that conventional RL methods, including common deep reinforcement learning (DRL) methods, perform poorly when the search space is large. Therefore, we first propose a practical DRL framework with an outage-based reward function, which is used as a baseline. Then, we further propose our novel hierarchical reinforcement learning (HRL) algorithm for dynamic relay selection and power allocation. A key difference from other RL-based methods in existing literatures is that, our HRL approach decomposes relay selection and power allocation into two hierarchical optimization objectives, which are trained in different levels. Simulation results reveal that our HRL algorithm trains faster and obtains a lower outage probability when compared with traditional DRL methods, especially in a sparse reward environment.      
### 26.Spectral Efficiency vs Complexity in Downlink Algorithms for Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2011.04859.pdf)
>  Reconfigurable Intelligent Surfaces (RIS) are an emerging technology that can be used to reconfigure the propagation environment to improve cellular communication link rates. RIS, which are thin metasurfaces composed of discrete elements, passively manipulate incident electromagnetic waves through controlled reflective phase tuning. In this paper, we investigate co-design of the multiantenna basestation beamforming vector and multielement RIS phase shifts. The downlink narrowband transmission uses sub-6 GHz frequency bands, and the user equipment has a single antenna. Subject to the non-convex constraints due to the RIS phase shifts, we maximize the spectral efficiency or equivalent channel power as a proxy. Our contributions in improving RIS-aided links include (1) design of gradient ascent codesign algorithms, and (2) comparison of seven codesign algorithms in spectral efficiency vs. computational complexity. In simulation, the best spectral efficiency vs. computational complexity tradeoffs are shown by two of our proposed gradient ascent algorithms.      
### 27.Optimizing Age of Information Through Aerial Reconfigurable Intelligent Surfaces: A Deep Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2011.04817.pdf)
>  We investigate the benefits of integrating unmanned aerial vehicles (UAVs) with reconfigurable intelligent surface (RIS) elements to passively relay information sampled by Internet of Things devices (IoTDs) to the base station (BS). In order to maintain the freshness of relayed information, an optimization problem with the objective of minimizing the expected sum Age-of-Information (AoI) is formulated to optimize the altitude of the UAV, the communication schedule, and phases-shift of RIS elements. In the absence of prior knowledge of the activation pattern of the IoTDs, proximal policy optimization algorithm is developed to solve this mixed-integer non-convex optimization problem. Numerical results show that our proposed algorithm outperforms all others in terms of AoI.      
### 28.Benchmarking LF-MMI, CTC and RNN-T Criteria for Streaming ASR  [ :arrow_down: ](https://arxiv.org/pdf/2011.04785.pdf)
>  In this work, to measure the accuracy and efficiency for a latency-controlled streaming automatic speech recognition (ASR) application, we perform comprehensive evaluations on three popular training criteria: LF-MMI, CTC and RNN-T. In transcribing social media videos of 7 languages with training data 3K-14K hours, we conduct large-scale controlled experimentation across each criterion using identical datasets and encoder model architecture. We find that RNN-T has consistent wins in ASR accuracy, while CTC models excel at inference efficiency. Moreover, we selectively examine various modeling strategies for different training criteria, including modeling units, encoder architectures, pre-training, etc. Given such large-scale real-world streaming ASR application, to our best knowledge, we present the first comprehensive benchmark on these three widely used training criteria across a great many languages.      
### 29.Thompson sampling for linear quadratic mean-field teams  [ :arrow_down: ](https://arxiv.org/pdf/2011.04686.pdf)
>  We consider optimal control of an unknown multi-agent linear quadratic (LQ) system where the dynamics and the cost are coupled across the agents through the mean-field (i.e., empirical mean) of the states and controls. Directly using single-agent LQ learning algorithms in such models results in regret which increases polynomially with the number of agents. We propose a new Thompson sampling based learning algorithm which exploits the structure of the system model and show that the expected Bayesian regret of our proposed algorithm for a system with agents of $|M|$ different types at time horizon $T$ is $\tilde{\mathcal{O}} \big( |M|^{1.5} \sqrt{T} \big)$ irrespective of the total number of agents, where the $\tilde{\mathcal{O}}$ notation hides logarithmic factors in $T$. We present detailed numerical experiments to illustrate the salient features of the proposed algorithm.      
### 30.A Distributionally Robust Optimization Approach for Unit Commitment in Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2011.05314.pdf)
>  This paper proposes a distributionally robust unit commitment approach for microgrids under net load and electricity market price uncertainty. The key thrust of the proposed approach is to leverage the Kullback-Leibler divergence to construct an ambiguity set of probability distributions and formulate an optimization problem that minimizes the expected costs brought about by the worst-case distribution in the ambiguity set. The proposed approach effectively exploits historical data and capitalizes on the k-means clustering algorithm---in conjunction with the soft dynamic time warping score---to form the nominal probability distribution and its associated support. A two-level decomposition method is developed to enable the efficient solution of the devised problem. We carry out representative studies and quantify the relative merits of the proposed approach vis-Ã -vis a stochastic optimization-based model under different divergence tolerance values.      
### 31.Understanding the physics of coherent LiDAR  [ :arrow_down: ](https://arxiv.org/pdf/2011.05313.pdf)
>  Coherent LiDAR (Light Detecting And Ranging) is a promising 3D imaging technology that provides significant advantages over more traditional LiDAR systems. In addition to being immune to ambient light, it directly measures the velocity of moving objects by sensing Doppler shift of light, and can achieve exceptional depth accuracies. The goal of this manuscript is to explain the basic physics of coherent LiDAR with rigorous derivations from first principles. We first discuss the sensitivity of coherent detection, and derive the number of photons needed to robustly detect a LiDAR return. We then turn our attention to the collection efficiency of coherent LiDAR, and show that signal strength is strongly dependent upon how well the laser beams are focused.      
### 32.Perception Improvement for Free: Exploring Imperceptible Black-box Adversarial Attacks on Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2011.05254.pdf)
>  Deep neural networks are vulnerable to adversarial attacks. White-box adversarial attacks can fool neural networks with small adversarial perturbations, especially for large size images. However, keeping successful adversarial perturbations imperceptible is especially challenging for transfer-based black-box adversarial attacks. Often such adversarial examples can be easily spotted due to their unpleasantly poor visual qualities, which compromises the threat of adversarial attacks in practice. In this study, to improve the image quality of black-box adversarial examples perceptually, we propose structure-aware adversarial attacks by generating adversarial images based on psychological perceptual models. Specifically, we allow higher perturbations on perceptually insignificant regions, while assigning lower or no perturbation on visually sensitive regions. In addition to the proposed spatial-constrained adversarial perturbations, we also propose a novel structure-aware frequency adversarial attack method in the discrete cosine transform (DCT) domain. Since the proposed attacks are independent of the gradient estimation, they can be directly incorporated with existing gradient-based attacks. Experimental results show that, with the comparable attack success rate (ASR), the proposed methods can produce adversarial examples with considerably improved visual quality for free. With the comparable perceptual quality, the proposed approaches achieve higher attack success rates: particularly for the frequency structure-aware attacks, the average ASR improves more than 10% over the baseline attacks.      
### 33.VFH+ based shared control for remotely operated mobile robots  [ :arrow_down: ](https://arxiv.org/pdf/2011.05228.pdf)
>  This paper addresses the problem of safe and efficient navigation in remotely controlled robots operating in hazardous and unstructured environments; or conducting other remote robotic tasks. A shared control method is presented which blends the commands from a VFH+ obstacle avoidance navigation module with the teleoperation commands provided by an operator via a joypad. The presented approach offers several advantages such as flexibility allowing for a straightforward adaptation of the controller's behaviour and easy integration with variable autonomy systems; as well as the ability to cope with dynamic environments. The advantages of the presented controller are demonstrated by an experimental evaluation in a disaster response scenario. More specifically, presented evidence show a clear performance increase in terms of safety and task completion time compared to a pure teleoperation approach, as well as an ability to cope with previously unobserved obstacles.      
### 34.Supervised attention for speaker recognition  [ :arrow_down: ](https://arxiv.org/pdf/2011.05189.pdf)
>  The recently proposed self-attentive pooling (SAP) has shown good performance in several speaker recognition systems. In SAP systems, the context vector is trained end-to-end together with the feature extractor, where the role of context vector is to select the most discriminative frames for speaker recognition. However, the SAP underperforms compared to the temporal average pooling (TAP) baseline in some settings, which implies that the attention is not learnt effectively in end-to-end training. To tackle this problem, we introduce strategies for training the attention mechanism in a supervised manner, which learns the context vector using classified samples. With our proposed methods, context vector can be boosted to select the most informative frames. We show that our method outperforms existing methods in various experimental settings including short utterance speaker recognition, and achieves competitive performance over the existing baselines on the VoxCeleb datasets.      
### 35.Quantitative imaging for complex-objects via a single-pixel detector  [ :arrow_down: ](https://arxiv.org/pdf/2011.05182.pdf)
>  Quantitative phase imaging (QPI) is important in many applications such as microscopy and crystallography. To quantitatively reveal phase information, people could either employ interference to map phase distribution into intensity fringes, or analyze intensity-only diffraction patterns through phase retrieval algorithms. Traditionally, both of these two ways use pixelated detectors. In this work, a novel QPI scheme is reported inspired by single-pixel camera (SPC), which adopts the principle of SPC that retrieves images through structured illumination and corresponding single-pixel signals. Particularly for complex-valued imaging, the structured illumination is performed in the phase domain, and a point detector with restricted sensor size detects the intensity of zero-frequency area. Based on the illumination structures and point signals, a complex image is reconstructed by running a phase retrieval algorithm. This approach is universal for various wavelengths, and needs no a priori information of the targets. Both simulation and experiment show that our single-pixel QPI scheme exhibits great performance even with objects in an extremely rough phase distribution.      
### 36.GANterpretations  [ :arrow_down: ](https://arxiv.org/pdf/2011.05158.pdf)
>  Since the introduction of Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] there has been a regular stream of both technical advances (e.g., Arjovsky et al. [2017]) and creative uses of these generative models (e.g., [Karras et al., 2019, Zhu et al., 2017, Jin et al., 2017]). In this work we propose an approach for using the power of GANs to automatically generate videos to accompany audio recordings by aligning to spectral properties of the recording. This allows musicians to explore new forms of multi-modal creative expression, where musical performance can induce an AI-generated musical video that is guided by said performance, as well as a medium for creating a visual narrative to follow a storyline (similar to what was proposed by Frosst and Kereliuk [2019]).      
### 37.Multi-modal, multi-task, multi-attention (M3) deep learning detection of reticular pseudodrusen: 1 towards automated and accessible classification of age-related macular degeneration  [ :arrow_down: ](https://arxiv.org/pdf/2011.05142.pdf)
>  Objective Reticular pseudodrusen (RPD), a key feature of age-related macular degeneration (AMD), are poorly detected by human experts on standard color fundus photography (CFP) and typically require advanced imaging modalities such as fundus autofluorescence (FAF). The objective was to develop and evaluate the performance of a novel 'M3' deep learning framework on RPD detection. Materials and Methods A deep learning framework M3 was developed to detect RPD presence accurately using CFP alone, FAF alone, or both, employing &gt;8000 CFP-FAF image pairs obtained prospectively (Age-Related Eye Disease Study 2). The M3 framework includes multi-modal (detection from single or multiple image modalities), multi-task (training different tasks simultaneously to improve generalizability), and multi-attention (improving ensembled feature representation) operation. Performance on RPD detection was compared with state-of-the-art deep learning models and 13 ophthalmologists; performance on detection of two other AMD features (geographic atrophy and pigmentary abnormalities) was also evaluated. Results For RPD detection, M3 achieved area under receiver operating characteristic (AUROC) 0.832, 0.931, and 0.933 for CFP alone, FAF alone, and both, respectively. M3 performance on CFP was very substantially superior to human retinal specialists (median F1-score 0.644 versus 0.350). External validation (on Rotterdam Study, Netherlands) demonstrated high accuracy on CFP alone (AUROC 0.965). The M3 framework also accurately detected geographic atrophy and pigmentary abnormalities (AUROC 0.909 and 0.912, respectively), demonstrating its generalizability. Conclusion This study demonstrates the successful development, robust evaluation, and external validation of a novel deep learning framework that enables accessible, accurate, and automated AMD diagnosis and prognosis.      
### 38.Feedback-Based Dynamic Feature Selection for Constrained Continuous Data Acquisition  [ :arrow_down: ](https://arxiv.org/pdf/2011.05112.pdf)
>  Relevant and high-quality data are critical to successful development of machine learning applications. For machine learning applications on dynamic systems equipped with a large number of sensors, such as connected vehicles and robots, how to find relevant and high-quality data features in an efficient way is a challenging problem. In this work, we address the problem of feature selection in constrained continuous data acquisition. We propose a feedback-based dynamic feature selection algorithm that efficiently decides on the feature set for data collection from a dynamic system in a step-wise manner. We formulate the sequential feature selection procedure as a Markov Decision Process. The machine learning model performance feedback with an exploration component is used as the reward function in an $\epsilon$-greedy action selection. Our evaluation shows that the proposed feedback-based feature selection algorithm has superior performance over constrained baseline methods and matching performance with unconstrained baseline methods.      
### 39.Model Predictive Control for Human-Centred Lower Limb Robotic Assistance  [ :arrow_down: ](https://arxiv.org/pdf/2011.05079.pdf)
>  Loss of mobility or balance resulting from neural trauma is a critical consideration in public health. Robotic exoskeletons hold great potential for rehabilitation and assisted movement, yet optimal assist-as-needed (AAN) control remains unresolved given pathological variance among patients. We introduce a model predictive control (MPC) architecture for lower limb exoskeletons centred around a fuzzy logic algorithm (FLA) identifying modes of assistance based on human involvement. Assistance modes are: 1) passive for human relaxed and robot dominant, 2) active-assist for human cooperation with the task, and 3) safety in the case of human resistance to the robot. Human torque is estimated from electromyography (EMG) signals prior to joint motions, enabling advanced prediction of torque by the MPC and selection of assistance mode by the FLA. The controller is demonstrated in hardware with three subjects on a 1-DOF knee exoskeleton tracking a sinusoidal trajectory with human relaxed assistive, and resistive. Experimental results show quick and appropriate transfers among the assistance modes and satisfied assistive performance in each mode. Results illustrate an objective approach to lower limb robotic assistance through on-the-fly transition between modes of movement, providing a new level of human-robot synergy for mobility assist and rehabilitation.      
### 40.Neural Networks Optimally Compress the Sawbridge  [ :arrow_down: ](https://arxiv.org/pdf/2011.05065.pdf)
>  Neural-network-based compressors have proven to be remarkably effective at compressing sources, such as images, that are nominally high-dimensional but presumed to be concentrated on a low-dimensional manifold. We consider a continuous-time random process that models an extreme version of such a source, wherein the realizations fall along a one-dimensional "curve" in function space that has infinite-dimensional linear span. We precisely characterize the optimal entropy-distortion tradeoff for this source and show numerically that it is achieved by neural-network-based compressors trained via stochastic gradient descent. In contrast, we show both analytically and experimentally that compressors based on the classical Karhunen-LoÃ¨ve transform are highly suboptimal at high rates.      
### 41.Federated Learning via Intelligent Reflecting Surface  [ :arrow_down: ](https://arxiv.org/pdf/2011.05051.pdf)
>  Over-the-air computation (AirComp) based federated learning (FL) is capable of achieving fast model aggregation by exploiting the waveform superposition property of multiple access channels. However, the model aggregation performance is severely limited by the unfavorable wireless propagation channels. In this paper, we propose to leverage intelligent reflecting surface (IRS) to achieve fast yet reliable model aggregation for AirComp-based FL. To optimize the learning performance, we formulate an optimization problem that jointly optimizes the device selection, the aggregation beamformer at the base station (BS), and the phase shifts at the IRS to maximize the number of devices participating in the model aggregation of each communication round under certain mean-squared-error (MSE) requirements. To tackle the formulated highly-intractable problem, we propose a two-step optimization framework. Specifically, we induce the sparsity of device selection in the first step, followed by solving a series of MSE minimization problems to find the maximum feasible device set in the second step. We then propose an alternating optimization framework, supported by the difference-of-convex-functions programming algorithm for low-rank optimization, to efficiently design the aggregation beamformers at the BS and phase shifts at the IRS. Simulation results will demonstrate that our proposed algorithm and the deployment of an IRS can achieve a lower training loss and higher FL prediction accuracy.      
### 42.On the Downlink Performance of RSMA-based UAV Communications  [ :arrow_down: ](https://arxiv.org/pdf/2011.05019.pdf)
>  The use of unmanned aerial vehicles (UAVs) as base stations (BSs) is envisaged as a key enabler for the fifth generation (5G) and beyond-5G networks. Specifically, aerial base stations (UAV-BS) are expected to provide ubiquitous connectivity and high spectral efficiency. To this end, we present in this correspondence an in-depth look into the integration of rate-splitting multiple access (RSMA) with UAV-BSs and downlink transmissions. A non-convex problem of joint UAV placement, RSMA precoding, and rate splitting, aiming to maximize the weighted sum data rate of users is formulated. Due to its complexity, two sub-problems are investigated, namely the UAV placement and RSMA parameters optimization. The resulting solutions are then combined to propose a novel alternating optimization method. Simulation results illustrate the latter's efficiency compared to baseline approaches.      
### 43.Tattoo tomography: Freehand 3D photoacoustic image reconstruction with an optical pattern  [ :arrow_down: ](https://arxiv.org/pdf/2011.04997.pdf)
>  Purpose: Photoacoustic tomography (PAT) is a novel imaging technique that can spatially resolve both morphological and functional tissue properties, such as the vessel topology and tissue oxygenation. While this capacity makes PAT a promising modality for the diagnosis, treatment and follow-up of various diseases, a current drawback is the limited field-of-view (FoV) provided by the conventionally applied 2D probes. <br>Methods: In this paper, we present a novel approach to 3D reconstruction of PAT data (Tattoo tomography) that does not require an external tracking system and can smoothly be integrated into clinical workflows. It is based on an optical pattern placed on the region of interest prior to image acquisition. This pattern is designed in a way that a tomographic image of it enables the recovery of the probe pose relative to the coordinate system of the pattern. This allows the transformation of a sequence of acquired PA images into one common global coordinate system and thus the consistent 3D reconstruction of PAT imaging data. <br>Results: An initial feasibility study conducted with experimental phantom data and in vivo forearm data indicates that the Tattoo approach is well-suited for 3D reconstruction of PAT data with high accuracy and precision. <br>Conclusion: In contrast to previous approaches to 3D ultrasound (US) or PAT reconstruction, the Tattoo approach neither requires complex external hardware nor training data acquired for a specific application. It could thus become a valuable tool for clinical freehand PAT.      
### 44.AIM 2020 Challenge on Learned Image Signal Processing Pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2011.04994.pdf)
>  This paper reviews the second AIM learned ISP challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world RAW-to-RGB mapping problem, where to goal was to map the original low-quality RAW images captured by the Huawei P20 device to the same photos obtained with the Canon 5D DSLR camera. The considered task embraced a number of complex computer vision subtasks, such as image demosaicing, denoising, white balancing, color and contrast correction, demoireing, etc. The target metric used in this challenge combined fidelity scores (PSNR and SSIM) with solutions' perceptual results measured in a user study. The proposed solutions significantly improved the baseline results, defining the state-of-the-art for practical image signal processing pipeline modeling.      
### 45.Conceptual Compression via Deep Structure and Texture Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2011.04976.pdf)
>  Existing compression methods typically focus on the removal of signal-level redundancies, while the potential and versatility of decomposing visual data into compact conceptual components still lack further study. To this end, we propose a novel conceptual compression framework that encodes visual data into compact structure and texture representations, then decodes in a deep synthesis fashion, aiming to achieve better visual reconstruction quality, flexible content manipulation, and potential support for various vision tasks. In particular, we propose to compress images by a dual-layered model consisting of two complementary visual features: 1) structure layer represented by structural maps and 2) texture layer characterized by low-dimensional deep representations. At the encoder side, the structural maps and texture representations are individually extracted and compressed, generating the compact, interpretable, inter-operable bitstreams. During the decoding stage, a hierarchical fusion GAN (HF-GAN) is proposed to learn the synthesis paradigm where the textures are rendered into the decoded structural maps, leading to high-quality reconstruction with remarkable visual realism. Extensive experiments on diverse images have demonstrated the superiority of our framework with lower bitrates, higher reconstruction quality, and increased versatility towards visual analysis and content manipulation tasks.      
### 46.Deconstruct and Reconstruct Dizi Music of the Northern School and the Southern School  [ :arrow_down: ](https://arxiv.org/pdf/2011.04974.pdf)
>  Today's research on Chinese music technology is mainly focused on three aspects: data collection, music deconstruction, and music reconstruction. In this paper, a general method is proposed to collect Chinese music in the form of numbered musical notation, and a Dizi dataset is collected using the method. Based on the collected Dizi dataset, we conduct research on the Dizi music style of the Northern school and the Southern School. Characteristics include melody and playing techniques of the two different music styles are deconstructed. A reconstruction example, music style transfer which includes melody transfer and playing techniques transfer is given and subjective evaluation is done to evaluate the reconstruction results.      
### 47.Model-based Reinforcement Learning from Signal Temporal Logic Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2011.04950.pdf)
>  Techniques based on Reinforcement Learning (RL) are increasingly being used to design control policies for robotic systems. RL fundamentally relies on state-based reward functions to encode desired behavior of the robot and bad reward functions are prone to exploitation by the learning agent, leading to behavior that is undesirable in the best case and critically dangerous in the worst. On the other hand, designing good reward functions for complex tasks is a challenging problem. In this paper, we propose expressing desired high-level robot behavior using a formal specification language known as Signal Temporal Logic (STL) as an alternative to reward/cost functions. We use STL specifications in conjunction with model-based learning to design model predictive controllers that try to optimize the satisfaction of the STL specification over a finite time horizon. The proposed algorithm is empirically evaluated on simulations of robotic system such as a pick-and-place robotic arm, and adaptive cruise control for autonomous vehicles.      
### 48.An End-to-End Differentiable but Explainable Physics Engine for Tensegrity Robots: Modeling and Control  [ :arrow_down: ](https://arxiv.org/pdf/2011.04929.pdf)
>  This work proposes an end-to-end differentiable physics engine for tensegrity robots, which introduces a data-efficient linear contact model for accurately predicting collision responses that arise due to contacting surfaces, and a linear actuator model that can drive these robots by expanding and contracting their flexible cables. To the best of the authors' knowledge, this is the \emph{first} differentiable physics engine for tensegrity robots that supports cable modeling, contact, and actuation. This engine can be used inside an off-the-shelf, RL-based locomotion controller in order to provide training examples. This paper proposes a progressive training pipeline for the differentiable physics engine that helps avoid local optima during the training phase and reduces data requirements. It demonstrates the data-efficiency benefits of using the differentiable engine for learning locomotion policies for NASA's icosahedron SUPERballBot. In particular, after the engine has been trained with few trajectories to match a ground truth simulated model, then a policy learned on the differentiable engine is shown to be transferable back to the ground-truth model. Training the controller requires orders of magnitude more data than training the differential engine.      
### 49.On the Usefulness of Self-Attention for Automatic Speech Recognition with Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2011.04906.pdf)
>  Self-attention models such as Transformers, which can capture temporal relationships without being limited by the distance between events, have given competitive speech recognition results. However, we note the range of the learned context increases from the lower to upper self-attention layers, whilst acoustic events often happen within short time spans in a left-to-right order. This leads to a question: for speech recognition, is a global view of the entire sequence useful for the upper self-attention encoder layers in Transformers? To investigate this, we train models with lower self-attention/upper feed-forward layers encoders on Wall Street Journal and Switchboard. Compared to baseline Transformers, no performance drop but minor gains are observed. We further developed a novel metric of the diagonality of attention matrices and found the learned diagonality indeed increases from the lower to upper encoder self-attention layers. We conclude the global view is unnecessary in training upper encoder layers.      
### 50.Multi-Agent Active Search using Realistic Depth-Aware Noise Model  [ :arrow_down: ](https://arxiv.org/pdf/2011.04825.pdf)
>  The search for objects of interest in an unknown environment by making data-collection decisions (i.e., active search or active sensing) has robotics applications in many fields, including the search and rescue of human survivors following disasters, detecting gas leaks or locating and preventing animal poachers. Existing algorithms often prioritize the location accuracy of objects of interest while other practical issues such as the reliability of object detection as a function of distance and lines of sight remain largely ignored. An additional challenge is that in many active search scenarios, communication infrastructure may be damaged, unreliable, or unestablished, making centralized control of multiple search agents impractical. We present an algorithm called Noise-Aware Thompson Sampling (NATS) that addresses these issues for multiple ground-based robot agents performing active search considering two sources of sensory information from monocular optical imagery and sonar tracking. NATS utilizes communications between robot agents in a decentralized manner that is robust to intermittent loss of communication links. Additionally, it takes into account object detection uncertainty from depth as well as environmental occlusions. Using simulation results, we show that NATS significantly outperforms existing methods such as information-greedy policies or exhaustive search. We demonstrate the real-world viability of NATS using a photo-realistic environment created in the Unreal Engine 4 game development platform with the AirSim plugin.      
### 51.Encoding Defensive Driving as a Dynamic Nash Game  [ :arrow_down: ](https://arxiv.org/pdf/2011.04815.pdf)
>  Robots deployed in real-world environments should operate safely in a robust manner. In scenarios where an "ego" agent navigates in an environment with multiple other "non-ego" agents, two modes of safety are commonly proposed: adversarial robustness and probabilistic constraint satisfaction. However, while the former is generally computationally-intractable and leads to overconservative solutions, the latter typically relies on strong distributional assumptions and ignores strategic coupling between agents. To avoid these drawbacks, we present a novel formulation of robustness within the framework of general sum dynamic game theory, modeled on defensive driving. More precisely, we inject the ego's cost function with an adversarial phase, a time interval during which other agents are assumed to be temporarily distracted, to robustify the ego agent's trajectory against other agents' potentially dangerous behavior during this time. We demonstrate the effectiveness of our new formulation in encoding safety via multiple traffic scenarios.      
### 52.Similarity-Based Clustering for Enhancing Image Classification Architectures  [ :arrow_down: ](https://arxiv.org/pdf/2011.04728.pdf)
>  Convolutional networks are at the center of best in class computer vision applications for a wide assortment of undertakings. Since 2014, profound amount of work began to make better convolutional architectures, yielding generous additions in different benchmarks. Albeit expanded model size and computational cost will, in general, mean prompt quality increases for most undertakings but, the architectures now need to have some additional information to increase the performance. We show empirical evidence that with the amalgamation of content-based image similarity and deep learning models, we can provide the flow of information which can be used in making clustered learning possible. We show how parallel training of sub-dataset clusters not only reduces the cost of computation but also increases the benchmark accuracies by 5-11 percent.      
### 53.Real-time Locational Marginal Price Forecasting Using Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2011.04717.pdf)
>  In this paper, we propose a model-free unsupervised learning approach to forecast real-time locational marginal prices (RTLMPs) in wholesale electricity markets. By organizing system-wide hourly RTLMP data into a 3-dimensional (3D) tensor consisting of a series of time-indexed matrices, we formulate the RTLMP forecasting problem as a problem of generating the next matrix with forecasted RTLMPs given the historical RTLMP tensor, and propose a generative adversarial network (GAN) model to forecast RTLMPs. The proposed formulation preserves the spatio-temporal correlations among system-wide RTLMPs in the format of historical RTLMP tensor. The proposed GAN model learns the spatio-temporal correlations using the historical RTLMP tensors and generate RTLMPs that are statistically similar and temporally coherent to the historical RTLMP tensor. The proposed approach forecasts system-wide RTLMPs using only publicly available historical price data, without involving confidential information of system model, such as system parameters, topology, or operating conditions. The effectiveness of the proposed approach is verified through case studies using historical RTLMP data in Southwest Power Pool (SPP).      
### 54.Speaker De-identification System using Autoencodersand Adversarial Training  [ :arrow_down: ](https://arxiv.org/pdf/2011.04696.pdf)
>  The fast increase of web services and mobile apps, which collect personal data from users, increases the risk that their privacy may be severely compromised. In particular, the increasing variety of spoken language interfaces and voice assistants empowered by the vertiginous breakthroughs in Deep Learning are prompting important concerns in the European Union to preserve speech data privacy. For instance, an attacker can record speech from users and impersonate them to get access to systems requiring voice identification. Hacking speaker profiles from users is also possible by means of existing technology to extract speaker, linguistic (e.g., dialect) and paralinguistic features (e.g., age) from the speech signal. In order to mitigate these weaknesses, in this paper, we propose a speaker de-identification system based on adversarial training and autoencoders in order to suppress speaker, gender, and accent information from speech. Experimental results show that combining adversarial learning and autoencoders increase the equal error rate of a speaker verification system while preserving the intelligibility of the anonymized spoken content.      
### 55.Impedance Optimization for Uncertain Contact Interactions Through Risk Sensitive Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2011.04684.pdf)
>  This paper addresses the problem of computing optimal impedance schedules for legged locomotion tasks involving complex contact interactions. We formulate the problem of impedance regulation as a trade-off between disturbance rejection and measurement uncertainty. We extend a stochastic optimal control algorithm known as Risk Sensitive Control to take into account measurement uncertainty and propose a formal way to include such uncertainty for unknown contact locations. The approach can efficiently generate optimal state and control trajectories along with local feedback control gains, i.e. impedance schedules. Extensive simulations demonstrate the capabilities of the approach in generating meaningful stiffness and damping modulation patterns before and after contact interaction. For example, contact forces are reduced during early contacts, damping increases to anticipate a high impact event and tracking is automatically traded-off for increased stability. In particular, we show a significant improvement in performance during jumping and trotting tasks with a simulated quadruped robot.      
