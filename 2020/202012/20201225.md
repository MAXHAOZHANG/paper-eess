# ArXiv eess --Fri, 25 Dec 2020
### 1.Spatio-temporal Multi-task Learning for Cardiac MRI Left Ventricle Quantification  [ :arrow_down: ](https://arxiv.org/pdf/2012.13364.pdf)
>  Quantitative assessment of cardiac left ventricle (LV) morphology is essential to assess cardiac function and improve the diagnosis of different cardiovascular diseases. In current clinical practice, LV quantification depends on the measurement of myocardial shape indices, which is usually achieved by manual contouring of the endo- and epicardial. However, this process subjected to inter and intra-observer variability, and it is a time-consuming and tedious task. In this paper, we propose a spatio-temporal multi-task learning approach to obtain a complete set of measurements quantifying cardiac LV morphology, regional-wall thickness (RWT), and additionally detecting the cardiac phase cycle (systole and diastole) for a given 3D Cine-magnetic resonance (MR) image sequence. We first segment cardiac LVs using an encoder-decoder network and then introduce a multitask framework to regress 11 LV indices and classify the cardiac phase, as parallel tasks during model optimization. The proposed deep learning model is based on the 3D spatio-temporal convolutions, which extract spatial and temporal features from MR images. We demonstrate the efficacy of the proposed method using cine-MR sequences of 145 subjects and comparing the performance with other state-of-the-art quantification methods. The proposed method obtained high prediction accuracy, with an average mean absolute error (MAE) of 129 $mm^2$, 1.23 $mm$, 1.76 $mm$, Pearson correlation coefficient (PCC) of 96.4%, 87.2%, and 97.5% for LV and myocardium (Myo) cavity regions, 6 RWTs, 3 LV dimensions, and an error rate of 9.0\% for phase classification. The experimental results highlight the robustness of the proposed method, despite varying degrees of cardiac morphology, image appearance, and low contrast in the cardiac MR sequences.      
### 2.Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast  [ :arrow_down: ](https://arxiv.org/pdf/2012.13340.pdf)
>  Most existing algorithms for automatic 3D morphometry of human brain MRI scans are designed for data with near-isotropic voxels at approximately 1 mm resolution, and frequently have contrast constraints as well - typically requiring T1 scans (e.g., MP-RAGE). This limitation prevents the analysis of millions of MRI scans acquired with large inter-slice spacing ("thick slice") in clinical settings every year. The inability to quantitatively analyze these scans hinders the adoption of quantitative neuroimaging in healthcare, and precludes research studies that could attain huge sample sizes and hence greatly improve our understanding of the human brain. Recent advances in CNNs are producing outstanding results in super-resolution and contrast synthesis of MRI. However, these approaches are very sensitive to the contrast, resolution and orientation of the input images, and thus do not generalize to diverse clinical acquisition protocols - even within sites. Here we present SynthSR, a method to train a CNN that receives one or more thick-slice scans with different contrast, resolution and orientation, and produces an isotropic scan of canonical contrast (typically a 1 mm MP-RAGE). The presented method does not require any preprocessing, e.g., skull stripping or bias field correction. Crucially, SynthSR trains on synthetic input images generated from 3D segmentations, and can thus be used to train CNNs for any combination of contrasts, resolutions and orientations without high-resolution training data. We test the images generated with SynthSR in an array of common downstream analyses, and show that they can be reliably used for subcortical segmentation and volumetry, image registration (e.g., for tensor-based morphometry), and, if some image quality requirements are met, even cortical thickness morphometry. The source code is publicly available at <a class="link-external link-http" href="http://github.com/BBillot/SynthSR" rel="external noopener nofollow">this http URL</a>.      
### 3.LEUGAN:Low-Light Image Enhancement by Unsupervised Generative Attentional Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.13322.pdf)
>  Restoring images from low-light data is a challenging problem. Most existing deep-network based algorithms are designed to be trained with pairwise images. Due to the lack of real-world datasets, they usually perform poorly when generalized in practice in terms of loss of image edge and color information. In this paper, we propose an unsupervised generation network with attention-guidance to handle the low-light image enhancement task. Specifically, our network contains two parts: an edge auxiliary module that restores sharper edges and an attention guidance module that recovers more realistic colors. Moreover, we propose a novel loss function to make the edges of the generated images more visible. Experiments validate that our proposed algorithm performs favorably against state-of-the-art methods, especially for real-world images in terms of image clarity and noise control.      
### 4.Blind Demixing of Diffused Graph Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.13301.pdf)
>  Using graphs to model irregular information domains is an effective approach to deal with some of the intricacies of contemporary (network) data. A key aspect is how the data, represented as graph signals, depend on the topology of the graph. Widely-used approaches assume that the observed signals can be viewed as outputs of graph filters (i.e., polynomials of a matrix representation of the graph) whose inputs have a particular structure. Diffused graph signals, which correspond to an originally sparse (node-localized) signal percolated through the graph via filtering, fall into this class. In that context, this paper deals with the problem of jointly identifying graph filters and separating their (sparse) input signals from a mixture of diffused graph signals, thus generalizing to the graph signal processing framework the classical blind demixing (blind source separation) of temporal and spatial signals. We first consider the scenario where the supporting graphs are different across the signals, providing a theorem for demixing feasibility along with probabilistic bounds on successful recovery. Additionally, an analysis of the degenerate problem of demixing with a single graph is also presented. Numerical experiments with synthetic and real-world graphs empirically illustrating the main theoretical findings close the paper.      
### 5.Real-time control of water reservoir operations: a learning-based hierarchical approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.13224.pdf)
>  Hydropower dams represent a way to combat fossil fuel consumption, while also providing irrigation and urban water supply. The optimal control of a water reservoir operations still represents a challenging problem, due to uncertain hydrologic conditions and the need to adapt to changing environment and varying control objectives. In this work, we propose a real-time learning-based control strategy based on a hierarchical predictive control architecture. More specifically, two control loops are considered: the inner loop is aimed to make the overall dynamics similar to an assigned linear one, then the outer economic model-predictive controller compensates for model mismatches, enforces suitable constraints and boosts the tracking performance. The effectiveness of the proposed approach as compared to traditional dynamic programming strategies is illustrated on an accurate simulator of the Hoa Binh reservoir in Vietnam.      
### 6.UMLE: Unsupervised Multi-discriminator Network for Low Light Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2012.13177.pdf)
>  Low-light image enhancement, such as recovering color and texture details from low-light images, is a complex and vital task. For automated driving, low-light scenarios will have serious implications for vision-based applications. To address this problem, we propose a real-time unsupervised generative adversarial network (GAN) containing multiple discriminators, i.e. a multi-scale discriminator, a texture discriminator, and a color discriminator. These distinct discriminators allow the evaluation of images from different perspectives. Further, considering that different channel features contain different information and the illumination is uneven in the image, we propose a feature fusion attention module. This module combines channel attention with pixel attention mechanisms to extract image features. Additionally, to reduce training time, we adopt a shared encoder for the generator and the discriminator. This makes the structure of the model more compact and the training more stable. Experiments indicate that our method is superior to the state-of-the-art methods in qualitative and quantitative evaluations, and significant improvements are achieved for both autopilot positioning and detection results.      
### 7.LTE Cell Load Estimation Based on DCI Message Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2012.13160.pdf)
>  The modern mobile communication systems, the task of analyzing the parameters of quality of service and network information load remains one of the most popular. There are several ways getting this information in LTE networks. For example, by evaluation of test signals or estimation special reference signals in LTE frame. In this paper, it is proposed to obtain information about the cell load by decoding service messages of the DCI control information block. The main idea of this method is that the information load of LTE cell is estimated by decoding DCI messages in the LTE physical control channel and establishing the number of unique identifiers. For decoding the LTE signal, was used MATLAB LTE Toolbox and software-defined radio platform USRP 2920. In the work, the UE detection algorithm was tested on the signal generated using MATLAB software, and evaluated the information load of the network of a real LTE base station.      
### 8.Resilient Self/Event-Triggered Consensus Based on Ternary Control  [ :arrow_down: ](https://arxiv.org/pdf/2012.13158.pdf)
>  The paper considers the problem of multi-agent consensus in the presence of adversarial agents which may try to prevent and introduce undesired influence on the coordination among the regular agents. To our setting, we extend the so-called mean subsequence reduced algorithms with the aim to reduce the amount of communication via two measures: The agents exchange information in the form of ternary data at each transmission and moreover keep the frequency of data exchange low by employing self- and event-triggered communication. We will observe that in hostile environments with adversaries, the self-triggered approach can bring certain advantages over the event-triggered counterpart.      
### 9.Distributed Fusion Estimation for Stochastic Uncertain Systems with Network-Induced Complexity and Multiple Noise  [ :arrow_down: ](https://arxiv.org/pdf/2012.13155.pdf)
>  This paper investigates an issue of distributed fusion estimation under network-induced complexity and stochastic parameter uncertainties. First, a novel signal selection method based on event-trigger is developed to handle network-induced packet dropouts as well as packet disorders resulting from random transmission delays, where the ${H_2}/{H_\infty }$ performance of the system is analyzed in different noise environments. In addition, a linear delay compensation strategy is further employed for solving the complexity network-induced problem, which may deteriorate the system performance. Moreover, a weighted fusion scheme is used to integrate multiple resources through an error cross-covariance matrix. Several case studies validate the proposed algorithm and demonstrate satisfactory system performance in target tracking.      
### 10.Wheel-Rail Interface Condition Estimation (W-RICE)  [ :arrow_down: ](https://arxiv.org/pdf/2012.13096.pdf)
>  The surface roughness between the wheel and rail has a huge influence on rolling noise level. The presence of the third body such as frost or grease at wheel-rail interface contributes towards change in adhesion coefficient resulting in the generation of noise at various levels. Therefore, it is possible to estimate adhesion conditions between the wheel and rail from the analysis of noise patterns originating from wheel-rail interaction. In this study, a new approach to estimate adhesion condition is proposed which takes rolling noise as input.      
### 11.A Plug-and-Play Priors Framework for Hyperspectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2012.13074.pdf)
>  Spectral unmixing is a widely used technique in hyperspectral image processing and analysis. It aims to separate mixed pixels into the component materials and their corresponding abundances. Early solutions to spectral unmixing are performed independently on each pixel. Nowadays, investigating proper priors into the unmixing problem has been popular as it can significantly enhance the unmixing performance. However, it is non-trivial to handcraft a powerful regularizer, and complex regularizers may introduce extra difficulties in solving optimization problems in which they are involved. To address this issue, we present a plug-and-play (PnP) priors framework for hyperspectral unmixing. More specifically, we use the alternating direction method of multipliers (ADMM) to decompose the optimization problem into two iterative subproblems. One is a regular optimization problem depending on the forward model, and the other is a proximity operator related to the prior model and can be regarded as an image denoising problem. Our framework is flexible and extendable which allows a wide range of denoisers to replace prior models and avoids handcrafting regularizers. Experiments conducted on both synthetic data and real airborne data illustrate the superiority of the proposed strategy compared with other state-of-the-art hyperspectral unmixing methods.      
### 12.White matter hyperintensities volume and cognition: Assessment of a deep learning based lesion detection and quantification algorithm on the Alzheimers Disease Neuroimaging Initiative  [ :arrow_down: ](https://arxiv.org/pdf/2012.13059.pdf)
>  The relationship between cognition and white matter hyperintensities (WMH) volumes often depends on the accuracy of the lesion segmentation algorithm used. As such, accurate detection and quantification of WMH is of great interest. Here, we use a deep learning-based WMH segmentation algorithm, StackGen-Net, to detect and quantify WMH on 3D FLAIR volumes from ADNI. We used a subset of subjects (n=20) and obtained manual WMH segmentations by an experienced neuro-radiologist to demonstrate the accuracy of our algorithm. On a larger cohort of subjects (n=290), we observed that larger WMH volumes correlated with worse performance on executive function (P=.004), memory (P=.01), and language (P=.005).      
### 13.Stochastic Pre-Event Preparation for Enhancing Resilience of Distribution Systems with High DER Penetration  [ :arrow_down: ](https://arxiv.org/pdf/2012.13043.pdf)
>  This paper proposes a stochastic optimal preparation and resource allocation method for upcoming extreme weather events in distribution systems, which can assist utilities to achieve faster and more efficient post-event restoration. With the objective of maximizing served load and minimizing operation cost, this paper develops a two-stage stochastic mixed-integer linear programming (SMILP) model. The first-stage determines the optimal positions and numbers of mobile resources, fuel resources, and labor resources. The second-stage considers network operational constraints and repair crew scheduling constraints. The proposed stochastic pre-event preparation model is solved by a scenario decomposition method, Progressive Hedging (PH), to ease the computational complexity introduced by a large number of scenarios. Furthermore, to show the impact of solar photovoltaic (PV) generation on system resilience, we consider three types of PV systems during power outage and compare the resilience improvements with different PV penetration levels. Numerical results from simulations on a large-scale (more than 10,000 nodes) distribution feeder have been used to validate the scalability and effectiveness of the proposed method.      
### 14.An Efficient Recurrent Adversarial Framework for Unsupervised Real-Time Video Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2012.13033.pdf)
>  Video enhancement is a challenging problem, more than that of stills, mainly due to high computational cost, larger data volumes and the difficulty of achieving consistency in the spatio-temporal domain. In practice, these challenges are often coupled with the lack of example pairs, which inhibits the application of supervised learning strategies. To address these challenges, we propose an efficient adversarial video enhancement framework that learns directly from unpaired video examples. In particular, our framework introduces new recurrent cells that consist of interleaved local and global modules for implicit integration of spatial and temporal information. The proposed design allows our recurrent cells to efficiently propagate spatio-temporal information across frames and reduces the need for high complexity networks. Our setting enables learning from unpaired videos in a cyclic adversarial manner, where the proposed recurrent units are employed in all architectures. Efficient training is accomplished by introducing one single discriminator that learns the joint distribution of source and target domain simultaneously. The enhancement results demonstrate clear superiority of the proposed video enhancer over the state-of-the-art methods, in all terms of visual quality, quantitative metrics, and inference speed. Notably, our video enhancer is capable of enhancing over 35 frames per second of FullHD video (1080x1920).      
### 15.Quickest Detection over Sensor Networks with Unknown Post-Change Distribution  [ :arrow_down: ](https://arxiv.org/pdf/2012.13027.pdf)
>  We propose a quickest change detection problem over sensor networks where both the subset of sensors undergoing a change and the local post-change distributions are unknown. Each sensor in the network observes a local discrete time random process over a finite alphabet. Initially, the observations are independent and identically distributed (i.i.d.) with known pre-change distributions independent from other sensors. At a fixed but unknown change point, a fixed but unknown subset of the sensors undergo a change and start observing samples from an unknown distribution. We assume the change can be quantified using concave (or convex) local statistics over the space of distributions. We propose an asymptotically optimal and computationally tractable stopping time for Lorden's criterion. Under this scenario, our proposed method uses a concave global cumulative sum (CUSUM) statistic at the fusion center and suppresses the most likely false alarms using information projection. Finally, we show some numerical results of the simulation of our algorithm for the problem described.      
### 16.The 2020 ESPnet update: new features, broadened applications, performance improvements, and future plans  [ :arrow_down: ](https://arxiv.org/pdf/2012.13006.pdf)
>  This paper describes the recent development of ESPnet (<a class="link-external link-https" href="https://github.com/espnet/espnet" rel="external noopener nofollow">this https URL</a>), an end-to-end speech processing toolkit. This project was initiated in December 2017 to mainly deal with end-to-end speech recognition experiments based on sequence-to-sequence modeling. The project has grown rapidly and now covers a wide range of speech processing applications. Now ESPnet also includes text to speech (TTS), voice conversation (VC), speech translation (ST), and speech enhancement (SE) with support for beamforming, speech separation, denoising, and dereverberation. All applications are trained in an end-to-end manner, thanks to the generic sequence to sequence modeling properties, and they can be further integrated and jointly optimized. Also, ESPnet provides reproducible all-in-one recipes for these applications with state-of-the-art performance in various benchmarks by incorporating transformer, advanced data augmentation, and conformer. This project aims to provide up-to-date speech processing experience to the community so that researchers in academia and various industry scales can develop their technologies collaboratively.      
### 17.Eurythmic Dancing with Plants -- Measuring Plant Response to Human Body Movement in an Anthroposophic Environment  [ :arrow_down: ](https://arxiv.org/pdf/2012.12978.pdf)
>  This paper describes three experiments measuring interaction of humans with garden plants. In particular, body movement of a human conducting eurythmic dances near the plants (beetroots, tomatoes, lettuce) is correlated with the action potential measured by a plant SpikerBox, a device measuring the electrical activity of plants, and the leaf movement of the plant, tracked with a camera. The first experiment shows that our measurement system captures external stimuli identically for different plants, validating the measurement system. The second experiment illustrates that the plants' response is correlated to the movements of the dancer. The third experiment indicates that plants that have been exposed for multiple weeks to eurythmic dancing might respond differently to plants which are exposed for the first time to eurythmic dancing.      
### 18.Synthesis of Supervisors Robust Against Sensor Deception Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2012.12932.pdf)
>  We consider feedback control systems where sensor readings may be compromised by a malicious attacker intending on causing damage to the system. We study this problem at the supervisory layer of the control system, using discrete event systems techniques. We assume that the attacker can edit the outputs from the sensors of the system before they reach the supervisory controller. In this context, we formulate the problem of synthesizing a supervisor that is robust against the class of edit attacks on the sensor readings and present a solution methodology for this problem. This methodology blends techniques from games on automata with imperfect information with results from supervisory control theory of partially-observed discrete event systems. Necessary and sufficient conditions are provided for the investigated problem.      
### 19.A Physics-Informed Deep Learning Paradigm for Car-Following Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.13376.pdf)
>  Car-following behavior has been extensively studied using physics-based models, such as the Intelligent Driver Model. These models successfully interpret traffic phenomena observed in the real-world but may not fully capture the complex cognitive process of driving. Deep learning models, on the other hand, have demonstrated their power in capturing observed traffic phenomena but require a large amount of driving data to train. This paper aims to develop a family of neural network based car-following models that are informed by physics-based models, which leverage the advantage of both physics-based (being data-efficient and interpretable) and deep learning based (being generalizable) models. We design physics-informed deep learning for car-following (PIDL-CF) architectures encoded with two popular physics-based models - IDM and OVM, on which acceleration is predicted for four traffic regimes: acceleration, deceleration, cruising, and emergency braking. Two types of PIDL-CFM problems are studied, one to predict acceleration only and the other to jointly predict acceleration and discover model parameters. We also demonstrate the superior performance of PIDL with the Next Generation SIMulation (NGSIM) dataset over baselines, especially when the training data is sparse. The results demonstrate the superior performance of neural networks informed by physics over those without. The developed PIDL-CF framework holds the potential for system identification of driving models and for the development of driving-based controls for automated vehicles.      
### 20.Towards Radio Designs with Non-Linear Processing for Next Generation Mobile Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.13371.pdf)
>  MIMO mobile systems, with a large number of antennas at the base-station side, enable the concurrent transmission of multiple, spatially separated information streams and, therefore, enable improved network throughput and connectivity both in uplink and downlink transmissions. Traditionally, to efficiently facilitate such MIMO transmissions, linear base-station processing is adopted, that translates the MIMO channel into several single-antenna channels. Still, while such approaches are relatively easy to implement, they can leave on the table a significant amount of unexploited MIMO capacity. Recently proposed non-linear base-station processing methods claim this unexplored capacity and promise a substantially increased network throughput. Still, to the best of the authors' knowledge, non-linear base-station processing methods not only have not yet been adopted by actual systems, but have not even been evaluated in a standard-compliant framework, involving of all the necessary algorithmic modules required by a practical system. This work, outlines our experience by trying to incorporate and evaluate the gains of non-linear base-station processing in a 3GPP standard environment. We discuss the several corresponding challenges and our adopted solutions, together with their corresponding limitations. We report gains that we have managed to verify, and we also discuss remaining challenges, missing algorithmic components and future research directions that would be required towards highly efficient, future mobile systems that can efficiently exploit the gains of non-linear, base-station processing.      
### 21.Waveguide Components and Aperture Antennas With Frequency- and Time-Domain Selectivity Properties  [ :arrow_down: ](https://arxiv.org/pdf/2012.13350.pdf)
>  Filtering modules are essential devices of modern microwave systems given their capability to improve the signal-to-noise ratio of the received signal or to eliminate the unwanted interferences. For discriminating between different components, a filter exhibits a frequency-selective response that, however, is not able to distinguish between different signals whose spectrum falls within the passband of the filter itself. In this regard, some electromagnetic structures exhibiting, at the same frequency, different responses depending on the waveform of the incoming waves have been recently proposed. In this communication, we extend the aforementioned approach to the case of a standard waveguide filtering module. In particular, by loading a bandpass filtering iris with a proper lumped-element circuit, we design a waveguide component able to distinguish between different pulsed waves, even at the same frequency, depending on their pulsewidth. Moreover, by using this filter for capping an open-ended rectangular waveguide, a radiating element with both frequency- and time-domain selectivity properties is presented. The structures discussed in this communication may pave the way to a new class of microwave systems that, being both frequency selective and time selective, are less sensitive to noise and interferences.      
### 22.Generating Long-term Continuous Multi-type Generation Profiles using Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2012.13344.pdf)
>  Today, the adoption of new technologies has increased power system dynamics significantly. Traditional long-term planning studies that most utility companies perform based on discrete power levels such as peak or average values cannot reflect system dynamics and often fail to accurately predict system reliability deficiencies. As a result, long-term future continuous profiles such as the 8760 hourly profiles are required to enable time-series based long-term planning studies. However, unlike short-term profiles used for operation studies, generating long-term continuous profiles that can reflect both historical time-varying characteristics and future expected power magnitude is very challenging. Current methods such as average profiling have major drawbacks. To solve this challenge, this paper proposes a completely novel approach to generate such profiles for multiple generation types using Generative Adversarial Networks (GAN). A multi-level profile synthesis process is proposed to capture time-varying characteristics at different time levels. Both Single-type GAN and a modified Conditional GAN systems are developed. Unique profile evaluation metrics are proposed. The proposed approach was evaluated based on a public dataset and demonstrated great performance and application value for generating long-term continuous multi-type generation profiles.      
### 23.AudioViewer: Learning to Visualize Sound  [ :arrow_down: ](https://arxiv.org/pdf/2012.13341.pdf)
>  Sensory substitution can help persons with perceptual deficits. In this work, we attempt to visualize audio with video. Our long-term goal is to create sound perception for hearing impaired people, for instance, to facilitate feedback for training deaf speech. Different from existing models that translate between speech and text or text and images, we target an immediate and low-level translation that applies to generic environment sounds and human speech without delay. No canonical mapping is known for this artificial translation task. Our design is to translate from audio to video by compressing both into a common latent space with shared structure. Our core contribution is the development and evaluation of learned mappings that respect human perception limits and maximize user comfort by enforcing priors and combining strategies from unpaired image translation and disentanglement. We demonstrate qualitatively and quantitatively that our AudioViewer model maintains important audio features in the generated video and that generated videos of faces and numbers are well suited for visualizing high-dimensional audio features since they can easily be parsed by humans to match and distinguish between sounds, words, and speakers.      
### 24.Distortion-Aware Linear Precoding for Massive MIMO Downlink Systems with Nonlinear Power Amplifiers  [ :arrow_down: ](https://arxiv.org/pdf/2012.13337.pdf)
>  We introduce a framework for linear precoder design over a massive multiple-input multiple-output downlink system and in presence of nonlinear power amplifiers (PAs). By studying the spatial characteristics of the distortion, we demonstrate that conventional linear precoding techniques steer nonlinear distortions in the direction of the users. We show that, by taking into account PA nonlinearity characteristics, one can design linear precoders that reduce, and in single-user scenarios, even remove completely the distortion transmitted in the direction of the users. This, however, is achieved at the price of a considerably reduced array gain. To address this issue, we present precoder optimization algorithms which simultaneously take into account the effects of array gain, distortion, multiuser interference, and receiver noise. Specifically, we derive an expression for the achievable sum rate and propose an iterative algorithm that attempts to find the precoding matrix maximizing this expression. Moreover, using a model for PA power consumption, we propose an algorithm that attempts to find the precoding matrix minimizing the consumed power for a given minimum achievable sum rate. Our numerical results demonstrate that the proposed distortion-aware precoding techniques yield considerable improvements in terms of spectral and energy efficiency compared to conventional linear precoding techniques.      
### 25.Computation of Convex Hull Prices in Electricity Markets with Non-Convexities using Dantzig-Wolfe Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2012.13331.pdf)
>  The presence of non-convexities in electricity markets has been an active research area for about two decades. The - inevitable under current marginal cost pricing - problem of guaranteeing that no truthful-bidding market participant incurs losses in the day-ahead (DA) market is addressed in current practice through make-whole payments a.k.a. uplift. Alternative pricing rules have been studied to deal with this problem. Among them, Convex Hull (CH) prices associated with minimum uplift have attracted significant attention. Several US Independent System Operators (ISOs) have considered CH prices but resorted to approximations, mainly because determining exact CH prices is computationally challenging, while providing little intuition about the price formation rational. In this paper, we describe CH price estimation problem by relying on Dantzig-Wolfe decomposition and Column Generation. Moreover, the approach provides intuition on the underlying price formation rational. A test bed of stylized examples elucidate an exposition of the intuition in the CH price formation. In addition, a realistic ISO dataset is used to suggest scalability and validate the proof-of-concept.      
### 26.Toward the use of temporary tattoo electrodes for impedancemetric respiration monitoring and other electrophysiological recordings  [ :arrow_down: ](https://arxiv.org/pdf/2012.13319.pdf)
>  Development of dry, ultra-conformable and unperceivable temporary tattoo electrodes (TTEs), based on the ink-jet printing of PEDOT:PSS on top of commercially available temporary tattoo paper, has gained increasing attention as a new and promising technology for electrophysiological recordings on skin. In this work we present a TTEs epidermal sensor for real time monitoring of respiration through transthoracic impedance measurements, exploiting a new design based on the application of soft screen printed Ag ink and magnetic interlink, that guarantees a repositionable, long term stable and robust interconnection of TTEs with external docking devices. The efficiency of the TTE and the proposed interconnection strategy under stretching (up to 10%) and over time (up to 96 hours) has been verified on a dedicated experimental setup and on humans, fulfilling the proposed specific application of transthoracic impedance measurements. The proposed approach makes this technology suitable for large-scale production and suitable not only for the specific use case presented, but also for real time monitoring of different bio-electric signals, as demonstrated through specific proof of concept demonstrators.      
### 27.On Statistical Efficiency in Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.13307.pdf)
>  A central issue of many statistical learning problems is to select an appropriate model from a set of candidate models. Large models tend to inflate the variance (or overfitting), while small models tend to cause biases (or underfitting) for a given fixed dataset. In this work, we address the critical challenge of model selection to strike a balance between model fitting and model complexity, thus gaining reliable predictive power. We consider the task of approaching the theoretical limit of statistical learning, meaning that the selected model has the predictive performance that is as good as the best possible model given a class of potentially misspecified candidate models. We propose a generalized notion of Takeuchi's information criterion and prove that the proposed method can asymptotically achieve the optimal out-sample prediction loss under reasonable assumptions. It is the first proof of the asymptotic property of Takeuchi's information criterion to our best knowledge. Our proof applies to a wide variety of nonlinear models, loss functions, and high dimensionality (in the sense that the models' complexity can grow with sample size). The proposed method can be used as a computationally efficient surrogate for leave-one-out cross-validation. Moreover, for modeling streaming data, we propose an online algorithm that sequentially expands the model complexity to enhance selection stability and reduce computation cost. Experimental studies show that the proposed method has desirable predictive power and significantly less computational cost than some popular methods.      
### 28.Using Spatial Logic and Model Checking for Nevus Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.13289.pdf)
>  Spatial and spatio-temporal model checking techniques have a wide range of application domains, among which large scale distributed systems and signal and image analysis. In the latter domain, automatic and semi-automatic contouring in Medical Imaging has shown to be a very promising and versatile application that can greatly facilitate the work of professionals in this domain, while supporting explainability, easy replicability and exchange of medical image analysis methods. In recent work we have applied this model-checking technique to the (3D) contouring of tumours and related oedema in magnetic resonance images of the brain. In the current work we address the contouring of (2D) images of nevi. One of the challenges of treating nevi images is their considerable inhomogeneity in shape, colour, texture and size. To deal with this challenge we use a texture similarity operator, in combination with spatial logic operators. We apply our technique on images of a large public database and compare the results with associated ground truth segmentation provided by domain experts.      
### 29.Learning Maximally Monotone Operators for Image Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2012.13247.pdf)
>  We introduce a new paradigm for solving regularized variational problems. These are typically formulated to address ill-posed inverse problems encountered in signal and image processing. The objective function is traditionally defined by adding a regularization function to a data fit term, which is subsequently minimized by using iterative optimization algorithms. Recently, several works have proposed to replace the operator related to the regularization by a more sophisticated denoiser. These approaches, known as plug-and-play (PnP) methods, have shown excellent performance. Although it has been noticed that, under nonexpansiveness assumptions on the denoisers, the convergence of the resulting algorithm is guaranteed, little is known about characterizing the asymptotically delivered solution. In the current article, we propose to address this limitation. More specifically, instead of employing a functional regularization, we perform an operator regularization, where a maximally monotone operator (MMO) is learned in a supervised manner. This formulation is flexible as it allows the solution to be characterized through a broad range of variational inequalities, and it includes convex regularizations as special cases. From an algorithmic standpoint, the proposed approach consists in replacing the resolvent of the MMO by a neural network (NN). We provide a universal approximation theorem proving that nonexpansive NNs provide suitable models for the resolvent of a wide class of MMOs. The proposed approach thus provides a sound theoretical framework for analyzing the asymptotic behavior of first-order PnP algorithms. In addition, we propose a numerical strategy to train NNs corresponding to resolvents of MMOs. We apply our approach to image restoration problems and demonstrate its validity in terms of both convergence and quality.      
### 30.Path Planning of Unmanned System using Carrot-chasing Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2012.13227.pdf)
>  When an unmanned system is launched for a mission-critical task, it is required to follow a predetermined path. It means the unmanned system requires a path following algorithm for the completion of the mission. Since the predetermined path is typically given by a set of data-points, not only the curvature and derivative of the pre-determined path are absent, but also it requires a large size of on-board memory. In this work, we study a simple path following algorithm called Carrot-chasing algorithm that uses a simple controller in the form of a proportional controller to control the movement of an unmanned system.      
### 31.Tunnel Facility-based Vehicle Localization in Highway Tunnel using 3D LIDAR  [ :arrow_down: ](https://arxiv.org/pdf/2012.13168.pdf)
>  Vehicle localization in highway tunnels is a challenging issue for autonomous vehicle navigation. Since GPS signals from satellites cannot be received inside a highway tunnel, map-aided localization is essential. However, the environment around the tunnel is composed mostly of an elliptical wall. Thereby, the unique feature points for map matching are few unlike the case outdoors. As a result, it is a very difficult condition to perform vehicle navigation in the tunnel with existing map-aided localization. In this paper, we propose tunnel facility-based precise vehicle localization in highway tunnels using 3D LIDAR. For vehicle localization in a highway tunnel, a point landmark map that stores the center points of tunnel facilities and a probability distribution map that stores the probability distributions of the lane markings are used. Point landmark-based localization is possible regardless of the number of feature points, if only representative points of an object can be extracted. Therefore, it is a suitable localization method for highway tunnels where the feature points are few. The tunnel facility points were extracted using 3D LIDAR. Position estimation is conducted using an EKF-based navigation filter. The proposed localization algorithm is verified through experiments using actual highway driving data. The experimental results verify that the tunnel facility-based vehicle localization yields precise results in real time.      
### 32.Unsupervised neural adaptation model based on optimal transport for spoken language identification  [ :arrow_down: ](https://arxiv.org/pdf/2012.13152.pdf)
>  Due to the mismatch of statistical distributions of acoustic speech between training and testing sets, the performance of spoken language identification (SLID) could be drastically degraded. In this paper, we propose an unsupervised neural adaptation model to deal with the distribution mismatch problem for SLID. In our model, we explicitly formulate the adaptation as to reduce the distribution discrepancy on both feature and classifier for training and testing data sets. Moreover, inspired by the strong power of the optimal transport (OT) to measure distribution discrepancy, a Wasserstein distance metric is designed in the adaptation loss. By minimizing the classification loss on the training data set with the adaptation loss on both training and testing data sets, the statistical distribution difference between training and testing domains is reduced. We carried out SLID experiments on the oriental language recognition (OLR) challenge data corpus where the training and testing data sets were collected from different conditions. Our results showed that significant improvements were achieved on the cross domain test tasks.      
### 33.On Radiation-Based Thermal Servoing: New Models, Controls and Experiments  [ :arrow_down: ](https://arxiv.org/pdf/2012.13147.pdf)
>  In this paper, we introduce a new sensor-based control method that regulates (by means of robot motions) the heat transfer between a radiative source and an object of interest. This valuable sensorimotor capability is needed in many industrial, dermatology and field robot applications, and it is an essential component for creating machines with advanced thermo-motor intelligence. To this end, we derive a geometric-thermal-motor model which describes the relationship between the robot's active configuration and the produced dynamic thermal response. We then use the model to guide the design of two new thermal servoing controllers (one model-based and one adaptive), and analyze their stability with Lyapunov theory. To validate our method, we report a detailed experimental study with a robotic manipulator conducting autonomous thermal servoing tasks. To the best of the authors' knowledge, this is the first time that temperature regulation has been formulated as a motion control problem for robots.      
### 34.An AGC Reformulation for the Decomposed Security-Constrained ACOPF Problem  [ :arrow_down: ](https://arxiv.org/pdf/2012.13110.pdf)
>  This paper presents a reformulation for the automatic generation control (AGC) formulation in a decomposed convex relaxation algorithm to find an optimal solution to the AC optimal power flow (AC-OPF) problem which is secure against a large set of contingencies. First, the master problem, which represents the system without contingency constraints, is convexified by applying the second-order cone relaxation approach. Second, the contingencies are filtered for corrective or preventive actions. The contingencies for preventive security check sub-problems are evaluated in a parallel computing process to improve computational efficiency. The AGC is modeled by a set of proposed valid constraints, so the solution obtained in each security check sub-problem is the physical response of the system during a contingency. Third, Benders optimality cuts are generated for the sub-problems with mismatches. The cuts are passed to the master problem to encounter the security-constraints. The proposed convex relaxation for the master problem ensures the convergence of the decomposition algorithm. The effectiveness of the presented valid AGC constraints and scalability of the proposed algorithm are illustrated in several case studies.      
### 35.Analysis and Design of Partially Information- and Partially Parity-Coupled Turbo Codes  [ :arrow_down: ](https://arxiv.org/pdf/2012.13082.pdf)
>  In this paper, we study a class of spatially coupled turbo codes, namely partially information- and partially parity-coupled turbo codes. This class of codes enjoy several advantages such as flexible code rate adjustment by varying the coupling ratio and the encoding and decoding architectures of the underlying component codes can remain unchanged. For this work, we first provide the construction methods for partially coupled turbo codes with coupling memory $m$ and study the corresponding graph models. We then derive the density evolution equations for the corresponding ensembles on the binary erasure channel to precisely compute their iterative decoding thresholds. Rate-compatible designs and their decoding thresholds are also provided, where the coupling and puncturing ratios are jointly optimized to achieve the largest decoding threshold for a given target code rate. Our results show that for a wide range of code rates, the proposed codes attain close-to-capacity performance and the decoding performance improves with increasing the coupling memory. In particular, the proposed partially parity-coupled turbo codes have thresholds within 0.0002 of the BEC capacity for rates ranging from $1/3$ to $9/10$, yielding an attractive way for constructing rate-compatible capacity-approaching channel codes.      
### 36.A Generalized A* Algorithm for Finding Globally Optimal Paths in Weighted Colored Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2012.13057.pdf)
>  Both geometric and semantic information of the search space is imperative for a good plan. We encode those properties in a weighted colored graph (geometric information in terms of edge weight and semantic information in terms of edge and vertex color), and propose a generalized A* to find the shortest path among the set of paths with minimal inclusion of low-ranked color edges. We prove the completeness and optimality of this Class-Ordered A* (COA*) algorithm with respect to the hereto defined notion of optimality. The utility of COA* is numerically validated in a ternary graph with feasible, infeasible, and unknown vertices and edges for the cases of a 2D mobile robot, a 3D robotic arm, and a 5D robotic arm with limited sensing capabilities. We compare the results of COA* to that of the regular A* algorithm, the latter of which finds the shortest path regardless of uncertainty, and we show that the COA* dominates the A* solution in terms of finding less uncertain paths.      
### 37.Assured RL: Reinforcement Learning with Almost Sure Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2012.13036.pdf)
>  We consider the problem of finding optimal policies for a Markov Decision Process with almost sure constraints on state transitions and action triplets. We define value and action-value functions that satisfy a barrier-based decomposition which allows for the identification of feasible policies independently of the reward process. We prove that, given a policy {\pi}, certifying whether certain state-action pairs lead to feasible trajectories under {\pi} is equivalent to solving an auxiliary problem aimed at finding the probability of performing an unfeasible transition. Using this interpretation,we develop a Barrier-learning algorithm, based on Q-Learning, that identifies such unsafe state-action pairs. Our analysis motivates the need to enhance the Reinforcement Learning (RL) framework with an additional signal, besides rewards, called here damage function that provides feasibility information and enables the solution of RL problems with model-free constraints. Moreover, our Barrier-learning algorithm wraps around existing RL algorithms, such as Q-Learning and SARSA, giving them the ability to solve almost-surely constrained problems.      
### 38.Low-latency Perception in Off-Road Dynamical Low Visibility Environments  [ :arrow_down: ](https://arxiv.org/pdf/2012.13014.pdf)
>  This work proposes a perception system for autonomous vehicles and advanced driver assistance specialized on unpaved roads and off-road environments. In this research, the authors have investigated the behavior of Deep Learning algorithms applied to semantic segmentation of off-road environments and unpaved roads under differents adverse conditions of visibility. Almost 12,000 images of different unpaved and off-road environments were collected and labeled. It was assembled an off-road proving ground exclusively for its development. The proposed dataset also contains many adverse situations such as rain, dust, and low light. To develop the system, we have used convolutional neural networks trained to segment obstacles and areas where the car can pass through. We developed a Configurable Modular Segmentation Network (CMSNet) framework to help create different architectures arrangements and test them on the proposed dataset. Besides, we also have ported some CMSNet configurations by removing and fusing many layers using TensorRT, C++, and CUDA to achieve embedded real-time inference and allow field tests. The main contributions of this work are: a new dataset for unpaved roads and off-roads environments containing many adverse conditions such as night, rain, and dust; a CMSNet framework; an investigation regarding the feasibility of applying deep learning to detect region where the vehicle can pass through when there is no clear boundary of the track; a study of how our proposed segmentation algorithms behave in different severity levels of visibility impairment; and an evaluation of field tests carried out with semantic segmentation architectures ported for real-time inference.      
### 39.Speech Synthesis as Augmentation for Low-Resource ASR  [ :arrow_down: ](https://arxiv.org/pdf/2012.13004.pdf)
>  Speech synthesis might hold the key to low-resource speech recognition. Data augmentation techniques have become an essential part of modern speech recognition training. Yet, they are simple, naive, and rarely reflect real-world conditions. Meanwhile, speech synthesis techniques have been rapidly getting closer to the goal of achieving human-like speech. In this paper, we investigate the possibility of using synthesized speech as a form of data augmentation to lower the resources necessary to build a speech recognizer. We experiment with three different kinds of synthesizers: statistical parametric, neural, and adversarial. Our findings are interesting and point to new research directions for the future.      
### 40.Extracting quantitative biological information from brightfield cell images using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.12986.pdf)
>  Quantitative analysis of cell structures is essential for biomedical and pharmaceutical research. The standard imaging approach relies on fluorescence microscopy, where cell structures of interest are labeled by chemical staining techniques. However, these techniques are often invasive and sometimes even toxic to the cells, in addition to being time-consuming, labor-intensive, and expensive. Here, we introduce an alternative deep-learning-powered approach based on the analysis of brightfield images by a conditional generative adversarial neural network (cGAN). We show that this approach can extract information from the brightfield images to generate virtually-stained images, which can be used in subsequent downstream quantitative analyses of cell structures. Specifically, we train a cGAN to virtually stain lipid droplets, cytoplasm, and nuclei using brightfield images of human stem-cell-derived fat cells (adipocytes), which are of particular interest for nanomedicine and vaccine development. Subsequently, we use these virtually-stained images to extract quantitative measures about these cell structures. Generating virtually-stained fluorescence images is less invasive, less expensive, and more reproducible than standard chemical staining; furthermore, it frees up the fluorescence microscopy channels for other analytical probes, thus increasing the amount of information that can be extracted from each cell.      
### 41.Automatic Recognition of Landmarks on Digital Dental Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.12946.pdf)
>  Fundamental to improving Dental and Orthodontic treatments is the ability to quantitatively assess and cross-compare their outcomes. Such assessments require calculating distances and angles from 3D coordinates of dental landmarks. The costly and repetitive task of hand-labelling dental models impedes studies requiring large sample size to penetrate statistical noise. We have developed techniques and software implementing these techniques to map out automatically, 3D dental scans. This process is divided into consecutive steps - determining a model's orientation, separating and identifying the individual tooth and finding landmarks on each tooth - described in this paper. Examples to demonstrate techniques and the software and discussions on remaining issues are provided as well. The software is originally designed to automate Modified Huddard Bodemham (MHB) landmarking for assessing cleft lip/palate patients. Currently only MHB landmarks are supported, but is extendable to any predetermined landmarks. This software, coupled with intra-oral scanning innovation, should supersede the arduous and error prone plaster model and caliper approach to Dental research and provide a stepping-stone towards automation of routine clinical assessments such as "index of orthodontic treatment need" (IOTN).      
### 42.Minimal controllability time for systems with nonlinear drift under a compact convex state constraint  [ :arrow_down: ](https://arxiv.org/pdf/2012.12937.pdf)
>  In this paper we estimate the minimal controllability time for a class of non-linear control systems with a bounded convex state constraint. An explicit expression is given for the controllability time if the image of the control matrix is of co-dimension one. A lower bound for the controllability time is given in the general case. The technique is based on finding a lower dimension system with the similar controllability properties as the original system. The controls corresponding to the minimal time, or time close to the minimal one, are discussed and computed analytically. The effectiveness of the proposed approach is illustrated by a few examples.      
