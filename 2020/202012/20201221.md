# ArXiv eess --Mon, 21 Dec 2020
### 1.Interpolatory Methods for Generic BizJet Gust Load Alleviation Function  [ :arrow_down: ](https://arxiv.org/pdf/2012.10418.pdf)
>  The paper's main contribution concerns the use of interpolatory methods to solve end to end industrial control problems involving complex linear dynamical systems. More in details, contributions show how the rational data and function interpolation framework is a pivotal tool (i) to construct (frequency-limited) reduced order dynamical models appropriate for model-based control design and (ii) to accurately discretise controllers in view of on-board computer-limited implementation. These contributions are illustrated along the paper through the design of an active feedback gust load alleviation function, applied on an industrial generic business jet aircraft use-case. The closed-loop validation and performances evaluation are assessed through the use of an industrial dedicated simulator and considering certification objectives. Although application is centred on aircraft applications, the method is not restrictive and can be extended to any linear dynamical systems.      
### 2.Improving 3D convolutional neural network comprehensibility via interactive visualization of relevance maps: Evaluation in Alzheimer's disease  [ :arrow_down: ](https://arxiv.org/pdf/2012.10294.pdf)
>  Although convolutional neural networks (CNN) achieve high diagnostic accuracy for detecting Alzheimer's disease (AD) dementia based on magnetic resonance imaging (MRI) scans, they are not yet applied in clinical routine. One important reason for this is a lack of model comprehensibility. Recently developed visualization methods for deriving CNN relevance maps may help to fill this gap. We investigated whether models with higher accuracy also rely more on discriminative brain regions predefined by prior knowledge. We trained a CNN for the detection of AD in N=663 T1-weighted MRI scans of patients with dementia and amnestic mild cognitive impairment (MCI) and verified the accuracy of the models via cross-validation and in three independent samples including N=1655 cases. We evaluated the association of relevance scores and hippocampus volume to validate the clinical utility of this approach. To improve model comprehensibility, we implemented an interactive visualization of 3D CNN relevance maps. Across three independent datasets, group separation showed high accuracy for AD dementia vs. controls (AUC$\geq$0.92) and moderate accuracy for MCI vs. controls (AUC$\approx$0.75). Relevance maps indicated that hippocampal atrophy was considered as the most informative factor for AD detection, with additional contributions from atrophy in other cortical and subcortical regions. Relevance scores within the hippocampus were highly correlated with hippocampal volumes (Pearson's r$\approx$-0.81). The relevance maps highlighted atrophy in regions that we had hypothesized a priori. This strengthens the comprehensibility of the CNN models, which were trained in a purely data-driven manner based on the scans and diagnosis labels. The high hippocampus relevance scores and high performance achieved in independent samples support the validity of the CNN models in the detection of AD-related MRI abnormalities.      
### 3.Low-Cost Compact Theft-Detection System using MPU-6050 and Blynk IoT Platform  [ :arrow_down: ](https://arxiv.org/pdf/2012.10293.pdf)
>  The system explained in this paper provides a compact smart surveillance system. Recent years have seen the Internet of Things (IoT) dominating in various fields of applications. With devices getting smarter and insurgence of 5G technology, the connectivity of people with devices is increasing. Smarter surveillance systems are more reliable and accessible. A gyroscope is a MEM sensor which detects angular disturbances. The principle is to detect opening or knockdown of the door physically or by a gas cutter. The system is connected to the user via Wi-Fi using ESP8266. Being a system with a low form factor, this system can be implemented on doors, shops, cars, etc. An alarm system is included in the system to alert the neighbors as well as to send a notification to the user via Blynk mobile application. The proposed system is a portable smart home solution for theft detection. The code for this system is available here: <a class="link-external link-https" href="https://github.com/atharvakarnik/TheftDetectionMPU.git" rel="external noopener nofollow">this https URL</a>      
### 4.Collision Avoidance and Liveness of Multi-agent Systems with CBF-based Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2012.10261.pdf)
>  In this paper we consider multi-agent navigation with collision avoidance using Control Barrier Functions (CBF). In the case of non-communicating agents, we consider trade-offs between level of safety guarantee and liveness - the ability to reach destination in short time without large detours or gridlock. We compare several CBF-based driving policies against the benchmark established by the Centralized controller that requires communication. One of the policies (CCS2) being compared is new and straddles the space between policies with only local control available and a more complex Predictor-Corrector for Collision Avoidance (PCCA) policy that adjusts local copies of everyone's control actions based on observed behavior. The paper establishes feasibility for the Centralized, PCCA and CCS2 policies. Monte Carlo simulations show that decentralized, host-only control policies lack liveness compared to the ones that use all the control inputs in calculations and that the PCCA policy performs equally well as the Centralized, even though it is decentralized.      
### 5.Time Aggregation Techniques Applied to a Capacity Expansion Model for Real-Life Sector Coupled Energy Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.10244.pdf)
>  Simulating energy systems is vital for energy planning to understand the effects of fluctuating renewable energy sources and integration of multiple energy sectors. Capacity expansion is a powerful tool for energy analysts and consists of simulating energy systems with the option of investing in new energy sources. In this paper, we apply clustering based aggregation techniques from the literature to very different real-life sector coupled energy systems. We systematically compare the aggregation techniques with respect to solution quality and simulation time. Furthermore, we propose two new clustering approaches with promising results. We show that the aggregation techniques result in consistent solution time savings between 75% and 90%. Also, the quality of the aggregated solutions is generally very good. To the best of our knowledge, we are the first to analyze and conclude that a weighted representation of clusters is beneficial. Furthermore, to the best of our knowledge, we are the first to recommend a clustering technique with good performance across very different energy systems: the k-means with Euclidean distance measure, clustering days and with weighted selection, where the median, maximum and minimum elements from clusters are selected. A deeper analysis of the results reveal that the aggregation techniques excel when the investment decisions correlate well with the overall behavior of the energy system. We propose future research directions to remedy when this is not the case.      
### 6.Computational interference microscopy enabled by deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.10239.pdf)
>  Quantitative phase imaging (QPI) has been widely applied in characterizing cells and tissues. Spatial light interference microscopy (SLIM) is a highly sensitive QPI method, due to its partially coherent illumination and common path interferometry geometry. However, its acquisition rate is limited because of the four-frame phase-shifting scheme. On the other hand, off-axis methods like diffraction phase microscopy (DPM), allows for single-shot QPI. However, the laser-based DPM system is plagued by spatial noise due to speckles and multiple reflections. In a parallel development, deep learning was proven valuable in the field of bioimaging, especially due to its ability to translate one form of contrast into another. Here, we propose using deep learning to produce synthetic, SLIM-quality, high-sensitivity phase maps from DPM, single-shot images as input. We used an inverted microscope with its two ports connected to the DPM and SLIM modules, such that we have access to the two types of images on the same field of view. We constructed a deep learning model based on U-net and trained on over 1,000 pairs of DPM and SLIM images. The model learned to remove the speckles in laser DPM and overcame the background phase noise in both the test set and new data. Furthermore, we implemented the neural network inference into the live acquisition software, which now allows a DPM user to observe in real-time an extremely low-noise phase image. We demonstrated this principle of computational interference microscopy (CIM) imaging using blood smears, as they contain both erythrocytes and leukocytes, in static and dynamic conditions.      
### 7.Resource Allocation for Intelligent Reflecting Surface Aided Cooperative Communications  [ :arrow_down: ](https://arxiv.org/pdf/2012.10229.pdf)
>  This paper investigates an intelligent reflecting surface (IRS) aided cooperative communication network, where the IRS exploits large reflecting elements to proactively steer the incident radio-frequency wave towards destination terminals (DTs). As the number of reflecting elements increases, the reflection resource allocation (RRA) will become urgently needed in this context, which is due to the non-ignorable energy consumption. The goal of this paper, therefore, is to realize the RRA besides the active-passive beamforming design, where RRA is based on the introduced modular IRS architecture. The modular IRS consists with multiple modules, each of which has multiple reflecting elements and is equipped with a smart controller, all the controllers can communicate with each other in a point-to-point fashion via fiber links. Consequently, an optimization problem is formulated to maximize the minimum SINR at DTs, subject to the module size constraint and both individual source terminal (ST) transmit power and the reflecting coefficients constraints. Whereas this problem is NP-hard due to the module size constraint, we develop an approximate solution by introducing the mixed row block $\ell_{1,F}$-norm to transform it into a suitable semidefinite relaxation. Finally, numerical results demonstrate the meaningfulness of the introduced modular IRS architecture.      
### 8.Multimodal Transfer Learning-based Approaches for Retinal Vascular Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.10160.pdf)
>  In ophthalmology, the study of the retinal microcirculation is a key issue in the analysis of many ocular and systemic diseases, like hypertension or diabetes. This motivates the research on improving the retinal vasculature segmentation. Nowadays, Fully Convolutional Neural Networks (FCNs) usually represent the most successful approach to image segmentation. However, the success of these models is conditioned by an adequate selection and adaptation of the architectures and techniques used, as well as the availability of a large amount of annotated data. These two issues become specially relevant when applying FCNs to medical image segmentation as, first, the existent models are usually adjusted from broad domain applications over photographic images, and second, the amount of annotated data is usually scarcer. In this work, we present multimodal transfer learning-based approaches for retinal vascular segmentation, performing a comparative study of recent FCN architectures. In particular, to overcome the annotated data scarcity, we propose the novel application of self-supervised network pretraining that takes advantage of existent unlabelled multimodal data. The results demonstrate that the self-supervised pretrained networks obtain significantly better vascular masks with less training in the target task, independently of the network architecture, and that some FCN architecture advances motivated for broad domain applications do not translate into significant improvements over the vasculature segmentation task.      
### 9.Resource-efficient DNNs for Keyword Spotting using Neural Architecture Search and Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2012.10138.pdf)
>  This paper introduces neural architecture search (NAS) for the automatic discovery of small models for keyword spotting (KWS) in limited resource environments. We employ a differentiable NAS approach to optimize the structure of convolutional neural networks (CNNs) to maximize the classification accuracy while minimizing the number of operations per inference. Using NAS only, we were able to obtain a highly efficient model with 95.4% accuracy on the Google speech commands dataset with 494.8 kB of memory usage and 19.6 million operations. Additionally, weight quantization is used to reduce the memory consumption even further. We show that weight quantization to low bit-widths (e.g. 1 bit) can be used without substantial loss in accuracy. By increasing the number of input features from 10 MFCC to 20 MFCC we were able to increase the accuracy to 96.3% at 340.1 kB of memory usage and 27.1 million operations.      
### 10.End-to-End ASR and Audio Segmentation with Non-autoregressive Insertion-based model  [ :arrow_down: ](https://arxiv.org/pdf/2012.10128.pdf)
>  End-to-end models are suitable for realizing practical automatic speech recognition (ASR) systems run on a device with limited computing capability. This is because the models are composed of a single neural network hence many techniques like quantization and compression to make the model footprint lower are easily applied. One important issue when realizing such an ASR system is the segmentation of input audio. The ideal scenario is to integrate audio segmentation and ASR into a single model achieving low latency and high accuracy. In addition, reducing the real-time factor of decoding is important to achieve low latency. Non-autoregressive Transformer is a promising approach since it can decode an $N$-length token sequence with less than $N$ iterations. We propose a method to realize audio segmentation and non-autoregressive decoding using a single model. It is an insertion-based model that jointly trains with connectionist temporal classification (CTC) loss. The CTC part is used for segmentation. Then, the insertion-based non-autoregressive decoding generates a hypothesis considering the full context inside the segment. It can decode an $N$-length token sequence with approximately $\log_{\text{2}} (N)$ iterations. Experimental results on unsegmented evaluation sets of a Japanese dataset show the method achieves better accuracy than baseline CTC.      
### 11.Spectral Reflectance Estimation Using Projector with Unknown Spectral Power Distribution  [ :arrow_down: ](https://arxiv.org/pdf/2012.10083.pdf)
>  A lighting-based multispectral imaging system using an RGB camera and a projector is one of the most practical and low-cost systems to acquire multispectral observations for estimating the scene's spectral reflectance information. However, existing projector-based systems assume that the spectral power distribution (SPD) of each projector primary is known, which requires additional equipment such as a spectrometer to measure the SPD. In this paper, we present a method for jointly estimating the spectral reflectance and the SPD of each projector primary. In addition to adopting a common spectral reflectance basis model, we model the projector's SPD by a low-dimensional model using basis functions obtained by a newly collected projector's SPD database. Then, the spectral reflectances and the projector's SPDs are alternatively estimated based on the basis models. We experimentally show the performance of our joint estimation using a different number of projected illuminations and investigate the potential of the spectral reflectance estimation using a projector with unknown SPD.      
### 12.Estimating Sparsity Level for Enabling Compressive Sensing of Wireless Channels and Spectra in 5G and Beyond  [ :arrow_down: ](https://arxiv.org/pdf/2012.10082.pdf)
>  Applying compressive sensing (CS) allows for sub-Nyquist sampling in several application areas in 5G and beyond. This reduces the associated training, feedback, and computation overheads in many applications. However, the applicability of CS relies on the validity of a signal sparsity assumption and knowing the exact sparsity level. It is customary to assume a foreknown sparsity level. Still, this assumption is not valid in practice, especially when applying learned dictionaries as sparsifying transforms. The problem is more strongly pronounced with multidimensional sparsity. In this paper, we propose an algorithm for estimating the composite sparsity lying in multiple domains defined by learned dictionaries. The proposed algorithm estimates the sparsity level over a dictionary by inferring it from its counterpart with respect to a compact discrete Fourier basis. This inference is achieved by a machine learning prediction. This setting learns the intrinsic relationship between the columns of a dictionary and those of such a fixed basis. The proposed algorithm is applied to estimating sparsity levels in wireless channels, and in cognitive radio spectra. Extensive simulations validate a high quality of sparsity estimation leading to performances very close to the impractical case of assuming known sparsity.      
### 13.End-to-End Speaker Diarization as Post-Processing  [ :arrow_down: ](https://arxiv.org/pdf/2012.10055.pdf)
>  This paper investigates the utilization of an end-to-end diarization model as post-processing of conventional clustering-based diarization. Clustering-based diarization methods partition frames into clusters of the number of speakers; thus, they typically cannot handle overlapping speech because each frame is assigned to one speaker. On the other hand, some end-to-end diarization methods can handle overlapping speech by treating the problem as multi-label classification. Although some methods can treat a flexible number of speakers, they do not perform well when the number of speakers is large. To compensate for each other's weakness, we propose to use a two-speaker end-to-end diarization method as post-processing of the results obtained by a clustering-based method. We iteratively select two speakers from the results and update the results of the two speakers to improve the overlapped region. Experimental results show that the proposed algorithm consistently improved the performance of the state-of-the-art methods across CALLHOME, AMI, and DIHARD II datasets.      
### 14.Automatic detection of abnormal EEG signals using wavelet feature extraction and gradient boosting decision tree  [ :arrow_down: ](https://arxiv.org/pdf/2012.10034.pdf)
>  Electroencephalography is frequently used for diagnostic evaluation of various brain-related disorders due to its excellent resolution, non-invasive nature and low cost. However, manual analysis of EEG signals could be strenuous and a time-consuming process for experts. It requires long training time for physicians to develop expertise in it and additionally experts have low inter-rater agreement (IRA) among themselves. Therefore, many Computer Aided Diagnostic (CAD) based studies have considered the automation of interpreting EEG signals to alleviate the workload and support the final diagnosis. In this paper, we present an automatic binary classification framework for brain signals in multichannel EEG recordings. We propose to use Wavelet Packet Decomposition (WPD) techniques to decompose the EEG signals into frequency sub-bands and extract a set of statistical features from each of the selected coefficients. Moreover, we propose a novel method to reduce the dimension of the feature space without compromising the quality of the extracted features. The extracted features are classified using different Gradient Boosting Decision Tree (GBDT) based classification frameworks, which are CatBoost, XGBoost and LightGBM. We used Temple University Hospital EEG Abnormal Corpus V2.0.0 to test our proposed technique. We found that CatBoost classifier achieves the binary classification accuracy of 87.68%, and outperforms state-of-the-art techniques on the same dataset by more than 1% in accuracy and more than 3% in sensitivity. The obtained results in this research provide important insights into the usefulness of WPD feature extraction and GBDT classifiers for EEG classification.      
### 15.Optimal Real-time Bidding Policies for Contract Fulfillment in Second Price Auctions  [ :arrow_down: ](https://arxiv.org/pdf/2012.10001.pdf)
>  We study a real-time bidding problem resulting from a set of contractual obligations stipulating that a firm win a specified number of heterogeneous impressions or ad placements over a defined duration in a real-time auction. The contracts specify item targeting criteria (which may be overlapping), and a supply requirement. Using the Pontryagin maximum principle, we show that the resulting continuous time and time inhomogeneous planning problem can be reduced into a finite dimensional convex optimization problem and solved to optimality. In addition, we provide algorithms to update the bidding plan over time via a receding horizon. Finally, we provide numerical results based on real data and show a connection to production-transportation problems.      
### 16.Fast 3-dimensional estimation of the Foveal Avascular Zone from OCTA  [ :arrow_down: ](https://arxiv.org/pdf/2012.09945.pdf)
>  The area of the foveal avascular zone (FAZ) from en face images of optical coherence tomography angiography (OCTA) is one of the most common measurement based on this technology. However, its use in clinic is limited by the high variation of the FAZ area across normal subjects, while the calculation of the volumetric measurement of the FAZ is limited by the high noise that characterizes OCTA scans. We designed an algorithm that exploits the higher signal-to-noise ratio of en face images to efficiently identify the capillary network of the inner retina in 3-dimensions (3D), under the assumption that the capillaries in separate plexuses do not overlap. The network is then processed with morphological operations to identify the 3D FAZ within the bounding segmentations of the inner retina. The FAZ volume and area in different plexuses were calculated for a dataset of 430 eyes. Then, the measurements were analyzed using linear mixed effect models to identify differences between three groups of eyes: healthy, diabetic without diabetic retinopathy (DR) and diabetic with DR. Results showed significant differences in the FAZ volume between the different groups but not in the area measurements. These results suggest that volumetric FAZ could be a better diagnostic detector than the planar FAZ. The efficient methodology that we introduced could allow the fast calculation of the FAZ volume in clinics, as well as providing the 3D segmentation of the capillary network of the inner retina.      
### 17.Reduction of the Number of Variables in Parametric Constrained Least-Squares Problems  [ :arrow_down: ](https://arxiv.org/pdf/2012.10423.pdf)
>  For linearly constrained least-squares problems that depend on a vector of parameters, this paper proposes techniques for reducing the number of involved optimization variables. After first eliminating equality constraints in a numerically robust way by QR factorization, we propose a technique based on singular value decomposition (SVD) and unsupervised learning, that we call $K$-SVD, and neural classifiers to automatically partition the set of parameter vectors in $K$ nonlinear regions in which the original problem is approximated by using a smaller set of variables. For the special case of parametric constrained least-squares problems that arise from model predictive control (MPC) formulations, we propose a novel and very efficient QR factorization method for equality constraint elimination. Together with SVD or $K$-SVD, the method provides a numerically robust alternative to standard condensing and move blocking, and to other complexity reduction methods for MPC based on basis functions. We show the good performance of the proposed techniques in numerical tests and in a linearized MPC problem of a nonlinear benchmark process.      
### 18.LiveMap: Real-Time Dynamic Map in Automotive Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2012.10252.pdf)
>  Autonomous driving needs various line-of-sight sensors to perceive surroundings that could be impaired under diverse environment uncertainties such as visual occlusion and extreme weather. To improve driving safety, we explore to wirelessly share perception information among connected vehicles within automotive edge computing networks. Sharing massive perception data in real time, however, is challenging under dynamic networking conditions and varying computation workloads. In this paper, we propose LiveMap, a real-time dynamic map, that detects, matches, and tracks objects on the road with crowdsourcing data from connected vehicles in sub-second. We develop the data plane of LiveMap that efficiently processes individual vehicle data with object detection, projection, feature extraction, object matching, and effectively integrates objects from multiple vehicles with object combination. We design the control plane of LiveMap that allows adaptive offloading of vehicle computations, and develop an intelligent vehicle scheduling and offloading algorithm to reduce the offloading latency of vehicles based on deep reinforcement learning (DRL) techniques. We implement LiveMap on a small-scale testbed and develop a large-scale network simulator. We evaluate the performance of LiveMap with both experiments and simulations, and the results show LiveMap reduces 34.1% average latency than the baseline solution.      
### 19.Exact Reduction of Huge Action Spaces in General Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.10200.pdf)
>  The reinforcement learning (RL) framework formalizes the notion of learning with interactions. Many real-world problems have large state-spaces and/or action-spaces such as in Go, StarCraft, protein folding, and robotics or are non-Markovian, which cause significant challenges to RL algorithms. In this work we address the large action-space problem by sequentializing actions, which can reduce the action-space size significantly, even down to two actions at the expense of an increased planning horizon. We provide explicit and exact constructions and equivalence proofs for all quantities of interest for arbitrary history-based processes. In the case of MDPs, this could help RL algorithms that bootstrap. In this work we show how action-binarization in the non-MDP case can significantly improve Extreme State Aggregation (ESA) bounds. ESA allows casting any (non-MDP, non-ergodic, history-based) RL problem into a fixed-sized non-Markovian state-space with the help of a surrogate Markovian process. On the upside, ESA enjoys similar optimality guarantees as Markovian models do. But a downside is that the size of the aggregated state-space becomes exponential in the size of the action-space. In this work, we patch this issue by binarizing the action-space. We provide an upper bound on the number of states of this binarized ESA that is logarithmic in the original action-space size, a double-exponential improvement.      
### 20.Structural Balance and Interpersonal Appraisals Dynamics: Beyond All-to-All andTwo-Faction Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.10151.pdf)
>  Structural balance theory describes stable configurations of topologies of signed interpersonal appraisal networks. Existing models explaining the convergence of appraisal networks to structural balance either diverge in finite time, or could get stuck in jammed states, or converge to only complete graphs. In this paper, we study the open problem how steady non-all-to-all structural balance emerges via local dynamics of interpersonal appraisals. We first compare two well-justified definitions of structural balance for general non-all-to-all graphs, i.e., the triad-wise structural balance and the two-faction structural balance, and thoroughly study their relations. Secondly, based on three widely adopted sociological mechanisms: the symmetry mechanism, the influence mechanism, and the homophily mechanism, we propose two simple models of gossip-like appraisal dynamics, the symmetry-influence-homophily (SIH) dynamics and the symmetry-influence-opinion-homophily (SIOH) dynamics. In these models, the appraisal network starting from any initial condition almost surely achieves non-all-to-all triad-wise and two-faction structural balance in finite time respectively. Moreover, the SIOH dynamics capture the co-evolution of interpersonal appraisals and individuals' opinions. Regarding the theoretical contributions, we show that the equilibrium set of the SIH (SIOH resp.) dynamics corresponds to the set of all the possible triad-wise (two-faction resp.) structural balance configurations of the appraisal networks. Moreover, we prove that, for any initial condition, the appraisal networks in the SIH (SIOH resp.) dynamics almost surely achieve triad-wise (two-faction resp.) structural balance in finite time. Numerical studies of the SIH dynamics also imply some insightful take-home messages on whether multilateral relations reduce or exacerbate conflicts.      
### 21.Learning and balancing time-varying loads in large-scale systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.10142.pdf)
>  Consider a system of $n$ parallel server pools where tasks arrive as a time-varying Poisson process. The system aims at balancing the load by using an inner control loop with an admission threshold to assign incoming tasks to server pools; as an outer control loop, a learning scheme adjusts this threshold over time in steps of $\Delta$ units, to keep it aligned with the time-varying overall load. If the fluctuations in the normalized load are smaller than $\Delta$, then we prove that the threshold settles for all large enough $n$ and balances the load when $\Delta = 1$. Our model captures a tradeoff between optimality and stability, since for higher $\Delta$ the degree of balance decreases, but the threshold remains constant under larger load fluctuations. The analysis of this model is mathematically challenging, particularly since the learning scheme relies on subtle variations in the occupancy state of the system which vanish on the fluid scale; the methodology developed in this paper overcomes this hurdle by leveraging the tractability of the specific system dynamics. Strong approximations are used to prove certain dynamical properties which are then used to characterize the behavior of the system, without relying on a traditional fluid-limit analysis.      
### 22.Voronoi Progressive Widening: Efficient Online Solvers for Continuous Space MDPs and POMDPs with Provably Optimal Components  [ :arrow_down: ](https://arxiv.org/pdf/2012.10140.pdf)
>  Markov decision processes (MDPs) and partially observable MDPs (POMDPs) can effectively represent complex real-world decision and control problems. However, continuous space MDPs and POMDPs, i.e. those having continuous state, action and observation spaces, are extremely difficult to solve, and there are few online algorithms with convergence guarantees. This paper introduces Voronoi Progressive Widening (VPW), a general technique to modify tree search algorithms to effectively handle continuous or hybrid action spaces, and proposes and evaluates three continuous space solvers: VOSS, VOWSS, and VOMCPOW. VOSS and VOWSS are theoretical tools based on sparse sampling and Voronoi optimistic optimization designed to justify VPW-based online solvers. While previous algorithms have enjoyed convergence guarantees for problems with continuous state and observation spaces, VOWSS is the first with global convergence guarantees for problems that additionally have continuous action spaces. VOMCPOW is a versatile and efficient VPW-based algorithm that consistently outperforms POMCPOW and BOMCP in several simulation experiments.      
### 23.A Data-Driven Warm Start Approach for Convex Relaxation in Optimal Gas Flow  [ :arrow_down: ](https://arxiv.org/pdf/2012.10125.pdf)
>  In this letter, we propose a data-driven warm start approach, empowered by artificial neural networks, to boost the efficiency of convex relaxations in optimal gas flow. Case studies show that this approach significantly decreases the number of iterations for the convex-concave procedure algorithm, and optimality and feasibility of the solution can still be guaranteed.      
### 24.Reconfigurable Intelligent Surface Enhanced NOMA Assisted Backscatter Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2012.10111.pdf)
>  A reconfigurable intelligent surface (RIS) enhanced non-orthogonal multiple access assisted backscatter communication (RIS-NOMABC) system is considered. A joint optimization problem over power reflection coefficients and phase shifts is formulated. To solve this non-convex problem, a low complexity algorithm is proposed by invoking the alternative optimization, successive convex approximation and manifold optimization algorithms. Numerical results corroborate that the proposed <br>RIS-NOMABC system outperforms the conventional non-orthogonal multiple access assisted backscatter communication (NOMABC) system without RIS, and demonstrate the feasibility and effectiveness of the proposed algorithm.      
### 25.NeurST: Neural Speech Translation Toolkit  [ :arrow_down: ](https://arxiv.org/pdf/2012.10018.pdf)
>  NeurST is an open-source toolkit for neural speech translation developed by ByteDance AI Lab. The toolkit mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products. NeurST aims at facilitating the speech translation research for NLP researchers and provides a complete setup for speech translation benchmarks, including feature extraction, data preprocessing, distributed training, and evaluation. Moreover, The toolkit implements several major architectures for end-to-end speech translation. It shows experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The toolkit is publicly available at <a class="link-external link-https" href="https://github.com/bytedance/neurst" rel="external noopener nofollow">this https URL</a>.      
### 26.Secrecy of Multi-Antenna Transmission with Full-Duplex User in the Presence of Randomly Located Eavesdroppers  [ :arrow_down: ](https://arxiv.org/pdf/2012.09952.pdf)
>  This paper considers the secrecy performance of several schemes for multi-antenna transmission to single-antenna users with full-duplex (FD) capability against randomly distributed single-antenna eavesdroppers (EDs). These schemes and related scenarios include transmit antenna selection (TAS), transmit antenna beamforming (TAB), artificial noise (AN) from the transmitter, user selection based their distances to the transmitter, and colluding and non-colluding EDs. The locations of randomly distributed EDs and users are assumed to be distributed as Poisson Point Process (PPP). We derive closed form expressions for the secrecy outage probabilities (SOP) of all these schemes and scenarios. The derived expressions are useful to reveal the impacts of various environmental parameters and user's choices on the SOP, and hence useful for network design purposes. Examples of such numerical results are discussed.      
### 27.Quantifying the Unknown: Impact of Segmentation Uncertainty on Image-Based Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2012.09913.pdf)
>  Image-based simulation, the use of 3D images to calculate physical quantities, fundamentally relies on image segmentation to create the computational geometry. However, this process introduces image segmentation uncertainty because there is a variety of different segmentation tools (both manual and machine-learning-based) that will each produce a unique and valid segmentation. First, we demonstrate that these variations propagate into the physics simulations, compromising the resulting physics quantities. Second, we propose a general framework for rapidly quantifying segmentation uncertainty. Through the creation and sampling of segmentation uncertainty probability maps, we systematically and objectively create uncertainty distributions of the physics quantities. We show that physics quantity uncertainty distributions can follow a Normal distribution, but, in more complicated physics simulations, the resulting uncertainty distribution can be both nonintuitive and surprisingly nontrivial. We also establish that simply bounding the uncertainty can fail in situations that are sensitive to image segmentation. While our work does not eliminate segmentation uncertainty, it makes visible the previously unrecognized range of uncertainty currently plaguing image-based simulation, enabling more credible simulations.      
