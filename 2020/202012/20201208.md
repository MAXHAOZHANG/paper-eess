# ArXiv eess --Tue, 8 Dec 2020
### 1.A Quantum Computing Framework for Complex System Reliability Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2012.03919.pdf)
>  This paper proposed a framework based on quantum computing for reliability assessment of complex systems. The 'Quantum Twin' concept was also proposed. The framework can be used to accelerate the reliability assessment of large-scale complex systems, which could take much computation time for classical computers to achieve accurate results. Power system is used as an example of complex systems to illustrate the framework.      
### 2.Geostationary Real-Time 3D Polarimetric RADAR Imaging by Orbital Angular Momentum Interferometry and Multi-Chromatic Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.03899.pdf)
>  We design the proof of concept for high-resolution (HR) real-time (RT), Geosynchronous and Geostationary (Geo) Polarimetric (Pol) using orbital angular momentum (OAM) interferometry - radio detection and ranging (RADAR) (HR-RT-GeoPolInt-OAM-RADAR) and multi-chromatic analysis (MCA) extended to Tomography (HR-RT-GeoPolInt-OAM-MCA-TomoRADAR). The OAM interferometry communication channel is generated by two fixed sources distanced by a given spatial baseline and used for range-azimuth synthesis. The frequency channel, instead, is used to provide information about the altitude. Finally, the information encoded in the polarization of the electromagnetic (EM) waves, which is related to the Spin Angular Momentum (SAM), is used to synthesize full-Pol RADAR images, with technological redundancy. Here we present the design of a planar vortex antenna, tailored for Geo applications, where the imaging system transmits ''ad-hoc`` structured wave packets using an incremental stepped chirp strategy and with single-mode OAM linearly incremented modulation. We assign the resolutions of each dimension to three bands assumed by the EM wave. The radial and tangential components received from the HR-RT-GeoPolInt-OAM communication channel backscattered signals are used to focus, through fast-Fourier transform (FFT) techniques, a range-azimuth image that belongs to a single epoch at a given constant frequency. Each OAM fast-time RADAR image is separated in frequency by using MCA. This procedure is repeated for all the epochs of the entire stepped-frequency chirp. Once each two-dimensional image is synthesized, they are co-registered, and the HR-RT-GeoPolInt-OAM-MCA-TomoRADAR slices are focused in altitude by using FFT techniques. Range-azimuth and tomographic resolutions depend on the OAM value and the stepped frequency chirp bandwidths.      
### 3.Modeling the effects of dynamic range compression on signals in noise  [ :arrow_down: ](https://arxiv.org/pdf/2012.03860.pdf)
>  Hearing aids use dynamic range compression (DRC), a form of automatic gain control, to make quiet sounds louder and loud sounds quieter. Compression can improve listening comfort, but it can also cause distortion in noisy environments. It has been widely reported that DRC performs poorly in noise, but there has been little mathematical analysis of these distortion effects. This work introduces a mathematical model to study the behavior of DRC in noise. Using statistical assumptions about the signal envelopes, we define an effective compression function that models the compression applied to one signal in the presence of another. This framework is used to prove results about DRC that have been previously observed experimentally: that when DRC is applied to a mixture of signals, uncorrelated signal envelopes become negatively correlated; that the effective compression applied to each sound in a mixture is weaker than it would have been for the signal alone; and that compression can reduce the long-term signal-to-noise ratio in certain conditions. These theoretical results are supported by software experiments using recorded speech signals.      
### 4.The Role of Regularization in Shaping Weight and Node Pruning Dependency and Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2012.03827.pdf)
>  The pressing need to reduce the capacity of deep neural networks has stimulated the development of network dilution methods and their analysis. While the ability of $L_1$ and $L_0$ regularization to encourage sparsity is often mentioned, $L_2$ regularization is seldom discussed in this context. We present a novel framework for weight pruning by sampling from a probability function that favors the zeroing of smaller weights. In addition, we examine the contribution of $L_1$ and $L_2$ regularization to the dynamics of node pruning while optimizing for weight pruning. We then demonstrate the effectiveness of the proposed stochastic framework when used together with a weight decay regularizer on popular classification models in removing 50% of the nodes in an MLP for MNIST classification, 60% of the filters in VGG-16 for CIFAR10 classification, and on medical image models in removing 60% of the channels in a U-Net for instance segmentation and 50% of the channels in CNN model for COVID-19 detection. For these node-pruned networks, we also present competitive weight pruning results that are only slightly less accurate than the original, dense networks.      
### 5.ECG Signal Super-resolution by Considering Reconstruction and Cardiac Arrhythmias Classification Loss  [ :arrow_down: ](https://arxiv.org/pdf/2012.03803.pdf)
>  With recent advances in deep learning algorithms, computer-assisted healthcare services have rapidly grown, especially for those that combine with mobile devices. Such a combination enables wearable and portable services for continuous measurements and facilitates real-time disease alarm based on physiological signals, e.g., cardiac arrhythmias (CAs) from electrocardiography (ECG). However, long-term and continuous monitoring confronts challenges arising from limitations of batteries, and the transmission bandwidth of devices. Therefore, identifying an effective way to improve ECG data transmission and storage efficiency has become an emerging topic. In this study, we proposed a deep-learning-based ECG signal super-resolution framework (termed ESRNet) to recover compressed ECG signals by considering the joint effect of signal reconstruction and CA classification accuracies. In our experiments, we downsampled the ECG signals from the CPSC 2018 dataset and subsequently evaluated the super-resolution performance by both reconstruction errors and classification accuracies. Experimental results showed that the proposed ESRNet framework can well reconstruct ECG signals from the 10-times compressed ones. Moreover, approximately half of the CA recognition accuracies were maintained within the ECG signals recovered by the ESRNet. The promising results confirm that the proposed ESRNet framework can be suitably used as a front-end process to reconstruct compressed ECG signals in real-world CA recognition scenarios.      
### 6.A Novel Hybrid Framework for Hourly PM2.5 Concentration Forecasting Using CEEMDAN and Deep Temporal Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2012.03781.pdf)
>  For hourly PM2.5 concentration prediction, accurately capturing the data patterns of external factors that affect PM2.5 concentration changes, and constructing a forecasting model is one of efficient means to improve forecasting accuracy. In this study, a novel hybrid forecasting model based on complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) and deep temporal convolutional neural network (DeepTCN) is developed to predict PM2.5 concentration, by modelling the data patterns of historical pollutant concentrations data, meteorological data, and discrete time variables' data. Taking PM2.5 concentration of Beijing as the sample, experimental results showed that the forecasting accuracy of the proposed CEEMDAN-DeepTCN model is verified to be the highest when compared with the time series model, artificial neural network, and the popular deep learning models. The new model has improved the capability to model the PM2.5-related factor data patterns, and can be used as a promising tool for forecasting PM2.5 concentrations.      
### 7.Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2012.03769.pdf)
>  Privacy concerns around sharing personally identifiable information are a major practical barrier to data sharing in medical research. However, in many cases, researchers have no interest in a particular individual's information but rather aim to derive insights at the level of cohorts. Here, we utilize Generative Adversarial Networks (GANs) to create derived medical imaging datasets consisting entirely of synthetic patient data. The synthetic images ideally have, in aggregate, similar statistical properties to those of a source dataset but do not contain sensitive personal information. We assess the quality of synthetic data generated by two GAN models for chest radiographs with 14 different radiology findings and brain computed tomography (CT) scans with six types of intracranial hemorrhages. We measure the synthetic image quality by the performance difference of predictive models trained on either the synthetic or the real dataset. We find that synthetic data performance disproportionately benefits from a reduced number of unique label combinations and determine at what number of samples per class overfitting effects start to dominate GAN training. Our open-source benchmark findings also indicate that synthetic data generation can benefit from higher levels of spatial resolution. We additionally conducted a reader study in which trained radiologists do not perform better than random on discriminating between synthetic and real medical images for both data modalities to a statistically significant extent. Our study offers valuable guidelines and outlines practical conditions under which insights derived from synthetic medical images are similar to those that would have been derived from real imaging data. Our results indicate that synthetic data sharing may be an attractive and privacy-preserving alternative to sharing real patient-level data in the right settings.      
### 8.Stability of discrete-time feed-forward neural networks in NARX configuration  [ :arrow_down: ](https://arxiv.org/pdf/2012.03741.pdf)
>  The idea of using Feed-Forward Neural Networks (FFNNs) as regression functions for Nonlinear AutoRegressive eXogenous (NARX) models, leading to models herein named Neural NARXs (NNARXs), has been quite popular in the early days of machine learning applied to nonlinear system identification, owing to their simple structure and ease of application to control design. Nonetheless, few theoretical results are available concerning the stability properties of these models. In this paper we address this problem, providing a sufficient condition under which NNARX models are guaranteed to enjoy the Input-to-State Stability (ISS) and the Incremental Input-to-State Stability ({\delta}ISS) properties. This condition, which is an inequality on the weights of the underlying FFNN, can be enforced during the training procedure to ensure the stability of the model. The proposed model, along with this stability condition, are tested on the pH neutralization process benchmark, showing satisfactory results.      
### 9.Data-driven Model Predictive Control Method for DFIG-based Wind Farm to Provide Primary Frequency Regulation Service  [ :arrow_down: ](https://arxiv.org/pdf/2012.03732.pdf)
>  As wind power penetration increases, the wind farms are required by newly released grid codes to provide frequency regulation service. The most critical challenge is how to formulate the dynamic model of wind farm for dynamic control, since it is essentially is nonlinear and there are huge amount of parameters to be maintained frequently. This paper proposes a data-driven model predictive control (data-driven MPC) method to make wind farms participate primary frequency regulation. In this method,a specialized dynamic mode decomposition (SDMD) algorithm is developed, which can linearly approximate the dynamics of wind farm from measurements based on Koopman operator theory.Compared with the existing extended dynamic mode decomposition (EDMD) method,this tailored SDMD has two advantages: 1) fully capturing the nonlinear transients of wind turbine dynamics with good accuracy under a wide range of working conditions; 2) much less computational burden with model dimensionality reduction. Based on the recursively updated linear dynamic model, a model predictive control solution is implemented. The simulation results show this model-free solution can dynamically optimize wind turbine generators' active power to track the frequency response requirement from system operator and minimize the rotor speed distortion.      
### 10.Reconfigurable Intelligent Surface Aided Constant-Envelope Wireless Power Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2012.03687.pdf)
>  By reconfiguring the propagation environment of electromagnetic waves artificially, reconfigurable intelligent surfaces (RISs) have been regarded as a promising and revolutionary hardware technology to improve the energy and spectrum efficiency of wireless networks. In this paper, we study a RIS aided multiuser multiple-input multiple-output (MIMO) wireless power transfer (WPT) system, where the transmitter is equipped with a constant-envelope analog beamformer. First, we maximize the total received power of the users by jointly optimizing the beamformer at transmitter and the phase-shifts at the RIS, and propose two alternating optimization based suboptimal solutions by leveraging the semidefinite relaxation (SDR) and the successive convex approximation (SCA) techniques respectively. Then, considering the user fairness, we formulate another problem to maximize the total received power subject to the users' individual minimum received power constraints. A low complexity iterative algorithm based on both alternating direction method of multipliers (ADMM) and SCA techniques is proposed to solve this problem. In the case of multiple users, we further analyze the asymptotic performance as the number of RIS elements approaches infinity, and bound the performance loss caused by RIS phase quantization. Numerical results show the correctness of the analysis results and the effectiveness of the proposed algorithms.      
### 11.Multi-Decoder Networks with Multi-Denoising Inputs for Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.03684.pdf)
>  Automatic segmentation of brain glioma from multimodal MRI scans plays a key role in clinical trials and practice. Unfortunately, manual segmentation is very challenging, time-consuming, costly, and often inaccurate despite human expertise due to the high variance and high uncertainty in the human annotations. In the present work, we develop an end-to-end deep-learning-based segmentation method using a multi-decoder architecture by jointly learning three separate sub-problems using a partly shared encoder. We also propose to apply smoothing methods to the input images to generate denoised versions as additional inputs to the network. The validation performance indicate an improvement when using the proposed method. The proposed method was ranked 2nd in the task of Quantification of Uncertainty in Segmentation in the Brain Tumors in Multimodal Magnetic Resonance Imaging Challenge 2020.      
### 12.Learning normal appearance for fetal anomaly screening: Application to the unsupervised detection of Hypoplastic Left Heart Syndrome  [ :arrow_down: ](https://arxiv.org/pdf/2012.03679.pdf)
>  Congenital heart disease is considered as one the most common groups of congenital malformations which affects $6-11$ per $1000$ newborns. In this work, an automated framework for detection of cardiac anomalies during ultrasound screening is proposed and evaluated on the example of Hypoplastic Left Heart Syndrome (HLHS), a sub-category of congenital heart disease. We propose an unsupervised approach that learns healthy anatomy exclusively from clinically confirmed normal control patients. We evaluate a number of known anomaly detection frameworks together with a new model architecture based on the $\alpha$-GAN network and find evidence that the proposed model performs significantly better than the state-of-the-art in image-based anomaly detection, yielding average $0.81$ AUC \emph{and} a better robustness towards initialisation compared to previous works.      
### 13.Consensus Control of Linear Multi-Agent Systems with Non-uniform Time-varying Communication Delays  [ :arrow_down: ](https://arxiv.org/pdf/2012.03676.pdf)
>  This paper is concerned with the consensus problem for multi-agent systems subject to communication delays between the neighboring agents. We consider a scenario where each agent is characterized by a general high-order linear system and the communication delays between the agents are non-uniform and time-varying. We design a distributed control protocol for the agents and provide an equivalent stability problem to be solved that guarantees the state consensus in the group of agents. Moreover, a delay-dependent stability criterion is provided by combining the Lyapunov-Krasovskii method with the linear matrix inequality approach.      
### 14.Binary Segmentation of Seismic Facies Using Encoder-Decoder Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.03675.pdf)
>  The interpretation of seismic data is vital for characterizing sediments' shape in areas of geological study. In seismic interpretation, deep learning becomes useful for reducing the dependence on handcrafted facies segmentation geometry and the time required to study geological areas. This work presents a Deep Neural Network for Facies Segmentation (DNFS) to obtain state-of-the-art results for seismic facies segmentation. DNFS is trained using a combination of cross-entropy and Jaccard loss functions. Our results show that DNFS obtains highly detailed predictions for seismic facies segmentation using fewer parameters than StNet and U-Net.      
### 15.Efficient Medical Image Segmentation with Intermediate Supervision Mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2012.03673.pdf)
>  Because the expansion path of U-Net may ignore the characteristics of small targets, intermediate supervision mechanism is proposed. The original mask is also entered into the network as a label for intermediate output. However, U-Net is mainly engaged in segmentation, and the extracted features are also targeted at segmentation location information, and the input and output are different. The label we need is that the input and output are both original masks, which is more similar to the refactoring process, so we propose another intermediate supervision mechanism. However, the features extracted by the contraction path of this intermediate monitoring mechanism are not necessarily consistent. For example, U-Net's contraction path extracts transverse features, while auto-encoder extracts longitudinal features, which may cause the output of the expansion path to be inconsistent with the label. Therefore, we put forward the intermediate supervision mechanism of shared-weight decoder module. Although the intermediate supervision mechanism improves the segmentation accuracy, the training time is too long due to the extra input and multiple loss functions. For one of these problems, we have introduced tied-weight decoder. To reduce the redundancy of the model, we combine shared-weight decoder module with tied-weight decoder module.      
### 16.Impact of Power Supply Noise on Image Sensor Performance in Automotive Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.03666.pdf)
>  Vision Systems are quickly becoming a large component of Active Automotive Safety Systems. In order to be effective in critical safety applications these systems must produce high quality images in both daytime and night-time scenarios in order to provide the large informational content required for software analysis in applications such as lane departure, pedestrian detection and collision detection. The challenge in capturing high quality images in low light scenarios is that the signal to noise ratio is greatly reduced, which can result in noise becoming the dominant factor in a captured image, thereby making these safety systems less effective at night. Research has been undertaken to develop a systematic method of characterising image sensor performance in response to electrical noise in order to improve the design and performance of automotive cameras in low light scenarios. The root cause of image row noise has been established and a mathematical algorithm for determining the magnitude of row noise in an image has been devised. An automated characterisation method has been developed to allow performance characterisation in response to a large frequency spectrum of electrical noise on the image sensor power supply. Various strategies of improving image sensor performance for low light applications have also been proposed from the research outcomes.      
### 17.Deep Metric Learning-based Image Retrieval System for Chest Radiograph and its Clinical Applications in COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2012.03663.pdf)
>  In recent years, deep learning-based image analysis methods have been widely applied in computer-aided detection, diagnosis and prognosis, and has shown its value during the public health crisis of the novel coronavirus disease 2019 (COVID-19) pandemic. Chest radiograph (CXR) has been playing a crucial role in COVID-19 patient triaging, diagnosing and monitoring, particularly in the United States. Considering the mixed and unspecific signals in CXR, an image retrieval model of CXR that provides both similar images and associated clinical information can be more clinically meaningful than a direct image diagnostic model. In this work we develop a novel CXR image retrieval model based on deep metric learning. Unlike traditional diagnostic models which aims at learning the direct mapping from images to labels, the proposed model aims at learning the optimized embedding space of images, where images with the same labels and similar contents are pulled together. It utilizes multi-similarity loss with hard-mining sampling strategy and attention mechanism to learn the optimized embedding space, and provides similar images to the query image. The model is trained and validated on an international multi-site COVID-19 dataset collected from 3 different sources. Experimental results of COVID-19 image retrieval and diagnosis tasks show that the proposed model can serve as a robust solution for CXR analysis and patient management for COVID-19. The model is also tested on its transferability on a different clinical decision support task, where the pre-trained model is applied to extract image features from a new dataset without any further training. These results demonstrate our deep metric learning based image retrieval model is highly efficient in the CXR retrieval, diagnosis and prognosis, and thus has great clinical value for the treatment and management of COVID-19 patients.      
### 18.Multicell Power Control under Rate Constraints with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.03655.pdf)
>  In the paper we study a deep learning based method to solve the multicell power control problem for sum rate maximization subject to per-user rate constraints and per-base station (BS) power constraints. The core difficulty of this problem is how to ensure that the learned power control results by the deep neural network (DNN) satisfy the per-user rate constraints. To tackle the difficulty, we propose to cascade a projection block after a traditional DNN, which projects the infeasible power control results onto the constraint set. The projection block is designed based on a geometrical interpretation of the constraints, which is of low complexity, meeting the real-time requirement of online applications. Explicit-form expression of the backpropagated gradient is derived for the proposed projection block, with which the DNN can be trained to directly maximize the sum rate via unsupervised learning. We also develop a heuristic implementation of the projection block to reduce the size of DNN. Simulation results demonstrate the advantages of the proposed method over existing deep learning and numerical optimization~methods, and show the robustness of the proposed method with the model mismatch between training and testing~datasets.      
### 19.IRS-Assisted Massive MIMO-NOMA Networks: Exploiting Wave Polarization  [ :arrow_down: ](https://arxiv.org/pdf/2012.03639.pdf)
>  A dual-polarized intelligent reflecting surface (IRS) can contribute to a better multiplexing of interfering wireless users. In this paper, we use this feature to improve the performance of dual-polarized massive multiple-input multiple-output (MIMO) with non-orthogonal multiple access (NOMA) under imperfect successive interference cancellation (SIC). By considering the downlink of a multi-cluster scenario, the IRSs assist the base station (BS) to multiplex subsets of users in the polarization domain. Our novel strategy alleviates the impact of imperfect SIC and enables users to exploit polarization diversity with near-zero inter-subset interference. To this end, the IRSs are optimized to mitigate transmissions originated at the BS from the interfering polarization. The formulated optimization is transformed into quadratic constrained quadratic sub-problems, which makes it possible to obtain the optimal solution via interior-points methods. We also derive analytically a closed-form expression for the users' ergodic rates by considering large numbers of reflecting elements. This is followed by representative simulation examples and comprehensive discussions. The results show that when the IRSs are large enough, the proposed scheme always outperforms conventional massive MIMO-NOMA and MIMO-OMA systems even if SIC error propagation is present. It is also confirmed that dual-polarized IRSs can make cross-polar transmissions beneficial to the users, allowing them to improve their performance through diversity.      
### 20.Noise2Kernel: Adaptive Self-Supervised Blind Denoising using a Dilated Convolutional Kernel Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2012.03623.pdf)
>  With the advent of recent advances in unsupervised learning, efficient training of a deep network for image denoising without pairs of noisy and clean images has become feasible. However, most current unsupervised denoising methods are built on the assumption of zero-mean noise under the signal-independent condition. This assumption causes blind denoising techniques to suffer brightness shifting problems on images that are greatly corrupted by extreme noise such as salt-and-pepper noise. Moreover, most blind denoising methods require a random masking scheme for training to ensure the invariance of the denoising process. In this paper, we propose a dilated convolutional network that satisfies an invariant property, allowing efficient kernel-based training without random masking. We also propose an adaptive self-supervision loss to circumvent the requirement of zero-mean constraint, which is specifically effective in removing salt-and-pepper or hybrid noise where a prior knowledge of noise statistics is not readily available. We demonstrate the efficacy of the proposed method by comparing it with state-of-the-art denoising methods using various examples.      
### 21.Intelligent Reflecting Surface Aided Multi-Cell NOMA Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.03611.pdf)
>  This paper proposes a novel framework of resource allocation in intelligent reflecting surface (IRS) aided multi-cell non-orthogonal multiple access (NOMA) networks, where a sum-rate maximization problem is formulated. To address this challenging mixed-integer non-linear problem, we decompose it into an optimization problem (P1) with continuous variables and a matching problem (P2) with integer variables. For the non-convex optimization problem (P1), iterative algorithms are proposed for allocating transmit power, designing reflection matrix, and determining decoding order by invoking relaxation methods such as convex upper bound substitution, successive convex approximation and semidefinite relaxation. For the combinational problem (P2), swap matching-based algorithms are proposed to achieve a two-sided exchange-stable state among users, BSs and subchannels. Numerical results are provided for demonstrating that the sum-rate of the NOMA networks is capable of being enhanced with the aid of the IRS.      
### 22.On the characterization of butterfly and multi-loop hysteresis behavior  [ :arrow_down: ](https://arxiv.org/pdf/2012.03605.pdf)
>  While it is widely used to represent hysteresis phenomena with unidirectional-oriented loops, we study in this paper the use of Preisach operator for describing hysteresis behavior with multidirectional-oriented loops. This complex hysteresis behavior is commonly found in advanced materials, such as, shape-memory alloys or piezoelectric materials, that are used for high-precision sensor and actuator systems. We provide characterization of the Preisach operators exhibiting such input-output behaviors and we show the richness of the operators that are capable of producing intricate loops.      
### 23.Efficient Kernel based Matched Filter Approach for Segmentation of Retinal Blood Vessels  [ :arrow_down: ](https://arxiv.org/pdf/2012.03601.pdf)
>  Retinal blood vessels structure contains information about diseases like obesity, diabetes, hypertension and glaucoma. This information is very useful in identification and treatment of these fatal diseases. To obtain this information, there is need to segment these retinal vessels. Many kernel based methods have been given for segmentation of retinal vessels but their kernels are not appropriate to vessel profile cause poor performance. To overcome this, a new and efficient kernel based matched filter approach has been proposed. The new matched filter is used to generate the matched filter response (MFR) image. We have applied Otsu thresholding method on obtained MFR image to extract the vessels. We have conducted extensive experiments to choose best value of parameters for the proposed matched filter kernel. The proposed approach has examined and validated on two online available DRIVE and STARE datasets. The proposed approach has specificity 98.50%, 98.23% and accuracy 95.77 %, 95.13% for DRIVE and STARE dataset respectively. Obtained results confirm that the proposed method has better performance than others. The reason behind increased performance is due to appropriate proposed kernel which matches retinal blood vessel profile more accurately.      
### 24.Towards end-to-end speech enhancement with a variational U-Net architecture  [ :arrow_down: ](https://arxiv.org/pdf/2012.03594.pdf)
>  In this paper, we investigate the viability of a variational U-Net architecture for denoising of single-channel audio data. Deep network speech enhancement systems commonly aim to estimate filter masks, or opt to skip preprocessing steps to directly work on the waveform signal, potentially neglecting relationships across higher dimensional spectro-temporal features. We study the adoption of a probabilistic bottleneck, as well as dilated convolutions, into the classic U-Net architecture. Evaluation of a number of network variants is carried out using signal-to-distortion ratio and perceptual model scores, with audio data including known and unknown noise types as well as reverberation. Our experiments show that the residual (skip) connections in the proposed system are required for successful end-to-end signal enhancement, i.e., without filter mask estimation. Further, they indicate a slight advantage of the variational U-Net architecture over its non-variational version in terms of signal enhancement performance under reverberant conditions. Specifically, PESQ scores show increases of 0.28 and 0.49 in reverberant and non-reverberant scenes, respectively. Anecdotal evidence points to improved suppression of impulsive noise sources with the variational end-to-end U-Net compared to the recurrent mask estimation network baseline.      
### 25.Self-Supervision Closes the Gap Between Weak and Strong Supervision in Histology  [ :arrow_down: ](https://arxiv.org/pdf/2012.03583.pdf)
>  One of the biggest challenges for applying machine learning to histopathology is weak supervision: whole-slide images have billions of pixels yet often only one global label. The state of the art therefore relies on strongly-supervised model training using additional local annotations from domain experts. However, in the absence of detailed annotations, most weakly-supervised approaches depend on a frozen feature extractor pre-trained on ImageNet. We identify this as a key weakness and propose to train an in-domain feature extractor on histology images using MoCo v2, a recent self-supervised learning algorithm. Experimental results on Camelyon16 and TCGA show that the proposed extractor greatly outperforms its ImageNet counterpart. In particular, our results improve the weakly-supervised state of the art on Camelyon16 from 91.4% to 98.7% AUC, thereby closing the gap with strongly-supervised models that reach 99.3% AUC. Through these experiments, we demonstrate that feature extractors trained via self-supervised learning can act as drop-in replacements to significantly improve existing machine learning techniques in histology. Lastly, we show that the learned embedding space exhibits biologically meaningful separation of tissue structures.      
### 26.Robustness Investigation on Deep Learning CT Reconstruction for Real-Time Dose Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2012.03579.pdf)
>  In computed tomography (CT), automatic exposure control (AEC) is frequently used to reduce radiation dose exposure to patients. For organ-specific AEC, a preliminary CT reconstruction is necessary to estimate organ shapes for dose optimization, where only a few projections are allowed for real-time reconstruction. In this work, we investigate the performance of automated transform by manifold approximation (AUTOMAP) in such applications. For proof of concept, we investigate its performance on the MNIST dataset first, where the dataset containing all the 10 digits are randomly split into a training set and a test set. We train the AUTOMAP model for image reconstruction from 2 projections or 4 projections directly. The test results demonstrate that AUTOMAP is able to reconstruct most digits well with a false rate of 1.6% and 6.8% respectively. In our subsequent experiment, the MNIST dataset is split in a way that the training set contains 9 digits only while the test set contains the excluded digit only, for instance "2". In the test results, the digit "2"s are falsely predicted as "3" or "5" when using 2 projections for reconstruction, reaching a false rate of 94.4%. For the application in medical images, AUTOMAP is also trained on patients' CT images. The test images reach an average root-mean-square error of 290 HU. Although the coarse body outlines are well reconstructed, some organs are misshaped.      
### 27.EfficientTTS: An Efficient and High-Quality Text-to-Speech Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2012.03500.pdf)
>  In this work, we address the Text-to-Speech (TTS) task by proposing a non-autoregressive architecture called EfficientTTS. Unlike the dominant non-autoregressive TTS models, which are trained with the need of external aligners, EfficientTTS optimizes all its parameters with a stable, end-to-end training procedure, while allowing for synthesizing high quality speech in a fast and efficient manner. EfficientTTS is motivated by a new monotonic alignment modeling approach (also introduced in this work), which specifies monotonic constraints to the sequence alignment with almost no increase of computation. By combining EfficientTTS with different feed-forward network structures, we develop a family of TTS models, including both text-to-melspectrogram and text-to-waveform networks. We experimentally show that the proposed models significantly outperform counterpart models such as Tacotron 2 and Glow-TTS in terms of speech quality, training efficiency and synthesis speed, while still producing the speeches of strong robustness and great diversity. In addition, we demonstrate that proposed approach can be easily extended to autoregressive models such as Tacotron 2.      
### 28.Deterministic Scheduling for Low-latency Wireless Transmissions with Continuous Channel States  [ :arrow_down: ](https://arxiv.org/pdf/2012.03440.pdf)
>  High energy efficiency and low latency have always been the significant goals pursued by the designer of wireless networks. One efficient way to achieve these goals is cross-layer scheduling based on the system states in different layers, such as queuing state and channel state. However, most existing works in cross-layer design focus on the scheduling based on the discrete channel state. Little attention is paid to considering the continuous channel state that is closer to the practical scenario. Therefore, in this paper, we study the optimal cross-layer scheduling policy on data transmission in a single communication link with continuous state-space of channel. The aim of scheduling is to minimize the average power for data transmission when the average delay is constrained. More specifically, the optimal cross-layer scheduling problem was formulated as a variational problem. Based on the variational problem, we show the optimality of the deterministic scheduling policy through a constructive proof. The optimality of the deterministic policy allows us to compress the searching space from the probabilistic policies to the deterministic policies.      
### 29.Deep Learning Based Signal Enhancement of Low-Resolution Accelerometer for Fall Detection Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.03426.pdf)
>  In the last two decades, fall detection (FD) systems have been developed as a popular assistive technology. Such systems automatically detect critical fall events and immediately alert medical professionals or caregivers. To support long-term FD services, various power-saving strategies have been implemented. Among them, a reduced sampling rate is a common approach for an energy-efficient system in the real-world. However, the performance of FD systems is diminished owing to low-resolution (LR) accelerometer signals. To improve the detection accuracy with LR accelerometer signals, several technical challenges must be considered, including misalignment, mismatch of effective features, and the degradation effects. In this work, a deep-learning-based accelerometer signal enhancement (ASE) model is proposed to improve the detection performance of LR-FD systems. This proposed model reconstructs high-resolution (HR) signals from the LR signals by learning the relationship between the LR and HR signals. The results show that the FD system using support vector machine and the proposed ASE model at an extremely low sampling rate (sampling rate &lt; 2 Hz) achieved 97.34% and 90.52% accuracies in the SisFall and FallAllD datasets, respectively, while those without ASE models only achieved 95.92% and 87.47% accuracies in the SisFall and FallAllD datasets, respectively. This study demonstrates that the ASE model helps the FD systems tackle the technical challenges of LR signals and achieve better detection performance.      
### 30.MLS: A Large-Scale Multilingual Dataset for Speech Research  [ :arrow_down: ](https://arxiv.org/pdf/2012.03411.pdf)
>  This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at <a class="link-external link-http" href="http://www.openslr.org" rel="external noopener nofollow">this http URL</a>.      
### 31.Low-Latency Asynchronous Logic Design for Inference at the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2012.03402.pdf)
>  Modern internet of things (IoT) devices leverage machine learning inference using sensed data on-device rather than offloading them to the cloud. Commonly known as inference at-the-edge, this gives many benefits to the users, including personalization and security. However, such applications demand high energy efficiency and robustness. In this paper we propose a method for reduced area and power overhead of self-timed early-propagative asynchronous inference circuits, designed using the principles of learning automata. Due to natural resilience to timing as well as logic underpinning, the circuits are tolerant to variations in environment and supply voltage whilst enabling the lowest possible latency. Our method is exemplified through an inference datapath for a low power machine learning application. The circuit builds on the Tsetlin machine algorithm further enhancing its energy efficiency. Average latency of the proposed circuit is reduced by 10x compared with the synchronous implementation whilst maintaining similar area. Robustness of the proposed circuit is proven through post-synthesis simulation with 0.25 V to 1.2 V supply. Functional correctness is maintained and latency scales with gate delay as voltage is decreased.      
### 32.Traffic Assignment Problem for Pedestrian Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.03389.pdf)
>  The estimation of pedestrian traffic in urban areas is often performed with computationally intensive microscopic models that usually suffer from scalability issues in large-scale walking networks. In this study, we present a new macroscopic user equilibrium traffic assignment problem (UE-pTAP) framework for pedestrian networks while taking into account fundamental microscopic properties such as self-organization in bidirectional streams and stochastic walking travel times. We propose four different types of pedestrian volume-delay functions (pVDFs), calibrate them with empirical data, and discuss their implications on the existence and uniqueness of the assignment solution. We demonstrate the applicability of the developed UE-pTAP framework in a small network as well as a larger scale network of Sydney footpaths.      
### 33.Linear Reduced Order Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2012.03384.pdf)
>  Model predictive controllers leverage system dynamics models to solve constrained optimal control problems. However, computational requirements for real-time control have limited their use to systems with low-dimensional models. Nevertheless many systems naturally produce high-dimensional models, such as those modeled by partial differential equations that when discretized can result in models with thousands to millions of dimensions. In such cases the use of reduced order models (ROMs) can significantly reduce computational requirements, but model approximation error must be considered to guarantee controller performance. In this work a reduced order model predictive control (ROMPC) scheme is proposed to solve robust, output feedback, constrained optimal control problems for high-dimensional linear systems. Computational efficiency is obtained by leveraging ROMs obtained via projection-based techniques, and guarantees on robust constraint satisfaction and stability are provided. Performance of the approach is demonstrated in simulation for several examples, including an aircraft control problem leveraging an inviscid computational fluid dynamics model with dimension 998,930.      
### 34.Spatio-Temporal Graph Scattering Transform  [ :arrow_down: ](https://arxiv.org/pdf/2012.03363.pdf)
>  Although spatio-temporal graph neural networks have achieved great empirical success in handling multiple correlated time series, they may be impractical in some real-world scenarios due to a lack of sufficient high-quality training data. Furthermore, spatio-temporal graph neural networks lack theoretical interpretation. To address these issues, we put forth a novel mathematically designed framework to analyze spatio-temporal data. Our proposed spatio-temporal graph scattering transform (ST-GST) extends traditional scattering transforms to the spatio-temporal domain. It performs iterative applications of spatio-temporal graph wavelets and nonlinear activation functions, which can be viewed as a forward pass of spatio-temporal graph convolutional networks without training. Since all the filter coefficients in ST-GST are mathematically designed, it is promising for the real-world scenarios with limited training data, and also allows for a theoretical analysis, which shows that the proposed ST-GST is stable to small perturbations of input signals and structures. Finally, our experiments show that i) ST-GST outperforms spatio-temporal graph convolutional networks by an increase of 35% in accuracy for MSR Action3D dataset; ii) it is better and computationally more efficient to design the transform based on separable spatio-temporal graphs than the joint ones; and iii) the nonlinearity in ST-GST is critical to empirical performance.      
### 35.An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.03352.pdf)
>  Organ segmentation in CT volumes is an important pre-processing step in many computer assisted intervention and diagnosis methods. In recent years, convolutional neural networks have dominated the state of the art in this task. However, since this problem presents a challenging environment due to high variability in the organ's shape and similarity between tissues, the generation of false negative and false positive regions in the output segmentation is a common issue. Recent works have shown that the uncertainty analysis of the model can provide us with useful information about potential errors in the segmentation. In this context, we proposed a segmentation refinement method based on uncertainty analysis and graph convolutional networks. We employ the uncertainty levels of the convolutional network in a particular input volume to formulate a semi-supervised graph learning problem that is solved by training a graph convolutional network. To test our method we refine the initial output of a 2D U-Net. We validate our framework with the NIH pancreas dataset and the spleen dataset of the medical segmentation decathlon. We show that our method outperforms the state-of-the-art CRF refinement method by improving the dice score by 1% for the pancreas and 2% for spleen, with respect to the original U-Net's prediction. Finally, we perform a sensitivity analysis on the parameters of our proposal and discuss the applicability to other CNN architectures, the results, and current limitations of the model for future work in this research direction. For reproducibility purposes, we make our code publicly available at <a class="link-external link-https" href="https://github.com/rodsom22/gcn_refinement" rel="external noopener nofollow">this https URL</a>.      
### 36.Ellipsoidal constrained state estimation in presence of bounded disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2012.03267.pdf)
>  This contribution proposes a recursive, input-to-state stable and ready-to-use online algorithm for the state estimation of linear discrete-time systems with unknown but bounded disturbances corrupting both the state and the sporadic measurement vectors and subject to linear inequality and equality constraints on the state vector. Two set representation techniques are used: the state vector is characterized by an ellipsoid whereas the disturbances vectors are bounded by possibly degenerate zonotopes. The proposed algorithm is decomposed into two steps: time updating and observation updating that uses a switching estimation gain.      
### 37.Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data  [ :arrow_down: ](https://arxiv.org/pdf/2012.03255.pdf)
>  Recent work has shown impressive results on data-driven defocus deblurring using the two-image views available on modern dual-pixel (DP) sensors. One significant challenge in this line of research is access to DP data. Despite many cameras having DP sensors, only a limited number provide access to the low-level DP sensor images. In addition, capturing training data for defocus deblurring involves a time-consuming and tedious setup requiring the camera's aperture to be adjusted. Some cameras with DP sensors (e.g., smartphones) do not have adjustable apertures, further limiting the ability to produce the necessary training data. We address the data capture bottleneck by proposing a procedure to generate realistic DP data synthetically. Our synthesis approach mimics the optical image formation found on DP sensors and can be applied to virtual scenes rendered with standard computer software. Leveraging these realistic synthetic DP images, we introduce a new recurrent convolutional network (RCN) architecture that can improve deblurring results and is suitable for use with single-frame and multi-frame data captured by DP sensors. Finally, we show that our synthetic DP data is useful for training DNN models targeting video deblurring applications where access to DP data remains challenging.      
### 38.Esophageal Tumor Segmentation in CT Images using a 3D Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2012.03242.pdf)
>  Manual or automatic delineation of the esophageal tumor in CT images is known to be very challenging. This is due to the low contrast between the tumor and adjacent tissues, the anatomical variation of the esophagus, as well as the occasional presence of foreign bodies (e.g. feeding tubes). Physicians therefore usually exploit additional knowledge such as endoscopic findings, clinical history, additional imaging modalities like PET scans. Achieving his additional information is time-consuming, while the results are error-prone and might lead to non-deterministic results. In this paper we aim to investigate if and to what extent a simplified clinical workflow based on CT alone, allows one to automatically segment the esophageal tumor with sufficient quality. For this purpose, we present a fully automatic end-to-end esophageal tumor segmentation method based on convolutional neural networks (CNNs). The proposed network, called Dilated Dense Attention Unet (DDAUnet), leverages spatial and channel attention gates in each dense block to selectively concentrate on determinant feature maps and regions. Dilated convolutional layers are used to manage GPU memory and increase the network receptive field. We collected a dataset of 792 scans from 288 distinct patients including varying anatomies with \mbox{air pockets}, feeding tubes and proximal tumors. Repeatability and reproducibility studies were conducted for three distinct splits of training and validation sets. The proposed network achieved a $\mathrm{DSC}$ value of $0.79 \pm 0.20$, a mean surface distance of $5.4 \pm 20.2mm$ and $95\%$ Hausdorff distance of $14.7 \pm 25.0mm$ for 287 test scans, demonstrating promising results with a simplified clinical workflow based on CT alone. Our code is publicly available via \url{<a class="link-external link-https" href="https://github.com/yousefis/DenseUnet_Esophagus_Segmentation" rel="external noopener nofollow">this https URL</a>}.      
### 39.Modified Auto Regressive Technique for Univariate Time Series Prediction of Solar Irradiance  [ :arrow_down: ](https://arxiv.org/pdf/2012.03215.pdf)
>  The integration of renewable resources has increased in power generation as a means to reduce the fossil fuel usage and mitigate its adverse effects on the environment. However, renewables like solar energy are stochastic in nature due to its high dependency on weather patterns. This uncertainty vastly diminishes the benefit of solar panel integration and increases the operating costs due to larger energy reserve requirement. To address this issue, a Modified Auto Regressive model, a Convolutional Neural Network and a Long Short Term Memory neural network that can accurately predict the solar irradiance are proposed. The proposed techniques are compared against each other by means of multiple error metrics of validation. The Modified Auto Regressive model has a mean absolute percentage error of 14.2%, 19.9% and 22.4% for 10 minute, 30 minute and 1 hour prediction horizons. Therefore, the Modified Auto Regressive model is proposed as the most robust method, assimilating the state of the art neural networks for the solar forecasting problem.      
### 40.Scheduling of Separable Mobile Energy Storage Systems with Mobile Generators and Fuel Tankers to Boost Distribution System Resilience  [ :arrow_down: ](https://arxiv.org/pdf/2012.03209.pdf)
>  Mobile energy resources (MERs) have been shown to boost DS resilience effectively in recent years. In this paper, we propose a novel idea, the separable mobile energy storage system (SMESS), as an attempt to further extend the flexibility of MER applications. "Separable" denotes that the carrier and the energy storage modules are treated as independent parts, which allows the carrier to carry multiple modules and scatter them independently throughout the DS. The constraints for scheduling SMESSs involving carriers and modules are derived based upon the interactive behavior among them and the DS. In addition, the fuel delivery issue of feeding mobile emergency generators (MEGs), which was usually bypassed in previous studies involving the scheduling of MEGs, is also considered and modeled. SMESSs, MEGs, and fuel tankers (FTs) are then jointly routed and scheduled, along with the dynamic DS reconfiguration, for DS service restoration by integrating them in a mixed-integer linear programming (MILP) model. Finally, the test is conducted on a modified IEEE 33-node test system, and results verify the effectiveness of the model in boosting DS resilience.      
### 41.Coverage Probability Analysis of IRS-Aided Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.03171.pdf)
>  The intelligent reflective surface (IRS) technology has received many interests in recent years, thanks to its potential uses in future wireless communications, in which one of the promising use cases is to widen coverage, especially in the line-of-sight-blocked scenarios. Therefore, it is critical to analyze the corresponding coverage probability of IRS-aided communication systems. To our best knowledge, however, previous works focusing on this issue are very limited. In this paper, we analyze the coverage probability under the Rayleigh fading channel, taking the number and size of the array elements into consideration. We first derive the exact closed-form of coverage probability for the unit element. Afterward, with the method of moment matching, the approximation of the coverage probability can be formulated as the ratio of upper incomplete Gamma function and Gamma function, allowing an arbitrary number of elements. Finally, we comprehensively evaluate the impacts of essential factors on the coverage probability, such as the coefficient of fading channel, the number and size of the element, and the angle of incidence. Overall, the paper provides a succinct and general expression of coverage probability, which can be helpful in the performance evaluation and practical implementation of the IRS.      
### 42.Magnetless Circulators Based on Synthetic Angular-Momentum Bias: Recent Advances and Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.03164.pdf)
>  In this paper, we discuss recent progress in magnet-free non-reciprocal structures based on a synthetic form of angular momentum bias imparted via spatiotemporal modulation. We discuss how such components can support metrics of performance comparable with traditional magnetic-biased ferrite devices, while at the same time offering distinct advantages in terms of reduced size, weight, and cost due to the elimination of magnetic bias. We further provide an outlook on potential applications and future directions based on these components, ranging from wireless full-duplex communications to metasurfaces and topological insulators.      
### 43.Stabilizing Transient Disturbances With Utility-Scale Inverter-Based Resources  [ :arrow_down: ](https://arxiv.org/pdf/2012.03161.pdf)
>  This paper presents a trajectory tracking control strategy that modulates the active power injected by geographically distributed inverter-based resources to support transient stability. Each resource is independently controlled, and its response drives the local bus voltage angle toward a trajectory that tracks the angle of the center of inertia. The center-of-inertia angle is estimated in real time from wide-area measurements. The main objectives are to stabilize transient disturbances and increase the amount of power that can be safely transferred over key transmission paths without loss of synchronism. Here we envision the actuators as utility-scale energy storage systems; however, equivalent examples could be developed for partially-curtailed photovoltaic generation and/or Type 4 wind turbine generators. The strategy stems from a time-varying linearization of the equations of motion for a synchronous machine. The control action produces synchronizing torque in a special reference frame that accounts for the motion of the center of inertia. This drives the system states toward the desired trajectory and promotes rotor angle stability. For testing we employ a reduced-order dynamic model of the North American Western Interconnection. The results show that this approach improves system reliability and can increase capacity utilization on stability-limited transmission corridors.      
### 44.Multi-task Learning Based Spoofing-Robust Automatic Speaker Verification System  [ :arrow_down: ](https://arxiv.org/pdf/2012.03154.pdf)
>  Spoofing attacks posed by generating artificial speech can severely degrade the performance of a speaker verification system. Recently, many anti-spoofing countermeasures have been proposed for detecting varying types of attacks from synthetic speech to replay presentations. While there are numerous effective defenses reported on standalone anti-spoofing solutions, the integration for speaker verification and spoofing detection systems has obvious benefits. In this paper, we propose a spoofing-robust automatic speaker verification (SR-ASV) system for diverse attacks based on a multi-task learning architecture. This deep learning based model is jointly trained with time-frequency representations from utterances to provide recognition decisions for both tasks simultaneously. Compared with other state-of-the-art systems on the ASVspoof 2017 and 2019 corpora, a substantial improvement of the combined system under different spoofing conditions can be obtained.      
### 45.Using Machine Learning to Automate Mammogram Images Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.03151.pdf)
>  Breast cancer is the second leading cause of cancer-related death after lung cancer in women. Early detection of breast cancer in X-ray mammography is believed to have effectively reduced the mortality rate. However, a relatively high false positive rate and a low specificity in mammography technology still exist. In this work, a computer-aided automatic mammogram analysis system is proposed to process the mammogram images and automatically discriminate them as either normal or cancerous, consisting of three consecutive image processing, feature selection, and image classification stages. In designing the system, the discrete wavelet transforms (Daubechies 2, Daubechies 4, and Biorthogonal 6.8) and the Fourier cosine transform were first used to parse the mammogram images and extract statistical features. Then, an entropy-based feature selection method was implemented to reduce the number of features. Finally, different pattern recognition methods (including the Back-propagation Network, the Linear Discriminant Analysis, and the Naive Bayes Classifier) and a voting classification scheme were employed. The performance of each classification strategy was evaluated for sensitivity, specificity, and accuracy and for general performance using the Receiver Operating Curve. Our method is validated on the dataset from the Eastern Health in Newfoundland and Labrador of Canada. The experimental results demonstrated that the proposed automatic mammogram analysis system could effectively improve the classification performances.      
### 46.Development and Characterization of a Chest CT Atlas  [ :arrow_down: ](https://arxiv.org/pdf/2012.03124.pdf)
>  A major goal of lung cancer screening is to identify individuals with particular phenotypes that are associated with high risk of cancer. Identifying relevant phenotypes is complicated by the variation in body position and body composition. In the brain, standardized coordinate systems (e.g., atlases) have enabled separate consideration of local features from gross/global structure. To date, no analogous standard atlas has been presented to enable spatial mapping and harmonization in chest computational tomography (CT). In this paper, we propose a thoracic atlas built upon a large low dose CT (LDCT) database of lung cancer screening program. The study cohort includes 466 male and 387 female subjects with no screening detected malignancy (age 46-79 years, mean 64.9 years). To provide spatial mapping, we optimize a multi-stage inter-subject non-rigid registration pipeline for the entire thoracic space. We evaluate the optimized pipeline relative to two baselines with alternative non-rigid registration module: the same software with default parameters and an alternative software. We achieve a significant improvement in terms of registration success rate based on manual QA. For the entire study cohort, the optimized pipeline achieves a registration success rate of 91.7%. The application validity of the developed atlas is evaluated in terms of discriminative capability for different anatomic phenotypes, including body mass index (BMI), chronic obstructive pulmonary disease (COPD), and coronary artery calcification (CAC).      
### 47.Bayesian optimization assisted unsupervised learning for efficient intra-tumor partitioning in MRI and survival prediction for glioblastoma patients  [ :arrow_down: ](https://arxiv.org/pdf/2012.03115.pdf)
>  Glioblastoma is profoundly heterogeneous in microstructure and vasculature, which may lead to tumor regional diversity and distinct treatment response. Although successful in tumor sub-region segmentation and survival prediction, radiomics based on machine learning algorithms, is challenged by its robustness, due to the vague intermediate process and track changes. Also, the weak interpretability of the model poses challenges to clinical application. Here we proposed a machine learning framework to semi-automatically fine-tune the clustering algorithms and quantitatively identify stable sub-regions for reliable clinical survival prediction. Hyper-parameters are automatically determined by the global minimum of the trained Gaussian Process (GP) surrogate model through Bayesian optimization(BO) to alleviate the difficulty of tuning parameters for clinical researchers. To enhance the interpretability of the survival prediction model, we incorporated the prior knowledge of intra-tumoral heterogeneity, by segmenting tumor sub-regions and extracting sub-regional features. The results demonstrated that the global minimum of the trained GP surrogate can be used as sub-optimal hyper-parameter solutions for efficient. The sub-regions segmented based on physiological MRI can be applied to predict patient survival, which could enhance the clinical interpretability for the machine learning model.      
### 48.SPIRAP Wireless Uplink Random Access Protocol Using Spinal Code  [ :arrow_down: ](https://arxiv.org/pdf/2012.03100.pdf)
>  In this paper we present SPIRAP, SPinal Random Access Protocol, a new method for multiuser detection over wireless fading channel. SPIRAP combines sequential decoding with rateless Spinal code. SPIRAP appears to be an efficient protocol for transmitting small packets in a minimally controlled network and can be attractive for the Internet of Things (IOT) applications. The algorithm applies equivalently to any other rateless code, we chose Spinal for its good performance. We show that SPIRAP may achieve higher rate than TDMA and ALOHA in some cases and without the need for users synchronization.      
### 49.deSpeckNet: Generalizing Deep Learning Based SAR Image Despeckling  [ :arrow_down: ](https://arxiv.org/pdf/2012.03066.pdf)
>  Deep learning (DL) has proven to be a suitable approach for despeckling synthetic aperture radar (SAR) images. So far, most DL models are trained to reduce speckle that follows a particular distribution, either using simulated noise or a specific set of real SAR images, limiting the applicability of these methods for real SAR images with unknown noise statistics. In this paper, we present a DL method, deSpeckNet1, that estimates the speckle noise distribution and the despeckled image simultaneously. Since it does not depend on a specific noise model, deSpeckNet generalizes well across SAR acquisitions in a variety of landcover conditions. We evaluated the performance of deSpeckNet on single polarized Sentinel-1 images acquired in Indonesia, The Democratic Republic of Congo and The Netherlands, a single polarized ALOS-2/PALSAR-2 image acquired in Japan and an Iceye X2 image acquired in Germany. In all cases, deSpeckNet was able to effectively reduce speckle and restore      
### 50.An Orthogonal Basis Approach to Formation Shape Control (Extended Version)  [ :arrow_down: ](https://arxiv.org/pdf/2012.03064.pdf)
>  In this paper, we propose a novel approach to the problem of augmenting distance-based formation controllers with a secondary constraint for the purpose of preventing 3D formation ambiguities. Specifically, we introduce three controlled variables that form an orthogonal space and uniquely characterize a tetrahedron formation in 3D. This orthogonal space incorporates constraints on the inter-agent distances and the signed volume of tetrahedron substructures. The formation is modeled using a directed graph with a leader-follower type configuration and single-integrator dynamics. We show that the proposed decentralized formation controller ensures the \textit{global} asymptotic stability and the local exponential stability of the desired formation for an \textit{n}-agent system with no ambiguities. Unlike previous work, this result is achieved without conditions on the tetrahedrons that form the desired formation or on the control gains.      
### 51.A Transactive Retail Market Mechanism for Active Distribution Network Integrated with Large-scale Distributed Energy Resources  [ :arrow_down: ](https://arxiv.org/pdf/2012.03034.pdf)
>  The burgeoning integration of distributed energy resources (DER) poses new challenges for the economic and safe operation of the electricity system. The current distribution-side policy is largely based on mandatory regulations and incentives, rather than the design of a competitive market mechanism to arouse DERs to freely compete in the retail market. To address this issue, we proposed a transactive distribution retail market mechanism to attract the active participation of profit-driven DER retailers in a deregulated way. Considering the limited competitive property of the retail market, a bi-level DSO-dominated framework is constructed to simulate the virtual game between distribution system operator (DSO) and DER retailers. Specifically, the flexible interval pricing of retailers is modeled as a series of binary revenue constraints to influence the DSO's centralized economic dispatch decisions, where the time-varying distribution locational marginal price is adopted to settle energy activities at different locations over the time horizon. Due to the uncertain power flow path during the coordinated decision-making process, we proposed a general undirected second-order cone-based AC radial power flow model and provided sufficient conditions to ensure its exactness. Also, we applied a series of approximation and relaxation techniques to transform the bi-level mixed-integer quadratic framework with highly discrete induced domains into a solvable mixed-integer semidefinite programming problem. It is demonstrated in the case study that the proposed mechanism can not only improve market efficiency but also eliminate market power and the resulting market failures.      
### 52.Automatic Segmentation and Location Learning of Neonatal Cerebral Ventricles in 3D Ultrasound Data Combining CNN and CPPN  [ :arrow_down: ](https://arxiv.org/pdf/2012.03014.pdf)
>  Preterm neonates are highly likely to suffer from ventriculomegaly, a dilation of the Cerebral Ventricular System (CVS). This condition can develop into life-threatening hydrocephalus and is correlated with future neuro-developmental impairments. Consequently, it must be detected and monitored by physicians. In clinical routing, manual 2D measurements are performed on 2D ultrasound (US) images to estimate the CVS volume but this practice is imprecise due to the unavailability of 3D information. A way to tackle this problem would be to develop automatic CVS segmentation algorithms for 3D US data. In this paper, we investigate the potential of 2D and 3D Convolutional Neural Networks (CNN) to solve this complex task and propose to use Compositional Pattern Producing Network (CPPN) to enable the CNNs to learn CVS location. Our database was composed of 25 3D US volumes collected on 21 preterm nenonates at the age of $35.8 \pm 1.6$ gestational weeks. We found that the CPPN enables to encode CVS location, which increases the accuracy of the CNNs when they have few layers. Accuracy of the 2D and 3D CNNs reached intraobserver variability (IOV) in the case of dilated ventricles with Dice of $0.893 \pm 0.008$ and $0.886 \pm 0.004$ respectively (IOV = $0.898 \pm 0.008$) and with volume errors of $0.45 \pm 0.42$ cm$^3$ and $0.36 \pm 0.24$ cm$^3$ respectively (IOV = $0.41 \pm 0.05$ cm$^3$). 3D CNNs were more accurate than 2D CNNs in the case of normal ventricles with Dice of $0.797 \pm 0.041$ against $0.776 \pm 0.038$ (IOV = $0.816 \pm 0.009$) and volume errors of $0.35 \pm 0.29$ cm$^3$ against $0.35 \pm 0.24$ cm$^3$ (IOV = $0.2 \pm 0.11$ cm$^3$). The best segmentation time of volumes of size $320 \times 320 \times 320$ was obtained by a 2D CNN in $3.5 \pm 0.2$ s.      
### 53.Fusing Optical and SAR time series for LAI gap filling with multioutput Gaussian processes  [ :arrow_down: ](https://arxiv.org/pdf/2012.02998.pdf)
>  The availability of satellite optical information is often hampered by the natural presence of clouds, which can be problematic for many applications. Persistent clouds over agricultural fields can mask key stages of crop growth, leading to unreliable yield predictions. Synthetic Aperture Radar (SAR) provides all-weather imagery which can potentially overcome this limitation, but given its high and distinct sensitivity to different surface properties, the fusion of SAR and optical data still remains an open challenge. In this work, we propose the use of Multi-Output Gaussian Process (MOGP) regression, a machine learning technique that learns automatically the statistical relationships among multisensor time series, to detect vegetated areas over which the synergy between SAR-optical imageries is profitable. For this purpose, we use the Sentinel-1 Radar Vegetation Index (RVI) and Sentinel-2 Leaf Area Index (LAI) time series over a study area in north west of the Iberian peninsula. Through a physical interpretation of MOGP trained models, we show its ability to provide estimations of LAI even over cloudy periods using the information shared with RVI, which guarantees the solution keeps always tied to real measurements. Results demonstrate the advantage of MOGP especially for long data gaps, where optical-based methods notoriously fail. The leave-one-image-out assessment technique applied to the whole vegetation cover shows MOGP predictions improve standard GP estimations over short-time gaps (R$^2$ of 74\% vs 68\%, RMSE of 0.4 vs 0.44 $[m^2m^{-2}]$) and especially over long-time gaps (R$^2$ of 33\% vs 12\%, RMSE of 0.5 vs 1.09 $[m^2m^{-2}]$).      
### 54.A Mechanical System Inspired Microscopic Traffic Model: Modeling, Analysis, and Validation  [ :arrow_down: ](https://arxiv.org/pdf/2012.02948.pdf)
>  In this paper, we develop a mechanical system inspired microscopic traffic model to characterize the longitudinal interaction dynamics among a chain of vehicles. In particular, we extend our prior work on mass-spring-damper-clutch based car-following model between two vehicles to multi-vehicle scenario. This model can naturally capture the driver's tendency to maintain the same speed as the vehicle ahead while keeping a (speed-dependent) desired spacing. It is also capable of characterizing the impact of the following vehicle on the preceding vehicle, which is generally neglected in existing models. A new string stability criterion is defined for the considered multi-vehicle dynamics, and stability analysis is performed on the system parameters and time delays. An efficient online parameter identification algorithm, sequential recursive least squares with inverse QR decomposition (SRLS-IQR), is developed to estimate the driving-related model parameters. These real-time estimated parameters can be employed in advanced longitudinal control systems to enable accurate prediction of vehicle trajectories for improved safety and fuel efficiency. The proposed model and the parameter identification algorithm are validated on NGSIM, a naturalistic driving dataset, as well as our own connected vehicle driving data. Promising performance is demonstrated.      
### 55.Experimental Implementation of an Adaptive Digital Autopilot  [ :arrow_down: ](https://arxiv.org/pdf/2012.02896.pdf)
>  This paper develops an adaptive digital autopilot for quadcopters and presents experimental results. The adaptive digital autopilot is constructed by augmenting the PX4 autopilot control system architecture with adaptive digital control laws based on retrospective cost adaptive control (RCAC). In order to investigate the performance of the adaptive digital autopilot, the default gains of the fixed-gain autopilot are scaled by a small factor, which severely degrades its performance. This scenario thus provides a venue for determining the ability of the adaptive digital autopilot to compensate for the detuned fixed-gain autopilot. The adaptive digital autopilot is tested in simulation and physical flight tests, and the resulting performance improvements are examined.      
### 56.Hybrid Beamforming for mm-Wave Massive MIMO Systems with Partially Connected RF Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2012.02889.pdf)
>  To satisfy the capacity requirements of future mobile systems, under-utilized millimeter wave frequencies can be efficiently exploited by employing massive MIMO technology with highly directive beamforming. Hybrid analog-digital beamforming has been recognised as a promising approach for large-scale MIMO implementations with a reduced number of costly and power-hungry RF chains. In comparison to fully connected architecture, hybrid beamforming (HBF) with partially connected RF architecture is particularly appealing for the practical implementation due to less complex RF power division and combining networks. In this paper, we first formulate single- and multi-user rate maximization problems as weighted minimum mean square error (WMMSE) and derive solutions for hybrid beamformers using alternating optimization. The algorithms are designed for the full-array- and sub-array-based processing strategies of partially connected HBF architecture. In addition to the rate maximizing WMMSE solutions, we propose lower complexity sub-array-based zero-forcing algorithms. The performance of the proposed algorithms is evaluated in two different channel models, i.e., a simple geometric model and a realistic statistical millimeter wave model known as NYUSIM. The performance results of the WMMSE HBF algorithms are meant to reveal the potential of partially connected HBF and serve as upper bounds for lower complexity methods. Numerical results imply that properly designed partially connected HBF has the potential to provide an good compromise between hardware complexity and system performance in comparison to fully digital beamforming.      
### 57.A Hierarchical Deep Actor-Critic Learning Method for Joint Distribution System State Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2012.02880.pdf)
>  Due to increasing penetration of volatile distributed photovoltaic (PV) resources, real-time monitoring of customers at the grid-edge has become a critical task. However, this requires solving the distribution system state estimation (DSSE) jointly for both primary and secondary levels of distribution grids, which is computationally complex and lacks scalability to large systems. To achieve near real-time solutions for DSSE, we present a novel hierarchical reinforcement learning-aided framework: at the first layer, a weighted least squares (WLS) algorithm solves the DSSE over primary medium-voltage feeders; at the second layer, deep actor-critic (A-C) modules are trained for each secondary transformer using measurement residuals to estimate the states of low-voltage circuits and capture the impact of PVs at the grid-edge. While the A-C parameter learning process takes place offline, the trained A-C modules are deployed online for fast secondary grid state estimation; this is the key factor in scalability and computational efficiency of the framework. To maintain monitoring accuracy, the two levels exchange boundary information with each other at the secondary nodes, including transformer voltages (first layer to second layer) and active/reactive total power injection (second layer to first layer). This interactive information passing strategy results in a closed-loop structure that is able to track optimal solutions at both layers in few iterations. Moreover, our model can handle the topology changes using the Jacobian matrices of the first layer. We have performed numerical experiments using real utility data and feeder models to verify the performance of the proposed framework.      
### 58.Multi-Source Data-Driven Outage Location in Distribution Systems Using Probabilistic Graph Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.02877.pdf)
>  Efficient outage location is critical to enhancing the resilience of power systems. However, accurate outage location requires combining massive evidence received from diverse data sources, including smart meter (SM) signals, customer trouble calls, social media messages, weather data, vegetation information, and physical parameters of the network. This is a computationally complex task due to the high dimensionality of data in distribution grids. In this paper, we propose a multi-source data fusion approach to locate outage events in partially observable distribution systems using Bayesian networks (BNs). A novel aspect of the proposed approach is that it takes multi-source evidence and the complex structure of distribution systems into account using a probabilistic graphical method. This method can radically reduce the computational complexity of outage location inference in high-dimensional spaces. The graphical structure of the proposed BN is established based on the grid's topology and the causal relationship between random variables, such as the states of branches/customers and evidence. Utilizing this graphical model, the locations of outages are obtained by leveraging a Gibbs sampling (GS) method, to infer the probabilities of de-energization for all branches. Compared with commonly-used exact inference methods that have exponential complexity in the size of the BN, GS quantifies the target conditional probability distributions in a timely manner. A case study of several real partially observable systems is presented to validate the method.      
### 59.A Numerical Method to Compute Stability Margins of Switching Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.02874.pdf)
>  Stability margins for linear time-varying (LTV) and switched-linear systems are traditionally computed via quadratic Lyapunov functions, and these functions certify the stability of the system under study. In this work, we show how the more general class of homogeneous polynomial Lyapunov functions is used to compute stability margins with reduced conservatism, and we show how these Lyapunov functions aid in the search for periodic trajectories for marginally stable LTV systems. Our work is premised on the recent observation that the search for a homogeneous polynomial Lyapunov function for some LTV systems is easily encoded as the search for a quadratic Lyapunov function for a related LTV system, and our main contribution is an intuitive algorithm for generating upper and lower bounds on the system's stability margin. We show also how the worst-case switching scheme - which draws an LTV system closest to a periodic orbit - is generated. Three numerical examples are provided to aid the reader and demonstrate the theoretical contributions of the work.      
### 60.Multi-Layer Wind Velocity Field Visualization in Infrared Images of Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2012.02861.pdf)
>  The energy available in a solar energy powered grid is related to the weather conditions at the time of generation. The forecast of Global Solar Irradiance (GSI) provides the power grid with the capability of scheduling the dispatch of energy. This article investigates how to infer multiple wind velocity fields using consecutive longwave infrared (IR) images of clouds. The objective is to forecast the occlusion of the Sun by clouds. Unsupervised learning is implemented to infer the distribution of the clouds' velocity vectors and heights in multiple wind velocity fields in an IR image. A Multi-Output Weighted Support Vector Machine with Flow Constrains ($\varepsilon$-MO-WSVM-FC) is used to extrapolate the wind velocity fields to the entire frame, visualizing the path of the clouds. The proposed algorithm is capable of approximating the wind velocity using the velocity vectors and physical features of clouds extracted from IR images. It is possible to forecast Sun occlusions when the wind velocity field is approximated in a small air parcel with the assumption that the streamlines are pathlines.      
### 61.Idle speed control with low-complexity offset-free explicit model predictive control in presence of system delay  [ :arrow_down: ](https://arxiv.org/pdf/2012.02859.pdf)
>  The requirement for continual improvement of idle speed control (ISC) performance is increasing due to the stringent regulation on emission and fuel economy these days. In this regard, a low-complexity offset-free explicit model predictive control (EMPC) with constraint horizon is designed to regulate the idle speed under unmeasured disturbance in presence of system delay with rigorous formulation. Particularly, we developed a high-fidelity 4-stroke gasoline-direct injected spark-ignited engine model based on first-principles and test vehicle driving data, and designed a model predictive ISC system. To handle the delay from intake to torque production, we constructed a control-oriented model with delay augmentation. To reject the influence of torque loss, we implemented the offset-free MPC scheme with disturbance model and estimator. Moreover, to deal with the limited capacity assigned for the controller in the engine control unit and the short sampling instant of the engine system, we formulated a low-complexity multiparametric quadratic program with constraint horizon in presence of system delay in state and input variables, and obtained an explicit solution map. To demonstrate the performance of the designed controller, a series of closed-loop simulations were performed. The developed explicit controller showed proper ISC performance in presence of torque loss and system delay.      
### 62.A software decoder implementation for H.266/VVC video coding standard  [ :arrow_down: ](https://arxiv.org/pdf/2012.02832.pdf)
>  Versatile Video Coding Standard (H.266/VVC) was completed by Joint Video Expert Team (JVET) of ITU-T and ISO/IEC, in July 2020. This new ITU recommendation/international standard is a successor to the well-known H.265/HEVC video coding standard with roughly doubled compression efficiency, but also at the cost of an increased computational complexity. The complexity of H.266/VVC decoder processing modules is studied in this paper. An optimized decoder implementation using SIMD instruction extensions and additional parallel processing including data and task level parallelism is presented, which can achieve real-time decoding of 4K 60fps VVC bitstreams on an x86 based CPU.      
### 63.Automatic image-based respiratory signal extraction in real-time CMR  [ :arrow_down: ](https://arxiv.org/pdf/2012.02828.pdf)
>  Purpose: To develop a fully automatic method for extraction and directionality determination of respiratory signal in free-breathing, real-time (RT) cardiac MRI. <br>Methods: The respiratory signal is extracted by a principal component analysis method from RT cine images. Then, a two-step procedure is used to determine the directionality (sign) of the respiratory signal. First, the signal polarity of all slices is made consistent with a reference slice. Second, a global sign correction is performed by maximizing the correlation of the respiratory signal with the zeroth-moment center curve. The proposed method is evaluated in multi-slice RT cine from eleven volunteers and two patients. The motion in a manually selected region-of-interest is used as reference. <br>Results: The extracted respiratory signal using the proposed method exhibits high, positive correlation with the reference in all cases and is more robust compared to a recently proposed method. To demonstrate its clinical utility, the method is used to identify heartbeats from inspiratory and expiratory phases and, in turn, to account for respiration-induced changes in the cardiac output. <br>Conclusions: The proposed method enables fully automatic extraction and directionality determinations of respiratory signal from RT cardiac cine images, allowing accurate cardiac function quantification.      
### 64.ACN-Sim: An Open-Source Simulator for Data-Driven Electric Vehicle Charging Research  [ :arrow_down: ](https://arxiv.org/pdf/2012.02809.pdf)
>  ACN-Sim is a data-driven, open-source simulation environment designed to accelerate research in the field of smart electric vehicle (EV) charging. It fills the need in this community for a widely available, realistic simulation environment in which researchers can evaluate algorithms and test assumptions. ACN-Sim provides a modular, extensible architecture, which models the complexity of real charging systems, including battery charging behavior and unbalanced three-phase infrastructure. It also integrates with a broader ecosystem of research tools. These include ACN-Data, an open dataset of EV charging sessions, which provides realistic simulation scenarios and ACN-Live, a framework for field-testing charging algorithms. It also integrates with grid simulators like MATPOWER, PandaPower and OpenDSS, and OpenAI Gym for training reinforcement learning agents.      
### 65.Independent Elliptical Distributions Minimize Their $\mathcal{W}_2$ Wasserstein Distance from Independent Elliptical Distributions with the Same Density Generator  [ :arrow_down: ](https://arxiv.org/pdf/2012.03809.pdf)
>  This short note is on a property of the $\mathcal{W}_2$ Wasserstein distance which indicates that independent elliptical distributions minimize their $\mathcal{W}_2$ Wasserstein distance from given independent elliptical distributions with the same density generators. Furthermore, we examine the implications of this property in the Gelbrich bound when the distributions are not necessarily elliptical. Meanwhile, we also generalize the results to the cases when the distributions are not independent. The primary purpose of this note is for the referencing of papers that need to make use of this property or its implications.      
### 66.Diverse Melody Generation from Chinese Lyrics via Mutual Information Maximization  [ :arrow_down: ](https://arxiv.org/pdf/2012.03805.pdf)
>  In this paper, we propose to adapt the method of mutual information maximization into the task of Chinese lyrics conditioned melody generation to improve the generation quality and diversity. We employ scheduled sampling and force decoding techniques to improve the alignment between lyrics and melodies. With our method, which we called Diverse Melody Generation (DMG), a sequence-to-sequence model learns to generate diverse melodies heavily depending on the input style ids, while keeping the tonality and improving the alignment. The experimental results of subjective tests show that DMG can generate more pleasing and coherent tunes than baseline methods.      
### 67.Triplet Entropy Loss: Improving The Generalisation of Short Speech Language Identification Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.03775.pdf)
>  We present several methods to improve the generalisation of language identification (LID) systems to new speakers and to new domains. These methods involve Spectral augmentation, where spectrograms are masked in the frequency or time bands during training and CNN architectures that are pre-trained on the Imagenet dataset. The paper also introduces the novel Triplet Entropy Loss training method, which involves training a network simultaneously using Cross Entropy and Triplet loss. It was found that all three methods improved the generalisation of the models, though not significantly. Even though the models trained using Triplet Entropy Loss showed a better understanding of the languages and higher accuracies, it appears as though the models still memorise word patterns present in the spectrograms rather than learning the finer nuances of a language. The research shows that Triplet Entropy Loss has great potential and should be investigated further, not only in language identification tasks but any classification task.      
### 68.Dimmer: Self-Adaptive Network-Wide Flooding with Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.03719.pdf)
>  In low-power wireless networks, Synchronous transmissions (ST) protocols provide high reliability and energy-efficiency in the presence of little or no interference, while dependable ST protocols provide high reliability in harsher environments, through the use of custom rules, fixed configurations and higher retransmissions. Yet, such dependable solutions often trade energy-efficiency for dependability, favoring wasting energy under normal conditions to survive highly-interfered episodes. We argue that, complementary to their dependability, ST protocols should be adaptive: their wireless stack should (1) tackle external environment dynamics and (2) adapt to its topology over time. <br>We introduce Dimmer as a self-adaptive, all-to-all communication primitive. Dimmer builds on LWB and uses Reinforcement Learning to tune its flooding parameters and match the current properties of the medium. By learning how to behave from unlabeled traces, Dimmer adapts to different interference types and patterns, and is even able to tackle previously unseen interference. In addition, we share through Dimmer insights on how to efficiently design AI-based systems for constrained devices, and evaluate our protocol on two deployments of 18 and 48 resource-constrained sensor nodes (4 MHz CPU, 10 kB RAM), showing it improves reliability under WiFi interference and IEEE 802.15.4 jamming, while turning superfluous transmitters off in the absence of disturbances.      
### 69.A CSMT challenge dataset for the identification of computer generated melodies  [ :arrow_down: ](https://arxiv.org/pdf/2012.03646.pdf)
>  In this paper, the dataset used for the data challenge organised by Conference on Sound and Music Technology (CSMT) is introduced. The CSMT data challenge requires participants to identify whether a given piece of melody is generated by computer or is composed by human. The dataset is formed by two parts: development dataset and evaluation dataset. The development dataset contains only computer generated melodies whereas the evaluation dataset contain both computer generated melodies and human composed melodies. The aim of the dataset is to examine whether it is possible to distinguish computer generated melodies by learning the feature of generated melodies.      
### 70.Speech Imagery Classification using Length-Wise Training based on Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.03632.pdf)
>  Brain-computer interface uses brain signals to control external devices without actual control behavior. Recently, speech imagery has been studied for direct communication using language. Speech imagery uses brain signals generated when the user imagines speech. Unlike motor imagery, speech imagery still has unknown characteristics. Additionally, electroencephalography has intricate and non-stationary properties resulting in insufficient decoding performance. In addition, speech imagery is difficult to utilize spatial features. In this study, we designed length-wise training that allows the model to classify a series of a small number of words. In addition, we proposed hierarchical convolutional neural network structure and loss function to maximize the training strategy. The proposed method showed competitive performance in speech imagery classification. Hence, we demonstrated that the length of the word is a clue at improving classification performance.      
### 71.Fast Pattern Recognition for Electron Emission Micrograph Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.03578.pdf)
>  In this work, a pattern recognition algorithm was developed to process and analyze electron emission micrographs. Various examples of dc and rf emission are given that demonstrate this algorithm applicability to determine emitters spatial location and distribution and calculate apparent emission area. The algorithm is fast and only takes $\sim$10 seconds to process and analyze one micrograph using resources of an Intel Core i5.      
### 72.Reverberant Sound Localization with a Robot Head Based on Direct-Path Relative Transfer Function  [ :arrow_down: ](https://arxiv.org/pdf/2012.03574.pdf)
>  This paper addresses the problem of sound-source localization (SSL) with a robot head, which remains a challenge in real-world environments. In particular we are interested in locating speech sources, as they are of high interest for human-robot interaction. The microphone-pair response corresponding to the direct-path sound propagation is a function of the source direction. In practice, this response is contaminated by noise and reverberations. The direct-path relative transfer function (DP-RTF) is defined as the ratio between the direct-path acoustic transfer function (ATF) of the two microphones, and it is an important feature for SSL. We propose a method to estimate the DP-RTF from noisy and reverberant signals in the short-time Fourier transform (STFT) domain. First, the convolutive transfer function (CTF) approximation is adopted to accurately represent the impulse response of the microphone array, and the first coefficient of the CTF is mainly composed of the direct-path ATF. At each frequency, the frame-wise speech auto- and cross-power spectral density (PSD) are obtained by spectral subtraction. Then a set of linear equations is constructed by the speech auto- and cross-PSD of multiple frames, in which the DP-RTF is an unknown variable, and is estimated by solving the equations. Finally, the estimated DP-RTFs are concatenated across frequencies and used as a feature vector for SSL. Experiments with a robot, placed in various reverberant environments, show that the proposed method outperforms two state-of-the-art methods.      
### 73.Space-Filling Subset Selection for an Electric Battery Model  [ :arrow_down: ](https://arxiv.org/pdf/2012.03541.pdf)
>  Dynamic models of the battery performance are an essential tool throughout the development process of automotive drive trains. The present study introduces a method making a large data set suitable for modeling the electrical impedance. When obtaining data-driven models, a usual assumption is that more observations produce better models. However, real driving data on the battery's behavior represent a strongly non-uniform excitation of the system, which negatively affects the modeling. For that reason, a subset selection of the available data was developed. It aims at building accurate nonlinear autoregressive exogenous (NARX) models more efficiently. The algorithm selects those dynamic data points that fill the input space of the nonlinear model more homogeneously. It is shown, that this reduction of the training data leads to a higher model quality in comparison to a random subset and a faster training compared to modeling using all data points.      
### 74.Improved Hoeffding's Lemma and Hoeffding's Tail Bounds  [ :arrow_down: ](https://arxiv.org/pdf/2012.03535.pdf)
>  The purpose of this letter is to improve Hoeffding's lemma and consequently Hoeffding's tail bounds. The improvement pertains to left skewed zero mean random variables $X\in[a,b]$, where $a&lt;0$ and $-a&gt;b$. The proof of Hoeffding's improved lemma uses Taylor's expansion, the convexity of $\exp(sx), s\in {\bf R}$ and an unnoticed observation since Hoeffding's publication in 1963 that for $-a&gt;b$ the maximum of the intermediate function $\tau(1-\tau)$ appearing in Hoeffding's proof is attained <br>at an endpoint rather than at $\tau=0.5$ as in the case $b&gt;-a$. Using Hoeffding's improved lemma we obtain one sided and two sided tail bounds for $P(S_n\ge t)$ and $P(|S_n|\ge t)$, respectively, where <br>$S_n=\sum_{i=1}^nX_i$ and the $X_i\in[a_i,b_i],i=1,...,n$ are independent zero mean random variables (not necessarily identically distributed). It is interesting to note that we could also improve Hoeffding's two sided bound for all $\{X_i: a_i\ne b_i,i=1,...,n\}$. This is so because here the one sided bound should be increased by $P(-S_n\ge t)$, wherein the left skewed intervals become right skewed and vice versa.      
### 75.BinArray: A Scalable Hardware Accelerator for Binary Approximated CNNs  [ :arrow_down: ](https://arxiv.org/pdf/2012.03481.pdf)
>  Deep Convolutional Neural Networks (CNNs) have become state-of-the art for computer vision and other signal processing tasks due to their superior accuracy. In recent years, large efforts have been made to reduce the computational costs of CNNs in order to achieve real-time operation on low-power embedded devices. <br>Towards this goal we present BinArray, a custom hardware accelerator for CNNs with binary approximated weights. The binary approximation used in this paper is an improved version of a network compression technique initially suggested in [1]. It drastically reduces the number of multiplications required per inference with no or very little accuracy degradation. BinArray easily scales and allows to compromise between hardware resource usage and throughput by means of three design parameters transparent to the user. Furthermore, it is possible to select between high accuracy or throughput dynamically during runtime. BinArray has been optimized at the register transfer level and operates at 400 MHz as instruction-set processor within a heterogenous XC7Z045-2 FPGA-SoC platform. <br>Experimental results show that BinArray scales to match the performance of other accelerators like EdgeTPU [2] for different network sizes. Even for the largest MobileNet only 50% of the target device and only 96 DSP blocks are utilized.      
### 76.Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements  [ :arrow_down: ](https://arxiv.org/pdf/2012.03478.pdf)
>  We propose a novel system that takes as an input body movements of a musician playing a musical instrument and generates music in an unsupervised setting. Learning to generate multi-instrumental music from videos without labeling the instruments is a challenging problem. To achieve the transformation, we built a pipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline learns a discrete latent representation of various instruments music from log-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with multi-band residual blocks. The pipeline is then trained along with an autoregressive prior conditioned on the musician's body keypoints movements encoded by a recurrent neural network. Joint training of the prior with the body movements encoder succeeds in the disentanglement of the music into latent features indicating the musical components and the instrumental features. The latent space results in distributions that are clustered into distinct instruments from which new music can be generated. Furthermore, the VQ-VAE architecture supports detailed music generation with additional conditioning. We show that a Midi can further condition the latent space such that the pipeline will generate the exact content of the music being played by the instrument in the video. We evaluate MI Net on two datasets containing videos of 13 instruments and obtain generated music of reasonable audio quality, easily associated with the corresponding instrument, and consistent with the music audio content.      
### 77.Data-Driven Predictive Control for Continuous-Time Industrial Processes with Completely Unknown Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2012.03452.pdf)
>  This paper investigates the data-driven predictive control problems for a class of continuous-time industrial processes with completely unknown dynamics. The proposed approach employs the data-driven technique to get the system matrices online, using input-output measurements. Then, a model-free predictive control approach is designed to implement the receding-horizon optimization and realize the reference tracking. Feasibility of the proposed algorithm and stability of the closed-loop control systems are analyzed, respectively. Finally, a simulation example is provided to demonstrate the effectiveness of the proposed approach.      
### 78.Variational Autoencoders for Learning Nonlinear Dynamics of Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.03448.pdf)
>  We develop data-driven methods for incorporating physical information for priors to learn parsimonious representations of nonlinear systems arising from parameterized PDEs and mechanics. Our approach is based on Variational Autoencoders (VAEs) for learning from observations nonlinear state space models. We develop ways to incorporate geometric and topological priors through general manifold latent space representations. We investigate the performance of our methods for learning low dimensional representations for the nonlinear Burgers equation and constrained mechanical systems.      
### 79.An Improved Benders Decomposition Algorithm for Steady-State Dispatch Problem in an Integrated Electricity-Gas System  [ :arrow_down: ](https://arxiv.org/pdf/2012.03447.pdf)
>  Optimally operating an integrated electricity-gas system (IEGS) is significant for the energy sector. However, the IEGS operation model's nonconvexity makes it challenging to solve the optimal dispatch problem in the IEGS. This letter proposes an improved Benders decomposition (IBD) algorithm catering to a commonly used steady-state dispatch model of the IEGS. This IBD algorithm leverages a refined decomposition structure where the subproblems become linear and ready to be solved in parallel. We analytically compare our IBD algorithm with an existing Benders decomposition algorithm and a typical piecewise linearization method. Case studies have substantiated the higher computational efficiency of our IBD algorithm.      
### 80.Data-driven approximation for feasible regions in nonlinear model predictive control  [ :arrow_down: ](https://arxiv.org/pdf/2012.03428.pdf)
>  This paper develops a data-driven learning framework for approximating the feasible region and invariant set of a nonlinear system under the nonlinear Model Predictive Control (MPC) scheme. The developed approach is based on the feasibility information of a point-wise data set using low-discrepancy sequence. Using kernel-based Support Vector Machine (SVM) learning, we construct outer and inner approximations of the boundary of the feasible region and then, obtain the feasible region of MPC for the system. Furthermore, we extend our approach to the perturbed nonlinear systems using set-theoretic method. Finally, an illustrative numerical example is provided to show the effectiveness of the proposed approach.      
### 81.A General BER Expression of Rectangular QAM with the Presence of Phase Noise  [ :arrow_down: ](https://arxiv.org/pdf/2012.03422.pdf)
>  In this letter, we first present a closed-form bit-error rate (BER) expression for an $M-$ary pulse-amplitude modulation ($M$-PAM) over additive white Gaussian noise (AWGN) channels by analytically characterizing the bit decision regions and positions. The obtained expression is then used to derive the BER of a rectangular quadrature amplitude modulation (QAM) under the presence of phase noise. Numerical results show that the impact of phase noise on the BER performance is proportional to the constellation size. Moreover, it is observed that given a constellation size, the square QAM achieves the lowest phase noise-induced performance loss compared to other rectangular constellations.      
### 82.On Infusing Reachability-Based Safety Assurance within Planning Frameworks for Human-Robot Vehicle Interactions  [ :arrow_down: ](https://arxiv.org/pdf/2012.03390.pdf)
>  Action anticipation, intent prediction, and proactive behavior are all desirable characteristics for autonomous driving policies in interactive scenarios. Paramount, however, is ensuring safety on the road -- a key challenge in doing so is accounting for uncertainty in human driver actions without unduly impacting planner performance. This paper introduces a minimally-interventional safety controller operating within an autonomous vehicle control stack with the role of ensuring collision-free interaction with an externally controlled (e.g., human-driven) counterpart while respecting static obstacles such as a road boundary wall. We leverage reachability analysis to construct a real-time (100Hz) controller that serves the dual role of (i) tracking an input trajectory from a higher-level planning algorithm using model predictive control, and (ii) assuring safety by maintaining the availability of a collision-free escape maneuver as a persistent constraint regardless of whatever future actions the other car takes. A full-scale steer-by-wire platform is used to conduct traffic weaving experiments wherein two cars, initially side-by-side, must swap lanes in a limited amount of time and distance, emulating cars merging onto/off of a highway. We demonstrate that, with our control stack, the autonomous vehicle is able to avoid collision even when the other car defies the planner's expectations and takes dangerous actions, either carelessly or with the intent to collide, and otherwise deviates minimally from the planned trajectory to the extent required to maintain safety.      
### 83.Combining Spatial Clustering with LSTM Speech Models for Multichannel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2012.03388.pdf)
>  Recurrent neural networks using the LSTM architecture can achieve significant single-channel noise reduction. It is not obvious, however, how to apply them to multi-channel inputs in a way that can generalize to new microphone configurations. In contrast, spatial clustering techniques can achieve such generalization, but lack a strong signal model. This paper combines the two approaches to attain both the spatial separation performance and generality of multichannel spatial clustering and the signal modeling performance of multiple parallel single-channel LSTM speech enhancers. The system is compared to several baselines on the CHiME3 dataset in terms of speech quality predicted by the PESQ algorithm and word error rate of a recognizer trained on mis-matched conditions, in order to focus on generalization. Our experiments show that by combining the LSTM models with the spatial clustering, we reduce word error rate by 4.6\% absolute (17.2\% relative) on the development set and 11.2\% absolute (25.5\% relative) on test set compared with spatial clustering system, and reduce by 10.75\% (32.72\% relative) on development set and 6.12\% absolute (15.76\% relative) on test data compared with LSTM model.      
### 84.Source Separation and Depthwise Separable Convolutions for Computer Audition  [ :arrow_down: ](https://arxiv.org/pdf/2012.03359.pdf)
>  Given recent advances in deep music source separation, we propose a feature representation method that combines source separation with a state-of-the-art representation learning technique that is suitably repurposed for computer audition (i.e. machine listening). We train a depthwise separable convolutional neural network on a challenging electronic dance music (EDM) data set and compare its performance to convolutional neural networks operating on both source separated and standard spectrograms. It is shown that source separation improves classification performance in a limited-data setting compared to the standard single spectrogram approach.      
### 85.Guitar Effects Recognition and Parameter Estimation with Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.03216.pdf)
>  Despite the popularity of guitar effects, there is very little existing research on classification and parameter estimation of specific plugins or effect units from guitar recordings. In this paper, convolutional neural networks were used for classification and parameter estimation for 13 overdrive, distortion and fuzz guitar effects. A novel dataset of processed electric guitar samples was assembled, with four sub-datasets consisting of monophonic or polyphonic samples and discrete or continuous settings values, for a total of about 250 hours of processed samples. Results were compared for networks trained and tested on the same or on a different sub-dataset. We found that discrete datasets could lead to equally high performance as continuous ones, whilst being easier to design, analyse and modify. Classification accuracy was above 80\%, with confusion matrices reflecting similarities in the effects timbre and circuits design. With parameter values between 0.0 and 1.0, the mean absolute error is in most cases below 0.05, while the root mean square error is below 0.1 in all cases but one.      
### 86.Estimating Vector Fields from Noisy Time Series  [ :arrow_down: ](https://arxiv.org/pdf/2012.03199.pdf)
>  While there has been a surge of recent interest in learning differential equation models from time series, methods in this area typically cannot cope with highly noisy data. We break this problem into two parts: (i) approximating the unknown vector field (or right-hand side) of the differential equation, and (ii) dealing with noise. To deal with (i), we describe a neural network architecture consisting of tensor products of one-dimensional neural shape functions. For (ii), we propose an alternating minimization scheme that switches between vector field training and filtering steps, together with multiple trajectories of training data. We find that the neural shape function architecture retains the approximation properties of dense neural networks, enables effective computation of vector field error, and allows for graphical interpretability, all for data/systems in any finite dimension $d$. We also study the combination of either our neural shape function method or existing differential equation learning methods with alternating minimization and multiple trajectories. We find that retrofitting any learning method in this way boosts the method's robustness to noise. While in their raw form the methods struggle with 1% Gaussian noise, after retrofitting, they learn accurate vector fields from data with 10% Gaussian noise.      
### 87.Conditional Generative Adversarial Networks for Optimal Path Planning  [ :arrow_down: ](https://arxiv.org/pdf/2012.03166.pdf)
>  Path planning plays an important role in autonomous robot systems. Effective understanding of the surrounding environment and efficient generation of optimal collision-free path are both critical parts for solving path planning problem. Although conventional sampling-based algorithms, such as the rapidly-exploring random tree (RRT) and its improved optimal version (RRT*), have been widely used in path planning problems because of their ability to find a feasible path in even complex environments, they fail to find an optimal path efficiently. To solve this problem and satisfy the two aforementioned requirements, we propose a novel learning-based path planning algorithm which consists of a novel generative model based on the conditional generative adversarial networks (CGAN) and a modified RRT* algorithm (denoted by CGANRRT*). Given the map information, our CGAN model can generate an efficient possibility distribution of feasible paths, which can be utilized by the CGAN-RRT* algorithm to find the optimal path with a non-uniform sampling strategy. The CGAN model is trained by learning from ground truth maps, each of which is generated by putting all the results of executing RRT algorithm 50 times on one raw map. We demonstrate the efficient performance of this CGAN model by testing it on two groups of maps and comparing CGAN-RRT* algorithm with conventional RRT* algorithm.      
### 88.YieldNet: A Convolutional Neural Network for Simultaneous Corn and Soybean Yield Prediction Based on Remote Sensing Data  [ :arrow_down: ](https://arxiv.org/pdf/2012.03129.pdf)
>  Large scale crop yield estimation is, in part, made possible due to the availability of remote sensing data allowing for the continuous monitoring of crops throughout its growth state. Having this information allows stakeholders the ability to make real-time decisions to maximize yield potential. Although various models exist that predict yield from remote sensing data, there currently does not exist an approach that can estimate yield for multiple crops simultaneously, and thus leads to more accurate predictions. A model that predicts yield of multiple crops and concurrently considers the interaction between multiple crop's yield. We propose a new model called YieldNet which utilizes a novel deep learning framework that uses transfer learning between corn and soybean yield predictions by sharing the weights of the backbone feature extractor. Additionally, to consider the multi-target response variable, we propose a new loss function. Numerical results demonstrate that our proposed method accurately predicts yield from one to four months before the harvest, and is competitive to other state-of-the-art approaches.      
### 89.Generating Synthetic Multispectral Satellite Imagery from Sentinel-2  [ :arrow_down: ](https://arxiv.org/pdf/2012.03108.pdf)
>  Multi-spectral satellite imagery provides valuable data at global scale for many environmental and socio-economic applications. Building supervised machine learning models based on these imagery, however, may require ground reference labels which are not available at global scale. Here, we propose a generative model to produce multi-resolution multi-spectral imagery based on Sentinel-2 data. The resulting synthetic images are indistinguishable from real ones by humans. This technique paves the road for future work to generate labeled synthetic imagery that can be used for data augmentation in data scarce regions and applications.      
### 90.When Do Curricula Work?  [ :arrow_down: ](https://arxiv.org/pdf/2012.03107.pdf)
>  Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the \emph{implicit curricula} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of \emph{explicit curricula}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.      
### 91.Semantic Segmentation of Medium-Resolution Satellite Imagery using Conditional Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.03093.pdf)
>  Semantic segmentation of satellite imagery is a common approach to identify patterns and detect changes around the planet. Most of the state-of-the-art semantic segmentation models are trained in a fully supervised way using Convolutional Neural Network (CNN). The generalization property of CNN is poor for satellite imagery because the data can be very diverse in terms of landscape types, image resolutions, and scarcity of labels for different geographies and seasons. Hence, the performance of CNN doesn't translate well to images from unseen regions or seasons. Inspired by Conditional Generative Adversarial Networks (CGAN) based approach of image-to-image translation for high-resolution satellite imagery, we propose a CGAN framework for land cover classification using medium-resolution Sentinel-2 imagery. We find that the CGAN model outperforms the CNN model of similar complexity by a significant margin on an unseen imbalanced test dataset.      
### 92.Multi-target normal behaviour models for wind farm condition monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2012.03074.pdf)
>  The trend towards larger wind turbines and remote locations of wind farms fuels the demand for automated condition monitoring and condition-based maintenance strategies that can reduce the operating cost and avoid unplanned downtime. Normal behaviour modelling has been introduced to automatically detect anomalous deviations from normal operation based on the turbine's SCADA data. A growing number of machine learning models and related threshold values of the normal behaviour of turbine subsystems are being developed by wind farm managers to this end. However, these models need to be kept track of, be maintained and require frequent updates. Every additional model increases the overall lifetime management effort in practice. This research explores and benchmarks multi-target machine learning models as a new approach to capturing a wind turbine's normal behaviour. We present an overview of multi-target regression methods and motivate their application and benefits in wind turbine condition monitoring. As a second contribution, we evaluate and benchmark the performance of multi-target normal behaviour models in a wind turbine case study. We find that multi-target models are advantageous in comparison to single-target modelling in that they can substantially reduce the lifecycle management effort of normal behaviour models without compromising on the accuracy of the models. Finally, we also outline some areas of future research.      
### 93.Depth estimation from 4D light field videos  [ :arrow_down: ](https://arxiv.org/pdf/2012.03021.pdf)
>  Depth (disparity) estimation from 4D Light Field (LF) images has been a research topic for the last couple of years. Most studies have focused on depth estimation from static 4D LF images while not considering temporal information, i.e., LF videos. This paper proposes an end-to-end neural network architecture for depth estimation from 4D LF videos. This study also constructs a medium-scale synthetic 4D LF video dataset that can be used for training deep learning-based methods. Experimental results using synthetic and real-world 4D LF videos show that temporal information contributes to the improvement of depth estimation accuracy in noisy regions. Dataset and code is available at: <a class="link-external link-https" href="https://mediaeng-lfv.github.io/LFV_Disparity_Estimation" rel="external noopener nofollow">this https URL</a>      
### 94.Design and Implementation of Path Trackers for Ackermann Drive based Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2012.02978.pdf)
>  This article is an overview of the various literature on path tracking methods and their implementation in simulation and realistic operating environments.The scope of this study includes analysis, implementation,tuning, and comparison of some selected path tracking methods commonly used in practice for trajectory tracking in autonomous vehicles. Many of these methods are applicable at low speed due to the linear assumption for the system model, and hence, some methods are also included that consider nonlinearities present in lateral vehicle dynamics during high-speed navigation. The performance evaluation and comparison of tracking methods are carried out on realistic simulations and a dedicated instrumented passenger car, Mahindra e2o, to get a performance idea of all the methods in realistic operating conditions and develop tuning methodologies for each of the methods. It has been observed that our model predictive control-based approach is able to perform better compared to the others in medium velocity ranges.      
### 95.Sharing Energy Storage Between Transmission and Distribution  [ :arrow_down: ](https://arxiv.org/pdf/1901.02977.pdf)
>  This paper addresses the problem of how best to coordinate, or `stack,' energy storage services in systems that lack centralized markets. Specifically, its focus is on how to coordinate transmission-level congestion relief with local, distribution-level objectives. We describe and demonstrate a unified communication and optimization framework for performing this coordination. The congestion relief problem formulation employs a weighted $\ell_{1}$-norm objective. This approach determines a set of corrective actions, i.e., energy storage injections and conventional generation adjustments, that minimize the required deviations from a planned schedule. To exercise this coordination framework, we present two case studies. The first is based on a 3-bus test system, and the second on a realistic representation of the Pacific Northwest region of the United States. The results indicate that the scheduling methodology provides congestion relief, cost savings, and improved renewable energy integration. The large-scale case study informed the design of a live demonstration carried out in partnership with the University of Washington, Doosan GridTech, Snohomish County PUD, and the Bonneville Power Administration. The goal of the demonstration was to test the feasibility of the scheduling framework in a production environment with real-world energy storage assets. The demonstration results were consistent with computational simulations.      
