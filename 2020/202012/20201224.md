# ArXiv eess --Thu, 24 Dec 2020
### 1.Point-Based Value Iteration and Approximately Optimal Dynamic Sensor Selection for Linear-Gaussian Processes  [ :arrow_down: ](https://arxiv.org/pdf/2012.12842.pdf)
>  The problem of synthesizing an optimal sensor selection policy is pertinent to a variety of engineering applications ranging from event detection to autonomous navigation. We consider such a synthesis problem over an infinite time horizon with a discounted cost criterion. We formulate this problem in terms of a value iteration over the continuous space of covariance matrices. To obtain a computationally tractable solution, we subsequently formulate an approximate sensor selection problem, which is solvable through a point-based value iteration over a finite "mesh" of covariance matrices with a user-defined bounded trace. We provide theoretical guarantees bounding the suboptimality of the sensor selection policies synthesized through this method and provide numerical examples comparing them to known results.      
### 2.Multiclass Spinal Cord Tumor Segmentation on MRI with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.12820.pdf)
>  Spinal cord tumors lead to neurological morbidity and mortality. Being able to obtain morphometric quantification (size, location, growth rate) of the tumor, edema, and cavity can result in improved monitoring and treatment planning. Such quantification requires the segmentation of these structures into three separate classes. However, manual segmentation of 3-dimensional structures is time-consuming and tedious, motivating the development of automated methods. Here, we tailor a model adapted to the spinal cord tumor segmentation task. Data were obtained from 343 patients using gadolinium-enhanced T1-weighted and T2-weighted MRI scans with cervical, thoracic, and/or lumbar coverage. The dataset includes the three most common intramedullary spinal cord tumor types: astrocytomas, ependymomas, and hemangioblastomas. The proposed approach is a cascaded architecture with U-Net-based models that segments tumors in a two-stage process: locate and label. The model first finds the spinal cord and generates bounding box coordinates. The images are cropped according to this output, leading to a reduced field of view, which mitigates class imbalance. The tumor is then segmented. The segmentation of the tumor, cavity, and edema (as a single class) reached 76.7 $\pm$ 1.5% of Dice score and the segmentation of tumors alone reached 61.8 $\pm$ 4.0% Dice score. The true positive detection rate was above 87% for tumor, edema, and cavity. To the best of our knowledge, this is the first fully automatic deep learning model for spinal cord tumor segmentation. The multiclass segmentation pipeline is available in the Spinal Cord Toolbox (<a class="link-external link-https" href="https://spinalcordtoolbox.com/" rel="external noopener nofollow">this https URL</a>). It can be run with custom data on a regular computer within seconds.      
### 3.Investigating the Impact of Electric Vehicles on the Voltage Profile of Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.12806.pdf)
>  This paper investigates the impact of high penetration of Electric Vehicles (EVs) on the distribution network in the presence of photovoltaic (PV) systems. Two models for EVs are presented and the voltage profile of buses is investigated considering both models and various penetration levels of EVs within the distribution network. The analysis is conducted by presenting an exact convex relaxed form of the full ACOPF problem of the distribution network with fixed power EVs and presenting the full ACOPF problem of the distribution network with fixed current EVs. The performance of each model is illustrated in the case studies leveraging the modified IEEE 33-bus system and considering time-of-use (TOU) pricing. Besides the sensitivity of the voltage profile of buses in the distribution network on the time-of-use prices is investigated in the case studies.      
### 4.Learning the Gap in the Day-Ahead and Real-Time Locational Marginal Prices in the Electricity Market  [ :arrow_down: ](https://arxiv.org/pdf/2012.12792.pdf)
>  In this paper, statistical machine learning algorithms, as well as deep neural networks, are used to predict the values of the price gap between day-ahead and real-time electricity markets. Several exogenous features are collected and impacts of these features are examined to capture the best relations between the features and the target variable. Ensemble learning algorithm namely the Random Forest issued to calculate the probability distribution of the predicted electricity prices for day-ahead and real-time markets. Long-Short-Term-Memory (LSTM) is utilized to capture long term dependencies in predicting direct gap values between mentioned markets and the benefits of directly predicting the gap price rather than subtracting the predictions of day-ahead and real-time markets are illustrated. Case studies are implemented on the California Independent System Operator (CAISO) electricity market data for a two years period. The proposed methods are evaluated and neural networks showed promising results in predicting the exact values of the gap.      
### 5.Short-term Operational Planning Problem of the Multiple-Energy Carrier Hybrid AC/DC Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2012.12788.pdf)
>  In this paper, the short-term operation problem for a multiple energy carrier hybrid AC/DC microgrid is discussed. The hybrid microgrid consists of AC and DC parts, which are connected by means of inverters as well as natural gas network. The microgrid includes photovoltaic (PV) unit, wind turbine (WT), battery storage unit and gas-fired microturbines. A mixed integer linear programming is formed to minimize the overall cost of the microgrid including cost of natural gas supply, the value of lost load and battery degradation cost. The presented case study explored the importance of inverter characteristics and pipeline capacity.      
### 6.Intelligent Reflecting Surface Assisted Anti-Jamming Communications Based on Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.12761.pdf)
>  Malicious jamming launched by smart jammer, which attacks legitimate transmissions has been regarded as one of the critical security challenges in wireless communications. Thus, this paper exploits intelligent reflecting surface (IRS) to enhance anti-jamming communication performance and mitigate jamming interference by adjusting the surface reflecting elements at the IRS. Aiming to enhance the communication performance against smart jammer, an optimization problem for jointly optimizing power allocation at the base station (BS) and reflecting beamforming at the IRS is formulated. As the jamming model and jamming behavior are dynamic and unknown, a win or learn fast policy hill-climbing (WoLF-PHC) learning approach is proposed to jointly optimize the anti-jamming power allocation and reflecting beamforming strategy without the knowledge of the jamming model. Simulation results demonstrate that the proposed anti-jamming based-learning approach can efficiently improve both the IRS-assisted system rate and transmission protection level compared with existing solutions.      
### 7.Joint deconvolution and unsupervised source separation for data on the sphere  [ :arrow_down: ](https://arxiv.org/pdf/2012.12740.pdf)
>  Tackling unsupervised source separation jointly with an additional inverse problem such as deconvolution is central for the analysis of multi-wavelength data. This becomes highly challenging when applied to large data sampled on the sphere such as those provided by wide-field observations in astrophysics, whose analysis requires the design of dedicated robust and yet effective algorithms. We therefore investigate a new joint deconvolution/sparse blind source separation method dedicated for data sampled on the sphere, coined SDecGMCA. It is based on a projected alternate least-squares minimization scheme, whose accuracy is proved to strongly rely on some regularization scheme in the present joint deconvolution/blind source separation setting. To this end, a regularization strategy is introduced that allows designing a new robust and effective algorithm, which is key to analyze large spherical data. Numerical experiments are carried out on toy examples and realistic astronomical data.      
### 8.Toward Localization in Terahertz-Operating Energy Harvesting Software-Defined Metamaterials: Context Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.12730.pdf)
>  Software-defined metamaterials (SDMs) represent a novel paradigm for real-time control of metamaterials. SDMs are envisioned to enable a variety of exciting applications in the domains such as smart textiles and sensing in challenging conditions. Many of these applications envisage deformations of the SDM structure (e.g., rolling, bending, stretching). This affects the relative position of the metamaterial elements and requires their localization relative to each other. The question of how to perform such localization is, however, yet to spark in the community. We consider that the metamaterial elements are controlled wirelessly through a Terahertz (THz)-operating nanonetwork. Moreover, we consider the elements to be energy constrained, with their sole powering option being to harvest environmental energy. For such a setup, we demonstrate sub-millimeter accuracy of the two-way Time of Flight (ToF)-based localization, as well as high availability of the service (i.e., consistently more than 80% of the time), which is a result of the low energy consumed in localization. Finally, we provide the localization context for a number of relevant system parameters such as operational frequency, bandwidth, and harvesting rate.      
### 9.Polarization amplitude-phase direction finding methods in two-canal UHF radio beacon navigation systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.12728.pdf)
>  There are investigated amplitude-phase method of the moving object bearing when there are used orthogonal linear polarized radio signals illuminated simultaneously from two horizontally spaced points with known co-ordinates. The bearing is measured onboard the moving object with two canal UHF system utilizing the amplitude-phase processing of the signals received within the linear polarization basis. The analysis is carried out for the general case when the orts of the linear polarization basis in which the measurements are made are oriented at a certain angle to the measurement plane.      
### 10.Low Complexity Component Nonlinear Distortions Mitigation Scheme for Probabilistically Shaped 64-QAM Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.12727.pdf)
>  We propose a degenerated hierarchical look-up table (DH-LUT) scheme to compensate component nonlinearities. For probabilistically shaped 64-QAM signals, it achieves up to 2-dB SNR improvement, while the size of table is only 8.59% compared to the conventional LUT method.      
### 11.Learning-based Prediction and Uplink Retransmission for Wireless Virtual Reality (VR) Network  [ :arrow_down: ](https://arxiv.org/pdf/2012.12725.pdf)
>  Wireless Virtual Reality (VR) users are able to enjoy immersive experience from anywhere at anytime. However, providing full spherical VR video with high quality under limited VR interaction latency is challenging. If the viewpoint of the VR user can be predicted in advance, only the required viewpoint is needed to be rendered and delivered, which can reduce the VR interaction latency. Therefore, in this paper, we use offline and online learning algorithms to predict viewpoint of the VR user using real VR dataset. For the offline learning algorithm, the trained learning model is directly used to predict the viewpoint of VR users in continuous time slots. While for the online learning algorithm, based on the VR user's actual viewpoint delivered through uplink transmission, we compare it with the predicted viewpoint and update the parameters of the online learning algorithm to further improve the prediction accuracy. To guarantee the reliability of the uplink transmission, we integrate the Proactive retransmission scheme into our proposed online learning algorithm. Simulation results show that our proposed online learning algorithm for uplink wireless VR network with the proactive retransmission scheme only exhibits about 5% prediction error.      
### 12.Generalized Circuit Averaging Technique for Two Switch DC-DC Converters  [ :arrow_down: ](https://arxiv.org/pdf/2012.12724.pdf)
>  Cuk and SEPIC are some of the important DC-DC converters used for charging batteries. In this paper, a generalized circuit averaging technique is employed for Cuk and SEPIC converters. The derived equations are used to obtain the frequency response of open loop transfer function. The ratio of perturbed output voltage to duty cycle ($G_{vd}$) is simulated using LTSpice software package. The derived averaged models of the converters aids in faster and simpler simulation. The behavior of the converters in CCM and DCM was also simulated. The derived expressions can be generalized to power converters with two switches.      
### 13.Chest x-ray automated triage: a semiologic approach designed for clinical implementation, exploiting different types of labels through a combination of four Deep Learning architectures  [ :arrow_down: ](https://arxiv.org/pdf/2012.12712.pdf)
>  BACKGROUND AND OBJECTIVES: The multiple chest x-ray datasets released in the last years have ground-truth labels intended for different computer vision tasks, suggesting that performance in automated chest-xray interpretation might improve by using a method that can exploit diverse types of annotations. This work presents a Deep Learning method based on the late fusion of different convolutional architectures, that allows training with heterogeneous data with a simple implementation, and evaluates its performance on independent test data. We focused on obtaining a clinically useful tool that could be successfully integrated into a hospital workflow. MATERIALS AND METHODS: Based on expert opinion, we selected four target chest x-ray findings, namely lung opacities, fractures, pneumothorax and pleural effusion. For each finding we defined the most adequate type of ground-truth label, and built four training datasets combining images from public chest x-ray datasets and our institutional archive. We trained four different Deep Learning architectures and combined their outputs with a late fusion strategy, obtaining a unified tool. Performance was measured on two test datasets: an external openly-available dataset, and a retrospective institutional dataset, to estimate performance on local population. RESULTS: The external and local test sets had 4376 and 1064 images, respectively, for which the model showed an area under the Receiver Operating Characteristics curve of 0.75 (95%CI: 0.74-0.76) and 0.87 (95%CI: 0.86-0.89) in the detection of abnormal chest x-rays. For the local population, a sensitivity of 86% (95%CI: 84-90), and a specificity of 88% (95%CI: 86-90) were obtained, with no significant differences between demographic subgroups. We present examples of heatmaps to show the accomplished level of interpretability, examining true and false positives.      
### 14.The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?  [ :arrow_down: ](https://arxiv.org/pdf/2012.12689.pdf)
>  We dare to make use of a possible analogy between neurons in a brain and people in society, asking ourselves whether individual intelligence is necessary in order to collective wisdom to emerge and, most importantly, what sort of individual intelligence is conducive of greater collective wisdom. We review insights and findings from connectionism, agent-based modeling, group psychology, economics and physics, casting them in terms of changing structure of the system's Lyapunov function. Finally, we apply these insights to the sort and degrees of intelligence of preys and predators in the Lotka-Volterra model, explaining why certain individual understandings lead to co-existence of the two species whereas other usages of their individual intelligence cause global extinction.      
### 15.Adaptive Radar Detection and Classification Algorithms for Multiple Coherent Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.12688.pdf)
>  In this paper, we address the problem of target detection in the presence of coherent (or fully correlated) signals, which can be due to multipath propagation effects or electronic attacks by smart jammers. To this end, we formulate the problem at hand as a multiple-hypothesis test that, besides the conventional radar alternative hypothesis, contains additional hypotheses accounting for the presence of an unknown number of interfering signals. In this context and leveraging the classification capabilities of the Model Order Selection rules, we devise penalized likelihood-ratio-based detection architectures that can establish, as a byproduct, which hypothesis is in force. Moreover, we propose a suboptimum procedure to estimate the angles of arrival of multiple coherent signals ensuring (at least for the considered parameters) almost the same performance as the exhaustive search. Finally, the performance assessment, conducted over simulated data and in comparison with conventional radar detectors, highlights that the proposed architectures can provide satisfactory performance in terms of probability of detection and correct classification.      
### 16.Adorym: A multi-platform generic x-ray image reconstruction framework based on automatic differentiation  [ :arrow_down: ](https://arxiv.org/pdf/2012.12686.pdf)
>  We describe and demonstrate an optimization-based x-ray image reconstruction framework called Adorym. Our framework provides a generic forward model, allowing one code framework to be used for a wide range of imaging methods ranging from near-field holography to and fly-scan ptychographic tomography. By using automatic differentiation for optimization, Adorym has the flexibility to refine experimental parameters including probe positions, multiple hologram alignment, and object tilts. It is written with strong support for parallel processing, allowing large datasets to be processed on high-performance computing systems. We demonstrate its use on several experimental datasets to show improved image quality through parameter refinement.      
### 17.Assume/Guarantee Contracts for Dynamical Systems: Theory and Computational Tools  [ :arrow_down: ](https://arxiv.org/pdf/2012.12657.pdf)
>  Modern engineering systems include many components of different types and functions. Verifying that these systems satisfy given specifications can be an arduous task, as most formal verification methods are limited to systems of moderate size. Recently, contract theory has been proposed as a modular framework for defining specifications. In this paper, we present a contract theory for discrete-time dynamical control systems relying on assume/guarantee contracts, which prescribe assumptions on the input of the system and guarantees on the output. We then focus on contracts defined by linear constraints, and develop efficient computational tools for verification of satisfaction and refinement based on linear programming. We exemplify these tools in a simulation example, proving safety for a two-vehicle autonomous driving setting.      
### 18.Prognostic Power of Texture Based Morphological Operations in a Radiomics Study for Lung Cancer  [ :arrow_down: ](https://arxiv.org/pdf/2012.12652.pdf)
>  The importance of radiomics features for predicting patient outcome is now well-established. Early study of prognostic features can lead to a more efficient treatment personalisation. For this reason new radiomics features obtained through mathematical morphology-based operations are proposed. Their study is conducted on an open database of patients suffering from Nonsmall Cells Lung Carcinoma (NSCLC). The tumor features are extracted from the CT images and analyzed via PCA and a Kaplan-Meier survival analysis in order to select the most relevant ones. Among the 1,589 studied features, 32 are found relevant to predict patient survival: 27 classical radiomics features and five MM features (including both granularity and morphological covariance features). These features will contribute towards the prognostic models, and eventually to clinical decision making and the course of treatment for patients.      
### 19.Parallelized Instantaneous Velocity and Heading Estimation of Objects using Single Imaging Radar  [ :arrow_down: ](https://arxiv.org/pdf/2012.12618.pdf)
>  The development of high-resolution imaging radars introduce a plethora of useful applications, particularly in the automotive sector. With increasing attention on active transport safety and autonomous driving, these imaging radars are set to form the core of an autonomous engine. One of the most important tasks of such high-resolution radars is to estimate the instantaneous velocities and heading angles of the detected objects (vehicles, pedestrians, etc.). Feasible estimation methods should be fast enough in real-time scenarios, bias-free and robust against micro-Dopplers, noise and other systemic variations. This work proposes a parallel-computing scheme that achieves a real-time and accurate implementation of vector velocity determination using frequency modulated continuous wave (FMCW) radars. The proposed scheme is tested against traffic data collected using an FMCW radar at a center frequency of 78.6 GHz and a bandwidth of 4 GHz. Experiments show that the parallel algorithm presented performs much faster than its conventional counterparts without any loss in precision.      
### 20.Single Stage PFC Flyback AC-DC Converter Design  [ :arrow_down: ](https://arxiv.org/pdf/2012.12610.pdf)
>  This paper discusses a 100 W single stage Power Factor Correction (PFC) flyback converter operating in boundary mode constant ON time methodology using a synchronous MOS-FET rectifier on the secondary side to achieve higher efficiency. Unlike conventional designs which use two stage approach such as PFC plus a LLC resonant stage or a two stage PFC plus flyback, the proposed design integrates the PFC and constant voltage regulation in a single stage without compromising the efficiency of the converter. The proposed design is advantageous as it has a lower component count. A design of 100 W flyback operating from universal input AC line voltage is demonstrated in this paper. The experimental results show that the power factor (PF) is greater than 0.92 and total harmonic distortion (iTHD) is less than 20% for a load varying from 25 % to 100 %. The experimental results show the advantages of a single stage design.      
### 21.GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly  [ :arrow_down: ](https://arxiv.org/pdf/2012.12561.pdf)
>  Intratumoral nanoparticles (NPs) distribution is critical for the diagnostic and therapeutic effect, but methods to predict the distribution remain unavailable due to the complex bio-nano interactions. Here, we developed a Generative Adversarial Network for Distribution Analysis (GANDA) to make pixels-to-pixels prediction of the NPs distribution across tumors. This predictive model used deep learning approaches to automatically learn the features of tumor vessels and cell nuclei from whole-slide images of tumor sections. We showed that the GANDA could generate images of NPs distribution with the same spatial resolution as original images of tumor vessels and nuclei. The GANDA enabled quantitative analysis of NPs distribution (R2=0.93) and extravasation without knowing their real distribution. This model provides opportunities to investigate how influencing factors affect NPs distribution in individual tumors and may guide nanomedicine optimization for personalized treatments.      
### 22.StainNet: a fast and robust stain normalization network  [ :arrow_down: ](https://arxiv.org/pdf/2012.12535.pdf)
>  Pathological images may have large variabilities in color intensities due to differences in staining process, operator ability, and scanner specifications. These variations hamper the performance of computer-aided diagnosis (CAD) systems. Stain normalization is used to reduce the variability in color intensities and increase the prediction accuracy. However, the conventional methods highly depend on a reference image, and the current deep learning based methods may have a wrong change in color intensities or texture. In this paper, a fully 1x1 convolutional stain normalization network with only 1.28K parameters is proposed. Our StainNet can learn the color mapping relation from the whole dataset and adjust the color value depended on a single pixel. The proposed method outperforms the state-of-art methods and achieves better accuracy and image quality.      
### 23.Diabetic Retinopathy Grading System Based on Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.12515.pdf)
>  Much effort is being made by the researchers in order to detect and diagnose diabetic retinopathy (DR) accurately automatically. The disease is very dangerous as it can cause blindness suddenly if it is not continuously screened. Therefore, many computers aided diagnosis (CAD) systems have been developed to diagnose the various DR grades. Recently, many CAD systems based on deep learning (DL) methods have been adopted to get deep learning merits in diagnosing the pathological abnormalities of DR disease. In this paper, we present a full based-DL CAD system depending on multi-label classification. In the proposed DL CAD system, we present a customized efficientNet model in order to diagnose the early and advanced grades of the DR disease. Learning transfer is very useful in training small datasets. We utilized IDRiD dataset. It is a multi-label dataset. The experiments manifest that the proposed DL CAD system is robust, reliable, and deigns promising results in detecting and grading DR. The proposed system achieved accuracy (ACC) equals 86%, and the Dice similarity coefficient (DSC) equals 78.45.      
### 24.Shaping the Transient Response of Nonlinear Systems to Satisfy a Class of Integral Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2012.12493.pdf)
>  We consider the problem of shaping the transient step response of nonlinear systems to satisfy a class of integral constraints. Such constraints are inherent in hybrid energy systems consisting of energy sources and storage elements. While typical transient specifications aim to minimize overshoot, this problem is unique in that it requires the presence of an appreciable overshoot to satisfy the foregoing constraints. The problem was previously studied in the context of stable linear systems. A combined integral and feedforward control, that requires minimal knowledge of the plant model, was shown to make the system amenable to meeting such constraints. This paper extends that work to nonlinear systems and proves the effectiveness of the same compensation structure under added conditions. Broadly, it is shown that the integral constraint is satisfied when this compensation structure is applied to nonlinear systems with stable open-loop step response and a positive DC gain. However, stability of the resulting closed-loop system mandates bounds on the controller gain.      
### 25.Towards Boosting the Channel Attention in Real Image Denoising : Sub-band Pyramid Attention  [ :arrow_down: ](https://arxiv.org/pdf/2012.12481.pdf)
>  Convolutional layers in Artificial Neural Networks (ANN) treat the channel features equally without feature selection flexibility. While using ANNs for image denoising in real-world applications with unknown noise distributions, particularly structured noise with learnable patterns, modeling informative features can substantially boost the performance. Channel attention methods in real image denoising tasks exploit dependencies between the feature channels, hence being a frequency component filtering mechanism. Existing channel attention modules typically use global statics as descriptors to learn the inter-channel correlations. This method deems inefficient at learning representative coefficients for re-scaling the channels in frequency level. This paper proposes a novel Sub-band Pyramid Attention (SPA) based on wavelet sub-band pyramid to recalibrate the frequency components of the extracted features in a more fine-grained fashion. We equip the SPA blocks on a network designed for real image denoising. Experimental results show that the proposed method achieves a remarkable improvement than the benchmark naive channel attention block. Furthermore, our results show how the pyramid level affects the performance of the SPA blocks and exhibits favorable generalization capability for the SPA blocks.      
### 26.Multi-Contrast Computed Tomography Healthy Kidney Atlas  [ :arrow_down: ](https://arxiv.org/pdf/2012.12432.pdf)
>  The construction of three-dimensional multi-modal tissue maps provides an opportunity to spur interdisciplinary innovations across temporal and spatial scales through information integration. While the preponderance of effort is allocated to the cellular level and explore the changes in cell interactions and organizations, contextualizing findings within organs and systems is essential to visualize and interpret higher resolution linkage across scales. There is a substantial normal variation of kidney morphometry and appearance across body size, sex, and imaging protocols in abdominal computed tomography (CT). A volumetric atlas framework is needed to integrate and visualize the variability across scales. However, there is no abdominal and retroperitoneal organs atlas framework for multi-contrast CT. Hence, we proposed a high-resolution CT retroperitoneal atlas specifically optimized for the kidney across non-contrast CT and early arterial, late arterial, venous and delayed contrast enhanced CT. Briefly, we introduce a deep learning-based volume of interest extraction method and an automated two-stage hierarchal registration pipeline to register abdominal volumes to a high-resolution CT atlas template. To generate and evaluate the atlas, multi-contrast modality CT scans of 500 subjects (without reported history of renal disease, age: 15-50 years, 250 males &amp; 250 females) were processed. We demonstrate a stable generalizability of the atlas template for integrating the normal kidney variation from small to large, across contrast modalities and populations with great variability of demographics. The linkage of atlas and demographics provided a better understanding of the variation of kidney anatomy across populations.      
### 27.RAP-Net: Coarse-to-Fine Multi-Organ Segmentation with Single Random Anatomical Prior  [ :arrow_down: ](https://arxiv.org/pdf/2012.12425.pdf)
>  Performing coarse-to-fine abdominal multi-organ segmentation facilitates to extract high-resolution segmentation minimizing the lost of spatial contextual information. However, current coarse-to-refine approaches require a significant number of models to perform single organ refine segmentation corresponding to the extracted organ region of interest (ROI). We propose a coarse-to-fine pipeline, which starts from the extraction of the global prior context of multiple organs from 3D volumes using a low-resolution coarse network, followed by a fine phase that uses a single refined model to segment all abdominal organs instead of multiple organ corresponding models. We combine the anatomical prior with corresponding extracted patches to preserve the anatomical locations and boundary information for performing high-resolution segmentation across all organs in a single model. To train and evaluate our method, a clinical research cohort consisting of 100 patient volumes with 13 organs well-annotated is used. We tested our algorithms with 4-fold cross-validation and computed the Dice score for evaluating the segmentation performance of the 13 organs. Our proposed method using single auto-context outperforms the state-of-the-art on 13 models with an average Dice score 84.58% versus 81.69% (p&lt;0.0001).      
### 28.Understanding UAV-Based WPCN-Aided Capabilities for Offshore Monitoring Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.12424.pdf)
>  Despite the immense progress in the recent years, efficient solutions for monitoring remote areas are still missing today. This is especially notable in the context of versatile maritime and offshore use cases, owing to a broader span of operating regions and a lack of radio network infrastructures. In this article, we address the noted challenge by delivering a conceptual solution based on the convergence of three emerging technologies -- unmanned aerial vehicles (UAVs), battery-less sensors, and wireless powered communication networks (WPCNs). Our contribution offers a systematic description of the ecosystem related to the proposed solution by identifying its key actors and design dimensions together with the relevant resources and performance metrics. A system-level modeling-based evaluation of an illustrative scenario delivers deeper insights into the considered operation and the associated trade-offs. Further, unresolved challenges and perspective directions are underpinned for a subsequent study.      
### 29.Towards Histopathological Stain Invariance by Unsupervised Domain Augmentation using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.12413.pdf)
>  The application of supervised deep learning methods in digital pathology is limited due to their sensitivity to domain shift. Digital Pathology is an area prone to high variability due to many sources, including the common practice of evaluating several consecutive tissue sections stained with different staining protocols. Obtaining labels for each stain is very expensive and time consuming as it requires a high level of domain knowledge. In this article, we propose an unsupervised augmentation approach based on adversarial image-to-image translation, which facilitates the training of stain invariant supervised convolutional neural networks. By training the network on one commonly used staining modality and applying it to images that include corresponding, but differently stained, tissue structures, the presented method demonstrates significant improvements over other approaches. These benefits are illustrated in the problem of glomeruli segmentation in seven different staining modalities (PAS, Jones H&amp;E, CD68, Sirius Red, CD34, H&amp;E and CD3) and analysis of the learned representations demonstrate their stain invariance.      
### 30.QuickTumorNet: Fast Automatic Multi-Class Segmentation of Brain Tumors  [ :arrow_down: ](https://arxiv.org/pdf/2012.12410.pdf)
>  Non-invasive techniques such as magnetic resonance imaging (MRI) are widely employed in brain tumor diagnostics. However, manual segmentation of brain tumors from 3D MRI volumes is a time-consuming task that requires trained expert radiologists. Due to the subjectivity of manual segmentation, there is low inter-rater reliability which can result in diagnostic discrepancies. As the success of many brain tumor treatments depends on early intervention, early detection is paramount. In this context, a fully automated segmentation method for brain tumor segmentation is necessary as an efficient and reliable method for brain tumor detection and quantification. In this study, we propose an end-to-end approach for brain tumor segmentation, capitalizing on a modified version of QuickNAT, a brain tissue type segmentation deep convolutional neural network (CNN). Our method was evaluated on a data set of 233 patient's T1 weighted images containing three tumor type classes annotated (meningioma, glioma, and pituitary). Our model, QuickTumorNet, demonstrated fast, reliable, and accurate brain tumor segmentation that can be utilized to assist clinicians in diagnosis and treatment.      
### 31.Performance Analysis of Adaptive Dynamic Tube MPC  [ :arrow_down: ](https://arxiv.org/pdf/2012.12403.pdf)
>  Model predictive control (MPC) is an effective method for control of constrained systems but is susceptible to the external disturbances and modeling error often encountered in real-world applications. To address these issues, techniques such as Tube MPC (TMPC) utilize an ancillary offline-generated robust controller to ensure that the system remains within an invariant set, referred to as a tube, around an online-generated trajectory. However, TMPC is unable to modify its tube and ancillary controller in response to changing state-dependent uncertainty, often resulting in overly-conservative solutions. Dynamic Tube MPC (DTMPC) addresses these problems by simultaneously optimizing the desired trajectory and tube geometry online. Building upon this framework, Adaptive DTMPC (ADTMPC) produces better model approximations by reducing model uncertainty, resulting in more accurate control policies. This work presents an experimental analysis and performance evaluation of TMPC, DTMPC, and ADTMPC for an uncertain nonlinear system. In particular, DTMPC is shown to outperform TMPC because it is able to dynamically adjust to changing environments, limiting aggressive control and conservative behavior to only the cases when the constraints and uncertainty require it. Applied to a pendulum testbed, this enables DTMPC to use up to 30% less control effort while achieving up to 80% higher speeds. This performance is further improved by ADTMPC, which reduces the feedback control effort by up to another 35%, while delivering up to 34% better trajectory tracking. This analysis establishes that the DTMPC and ADTMPC frameworks yield significantly more effective robust control policies for systems with changing uncertainty, goals, and operating conditions.      
### 32.Millimeter-Wave Circular Synthetic Aperture Radar Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2012.12389.pdf)
>  In this paper, we present a high resolution microwave imaging technique using a compact and low cost single channel Frequency Modulated Continuous Wave (FMCW) radar based on Circular Synthetic Aperture Radar (CSAR) technique. We develop an algorithm to reconstruct the image from the raw data and analyse different aspects of the system analytically. Furthermore, we discuss the differences between the proposed systems in the literature and the one presented in this work. <br>Finally, we apply the proposed approach to the experimental data collected from a single channel FMCW radar operating at $\rm 79 \;GHz$ and present the results.      
### 33.Real-Time Vehicular Wireless System-Level Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2012.12331.pdf)
>  Future automation and control units for advanced driver assistance systems (ADAS) will exchange sensor and kinematic data with nearby vehicles using wireless communication links to improve traffic safety. In this paper we present an accurate real-time system-level simulation for multi-vehicle communication scenarios to support the development and test of connected ADAS systems. The physical and data-link layer are abstracted and provide the frame error rate (FER) to a network simulator. The FER is strongly affected by the non-stationary doubly dispersive fading process of the vehicular radio communication channel. We use a geometry-based stochastic channel model (GSCM) to enable a simplified but still accurate representation of the non-stationary vehicular fading process. The propagation path parameters of the GSCM are used to efficiently compute the time-variant condensed radio channel parameters per stationarity region of each communication link during run-time. Five condensed radio channel parameters mainly determine the FER forming a parameter vector: path loss, root mean square delay spread, Doppler bandwidth, $K$-factor, and line-of-sight Doppler shift. We measure the FER for a pre-defined set of discrete grid points of the parameter vector using a channel emulator and a given transmitter-receiver modem pair. The FER data is stored in a table and looked up during run-time of the real-time system-level simulation. We validate our methodology using empirical measurement data from a street crossing scenarios demonstrating a close match in terms of FER between simulation and measurement.      
### 34.EQ-Net: A Unified Deep Learning Framework for Log-Likelihood Ratio Estimation and Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2012.12843.pdf)
>  In this work, we introduce EQ-Net: the first holistic framework that solves both the tasks of log-likelihood ratio (LLR) estimation and quantization using a data-driven method. We motivate our approach with theoretical insights on two practical estimation algorithms at the ends of the complexity spectrum and reveal a connection between the complexity of an algorithm and the information bottleneck method: simpler algorithms admit smaller bottlenecks when representing their solution. This motivates us to propose a two-stage algorithm that uses LLR compression as a pretext task for estimation and is focused on low-latency, high-performance implementations via deep neural networks. We carry out extensive experimental evaluation and demonstrate that our single architecture achieves state-of-the-art results on both tasks when compared to previous methods, with gains in quantization efficiency as high as $20\%$ and reduced estimation latency by up to $60\%$ when measured on general purpose and graphical processing units (GPU). In particular, our approach reduces the GPU inference latency by more than two times in several multiple-input multiple-output (MIMO) configurations. Finally, we demonstrate that our scheme is robust to distributional shifts and retains a significant part of its performance when evaluated on 5G channel models, as well as channel estimation errors.      
### 35.Focal Frequency Loss for Generative Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.12821.pdf)
>  Despite the remarkable success of generative models in creating photorealistic images using deep neural networks, gaps could still exist between the real and generated images, especially in the frequency domain. In this study, we find that narrowing the frequency domain gap can ameliorate the image synthesis quality further. To this end, we propose the focal frequency loss, a novel objective function that brings optimization of generative models into the frequency domain. The proposed loss allows the model to dynamically focus on the frequency components that are hard to synthesize by down-weighting the easy frequencies. This objective function is complementary to existing spatial losses, offering great impedance against the loss of important frequency information due to the inherent crux of neural networks. We demonstrate the versatility and effectiveness of focal frequency loss to improve various baselines in both perceptual quality and quantitative performance.      
### 36.Warping of Radar Data into Camera Image for Cross-Modal Supervision in Automotive Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.12809.pdf)
>  In this paper, we present a novel framework to project automotive radar range-Doppler (RD) spectrum into camera image. The utilized warping operation is designed to be fully differentiable, which allows error backpropagation through the operation. This enables the training of neural networks (NN) operating exclusively on RD spectrum by utilizing labels provided from camera vision models. As the warping operation relies on accurate scene flow, additionally, we present a novel scene flow estimation algorithm fed from camera, lidar and radar, enabling us to improve the accuracy of the warping operation. We demonstrate the framework in multiple applications like direction-of-arrival (DoA) estimation, target detection, semantic segmentation and estimation of radar power from camera data. Extensive evaluations have been carried out for the DoA application and suggest superior quality for NN based estimators compared to classical estimators. The novel scene flow estimation approach is benchmarked against state-of-the-art scene flow algorithms and outperforms them by roughly a third.      
### 37.Strategic Bidding in Electricity Markets with Convexfied AC Power Flow Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2012.12791.pdf)
>  This paper presents a tractable algorithm to procure the strategic bidding of generation units in an electricity market with AC power flow constraints. The strategic bidding problem is formulated as a profit maximization problem constrained by the market-clearing problem. The market-clearing problem which is solved by the independent system operator (ISO) is formulated as a cost minimization problem with a non-linear and non-convex AC power flow model. The problem is formulated as a bi-level optimization problem, where the upper-level problem is the profit maximization problem and the lower-level problem is the market-clearing problem. To solve the bi-level problem, the lower-level problem is reformulated into second-order conic programming (SOCP) relaxation form. The convexified lower-level problem is then presented in a closed-form by the set of primal-dual constraints. However, the strategic bidding encounters nonlinear terms in the lower-level problem. The dual form of the lower-level problem and complementary slackness conditions are leveraged to replace a set of nonlinear terms with equivalent mixed-integer terms. The merit of the procured strategic bidding is illustrated in the case study by comparison with a market-clearing problem with DC power flow constraints and the original non-convex power flow formulation.      
### 38.Secure Wireless Communication in RIS-Aided MISO Systems with Hardware Impairments  [ :arrow_down: ](https://arxiv.org/pdf/2012.12733.pdf)
>  In practice, residual transceiver hardware impairments inevitably lead to distortion noise which causes the performance loss. In this paper, we study the robust transmission design for a reconfigurable intelligent surface (RIS)-aided secure communication system in the presence of transceiver hardware impairments. We aim for maximizing the secrecy rate while ensuring the transmit power constraint on the active beamforming at the base station and the unit-modulus constraint on the passive beamforming at the RIS. To address this problem, we adopt the alternate optimization method to iteratively optimize one set of variables while keeping the other set fixed. Specifically, the successive convex approximation (SCA) method is used to solve the active beamforming optimization subproblem, while the passive beamforming is obtained by using the semidefinite program (SDP) method. Numerical results illustrate that the proposed transmission design scheme is more robust to the hardware impairments than the conventional non-robust scheme that ignores the impact of the hardware impairments.      
### 39.Simultaneous optimisation of temperature and energy in linear energy system models  [ :arrow_down: ](https://arxiv.org/pdf/2012.12664.pdf)
>  Linear programming is used as a standard tool for optimising unit commitment or power flows in energy supply systems. For heat supply systems, however, it faces a relevant limitation: For them, energy yield depends on the output temperature, thus both quantities would have to be optimised simultaneously and the resulting problem is quadratic. As a solution, we describe a method working with discrete temperature levels. This paper presents mathematical models of various technologies and displays their potential in a case study focused on integrated residential heat and electricity supply. It is shown that the technique yields reasonable results including the choice of operational temperatures.      
### 40.Incremental Text-to-Speech Synthesis Using Pseudo Lookahead with Large Pretrained Language Model  [ :arrow_down: ](https://arxiv.org/pdf/2012.12612.pdf)
>  Text-to-speech (TTS) synthesis, a technique for artificially generating human-like utterances from texts, has dramatically evolved with the advances of end-to-end deep neural network-based methods in recent years. The majority of these methods are sentence-level TTS, which can take into account time-series information in the whole sentence. However, it is necessary to establish incremental TTS, which performs synthesis in smaller linguistic units, to realize low-latency synthesis usable for simultaneous speech-to-speech translation systems. In general, incremental TTS is subject to a trade-off between the latency and quality of output speech. It is challenging to produce high-quality speech with a low-latency setup that does not make much use of an unobserved future sentence (hereafter, "lookahead"). This study proposes an incremental TTS method that uses the pseudo lookahead generated with a language model to consider the future contextual information without increasing latency. Our method can be regarded as imitating a human's incremental reading and uses pretrained GPT2, which accounts for the large-scale linguistic knowledge, for the lookahead generation. Evaluation results show that our method 1) achieves higher speech quality without increasing the latency than the method using only observed information and 2) reduces the latency while achieving the equivalent speech quality to waiting for the future context observation.      
### 41.Dynamics of a Stratified Population of Optimum Seeking Agents on a Network -- Part II: Steady State Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.12604.pdf)
>  In this second part of our work, we study the steady state of the population and the social utility for the three dynamics SSD, NBRD and NRPM; which were introduced in the first part. We provide sufficient conditions on the network based on a maximum payoff density parameter of each node under which there exists a unique Nash equilibrium. We then utilize positive correlation properties of the dynamics to reduce the flow graph in order to provide an upper bound on the steady state social utility. Finally we extend the idea behind the sufficient condition for the existence of a unique Nash equilibrium to partition the graph appropriately in order to provide a lower bound on the steady state social utility. We also illustrate interesting cases as well as our results using simulations.      
### 42.Dynamics of a Stratified Population of Optimum Seeking Agents on a Network -- Part I: Modeling and Convergence Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.12599.pdf)
>  In this work, we consider a population composed of a continuum of agents that seek to maximize a payoff function by moving on a network. The nodes in the network may represent physical locations or abstract choices. The population is stratified and hence agents opting for the same choice may not get the same payoff. In particular, we assume payoff functions that model diminishing returns, that is, agents in "newer" strata of a node receive a smaller payoff compared to "older" strata. In this first part of two-part work, we model the population dynamics under three choice revision policies, each having varying levels of coordination -- i. no coordination and the agents are selfish, ii. coordination among agents in each node and iii. coordination across the entire population. To model the case with selfish agents, we generalize the Smith dynamics to our setting, where we have a stratified population and network constraints. To model nodal coordination, we allow the fraction of population in a node, as a whole, to take the `best response' to the state of the population in the node's neighborhood. For the case of population-wide coordination, we explore a dynamics where the population evolves according to centralized gradient ascent of the social utility, though constrained by the network. In each case, we show that the dynamics has existence and uniqueness of solutions and also show that the solutions from any initial condition asymptotically converge to the set of Nash equilibria.      
### 43.Distributed Adaptive Control: An ideal Cognitive Architecture candidate for managing a robotic recycling plant  [ :arrow_down: ](https://arxiv.org/pdf/2012.12586.pdf)
>  In the past decade, society has experienced notable growth in a variety of technological areas. However, the Fourth Industrial Revolution has not been embraced yet. Industry 4.0 imposes several challenges which include the necessity of new architectural models to tackle the uncertainty that open environments represent to cyber-physical systems (CPS). Waste Electrical and Electronic Equipment (WEEE) recycling plants stand for one of such open environments. Here, CPSs must work harmoniously in a changing environment, interacting with similar and not so similar CPSs, and adaptively collaborating with human workers. In this paper, we support the Distributed Adaptive Control (DAC) theory as a suitable Cognitive Architecture for managing a recycling plant. Specifically, a recursive implementation of DAC (between both single-agent and large-scale levels) is proposed to meet the expected demands of the European Project HR-Recycler. Additionally, with the aim of having a realistic benchmark for future implementations of the recursive DAC, a micro-recycling plant prototype is presented.      
### 44.Code Switching Language Model Using Monolingual Training Data  [ :arrow_down: ](https://arxiv.org/pdf/2012.12543.pdf)
>  Training a code-switching (CS) language model using only monolingual data is still an ongoing research problem. In this paper, a CS language model is trained using only monolingual training data. As recurrent neural network (RNN) models are best suited for predicting sequential data. In this work, an RNN language model is trained using alternate batches from only monolingual English and Spanish data and the perplexity of the language model is computed. From the results, it is concluded that using alternate batches of monolingual data in training reduced the perplexity of a CS language model. The results were consistently improved using mean square error (MSE) in the output embeddings of RNN based language model. By combining both methods, perplexity is reduced from 299.63 to 80.38. The proposed methods were comparable to the language model fine tune with code-switch training data.      
### 45.Comparison of Classification Algorithms Towards Subject-Specific and Subject-Independent BCI  [ :arrow_down: ](https://arxiv.org/pdf/2012.12473.pdf)
>  Motor imagery brain computer interface designs are considered difficult due to limitations in subject-specific data collection and calibration, as well as demanding system adaptation requirements. Recently, subject-independent (SI) designs received attention because of their possible applicability to multiple users without prior calibration and rigorous system adaptation. SI designs are challenging and have shown low accuracy in the literature. Two major factors in system performance are the classification algorithm and the quality of available data. This paper presents a comparative study of classification performance for both SS and SI paradigms. Our results show that classification algorithms for SS models display large variance in performance. Therefore, distinct classification algorithms per subject may be required. SI models display lower variance in performance but should only be used if a relatively large sample size is available. For SI models, LDA and CART had the highest accuracy for small and moderate sample size, respectively, whereas we hypothesize that SVM would be superior to the other classifiers if large training sample-size was available. Additionally, one should choose the design approach considering the users. While the SS design sound more promising for a specific subject, an SI approach can be more convenient for mentally or physically challenged users.      
### 46.Understanding Age of Information in Large-Scale Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.12472.pdf)
>  The notion of age-of-information (AoI) is investigated in the context of large-scale wireless networks, in which transmitters need to send a sequence of information packets, which are generated as independent Bernoulli processes, to their intended receivers over a shared spectrum. Due to interference, the rate of packet depletion at any given node is entangled with both the spatial configurations, which determine the path loss, and temporal dynamics, which influence the active states, of the other transmitters, resulting in the queues to interact with each other in both space and time over the entire network. To that end, variants in the packet update frequency affect not just the inter-arrival time but also the departure process, and the impact of such phenomena on the AoI is not well understood. In this paper, we establish a theoretical framework to characterize the AoI performance in the aforementioned setting. Particularly, tractable expressions are derived for both the peak and average AoI under two different transmission protocols, namely the FCFS and the LCFS-PR. Based on the theoretical outcomes, we find that: i) networks operating under LCFS-PR are able to attain smaller values of peak and average AoI than that under FCFS, whereas the gain is more pronounced when the infrastructure is densely deployed, ii) in sparsely deployed networks, ALOHA with a universally designed channel access probability is not instrumental in reducing the AoI, thus calling for more advanced channel access approaches, and iii) when the infrastructure is densely rolled out, there exists a non-trivial ALOHA channel access probability that minimizes the peak and average AoI under both FCFS and LCFS-PR.      
### 47.A Principle Solution for Enroll-Test Mismatch in Speaker Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2012.12471.pdf)
>  Mismatch between enrollment and test conditions causes serious performance degradation on speaker recognition systems. This paper presents a statistics decomposition (SD) approach to solve this problem. This approach is based on the normalized likelihood (NL) scoring framework, and is theoretically optimal if the statistics on both the enrollment and test conditions are accurate. A comprehensive experimental study was conducted on three datasets with different types of mismatch: (1) physical channel mismatch, (2) speaking style mismatch, (3) near-far recording mismatch. The results demonstrated that the proposed SD approach is highly effective, and outperforms the ad-hoc multi-condition training approach that is commonly adopted but not optimal in theory.      
### 48.Hybrid Feedback for Global Attitude Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2012.12470.pdf)
>  We introduce a new hybrid control strategy, which is conceptually different from the commonly used synergistic hybrid approaches, to efficiently deal with the problem of the undesired equilibria that precludes smooth vectors fields on $SO(3)$ from achieving global stability. The key idea consists in constructing a suitable potential function on $SO(3)\times \mathbb{R}$ involving an auxiliary scalar variable, with flow and jump dynamics, which keeps the state away from the undesired critical points while, at the same time, guarantees a decrease of the potential function over the flows and jumps. Based on this new hybrid mechanism, a hybrid feedback control scheme for the attitude tracking problem on $SO(3)$, endowed with global asymptotic stability and semi-global exponential stability guarantees, is proposed. This control scheme is further improved through a smoothing mechanism that removes the discontinuities in the input torque. The third hybrid control scheme, proposed in this paper, removes the requirement of the angular velocity measurements, while preserving the strong stability guarantees of the first hybrid control scheme. Finally, some simulation results are presented to illustrate the performance of the proposed hybrid controllers.      
### 49.CN-Celeb: multi-genre speaker recognition  [ :arrow_down: ](https://arxiv.org/pdf/2012.12468.pdf)
>  Research on speaker recognition is extending to address the vulnerability in the wild conditions, among which genre mismatch is perhaps the most challenging, for instance, enrollment with reading speech while testing with conversational or singing audio. This mismatch leads to complex and composite inter-session variations, both intrinsic (i.e., speaking style, physiological status) and extrinsic (i.e., recording device, background noise). Unfortunately, the few existing multi-genre corpora are not only limited in size but are also recorded under controlled conditions, which cannot support conclusive research on the multi-genre problem. In this work, we firstly publish CN-Celeb, a large-scale multi-genre corpus that includes in-the-wild speech utterances of 3,000 speakers in 11 different genres. Secondly, using this dataset, we conduct a comprehensive study on the multi-genre phenomenon, in particular the impact of the multi-genre challenge on speaker recognition, and on how to utilize the valuable multi-genre data more efficiently.      
### 50.Demand Variation Impact on Tightness of Convex Relaxation Approaches for the ACOPF Problem  [ :arrow_down: ](https://arxiv.org/pdf/2012.12454.pdf)
>  This paper investigates the impact of the changes in the demand of power systems on the quality of the solution procured by the convex relaxation methods for the AC optimal power flow (ACOPF) problem. This investigation needs various measures to evaluate the tightness of the solution procured by the convex relaxation approaches. Therefore, three tightness measures are leveraged to illustrate the performance of convex relaxation methods under different demand scenarios. The main issue of convex relaxation methods is recovering an optimal solution which is not necessarily feasible for the original non-convex problem in networks with cycles. Thus, a cycle measure is introduced to evaluate the performance of relaxation schemes. The presented case study investigates the merit of using various tightness measures to evaluate the performance of various relaxation methods under different circumstances.      
### 51.SmartShuttle: Model Based Design and Evaluation of Automated On-Demand Shuttles for Solving the First-Mile and Last-Mile Problem in a Smart City  [ :arrow_down: ](https://arxiv.org/pdf/2012.12431.pdf)
>  The final project report for the SmartShuttle sub-project of the Ohio State University is presented in this report. This has been a two year project where the unified, scalable and replicable automated driving architecture introduced by the Automated Driving Lab of the Ohio State University has been further developed, replicated in different vehicles and scaled between different vehicle sizes. A limited scale demonstration was also conducted during the first year of the project. The architecture used was further developed in the second project year including parameter space based low level controller design, perception methods and data collection. Perception sensor and other relevant vehicle data were collected in the second project year. Our approach changed to using soft AVs in a hardware-in-the-loop simulation environment for proof-of-concept testing. Our second year work also had a change of localization from GPS and lidar based SLAM to GPS and map matching using a previously constructed lidar map in a geo-fenced area. An example lidar map was also created. Perception sensor and other collected data and an example lidar map are shared as datasets as further outcomes of the project.      
### 52.CUDA-Accelerated Application Scheduling in Vehicular Clouds Under Advanced Multichannel Operations in WAVE  [ :arrow_down: ](https://arxiv.org/pdf/2012.12419.pdf)
>  This paper presents a novel Advanced Activity-Aware (AAA) scheme to optimize and improve Multi-Channel Operations based on the IEEE 1609.4 standard in wireless access vehicular environments (WAVE). The proposed scheme relies on the awareness of the vehicular safety load to dynamically find an optimal setup for switching between service channel intervals (SCHI) and control channel intervals (CCHI). SCHI are chosen for non-critical applications (e.g. infotainment), while CCHI are utilized for critical applications (e.g. safety-related). We maximize the channel utilization without sacrificing other application requirements such as latency and bandwidth. Our scheme is implemented and evaluated network simulator-3 (NS3). We guarantee the default Synchronization Interval (SI), like implemented by the standard in vehicular ad hoc networks (VANETs), when tested on real-time simulations of vehicular cloud (VC) load and VANET setups using NS3. We also evaluate a Markov Decision Process (MDP) based scheme and a fast greedy heuristic to optimize the problem of vehicular task placement with both IEEE 1609.4 and an opportunistic V2I version of the proposed AAA scheme. Our solution offers the reward of the VC by taking into account the overall utilization of the distributed virtualized VCs resources and vehicular bag-of-tasks (BOTs) placements both sequentially and in parallel. We present the simulation metrics proving that our proposed solution significantly improve the throughput and decreases the average delay of uploaded packets used for non-safety applications, while maintaining reliable communication (via CCHI) for safety-related applications similar to the IEEE 1609.4 standard.      
### 53.Scalable Optical Learning Operator  [ :arrow_down: ](https://arxiv.org/pdf/2012.12404.pdf)
>  Today's heavy machine learning tasks are fueled by large datasets. Computing is performed with power hungry processors whose performance is ultimately limited by the data transfer to and from memory. Optics is one of the powerful means of communicating and processing information and there is intense current interest in optical information processing for realizing high-speed computations. Here we present and experimentally demonstrate an optical computing framework based on spatiotemporal effects in multimode fibers for a range of learning tasks from classifying COVID-19 X-ray lung images and speech recognition to predicting age from face images. The presented framework overcomes the energy scaling problem of existing systems without compromising speed. We leveraged simultaneous, linear, and nonlinear interaction of spatial modes as a computation engine. We numerically and experimentally showed the ability of the method to execute several different tasks with accuracy comparable to a digital implementation. Our results indicate that a powerful supercomputer would be required to duplicate the performance of the multimode fiber-based computer.      
### 54.Workspace Analysis and Optimal Design of Cable-Driven Parallel Robots via Auxiliary Counterbalances  [ :arrow_down: ](https://arxiv.org/pdf/2012.12387.pdf)
>  Cable-driven parallel robots (CDPRs) are widely investigated and applied in the worldwide; however, traditional configurations make them to be limited in reaching their maximum workspace duo to constraints such as the maximum allowable tensions of cables. In this paper, we introduce auxiliary counterbalances to tackle this problem and focus on workspace analysis and optimal design of CDPRs with such systems. Besides, kinematics, dynamics, and parameters optimization formulas and algorithm are provided to maximize the reachable workspace of CDPRs. Case studies for different configurations are presented and discussed. Numerical results suggest the effectiveness of the aforementioned approaches, and the obtained parameters can also be applied for actual CDPRs design.      
### 55.Is the brain macroscopically linear? A system identification of resting state dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2012.12351.pdf)
>  A central challenge in the computational modeling of neural dynamics is the trade-off between accuracy and simplicity. At the level of individual neurons, nonlinear dynamics are both experimentally established and essential for neuronal functioning. An implicit assumption has thus formed that an accurate computational model of whole-brain dynamics must also be highly nonlinear, whereas linear models may provide a first-order approximation. Here, we provide a rigorous and data-driven investigation of this hypothesis at the level of whole-brain blood-oxygen-level-dependent (BOLD) and macroscopic field potential dynamics by leveraging the theory of system identification. Using functional MRI (fMRI) and intracranial EEG (iEEG), we model the resting state activity of 700 subjects in the Human Connectome Project (HCP) and 122 subjects from the Restoring Active Memory (RAM) project using state-of-the-art linear and nonlinear model families. We assess relative model fit using predictive power, computational complexity, and the extent of residual dynamics unexplained by the model. Contrary to our expectations, linear auto-regressive models achieve the best measures across all three metrics, eliminating the trade-off between accuracy and simplicity. To understand and explain this linearity, we highlight four properties of macroscopic neurodynamics which can counteract or mask microscopic nonlinear dynamics: averaging over space, averaging over time, observation noise, and limited data samples. Whereas the latter two are technological limitations and can improve in the future, the former two are inherent to aggregated macroscopic brain activity. Our results, together with the unparalleled interpretability of linear models, can greatly facilitate our understanding of macroscopic neural dynamics and the principled design of model-based interventions for the treatment of neuropsychiatric disorders.      
### 56.Hybrid RIS-Empowered Reflection and Decode-and-Forward Relaying for Coverage Extension  [ :arrow_down: ](https://arxiv.org/pdf/2012.12329.pdf)
>  In this letter, we introduce two hybrid transmission schemes combining a passive reconfigurable intelligent surface (RIS) with decode-and-forward relaying in a synergistic manner. The proposed schemes offer a flexible as well as cost- and power-efficient solution for coverage extension in future generation wireless networks. We present closed-form expressions for the end-to-end signal-to-noise ratio of both schemes and a sequential optimization algorithm for the power allocation and the RIS phase configurations. Our computer simulations and theoretical analysis demonstrate that the RIS and relaying technologies enhance the achievable rate and error performance remarkably when working complementary to each other, rather than being considered as competing technologies.      
### 57.Video Influencers: Unboxing the Mystique  [ :arrow_down: ](https://arxiv.org/pdf/2012.12311.pdf)
>  Influencer marketing is being used increasingly as a tool to reach customers because of the growing popularity of social media stars who primarily reach their audience(s) via custom videos. Despite the rapid growth in influencer marketing, there has been little research on the design and effectiveness of influencer videos. Using publicly available data on YouTube influencer videos, we implement novel interpretable deep learning architectures, supported by transfer learning, to identify significant relationships between advertising content in videos (across text, audio, and images) and video views, interaction rates and sentiment. By avoiding ex-ante feature engineering and instead using ex-post interpretation, our approach avoids making a trade-off between interpretability and predictive ability. We filter out relationships that are affected by confounding factors unassociated with an increase in attention to video elements, thus facilitating the generation of plausible causal relationships between video elements and marketing outcomes which can be tested in the field. A key finding is that brand mentions in the first 30 seconds of a video are on average associated with a significant increase in attention to the brand but a significant decrease in sentiment expressed towards the video. We illustrate the learnings from our approach for both influencers and brands.      
