# ArXiv eess --Tue, 1 Dec 2020
### 1.Reducing Textural Bias Improves Robustness of Deep Segmentation CNNs  [ :arrow_down: ](https://arxiv.org/pdf/2011.15093.pdf)
>  Despite current advances in deep learning, domain shift remains a common problem in medical imaging settings. Recent findings on natural images suggest that deep neural models can show a textural bias when carrying out image classification tasks, which goes against the common understanding of convolutional neural networks (CNNs) recognising objects through increasingly complex representations of shape. This study draws inspiration from recent findings on natural images and aims to investigate ways in which addressing the textural bias phenomenon could be used to bring up the robustness and transferability of deep segmentation models when applied to three-dimensional (3D) medical data. To achieve this, publicly available MRI scans from the Developing Human Connectome Project are used to investigate ways in which simulating textural noise can help train robust models in a complex segmentation task. Our findings illustrate how applying specific types of textural filters prior to training the models can increase their ability to segment scans corrupted by previously unseen noise.      
### 2.Long-range medical image registration through generalized mutual information (GMI): toward a fully automatic volumetric alignment  [ :arrow_down: ](https://arxiv.org/pdf/2011.15049.pdf)
>  Image registration is a key operation in medical image processing, allowing a plethora of applications. Mutual information (MI) is consolidated as a robust similarity metric often used for medical image registration. Although MI provides a robust medical image registration, it usually fails when the needed image transform is too big due to MI local maxima traps. In this paper, we propose and evaluate a generalized parametric MI as an affine registration cost function. We assessed the generalized MI (GMI) functions for separable affine transforms and exhaustively evaluated the GMI mathematical image seeking the maximum registration range through a gradient descent simulation. We also employed Monte Carlo simulation essays for testing translation registering of randomized T1 versus T2 images. GMI functions showed to have smooth isosurfaces driving the algorithm to the global maxima. Results show significantly prolonged registration ranges, avoiding the traps of local maxima. We evaluated a range of [-150mm,150mm] for translations, [-180°,180°] for rotations, [0.5,2] for scales, and [-1,1] for skew with a success rate of 99.99%, 97.58%, 99.99%, and 99.99% respectively for the transforms in the simulated gradient descent. We also obtained 99.75% success in Monte Carlo simulation from 2,000 randomized translations trials with 1,113 subjects T1 and T2 MRI images. The findings point towards the reliability of GMI for long-range registration with enhanced speed performance      
### 3.Unsupervised Deep Video Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2011.15045.pdf)
>  Deep convolutional neural networks (CNNs) currently achieve state-of-the-art performance in denoising videos. They are typically trained with supervision, minimizing the error between the network output and ground-truth clean videos. However, in many applications, such as microscopy, noiseless videos are not available. To address these cases, we build on recent advances in unsupervised still image denoising to develop an Unsupervised Deep Video Denoiser (UDVD). UDVD is shown to perform competitively with current state-of-the-art supervised methods on benchmark datasets, even when trained only on a single short noisy video sequence. Experiments on fluorescence-microscopy and electron-microscopy data illustrate the promise of our approach for imaging modalities where ground-truth clean data is generally not available. In addition, we study the mechanisms used by trained CNNs to perform video denoising. An analysis of the gradient of the network output with respect to its input reveals that these networks perform spatio-temporal filtering that is adapted to the particular spatial structures and motion of the underlying content. We interpret this as an implicit and highly effective form of motion compensation, a widely used paradigm in traditional video denoising, compression, and analysis. Code and iPython notebooks for our analysis are available in <a class="link-external link-https" href="https://sreyas-mohan.github.io/udvd/" rel="external noopener nofollow">this https URL</a> .      
### 4.Image Quality Assessment for Perceptual Image Restoration: A New Dataset, Benchmark and Metric  [ :arrow_down: ](https://arxiv.org/pdf/2011.15002.pdf)
>  Image quality assessment (IQA) is the key factor for the fast development of image restoration (IR) algorithms. The most recent perceptual IR algorithms based on generative adversarial networks (GANs) have brought in significant improvement on visual performance, but also pose great challenges for quantitative evaluation. Notably, we observe an increasing inconsistency between perceptual quality and the evaluation results. We present two questions: Can existing IQA methods objectively evaluate recent IR algorithms? With the focus on beating current benchmarks, are we getting better IR algorithms? To answer the questions and promote the development of IQA methods, we contribute a large-scale IQA dataset, called Perceptual Image Processing ALgorithms (PIPAL) dataset. Especially, this dataset includes the results of GAN-based IR algorithms, which are missing in previous datasets. We collect more than 1.13 million human judgments to assign subjective scores for PIPAL images using the more reliable Elo system. Based on PIPAL, we present new benchmarks for both IQA and SR methods. Our results indicate that existing IQA methods cannot fairly evaluate GAN-based IR algorithms. While using appropriate evaluation methods is important, IQA methods should also be updated along with the development of IR algorithms. At last, we shed light on how to improve the IQA performance on GAN-based distortion. Inspired by the find that the existing IQA methods have an unsatisfactory performance on the GAN-based distortion partially because of their low tolerance to spatial misalignment, we propose to improve the performance of an IQA network on GAN-based distortion by explicitly considering this misalignment. We propose the Space Warping Difference Network, which includes the novel l_2 pooling layers and Space Warping Difference layers. Experiments demonstrate the effectiveness of the proposed method.      
### 5.Early Detection of Thermoacoustic Instabilities in a Cryogenic Rocket Thrust Chamber using Combustion Noise Features and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.14985.pdf)
>  Combustion instabilities are particularly problematic for rocket thrust chambers because of their high energy release rates and their operation close to the structural limits. In the last decades, progress has been made in predicting high amplitude combustion instabilities but still, no reliable prediction ability is given. Reliable early warning signals are the main requirement for active combustion control systems. In this paper, we present a data-driven method for the early detection of thermoacoustic instabilities. Recurrence quantification analysis is used to calculate characteristic combustion features from short-length time series of dynamic pressure sensor data. Features like the recurrence rate are used to train support vector machines to detect the onset of an instability a few hundred milliseconds in advance. The performance of the proposed method is investigated on experimental data from a representative LOX/H$_2$ research thrust chamber. In most cases, the method is able to timely predict two types of thermoacoustic instabilities on test data not used for training. The results are compared with state-of-the-art early warning indicators.      
### 6.MAVIDH Score: A Corona Severity Scoring using Interpretable Chest X-Ray Pathology Features  [ :arrow_down: ](https://arxiv.org/pdf/2011.14983.pdf)
>  The application of computer vision for COVID-19 diagnosis is complex and challenging, given the risks associated with patient misclassifications. Arguably, the primary value of medical imaging for COVID-19 lies rather on patient prognosis. Radiological images can guide physicians assessing the severity of the disease, and a series of images from the same patient at different stages can help to gauge disease progression. Based on these premises, a simple method based on lung-pathology features for scoring disease severity from Chest X-rays is proposed here. As the primary contribution, this method shows to be correlated to patient severity in different stages of disease progression comparatively well when contrasted with other existing methods. An original approach for data selection is also proposed, allowing the simple model to learn the severity-related features. It is hypothesized that the resulting competitive performance presented here is related to the method being feature-based rather than reliant on lung involvement or compromise as others in the literature. The fact that it is simpler and interpretable than other end-to-end, more complex models, also sets aside this work. As the data set is small, bias-inducing artifacts that could lead to overfitting are minimized through an image normalization and lung segmentation step at the learning phase. A second contribution comes from the validation of the results, conceptualized as the scoring of patients groups from different stages of the disease. Besides performing such validation on an independent data set, the results were also compared with other proposed scoring methods in the literature. The expressive results show that although imaging alone is not sufficient for assessing severity as a whole, there is a strong correlation with the scoring system, termed as MAVIDH score, with patient outcome.      
### 7.Free Energy Minimization: A Unified Framework for Modelling, Inference, Learning,and Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2011.14963.pdf)
>  The goal of these lecture notes is to review the problem of free energy minimization as a unified framework underlying the definition of maximum entropy modelling, generalized Bayesian inference, learning with latent variables, statistical learning analysis of generalization,and local optimization. Free energy minimization is first introduced, here and historically, as a thermodynamic principle. Then, it is described mathematically in the context of Fenchel duality. Finally, the mentioned applications to modelling, inference, learning, and optimization are covered starting from basic principles.      
### 8.Extraction of Nystagmus Patterns from Eye-Tracker Data with Convolutional Sparse Coding  [ :arrow_down: ](https://arxiv.org/pdf/2011.14962.pdf)
>  The analysis of the Nystagmus waveforms from eye-tracking records is crucial for the clinicial interpretation of this pathological movement. A major issue to automatize this analysis is the presence of natural eye movements and eye blink artefacts that are mixed with the signal of interest. We propose a method based on Convolutional Dictionary Learning that is able to automaticcaly highlight the Nystagmus waveforms, separating the natural motion from the pathological movements. We show on simulated signals that our method can indeed improve the pattern recovery rate and provide clinical examples to illustrate how this algorithm performs.      
### 9.On Effect of Right-Half-Plane Zero Present in Buck Converters with Input Current Source in Wireless Power Receiver Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.14961.pdf)
>  In wireless power receiver systems, the buck converter is widely used to step down the higher rectified voltage derived from the wireless receiver coil, to a lower output voltage for the immediate battery charging process. In this work, the presence and effect of the right-half-plane (RHP) zeros found in the small-signal inductor-current-to-duty-ratio and output-voltage-to-duty ratio transfer functions of the buck converter in the wireless power receiver system on the control performance, are investigated. It is found and mathematically proved that the RHP zeros are introduced by the current source nature of the system attributed to the series-series compensation and finite DC-link capacitance. The RHP zero not only results in non-monotonic open-loop dynamic response but also complicates the design of feedback control and causes potential closed-loop instability. Theoretical and experimental results are provided to validate the presence of the RHP zeros and their effect on open-loop and closed-loop dynamic responses.      
### 10.On the Matching Equations of Kinetic Energy Shaping in IDA-PBC  [ :arrow_down: ](https://arxiv.org/pdf/2011.14958.pdf)
>  Interconnection and damping assignment passivity-based control scheme has been used to stabilize many physical systems such as underactuated mechanical systems through total energy shaping. In this method, some partial differential equations (PDEs) arisen by kinetic and potential energy shaping, shall be solved analytically. Finding a suitable desired inertia matrix as the solution of nonlinear PDEs related to kinetic energy shaping is a challenging problem. <br>In this paper, a systematic approach to solve this matching equation for systems with one degree of underactuation is proposed. A special structure for desired inertia matrix is proposed to simplify the solution of the corresponding PDE. It is shown that the proposed method is more general than that of some reported methods in the literature. In order to derive a suitable desired inertia matrix, a necessary condition is also derived. The proposed method is applied to three examples, including VTOL aircraft, pendubot and 2D SpiderCrane system.      
### 11.Neighbor Oblivious Learning (NObLe) for Device Localization and Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2011.14954.pdf)
>  On-device localization and tracking are increasingly crucial for various applications. Along with a rapidly growing amount of location data, machine learning (ML) techniques are becoming widely adopted. A key reason is that ML inference is significantly more energy-efficient than GPS query at comparable accuracy, and GPS signals can become extremely unreliable for specific scenarios. To this end, several techniques such as deep neural networks have been proposed. However, during training, almost none of them incorporate the known structural information such as floor plan, which can be especially useful in indoor or other structured environments. In this paper, we argue that the state-of-the-art-systems are significantly worse in terms of accuracy because they are incapable of utilizing these essential structural information. The problem is incredibly hard because the structural properties are not explicitly available, making most structural learning approaches inapplicable. Given that both input and output space potentially contain rich structures, we study our method through the intuitions from manifold-projection. Whereas existing manifold based learning methods actively utilized neighborhood information, such as Euclidean distances, our approach performs Neighbor Oblivious Learning (NObLe). We demonstrate our approach's effectiveness on two orthogonal applications, including WiFi-based fingerprint localization and inertial measurement unit(IMU) based device tracking, and show that it gives significant improvement over state-of-art prediction accuracy.      
### 12.Efficient Assessment of Electricity Distribution Network Adequacy with the Cross-Entropy Method  [ :arrow_down: ](https://arxiv.org/pdf/2011.14937.pdf)
>  Identifying future congestion points in electricity distribution networks is an important challenge distribution system operators face. A proven approach for addressing this challenge is to assess distribution grid adequacy using probabilistic models of future demand. However, computational cost can become a severe challenge when evaluating large probabilistic electricity demand forecasting models with long forecasting horizons. In this paper, Monte Carlo methods are developed to increase the computational efficiency of obtaining asset overload probabilities from a bottom-up stochastic demand model. Cross-entropy optimised importance sampling is contrasted with conventional Monte Carlo sampling. Benchmark results of the proposed methods suggest that the importance sampling-based methods introduced in this work are suitable for estimating rare overload probabilities for assets with a small number of customers.      
### 13.Compact Dual-Polarized Vivaldi Antenna for Ground Penetrating Radar (GPR) Application  [ :arrow_down: ](https://arxiv.org/pdf/2011.14918.pdf)
>  In this paper, a compact dual-polarized Vivaldi antenna is presented. Four Vivaldi elements are used as radiators, and are positioned obliquely and connected in a horn shape. By exciting two sets of elements, two orthogonally polarized radiations can be achieved. The dual-polarized antenna features a low operating frequency band with a wide bandwidth, high port isolation, good directional radiation performance, and a very compact size, making it highly suitable for the ground penetrating radar (GPR) application.      
### 14.Image Denoising for Strong Gaussian Noises With Specialized CNNs for Different Frequency Components  [ :arrow_down: ](https://arxiv.org/pdf/2011.14908.pdf)
>  In machine learning approach to image denoising a network is trained to recover a clean image from a noisy one. In this paper a novel structure is proposed based on training multiple specialized networks as opposed to existing structures that are base on a single network. The proposed model is an alternative for training a very deep network to avoid issues like vanishing or exploding gradient. By dividing a very deep network into two smaller networks the same number of learnable parameters will be available, but two smaller networks should be trained which are easier to train. Over smoothing and waxy artifacts are major problems with existing methods; because the network tries to keep the Mean Square Error (MSE) low for general structures and details, which leads to overlooking of details. This problem is more severe in the presence of strong noise. To reduce this problem, in the proposed structure, the image is decomposed into its low and high frequency components and each component is used to train a separate denoising convolutional neural network. One network is specialized to reconstruct the general structure of the image and the other one is specialized to reconstruct the details. Results of the proposed method show higher peak signal to noise ratio (PSNR), and structural similarity index (SSIM) compared to a popular state of the art denoising method in the presence of strong noises.      
### 15.Battery Asset Management with Cycle Life Prognosis  [ :arrow_down: ](https://arxiv.org/pdf/2011.14903.pdf)
>  Battery Asset Management problem determines the minimum cost replacement schedules for each individual asset in a group of battery assets that operate in parallel. Battery cycle life varies under different operating conditions including temperature, depth of discharge, charge rate, etc., and a battery deteriorates due to usage, which cannot be handled by current asset management models. This paper presents battery cycle life prognosis and its integration with parallel asset management to reduce lifecycle cost of the Battery Energy Storage System (BESS). A nonlinear capacity fade model is incorporated in the parallel asset management model to update battery capacity. Parametric studies have been conducted to explore the influence of different model inputs (e.g. usage rate, unit battery capacity, operating condition and periodical demand) for a five-year time horizon. Experiment results verify the reasonableness of this new framework and suggest that the increase in battery lifetime leads to decrease in lifecycle cost.      
### 16.Secure Vehicular Communications through Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2011.14899.pdf)
>  Reconfigurable intelligent surfaces (RIS) is considered as a revolutionary technique to improve the wireless system performance by reconfiguring the radio wave propagation environment artificially. Motivated by the potential of RIS in vehicular networks, we analyze the secrecy outage performance of RIS-aided vehicular communications in this paper. More specifically, two vehicular communication scenarios are considered, i.e., a vehicular-to-vehicular (V2V) communication where the RIS acts as a relay and a vehicular-to-infrastructure (V2I) scenario where the RIS functions as the receiver. In both scenarios, a passive eavesdropper is present attempting to retrieve the transmitted information. Closed-form expressions for the secrecy outage probability (SOP) are derived and verified. The results demonstrate the potential of improving secrecy with the aid of RIS under both V2V and V2I communications.      
### 17.Coordination of Heterogeneous Deferrable Loads using the F-MBC Mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2011.14898.pdf)
>  Increasing participation of prosumers in the electricity grid calls for efficient operational strategies for utilizing the flexibility offered by Distributed Energy Resources (DER) to match supply and demand. This paper investigates the coordination performance of a recently proposed coordination scheme for deferrable loads: Forecast Mediated Market Based Control (F-MBC). Enhancements are made to the simulation setup to enable an analysis of performance in realistic scenarios, with heterogeneous loads and an open-ended simulation horizon. Operational scenarios were formulated to showcase the ability of F-MBC to schedule heterogeneous populations of deferrable loads with dynamic load profiles, supported by a mix of renewable and flexible generation. Availability patterns of devices were generated to take into account varying user preferences. Simulation results indicate that F-MBC was able to achieve good distributed scheduling performance for devices with a high initial power consumption. However, performance for devices with low initial power consumption has been found to be less satisfactory. Several directions for further improvement of the F-MBC scheme and its applications are identified.      
### 18.Uncertainty-driven ensembles of deep architectures for multiclass classification. Application to COVID-19 diagnosis in chest X-ray images  [ :arrow_down: ](https://arxiv.org/pdf/2011.14894.pdf)
>  Respiratory diseases kill million of people each year. Diagnosis of these pathologies is a manual, time-consuming process that has inter and intra-observer variability, delaying diagnosis and treatment. The recent COVID-19 pandemic has demonstrated the need of developing systems to automatize the diagnosis of pneumonia, whilst Convolutional Neural Network (CNNs) have proved to be an excellent option for the automatic classification of medical images. However, given the need of providing a confidence classification in this context it is crucial to quantify the reliability of the model's predictions. In this work, we propose a multi-level ensemble classification system based on a Bayesian Deep Learning approach in order to maximize performance while quantifying the uncertainty of each classification decision. This tool combines the information extracted from different architectures by weighting their results according to the uncertainty of their predictions. Performance of the Bayesian network is evaluated in a real scenario where simultaneously differentiating between four different pathologies: control vs bacterial pneumonia vs viral pneumonia vs COVID-19 pneumonia. A three-level decision tree is employed to divide the 4-class classification into three binary classifications, yielding an accuracy of 98.06% and overcoming the results obtained by recent literature. The reduced preprocessing needed for obtaining this high performance, in addition to the information provided about the reliability of the predictions evidence the applicability of the system to be used as an aid for clinicians.      
### 19.Deep Interactive Denoiser (DID) for X-Ray Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2011.14873.pdf)
>  Low dose computed tomography (LDCT) is desirable for both diagnostic imaging and image guided interventions. Denoisers are openly used to improve the quality of LDCT. Deep learning (DL)-based denoisers have shown state-of-the-art performance and are becoming one of the mainstream methods. However, there exists two challenges regarding the DL-based denoisers: 1) a trained model typically does not generate different image candidates with different noise-resolution tradeoffs which sometimes are needed for different clinical tasks; 2) the model generalizability might be an issue when the noise level in the testing images is different from that in the training dataset. To address these two challenges, in this work, we introduce a lightweight optimization process at the testing phase on top of any existing DL-based denoisers to generate multiple image candidates with different noise-resolution tradeoffs suitable for different clinical tasks in real-time. Consequently, our method allows the users to interact with the denoiser to efficiently review various image candidates and quickly pick up the desired one, and thereby was termed as deep interactive denoiser (DID). Experimental results demonstrated that DID can deliver multiple image candidates with different noise-resolution tradeoffs, and shows great generalizability regarding various network architectures, as well as training and testing datasets with various noise levels.      
### 20.ViDi: Descriptive Visual Data Clustering as Radiologist Assistant in COVID-19 Streamline Diagnostic  [ :arrow_down: ](https://arxiv.org/pdf/2011.14871.pdf)
>  In the light of the COVID-19 pandemic, deep learning methods have been widely investigated in detecting COVID-19 from chest X-rays. However, a more pragmatic approach to applying AI methods to a medical diagnosis is designing a framework that facilitates human-machine interaction and expert decision making. Studies have shown that categorization can play an essential rule in accelerating real-world decision making. Inspired by descriptive document clustering, we propose a domain-independent explanatory clustering framework to group contextually related instances and support radiologists' decision making. While most descriptive clustering approaches employ domain-specific characteristics to form meaningful clusters, we focus on model-level explanation as a more general-purpose element of every learning process to achieve cluster homogeneity. We employ DeepSHAP to generate homogeneous clusters in terms of disease severity and describe the clusters using favorable and unfavorable saliency maps, which visualize the class discriminating regions of an image. These human-interpretable maps complement radiologist knowledge to investigate the whole cluster at once. Besides, as part of this study, we evaluate a model based on VGG-19, which can identify COVID and pneumonia cases with a positive predictive value of 95% and 97%, respectively, comparable to the recent explainable approaches for COVID diagnosis.      
### 21.Output-Feedback Symbolic Control  [ :arrow_down: ](https://arxiv.org/pdf/2011.14848.pdf)
>  Symbolic control is a an abstraction-based controller synthesis approach that provides, algorithmically, certifiable-by-construction controllers for cyber-physical systems. Current methodologies of symbolic control usually assume that full-state information is available. This is not suitable for many real-world applications with partially-observable states or output information. This article introduces a framework for output-feedback symbolic control. We propose relations between original systems and their symbolic models based on outputs. They enable designing symbolic controllers and refining them to enforce complex requirements on original systems. To demonstrate the effectiveness of the proposed framework, we provide three different methodologies. They are applicable to a wide range of linear and nonlinear systems, and support general logic specifications.      
### 22.Sparse-View Spectral CT Reconstruction Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.14842.pdf)
>  Spectral CT is an emerging technology capable of providing high chemical specificity, which is crucial for many applications such as detecting threats in luggage. Such applications often require both fast and high-quality image reconstruction based on sparse-view (few) projections. The conventional FBP method is fast but it produces low-quality images dominated by noise and artifacts when few projections are available. Iterative methods with, e.g., TV regularizers can circumvent that but they are computationally expensive, with the computational load proportionally increasing with the number of spectral channels. Instead, we propose an approach for fast reconstruction of sparse-view spectral CT data using U-Net with multi-channel input and output. The network is trained to output high-quality images from input images reconstructed by FBP. The network is fast at run-time and because the internal convolutions are shared between the channels, the computation load increases only at the first and last layers, making it an efficient approach to process spectral data with a large number of channels. We validated our approach using real CT scans. The results show qualitatively and quantitatively that our approach is able to outperform the state-of-the-art iterative methods. Furthermore, the results indicate that the network is able to exploit the coupling of the channels to enhance the overall quality and robustness.      
### 23.Reducing Road Vehicle Fuel Consumption by Exploiting Connectivity and Automation: A Literature Survey  [ :arrow_down: ](https://arxiv.org/pdf/2011.14805.pdf)
>  This paper examines the degree to which connectivity and automation can potentially reduce the overall fuel consumption of on-road vehicles. The paper begins with a simulation study highlighting the tradeoff between: (i) the fuel that a vehicle can save through speed trajectory shaping, versus (ii) the additional inter-vehicle spacing needed for this trajectory shaping to be feasible. This study shows that connectivity and automation are essential, rather than merely useful, for substantial reductions in the fuel consumed by fixed on-road vehicle powertrain/chassis configurations in traffic. Motivated by this insight, we survey the literature on the fuel savings achievable through different connected/automated vehicle technologies. This includes optimal vehicle routing, eco-arrival/departure at intersections, platooning, speed trajectory optimization, predictive driveline disengagement, predictive gear shifting, and predictive powertrain accessory control. This survey shows that the ability to shape vehicle speed trajectories collaboratively plays a dominant role in reducing urban/suburban fuel consumption, while platooning plays a dominant role in influencing the attainable fuel savings on the highway. Moreover, the survey shows that the degree to which connectivity/automation can reduce on-road vehicle fuel consumption, in both urban/suburban and highway settings, depends critically on the integration of powertrain- and chassis-level control.      
### 24.Deep learning approach to left ventricular non-compaction measurement  [ :arrow_down: ](https://arxiv.org/pdf/2011.14773.pdf)
>  Left ventricular non-compaction (LVNC) is a rare cardiomyopathy characterized by abnormal trabeculations in the left ventricle cavity. Although traditional computer vision approaches exist for LVNC diagnosis, deep learning-based tools could not be found in the literature. In this paper, a first approach using convolutional neural networks (CNNs) is presented. Four CNNs are trained to automatically segment the compacted and trabecular areas of the left ventricle for a population of patients diagnosed with Hypertrophic cardiomyopathy. Inference results confirm that deep learning-based approaches can achieve excellent results in the diagnosis and measurement of LVNC. The two best CNNs (U-Net and Efficient U-Net B1) perform image segmentation in less than 0.2 s on a CPU and in less than 0.01 s on a GPU. Additionally, a subjective evaluation of the output images with the identified zones is performed by expert cardiologists, with a perfect visual agreement for all the slices, outperforming already existing automatic tools.      
### 25.DRDr II: Detecting the Severity Level of Diabetic Retinopathy Using Mask RCNN and Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.14733.pdf)
>  DRDr II is a hybrid of machine learning and deep learning worlds. It builds on the successes of its antecedent, namely, DRDr, that was trained to detect, locate, and create segmentation masks for two types of lesions (exudates and microaneurysms) that can be found in the eyes of the Diabetic Retinopathy (DR) patients; and uses the entire model as a solid feature extractor in the core of its pipeline to detect the severity level of the DR cases. We employ a big dataset with over 35 thousand fundus images collected from around the globe and after 2 phases of preprocessing alongside feature extraction, we succeed in predicting the correct severity levels with over 92% accuracy.      
### 26.Learning-based lossless compression of 3D point cloud geometry  [ :arrow_down: ](https://arxiv.org/pdf/2011.14700.pdf)
>  This paper presents a learning-based, lossless compression method for static point cloud geometry, based on context-adaptive arithmetic coding. Unlike most existing methods working in the octree domain, our encoder operates in a hybrid mode, mixing octree and voxel-based coding. We adaptively partition the point cloud into multi-resolution voxel blocks according to the point cloud structure and use octree to signal the partitioning. On the one hand, octree representation can eliminate the sparsity in the point cloud. On the other hand, in the voxel domain, convolutions can be naturally expressed, and geometric information (i.e., planes, surfaces, etc.) is explicitly processed by a neural network. Our context model benefits from these properties and learns a probability distribution of the voxels using a deep convolutional neural network with masked filters, called VoxelDNN. Experiments show that our method outperforms the state-of-the-art MPEG G-PCC standard with average rate savings of 28% on a diverse set of point clouds from the Microsoft Voxelized Upper Bodies (MVUB) and MPEG.      
### 27.Thermal Analysis of PEM Fuel Cell and Lithium Ion Battery Pack in Confined Space  [ :arrow_down: ](https://arxiv.org/pdf/2011.14674.pdf)
>  Hybrid energy storage systems (HESS) have carved a niche in the industry. HESS improve the system efficiency, reduce the overall cost and increase the lifespan of the system. The proton exchange membrane (PEM) fuel cell is hybridized with Li-ion batteries (LIB) for vehicular applications, robotic applications etc. In applications which have geometrical space constraints, the temperature of the energy storage elements is influenced by convective heat transfer. In this paper the thermal analysis of the geometry of PEM-LIB hybrid system is carried out using COMSOL Multiphysics Software package for different discharge rates (C rates) of the LIB and different voltages of the PEM cell. The additional rise in temperature of the LIB pack when placed in close proximity with PEM cell was in the range of 0.03-0.6$^0$C at 4C. The cell temperature of the LIB pack increased with increase in C rate and decrease in PEM cell voltage.      
### 28.Carrier-Based Modulation Schemes Based on Symmetric Switching Patterns for Three-Phase Three-Switch Rectifier  [ :arrow_down: ](https://arxiv.org/pdf/2011.14648.pdf)
>  Electric Vehicle (EV) chargers are the need of the hour as EVs have invaded the commercial automobile market. The Three-Phase Three-Switch (TPTS) converter has features which make it ideal to act as the charging station because of its high efficiency at peak power rating. Traditionally, control of the TPTS converter has been achieved by the Space Vector (SV) based modulation scheme whose implementation is burdensome due to the trigonometric calculations involved. This paper introduces two simplified carrier based modulation schemes, which greatly reduce the implementation difficulty, as they require minimum mathematical computations. A quantitative comparison is made between SV scheme available in literature with the simplified scheme proposed here. The methodology followed to obtain the carrier waves is also provided. All the modulation schemes are validated with the help of a 1 kW hardware prototype.      
### 29.Identification of Errors-in-Variables ARX Models Using Modified Dynamic Iterative PCA  [ :arrow_down: ](https://arxiv.org/pdf/2011.14645.pdf)
>  Identification of autoregressive models with exogenous input (ARX) is a classical problem in system identification. This article considers the errors-in-variables (EIV) ARX model identification problem, where input measurements are also corrupted with noise. The recently proposed DIPCA technique solves the EIV identification problem but is only applicable to white measurement errors. We propose a novel identification algorithm based on a modified Dynamic Iterative Principal Components Analysis (DIPCA) approach for identifying the EIV-ARX model for single-input, single-output (SISO) systems where the output measurements are corrupted with coloured noise consistent with the ARX model. Most of the existing methods assume important parameters like input-output orders, delay, or noise-variances to be known. This work's novelty lies in the joint estimation of error variances, process order, delay, and model parameters. The central idea used to obtain all these parameters in a theoretically rigorous manner is based on transforming the lagged measurements using the appropriate error covariance matrix, which is obtained using estimated error variances and model parameters. Simulation studies on two systems are presented to demonstrate the efficacy of the proposed algorithm.      
### 30.Quantitative Assessment of Adulteration and Reuse of Coconut Oil Using Transmittance Multispectral Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2011.14644.pdf)
>  Coconut oil known for its wide range of uses is often adulterated with other edible oils. Repeated use of coconut oil in food preparation could lead to many health issues. Existing methods available for evaluating quality of oil are laborious and time consuming. Therefore, we propose an imaging system hardware and image processing-based algorithm to estimate the adulteration of coconut oil with palm oil as the adulterant. A clear functional relationship between adulteration level and Bhattacharyya distance was observed as R2 = 0.9876 on the training samples. Thereafter, another algorithm is proposed to develop a spectral-clustering based classifier to determine the effect of reheat and reuse of coconut oil. Distinct clusters were obtained for different levels of reheated oil classes and the classification was performed with an accuracy of 0.983 on training samples. Further, the input images for the proposed algorithms were generated using an in-house developed transmittance based multispectral imaging system.      
### 31.SAR Image Despeckling Based on Convolutional Denoising Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2011.14627.pdf)
>  In Synthetic Aperture Radar (SAR) imaging, despeckling is very important for image analysis,whereas speckle is known as a kind of multiplicative noise caused by the coherent imaging system. During the past three decades, various algorithms have been proposed to denoise the SAR image. Generally, the BM3D is considered as the state of art technique to despeckle the speckle noise with excellent performance. More recently, deep learning make a success in image denoising and achieved a improvement over conventional method where large train dataset is required. Unlike most of the images SAR image despeckling approach, the proposed approach learns the speckle from corrupted images directly. In this paper, the limited scale of dataset make a efficient exploration by using convolutioal denoising autoencoder (C-DAE) to reconstruct the speckle-free SAR images. Batch normalization strategy is integrated with C- DAE to speed up the train time. Moreover, we compute image quality in standard metrics, PSNR and SSIM. It is revealed that our approach perform well than some others.      
### 32.Robust Output Feedback Consensus for Networked Heterogeneous Nonlinear Negative-Imaginary Systems with Free Body Motion  [ :arrow_down: ](https://arxiv.org/pdf/2011.14610.pdf)
>  This paper presents a framework to address the robust output feedback consensus problem for networked heterogeneous nonlinear Negative-Imaginary (NI) systems with free body dynamics. The aim of this paper is to complete and extend the results in previous papers on robust output feedback consensus for multiple heterogeneous nonlinear NI systems so that the systems in the network are allowed to have free body motion. A subclass of NI systems called Output Strictly Negative-Imaginary (OSNI) systems are applied as controllers to ensure that the outputs of the nonlinear NI plants converge to the same limit trajectory. The definitions of nonlinear NI systems and nonlinear OSNI systems are extended and a new stability result is developed for the interconnection of a single nonlinear NI system and a single nonlinear OSNI system. Robust output feedback consensus is addressed by establishing a similar stability result for the interconnection of networked NI systems and networked OSNI systems.      
### 33.Exponential Stability and Tuning for a Class of Mechanical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.14543.pdf)
>  In this paper, we prove the exponential stability property of a class of mechanical systems represented in the port-Hamiltonian framework. To this end, we propose a Lyapunov candidate function different from the Hamiltonian of the system. Moreover, we study how the proposed analysis can be used to determine the exponential stability and the rate of convergence of some (nonlinear)-mechanical systems stabilized by two passivity-based control techniques, namely, PID passivity-based control and interconnection and damping assignment. We implement the former control approach to stabilize a three degrees-of-freedom robotic arm at the desired equilibrium point to illustrate the mentioned analysis.      
### 34.Deep Reinforcement Learning for Smart Grid Protection Against Coordinated Multistage Transmission Line Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2011.14526.pdf)
>  With the increase of connectivity in power grid, a cascading failure may be triggered by the failure of a transmission line, which can lead to substantial economic losses and serious negative social impacts. Therefore, it is very important to identify the critical lines under various types of attacks that may initiate a cascading failure and deploy defense resources to protect them. Since coordinated multistage line attacks can lead to larger negative impacts compared with a single-stage attack or a multistage attack without coordination, this paper intends to identify the critical lines under coordinated multistage attacks that may initiate a cascading failure and deploy limited defense resources optimally. To this end, we first formulate a total generation loss maximization problem with the consideration of multiple attackers and multiple stages. Due to the large size of solution space, it is very challenging to solve the formulated problem. To overcome the challenge, we reformulate the problem as a Markov game and design its components, e.g., state, action, and reward. Next, we propose a scalable algorithm to solve the Markov game based on multi-agent deep reinforcement learning and prioritized experience replay, which can determine the optimal attacking line sequences. Then, we design a defense strategy to decide the optimal defense line set. Extensive simulation results show the effectiveness of the proposed algorithm and the designed defense strategy.      
### 35.Adaptive noise imitation for image denoising  [ :arrow_down: ](https://arxiv.org/pdf/2011.14512.pdf)
>  The effectiveness of existing denoising algorithms typically relies on accurate pre-defined noise statistics or plenty of paired data, which limits their practicality. In this work, we focus on denoising in the more common case where noise statistics and paired data are unavailable. Considering that denoising CNNs require supervision, we develop a new \textbf{adaptive noise imitation (ADANI)} algorithm that can synthesize noisy data from naturally noisy images. To produce realistic noise, a noise generator takes unpaired noisy/clean images as input, where the noisy image is a guide for noise generation. By imposing explicit constraints on the type, level and gradient of noise, the output noise of ADANI will be similar to the guided noise, while keeping the original clean background of the image. Coupling the noisy data output from ADANI with the corresponding ground-truth, a denoising CNN is then trained in a fully-supervised manner. Experiments show that the noisy data produced by ADANI are visually and statistically similar to real ones so that the denoising CNN in our method is competitive to other networks trained with external paired data.      
### 36.Hybrid Imitation Learning for Real-Time Service Restoration in Resilient Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.14458.pdf)
>  Self-healing capability is one of the most critical factors for a resilient distribution system, which requires intelligent agents to automatically perform restorative actions online, including network reconfiguration and reactive power dispatch. These agents should be equipped with a predesigned decision policy to meet real-time requirements and handle highly complex $N-k$ scenarios. The disturbance randomness hampers the application of exploration-dominant algorithms like traditional reinforcement learning (RL), and the agent training problem under $N-k$ scenarios has not been thoroughly solved. In this paper, we propose the imitation learning (IL) framework to train such policies, where the agent will interact with an expert to learn its optimal policy, and therefore significantly improve the training efficiency compared with the RL methods. To handle tie-line operations and reactive power dispatch simultaneously, we design a hybrid policy network for such a discrete-continuous hybrid action space. We employ the 33-node system under $N-k$ disturbances to verify the proposed framework.      
### 37.A Socially-Efficient Emerging Mobility Market  [ :arrow_down: ](https://arxiv.org/pdf/2011.14399.pdf)
>  Connected and automated vehicles (CAVs) provide the most intriguing opportunity for enabling users to significantly improve safety and transportation efficiency by monitoring transportation network conditions and making better operating decisions. CAVs, however, could alter tendency-to-travel, which would eventually lead to a high traffic demand causing rebound effects (e.g., increasing vehicle miles traveled). In this paper, we focus on the social factors that could drive an emerging mobility system with CAVs to unsustainable congestion levels. We propose a mobility market to model the travelers' decision-making on how to travel in a smart city network with connected roads and public transit infrastructure. Using techniques from mechanism design, we introduce appropriate monetary incentives (e.g., tolls, fees, subsidies), and we show how a mobility system consisting of selfish travelers that seek to travel either with a CAV or use public transit can be socially efficient. We prove that our mobility market is incentive compatible, individually rational, and weakly budget balanced. Thus, our mobility market ensures that travelers always report their personal travel requirements truthfully, always benefit from participating in the market, and the market always generates revenue from each traveler.      
### 38.Overcoming Measurement Inconsistency in Deep Learning for Linear Inverse Problems: Applications in Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2011.14387.pdf)
>  The remarkable performance of deep neural networks (DNNs) currently makes them the method of choice for solving linear inverse problems. They have been applied to super-resolve and restore images, as well as to reconstruct MR and CT images. In these applications, DNNs invert a forward operator by finding, via training data, a map between the measurements and the input images. It is then expected that the map is still valid for the test data. This framework, however, introduces measurement inconsistency during testing. We show that such inconsistency, which can be critical in domains like medical imaging or defense, is intimately related to the generalization error. We then propose a framework that post-processes the output of DNNs with an optimization algorithm that enforces measurement consistency. Experiments on MR images show that enforcing measurement consistency via our method can lead to large gains in reconstruction performance.      
### 39.Single Image Super-resolution with a Switch Guided Hybrid Network for Satellite Images  [ :arrow_down: ](https://arxiv.org/pdf/2011.14380.pdf)
>  The major drawbacks with Satellite Images are low resolution, Low resolution makes it difficult to identify the objects present in Satellite images. We have experimented with several deep models available for Single Image Superresolution on the SpaceNet dataset and have evaluated the performance of each of them on the satellite image data. We will dive into the recent evolution of the deep models in the context of SISR over the past few years and will present a comparative study between these models. The entire Satellite image of an area is divided into equal-sized patches. Each patch will be used independently for training. These patches will differ in nature. Say, for example, the patches over urban areas have non-homogeneous backgrounds because of different types of objects like vehicles, buildings, roads, etc. On the other hand, patches over jungles will be more homogeneous in nature. Hence, different deep models will fit on different kinds of patches. In this study, we will try to explore this further with the help of a Switching Convolution Network. The idea is to train a switch classifier that will automatically classify a patch into one category of models best suited for it.      
### 40.Semi-Supervised Learning of Mutually Accelerated Multi-Contrast MRI Synthesis without Fully-Sampled Ground-Truths  [ :arrow_down: ](https://arxiv.org/pdf/2011.14347.pdf)
>  This study proposes a novel semi-supervised learning framework for mutually accelerated multi-contrast MRI synthesis that recovers high-quality images without demanding large training sets of costly fully-sampled source or ground-truth target images. The proposed method presents a selective loss function expressed only on a subset of the acquired k-space coefficients and further leverages randomized sampling patterns across training subjects to effectively learn relationships among acquired and nonacquired k-space coefficients at all locations. Comprehensive experiments performed on multi-contrast brain images clearly demonstrate that the proposed method maintains equivalent performance to the gold-standard method based on fully-supervised training while alleviating undesirable reliance of the current synthesis methods on large-scale fully-sampled MRI acquisitions.      
### 41.Malaria Detection and Classificaiton  [ :arrow_down: ](https://arxiv.org/pdf/2011.14329.pdf)
>  Malaria is a disease of global concern according to the World Health Organization. Billions of people in the world are at risk of Malaria today. Microscopy is considered the gold standard for Malaria diagnosis. Microscopic assessment of blood samples requires the need of trained professionals who at times are not available in rural areas where Malaria is a problem. Full automation of Malaria diagnosis is a challenging task. In this work, we put forward a framework for diagnosis of malaria. We adopt a two layer approach, where we detect infected cells using a Faster-RCNN in the first layer, crop them out, and feed the cropped cells to a seperate neural network for classification. The proposed methodology was tested on an openly available dataset, this will serve as a baseline for the future methods as currently there is no common dataset on which results are reported for Malaria Diagnosis.      
### 42.A Framework for Health-informed RUL-constrained Optimal Power Flow with Li-ion Batteries  [ :arrow_down: ](https://arxiv.org/pdf/2011.14318.pdf)
>  Battery energy storage systems are widely adopted in grid-connected applications to mitigate the impact of intermittent renewable generations and enhance power system resiliency. Degradation of the battery during its service time is one of the major concerns in the deployment that strongly affects the long-term lifetime. Apart from environmental factors, this intrinsic property of a battery depends on the daily operating conditions. Thus, optimally engaging the daily operation of the battery based on its current status in order to meet the required remaining useful life becomes a practical and demanding need. To address this issue, this paper proposes a health-informed RUL-constrained optimal power flow framework to characterize the corresponding optimal feasible operation space. The targeted service lifespan is achieved if the battery's working condition is confined within this feasible domain. Equivalent box constraints are then constructed for better computational efficiency in solving the optimization problem. In this framework, a Monte Carlo-based data-driven approach and a health indicator (HI) representing the battery's current states are introduced. The performance of the proposed method is illustrated with the IEEE 39-bus system.      
### 43.Automated Prostate Cancer Diagnosis Based on Gleason Grading Using Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2011.14301.pdf)
>  The Gleason grading system using histological images is the most powerful diagnostic and prognostic predictor of prostate cancer. The current standard inspection is evaluating Gleason H&amp;E-stained histopathology images by pathologists. However, it is complicated, time-consuming, and subject to observers. Deep learning (DL) based-methods that automatically learn image features and achieve higher generalization ability have attracted significant attention. However, challenges remain especially using DL to train the whole slide image (WSI), a predominant clinical source in the current diagnostic setting, containing billions of pixels, morphological heterogeneity, and artifacts. Hence, we proposed a convolutional neural network (CNN)-based automatic classification method for accurate grading of PCa using whole slide histopathology images. In this paper, a data augmentation method named Patch-Based Image Reconstruction (PBIR) was proposed to reduce the high resolution and increase the diversity of WSIs. In addition, a distribution correction (DC) module was developed to enhance the adaption of pretrained model to the target dataset by adjusting the data distribution. Besides, a Quadratic Weighted Mean Square Error (QWMSE) function was presented to reduce the misdiagnosis caused by equal Euclidean distances. Our experiments indicated the combination of PBIR, DC, and QWMSE function was necessary for achieving superior expert-level performance, leading to the best results (0.8885 quadratic-weighted kappa coefficient).      
### 44.Enriching Load Data Using Micro-PMUs and Smart Meters  [ :arrow_down: ](https://arxiv.org/pdf/2011.14271.pdf)
>  In modern distribution systems, load uncertainty can be fully captured by micro-PMUs, which can record high-resolution data; however, in practice, micro-PMUs are installed at limited locations in distribution networks due to budgetary constraints. In contrast, smart meters are widely deployed but can only measure relatively low-resolution energy consumption, which cannot sufficiently reflect the actual instantaneous load volatility within each sampling interval. In this paper, we have proposed a novel approach for enriching load data for service transformers that only have low-resolution smart meters. The key to our approach is to statistically recover the high-resolution load data, which is masked by the low-resolution data, using trained probabilistic models of service transformers that have both high and low-resolution data sources, i.e, micro-PMUs and smart meters. The overall framework consists of two steps: first, for the transformers with micro-PMUs, a Gaussian Process is leveraged to capture the relationship between the maximum/minimum load and average load within each low-resolution sampling interval of smart meters; a Markov chain model is employed to characterize the transition probability of known high-resolution load. Next, the trained models are used as teachers for the transformers with only smart meters to decompose known low-resolution load data into targeted high-resolution load data. The enriched data can recover instantaneous load uncertainty and significantly enhance distribution system observability and situational awareness. We have verified the proposed approach using real high- and low-resolution load data.      
### 45.Fully Quantized Image Super-Resolution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.14265.pdf)
>  With the rising popularity of intelligent mobile devices, it is of great practical significance to develop accurate, realtime and energy-efficient image Super-Resolution (SR) inference methods. A prevailing method for improving the inference efficiency is model quantization, which allows for replacing the expensive floating-point operations with efficient fixed-point or bitwise arithmetic. To date, it is still challenging for quantized SR frameworks to deliver feasible accuracy-efficiency trade-off. Here, we propose a Fully Quantized image Super-Resolution framework (FQSR) to jointly optimize efficiency and accuracy. In particular, we target on obtaining end-to-end quantized models for all layers, especially including skip connections, which was rarely addressed in the literature. We further identify training obstacles faced by low-bit SR networks and propose two novel methods accordingly. The two difficulites are caused by 1) activation and weight distributions being vastly distinctive in different layers; 2) the inaccurate approximation of the quantization. We apply our quantization scheme on multiple mainstream super-resolution architectures, including SRResNet, SRGAN and EDSR. Experimental results show that our FQSR using low bits quantization can achieve on par performance compared with the full-precision counterparts on five benchmark datasets and surpass state-of-the-art quantized SR methods with significantly reduced computational cost and memory consumption.      
### 46.Reinforcement Learning based Distributed Control of Dissipative Networked Systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.14263.pdf)
>  We consider the problem of designing distributed controllers to stabilize a class of networked systems, where each subsystem is dissipative and designs a reinforcement learning based local controller to maximize an individual cumulative reward function. We develop an approach that enforces dissipativity conditions on these local controllers at each subsystem to guarantee stability of the entire networked system. The proposed approach is illustrated on a DC microgrid example, where the objective is maintain voltage stability of the network using local distributed controllers at each generation unit.      
### 47.Optimal Quarantining Strategy for Interdependent Epidemics Spreading over Complex Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.14262.pdf)
>  Optimal quarantining strategy of suppressing interdependent epidemics spreading over complex networks is a critical issue. In this paper, we first establish a framework to capture the coupling between two epidemics, and then analyze the system's equilibrium states by categorizing them into three classes, and deriving their stability conditions. The designed quarantining strategy globally optimizes the trade-off between the quarantining cost and the severity of epidemics in the network. In addition, we provide structural results on the predictability of epidemic spreading by showing the existence and uniqueness of the solution. We also demonstrate the robustness of quarantining strategy by showing the continuity of epidemic severity with respect to the applied quarantining effort. A gradient descent algorithm based on a fixed-point iterative scheme is proposed to find the optimal quarantining strategy. Depending on the system parameters, the quarantining strategy can lead to switching between equilibria of the interdependent epidemic network as the control cost varies. Finally, we use case studies to corroborate and illustrate the obtained theoretical results.      
### 48.Artificial Intelligence applied to chest X-Ray images for the automatic detection of COVID-19. A thoughtful evaluation approach  [ :arrow_down: ](https://arxiv.org/pdf/2011.14259.pdf)
>  Current standard protocols used in the clinic for diagnosing COVID-19 include molecular or antigen tests, generally complemented by a plain chest X-Ray. The combined analysis aims to reduce the significant number of false negatives of these tests, but also to provide complementary evidence about the presence and severity of the disease. However, the procedure is not free of errors, and the interpretation of the chest X-Ray is only restricted to radiologists due to its complexity. With the long term goal to provide new evidence for the diagnosis, this paper presents an evaluation of different methods based on a deep neural network. These are the first steps to develop an automatic COVID-19 diagnosis tool using chest X-Ray images, that would additionally differentiate between controls, pneumonia or COVID-19 groups. The paper describes the process followed to train a Convolutional Neural Network with a dataset of more than 79,500 X-Ray images compiled from different sources, including more than 8,500 COVID-19 examples. For the sake of evaluation and comparison of the models developed, three different experiments were carried out following three preprocessing schemes. The aim is to evaluate how preprocessing the data affects the results and improves its explainability. Likewise, a critical analysis is carried out about different variability issues that might compromise the system and the effects on the performance. With the employed methodology, a 91.5% classification accuracy is obtained, with a 87.4% average recall for the worst but most explainable experiment, which requires a previous automatic segmentation of the lungs region.      
### 49.Reconstruction Condition of Quantized Signals in Unlimited Sampling Framework  [ :arrow_down: ](https://arxiv.org/pdf/2011.14249.pdf)
>  The latest theoretical advances in the field of unlimited sampling framework (USF) show the potential to avoid clipping problems of analog-to-digital converters (ADC). To date, most of the related works have focused on real-valued modulo samples, but little has been reported about the impact of quantization. In this paper, we study more practical USF system where modulo samples are quantized to a finite number of bits. In particular, we present a new requirement about the lower bound of sampling rate to ensure exact recovery of original signals from quantized modulo samples. The minimum sampling rate is jointly determined by signal bandwidth and quantization bits. Numerical results show that in the presence of quantization noise, signals with different waveforms and bandwidths are recovered perfectly at the new minimum sampling rate while the recovery fails at minimum sampling rate before modification, which also verifies the correctness of the theory. The trade-offs of sampling rates, quantization bits and computational complexity of recovery algorithm are also given for practitioners to weigh.      
### 50.DROPS: Deep Retrieval of Physiological Signals via Attribute-specific Clinical Prototypes  [ :arrow_down: ](https://arxiv.org/pdf/2011.14230.pdf)
>  The ongoing digitization of health records within the healthcare industry results in large-scale datasets. Manually extracting clinically-useful insight from such datasets is non-trivial. However, doing so at scale while simultaneously leveraging patient-specific attributes such as sex and age can assist with clinical-trial enrollment, medical school educational endeavours, and the evaluation of the fairness of neural networks. To facilitate the reliable extraction of clinical information, we propose to learn embeddings, known as clinical prototypes (CPs), via supervised contrastive learning. We show that CPs can be efficiently used for large-scale retrieval and clustering of physiological signals based on multiple patient attributes. We also show that CPs capture attribute-specific semantic relationships.      
### 51.Deep Learning for Regularization Prediction in Diffeomorphic Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2011.14229.pdf)
>  This paper presents a predictive model for estimating regularization parameters of diffeomorphic image registration. We introduce a novel framework that automatically determines the parameters controlling the smoothness of diffeomorphic transformations. Our method significantly reduces the effort of parameter tuning, which is time and labor-consuming. To achieve the goal, we develop a predictive model based on deep convolutional neural networks (CNN) that learns the mapping between pairwise images and the regularization parameter of image registration. In contrast to previous methods that estimate such parameters in a high-dimensional image space, our model is built in an efficient bandlimited space with much lower dimensions. We demonstrate the effectiveness of our model on both 2D synthetic data and 3D real brain images. Experimental results show that our model not only predicts appropriate regularization parameters for image registration, but also improving the network training in terms of time and memory efficiency.      
### 52.PCPs: Patient Cardiac Prototypes  [ :arrow_down: ](https://arxiv.org/pdf/2011.14227.pdf)
>  Many clinical deep learning algorithms are population-based and difficult to interpret. Such properties limit their clinical utility as population-based findings may not generalize to individual patients and physicians are reluctant to incorporate opaque models into their clinical workflow. To overcome these obstacles, we propose to learn patient-specific embeddings, entitled patient cardiac prototypes (PCPs), that efficiently summarize the cardiac state of the patient. To do so, we attract representations of multiple cardiac signals from the same patient to the corresponding PCP via supervised contrastive learning. We show that the utility of PCPs is multifold. First, they allow for the discovery of similar patients both within and across datasets. Second, such similarity can be leveraged in conjunction with a hypernetwork to generate patient-specific parameters, and in turn, patient-specific diagnoses. Third, we find that PCPs act as a compact substitute for the original dataset, allowing for dataset distillation.      
### 53.Machine Intelligent Techniques for Ramp Event Prediction in Offshore and Onshore Wind Farms  [ :arrow_down: ](https://arxiv.org/pdf/2011.14220.pdf)
>  Globally, wind energy has lessened the burden on conventional fossil fuel based power generation. Wind resource assessment for onshore and offshore wind farms aids in accurate forecasting and analyzing nature of ramp events. From an industrial point of view, a large ramp event in a short time duration is likely to cause damage to the wind farm connected to the utility grid. In this manuscript, ramp events are predicted using hybrid machine intelligent techniques such as Support vector regression (SVR) and its variants, random forest regression and gradient boosted machines for onshore and offshore wind farm sites. Wavelet transform based signal processing technique is used to extract features from wind speed. Results reveal that SVR based prediction models gives the best forecasting performance out of all models. In addition, gradient boosted machines (GBM) predicts ramp events closer to Twin support vector regression (TSVR) model. Furthermore, the randomness in ramp power is evaluated for onshore and offshore wind farms by calculating log energy entropy of features obtained from wavelet decomposition and empirical model decomposition.      
### 54.Privacy-Preserving Federated Learning for UAV-Enabled Networks: Learning-Based Joint Scheduling and Resource Management  [ :arrow_down: ](https://arxiv.org/pdf/2011.14197.pdf)
>  Unmanned aerial vehicles (UAVs) are capable of serving as flying base stations (BSs) for supporting data collection, artificial intelligence (AI) model training, and wireless communications. However, due to the privacy concerns of devices and limited computation or communication resource of UAVs, it is impractical to send raw data of devices to UAV servers for model training. Moreover, due to the dynamic channel condition and heterogeneous computing capacity of devices in UAV-enabled networks, the reliability and efficiency of data sharing require to be further improved. In this paper, we develop an asynchronous federated learning (AFL) framework for multi-UAV-enabled networks, which can provide asynchronous distributed computing by enabling model training locally without transmitting raw sensitive data to UAV servers. The device selection strategy is also introduced into the AFL framework to keep the low-quality devices from affecting the learning efficiency and accuracy. Moreover, we propose an asynchronous advantage actor-critic (A3C) based joint device selection, UAVs placement, and resource management algorithm to enhance the federated convergence speed and accuracy. Simulation results demonstrate that our proposed framework and algorithm achieve higher learning accuracy and faster federated execution time compared to other existing solutions.      
### 55.Lattice Fusion Networks for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2011.14196.pdf)
>  A novel method for feature fusion in convolutional neural networks is proposed in this work. Different feature fusion techniques are suggested to facilitate the flow of information and improve the training of deep neural networks. Some of these techniques as well as the proposed model can be considered a type of Directed Acyclic Graph (DAG) Network, where a layer can receive inputs from other layers and have outputs to other layers. In the proposed general framework of Lattice Fusion Network (LFN), feature maps of each convolutional layer are passed to other layers based on a lattice graph structure, where nodes are convolutional layers. To investigate the performance of the model, a specific design based on the general framework of LFN is implemented for image denoising. Results are compared with state of the art methods. The proposed model produced competitive results with far fewer learnable parameters, which shows the effectiveness of LFNs for training of deep neural networks      
### 56.Model Predictive Control with and without Terminal Weight: Stability and Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2011.14193.pdf)
>  This paper presents stability analysis tools for model predictive control (MPC) with and without terminal weight. Stability analysis of MPC with a limited horizon but without terminal weight is a long-standing open problem. By using a modified value function as an Lyapunov function candidate and the principle of optimality, this paper establishes stability conditions for this type of widely spread MPC algorithms. A new stability guaranteed MPC algorithm without terminal weight (MPCS) is presented, which has a promising property that its feasibility implies stability. With the help of designing a new level set defined by the value function of one step ahead stage cost, conditions for checking its recursive feasibility and stability of the proposed MPC algorithm are presented. The new stability condition and the derived MPCS overcome the difficulties arising in the existing terminal weight based MPC framework, including the need of searching a suitable terminal weight, possible poor performance caused by an inappropriate terminal weight and not being horizon dependent. This work is further extended to MPC with a terminal weight for the completeness. It is shown that the proposed stability condition is much less conservative than the existing ones that form the foundation of the current `standard' MPC framework in the sense that this condition is met as long as the current stability conditions are met. Numerical examples are presented to demonstrate the effectiveness of the proposed tool, whereas the existing stability analysis tools are either not applicable or lead to quite conservative results. It shows that the proposed tools offer a number of mechanisms to achieve stability: adjusting state and/or control weights, extending the length of horizon, and adding a simple extra constraint on the first or second state in the optimisation.      
### 57.Inter-slice Context Residual Learning for 3D Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2011.14155.pdf)
>  Automated and accurate 3D medical image segmentation plays an essential role in assisting medical professionals to evaluate disease progresses and make fast therapeutic schedules. Although deep convolutional neural networks (DCNNs) have widely applied to this task, the accuracy of these models still need to be further improved mainly due to their limited ability to 3D context perception. In this paper, we propose the 3D context residual network (ConResNet) for the accurate segmentation of 3D medical images. This model consists of an encoder, a segmentation decoder, and a context residual decoder. We design the context residual module and use it to bridge both decoders at each scale. Each context residual module contains both context residual mapping and context attention mapping, the formal aims to explicitly learn the inter-slice context information and the latter uses such context as a kind of attention to boost the segmentation accuracy. We evaluated this model on the MICCAI 2018 Brain Tumor Segmentation (BraTS) dataset and NIH Pancreas Segmentation (Pancreas-CT) dataset. Our results not only demonstrate the effectiveness of the proposed 3D context residual learning scheme but also indicate that the proposed ConResNet is more accurate than six top-ranking methods in brain tumor segmentation and seven top-ranking methods in pancreas segmentation. Code is available at <a class="link-external link-https" href="https://git.io/ConResNet" rel="external noopener nofollow">this https URL</a>      
### 58.Preclinical Stage Alzheimer's Disease Detection Using Magnetic Resonance Image Scans  [ :arrow_down: ](https://arxiv.org/pdf/2011.14139.pdf)
>  Alzheimer's disease is one of the diseases that mostly affects older people without being a part of aging. The most common symptoms include problems with communicating and abstract thinking, as well as disorientation. It is important to detect Alzheimer's disease in early stages so that cognitive functioning would be improved by medication and training. In this paper, we propose two attention model networks for detecting Alzheimer's disease from MRI images to help early detection efforts at the preclinical stage. We also compare the performance of these two attention network models with a baseline model. Recently available OASIS-3 Longitudinal Neuroimaging, Clinical, and Cognitive Dataset is used to train, evaluate and compare our models. The novelty of this research resides in the fact that we aim to detect Alzheimer's disease when all the parameters, physical assessments, and clinical data state that the patient is healthy and showing no symptoms      
### 59.Retrospective Motion Correction of MR Images using Prior-Assisted Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.14134.pdf)
>  In MRI, motion artefacts are among the most common types of artefacts. They can degrade images and render them unusable for accurate diagnosis. Traditional methods, such as prospective or retrospective motion correction, have been proposed to avoid or alleviate motion artefacts. Recently, several other methods based on deep learning approaches have been proposed to solve this problem. This work proposes to enhance the performance of existing deep learning models by the inclusion of additional information present as image priors. The proposed approach has shown promising results and will be further investigated for clinical validity.      
### 60.MIINet: An Image Quality Improvement Framework for Supporting Medical Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2011.14132.pdf)
>  Medical images have been indispensable and useful tools for supporting medical experts in making diagnostic decisions. However, taken medical images especially throat and endoscopy images are normally hazy, lack of focus, or uneven illumination. Thus, these could difficult the diagnosis process for doctors. In this paper, we propose MIINet, a novel image-to-image translation network for improving quality of medical images by unsupervised translating low-quality images to the high-quality clean version. Our MIINet is not only capable of generating high-resolution clean images, but also preserving the attributes of original images, making the diagnostic more favorable for doctors. Experiments on dehazing 100 practical throat images show that our MIINet largely improves the mean doctor opinion score (MDOS), which assesses the quality and the reproducibility of the images from the baseline of 2.36 to 4.11, while dehazed images by CycleGAN got lower score of 3.83. The MIINet is confirmed by three physicians to be satisfying in supporting throat disease diagnostic from original low-quality images.      
### 61.Characterizing Bipartite Consensus with Balancing Set on Signed Matrix-weighted Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.14105.pdf)
>  In contrast with scalar-weighted networks, where bipartite consensus can be achieved if and only if the underlying signed network is structurally balanced, structural balance is no longer a graph-theoretic equivalence to the bipartite consensus in the case of signed matrix-weighted networks. To re-establish the relationship between the network structure and the bipartite consensus, the non-trivial balancing set is introduced which is a set of edges whose sign negation can transform a structurally imbalanced network into a structurally balanced one and the weight matrices associated with edges in this set have a non-trivial intersection of null spaces. We show that necessary and/or sufficient conditions for bipartite consensus on matrix-weighted networks can be characterized by the uniqueness of the associated non-trivial balancing set, in the meanwhile, the contribution of the associated non-trivial intersection of null spaces to the steady-state of the matrix-weighted network is examined. Moreover, for matrix-weighted networks with a positive-negative spanning tree, necessary and sufficient condition for bipartite consensus using non-trivial balancing set is obtained. Simulation examples are provided to demonstrate the theoretical results.      
### 62.Theoretical Accuracy Analysis of RSS-Based Range Estimation for Visible Light Communication  [ :arrow_down: ](https://arxiv.org/pdf/2011.14080.pdf)
>  In this paper, an improved channel model of visible light communication (VLC) for ranging in presented. For indoor channel model of VLC, distance is estimated based on received signal strength. In this model, received shot noise as a distance-dependent parameter is considered in range estimation accuracy. Moreover, based on this model, the Cramer-Rao lower bound is computed as the theoretical limits on the performance and accuracy of any unbiased estimator. In this way, the effects of horizontal and vertical distances are investigated. In addition, the transmitted power effect on RSN and accordingly on CRLB is demonstrated.      
### 63.Unsupervised Spoken Term Discovery Based on Re-clustering of Hypothesized Speech Segments with Siamese and Triplet Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.14062.pdf)
>  Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery. Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.      
### 64.Unsupervised Spoken Term Discovery on Untranscribed Speech  [ :arrow_down: ](https://arxiv.org/pdf/2011.14060.pdf)
>  (Part of the abstract) In this thesis, we investigate the use of unsupervised spoken term discovery in tackling this problem. Unsupervised spoken term discovery aims to discover topic-related terminologies in a speech without knowing the phonetic properties of the language and content. It can be further divided into two parts: Acoustic segment modelling (ASM) and unsupervised pattern discovery. ASM learns the phonetic structures of zero-resource language audio with no phonetic knowledge available, generating self-derived "phonemes". The audio are labelled with these "phonemes" to obtain "phoneme" sequences. Unsupervised pattern discovery searches for repetitive patterns in the "phoneme" sequences. The discovered patterns can be grouped to determine the keywords of the audio. Multilingual neural network with bottleneck layer is used for feature extraction. Experiments show that bottleneck features facilitate the training of ASM compared to conventional features such as MFCC. The unsupervised spoken term discovery system is experimented with online lectures covering different topics by different speakers. It is shown that the system learns the phonetic information of the language and can discover frequent spoken terms that align with text transcription. By using information retrieval technology such as word embedding and TFIDF, it is shown that the discovered keywords can be further used for topic comparison.      
### 65.Differences between human and machine perception in medical diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2011.14036.pdf)
>  Deep neural networks (DNNs) show promise in image-based medical diagnosis, but cannot be fully trusted since their performance can be severely degraded by dataset shifts to which human perception remains invariant. If we can better understand the differences between human and machine perception, we can potentially characterize and mitigate this effect. We therefore propose a framework for comparing human and machine perception in medical diagnosis. The two are compared with respect to their sensitivity to the removal of clinically meaningful information, and to the regions of an image deemed most suspicious. Drawing inspiration from the natural image domain, we frame both comparisons in terms of perturbation robustness. The novelty of our framework is that separate analyses are performed for subgroups with clinically meaningful differences. We argue that this is necessary in order to avert Simpson's paradox and draw correct conclusions. We demonstrate our framework with a case study in breast cancer screening, and reveal significant differences between radiologists and DNNs. We compare the two with respect to their robustness to Gaussian low-pass filtering, performing a subgroup analysis on microcalcifications and soft tissue lesions. For microcalcifications, DNNs use a separate set of high frequency components than radiologists, some of which lie outside the image regions considered most suspicious by radiologists. These features run the risk of being spurious, but if not, could represent potential new biomarkers. For soft tissue lesions, the divergence between radiologists and DNNs is even starker, with DNNs relying heavily on spurious high frequency components ignored by radiologists. Importantly, this deviation in soft tissue lesions was only observable through subgroup analysis, which highlights the importance of incorporating medical domain knowledge into our comparison framework.      
### 66.Towards Optimal Coordination between Regional Groups: HVDC Supplementary Power Control  [ :arrow_down: ](https://arxiv.org/pdf/2011.14007.pdf)
>  With Europe dedicated to limiting climate change and greenhouse gas emissions, large shares of Renewable Energy Sources (RES) are being integrated in the national grids, phasing out conventional generation. The new challenges arising from the energy transition will require a better coordination between neighboring system operators to maintain system security. To this end, this paper studies the benefit of exchanging primary frequency reserves between asynchronous areas using the Supplementary Power Control (SPC) functionality of High-Voltage Direct-Current (HVDC) lines. First, we focus on the derivation of frequency metrics for asynchronous AC systems coupled by HVDC interconnectors. We compare two different control schemes for HVDC converters, which allow for unilateral or bilateral exchanges of reserves between neighboring systems. Second, we formulate frequency constraints and include them in a unit commitment problem to ensure the N-1 security criterion. A data-driven approach is proposed to better represent the frequency nadir constraint by means of cutting hyperplanes. Our results suggest that the exchange of primary reserves through HVDC can reduce up to 10% the cost of reserve procurement while maintaining the system N-1 secure.      
### 67.Offset-free setpoint tracking using neural network controllers  [ :arrow_down: ](https://arxiv.org/pdf/2011.14006.pdf)
>  In this paper, we present a method to analyze local and global stability in offset-free setpoint tracking using neural network controllers and we provide ellipsoidal inner approximations of the corresponding region of attraction. We consider a feedback interconnection using a neural network controller in connection with an integrator, which allows for offset-free tracking of a desired piecewise constant reference that enters the controller as an external input. The feedback interconnection considered in this paper allows for general configurations of the neural network controller that include the special cases of output error and state feedback. Exploiting the fact that activation functions used in neural networks are slope-restricted, we derive linear matrix inequalities to verify stability using Lyapunov theory. After stating a global stability result, we present less conservative local stability conditions (i) for a given reference and (ii) for any reference from a certain set. The latter result even enables guaranteed tracking under setpoint changes using a reference governor which can lead to a significant increase of the region of attraction. Finally, we demonstrate the applicability of our analysis by verifying stability and offset-free tracking of a neural network controller that was trained to stabilize an inverted pendulum.      
### 68.Three-dimensional Segmentation of the Scoliotic Spine from MRI using Unsupervised Volume-based MR-CT Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2011.14005.pdf)
>  Vertebral bone segmentation from magnetic resonance (MR) images is a challenging task. Due to the inherent nature of the modality to emphasize soft tissues of the body, common thresholding algorithms are ineffective in detecting bones in MR images. On the other hand, it is relatively easier to segment bones from CT images because of the high contrast between bones and the surrounding regions. For this reason, we perform a cross-modality synthesis between MR and CT domains for simple thresholding-based segmentation of the vertebral bones. However, this implicitly assumes the availability of paired MR-CT data, which is rare, especially in the case of scoliotic patients. In this paper, we present a completely unsupervised, fully three-dimensional (3D) cross-modality synthesis method for segmenting scoliotic spines. A 3D CycleGAN model is trained for an unpaired volume-to-volume translation across MR and CT domains. Then, the Otsu thresholding algorithm is applied to the synthesized CT volumes for easy segmentation of the vertebral bones. The resulting segmentation is used to reconstruct a 3D model of the spine. We validate our method on 28 scoliotic vertebrae in 3 patients by computing the point-to-surface mean distance between the landmark points for each vertebra obtained from pre-operative X-rays and the surface of the segmented vertebra. Our study results in a mean error of 3.41 $\pm$ 1.06 mm. Based on qualitative and quantitative results, we conclude that our method is able to obtain a good segmentation and 3D reconstruction of scoliotic spines, all after training from unpaired data in an unsupervised manner.      
### 69.Non-Contact Vital Signs Detection with UAV-Borne Radars  [ :arrow_down: ](https://arxiv.org/pdf/2011.13982.pdf)
>  Airborne radar carried on-board unmanned aerial vehicles (UAV) is serving as the harbinger of new remote sensing applications for security and rescue in inclement environments. The mobility and agility of UAVs along with intelligent on-board sensors (cameras, acoustics, and radar) are more effective during the early stages of disaster response. The ability of radars to penetrate through objects and operate during low visibility conditions enables detection of occluded human subjects on and under debris when other sensing modalities fail. Recently, radars have been deployed on UAVs to measure minute human physiological parameters such as respiratory and heart rates while sensing through clothing and building materials. Aggregating radar measurements with the information from other sensors is broadening the applications of drones in life-critical situations. Signal processing techniques are critical in enabling UAV-borne radars for human vital sign detection (VSD) in multiple operation modes. Novel radar configurations such as in a UAV swarm and tethered UAVs are required to facilitate multi-tasking and high endurance, respectively. This paper provides an overview of recent advances in UAV-borne VSD with a focus on deployment modes and processing methods.      
### 70.Co-design of Optimal Transmission Power and Controller for Networked Control Systems Under State-dependent Markovian Channels  [ :arrow_down: ](https://arxiv.org/pdf/2011.13980.pdf)
>  This paper considers a co-design problem for industrial networked control systems to ensure both the stability and efficiency properties of such systems. The assurance of such properties is particularly challenging due to the fact that wireless communications in industrial environments are not only subject to shadow fading but also stochastically correlated with their surrounding environments. To address such challenges, this paper first introduces a novel state-dependent Markov channel (SD-MC) model that explicitly captures the state-dependent features of industrial wireless communication systems by defining the proposed model's transition probabilities as a function of both its environment's states and transmission power. Under the proposed channel model, sufficient conditions on Maximum Allowable Transmission Interval (MATI) are presented to ensure both asymptotic stability in expectation and almost sure asymptotic stability properties of a continuous nonlinear control system with state-dependent fading channels. Based on such conditions, the co-design problem is then formulated as a constrained polynomial optimization problem (CPOP), which can be efficiently solved using semidefinite programming methods for the case of a two-state state dependent Markovian channel. The solutions to such a CPOP represent optimal control and power strategies that optimize the average expected joint costs in an infinite time horizon while still respect the stability constraints. For a general SD-MC model, this paper further shows that sub-optimal solutions can be obtained from linear programming formulations of the considered CPOP. Simulation results are given to illustrate the efficacy of the proposed co-design scheme.      
### 71.Trends in deep learning for medical hyperspectral image analysis  [ :arrow_down: ](https://arxiv.org/pdf/2011.13974.pdf)
>  Deep learning algorithms have seen acute growth of interest in their applications throughout several fields of interest in the last decade, with medical hyperspectral imaging being a particularly promising domain. So far, to the best of our knowledge, there is no review paper that discusses the implementation of deep learning for medical hyperspectral imaging, which is what this review paper aims to accomplish by examining publications that currently utilize deep learning to perform effective analysis of medical hyperspectral imagery. This paper discusses deep learning concepts that are relevant and applicable to medical hyperspectral imaging analysis, several of which have been implemented since the boom in deep learning. This will comprise of reviewing the use of deep learning for classification, segmentation, and detection in order to investigate the analysis of medical hyperspectral imaging. Lastly, we discuss the current and future challenges pertaining to this discipline and the possible efforts to overcome such trials.      
### 72.Modelling brain lesion volume in patches with CNN-based Poisson Regression  [ :arrow_down: ](https://arxiv.org/pdf/2011.13927.pdf)
>  Monitoring the progression of lesions is important for clinical response. Summary statistics such as lesion volume are objective and easy to interpret, which can help clinicians assess lesion growth or decay. CNNs are commonly used in medical image segmentation for their ability to produce useful features within large contexts and their associated efficient iterative patch-based training. Many CNN architectures require hundreds of thousands parameters to yield a good segmentation. In this work, an efficient, computationally inexpensive CNN is implemented to estimate the number of lesion voxels in a predefined patch size from magnetic resonance (MR) images. The output of the CNN is interpreted as the conditional Poisson parameter over the patch, allowing standard mini-batch gradient descent to be employed. The ISLES2015 (SISS) data is used to train and evaluate the model, which by estimating lesion volume from raw features, accurately identified the lesion image with the larger lesion volume for 86% of paired sample patches. An argument for the development and use of estimating lesion volumes to also aid in model selection for segmentation is made.      
### 73.Timely Group Updating  [ :arrow_down: ](https://arxiv.org/pdf/2011.15114.pdf)
>  We consider two closely related problems: anomaly detection in sensor networks and testing for infections in human populations. In both problems, we have $n$ nodes (sensors, humans), and each node exhibits an event of interest (anomaly, infection) with probability $p$. We want to keep track of the anomaly/infection status of all nodes at a central location. We develop a $group$ $updating$ scheme, akin to group testing, which updates a central location about the status of each member of the population by appropriately grouping their individual status. Unlike group testing, which uses the expected number of tests as a metric, in group updating, we use the expected age of information at the central location as a metric. We determine the optimal group size to minimize the age of information. We show that, when $p$ is small, the proposed group updating policy yields smaller age compared to a sequential updating policy.      
### 74.A proposal and evaluation of new timbre visualisation methods for audio sample browsers  [ :arrow_down: ](https://arxiv.org/pdf/2011.15096.pdf)
>  Searching through vast libraries of sound samples can be a daunting and time-consuming task. Modern audio sample browsers use mappings between acoustic properties and visual attributes to visually differentiate displayed items. There are few studies focused on how well these mappings help users search for a specific sample. We propose new methods for generating textural labels and positioning samples based on perceptual representations of timbre. We perform a series of studies to evaluate the benefits of using shape, color or texture as labels in a known-item search task. We describe the motivation and implementation of the study, and present an in-depth analysis of results. We find that shape significantly improves task performance, while color and texture have little effect. We also compare results between in-person and online participants and propose research directions for further studies.      
### 75.Transformer-Transducers for Code-Switched Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2011.15023.pdf)
>  We live in a world where 60% of the population can speak two or more languages fluently. Members of these communities constantly switch between languages when having a conversation. As automatic speech recognition (ASR) systems are being deployed to the real-world, there is a need for practical systems that can handle multiple languages both within an utterance or across utterances. In this paper, we present an end-to-end ASR system using a transformer-transducer model architecture for code-switched speech recognition. We propose three modifications over the vanilla model in order to handle various aspects of code-switching. First, we introduce two auxiliary loss functions to handle the low-resource scenario of code-switching. Second, we propose a novel mask-based training strategy with language ID information to improve the label encoder training towards intra-sentential code-switching. Finally, we propose a multi-label/multi-audio encoder structure to leverage the vast monolingual speech corpora towards code-switching. We demonstrate the efficacy of our proposed approaches on the SEAME dataset, a public Mandarin-English code-switching corpus, achieving a mixed error rate of 18.5% and 26.3% on test_man and test_sge sets respectively.      
### 76.Learning from Incremental Directional Corrections  [ :arrow_down: ](https://arxiv.org/pdf/2011.15014.pdf)
>  This paper proposes a technique which enables a robot to learn a control objective function incrementally from human user's corrections. The human's corrections can be as simple as directional corrections -- corrections that indicate the direction of a control change without indicating its magnitude -- applied at some time instances during the robot's motion. We only assume that each of the human's corrections, regardless of its magnitude, points in a direction that improves the robot's current motion relative to an implicit objective function. The proposed method uses the direction of a correction to update the estimate of the objective function based on a cutting plane technique. We establish the theoretical results to show that this process of incremental correction and update guarantees convergence of the learned objective function to the implicit one. The method is validated by both simulations and two human-robot games, where human players teach a 2-link robot arm and a 6-DoF quadrotor system for motion planning in environments with obstacles.      
### 77.Convolutive Transfer Function Invariant SDR training criteria for Multi-Channel Reverberant Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2011.15003.pdf)
>  Time-domain training criteria have proven to be very effective for the separation of single-channel non-reverberant speech mixtures. Likewise, mask-based beamforming has shown impressive performance in multi-channel reverberant speech enhancement and source separation. Here, we propose to combine neural network supported multi-channel source separation with a time-domain training objective function. For the objective we propose to use a convolutive transfer function invariant Signal-to-Distortion Ratio (CI-SDR) based loss. While this is a well-known evaluation metric (BSS Eval), it has not been used as a training objective before. To show the effectiveness, we demonstrate the performance on LibriSpeech based reverberant mixtures. On this task, the proposed system approaches the error rate obtained on single-source non-reverberant input, i.e., LibriSpeech test_clean, with a difference of only 1.2 percentage points, thus outperforming a conventional permutation invariant training based system and alternative objectives like Scale Invariant Signal-to-Distortion Ratio by a large margin.      
### 78.Fast, Self Supervised, Fully Convolutional Color Normalization of H&amp;E Stained Images  [ :arrow_down: ](https://arxiv.org/pdf/2011.15000.pdf)
>  Performance of deep learning algorithms decreases drastically if the data distributions of the training and testing sets are different. Due to variations in staining protocols, reagent brands, and habits of technicians, color variation in digital histopathology images is quite common. Color variation causes problems for the deployment of deep learning-based solutions for automatic diagnosis system in histopathology. Previously proposed color normalization methods consider a small patch as a reference for normalization, which creates artifacts on out-of-distribution source images. These methods are also slow as most of the computation is performed on CPUs instead of the GPUs. We propose a color normalization technique, which is fast during its self-supervised training as well as inference. Our method is based on a lightweight fully-convolutional neural network and can be easily attached to a deep learning-based pipeline as a pre-processing block. For classification and segmentation tasks on CAMELYON17 and MoNuSeg datasets respectively, the proposed method is faster and gives a greater increase in accuracy than the state of the art methods.      
### 79.Exploiting Symmetry in the Power Flow Equations Using Monodromy  [ :arrow_down: ](https://arxiv.org/pdf/2011.14977.pdf)
>  We propose solving the power flow equations using monodromy. We prove the variety under consideration decomposes into trivial and nontrivial subvarieties and that the nontrivial subvariety is irreducible. We also show various symmetries in the solutions. We finish by giving numerical results comparing monodromy against polyhedral and total degree homotopy methods and giving an example of a network where we can find all solutions to the power flow equation using monodromy where other homotopy techniques fail. This work gives hope that finding all solutions to the power flow equations for networks of realistic size is possible.      
### 80.On Loewner data-driven control for infinite-dimensional systems  [ :arrow_down: ](https://arxiv.org/pdf/2011.14950.pdf)
>  In this paper, we address extensions of the Loewner Data-Driven Control (L-DDC) methodology. First, this approach is extended by incorporating two alternative approximation methods known as Adaptive-Antoulas-Anderson (AAA) and Vector Fitting (VF). These algorithms also include least squares fitting which provides additional flexibility and enables possible adjustments for control tuning. Secondly, the standard model reference data-driven setting is extended to handle noise affecting the data and uncertainty in the closed-loop objective function. These proposed adaptations yield a more robust data-driven control design.      
### 81.Modeling of a multiple source heating plate  [ :arrow_down: ](https://arxiv.org/pdf/2011.14939.pdf)
>  Heating plates describe the transfer of heat from actuators to a target object. In other words, they separate the heat sources and heated object and can be further used to apply a specific heat distribution on this object. Therefore, an exact description of their thermal dynamics and an efficient coordination of their actuators is necessary to achieve a desired time-dependent temperature profile accurately. In this contribution, the thermal dynamics of a multiple source heating plate is modeled as a quasi-linear heat equation and the configuration of the spatially distributed actuators and sensors are discussed. Furthermore, the distributed parameter system is approximated using a Finite Volume scheme, and the influence of the actuators' spatial characterization on the plate's thermal dynamics is studied with the resulting high-dimensional system.      
### 82.Look who's not talking  [ :arrow_down: ](https://arxiv.org/pdf/2011.14885.pdf)
>  The objective of this work is speaker diarisation of speech recordings 'in the wild'. The ability to determine speech segments is a crucial part of diarisation systems, accounting for a large proportion of errors. In this paper, we present a simple but effective solution for speech activity detection based on the speaker embeddings. In particular, we discover that the norm of the speaker embedding is an extremely effective indicator of speech activity. The method does not require an independent model for speech activity detection, therefore allows speaker diarisation to be performed using a unified representation for both speaker modelling and speech activity detection. We perform a number of experiments on in-house and public datasets, in which our method outperforms popular baselines.      
### 83.A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints  [ :arrow_down: ](https://arxiv.org/pdf/2011.14858.pdf)
>  The world is going through one of the most dangerous pandemics of all time with the rapid spread of the novel coronavirus (COVID-19). According to the World Health Organisation, the most effective way to thwart the transmission of coronavirus is to wear medical face masks. Monitoring the use of face masks in public places has been a challenge because manual monitoring could be unsafe. This paper proposes an architecture for detecting medical face masks for deployment on resource-constrained endpoints having extremely low memory footprints. A small development board with an ARM Cortex-M7 microcontroller clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for the deployment of the model. Using the TensorFlow Lite framework, the model is quantized to further reduce its size. The proposed model is 138 KB post quantization and runs at the inference speed of 30 FPS.      
### 84.Low Phase-Rank Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2011.14811.pdf)
>  In this paper, we propose and solve a low phase-rank approximation problem, which serves as a counterpart to the well-known low-rank approximation problem and the Schmidt-Mirsky theorem. More specifically, a nonzero complex number can be specified by its gain and phase, and while it is generally accepted that the gains of a matrix may be defined by its singular values, there is no widely accepted definition for its phases. In this work, we consider sectorial matrices, whose numerical ranges do not contain the origin, and adopt the canonical angles of such matrices as their phases. Similarly to the rank of a matrix defined to be the number of its nonzero singular values, we define the phase-rank of a sectorial matrix as the number of its nonzero phases. While a low-rank approximation problem is associated with matrix arithmetic means, as a natural parallel we formulate a low phase-rank approximation problem using matrix geometric means to measure the approximation error. A characterization of the solutions to the proposed problem is then obtained, when both the objective matrix and the approximant are restricted to be positive-imaginary. Moreover, the obtained solution has the same flavor as the Schmidt-Mirsky theorem on low-rank approximation problems. In addition, we provide an alternative formulation of the low phase-rank approximation problem using geodesic distances between sectorial matrices. The two formulations give rise to the exact same set of solutions when the involved matrices are additionally assumed to be unitary.      
### 85.Independent Component Analysis for noise and artifact removal in Three-dimensional Polarized Light Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2011.14786.pdf)
>  In recent years, Independent Component Analysis (ICA) has successfully been applied to remove noise and artifacts in images obtained from Three-dimensional Polarized Light Imaging (3D-PLI) at the mesoscale (i.e., 64 $\mu$m). Here, we present an automatic denoising procedure for gray matter regions that allows to apply the ICA also to microscopic images, with reasonable computational effort. Apart from an automatic segmentation of gray matter regions, we applied the denoising procedure to several 3D-PLI images from a rat and a vervet monkey brain section.      
### 86.Joint Beamforming Design and Power Splitting Optimization in IRS-Assisted SWIPT NOMA Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.14778.pdf)
>  This paper proposes a novel network framework of intelligent reflecting surface (IRS)-assisted simultaneous wireless information and power transfer (SWIPT) non-orthogonal multiple access (NOMA) networks, in which IRS is used to enhance the performance of NOMA and the wireless power transfer (WPT) efficiency of SWIPT. We formulate a problem of minimizing base station (BS) transmit power by jointly optimizing successive interference cancellation (SIC) decoding order, BS transmit beamforming vector, power splitting (PS) ratio and IRS phase shifts while taking into account the quality-of-service (QoS) requirement and energy harvested threshold of each user. The formulated problem is non-convex optimization problem, which is difficult to solve it directly. Hence, a two-stage algorithm is proposed to solve the above-mentioned problem by applying semidefinite relaxation (SDR) and Gaussian randomization. Specifically, after determining the SIC decoding order by designing the IRS phase shifts in the first stage, we alternately optimize the transmit beamforming vector, PS ratio, and IRS phase shifts to minimize the BS transmit power. Numerical results validate the effectiveness of our proposed optimization algorithm in reducing BS transmit power compared to other baseline algorithms. Meanwhile, the IRS-assisted SWIPT NOMA networks require lower BS transmit power than the non-IRS-assisted networks.      
### 87.Crowd Size using CommSense Instrument for COVID-19 Echo Period  [ :arrow_down: ](https://arxiv.org/pdf/2011.14775.pdf)
>  The period after the COVID-19 wave is called the Echo-period. Estimation of crowd size in an outdoor environment is essential in the Echo-period. Making a simple and flexible working system for the same is the need of the hour. This article proposes and evaluates a non-intrusive, passive, and costeffective solution for crowd size estimation in an outdoor environment. We call the proposed system as LTE communication infrastructure based environment sensing or LTE-CommSense. This system does not need any active signal transmission as it uses LTE transmitted signal. So, this is a power-efficient, simple low footprint device. Importantly, the personal identity of the people in the crowd can not be obtained using this method. First, the system uses practical data to determine whether the outdoor environment is empty or not. If not, it tries to estimate the number of people occupying the near range locality. Performance evaluation with practical data confirms the feasibility of this proposed approach.      
### 88.On Synergistic Benefits of Rate Splitting in IRS-assisted Cloud Radio Access Networks  [ :arrow_down: ](https://arxiv.org/pdf/2011.14763.pdf)
>  The concept of intelligent reflecting surfaces (IRSs) is considered as a promising technology for increasing the efficiency of mobile wireless networks. This is achieved by employing a vast amount of low-cost individually adjustable passive reflect elements, that are able to apply changes to the reflected signal. To this end, the IRS makes the environment realtime controllable and can be adjusted to significantly increase the received signal quality at the users by passive beamsteering. However, the changes to the reflected signals have an effect on all users near the IRS, which makes it impossible to optimize the changes to positively influence every transmission, affected by the reflections. This results in some users not only experiencing better signal quality, but also an increase in received interference. To mitigate this negative side effect of the IRS, this paper utilizes the rate splitting (RS) technique, which enables the mitigation of interference within the network in such a way that it also mitigates the increased interference caused by the IRS. To investigate the effects on the overall power savings, that can be achieved by combining both techniques, we minimize the required transmit power, needed to satisfy per-user quality-of-service (QoS) constraints. Numerical results show the improved power savings, that can be gained by utilizing the IRS and the RS technique simultaneously. In fact, the concurrent use of both techniques yields power savings, which are beyond the cumulative power savings of using each technique separately.      
### 89.Robust Ultra-wideband Range Error Mitigation with Deep Learning at the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2011.14684.pdf)
>  Ultra-wideband (UWB) is the state-of-the-art and most popular technology for wireless localization. Nevertheless, precise ranging and localization in non-line-of-sight (NLoS) conditions is still an open research topic. Indeed, multipath effects, reflections, refractions and complexity of the indoor radio environment can easily introduce a positive bias in the ranging measurement, resulting in highly inaccurate and unsatisfactory position estimation. This article proposes an efficient representation learning methodology that exploits the latest advancement in deep learning and graph optimization techniques to achieve effective ranging error mitigation at the edge. Channel Impulse Response (CIR) signals are directly exploited to extract high semantic features to estimate corrections in either NLoS or LoS conditions. Extensive experimentation with different settings and configurations have proved the effectiveness of our methodology and demonstrated the feasibility of a robust and low computational power UWB range error mitigation.      
### 90.SIR: Self-supervised Image Rectification via Seeing the Same Scene from Multiple Different Lenses  [ :arrow_down: ](https://arxiv.org/pdf/2011.14611.pdf)
>  Deep learning has demonstrated its power in image rectification by leveraging the representation capacity of deep neural networks via supervised training based on a large-scale synthetic dataset. However, the model may overfit the synthetic images and generalize not well on real-world fisheye images due to the limited universality of a specific distortion model and the lack of explicitly modeling the distortion and rectification process. In this paper, we propose a novel self-supervised image rectification (SIR) method based on an important insight that the rectified results of distorted images of the same scene from different lens should be the same. Specifically, we devise a new network architecture with a shared encoder and several prediction heads, each of which predicts the distortion parameter of a specific distortion model. We further leverage a differentiable warping module to generate the rectified images and re-distorted images from the distortion parameters and exploit the intra- and inter-model consistency between them during training, thereby leading to a self-supervised learning scheme without the need for ground-truth distortion parameters or normal images. Experiments on synthetic dataset and real-world fisheye images demonstrate that our method achieves comparable or even better performance than the supervised baseline method and representative state-of-the-art methods. Self-supervised learning also improves the universality of distortion models while keeping their self-consistency.      
### 91.Zero-Shot Calibration of Fisheye Cameras  [ :arrow_down: ](https://arxiv.org/pdf/2011.14607.pdf)
>  In this paper, we present a novel zero-shot camera calibration method that estimates camera parameters with no calibration image. It is common sense that we need at least one or more pattern images for camera calibration. However, the proposed method estimates camera parameters from the horizontal and vertical field of view information of the camera without any image acquisition. The proposed method is particularly useful for wide-angle or fisheye cameras that have large image distortion. Image distortion is modeled in the way fisheye lenses are designed and estimated based on the square pixel assumption of the image sensors. The calibration accuracy of the proposed method is evaluated on eight different commercial cameras qualitatively and quantitatively, and compared with conventional calibration methods. The experimental results show that the calibration accuracy of the zero-shot method is comparable to conventional full calibration results. The method can be used as a practical alternative in real applications where individual calibration is difficult or impractical, and in most field applications where calibration accuracy is less critical. Moreover, the estimated camera parameters by the method can also be used to provide proper initialization of any existing calibration methods, making them to converge more stably and avoid local minima.      
### 92.Polar-Cap Codebook Design for MISO Rician Fading Channels with Limited Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2011.14582.pdf)
>  Most of the prior works on designing codebooks for limited feedback systems have not considered the presence of strong line-of-sight (LOS) channel component. This paper proposes the design of polar-cap codebook (PCC) for multipleinput single-output (MISO) limited feedback systems subject to Rician fading channels. The codewords of the designed PCC are adaptively constructed according to the instantaneous strength of the LOS channel component. Simulation results show that the codebook can significantly enhance the performance of transmit beamforming in terms of received signal-to-noise ratio (SNR).      
### 93.Optimally Supporting IoT with Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2011.14514.pdf)
>  We study internet of things (IoT) systems supported by cell-free (CF) massive MIMO (mMIMO) with optimal linear channel estimation. For the uplink, we consider optimal linear MIMO receiver and obtain an uplink SINR approximation involving only large-scale fading coefficients using random matrix (RM) theory. Using this approximation we design several max-min power control algorithms that incorporate power and rate weighting coefficients to achieve a target rate with high energy efficiency. For the downlink, we consider maximum ratio (MR) beamforming. Instead of solving a complex quasi-concave problem for downlink power control, we employ a neural network (NN) technique to obtain comparable power control with around 30 times reduction in computation time. For large networks we proposed a different NN based power control algorithm. This algorithm is sub-optimal, but its big advantage is that it is scalable.      
### 94.Multi-Agent Maximization of a Monotone Submodular Function via Maximum Consensus  [ :arrow_down: ](https://arxiv.org/pdf/2011.14499.pdf)
>  Constrained submodular set function maximization problems often appear in multi-agent decision-making problems with a discrete feasible set. A prominent example is the problem of multi-agent mobile sensor placement over a discrete domain. However, submodular set function optimization problems are known to be NP-hard. In this paper, we consider a class of submodular optimization problems that consists of maximization of a monotone and submodular set function subject to a uniform matroid constraint over a group of networked agents that communicate over a connected undirected graph. Our objective is to obtain a distributed suboptimal polynomial-time algorithm that enables each agent to obtain its respective policy via local interactions with its neighboring agents. Our solution is a fully distributed gradient-based algorithm using the multilinear extension of the submodular set functions and exploiting a maximum consensus scheme. This algorithm results in a policy set that when the team objective function is evaluated at worst case the objective function value is in $1-1/e-O(1/T)$ of the optimal solution. An example demonstrates our results.      
### 95.Cyber-Physical Security Through Resiliency: A Systems-centric Approach  [ :arrow_down: ](https://arxiv.org/pdf/2011.14469.pdf)
>  Cyber-physical systems (CPS) are often defended in the same manner as information technology (IT) systems -- by using perimeter security. Multiple factors make such defenses insufficient for CPS. Resiliency shows potential in overcoming these shortfalls. Techniques for achieving resilience exist; however, methods and theory for evaluating resilience in CPS are lacking. We argue that such methods and theory should assist stakeholders in deciding where and how to apply design patterns for resilience. Such a problem potentially involves tradeoffs between different objectives and criteria, and such decisions need to be driven by traceable, defensible, repeatable engineering evidence. Multi-criteria resiliency problems require a system-oriented approach that evaluates systems in the presence of threats as well as potential design solutions once vulnerabilities have been identified. We present a systems-oriented view of cyber-physical security, termed Mission Aware, that is based on a holistic understanding of mission goals, system dynamics, and risk.      
### 96.Audio, Speech, Language, &amp; Signal Processing for COVID-19: A Comprehensive Overview  [ :arrow_down: ](https://arxiv.org/pdf/2011.14445.pdf)
>  The Coronavirus (COVID-19) pandemic has been the research focus world-wide in the year 2020. Several efforts, from collection of COVID-19 patients' data to screening them for the virus's detection are taken with rigour. A major portion of COVID-19 symptoms are related to the functioning of the respiratory system, which in-turn critically influences the human speech production system. This drives the research focus towards identifying the markers of COVID-19 in speech and other human generated audio signals. In this paper, we give an overview of the speech and other audio signal, language and general signal processing-based work done using Artificial Intelligence techniques to screen, diagnose, monitor, and spread the awareness aboutCOVID-19. We also briefly describe the research related to detect accord-ing COVID-19 symptoms carried out so far. We aspire that this collective information will be useful in developing automated systems, which can help in the context of COVID-19 using non-obtrusive and easy to use modalities such as audio, speech, and language.      
### 97.Architectural Adversarial Robustness: The Case for Deep Pursuit  [ :arrow_down: ](https://arxiv.org/pdf/2011.14427.pdf)
>  Despite their unmatched performance, deep neural networks remain susceptible to targeted attacks by nearly imperceptible levels of adversarial noise. While the underlying cause of this sensitivity is not well understood, theoretical analyses can be simplified by reframing each layer of a feed-forward network as an approximate solution to a sparse coding problem. Iterative solutions using basis pursuit are theoretically more stable and have improved adversarial robustness. However, cascading layer-wise pursuit implementations suffer from error accumulation in deeper networks. In contrast, our new method of deep pursuit approximates the activations of all layers as a single global optimization problem, allowing us to consider deeper, real-world architectures with skip connections such as residual networks. Experimentally, our approach demonstrates improved robustness to adversarial noise.      
### 98.Short-Term Flow-Based Bandwidth Forecasting using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2011.14421.pdf)
>  This paper proposes a novel framework to predict traffic flows' bandwidth ahead of time. Modern network management systems share a common issue: the network situation evolves between the moment the decision is made and the moment when actions (countermeasures) are applied. This framework converts packets from real-life traffic into flows containing relevant features. Machine learning models, including Decision Tree, Random Forest, XGBoost, and Deep Neural Network, are trained on these data to predict the bandwidth at the next time instance for every flow. Predictions can be fed to the management system instead of current flows bandwidth in order to take decisions on a more accurate network state. Experiments were performed on 981,774 flows and 15 different time windows (from 0.03s to 4s). They show that the Random Forest is the best performing and most reliable model, with predictive performance consistently better than relying on the current bandwidth (+19.73% in mean absolute error and +18.00% in root mean square error). Experimental results indicate that this framework can help network management systems to take more informed decisions using a predicted network state.      
### 99.Reinforcement Learning in Linear Quadratic Deep Structured Teams: Global Convergence of Policy Gradient Methods  [ :arrow_down: ](https://arxiv.org/pdf/2011.14393.pdf)
>  In this paper, we study the global convergence of model-based and model-free policy gradient descent and natural policy gradient descent algorithms for linear quadratic deep structured teams. In such systems, agents are partitioned into a few sub-populations wherein the agents in each sub-population are coupled in the dynamics and cost function through a set of linear regressions of the states and actions of all agents. Every agent observes its local state and the linear regressions of states, called deep states. For a sufficiently small risk factor and/or sufficiently large population, we prove that model-based policy gradient methods globally converge to the optimal solution. Given an arbitrary number of agents, we develop model-free policy gradient and natural policy gradient algorithms for the special case of risk-neutral cost function. The proposed algorithms are scalable with respect to the number of agents due to the fact that the dimension of their policy space is independent of the number of agents in each sub-population. Simulations are provided to verify the theoretical results.      
### 100.There and Back Again: Learning to Simulate Radar Data for Real-World Applications  [ :arrow_down: ](https://arxiv.org/pdf/2011.14389.pdf)
>  Simulating realistic radar data has the potential to significantly accelerate the development of data-driven approaches to radar processing. However, it is fraught with difficulty due to the notoriously complex image formation process. Here we propose to learn a radar sensor model capable of synthesising faithful radar observations based on simulated elevation maps. In particular, we adopt an adversarial approach to learning a forward sensor model from unaligned radar examples. In addition, modelling the backward model encourages the output to remain aligned to the world state through a cyclical consistency criterion. The backward model is further constrained to predict elevation maps from real radar data that are grounded by partial measurements obtained from corresponding lidar scans. Both models are trained in a joint optimisation. We demonstrate the efficacy of our approach by evaluating a down-stream segmentation model trained purely on simulated data in a real-world deployment. This achieves performance within four percentage points of the same model trained entirely on real data.      
### 101.Sequential Fair Allocation of Limited Resources under Stochastic Demands  [ :arrow_down: ](https://arxiv.org/pdf/2011.14382.pdf)
>  We consider the problem of dividing limited resources between a set of agents arriving sequentially with unknown (stochastic) utilities. Our goal is to find a fair allocation - one that is simultaneously Pareto-efficient and envy-free. When all utilities are known upfront, the above desiderata are simultaneously achievable (and efficiently computable) for a large class of utility functions. In a sequential setting, however, no policy can guarantee these desiderata simultaneously for all possible utility realizations. <br>A natural online fair allocation objective is to minimize the deviation of each agent's final allocation from their fair allocation in hindsight. This translates into simultaneous guarantees for both Pareto-efficiency and envy-freeness. However, the resulting dynamic program has state-space which is exponential in the number of agents. We propose a simple policy, HopeOnline, that instead aims to `match' the ex-post fair allocation vector using the current available resources and `predicted' histogram of future utilities. We demonstrate the effectiveness of our policy compared to other heurstics on a dataset inspired by mobile food-bank allocations.      
### 102.A smartphone based multi input workflow for non-invasive estimation of haemoglobin levels using machine learning techniques  [ :arrow_down: ](https://arxiv.org/pdf/2011.14370.pdf)
>  We suggest a low cost, non invasive healthcare system that measures haemoglobin levels in patients and can be used as a preliminary diagnostic test for anaemia. A combination of image processing, machine learning and deep learning techniques are employed to develop predictive models to measure haemoglobin levels. This is achieved through the color analysis of the fingernail beds, palpebral conjunctiva and tongue of the patients. This predictive model is then encapsulated in a healthcare application. This application expedites data collection and facilitates active learning of the model. It also incorporates personalized calibration of the model for each patient, assisting in the continual monitoring of the haemoglobin levels of the patient. Upon validating this framework using data, it can serve as a highly accurate preliminary diagnostic test for anaemia.      
### 103.An Features Extraction and Recognition Method for Underwater Acoustic Target Based on ATCNN  [ :arrow_down: ](https://arxiv.org/pdf/2011.14336.pdf)
>  Facing the complex marine environment, it is extremely challenging to conduct underwater acoustic target recognition (UATR) using ship-radiated noise. Inspired by neural mechanism of auditory perception, this paper provides a new deep neural network trained by original underwater acoustic signals with depthwise separable convolution (DWS) and time-dilated convolution neural network, named auditory perception inspired time-dilated convolution neural network (ATCNN), and then implements detection and classification for underwater acoustic signals. The proposed ATCNN model consists of learnable features extractor and integration layer inspired by auditory perception, and time-dilated convolution inspired by language model. This paper decomposes original time-domain ship-radiated noise signals into different frequency components with depthwise separable convolution filter, and then extracts signal features based on auditory perception. The deep features are integrated on integration layer. The time-dilated convolution is used for long-term contextual modeling. As a result, like language model, intra-class and inter-class information can be fully used for UATR. For UATR task, the classification accuracy reaches 90.9%, which is the highest in contrast experiment. Experimental results show that ATCNN has great potential to improve the performance of UATR classification.      
### 104.Audio-visual Speech Separation with Adversarially Disentangled Visual Representation  [ :arrow_down: ](https://arxiv.org/pdf/2011.14334.pdf)
>  Speech separation aims to separate individual voice from an audio mixture of multiple simultaneous talkers. Although audio-only approaches achieve satisfactory performance, they build on a strategy to handle the predefined conditions, limiting their application in the complex auditory scene. Towards the cocktail party problem, we propose a novel audio-visual speech separation model. In our model, we use the face detector to detect the number of speakers in the scene and use visual information to avoid the permutation problem. To improve our model's generalization ability to unknown speakers, we extract speech-related visual features from visual inputs explicitly by the adversarially disentangled method, and use this feature to assist speech separation. Besides, the time-domain approach is adopted, which could avoid the phase reconstruction problem existing in the time-frequency domain models. To compare our model's performance with other models, we create two benchmark datasets of 2-speaker mixture from GRID and TCDTIMIT audio-visual datasets. Through a series of experiments, our proposed model is shown to outperform the state-of-the-art audio-only model and three audio-visual models.      
### 105.Compressive Circular Polarization Snapshot Spectral Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2011.14308.pdf)
>  A compressive sensing based circular polarization snapshot spectral imaging system is proposed in this paper to acquire two-dimensional spatial, one-dimensional circular polarization (the right and left circular polarization), and one-dimensional spectral information, simultaneously. Using snapshot can collect the entire four-dimensional datacube in a single integration period. The dispersion prism in the coded aperture snapshot spectral imager is replaced by the combination of an Amici prism and a Wollaston prism to implement the spectral shifting along two orthogonal directions, which greatly improves the spectral resolution of the image. The right and left circular polarization components of objects are extracted by the assemble with an achromatic quarter wave-plate and a Wollaston prism. The encoding and reconstruction are illustrated comprehensively. The feasibility is verified by the simulation. It provides us an alternative approach for circular polarization spectral imaging such as defogging, underwater imaging, and so on.      
### 106.Approximate Midpoint Policy Iteration for Linear Quadratic Control  [ :arrow_down: ](https://arxiv.org/pdf/2011.14212.pdf)
>  We present a midpoint policy iteration algorithm to solve linear quadratic optimal control problems in both model-based and model-free settings. The algorithm is a variation of Newton's method, and we show that in the model-based setting it achieves cubic convergence, which is superior to standard policy iteration and policy gradient algorithms that achieve quadratic and linear convergence, respectively. We also demonstrate that the algorithm can be approximately implemented without knowledge of the dynamics model by using least-squares estimates of the state-action value function from trajectory data, from which policy improvements can be obtained. With sufficient trajectory data, the policy iterates converge cubically to approximately optimal policies, and this occurs with the same available sample budget as the approximate standard policy iteration. Numerical experiments demonstrate effectiveness of the proposed algorithms.      
### 107.Towards Robust Medical Image Segmentation on Small-Scale Data with Incomplete Labels  [ :arrow_down: ](https://arxiv.org/pdf/2011.14164.pdf)
>  The data-driven nature of deep learning models for semantic segmentation requires a large number of pixel-level annotations. However, large-scale and fully labeled medical datasets are often unavailable for practical tasks. Recently, partially supervised methods have been proposed to utilize images with incomplete labels to mitigate the data scarcity problem in the medical domain. As an emerging research area, the breakthroughs made by existing methods rely on either large-scale data or complex model design, which makes them 1) less practical for certain real-life tasks and 2) less robust for small-scale data. It is time to step back and think about the robustness of partially supervised methods and how to maximally utilize small-scale and partially labeled data for medical image segmentation tasks. To bridge the methodological gaps in label-efficient deep learning with partial supervision, we propose RAMP, a simple yet efficient data augmentation framework for partially supervised medical image segmentation by exploiting the assumption that patients share anatomical similarities. We systematically evaluate RAMP and the previous methods in various controlled multi-structure segmentation tasks. Compared to the mainstream approaches, RAMP consistently improves the performance of traditional segmentation networks on small-scale partially labeled data and utilize additional image-wise weak annotations.      
### 108.Short-Term Load Forecasting using Bi-directional Sequential Models and Feature Engineering for Small Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2011.14137.pdf)
>  Electricity load forecasting enables the grid operators to optimally implement the smart grid's most essential features such as demand response and energy efficiency. Electricity demand profiles can vary drastically from one region to another on diurnal, seasonal and yearly scale. Hence to devise a load forecasting technique that can yield the best estimates on diverse datasets, specially when the training data is limited, is a big challenge. This paper presents a deep learning architecture for short-term load forecasting based on bidirectional sequential models in conjunction with feature engineering that extracts the hand-crafted derived features in order to aid the model for better learning and predictions. In the proposed architecture, named as Deep Derived Feature Fusion (DeepDeFF), the raw input and hand-crafted features are trained at separate levels and then their respective outputs are combined to make the final prediction. The efficacy of the proposed methodology is evaluated on datasets from five countries with completely different patterns. The results demonstrate that the proposed technique is superior to the existing state of the art.      
### 109.Towards Fast and Light-Weight Restoration of Dark Images  [ :arrow_down: ](https://arxiv.org/pdf/2011.14133.pdf)
>  The ability to capture good quality images in the dark and near-zero lux conditions has been a long-standing pursuit of the computer vision community. The seminal work by Chen et al. [5] has especially caused renewed interest in this area, resulting in methods that build on top of their work in a bid to improve the reconstruction. However, for practical utility and deployment of low-light enhancement algorithms on edge devices such as embedded systems, surveillance cameras, autonomous robots and smartphones, the solution must respect additional constraints such as limited GPU memory and processing power. With this in mind, we propose a deep neural network architecture that aims to strike a balance between the network latency, memory utilization, model parameters, and reconstruction quality. The key idea is to forbid computations in the High-Resolution (HR) space and limit them to a Low-Resolution (LR) space. However, doing the bulk of computations in the LR space causes artifacts in the restored image. We thus propose Pack and UnPack operations, which allow us to effectively transit between the HR and LR spaces without incurring much artifacts in the restored image. We show that we can enhance a full resolution, 2848 x 4256, extremely dark single-image in the ballpark of 3 seconds even on a CPU. We achieve this with 2 - 7x fewer model parameters, 2 - 3x lower memory utilization, 5 - 20x speed up and yet maintain a competitive image reconstruction quality compared to the state-of-the-art algorithms.      
### 110.Overview of Screen Content Coding in Recently Developed Video Coding Standards  [ :arrow_down: ](https://arxiv.org/pdf/2011.14068.pdf)
>  In recent years, screen content (SC) video including computer generated text, graphics and animations, have drawn more attention than ever, as many related applications become very popular. To address the need for efficient coding of such contents, a number of coding tools have been specifically developed and achieved great advances in terms of coding efficiency. The inclusion of screen content coding (SCC) features in all the recently developed video coding standards (namely, HEVC SCC, VVC, AVS3, AV1 and EVC) demonstrated the importance of supporting such features. This paper provides an overview and comparative study of screen content coding technologies, with discussions on the performance and complexity aspects for the tools developed in these standards.      
### 111.Multidimensional Persistence Module Classification via Lattice-Theoretic Convolutions  [ :arrow_down: ](https://arxiv.org/pdf/2011.14057.pdf)
>  Multiparameter persistent homology has been largely neglected as an input to machine learning algorithms. We consider the use of lattice-based convolutional neural network layers as a tool for the analysis of features arising from multiparameter persistence modules. We find that these show promise as an alternative to convolutions for the classification of multidimensional persistence modules.      
### 112.Fundamental Schemes for Efficient Unconditionally Stable Implicit Finite-Difference Time-Domain Methods  [ :arrow_down: ](https://arxiv.org/pdf/2011.14043.pdf)
>  This paper presents the generalized formulations of fundamental schemes for efficient unconditionally stable implicit finite-difference time-domain (FDTD) methods. The fundamental schemes constitute a family of implicit schemes that feature similar fundamental updating structures, which are in simplest forms with most efficient right-hand sides. The formulations of fundamental schemes are presented in terms of generalized matrix operator equations pertaining to some classical splitting formulae, including those of alternating direction implicit, locally one-dimensional and split-step schemes. To provide further insights into the implications and significance of fundamental schemes, the analyses are also extended to many other schemes with distinctive splitting formulae. Detailed algorithms are described for new efficient implementations of the unconditionally stable implicit FDTD methods based on the fundamental schemes. A comparative study of various implicit schemes in their original and new implementations is carried out, which includes comparisons of their computation costs and efficiency gains.      
### 113.Assessing Post-Disaster Damage from Satellite Imagery using Semi-Supervised Learning Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2011.14004.pdf)
>  To respond to disasters such as earthquakes, wildfires, and armed conflicts, humanitarian organizations require accurate and timely data in the form of damage assessments, which indicate what buildings and population centers have been most affected. Recent research combines machine learning with remote sensing to automatically extract such information from satellite imagery, reducing manual labor and turn-around time. A major impediment to using machine learning methods in real disaster response scenarios is the difficulty of obtaining a sufficient amount of labeled data to train a model for an unfolding disaster. This paper shows a novel application of semi-supervised learning (SSL) to train models for damage assessment with a minimal amount of labeled data and large amount of unlabeled data. We compare the performance of state-of-the-art SSL methods, including MixMatch and FixMatch, to a supervised baseline for the 2010 Haiti earthquake, 2017 Santa Rosa wildfire, and 2016 armed conflict in Syria. We show how models trained with SSL methods can reach fully supervised performance despite using only a fraction of labeled data and identify areas for further improvements.      
