# ArXiv eess --Thu, 17 Dec 2020
### 1.Exploration of Whether Skylight Polarization Patterns Contain Three-dimensional Attitude Information  [ :arrow_down: ](https://arxiv.org/pdf/2012.09154.pdf)
>  Our previous work has demonstrated that Rayleigh model, which is widely used in polarized skylight navigation to describe skylight polarization patterns, does not contain three-dimensional (3D) attitude information [1]. However, it is still necessary to further explore whether the skylight polarization patterns contain 3D attitude information. So, in this paper, a social spider optimization (SSO) method is proposed to estimate three Euler angles, which considers the difference of each pixel among polarization images based on template matching (TM) to make full use of the captured polarization information. In addition, to explore this problem, we not only use angle of polarization (AOP) and degree of polarization (DOP) information, but also the light intensity (LI) information. So, a sky model is established, which combines Berry model and Hosek model to fully describe AOP, DOP, and LI information in the sky, and considers the influence of four neutral points, ground albedo, atmospheric turbidity, and wavelength. The results of simulation show that the SSO algorithm can estimate 3D attitude and the established sky model contains 3D attitude information. However, when there are measurement noise or model error, the accuracy of 3D attitude estimation drops significantly. Especially in field experiment, it is very difficult to estimate 3D attitude. Finally, the results are discussed in detail.      
### 2.MagBB: Wireless Charging for Batteryless Sensors using Magnetic Blind Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2012.09144.pdf)
>  Tiny batteryless sensors are desirable since they create negligible impacts on the operation of the system being monitored or the surrounding environment. Wireless energy transfer for batteryless sensors is challenging since they cannot cooperate with the charger due to the lack of energy. In this paper, a Magnetic Blind Beamforming (MagBB) algorithm is developed for wireless energy transfer for batteryless sensors in inhomogeneous media. Batteryless sensors with randomly orientated coils may experience significant orientation losses and they may not receive any energy from the charger. MagBB uses a set of optimized current vectors to generate rotating magnetic fields which can ensure that coils on batteryless sensors with arbitrary orientations can receive sufficient voltages for charging. It does not require any information regarding the batteryless sensor's coil orientation or location. The efficiency of MagBB is proven by extensive numerical simulations.      
### 3.Generative Neural Network Channel Modeling for Millimeter-Wave UAV Communication  [ :arrow_down: ](https://arxiv.org/pdf/2012.09133.pdf)
>  The millimeter wave bands are being increasingly considered for wireless communication to unmanned aerial vehicles (UAVs). Critical to this undertaking are statistical channel models that describe the distribution of constituent parameters in scenarios of interest. This paper presents a general modeling methodology based on data-training a generative neural network. The proposed generative model has a two-stage structure that first predicts the state of the link (line-of-sight, non-line-of-sight, or outage), and subsequently feeds this state into a conditional variational autoencoder that generates the path losses, delays, and angles of arrival and departure for all the propagation paths. Importantly, minimal prior assumptions are made, enabling the model to capture complex relationships within the data. The methodology is demonstrated for 28 GHz air-to-ground channels between UAVs and a cellular system in representative urban environments, with training datasets produced through ray tracing. The demonstration extends to both terrestrial base stations (mounted at street-level) and aerial base stations (installed on rooftops).      
### 4.Ensemble-CVDNet: A Deep Learning based End-to-End Classification Framework for COVID-19 Detection using Ensembles of Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.09132.pdf)
>  The new type of coronavirus disease (COVID-19), which started in Wuhan, China in December 2019, continues to spread rapidly affecting the whole world. It is essential to have a highly sensitive diagnostic screening tool to detect the disease as early as possible. Currently, chest CT imaging is preferred as the primary screening tool for evaluating the COVID-19 pneumonia by radiological imaging. However, CT imaging requires larger radiation doses, longer exposure time, higher cost, and may suffer from patient movements. X-Ray imaging is a fast, cheap, more patient-friendly and available in almost every healthcare facility. Therefore, we have focused on X-Ray images and developed an end-to-end deep learning model, i.e. Ensemble-CVDNet, to distinguish COVID-19 pneumonia from non-COVID pneumonia and healthy cases in this work. The proposed model is based on a combination of three lightweight pre-trained models SqueezeNet, ShuffleNet, and EfficientNet-B0 at different depths, and combines feature maps in different abstraction levels. In the proposed end to-end model, networks are used as feature extractors in parallel after fine-tuning, and some additional layers are used at the top of them. The proposed model is evaluated in the COVID-19 Radiography Database, a public data set consisting of 219 COVID-19, 1341 Healthy, and 1345 Viral Pneumonia chest X-Ray images. Experimental results show that our lightweight Ensemble-CVDNet model provides 98.30% accuracy, 97.78% sensitivity, and 97.61% F1 score using only 5.62M parameters. Moreover, it takes about 10ms to process and predict an X-Ray image using the proposed method using a mid level GPU. We believe that the method proposed in this study can be a helpful diagnostic screening tool for radiologists in the early diagnosis of the disease.      
### 5.User Coordination for Fast Beam Training in FDD Multi-User Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2012.09106.pdf)
>  Massive multiple-input multiple-output (mMIMO) communications are one of the enabling technologies of 5G and beyond networks. While prior work indicates that mMIMO networks employing time division duplexing have a significant capacity growth potential, deploying mMIMO in frequency division duplexing (FDD) networks remains problematic. The two main difficulties in FDD networks are the scalability of the downlink reference signals and the overhead associated with the required uplink feedback for channel state information (CSI) acquisition. To address these difficulties, most existing methods utilize assumptions on the radio environment such as channel sparsity or angular reciprocity. In this work, we propose a novel cooperative method for a scalable and low-overhead approach to FDD mMIMO under the so-called grid-of-beams architecture. The key idea behind our scheme lies in the exploitation of the near-common signal propagation paths that are often found across several mobile users located in nearby regions, through a coordination mechanism. In doing so, we leverage the recently specified device-to-device communications capability in 5G networks. Specifically, we design beam selection algorithms capable of striking a balance between CSI acquisition overhead and multi-user interference mitigation. The selection exploits statistical information, through so-called covariance shaping. Simulation results demonstrate the effectiveness of the proposed algorithms, which prove particularly well-suited to rapidly-varying channels with short coherence time.      
### 6.A Hierarchical Performance Equation Library for Op-Amp Design  [ :arrow_down: ](https://arxiv.org/pdf/2012.09088.pdf)
>  The paper presents a new approach to automate the set-up of the design equations of the manual design process. A hierarchical performance equation library (HPEL) for analog op-amps is introduced. Based on the library and the functional block recognition method in [1], analytical performance models for various op-amp topologies are automatically instantiated. The method emulates the manual design process reducing the equation set-up effort to seconds. The method is primarily designed for operational amplifiers. It has been applied to many different op-amp topologies. In this paper, we use the method to size different op-amp topologies. Experimental results featuring four circuits, a symmetrical op-amp with high PSRR, a telescopic op-amp, a complementary op-amp and a folded-cascode op-amp with CMFB, are presented. HPEL makes the set-up of design equations topology independent. This allows the integration of the HPEL into a synthesis method featuring more than 1000 op-amp topologies [2].      
### 7.Evaluation of deep learning-based myocardial infarction quantification using Segment CMR software  [ :arrow_down: ](https://arxiv.org/pdf/2012.09070.pdf)
>  In this paper, the author evaluates the preliminary work related to automating the quantification of the size of the myocardial infarction (MI) using deep learning in Segment cardiovascular magnetic resonance (CMR) software. Here, deep learning is used to automate the segmentation of myocardial boundaries before triggering the automatic quantification of the size of the MI using the expectation-maximization, weighted intensity, a priori information (EWA) algorithm incorporated in the Segment CMR software. Experimental evaluation of the size of the MI shows that more than 50 % (average infarct scar volume), 75% (average infarct scar percentage), and 65 % (average microvascular obstruction percentage) of the network-based results are approximately very close to the expert delineation-based results. Also, in an experiment involving the visualization of myocardial and infarct contours, in all images of the selected stack, the network and expert-based results tie in terms of the number of infarcted and contoured images.      
### 8.Data-driven control of infinite dimensional systems: Application to a continuous crystallizer  [ :arrow_down: ](https://arxiv.org/pdf/2012.09069.pdf)
>  Controlling infinite dimensional models remains a challenging task for many practitioners since they are not suitable for traditional control design techniques or will result in a high-order controller too complex for implementation. Therefore, the model or the controller need to be reduced to an acceptable dimension, which is time-consuming, requires some expertise and may introduce numerical error. This paper tackles the control of such a system, namely a continuous crystallizer, and compares two different data-driven strategies: the first one is a structured robust technique while the other one, called L-DDC, is based on the Loewner interpolatory framework.      
### 9.PGMAN: An Unsupervised Generative Multi-adversarial Network for Pan-sharpening  [ :arrow_down: ](https://arxiv.org/pdf/2012.09054.pdf)
>  Pan-sharpening aims at fusing a low-resolution (LR) multi-spectral (MS) image and a high-resolution (HR) panchromatic (PAN) image acquired by a satellite to generate an HR MS image. Many deep learning based methods have been developed in the past few years. However, since there are no intended HR MS images as references for learning, almost all of the existing methods down-sample the MS and PAN images and regard the original MS images as targets to form a supervised setting for training. These methods may perform well on the down-scaled images, however, they generalize poorly to the full-resolution images. To conquer this problem, we design an unsupervised framework that is able to learn directly from the full-resolution images without any preprocessing. The model is built based on a novel generative multi-adversarial network. We use a two-stream generator to extract the modality-specific features from the PAN and MS images, respectively, and develop a dual-discriminator to preserve the spectral and spatial information of the inputs when performing fusion. Furthermore, a novel loss function is introduced to facilitate training under the unsupervised setting. Experiments and comparisons with other state-of-the-art methods on GaoFen-2 and QuickBird images demonstrate that the proposed method can obtain much better fusion results on the full-resolution images.      
### 10.A Functional Block Decomposition Method for Automatic Op-Amp Design  [ :arrow_down: ](https://arxiv.org/pdf/2012.09051.pdf)
>  This paper presents a method to decompose an op-amp into its functional blocks. The method is able to recognize functional blocks on a high level of abstraction as loads or amplification stages which have a large set of possible structural implementations. The paper presents a hierarchical library of functional blocks. With every hierarchy level the structural representation of the functional blocks becomes more variable while its function emerges. We use the hierarchical order to automatically compute the functional decomposition of an op-amp given as a flat netlist. Experimental results will illustrate the method. The functional block decomposition enables a comprehensive formalization of design knowledge for computer-aided design of op-amps. This is the first paper of a selection of three paper on automatic op-amp design. In [1], the design equations that correspond to the functional blocks are presented. Based on the op-amp decomposition and the design equation, op-amps are automatically sized. In [2], an automatic op-amp structure synthesis method will be presented, which is based on the functional block decomposition presented in the following and the design equations from [1].      
### 11.Forward-backward kernel-based state and parameter estimation for linear systems of arbitrary order  [ :arrow_down: ](https://arxiv.org/pdf/2012.09033.pdf)
>  Previous results pertaining to algebraic state and parameter estimation of linear systems based on a special construction of a forward-backward kernel representation of linear differential invariants are extended to handle large noise in output measurement. Explicit expressions for the kernel functions for single-input, single-output (SISO) linear time-invariant (LTI) systems of arbitrary order are first stated and proved. A two-stage solution of the estimation problem is proposed next. The parameter estimation sub-problem, which is the task of the first stage, is solved by way of stochastic regression. The parameter estimation method presented in earlier publications is enhanced using multiple regression. The regression model does not satisfy the assumptions of the Gauss-Markov theorem in that the random regressor is heteroskedastic. This does not impede achieving high accuracy of estimation. A recursive version of a feasible generalized least squares with covariance weighting is employed to attenuate adverse effects due to heteroskedasticity. The output of the system and its time derivatives are reconstructed smoothly in the second stage of the approach by way of projection, while minimizing a mean square criterion.      
### 12.CT Super Resolution via Zero Shot Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.08943.pdf)
>  Computed Tomography (CT) is an advanced imaging technology used in many important applications. Here we present a deep-learning (DL) based CT super-resolution (SR) method that can reconstruct low-resolution (LR) sinograms into high resolution (HR) CT images. The method synergistically combines a SR model in sinogram domain, a deblur model in image domain, and the iterative framework into a CT SR algorithm super resolution and deblur based iterative reconstruction (SADIR). We incorporated the CT domain knowledge into the SADIR and unrolled it into a DL network (SADIR Net). The SADIR Net is a zero shot learning (ZSL) network, which can be trained and tested with a single sinogram in the test time. The SADIR was evaluated via SR CT imaging of a Catphan700 physical phantom and a biological ham, and its performance was compared to the other state of the art (SotA) DL-based methods. The results show that the zero-shot SADIR-Net can indeed provide a performance comparable to the other SotA methods for CT SR reconstruction, especially in situations where training data is limited. The SADIR method can find use in improving CT resolution beyond hardware limits or lowering requirement on CT hardware.      
### 13.A Distributed Methodology for Approximate Uniform Global Minimum Sharing  [ :arrow_down: ](https://arxiv.org/pdf/2012.08940.pdf)
>  The paper deals with the distributed minimum sharing problem, in which a network of decision-makers - exchanging information through a communication network - computes the minimum of some local quantities of interest in a distributed and decentralized way. The problem is equivalently cast into a cost-coupled distributed optimization problem, and an adjustable approximate (or sub-optimal) solution is presented which enjoys several properties of crucial importance in applications. In particular, the proposed solution is scalable in that the dimension of the state space does not grow with the size or topology of the communication network. Moreover, a global and uniform (both in the initial time and in the initial condition) asymptotic stability result is provided, as well as an attractiveness property towards a steady state which can be made arbitrarily close to the sought minimum. Exact asymptotic convergence is also recovered at the price, however, of loosing uniformity of the convergence with respect to the initial time.      
### 14.Deep learning for fast MR imaging: a review for learning reconstruction from incomplete k-space data  [ :arrow_down: ](https://arxiv.org/pdf/2012.08931.pdf)
>  Magnetic resonance imaging is a powerful imaging modality that can provide versatile information but it has a bottleneck problem "slow imaging speed". Reducing the scanned measurements can accelerate MR imaging with the aid of powerful reconstruction methods, which have evolved from linear analytic models to nonlinear iterative ones. The emerging trend in this area is replacing human-defined signal models with that learned from data. Specifically, from 2016, deep learning has been incorporated into the fast MR imaging task, which draws valuable prior knowledge from big datasets to facilitate accurate MR image reconstruction from limited measurements. This survey aims to review deep learning based MR image reconstruction works from 2016- June 2020 and will discuss merits, limitations and challenges associated with such methods. Last but not least, this paper will provide a starting point for researchers interested in contributing to this field by pointing out good tutorial resources, state-of-the-art open-source codes and meaningful data sources.      
### 15.Learning-Based Algorithms for Vessel Tracking: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.08929.pdf)
>  Developing efficient vessel-tracking algorithms is crucial for imaging-based diagnosis and treatment of vascular diseases. Vessel tracking aims to solve recognition problems such as key (seed) point detection, centerline extraction, and vascular segmentation. Extensive image-processing techniques have been developed to overcome the problems of vessel tracking that are mainly attributed to the complex morphologies of vessels and image characteristics of angiography. This paper presents a literature review on vessel-tracking methods, focusing on machine-learning-based methods. First, the conventional machine-learning-based algorithms are reviewed, and then, a general survey of deep-learning-based frameworks is provided. On the basis of the reviewed methods, the evaluation issues are introduced. The paper is concluded with discussions about the remaining exigencies and future research.      
### 16.Secret Key Agreement with Physical Unclonable Functions: An Optimality Summary  [ :arrow_down: ](https://arxiv.org/pdf/2012.08924.pdf)
>  We address security and privacy problems for digital devices and biometrics from an information-theoretic optimality perspective, where a secret key is generated for authentication, identification, message encryption/decryption, or secure computations. A physical unclonable function (PUF) is a promising solution for local security in digital devices and this review gives the most relevant summary for information theorists, coding theorists, and signal processing community members who are interested in optimal PUF constructions. Low-complexity signal processing methods such as transform coding that are developed to make the information-theoretic analysis tractable are discussed. The optimal trade-offs between the secret-key, privacy-leakage, and storage rates for multiple PUF measurements are given. Proposed optimal code constructions that jointly design the vector quantizer and error-correction code parameters are listed. These constructions include modern and algebraic codes such as polar codes and convolutional codes, both of which can achieve small block-error probabilities at short block lengths, corresponding to a small number of PUF circuits. Open problems in the PUF literature from a signal processing, information theory, coding theory, and hardware complexity perspectives and their combinations are listed to stimulate further advancements in the research on local privacy and security.      
### 17.Robust self-triggered DMPC for linear discrete-time systems with local and global constraints  [ :arrow_down: ](https://arxiv.org/pdf/2012.08872.pdf)
>  This paper proposes a robust self-triggered distributed model predictive control (DMPC) scheme for a family of Discrete-Time linear systems with local (uncoupled) and global (coupled) constraints. To handle the additive disturbance, tube-based method is proposed for the satisfaction of local state and control constraints. Meanwhile, A special form of constraints tightening is given to guarantee the global coupled constraints. The self-triggering mechanism help reduce the computation burden by skip insignificant iteration steps, which determine a certain sampling instants to solve the DMPC optimization problem in parallel ways. The DMPC optimization problem is constructed as a dual form, and solved distributedly based on the Alternative Direction Multiplier Method (ADMM) with some known simplifications. Recursive feasibility and input-to-state stability of the closed-loop system are shown, the performance of proposed scheme is demonstrated by a simulation example.      
### 18.A Synergistic Kalman- and Deep Postfiltering Approach to Acoustic Echo Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2012.08867.pdf)
>  We introduce a synergistic approach to double-talk robust acoustic echo cancellation combining adaptive Kalman filtering with a deep neural network-based postfilter. The proposed algorithm overcomes the well-known limitations of Kalman filter-based adaptation control in scenarios characterized by abrupt echo path changes. As the key innovation, we suggest to exploit the different statistical properties of the interfering signal components for robustly estimating the adaptation step size. This is achieved by leveraging the postfilter near-end estimate and the estimation error of the Kalman filter. The proposed synergistic scheme allows for rapid reconvergence of the adaptive filter after abrupt echo path changes without compromising the steady state performance achieved by state-of-the-art approaches in static scenarios.      
### 19.Toolbox for Discovering Dynamic System Relations via TAG Guided Genetic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2012.08834.pdf)
>  Data-driven modeling of nonlinear dynamical systems often require an expert user to take critical decisions a priori to the identification procedure. Recently an automated strategy for data driven modeling of \textit{single-input single-output} (SISO) nonlinear dynamical systems based on \textit{Genetic Programming} (GP) and \textit{Tree Adjoining Grammars} (TAG) has been introduced. The current paper extends these latest findings by proposing a \textit{multi-input multi-output} (MIMO) TAG modeling framework for polynomial NARMAX models. Moreover we introduce a TAG identification toolbox in Matlab that provides implementation of the proposed methodology to solve multi-input multi-output identification problems under NARMAX noise assumption. The capabilities of the toolbox and the modelling methodology are demonstrated in the identification of two SISO and one MIMO nonlinear dynamical benchmark models.      
### 20.PALMNUT: An Enhanced Proximal Alternating Linearized Minimization Algorithm with Application to Separate Regularization of Magnitude and Phase  [ :arrow_down: ](https://arxiv.org/pdf/2012.08779.pdf)
>  We introduce a new algorithm for complex image reconstruction with separate regularization of the image magnitude and phase. This optimization problem is interesting in many different image reconstruction contexts, although is nonconvex and can be difficult to solve. In this work, we first describe a novel implementation of the previous proximal alternating linearized minization (PALM) algorithm to solve this optimization problem. We then make enhancements to PALM, leading to a new algorithm named PALMNUT that combines the PALM together with Nesterov's momentum and a novel approach that relies on uncoupled coordinatewise step sizes derived from coordinatewise Lipschitz-like bounds. Theoretically, we establish that a version of PALMNUT (without Nesterov's momentum) monotonically decreases the objective function, leading to guaranteed convergence in many cases of interest. Empirical results obtained in the context of magnetic resonance imaging demonstrate that PALMNUT has computational advantages over common existing approaches like alternating minimization. Although our focus is on the application to separate magnitude and phase regularization, we expect that the same approach may also be useful in other nonconvex optimization problems with similar objective function structure.      
### 21.Cross-Cohort Generalizability of Deep and Conventional Machine Learning for MRI-based Diagnosis and Prediction of Alzheimer's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2012.08769.pdf)
>  This work validates the generalizability of MRI-based classification of Alzheimer's disease (AD) patients and controls (CN) to an external data set and to the task of prediction of conversion to AD in individuals with mild cognitive impairment (MCI). We used a conventional support vector machine (SVM) and a deep convolutional neural network (CNN) approach based on structural MRI scans that underwent either minimal pre-processing or more extensive pre-processing into modulated gray matter (GM) maps. Classifiers were optimized and evaluated using cross-validation in the ADNI (334 AD, 520 CN). Trained classifiers were subsequently applied to predict conversion to AD in ADNI MCI patients (231 converters, 628 non-converters) and in the independent Health-RI Parelsnoer data set. From this multi-center study representing a tertiary memory clinic population, we included 199 AD patients, 139 participants with subjective cognitive decline, 48 MCI patients converting to dementia, and 91 MCI patients who did not convert to dementia. AD-CN classification based on modulated GM maps resulted in a similar AUC for SVM (0.940) and CNN (0.933). Application to conversion prediction in MCI yielded significantly higher performance for SVM (0.756) than for CNN (0.742). In external validation, performance was slightly decreased. For AD-CN, it again gave similar AUCs for SVM (0.896) and CNN (0.876). For prediction in MCI, performances decreased for both SVM (0.665) and CNN (0.702). Both with SVM and CNN, classification based on modulated GM maps significantly outperformed classification based on minimally processed images. Deep and conventional classifiers performed equally well for AD classification and their performance decreased only slightly when applied to the external cohort. We expect that this work on external validation contributes towards translation of machine learning to clinical practice.      
### 22.Learning-Based Quality Assessment for Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2012.08732.pdf)
>  Image Super-Resolution (SR) techniques improve visual quality by enhancing the spatial resolution of images. Quality evaluation metrics play a critical role in comparing and optimizing SR algorithms, but current metrics achieve only limited success, largely due to the lack of large-scale quality databases, which are essential for learning accurate and robust SR quality metrics. In this work, we first build a large-scale SR image database using a novel semi-automatic labeling approach, which allows us to label a large number of images with manageable human workload. The resulting SR Image quality database with Semi-Automatic Ratings (SISAR), so far the largest of SR-IQA database, contains 8,400 images of 100 natural scenes. We train an end-to-end Deep Image SR Quality (DISQ) model by employing two-stream Deep Neural Networks (DNNs) for feature extraction, followed by a feature fusion network for quality prediction. Experimental results demonstrate that the proposed method outperforms state-of-the-art metrics and achieves promising generalization performance in cross-database tests. The SISAR database and DISQ model will be made publicly available to facilitate reproducible research.      
### 23.Aging Bandits: Regret Analysis and Order-Optimal Learning Algorithm for Wireless Networks with Stochastic Arrivals  [ :arrow_down: ](https://arxiv.org/pdf/2012.08682.pdf)
>  We consider a single-hop wireless network with sources transmitting time-sensitive information to the destination over multiple unreliable channels. Packets from each source are generated according to a stochastic process with known statistics and the state of each wireless channel (ON/OFF) varies according to a stochastic process with unknown statistics. The reliability of the wireless channels is to be learned through observation. At every time slot, the learning algorithm selects a single pair (source, channel) and the selected source attempts to transmit its packet via the selected channel. The probability of a successful transmission to the destination depends on the reliability of the selected channel. The goal of the learning algorithm is to minimize the Age-of-Information (AoI) in the network over $T$ time slots. To analyze the performance of the learning algorithm, we introduce the notion of AoI regret, which is the difference between the expected cumulative AoI of the learning algorithm under consideration and the expected cumulative AoI of a genie algorithm that knows the reliability of the channels a priori. The AoI regret captures the penalty incurred by having to learn the statistics of the channels over the $T$ time slots. The results are two-fold: first, we consider learning algorithms that employ well-known solutions to the stochastic multi-armed bandit problem (such as $\epsilon$-Greedy, Upper Confidence Bound, and Thompson Sampling) and show that their AoI regret scales as $\Theta(\log T)$; second, we develop a novel learning algorithm and show that it has $O(1)$ regret. To the best of our knowledge, this is the first learning algorithm with bounded AoI regret.      
### 24.CUDA-Optimized real-time rendering of a Foveated Visual System  [ :arrow_down: ](https://arxiv.org/pdf/2012.08655.pdf)
>  The spatially-varying field of the human visual system has recently received a resurgence of interest with the development of virtual reality (VR) and neural networks. The computational demands of high resolution rendering desired for VR can be offset by savings in the periphery, while neural networks trained with foveated input have shown perceptual gains in i.i.d and o.o.d generalization. In this paper, we present a technique that exploits the CUDA GPU architecture to efficiently generate Gaussian-based foveated images at high definition (1920x1080 px) in real-time (165 Hz), with a larger number of pooling regions than previous Gaussian-based foveation algorithms by several orders of magnitude, producing a smoothly foveated image that requires no further blending or stitching, and that can be well fit for any contrast sensitivity function. The approach described can be adapted from Gaussian blurring to any eccentricity-dependent image processing and our algorithm can meet demand for experimentation to evaluate the role of spatially-varying processing across biological and artificial agents, so that foveation can be added easily on top of existing systems rather than forcing their redesign (emulated foveated renderer). Altogether, this paper demonstrates how a GPU, with a CUDA block-wise architecture, can be employed for radially-variant rendering, with opportunities for more complex post-processing to ensure a metameric foveation scheme. Code is provided.      
### 25.An anatomically-informed 3D CNN for brain aneurysm classification with weak labels  [ :arrow_down: ](https://arxiv.org/pdf/2012.08645.pdf)
>  A commonly adopted approach to carry out detection tasks in medical imaging is to rely on an initial segmentation. However, this approach strongly depends on voxel-wise annotations which are repetitive and time-consuming to draw for medical experts. An interesting alternative to voxel-wise masks are so-called "weak" labels: these can either be coarse or oversized annotations that are less precise, but noticeably faster to create. In this work, we address the task of brain aneurysm detection as a patch-wise binary classification with weak labels, in contrast to related studies that rather use supervised segmentation methods and voxel-wise delineations. Our approach comes with the non-trivial challenge of the data set creation: as for most focal diseases, anomalous patches (with aneurysm) are outnumbered by those showing no anomaly, and the two classes usually have different spatial distributions. To tackle this frequent scenario of inherently imbalanced, spatially skewed data sets, we propose a novel, anatomically-driven approach by using a multi-scale and multi-input 3D Convolutional Neural Network (CNN). We apply our model to 214 subjects (83 patients, 131 controls) who underwent Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) and presented a total of 111 unruptured cerebral aneurysms. We compare two strategies for negative patch sampling that have an increasing level of difficulty for the network and we show how this choice can strongly affect the results. To assess whether the added spatial information helps improving performances, we compare our anatomically-informed CNN with a baseline, spatially-agnostic CNN. When considering the more realistic and challenging scenario including vessel-like negative patches, the former model attains the highest classification results (accuracy$\simeq$95\%, AUROC$\simeq$0.95, AUPR$\simeq$0.71), thus outperforming the baseline.      
### 26.Distributed Wasserstein Barycenters via Displacement Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2012.08610.pdf)
>  Consider a multi-agent system whereby each agent has an initial probability measure. In this paper, we propose a distributed algorithm based upon stochastic, asynchronous and pairwise exchange of information and displacement interpolation in the Wasserstein space. We characterize the evolution of this algorithm and prove it computes the Wasserstein barycenter of the initial measures under various conditions. One version of the algorithm computes a standard Wasserstein barycenter, i.e., a barycenter based upon equal weights; and the other version computes a randomized Wasserstein barycenter, i.e., a barycenter based upon random weights for the initial measures. Finally, we specialize our results to Gaussian distributions and draw a connection with the modeling of opinion dynamics in mathematical sociology.      
### 27.Measurement Errors in Range-Based Localization Algorithms for UAVs: Analysis and Experimentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.08599.pdf)
>  Localizing ground devices (GDs) is an important requirement for a wide variety of applications, such as infrastructure monitoring, precision agriculture, search and rescue operations, to name a few. To this end, unmanned aerial vehicles (UAVs) or drones offer a promising technology due to their flexibility. However, the distance measurements performed using a drone, an integral part of a localization procedure, incur several errors that affect the localization accuracy. In this paper, we provide analytical expressions for the impact of different kinds of measurement errors on the ground distance between the UAV and GDs. We review three range-based and three range-free localization algorithms, identify their source of errors, and analytically derive the error bounds resulting from aggregating multiple inaccurate measurements. We then extend the range-free algorithms for improved accuracy. We validate our theoretical analysis and compare the observed localization error of the algorithms after collecting data from a testbed using ten GDs and one drone, equipped with ultra wide band (UWB) antennas and operating in an open field. Results show that our analysis closely matches with experimental localization errors. Moreover, compared to their original counterparts, the extended range-free algorithms significantly improve the accuracy.      
### 28.Personal Mental Health Navigator: Harnessing the Power of Data, Personal Models, and Health Cybernetics to Promote Psychological Well-being  [ :arrow_down: ](https://arxiv.org/pdf/2012.09131.pdf)
>  Traditionally, the regime of mental healthcare has followed an episodic psychotherapy model wherein patients seek care from a provider through a prescribed treatment plan developed over multiple provider visits. Recent advances in wearable and mobile technology have generated increased interest in digital mental healthcare that enables individuals to address episodic mental health symptoms. However, these efforts are typically reactive and symptom-focused and do not provide comprehensive, wrap-around, customized treatments that capture an individual's holistic mental health model as it unfolds over time. Recognizing that each individual is unique, we present the notion of Personalized Mental Health Navigation (MHN): a therapist-in-the-loop, cybernetic goal-based system that deploys a continuous cyclic loop of measurement, estimation, guidance, to steer the individual's mental health state towards a healthy zone. We outline the major components of MHN that is premised on the development of an individual's personal mental health state, holistically represented by a high-dimensional cover of multiple knowledge layers such as emotion, biological patterns, sociology, behavior, and cognition. We demonstrate the feasibility of the personalized MHN approach via a 12-month pilot case study for holistic stress management in college students and highlight an instance of a therapist-in-the-loop intervention using MHN for monitoring, estimating, and proactively addressing moderately severe depression over a sustained period of time. We believe MHN paves the way to transform mental healthcare from the current passive, episodic, reactive process (where individuals seek help to address symptoms that have already manifested) to a continuous and navigational paradigm that leverages a personalized model of the individual, promising to deliver timely interventions to individuals in a holistic manner.      
### 29.CMOS Quantum Computing: Toward A Quantum Computer System-on-Chip  [ :arrow_down: ](https://arxiv.org/pdf/2012.09021.pdf)
>  Quantum computing is experiencing the transition from a scientific to an engineering field with the promise to revolutionize an extensive range of applications demanding high-performance computing. Many implementation approaches have been pursued for quantum computing systems, where currently the main streams can be identified based on superconducting, photonic, trapped-ion, and semiconductor qubits. Semiconductor-based quantum computing, specifically using CMOS technologies, is promising as it provides potential for the integration of qubits with their control and readout circuits on a single chip. This paves the way for the realization of a large-scale quantum computing system for solving practical problems. In this paper, we present an overview and future perspective of CMOS quantum computing, exploring developed semiconductor qubit structures, quantum gates, as well as control and readout circuits, with a focus on the promises and challenges of CMOS implementation.      
### 30.Personalized Step Counting Using Wearable Sensors: A Domain Adapted LSTM Network Approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.08975.pdf)
>  Activity monitors are widely used to measure various physical activities (PA) as an indicator of mobility, fitness and general health. Similarly, real-time monitoring of longitudinal trends in step count has significant clinical potential as a personalized measure of disease related changes in daily activity. However, inconsistent step count accuracy across vendors, body locations, and individual gait differences limits clinical utility. The tri-axial accelerometer inside PA monitors can be exploited to improve step count accuracy across devices and individuals. In this study, we hypothesize: (1) raw tri-axial sensor data can be modeled to create reliable and accurate step count, and (2) a generalized step count model can then be efficiently adapted to each unique gait pattern using very little new data. Firstly, open-source raw sensor data was used to construct a long short term memory (LSTM) deep neural network to model step count. Then we generated a new, fully independent data set using a different device and different subjects. Finally, a small amount of subject-specific data was domain adapted to produce personalized models with high individualized step count accuracy. These results suggest models trained using large freely available datasets can be adapted to patient populations where large historical data sets are rare.      
### 31.SimuGAN: Unsupervised forward modeling and optimal design of a LIDAR Camera  [ :arrow_down: ](https://arxiv.org/pdf/2012.08951.pdf)
>  Energy-saving LIDAR camera for short distances estimates an object's distance using temporally intensity-coded laser light pulses and calculates the maximum correlation with the back-scattered pulse. <br>Though on low power, the backs-scattered pulse is noisy and unstable, which leads to inaccurate and unreliable depth estimation. <br>To address this problem, we use GANs (Generative Adversarial Networks), which are two neural networks that can learn complicated class distributions through an adversarial process. We learn the LIDAR camera's hidden properties and behavior, creating a novel, fully unsupervised forward model that simulates the camera. Then, we use the model's differentiability to explore the camera parameter space and optimize those parameters in terms of depth, accuracy, and stability. To achieve this goal, we also propose a new custom loss function designated to the back-scattered code distribution's weaknesses and its circular behavior. The results are demonstrated on both synthetic and real data.      
### 32.FuseVis: Interpreting neural networks for image fusion using per-pixel saliency visualization  [ :arrow_down: ](https://arxiv.org/pdf/2012.08932.pdf)
>  Image fusion helps in merging two or more images to construct a more informative single fused image. Recently, unsupervised learning based convolutional neural networks (CNN) have been utilized for different types of image fusion tasks such as medical image fusion, infrared-visible image fusion for autonomous driving as well as multi-focus and multi-exposure image fusion for satellite imagery. However, it is challenging to analyze the reliability of these CNNs for the image fusion tasks since no groundtruth is available. This led to the use of a wide variety of model architectures and optimization functions yielding quite different fusion results. Additionally, due to the highly opaque nature of such neural networks, it is difficult to explain the internal mechanics behind its fusion results. To overcome these challenges, we present a novel real-time visualization tool, named FuseVis, with which the end-user can compute per-pixel saliency maps that examine the influence of the input image pixels on each pixel of the fused image. We trained several image fusion based CNNs on medical image pairs and then using our FuseVis tool, we performed case studies on a specific clinical application by interpreting the saliency maps from each of the fusion methods. We specifically visualized the relative influence of each input image on the predictions of the fused image and showed that some of the evaluated image fusion methods are better suited for the specific clinical application. To the best of our knowledge, currently, there is no approach for visual analysis of neural networks for image fusion. Therefore, this work opens up a new research direction to improve the interpretability of deep fusion networks. The FuseVis tool can also be adapted in other deep neural network based image processing applications to make them interpretable.      
### 33.On The Verification of Neural ODEs with Stochastic Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2012.08863.pdf)
>  We show that Neural ODEs, an emerging class of time-continuous neural networks, can be verified by solving a set of global-optimization problems. For this purpose, we introduce Stochastic Lagrangian Reachability (SLR), an abstraction-based technique for constructing a tight Reachtube (an over-approximation of the set of reachable states over a given time-horizon), and provide stochastic guarantees in the form of confidence intervals for the Reachtube bounds. SLR inherently avoids the infamous wrapping effect (accumulation of over-approximation errors) by performing local optimization steps to expand safe regions instead of repeatedly forward-propagating them as is done by deterministic reachability methods. To enable fast local optimizations, we introduce a novel forward-mode adjoint sensitivity method to compute gradients without the need for backpropagation. Finally, we establish asymptotic and non-asymptotic convergence rates for SLR.      
### 34.Consistency of Distributionally Robust Risk- and Chance-Constrained Optimization under Wasserstein Ambiguity Sets  [ :arrow_down: ](https://arxiv.org/pdf/2012.08850.pdf)
>  We study stochastic optimization problems with chance and risk constraints, where in the latter, risk is quantified in terms of the conditional value-at-risk (CVaR). We consider the distributionally robust versions of these problems, where the constraints are required to hold for a family of distributions constructed from the observed realizations of the uncertainty via the Wasserstein distance. Our main results establish that if the samples are drawn independently from an underlying distribution and the problems satisfy suitable technical assumptions, then the optimal value and optimizers of the distributionally robust versions of these problems converge to the respective quantities of the original problems, as the sample size increases.      
### 35.Continuous Gesture Recognition from sEMG Sensor Data with Recurrent Neural Networks and Adversarial Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2012.08816.pdf)
>  Movement control of artificial limbs has made big advances in recent years. New sensor and control technology enhanced the functionality and usefulness of artificial limbs to the point that complex movements, such as grasping, can be performed to a limited extent. To date, the most successful results were achieved by applying recurrent neural networks (RNNs). However, in the domain of artificial hands, experiments so far were limited to non-mobile wrists, which significantly reduces the functionality of such prostheses. In this paper, for the first time, we present empirical results on gesture recognition with both mobile and non-mobile wrists. Furthermore, we demonstrate that recurrent neural networks with simple recurrent units (SRU) outperform regular RNNs in both cases in terms of gesture recognition accuracy, on data acquired by an arm band sensing electromagnetic signals from arm muscles (via surface electromyography or sEMG). Finally, we show that adding domain adaptation techniques to continuous gesture recognition with RNN improves the transfer ability between subjects, where a limb controller trained on data from one person is used for another person.      
### 36.Spatial light interference microscopy (SLIM): principle and applications to biomedicine  [ :arrow_down: ](https://arxiv.org/pdf/2012.08801.pdf)
>  In this paper, we review spatial light interference microscopy (SLIM), a common-path, phase-shifting interferometer, built onto a phase-contrast microscope, with white-light illumination. As one of the most sensitive quantitative phase imaging (QPI) methods, SLIM allows for speckle-free phase reconstruction with sub-nanometer path-length stability. We first review image formation in QPI, scattering, holography, and microcopy. Then, we outline SLIM imaging from theory to instrumentation. Zernike phase-contrast microscopy, phase retrieval in SLIM, and halo removal algorithms are discussed. Next, we discuss the requirements for operation, with a focus on software developed in-house for SLIM that high-throughput acquisition, whole slide scanning, mosaic tile registration, and imaging with a color camera. Lastly, we review the applications of SLIM in basic science and clinical studies. SLIM can study cell dynamics, cell growth and proliferation, cell migration, and mass transport, etc. In clinical settings, SLIM can assist with cancer studies, reproductive technology, and blood testing, etc. Finally, we review an emerging trend, where SLIM imaging in conjunction with artificial intelligence (AI) brings computational specificity and, in turn, offers new solutions to outstanding challenges in cell biology and pathology.      
### 37.Edge Entropy as an Indicator of the Effectiveness of GNNs over CNNs for Node Classification  [ :arrow_down: ](https://arxiv.org/pdf/2012.08698.pdf)
>  Graph neural networks (GNNs) extend convolutional neural networks (CNNs) to graph-based data. A question that arises is how much performance improvement does the underlying graph structure in the GNN provide over the CNN (that ignores this graph structure). To address this question, we introduce edge entropy and evaluate how good an indicator it is for possible performance improvement of GNNs over CNNs. Our results on node classification with synthetic and real datasets show that lower values of edge entropy predict larger expected performance gains of GNNs over CNNs, and, conversely, higher edge entropy leads to expected smaller improvement gains.      
### 38.A grid-point detection method based on U-net for a structured light system  [ :arrow_down: ](https://arxiv.org/pdf/2012.08641.pdf)
>  Accurate detection of the feature points of the projected pattern plays an extremely important role in one-shot 3D reconstruction systems, especially for the ones using a grid pattern. To solve this problem, this paper proposes a grid-point detection method based on U-net. A specific dataset is designed that includes the images captured with the two-shot imaging method and the ones acquired with the one-shot imaging method. Among them, the images in the first group after labeled as the ground truth images and the images captured at the same pose with the one-shot method are cut into small patches with the size of 64x64 pixels then feed to the training set. The remaining of the images in the second group is the test set. The experimental results show that our method can achieve a better detecting performance with higher accuracy in comparison with the previous methods.      
### 39.Spectral band selection for vegetation properties retrieval using Gaussian processes regression  [ :arrow_down: ](https://arxiv.org/pdf/2012.08640.pdf)
>  With current and upcoming imaging spectrometers, automated band analysis techniques are needed to enable efficient identification of most informative bands to facilitate optimized processing of spectral data into estimates of biophysical variables. This paper introduces an automated spectral band analysis tool (BAT) based on Gaussian processes regression (GPR) for the spectral analysis of vegetation properties. The GPR-BAT procedure sequentially backwards removes the least contributing band in the regression model for a given variable until only one band is kept. GPR-BAT is implemented within the framework of the free ARTMO's MLRA (machine learning regression algorithms) toolbox, which is dedicated to the transforming of optical remote sensing images into biophysical products. GPR-BAT allows (1) to identify the most informative bands in relating spectral data to a biophysical variable, and (2) to find the least number of bands that preserve optimized accurate predictions. This study concludes that a wise band selection of hyperspectral data is strictly required for optimal vegetation properties mapping.      
### 40.Post-Hurricane Damage Assessment Using Satellite Imagery and Geolocation Features  [ :arrow_down: ](https://arxiv.org/pdf/2012.08624.pdf)
>  Gaining timely and reliable situation awareness after hazard events such as a hurricane is crucial to emergency managers and first responders. One effective way to achieve that goal is through damage assessment. Recently, disaster researchers have been utilizing imagery captured through satellites or drones to quantify the number of flooded/damaged buildings. In this paper, we propose a mixed data approach, which leverages publicly available satellite imagery and geolocation features of the affected area to identify damaged buildings after a hurricane. The method demonstrated significant improvement from performing a similar task using only imagery features, based on a case study of Hurricane Harvey affecting Greater Houston area in 2017. This result opens door to a wide range of possibilities to unify the advancement in computer vision algorithms such as convolutional neural networks and traditional methods in damage assessment, for example, using flood depth or bare-earth topology. In this work, a creative choice of the geolocation features was made to provide extra information to the imagery features, but it is up to the users to decide which other features can be included to model the physical behavior of the events, depending on their domain knowledge and the type of disaster. The dataset curated in this work is made openly available (DOI: <a class="link-https link-external" data-doi="10.17603/ds2-3cca-f398" href="https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.17603%2Fds2-3cca-f398&amp;v=73eae366" rel="external noopener nofollow">10.17603/ds2-3cca-f398</a>).      
### 41.Lambda Inference Machine: accelerating computing by nonlinear evolution of spectrally modulated data  [ :arrow_down: ](https://arxiv.org/pdf/2012.08615.pdf)
>  We show that nonlinear spectral dynamics that underpin fascinating phenomena in nature can be leveraged to accelerate machine learning by transforming a difficult nonlinear classification problem into a simple linear separation task. This is done by seeding the nonlinear dynamics with the data that is to be classified and passing the output to a digital linear classifier. Data is first modulated onto the spectrum of a femtosecond pulse that is allowed to evolve through a nonlinear optical medium followed by a realtime spectrometer and the data acquisition. This unsupervised process projects the linearly non-separable data to a space where data become linearly separable. We validate this hypothesis by performing a nonlinear classification benchmark task followed by the classification of various datasets including brain intracranial pressure time series, time-stretch microscopy images, and spoken digit recognition. Since the data is first modulated onto the optical spectrum, this technique is fully compatible with the time stretch technique for ultrafast single-shot data acquisition and classification.      
### 42.A Laser Spiking Neuron in a Photonic Integrated Circuit  [ :arrow_down: ](https://arxiv.org/pdf/2012.08516.pdf)
>  There has been a recent surge of interest in the implementation of linear operations such as matrix multipications using photonic integrated circuit technology. However, these approaches require an efficient and flexible way to perform nonlinear operations in the photonic domain. We have fabricated an optoelectronic nonlinear device--a laser neuron--that uses excitable laser dynamics to achieve biologically-inspired spiking behavior. We demonstrate functionality with simultaneous excitation, inhibition, and summation across multiple wavelengths. We also demonstrate cascadability and compatibility with a wavelength multiplexing protocol, both essential for larger scale system integration. Laser neurons represent an important class of optoelectronic nonlinear processors that can complement both the enormous bandwidth density and energy efficiency of photonic computing operations.      
