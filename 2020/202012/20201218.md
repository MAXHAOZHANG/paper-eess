# ArXiv eess --Fri, 18 Dec 2020
### 1.Repairing dynamic models: a method to obtain identifiable and observable reparameterizations with mechanistic insights  [ :arrow_down: ](https://arxiv.org/pdf/2012.09826.pdf)
>  Mechanistic dynamic models allow for a quantitative and systematic interpretation of data and the generation of testable hypotheses. However, these models are often over-parameterized, leading to non-identifiability and non-observability, i.e. the impossibility of inferring their parameters and state variables. The lack of structural identifiability and observability (SIO) compromises a model's ability to make predictions and provide insight. Here we present a methodology, AutoRepar, that corrects SIO deficiencies automatically, yielding reparameterized models that are structurally identifiable and observable. The reparameterization preserves the mechanistic meaning of selected variables, and has the exact same dynamics and input-output mapping as the original model. We implement AutoRepar as an extension of the STRIKE-GOLDD software toolbox for SIO analysis, applying it to several models from the literature to demonstrate its ability to repair their structural deficiencies. AutoRepar increases the applicability of mechanistic models, enabling them to provide reliable information about their parameters and dynamics.      
### 2.Forward interval propagation through the Fourier discrete transform  [ :arrow_down: ](https://arxiv.org/pdf/2012.09778.pdf)
>  In this paper an algorithm for the forward interval propagation on the amplitude of the discrete Fourier transform (DFT) is presented. The algorithm yields best-possible bounds on the amplitude of the DFT for real and complex valued sequences. We show that computing the exact bounds for the amplitude of the DFT can be achieved with an exhaustive examination of all possible corners of the interval-shaped domain. However, because the number of corners increase exponentially (in base 2) with the number of intervals, such method is infeasible for large interval signals. We provide an algorithm that does not need such an exhaustive search, and show that the best possible bounds for the amplitude can be obtained propagating complex pairs only from the convex hull. Because the convex hull is always tightly inscribed in the respective rigorous bounding box resulting from interval arithmetic, we conclude that the obtained bounds are guaranteed to yield the true bounds.      
### 3.SAR Image Autofocusing using Wirtinger calculus and Cauchy regularization  [ :arrow_down: ](https://arxiv.org/pdf/2012.09772.pdf)
>  In this paper, an optimization model using Cauchy regularization is proposed for simultaneous SAR image reconstruction and autofocusing. A coordinate descent framework in which the desired image and the phase errors are optimized alternatively is designed to solve the model. For the subproblem of estimating the image, we utilize the techniques of Wirtinger calculus to directly minimize the cost function which involves complex variables. We also utilise a state-of-the-art, sparsity-enforcing Cauchy regularizer. The proposed method is demonstrated to give impressive autofocusing results by conducting experiments on both simulated scene and real SAR image.      
### 4.Describing the Structural Phenotype of the Glaucomatous Optic Nerve Head Using Artificial Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2012.09755.pdf)
>  The optic nerve head (ONH) typically experiences complex neural- and connective-tissue structural changes with the development and progression of glaucoma, and monitoring these changes could be critical for improved diagnosis and prognosis in the glaucoma clinic. The gold-standard technique to assess structural changes of the ONH clinically is optical coherence tomography (OCT). However, OCT is limited to the measurement of a few hand-engineered parameters, such as the thickness of the retinal nerve fiber layer (RNFL), and has not yet been qualified as a stand-alone device for glaucoma diagnosis and prognosis applications. We argue this is because the vast amount of information available in a 3D OCT scan of the ONH has not been fully exploited. In this study we propose a deep learning approach that can: \textbf{(1)} fully exploit information from an OCT scan of the ONH; \textbf{(2)} describe the structural phenotype of the glaucomatous ONH; and that can \textbf{(3)} be used as a robust glaucoma diagnosis tool. Specifically, the structural features identified by our algorithm were found to be related to clinical observations of glaucoma. The diagnostic accuracy from these structural features was $92.0 \pm 2.3 \%$ with a sensitivity of $90.0 \pm 2.4 \% $ (at $95 \%$ specificity). By changing their magnitudes in steps, we were able to reveal how the morphology of the ONH changes as one transitions from a `non-glaucoma' to a `glaucoma' condition. We believe our work may have strong clinical implication for our understanding of glaucoma pathogenesis, and could be improved in the future to also predict future loss of vision.      
### 5.Continuous Speech Separation Using Speaker Inventory for Long Multi-talker Recording  [ :arrow_down: ](https://arxiv.org/pdf/2012.09727.pdf)
>  Leveraging additional speaker information to facilitate speech separation has received increasing attention in recent years. Recent research includes extracting target speech by using the target speaker's voice snippet and jointly separating all participating speakers by using a pool of additional speaker signals, which is known as speech separation using speaker inventory (SSUSI). However, all these systems ideally assume that the pre-enrolled speaker signals are available and are only evaluated on simple data configurations. In realistic multi-talker conversations, the speech signal contains a large proportion of non-overlapped regions, where we can derive robust speaker embedding of individual talkers. In this work, we adopt the SSUSI model in long recordings and propose a self-informed, clustering-based inventory forming scheme for long recording, where the speaker inventory is fully built from the input signal without the need for external speaker signals. Experiment results on simulated noisy reverberant long recording datasets show that the proposed method can significantly improve the separation performance across various conditions.      
### 6.Parallel WaveNet conditioned on VAE latent vectors  [ :arrow_down: ](https://arxiv.org/pdf/2012.09703.pdf)
>  Recently the state-of-the-art text-to-speech synthesis systems have shifted to a two-model approach: a sequence-to-sequence model to predict a representation of speech (typically mel-spectrograms), followed by a 'neural vocoder' model which produces the time-domain speech waveform from this intermediate speech representation. This approach is capable of synthesizing speech that is confusable with natural speech recordings. However, the inference speed of neural vocoder approaches represents a major obstacle for deploying this technology for commercial applications. Parallel WaveNet is one approach which has been developed to address this issue, trading off some synthesis quality for significantly faster inference speed. In this paper we investigate the use of a sentence-level conditioning vector to improve the signal quality of a Parallel WaveNet neural vocoder. We condition the neural vocoder with the latent vector from a pre-trained VAE component of a Tacotron 2-style sequence-to-sequence model. With this, we are able to significantly improve the quality of vocoded speech.      
### 7.Calibration of Stochastic Radio Channel Models with Kernels  [ :arrow_down: ](https://arxiv.org/pdf/2012.09612.pdf)
>  Calibrating stochastic radio channel models to new measurement data is challenging when the likelihood function is intractable. The standard approach to this problem involves sophisticated algorithms for extraction and clustering of multipath components, following which, point estimates of the model parameters can be obtained using specialized estimators. We propose a likelihood-free calibration method using approximate Bayesian computation. The method is based on the maximum mean discrepancy, which is a notion of distance between probability distributions. Our method not only by-passes the need to implement any high-resolution or clustering algorithm, but is also automatic in that it does not require any additional input or manual pre-processing from the user. It also has the advantage of returning an entire posterior distribution on the value of the parameters, rather than a simple point estimate. We evaluate the performance of the proposed method by fitting two different stochastic channel models, namely the Saleh-Valenzuela model and the propagation graph model, to both simulated and measured data. The proposed method is able to estimate the parameters of both the models accurately in simulations, as well as when applied to 60 GHz indoor measurement data.      
### 8.Learned Block-based Hybrid Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2012.09550.pdf)
>  Learned image compression based on neural networks have made huge progress thanks to its superiority in learning better representation through non-linear transformation. Different from traditional hybrid coding frameworks, that are commonly block-based, existing learned image codecs usually process the images in a full-resolution manner thus not supporting acceleration via parallelism and explicit prediction. Compared to learned image codecs, traditional hybrid coding frameworks are in general hand-crafted and lack the adaptability of being optimized according to heterogeneous metrics. Therefore, in order to collect their good qualities and offset their weakness, we explore a learned block-based hybrid image compression (LBHIC) framework, which achieves a win-win between coding performance and efficiency. Specifically, we introduce block partition and explicit learned predictive coding into learned image compression framework. Compared to prediction through linear weighting of neighbor pixels in traditional codecs, our contextual prediction module (CPM) is designed to better capture long-range correlations by utilizing the strip pooling to extract the most relevant information in neighboring latent space. Moreover, to alleviate blocking artifacts, we further propose a boundary-aware post-processing module (BPM) with the importance of edge taken into account. Extensive experiments demonstrate that the proposed LBHIC codec outperforms state-of-the-art image compression methods in terms of both PSNR and MS-SSIM metrics and promises obvious time-saving.      
### 9.Denoising Text to Speech with Frame-Level Noise Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2012.09547.pdf)
>  While neural-based text to speech (TTS) models can synthesize natural and intelligible voice, they usually require high-quality speech data, which is costly to collect. In many scenarios, only noisy speech of a target speaker is available, which presents challenges for TTS model training for this speaker. Previous works usually address the challenge using two methods: 1) training the TTS model using the speech denoised with an enhancement model; 2) taking a single noise embedding as input when training with noisy speech. However, they usually cannot handle speech with real-world complicated noise such as those with high variations along time. In this paper, we develop DenoiSpeech, a TTS system that can synthesize clean speech for a speaker with noisy speech data. In DenoiSpeech, we handle real-world noisy speech by modeling the fine-grained frame-level noise with a noise condition module, which is jointly trained with the TTS model. Experimental results on real-world data show that DenoiSpeech outperforms the previous two methods by 0.31 and 0.66 MOS respectively.      
### 10.The effectiveness of unsupervised subword modeling with autoregressive and cross-lingual phone-aware networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.09544.pdf)
>  This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding (APC) as the front-end and a cross-lingual deep neural network (DNN) as the back-end. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies. Comprehensive and systematic analyses at the phoneme- and articulatory feature (AF)-level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information. Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information.      
### 11.Towards Optimal District Heating Temperature Control in China with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.09508.pdf)
>  Achieving efficiency gains in Chinese district heating networks, thereby reducing their carbon footprint, requires new optimal control methods going beyond current industry tools. Focusing on the secondary network, we propose a data-driven deep reinforcement learning (DRL) approach to address this task. We build a recurrent neural network, trained on simulated data, to predict the indoor temperatures. This model is then used to train two DRL agents, with or without expert guidance, for the optimal control of the supply water temperature. Our tests in a multi-apartment setting show that both agents can ensure a higher thermal comfort and at the same time a smaller energy cost, compared to an optimized baseline strategy.      
### 12.Low-Complexity Steered Response Power Mapping based on Nyquist-Shannon Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2012.09499.pdf)
>  The steered response power (SRP) approach to acoustic source localization computes a map of the acoustic scene from the output power of a beamformer steered towards a set of candidate locations. Equivalently, SRP may be expressed in terms of generalized cross-correlations (GCCs) at lags equal to the candidate locations' time-differences of arrival (TDOAs). Due to the dense grid of candidate locations, however, conventional SRP exhibits a high computational complexity, limiting its practical feasibility. In this paper, we propose a low-complexity SRP approach based on Nyquist-Shannon sampling theory. Noting that on the one hand the range of possible TDOAs is physically bounded, while on the other hand the GCCs are bandlimited, we critically sample the GCCs around their TDOA intervals and interpolate, thereby approximating the SRP map. In usual setups, the total number of sample points can be several orders of magnitude less than the number of candidate locations, yielding a significant complexity reduction. Simulations comparing the proposed approximation and conventional SRP indicate low approximation errors and equal localization performance. A MATLAB implementation is available online.      
### 13.Revisit 1D Total Variation restoration problem with new real-time algorithms for signal and hyper-parameter estimations  [ :arrow_down: ](https://arxiv.org/pdf/2012.09481.pdf)
>  1D Total Variation (TV) denoising, considering the data fidelity and the Total Variation (TV) regularization, proposes a good restored signal preserving shape edges. The main issue is how to choose the weight $\lambda$ balancing those two terms. In practice, this parameter is selected by assessing a list of candidates (e.g. cross validation), which is inappropriate for the real time application. In this work, we revisit 1D Total Variation restoration algorithm proposed by Tibshirani and Taylor. A heuristic method is integrated for estimating a good choice of $\lambda$ based on the extremums number of restored signal. We propose an offline version of restoration algorithm in O(n log n) as well as its online implementation in O(n). Combining the rapid algorithm and the automatic choice of $\lambda$, we propose a real-time automatic denoising algorithm, providing a large application fields. The simulations show that our proposition of $\lambda$ has a similar performance as the states of the art.      
### 14.A new semi-supervised self-training method for lung cancer prediction  [ :arrow_down: ](https://arxiv.org/pdf/2012.09472.pdf)
>  Background and Objective: Early detection of lung cancer is crucial as it has high mortality rate with patients commonly present with the disease at stage 3 and above. There are only relatively few methods that simultaneously detect and classify nodules from computed tomography (CT) scans. Furthermore, very few studies have used semi-supervised learning for lung cancer prediction. This study presents a complete end-to-end scheme to detect and classify lung nodules using the state-of-the-art Self-training with Noisy Student method on a comprehensive CT lung screening dataset of around 4,000 CT scans. <br>Methods: We used three datasets, namely LUNA16, LIDC and NLST, for this study. We first utilise a three-dimensional deep convolutional neural network model to detect lung nodules in the detection stage. The classification model known as Maxout Local-Global Network uses non-local networks to detect global features including shape features, residual blocks to detect local features including nodule texture, and a Maxout layer to detect nodule variations. We trained the first Self-training with Noisy Student model to predict lung cancer on the unlabelled NLST datasets. Then, we performed Mixup regularization to enhance our scheme and provide robustness to erroneous labels. <br>Results and Conclusions: Our new Mixup Maxout Local-Global network achieves an AUC of 0.87 on 2,005 completely independent testing scans from the NLST dataset. Our new scheme significantly outperformed the next highest performing method at the 5% significance level using DeLong's test (p = 0.0001). This study presents a new complete end-to-end scheme to predict lung cancer using Self-training with Noisy Student combined with Mixup regularization. On a completely independent dataset of 2,005 scans, we achieved state-of-the-art performance even with more images as compared to other methods.      
### 15.Uncertainty Quantification in Case of Imperfect Models: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.09449.pdf)
>  Uncertainty quantification of complex technical systems is often based on a computer model of the system. As all models such a computer model is always wrong in the sense that it does not describe the reality perfectly. The purpose of this article is to give a review of techniques which use observed values of the technical systems in order to take into account the inadequacy of a computer model in uncertainty quantification. The techniques reviewed in this article are illustrated and compared by applying them to applications in mechanical engineering.      
### 16.On-board Electrical, Electronics and Pose Estimation System for Hyperloop Pod Design  [ :arrow_down: ](https://arxiv.org/pdf/2012.09412.pdf)
>  Hyperloop is a high-speed ground-based transportation system utilizing sealed tubes, with the aim of ultimately transporting passengers between metropolitan cities in efficiently designed autonomous capsules. In recent years, the design and development of sub-scale prototypes for these Hyperloop pods has set the foundation for realizing more practical and scalable pod architectures. This paper proposes a practical, power and space optimized on-board electronics architecture, coupled with an end-to-end computationally efficient pose estimation algorithm. Considering the high energy density and discharge rate of on-board batteries, this work additionally presents a robust system for fault detection, protection and management of batteries, along with the design of the surrounding electrical system. Performance evaluation and verification of proposed algorithms and circuits has been carried out by software simulations using both Python and Simulink.      
### 17.Robust Phase Retrieval with Green Noise Binary Masks  [ :arrow_down: ](https://arxiv.org/pdf/2012.09410.pdf)
>  Phase retrieval with pre-defined optical masks can provide extra constraint and thus achieve improved performance. The recent progress in optimization theory demonstrates the superiority of random masks in phase retrieval algorithms. However, traditional approaches just focus on the randomness of the masks but ignore their non-bandlimited nature. When using these masks in the reconstruction process for phase retrieval, the high frequency part of the masks is often removed in the process and thus leads to degraded performance. Based on the concept of digital halftoning, this paper proposes a green noise binary masking scheme which can greatly reduce the high frequency content of the masks while fulfilling the randomness requirement. The experimental results show that the proposed green noise binary masking scheme outperform the traditional ones when using in a DMD-based coded diffraction pattern phase retrieval system.      
### 18.Interactive Speech and Noise Modeling for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2012.09408.pdf)
>  Speech enhancement is challenging because of the diversity of background noise types. Most of the existing methods are focused on modelling the speech rather than the noise. In this paper, we propose a novel idea to model speech and noise simultaneously in a two-branch convolutional neural network, namely SN-Net. In SN-Net, the two branches predict speech and noise, respectively. Instead of information fusion only at the final output layer, interaction modules are introduced at several intermediate feature domains between the two branches to benefit each other. Such an interaction can leverage features learned from one branch to counteract the undesired part and restore the missing component of the other and thus enhance their discrimination capabilities. We also design a feature extraction module, namely residual-convolution-and-attention (RA), to capture the correlations along temporal and frequency dimensions for both the speech and the noises. Evaluations on public datasets show that the interaction module plays a key role in simultaneous modeling and the SN-Net outperforms the state-of-the-art by a large margin on various evaluation metrics. The proposed SN-Net also shows superior performance for speaker separation.      
### 19.Speech Enhancement with Zero-Shot Model Selection  [ :arrow_down: ](https://arxiv.org/pdf/2012.09359.pdf)
>  Recent research on speech enhancement (SE) has seen the emergence of deep learning-based methods. It is still a challenging task to determine effective ways to increase the generalizability of SE under diverse test conditions. In this paper, we combine zero-shot learning and ensemble learning to propose a zero-shot model selection (ZMOS) approach to increase the generalization of SE performance. The proposed approach is realized in two phases, namely offline and online phases. The offline phase clusters the entire set of training data into multiple subsets, and trains a specialized SE model (termed component SE model) with each subset. The online phase selects the most suitable component SE model to carry out enhancement. Two selection strategies are developed: selection based on quality score (QS) and selection based on quality embedding (QE). Both QS and QE are obtained by a Quality-Net, a non-intrusive quality assessment network. In the offline phase, the QS or QE of a train-ing utterance is used to group the training data into clusters. In the online phase, the QS or QE of the test utterance is used to identify the appropriate component SE model to perform enhancement on the test utterance. Experimental results have confirmed that the proposed ZMOS approach can achieve better performance in both seen and unseen noise types compared to the baseline systems, which indicates the effectiveness of the proposed approach to provide robust SE performance.      
### 20.Dynamic Modeling and Real-time Management of a System of EV Fast-charging Stations  [ :arrow_down: ](https://arxiv.org/pdf/2012.09349.pdf)
>  Demand for electric vehicles (EVs), and thus EV charging, has steadily increased over the last decade. However, there is limited fast-charging infrastructure in most parts of the world to support EV travel, especially long-distance trips. The goal of this study is to develop a stochastic dynamic simulation modeling framework of a regional system of EV fast-charging stations for real-time management and strategic planning (i.e., capacity allocation) purposes. To model EV user behavior, specifically fast-charging station choices, the framework incorporates a multinomial logit station choice model that considers charging prices, expected wait times, and detour distances. To capture the dynamics of supply and demand at each fast-charging station, the framework incorporates a multi-server queueing model in the simulation. The study assumes that multiple fast-charging stations are managed by a single entity and that the demand for these stations are interrelated. To manage the system of stations, the study proposes and tests dynamic demand-responsive price adjustment (DDRPA) schemes based on station queue lengths. The study applies the modeling framework to a system of EV fast-charging stations in Southern California. The results indicate that DDRPA strategies are an effective mechanism to balance charging demand across fast-charging stations. Specifically, compared to the no DDRPA scheme case, the quadratic DDRPA scheme reduces average wait time by 26%, increases charging station revenue (and user costs) by 5.8%, while, most importantly, increasing social welfare by 2.7% in the base scenario. Moreover, the study also illustrates that the modeling framework can evaluate the allocation of EV fast-charging station capacity, to identify stations that require additional chargers and areas that would benefit from additional fast-charging stations.      
### 21.Securing Communications with Friendly Unmanned Aerial Vehicle Jammers  [ :arrow_down: ](https://arxiv.org/pdf/2012.09347.pdf)
>  In this paper, we analyze the impact of a friendly unmanned aerial vehicle (UAV) jammer on UAV communications in the presence of multiple eavesdroppers. We first present channel components determined by the line-of-sight (LoS) probability between the friendly UAV jammer and the ground device, and introduce different channel fadings for LoS and non-line-of-sight (NLoS) links. We then derive the secrecy transmission probability satisfying both constraints of legitimate and wiretap channels. We also analyze the secrecy transmission probability in the presence of randomly distributed multiple friendly UAV jammers. Finally, we show the existence of the optimal UAV jammer location, and the impact of the density of eavesdroppers, the transmission power of the UAV jammer, and the density of UAV jammers on the optimal location.      
### 22.Generation of bounded invariants via stroboscopic set-valued maps: Application to the stability analysis of parametric time-periodic systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.09310.pdf)
>  A method is given for generating a bounded invariant of a differential system with a given set of initial conditions around a point $x_0$. This invariant has the form of a tube centered on the Euler approximate solution starting at $x_0$, which has for radius an upper bound on the distance between the approximate solution and the exact ones. The method consists in finding a real $T&gt;0$ such that the "snapshot" of the tube at time $t=(i+1)T$ is included in the snapshot at $t=iT$, for some integer $i$. In the phase space, the invariant is therefore in the shape of a torus. A simple additional condition is also given to ensure that the solutions of the system can never converge to a point of equilibrium. In dimension 2, this ensures that all solutions converge towards a limit cycle. The method is extended in case the dynamic system contains a parameter $p$, thus allowing the stability analysis of the system for a range of values of $p$. This is illustrated on classical Van der Pol's system.      
### 23.Imitation Learning with Stability and Safety Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2012.09293.pdf)
>  A method is presented to synthesize neural network (NN) controllers with stability and safety guarantees through imitation learning. Convex stability and safety conditions are derived for linear time-invariant plant dynamics with NN controllers. The proposed approach merges Lyapunov theory with local quadratic constraints to bound the nonlinear activation functions in the NN. The safe imitation learning problem is formulated as an optimization problem with the goal of minimizing the imitation learning loss, and maximizing volume of the region of attraction associated with the NN controller, while enforcing the stability and safety conditions. An alternating direction method of multipliers based algorithm is proposed to solve the optimization. The method is illustrated on an inverted pendulum system and aircraft longitudinal dynamics.      
### 24.Spatial Context-Aware Self-Attention Model For Multi-Organ Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.09279.pdf)
>  Multi-organ segmentation is one of most successful applications of deep learning in medical image analysis. Deep convolutional neural nets (CNNs) have shown great promise in achieving clinically applicable image segmentation performance on CT or MRI images. State-of-the-art CNN segmentation models apply either 2D or 3D convolutions on input images, with pros and cons associated with each method: 2D convolution is fast, less memory-intensive but inadequate for extracting 3D contextual information from volumetric images, while the opposite is true for 3D convolution. To fit a 3D CNN model on CT or MRI images on commodity GPUs, one usually has to either downsample input images or use cropped local regions as inputs, which limits the utility of 3D models for multi-organ segmentation. In this work, we propose a new framework for combining 3D and 2D models, in which the segmentation is realized through high-resolution 2D convolutions, but guided by spatial contextual information extracted from a low-resolution 3D model. We implement a self-attention mechanism to control which 3D features should be used to guide 2D segmentation. Our model is light on memory usage but fully equipped to take 3D contextual information into account. Experiments on multiple organ segmentation datasets demonstrate that by taking advantage of both 2D and 3D models, our method is consistently outperforms existing 2D and 3D models in organ segmentation accuracy, while being able to directly take raw whole-volume image data as inputs.      
### 25.Reduction in the complexity of 1D 1H-NMR spectra by the use of Frequency to Information Transformation  [ :arrow_down: ](https://arxiv.org/pdf/2012.09267.pdf)
>  Analysis of 1H-NMR spectra is often hindered by large variations that occur during the collection of these spectra. Large solvent and standard peaks, base line drift and negative peaks (due to improper phasing) are among some of these variations. Furthermore, some instrument dependent alterations, such as incorrect shimming, are also embedded in the recorded spectrum. The unpredictable nature of these alterations of the signal has rendered the automated and instrument independent computer analysis of these spectra unreliable. In this paper, a novel method of extracting the information content of a signal (in this paper, frequency domain 1H-NMR spectrum), called the frequency-information transformation (FIT), is presented and compared to a previously used method (SPUTNIK). FIT can successfully extract the relevant information to a pattern matching task present in a signal, while discarding the remainder of a signal by transforming a Fourier transformed signal into an information spectrum (IS). This technique exhibits the ability of decreasing the inter-class correlation coefficients while increasing the intra-class correlation coefficients. Different spectra of the same molecule, in other words, will resemble more to each other while the spectra of different molecules will look more different from each other. This feature allows easier automated identification and analysis of molecules based on their spectral signatures using computer algorithms.      
### 26.Transfer Learning Through Weighted Loss Function and Group Normalization for Vessel Segmentation from Retinal Images  [ :arrow_down: ](https://arxiv.org/pdf/2012.09250.pdf)
>  The vascular structure of blood vessels is important in diagnosing retinal conditions such as glaucoma and diabetic retinopathy. Accurate segmentation of these vessels can help in detecting retinal objects such as the optic disc and optic cup and hence determine if there are damages to these areas. Moreover, the structure of the vessels can help in diagnosing glaucoma. The rapid development of digital imaging and computer-vision techniques has increased the potential for developing approaches for segmenting retinal vessels. In this paper, we propose an approach for segmenting retinal vessels that uses deep learning along with transfer learning. We adapted the U-Net structure to use a customized InceptionV3 as the encoder and used multiple skip connections to form the decoder. Moreover, we used a weighted loss function to handle the issue of class imbalance in retinal images. Furthermore, we contributed a new dataset to this field. We tested our approach on six publicly available datasets and a newly created dataset. We achieved an average accuracy of 95.60% and a Dice coefficient of 80.98%. The results obtained from comprehensive experiments demonstrate the robustness of our approach to the segmentation of blood vessels in retinal images obtained from different sources. Our approach results in greater segmentation accuracy than other approaches.      
### 27.Frequency Response of Transmission Lines with Unevenly Distributed Properties with Application to Railway Safety Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2012.09247.pdf)
>  This paper proposes a method to quickly and efficiently compute the voltage and current along a transmission line which can be "damaged"; that is its electrical properties can be unevenly distributed. The method approximates a transmission line by a self-similar circuit network and leverages our previous work regarding the frequency response for that class of networks. The main motivation arises from the research for railway track circuit systems where transmission line models are often employed. However, in contrast to real transmission lines, railway track circuits are more likely to be damaged due to its scale and environmental uncertainties; furthermore, changes in circuit properties due to a train occupying a segment of the track also is of great interest as a means to ensure safety. As a result, an accurate and quick simulation of damaged track circuits is necessary and can contribute to the corresponding health monitoring research area in the future.      
### 28.Damage Identification for The Tree-like Network through Frequency-domain Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2012.09234.pdf)
>  In this paper, we propose a method to identify the damaged component and quantify its damage amount in a large network given its overall frequency response. The identification procedure takes advantage of our previous work which exactly models the frequency response of that large network when it is damaged. As a result, the test shows that our method works well when some noise present in the frequency response measurement. In addition, the effects brought by a damaged component which is located deep inside that large network are also discussed.      
### 29.Damage Modeling for the Tree-Like Network with Fractional-Order Calculus  [ :arrow_down: ](https://arxiv.org/pdf/2012.09212.pdf)
>  In this paper, we propose that a tree-like network with damage can be modeled as the product of a fractional-order nominal plant and a fractional-order multiplicative disturbance, which is well structured and completely characterized by the damage amount at each damaged component. Such way of modeling brings us insights about that damaged network's behavior, helps us design robust controllers under uncertain damages and identify the damage. Although the main result in this paper is specialized to one model, we believe that this way of constructing a well-structured disturbance model can be applied to a class of damaged networks.      
### 30.Deep Learning Techniques for Super-Resolution in Video Games  [ :arrow_down: ](https://arxiv.org/pdf/2012.09810.pdf)
>  The computational cost of video game graphics is increasing and hardware for processing graphics is struggling to keep up. This means that computer scientists need to develop creative new ways to improve the performance of graphical processing hardware. Deep learning techniques for video super-resolution can enable video games to have high quality graphics whilst offsetting much of the computational cost. These emerging technologies allow consumers to have improved performance and enjoyment from video games and have the potential to become standard within the game development industry.      
### 31.Use of Bayesian Nonparametric methods for Estimating the Measurements in High Clutter  [ :arrow_down: ](https://arxiv.org/pdf/2012.09785.pdf)
>  Robust tracking of a target in a clutter environment is an important and challenging task. In recent years, the nearest neighbor methods and probabilistic data association filters were proposed. However, the performance of these methods diminishes as the number of measurements increases. In this paper, we propose a robust generative approach to effectively model multiple sensor measurements for tracking a moving target in an environment with high clutter. We assume a time-dependent number of measurements that include sensor observations with unknown origin, some of which may only contain clutter with no additional information. We robustly and accurately estimate the trajectory of the moving target in a high clutter environment with an unknown number of clutters by employing Bayesian nonparametric modeling. In particular, we employ a class of joint Bayesian nonparametric models to construct the joint prior distribution of target and clutter measurements such that the conditional distributions follow a Dirichlet process. The marginalized Dirichlet process prior of the target measurements is then used in a Bayesian tracker to estimate the dynamically-varying target state. We show through experiments that the tracking performance and effectiveness of our proposed framework are increased by suppressing high clutter measurements. In addition, we show that our proposed method outperforms existing methods such as nearest neighbor and probability data association filters.      
### 32.Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL  [ :arrow_down: ](https://arxiv.org/pdf/2012.09737.pdf)
>  Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems. Code is released in <a class="link-external link-https" href="https://github.com/MathPhysSim/FERMI_RL_Paper" rel="external noopener nofollow">this https URL</a>.      
### 33.App-based saccade latency and error determination across the adult age spectrum  [ :arrow_down: ](https://arxiv.org/pdf/2012.09723.pdf)
>  We aid in neurocognitive monitoring outside the hospital environment by enabling app-based measurements of visual reaction time (saccade latency) and error rate in a cohort of subjects spanning the adult age spectrum. Methods: We developed an iOS app to record subjects with the frontal camera during pro- and anti-saccade tasks. We further developed automated algorithms for measuring saccade latency and error rate that take into account the possibility that it might not always be possible to determine the eye movement from app-based recordings. Results: To measure saccade latency on a tablet, we ensured that the absolute timing error between on-screen task presentation and the camera recording is within 5 ms. We collected over 235,000 eye movements in 80 subjects ranging in age from 20 to 92 years, with 96% of recorded eye movements either declared good or directional errors. Our error detection code achieved a sensitivity of 0.97 and a specificity of 0.97. Confirming prior reports, we observed a positive correlation between saccade latency and age while the relationship between error rate and age was not significant. Finally, we observed significant intra- and inter-subject variations in saccade latency and error rate distributions, which highlights the importance of individualized tracking of these visual digital biomarkers. Conclusion and Significance: Our system and algorithms allow ubiquitous tracking of saccade latency and error rate, which opens up the possibility of quantifying patient state on a finer timescale in a broader population than previously possible.      
### 34.Analytical and fast Fiber Orientation Distribution reconstruction in 3D-Polarized Light Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2012.09655.pdf)
>  Three dimensional Polarized Light Imaging (3D-PLI) is an optical technique which allows mapping the spatial fiber architecture of fibrous postmortem tissues, at sub-millimeter resolutions. Here, we propose an analytical and fast approach to compute the fiber orientation distribution (FOD) from high-resolution vector data provided by 3D-PLI. The FOD is modeled as a sum of K orientations/Diracs on the unit sphere, described on a spherical harmonics basis and analytically computed using the spherical Fourier transform. Experiments are performed on rich synthetic data which simulate the geometry of the neuronal fibers and on human brain data. Results indicate the analytical FOD is computationally efficient and very fast, and has high angular precision and angular resolution. Furthermore, investigations on the right occipital lobe illustrate that our strategy of FOD computation enables the bridging of spatial scales from microscopic 3D-PLI information to macro- or mesoscopic dimensions of diffusion Magnetic Resonance Imaging (MRI), while being a means to evaluate prospective resolution limits for diffusion MRI to reconstruct region-specific white matter tracts. These results demonstrate the interest and great potential of our analytical approach.      
### 35.Detection and Prediction of Nutrient Deficiency Stress using Longitudinal Aerial Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2012.09654.pdf)
>  Early, precise detection of nutrient deficiency stress (NDS) has key economic as well as environmental impact; precision application of chemicals in place of blanket application reduces operational costs for the growers while reducing the amount of chemicals which may enter the environment unnecessarily. Furthermore, earlier treatment reduces the amount of loss and therefore boosts crop production during a given season. With this in mind, we collect sequences of high-resolution aerial imagery and construct semantic segmentation models to detect and predict NDS across the field. Our work sits at the intersection of agriculture, remote sensing, and modern computer vision and deep learning. First, we establish a baseline for full-field detection of NDS and quantify the impact of pretraining, backbone architecture, input representation, and sampling strategy. We then quantify the amount of information available at different points in the season by building a single-timestamp model based on a UNet. Next, we construct our proposed spatiotemporal architecture, which combines a UNet with a convolutional LSTM layer, to accurately detect regions of the field showing NDS; this approach has an impressive IOU score of 0.53. Finally, we show that this architecture can be trained to predict regions of the field which are expected to show NDS in a later flight -- potentially more than three weeks in the future -- maintaining an IOU score of 0.47-0.51 depending on how far in advance the prediction is made. We will also release a dataset which we believe will benefit the computer vision, remote sensing, as well as agriculture fields. This work contributes to the recent developments in deep learning for remote sensing and agriculture, while addressing a key social challenge with implications for economics and sustainability.      
### 36.Automatic source localization and spectra generation from deconvolved beamforming maps  [ :arrow_down: ](https://arxiv.org/pdf/2012.09643.pdf)
>  We present two methods for the automated detection of aeroacoustic source positions in deconvolved beamforming maps and the extraction of their corresponding spectra. We evaluate these methods on two scaled airframe half-model wind-tunnel measurements. The first relies on the spatial normal distribution of aeroacoustic broadband sources in CLEAN-SC maps. The second uses hierarchical clustering methods. Both methods predict a spatial probability estimation based on which aeroacoustic spectra are generated.      
### 37.Learning to Solve AC Optimal Power Flow by Differentiating through Holomorphic Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2012.09622.pdf)
>  Alternating current optimal power flow (AC-OPF) is one of the fundamental problems in power systems operation. AC-OPF is traditionally cast as a constrained optimization problem that seeks optimal generation set points whilst fulfilling a set of non-linear equality constraints -- the power flow equations. With increasing penetration of renewable generation, grid operators need to solve larger problems at shorter intervals. This motivates the research interest in learning OPF solutions with neural networks, which have fast inference time and is potentially scalable to large networks. The main difficulty in solving the AC-OPF problem lies in dealing with this equality constraint that has spurious roots, i.e. there are assignments of voltages that fulfill the power flow equations that however are not physically realizable. This property renders any method relying on projected-gradients brittle because these non-physical roots can act as attractors. In this paper, we show efficient strategies that circumvent this problem by differentiating through the operations of a power flow solver that embeds the power flow equations into a holomorphic function. The resulting learning-based approach is validated experimentally on a 200-bus system and we show that, after training, the learned agent produces optimized power flow solutions reliably and fast. Specifically, we report a 12x increase in speed and a 40% increase in robustness compared to a traditional solver. To the best of our knowledge, this approach constitutes the first learning-based approach that successfully respects the full non-linear AC-OPF equations.      
### 38.The voice of COVID-19: Acoustic correlates of infection  [ :arrow_down: ](https://arxiv.org/pdf/2012.09478.pdf)
>  COVID-19 is a global health crisis that has been affecting many aspects of our daily lives throughout the past year. The symptomatology of COVID-19 is heterogeneous with a severity continuum. A considerable proportion of symptoms are related to pathological changes in the vocal system, leading to the assumption that COVID-19 may also affect voice production. For the very first time, the present study aims to investigate voice acoustic correlates of an infection with COVID-19 on the basis of a comprehensive acoustic parameter set. We compare 88 acoustic features extracted from recordings of the vowels /i:/, /e:/, /o:/, /u:/, and /a:/ produced by 11 symptomatic COVID-19 positive and 11 COVID-19 negative German-speaking participants. We employ the Mann-Whitney U test and calculate effect sizes to identify features with the most prominent group differences. The mean voiced segment length and the number of voiced segments per second yield the most important differences across all vowels indicating discontinuities in the pulmonic airstream during phonation in COVID-19 positive participants. Group differences in the front vowels /i:/ and /e:/ are additionally reflected in the variation of the fundamental frequency and the harmonics-to-noise ratio, group differences in back vowels /o:/ and /u:/ in statistics of the Mel-frequency cepstral coefficients and the spectral slope. Findings of this study can be considered an important proof-of-concept contribution for a potential future voice-based identification of individuals infected with COVID-19.      
### 39.cif-based collaborative decoding for end-to-end contextual speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2012.09466.pdf)
>  End-to-end (E2E) models have achieved promising results on multiple speech recognition benchmarks, and shown the potential to become the mainstream. However, the unified structure and the E2E training hamper injecting contextual information into them for contextual biasing. Though contextual LAS (CLAS) gives an excellent all-neural solution, the degree of biasing to given context information is not explicitly controllable. In this paper, we focus on incorporating context information into the continuous integrate-and-fire (CIF) based model that supports contextual biasing in a more controllable fashion. Specifically, an extra context processing network is introduced to extract contextual embeddings, integrate acoustically relevant context information and decode the contextual output distribution, thus forming a collaborative decoding with the decoder of the CIF-based model. Evaluated on the named entity rich evaluation sets of HKUST/AISHELL-2, our method brings relative character error rate (CER) reduction of 8.83%/21.13% and relative named entity character error rate (NE-CER) reduction of 40.14%/51.50% when compared with a strong baseline. Besides, it keeps the performance on original evaluation set without degradation.      
### 40.A Contrast Synthesized Thalamic Nuclei Segmentation Scheme using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.09386.pdf)
>  Thalamic nuclei have been implicated in several neurological diseases. WMn-MPRAGE images have been shown to provide better intra-thalamic nuclear contrast compared to conventional MPRAGE images but the additional acquisition results in increased examination times. In this work, we investigated 3D Convolutional Neural Network (CNN) based techniques for thalamic nuclei parcellation from conventional MPRAGE images. Two 3D CNNs were developed and compared for thalamic nuclei parcellation using MPRAGE images: a) a native contrast segmentation (NCS) and b) a synthesized contrast segmentation (SCS) using WMn-MPRAGE images synthesized from MPRAGE images. We trained the two segmentation frameworks using MPRAGE images (n=35) and thalamic nuclei labels generated on WMn-MPRAGE images using a multi-atlas based parcellation technique. The segmentation accuracy and clinical utility were evaluated on a cohort comprising of healthy subjects and patients with alcohol use disorder (AUD) (n=45). The SCS network yielded higher Dice scores in the Medial geniculate nucleus (P=.003) and Centromedian nucleus (P=.01) with lower volume differences for Ventral anterior (P=.001) and Ventral posterior lateral (P=.01) nuclei when compared to the NCS network. A Bland-Altman analysis revealed tighter limits of agreement with lower coefficient of variation between true volumes and those predicted by the SCS network. The SCS network demonstrated a significant atrophy in Ventral lateral posterior nucleus in AUD patients compared to healthy age-matched controls (P=0.01), agreeing with previous studies on thalamic atrophy in alcoholism, whereas the NCS network showed spurious atrophy of the Ventral posterior lateral nucleus. CNN-based contrast synthesis prior to segmentation can provide fast and accurate thalamic nuclei segmentation from conventional MPRAGE images.      
### 41.Polyblur: Removing mild blur by polynomial reblurring  [ :arrow_down: ](https://arxiv.org/pdf/2012.09322.pdf)
>  We present a highly efficient blind restoration method to remove mild blur in natural images. Contrary to the mainstream, we focus on removing slight blur that is often present, damaging image quality and commonly generated by small out-of-focus, lens blur, or slight camera motion. The proposed algorithm first estimates image blur and then compensates for it by combining multiple applications of the estimated blur in a principled way. To estimate blur we introduce a simple yet robust algorithm based on empirical observations about the distribution of the gradient in sharp natural images. Our experiments show that, in the context of mild blur, the proposed method outperforms traditional and modern blind deblurring methods and runs in a fraction of the time. Our method can be used to blindly correct blur before applying off-the-shelf deep super-resolution methods leading to superior results than other highly complex and computationally demanding techniques. The proposed method estimates and removes mild blur from a 12MP image on a modern mobile phone in a fraction of a second.      
### 42.Projected Distribution Loss for Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2012.09289.pdf)
>  Features obtained from object recognition CNNs have been widely used for measuring perceptual similarities between images. Such differentiable metrics can be used as perceptual learning losses to train image enhancement models. However, the choice of the distance function between input and target features may have a consequential impact on the performance of the trained model. While using the norm of the difference between extracted features leads to limited hallucination of details, measuring the distance between distributions of features may generate more textures; yet also more unrealistic details and artifacts. In this paper, we demonstrate that aggregating 1D-Wasserstein distances between CNN activations is more reliable than the existing approaches, and it can significantly improve the perceptual performance of enhancement models. More explicitly, we show that in imaging applications such as denoising, super-resolution, demosaicing, deblurring and JPEG artifact removal, the proposed learning loss outperforms the current state-of-the-art on reference-based perceptual losses. This means that the proposed learning loss can be plugged into different imaging frameworks and produce perceptually realistic results.      
### 43.Sparse Signal Models for Data Augmentation in Deep Learning ATR  [ :arrow_down: ](https://arxiv.org/pdf/2012.09284.pdf)
>  Automatic Target Recognition (ATR) algorithms classify a given Synthetic Aperture Radar (SAR) image into one of the known target classes using a set of training images available for each class. Recently, learning methods have shown to achieve state-of-the-art classification accuracy if abundant training data is available, sampled uniformly over the classes, and their poses. In this paper, we consider the task of ATR with a limited set of training images. We propose a data augmentation approach to incorporate domain knowledge and improve the generalization power of a data-intensive learning algorithm, such as a Convolutional neural network (CNN). The proposed data augmentation method employs a limited persistence sparse modeling approach, capitalizing on commonly observed characteristics of wide-angle synthetic aperture radar (SAR) imagery. Specifically, we exploit the sparsity of the scattering centers in the spatial domain and the smoothly-varying structure of the scattering coefficients in the azimuthal domain to solve the ill-posed problem of over-parametrized model fitting. Using this estimated model, we synthesize new images at poses and sub-pixel translations not available in the given data to augment CNN's training data. The experimental results show that for the training data starved region, the proposed method provides a significant gain in the resulting ATR algorithm's generalization performance.      
### 44.IEEE 802.11be: Wi-Fi 7 Strikes Back  [ :arrow_down: ](https://arxiv.org/pdf/2008.02815.pdf)
>  As hordes of data-hungry devices challenge its current capabilities, Wi-Fi strikes back with 802.11be, alias Wi-Fi 7. This brand-new amendment promises a (r)evolution of unlicensed wireless connectivity as we know it. With its standardisation process being consolidated, we provide an updated digest of 802.11be essential features, vouching for multi-AP coordination as a must-have for critical and latency-sensitive applications. We then get down to the nitty-gritty of one of its most enticing implementations-coordinated beamforming-, for which our standard-compliant simulations confirm near-tenfold reductions in worst-case delays.      
