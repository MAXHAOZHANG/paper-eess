# ArXiv eess --Fri, 11 Sep 2020
### 1.Proximity Sensing for Contact Tracing  [ :arrow_down: ](https://arxiv.org/pdf/2009.04991.pdf)
>  The TC4TL (Too Close For Too Long) challenge is aimed towards designing an effective proximity sensing algorithm that can accurately provide exposure notifications. In this paper, we describe our approach to model sensor and other device-level data to estimate the distance between two phones. We also present our research and data analysis on the TC4TL challenge and discuss various limitations associated with the task, and the dataset used for this purpose.      
### 2.Unsupervised Domain Adaptation via CycleGAN for White Matter Hyperintensity Segmentation in Multicenter MR Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.04985.pdf)
>  Automatic segmentation of white matter hyperintensities in magnetic resonance images is of paramount clinical and research importance. Quantification of these lesions serve as a predictor for risk of stroke, dementia and mortality. During the last years, convolutional neural networks (CNN) specifically tailored for biomedical image segmentation have outperformed all previous techniques in this task. However, they are extremely data-dependent, and maintain a good performance only when data distribution between training and test datasets remains unchanged. When such distribution changes but we still aim at performing the same task, we incur in a domain adaptation problem (e.g. using a different MR machine or different acquisition parameters for training and test data). In this work, we explore the use of cycle-consistent adversarial networks (CycleGAN) to perform unsupervised domain adaptation on multicenter MR images with brain lesions. We aim at learning a mapping function to transform volumetric MR images between domains, which are characterized by different medical centers and MR machines with varying brand, model and configuration parameters. Our experiments show that CycleGAN allows us to reduce the Jensen-Shannon divergence between MR domains, enabling automatic segmentation with CNN models on domains where no labeled data was available.      
### 3.Exploration of End-to-end Synthesisers forZero Resource Speech Challenge 2020  [ :arrow_down: ](https://arxiv.org/pdf/2009.04983.pdf)
>  A Spoken dialogue system for an unseen language is referred to as Zero resource speech. It is especially beneficial for developing applications for languages that have low digital resources. Zero resource speech synthesis is the task of building text-to-speech (TTS) models in the absence of transcriptions. In this work, speech is modelled as a sequence of transient and steady-state acoustic units, and a unique set of acoustic units is discovered by iterative training. Using the acoustic unit sequence, TTS models are trained. The main goal of this work is to improve the synthesis quality of zero resource TTS system. Four different systems are proposed. All the systems consist of three stages: unit discovery, followed by unit sequence to spectrogram mapping, and finally spectrogram to speech inversion. Modifications are proposed to the spectrogram mapping stage. These modifications include training the mapping on voice data, using x-vectors to improve the mapping, two-stage learning, and gender-specific modelling. Evaluation of the proposed systems in the Zerospeech 2020 challenge shows that quite good quality synthesis can be achieved.      
### 4.ICASSP 2021 Acoustic Echo Cancellation Challenge: Datasets and Testing Framework  [ :arrow_down: ](https://arxiv.org/pdf/2009.04972.pdf)
>  The ICASSP 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication and conferencing systems. Many recent AEC studies report reasonable performance on synthetic datasets where the train and test samples come from the same underlying distribution. However, the AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ) do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 2,500 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source an online subjective test framework based on ITU-T P.808 for researchers to quickly test their results. The winners of this challenge will be selected based on the average P.808 Mean Opinion Score (MOS) achieved across all different single talk and double talk scenarios.      
### 5.Duality between Coronavirus Transmission and Air-based Macroscopic Molecular Communication  [ :arrow_down: ](https://arxiv.org/pdf/2009.04966.pdf)
>  This contribution exploits the duality between a viral infection process and macroscopic air-based molecular communication. Airborne aerosol and droplet transmission through the human respiratory processes is modeled as an instance of a multiuser molecular communication scenario employing respiratory-event-driven molecular (variable) concentration shift keying. Modeling is aided by an experimental molecular communication testbed based on fluorescent dyes and optical detection of aerosol and droplet clouds. A macroscopic molecular communication simulator is extended and used for estimating the transmission of infectious aerosols in different environments. The work is inspired by the recent outbreak of the coronavirus pandemic.      
### 6.A Configuration Based Pumped Storage Hydro Model in MISO Day-Ahead Market  [ :arrow_down: ](https://arxiv.org/pdf/2009.04944.pdf)
>  Pumped storage hydro units (PSHU) can provide flexibility to power systems. This becomes particularly valuable in recent years with the increasing shares of intermittent renewable resources. However, due to emphasis on thermal generation in the current market practices, the flexibility from PSHUs have not been fully explored and utilized. This paper proposes a configuration based pumped storage hydro (PSH) model for the day-ahead market, in order to enhance the use of PSH resources in the system. A strategic design of incorporating and fully optimizing PSHUs in the day-ahead market is presented. We show the compactness of the proposed model. Numerical studies are presented in an illustrative test system and the Midcontinent Independent System Operator (MISO) system.      
### 7.Artificial Intelligence for 5G Wireless Systems: Opportunities, Challenges, and Future Research Directions  [ :arrow_down: ](https://arxiv.org/pdf/2009.04943.pdf)
>  The advent of the wireless communications systems augurs new cutting-edge technologies, including self-driving vehicles, unmanned aerial systems, autonomous robots, Internet-of-Things, and virtual reality. These technologies require high data rates, ultra-low latency, and high reliability, all of which are promised by the fifth generation of wireless communication systems (5G). Many research groups state that 5G cannot meet its demands without artificial intelligence (AI) integration as 5G wireless networks are expected to generate unprecedented traffic giving wireless research designers access to big data that can help in predicting the demands and adjust cell designs to meet the users requirements. Subsequently, many researchers applied AI in many aspects of 5G wireless communication design including radio resource allocation, network management, and cyber-security. In this paper, we provide an in-depth review of AI for 5G wireless communication systems. In this respect, the aim of this paper is to survey AI in 5G wireless communication systems by discussing many case studies and the associated challenges, and shedding new light on future research directions for leveraging AI in 5G wireless communications.      
### 8.Ultrasound Liver Fibrosis Diagnosis using Multi-indicator guided Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.04924.pdf)
>  Accurate analysis of the fibrosis stage plays very important roles in follow-up of patients with chronic hepatitis B infection. In this paper, a deep learning framework is presented for automatically liver fibrosis prediction. On contrary of previous works, our approach can take use of the information provided by multiple ultrasound images. An indicator-guided learning mechanism is further proposed to ease the training of the proposed model. This follows the workflow of clinical diagnosis and make the prediction procedure interpretable. To support the training, a dataset is well-collected which contains the ultrasound videos/images, indicators and labels of 229 patients. As demonstrated in the experimental results, our proposed model shows its effectiveness by achieving the state-of-the-art performance, specifically, the accuracy is 65.6%(20% higher than previous best).      
### 9.A leak in PRNU based source identification? Questioning fingerprint uniqueness  [ :arrow_down: ](https://arxiv.org/pdf/2009.04878.pdf)
>  Photo Response Non Uniformity (PRNU) is considered the most effective trace for the image source attribution task. Its uniqueness ensures that the sensor pattern noises extracted from different cameras are strongly uncorrelated, even when they belong to the same camera model. However, with the advent of computational photography, most recent devices of the same model start exposing correlated patterns thus introducing the real chance of erroneous image source attribution. In this paper, after highlighting the issue under a controlled environment, we perform a large testing campaign on Flickr images to determine how widespread the issue is and which is the plausible cause. To this aim, we tested over $240000$ image pairs from $54$ recent smartphone models comprising the most relevant brands. Experiments show that many Samsung, Xiaomi and Huawei devices are strongly affected by this issue. Although the primary cause of high false alarm rates cannot be directly related to specific camera models, firmware nor image contents, it is evident that the effectiveness of PRNU-based source identification on the most recent devices must be reconsidered in light of these results. Therefore, this paper is to be intended as a call to action for the scientific community rather than a complete treatment of the subject.      
### 10.Sensorless rotor position estimation by PWM-induced signal injection  [ :arrow_down: ](https://arxiv.org/pdf/2009.04830.pdf)
>  We demonstrate how the rotor position of a PWM-controlled PMSM can be recovered from the measured currents, by suitably using the excitation provided by the PWM itself. This provides the benefits of signal injection, in particular the ability to operate even at low velocity, without the drawbacks of an external probing signal. We illustrate the relevance of the approach by simulations and experimental results.      
### 11.All-Weather sub-50-cm Radar-Inertial Positioning  [ :arrow_down: ](https://arxiv.org/pdf/2009.04814.pdf)
>  Deployment of automated ground vehicles beyond the confines of sunny and dry climes will require sub-lane-level positioning techniques based on radio waves rather than near-visible-light radiation. Like human sight, lidar and cameras perform poorly in low-visibility conditions. This paper develops and demonstrates a novel technique for robust sub-50-cm-accurate urban ground vehicle positioning based on all-weather sensors. The technique incorporates a computationally-efficient globally-optimal radar scan batch registration algorithm into a larger estimation pipeline that fuses data from commercially-available low-cost automotive radars, low-cost inertial sensors, vehicle motion constraints, and, when available, precise GNSS measurements. Performance is evaluated on an extensive and realistic urban data set. Comparison against ground truth shows that during 60 minutes of GNSS-denied driving in the urban center of Austin, TX, the technique maintains 95th-percentile errors below 50 cm in horizontal position and 0.5 degrees in heading.      
### 12.Centering noisy images with application to cryo-EM  [ :arrow_down: ](https://arxiv.org/pdf/2009.04810.pdf)
>  We target the problem of estimating the center of mass of noisy 2-D images. We assume that the noise dominates the image, and thus many standard approaches are vulnerable to estimation errors. Our approach uses a surrogate function to the geometric median, which is a robust estimator of the center of mass. We mathematically analyze cases in which the geometric median fails to provide a reasonable estimate of the center of mass, and prove that our surrogate function leads to a successful estimate. <br>One particular application for our method is to improve 3-D reconstruction in single-particle cryo-electron microscopy (cryo-EM). We show how to apply our approach for a better translational alignment of macromolecules picked from experimental data. In this way, we facilitate the succeeding steps of reconstruction and streamline the entire cryo-EM pipeline, saving valuable computational time and supporting resolution enhancement.      
### 13.Deep Iterative Residual Convolutional Network for Single Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.04809.pdf)
>  Deep convolutional neural networks (CNNs) have recently achieved great success for single image super-resolution (SISR) task due to their powerful feature representation capabilities. The most recent deep learning based SISR methods focus on designing deeper / wider models to learn the non-linear mapping between low-resolution (LR) inputs and high-resolution (HR) outputs. These existing SR methods do not take into account the image observation (physical) model and thus require a large number of network's trainable parameters with a great volume of training data. To address these issues, we propose a deep Iterative Super-Resolution Residual Convolutional Network (ISRResCNet) that exploits the powerful image regularization and large-scale optimization techniques by training the deep network in an iterative manner with a residual learning approach. Extensive experimental results on various super-resolution benchmarks demonstrate that our method with a few trainable parameters improves the results for different scaling factors in comparison with the state-of-art methods.      
### 14.Fully automated analysis of muscle architecture from B-mode ultrasound images with deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.04790.pdf)
>  B-mode ultrasound is commonly used to image musculoskeletal tissues, but one major bottleneck is data interpretation, and analyses of muscle thickness, pennation angle and fascicle length are often still performed manually. In this study we trained deep neural networks (based on U-net) to detect muscle fascicles and aponeuroses using a set of labelled musculoskeletal ultrasound images. We then compared neural network predictions on new, unseen images to those obtained via manual analysis and two existing semi/automated analysis approaches (SMA and Ultratrack). With a GPU, inference time for a single image with the new approach was around 0.7s, compared to 4.6s with a CPU. Our method detects the locations of the superficial and deep aponeuroses, as well as multiple fascicle fragments per image. For single images, the method gave similar results to those produced by a non-trainable automated method (SMA; mean difference in fascicle length: 1.1 mm) or human manual analysis (mean difference: 2.1 mm). Between-method differences in pennation angle were within 1$^\circ$, and mean differences in muscle thickness were less than 0.2 mm. Similarly, for videos, there was strong overlap between the results produced with Ultratrack and our method, with a mean ICC of 0.73, despite the fact that the analysed trials included hundreds of frames. Our method is fully automated and open source, and can estimate fascicle length, pennation angle and muscle thickness from single images or videos, as well as from multiple superficial muscles. We also provide all necessary code and training data for custom model development.      
### 15.Electric End-User Consumer Profit Maximization: An Online Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.04704.pdf)
>  The fast growth of communication technology within the concept of smart grids can provide data and control signals from/to all consumers in an online fashion. This could foster more participation for end-user customers. These types of customers do not necessarily have powerful prediction tools or capability of storing a large amount of historical data. Besides, the relevant information is not always known a priori, while decisions need to be made fast within a very limited time. These limitations and also the novel structure of decision making, which comes from the necessities to make the decision very fast with a limited amount of information, implies a requirement for investigating a novel framework: online decision-making. In this study, we propose an online constrained convex optimization framework for operating responsive end-user electrical customers in real-time. Within this online-decision-making framework, algorithms are proposed for two cases: no prediction data is available at the moment of decision-making, and a limited number of forward time periods predictions of uncertain parameters are available. The simulation results exhibit the capability of the model to achieve considerable profits in an easy-to-implement procedure. Comprehensive numerical test cases are performed for comparison with existent alternative models.      
### 16.Tensor-based Multi-dimensional Wideband Channel Estimation for mmWave Hybrid Cylindrical Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2009.04684.pdf)
>  Channel estimation is challenging for hybrid millimeter wave (mmWave) large-scale antenna arrays which are promising in 5G/B5G applications. The challenges are associated with angular resolution losses resulting from hybrid front-ends, beam squinting, and susceptibility to the receiver noises. Based on tensor signal processing, this paper presents a novel multi-dimensional approach to channel parameter estimation with large-scale mmWave hybrid uniform circular cylindrical arrays (UCyAs) which are compact in size and immune to mutual coupling but known to suffer from infinite-dimensional array responses and intractability. We design a new resolution-preserving hybrid beamformer and a low-complexity beam squinting suppression method, and reveal the existence of shift-invariance relations in the tensor models of received array signals at the UCyA. Exploiting these relations, we propose a new tensor-based subspace estimation algorithm to suppress the receiver noises in all dimensions (time, frequency, and space). The algorithm can accurately estimate the channel parameters from both coherent and incoherent signals. Corroborated by the Cramér-Rao lower bound (CRLB), simulation results show that the proposed algorithm is able to achieve substantially higher estimation accuracy than existing matrix-based techniques, with a comparable computational complexity.      
### 17.Optimal Eco-driving Control of Autonomous and Electric Trucks in Adaptation to Highway Topography: Energy Minimization and Battery Life Extension  [ :arrow_down: ](https://arxiv.org/pdf/2009.04683.pdf)
>  In this paper, we develop a model to plan energy-efficient speed trajectories of electric trucks in real-time by taking into account the information of topography and traffic ahead of the vehicle. In this real time control model, a novel state-space model is first developed to capture vehicle speed, acceleration, and state of charge. We then formulate an energy minimization problem and solve it by an alternating direction method of multipliers (ADMM) method that exploits the structure of the problem. A model predictive control framework is then employed to deal with topographic and traffic uncertainties in real-time. An empirical study is conducted on the performance of the proposed eco-driving algorithm and its impact on battery degradation. The experimental results show that the energy consumption by using the developed method is reduced by up to 5.05%, and the battery life extended by as high as 35.35% compared to benchmarking solutions.      
### 18.Towards Reliable UAV-Enabled Positioning in Mountainous Environments: System Design and Preliminary Results  [ :arrow_down: ](https://arxiv.org/pdf/2009.04638.pdf)
>  Reliable positioning services are extremely important for users and devices in mountainous environments as it enables a variety of location-based applications. However, in such environments, the service reliability of conventional wireless positioning technologies is often disappointing. Frequent non-line-of-sight (NLoS) propagation and poor geometry of available anchor nodes are two significant challenges. Due to the high maneuverability and flexible deployment of unmanned aerial vehicles (UAVs), UAV-enabled positioning could be a promising solution to these challenges. Compared with satellites and terrestrial base stations, UAVs are capable of flying to places where both the propagation conditions and geometry are favorable for positioning. The eventual aim of this research project is to design a novel UAV-enabled positioning system that uses a low-altitude UAV platform to provide highly reliable services for ground users in mountainous environments. In this article, we introduce the recent progress made in the first phase of our project, including the following. First, the structure of the proposed system and the positioning method used are determined after comprehensive consideration of various factors. Utilizing the digital elevation model of the realistic terrain, we then establish a geometry-based NLoS probability model so that the NLoS propagation can be treated as a type of fault during the reliability analysis. Most importantly, a reliability prediction method and the corresponding metric are developed to evaluate the system's ability to provide reliable positioning services. At the end of this article, we also propose a voting-based method for improving the service reliability. Numerical results demonstrate the tremendous potential of the proposed system in reliable positioning.      
### 19.Self-Supervised Annotation of Seismic Images using Latent Space Factorization  [ :arrow_down: ](https://arxiv.org/pdf/2009.04631.pdf)
>  Annotating seismic data is expensive, laborious and subjective due to the number of years required for seismic interpreters to attain proficiency in interpretation. In this paper, we develop a framework to automate annotating pixels of a seismic image to delineate geological structural elements given image-level labels assigned to each image. Our framework factorizes the latent space of a deep encoder-decoder network by projecting the latent space to learned sub-spaces. Using constraints in the pixel space, the seismic image is further factorized to reveal confidence values on pixels associated with the geological element of interest. Details of the annotated image are provided for analysis and qualitative comparison is made with similar frameworks.      
### 20.A Minimum Energy Filter for Localisation of an Unmanned Aerial Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2009.04630.pdf)
>  Accurate localisation of unmanned aerial vehicles is vital for the next generation of automation tasks. This paper proposes a minimum energy filter for velocity-aided pose estimation on the extended special Euclidean group. The approach taken exploits the Lie-group symmetry of the problem to combine Inertial Measurement Unit (IMU) sensor output with landmark measurements into a robust and high performance state estimate. We propose an asynchronous discrete-time implementation to fuse high bandwidth IMU with low bandwidth discrete-time landmark measurements typical of real-world scenarios. The filter's performance is demonstrated by simulation.      
### 21.A Non-Isolated High Step-Up Interleaved DC-DC Converter with Diode-Capacitor Multiplier Cells and Dual Coupled Inductors  [ :arrow_down: ](https://arxiv.org/pdf/2009.04602.pdf)
>  In this paper, a non-isolated high step-up dc-dc converter is presented. The proposed converter is composed of an interleaved structure and diode-capacitor multiplier cells for interfacing low-voltage renewable energy sources to high-voltage distribution buses. The aforementioned topology can provide a very high voltage gain due to employing the coupled inductors and the diode-capacitor cells. The coupled inductors are connected to the diode-capacitor multiplier cells to achieve the interleaved energy storage in the output side. Furthermore, the proposed topology provides continuous input current with low voltage stress on the power devices. The reverse recovery problem of the diodes is reduced. This topology can be operated at a reduced duty cycle by adjusting the turn ratio of the coupled inductors. Moreover, the performance comparison between the proposed topology and other converters are introduced. The design considerations operation principle, steady-state analysis, simulation results, and experimental verifications are presented. Therefore, a 500-W hardware prototype with an input voltage of 30-V and an output voltage of 1000-V is built to verify the performance and the theoretical analysis.      
### 22.Blind Image Restoration with Flow Based Priors  [ :arrow_down: ](https://arxiv.org/pdf/2009.04583.pdf)
>  Image restoration has seen great progress in the last years thanks to the advances in deep neural networks. Most of these existing techniques are trained using full supervision with suitable image pairs to tackle a specific degradation. However, in a blind setting with unknown degradations this is not possible and a good prior remains crucial. Recently, neural network based approaches have been proposed to model such priors by leveraging either denoising autoencoders or the implicit regularization captured by the neural network structure itself. In contrast to this, we propose using normalizing flows to model the distribution of the target content and to use this as a prior in a maximum a posteriori (MAP) formulation. By expressing the MAP optimization process in the latent space through the learned bijective mapping, we are able to obtain solutions through gradient descent. To the best of our knowledge, this is the first work that explores normalizing flows as prior in image enhancement problems. Furthermore, we present experimental results for a number of different degradations on data sets varying in complexity and show competitive results when comparing with the deep image prior approach.      
### 23.Frequency Regulation Model of Bulk Power Systems with Energy Storage  [ :arrow_down: ](https://arxiv.org/pdf/2009.04573.pdf)
>  This paper presents a dynamic Frequency Regulation (FR) model of a large interconnected power system including Energy Storage Systems (ESSs) such as Battery Energy Storage Systems (BESSs) and Flywheel Energy Storage Systems (FESSs), considering all relevant stages in the frequency control process. Communication delays are considered in the transmission of the signals in the FR control loop and ESSs, and their State of Charge (SoC) management model is considered. The system, ESSs and SoC components are modelled in detail from a FR perspective. The model is validated using real system and ESSs data, based on a practical transient stability model of the North American Eastern Interconnection (NAEI), and the results show that the proposed model accurately represents the FR process of a large interconnected power network including ESS, and can be used for long-term FR studies. The impact of communication delays and SoC management of ESS facilities in the Area Control Error (ACE) is also studied and discussed      
### 24.Segmentation-free Estimation of Aortic Diameters from MRI Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.04507.pdf)
>  Accurate and reproducible measurements of the aortic diameters are crucial for the diagnosis of cardiovascular diseases and for therapeutic decision making. Currently, these measurements are manually performed by healthcare professionals, being time consuming, highly variable, and suffering from lack of reproducibility. In this work we propose a supervised deep learning method for the direct estimation of aortic diameters. The approach is devised and tested over 100 magnetic resonance angiography scans without contrast agent. All data was expert-annotated at six aortic locations typically used in clinical practice. Our approach makes use of a 3D+2D convolutional neural network (CNN) that takes as input a 3D scan and outputs the aortic diameter at a given location. In a 5-fold cross-validation comparison against a fully 3D CNN and against a 3D multiresolution CNN, our approach was consistently superior in predicting the aortic diameters. Overall, the 3D+2D CNN achieved a mean absolute error between 2.2-2.4 mm depending on the considered aortic location. These errors are less than 1 mm higher than the inter-observer variability. Thus, suggesting that our method makes predictions almost reaching the expert's performance. We conclude that the work allows to further explore automatic algorithms for direct estimation of anatomical structures without the necessity of a segmentation step. It also opens possibilities for the automation of cardiovascular measurements in clinical settings.      
### 25.Hardware Aware Training for Efficient Keyword Spotting on General Purpose and Specialized Hardware  [ :arrow_down: ](https://arxiv.org/pdf/2009.04465.pdf)
>  Keyword spotting (KWS) provides a critical user interface for many mobile and edge applications, including phones, wearables, and cars. As KWS systems are typically 'always on', maximizing both accuracy and power efficiency are central to their utility. In this work we use hardware aware training (HAT) to build new KWS neural networks based on the Legendre Memory Unit (LMU) that achieve state-of-the-art (SotA) accuracy and low parameter counts. This allows the neural network to run efficiently on standard hardware (212$\mu$W). We also characterize the power requirements of custom designed accelerator hardware that achieves SotA power efficiency of 8.79$\mu$W, beating general purpose low power hardware (a microcontroller) by 24x and special purpose ASICs by 16x.      
### 26.Analysis of Theoretical and Numerical Properties of Sequential Convex Programming for Continuous-Time Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.05038.pdf)
>  Through the years, Sequential Convex Programming (SCP) has gained great interest as an efficient tool for non-convex optimal control. Despite the large number of existing algorithmic frameworks, only a few are accompanied by rigorous convergence analysis, which are often only tailored to discrete-time problem formulations. In this paper, we present a unifying theoretical analysis of a fairly general class of SCP procedures which is applied to the original continuous-time formulation. Besides the extension of classical convergence guarantees to continuous-time settings, our analysis reveals two new features inherited by SCP-type methods. First, we show how one can more easily account for manifold-type constraints, which play a key role in the optimal control of mechanical systems. Second, we demonstrate how the theoretical analysis may be leveraged to devise an accelerated implementation of SCP based on indirect methods. Detailed numerical experiments are provided to show the key benefits of a continuous-time analysis to improve performance.      
### 27.Learning Shape Features and Abstractions in 3D Convolutional Neural Networks for Detecting Alzheimer's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2009.05023.pdf)
>  Deep Neural Networks - especially Convolutional Neural Network (ConvNet) has become the state-of-the-art for image classification, pattern recognition and various computer vision tasks. ConvNet has a huge potential in medical domain for analyzing medical data to diagnose diseases in an efficient way. Based on extracted features by ConvNet model from MRI data, early diagnosis is very crucial for preventing progress and treating the Alzheimer's disease. Despite having the ability to deliver great performance, absence of interpretability of the model's decision can lead to misdiagnosis which can be life threatening. In this thesis, learned shape features and abstractions by 3D ConvNets for detecting Alzheimer's disease were investigated using various visualization techniques. How changes in network structures, used filters sizes and filters shapes affects the overall performance and learned features of the model were also inspected. LRP relevance map of different models revealed which parts of the brain were more relevant for the classification decision. Comparing the learned filters by Activation Maximization showed how patterns were encoded in different layers of the network. Finally, transfer learning from a convolutional autoencoder was implemented to check whether increasing the number of training samples with patches of input to extract the low-level features improves learned features and the model performance.      
### 28.The Cost of Denied Observation in Multiagent Submodular Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2009.05018.pdf)
>  A popular formalism for multiagent control applies tools from game theory, casting a multiagent decision problem as a cooperation-style game in which individual agents make local choices to optimize their own local utility functions in response to the observable choices made by other agents. When the system-level objective is submodular maximization, it is known that if every agent can observe the action choice of all other agents, then all Nash equilibria of a large class of resulting games are within a factor of $2$ of optimal; that is, the price of anarchy is $1/2$. However, little is known if agents cannot observe the action choices of other relevant agents. To study this, we extend the standard game-theoretic model to one in which a subset of agents either become \emph{blind} (unable to observe others' choices) or \emph{isolated} (blind, and also invisible to other agents), and we prove exact expressions for the price of anarchy as a function of the number of compromised agents. When $k$ agents are compromised (in any combination of blind or isolated), we show that the price of anarchy for a large class of utility functions is exactly $1/(2+k)$. We then show that if agents use marginal-cost utility functions and at least $1$ of the compromised agents is blind (rather than isolated), the price of anarchy improves to $1/(1+k)$. We also provide simulation results demonstrating the effects of these observation denials in a dynamic setting.      
### 29.Digital Signal Processing for Molecular Communication via Chemical Reactions-based Microfluidic Circuits  [ :arrow_down: ](https://arxiv.org/pdf/2009.05009.pdf)
>  Chemical reactions-based microfluidic circuits are expected to provide new opportunities to perform signal processing functions over molecular domain. To realize this vision, in this article, we exploit and present the digital signal processing capabilities of chemical reactions-based microfluidic circuits. Aiming to facilitate microfluidic circuit design, we describe a microfluidic circuit using a five-level architecture: 1) Molecular Propagation; 2) Chemical Transformation; 3) Microfluidic Modules; 4) Microfluidic Logic Gates; and 5) Microfluidic Circuits. We first identify the components at Levels 1 and 2, and present how their combinations can build the basic modules for Level 3. We then assemble basic modules to construct five types of logic gate for Level 4, including AND, NAND, OR, NOR, and XOR gates, which show advantages of microfluidic circuits in reusability and modularity. Last but not least, we discuss challenges and potential solutions for designing, building, and testing microfluidic circuits with complex signal processing functions in Level 5 based on the digital logic gates at Level 4.      
### 30.MedMeshCNN -- Enabling MeshCNN for Medical Surface Models  [ :arrow_down: ](https://arxiv.org/pdf/2009.04893.pdf)
>  Background and objective: MeshCNN is a recently proposed Deep Learning framework that drew attention due to its direct operation on irregular, non-uniform 3D meshes. On selected benchmarking datasets, it outperformed state-of-the-art methods within classification and segmentation tasks. Especially, the medical domain provides a large amount of complex 3D surface models that may benefit from processing with MeshCNN. However, several limitations prevent outstanding performances of MeshCNN on highly diverse medical surface models. Within this work, we propose MedMeshCNN as an expansion for complex, diverse, and fine-grained medical data. Methods: MedMeshCNN follows the functionality of MeshCNN with a significantly increased memory efficiency that allows retaining patient-specific properties during the segmentation process. Furthermore, it enables the segmentation of pathological structures that often come with highly imbalanced class distributions. Results: We tested the performance of MedMeshCNN on a complex part segmentation task of intracranial aneurysms and their surrounding vessel structures and reached a mean Intersection over Union of 63.24\%. The pathological aneurysm is segmented with an Intersection over Union of 71.4\%. Conclusions: These results demonstrate that MedMeshCNN enables the application of MeshCNN on complex, fine-grained medical surface meshes. The imbalanced class distribution deriving from the pathological finding is considered by MedMeshCNN and patient-specific properties are mostly retained during the segmentation process.      
### 31.Detecting the Presence of Vehicles and Equipment in SAR Imagery Using Image Texture Features  [ :arrow_down: ](https://arxiv.org/pdf/2009.04866.pdf)
>  In this work, we present a methodology for monitoring man-made, construction-like activities in low-resolution SAR imagery. Our source of data is the European Space Agency Sentinel-l satellite which provides global coverage at a 12-day revisit rate. Despite limitations in resolution, our methodology enables us to monitor activity levels (i.e. presence of vehicles, equipment) of a pre-defined location by analyzing the texture of detected SAR imagery. Using an exploratory dataset, we trained a support vector machine (SVM), a random binary forest, and a fully-connected neural network for classification. We use Haralick texture features in the VV and VH polarization channels as the input features to our classifiers. Each classifier showed promising results in being able to distinguish between two possible types of construction-site activity levels. This paper documents a case study that is centered around monitoring the construction process for oil and gas fracking wells.      
### 32.Variational Autoencoders for Jet Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2009.04842.pdf)
>  We introduce a novel variational autoencoder (VAE) architecture that can generate realistic and diverse high energy physics events. The model we propose utilizes several techniques from VAE literature in order to simulate high fidelity jet images. In addition to demonstrating the model's ability to produce high fidelity jet images through various assessments, we also demonstrate its ability to control the events it generates from the latent space. This can be potentially useful for other tasks such as jet tagging, where we can test how well jet taggers can classify signal from background for events generated by the VAE. We test this idea by seeing the signal efficiency vs background rejection for different types of jet images produced by our model. We compare our VAE with generative adversarial networks (GAN) in several ways, most notably in speed. The architecture we propose is ultimately a fast, stable, and easy-to-train deep generative model that demonstrates the potential of VAEs in simulating high energy physics events.      
### 33.Hard Occlusions in Visual Object Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2009.04787.pdf)
>  Visual object tracking is among the hardest problems in computer vision, as trackers have to deal with many challenging circumstances such as illumination changes, fast motion, occlusion, among others. A tracker is assessed to be good or not based on its performance on the recent tracking datasets, e.g., VOT2019, and LaSOT. We argue that while the recent datasets contain large sets of annotated videos that to some extent provide a large bandwidth for training data, the hard scenarios such as occlusion and in-plane rotation are still underrepresented. For trackers to be brought closer to the real-world scenarios and deployed in safety-critical devices, even the rarest hard scenarios must be properly addressed. In this paper, we particularly focus on hard occlusion cases and benchmark the performance of recent state-of-the-art trackers (SOTA) on them. We created a small-scale dataset containing different categories within hard occlusions, on which the selected trackers are evaluated. Results show that hard occlusions remain a very challenging problem for SOTA trackers. Furthermore, it is observed that tracker performance varies wildly between different categories of hard occlusions, where a top-performing tracker on one category performs significantly worse on a different category. The varying nature of tracker performance based on specific categories suggests that the common tracker rankings using averaged single performance scores are not adequate to gauge tracker performance in real-world scenarios.      
### 34.Data-Driven Open Set Fault Classification and Fault Size Estimation Using Quantitative Fault Diagnosis Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.04756.pdf)
>  Data-driven fault classification is complicated by imbalanced training data and unknown fault classes. Fault diagnosis of dynamic systems is done by detecting changes in time-series data, for example residuals, caused by faults or system degradation. Different fault classes can result in similar residual outputs, especially for small faults which can be difficult to distinguish from nominal system operation. Analyzing how easy it is to distinguish data from different fault classes is crucial during the design process of a diagnosis system to evaluate if classification performance requirements can be met. Here, a data-driven model of different fault classes is used based on the Kullback-Leibler divergence. This is used to develop a framework for quantitative fault diagnosis performance analysis and open set fault classification. A data-driven fault classification algorithm is proposed which can handle unknown faults and also estimate the fault size using training data from known fault scenarios. To illustrate the usefulness of the proposed methods, data have been collected from an engine test bench to illustrate the design process of a data-driven diagnosis system, including quantitative fault diagnosis analysis and evaluation of the developed open set fault classification algorithm.      
### 35.3D Facial Matching by Spiral Convolutional Metric Learning and a Biometric Fusion-Net of Demographic Properties  [ :arrow_down: ](https://arxiv.org/pdf/2009.04746.pdf)
>  Face recognition is a widely accepted biometric verification tool, as the face contains a lot of information about the identity of a person. In this study, a 2-step neural-based pipeline is presented for matching 3D facial shape to multiple DNA-related properties (sex, age, BMI and genomic background). The first step consists of a triplet loss-based metric learner that compresses facial shape into a lower dimensional embedding while preserving information about the property of interest. Most studies in the field of metric learning have only focused on Euclidean data. In this work, geometric deep learning is employed to learn directly from 3D facial meshes. To this end, spiral convolutions are used along with a novel mesh-sampling scheme that retains uniformly sampled 3D points at different levels of resolution. The second step is a multi-biometric fusion by a fully connected neural network. The network takes an ensemble of embeddings and property labels as input and returns genuine and imposter scores. Since embeddings are accepted as an input, there is no need to train classifiers for the different properties and available data can be used more efficiently. Results obtained by a 10-fold cross-validation for biometric verification show that combining multiple properties leads to stronger biometric systems. Furthermore, the proposed neural-based pipeline outperforms a linear baseline, which consists of principal component analysis, followed by classification with linear support vector machines and a Naive Bayes-based score-fuser.      
### 36.Rayleigh Fading Modeling and Channel Hardening for Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2009.04723.pdf)
>  A realistic performance assessment of any wireless technology requires the use of a channel model that reflects its main characteristics. The independent and identically distributed Rayleigh fading channel model has been (and still is) the basis of most theoretical research on multiple antenna technologies in scattering environments. This letter shows that such a model is not physically appearing when using a reconfigurable intelligent surface (RIS) with rectangular geometry and provides an alternative physically feasible Rayleigh fading model that can be used as a baseline when evaluating RIS-aided communications. The model is used to revisit the basic RIS properties, e.g., the rank of spatial correlation matrices and channel hardening.      
### 37.Multimodal Noisy Segmentation based fragmented burn scars identification in Amazon Rainforest  [ :arrow_down: ](https://arxiv.org/pdf/2009.04634.pdf)
>  Detection of burn marks due to wildfires in inaccessible rain forests is important for various disaster management and ecological studies. The fragmented nature of arable landscapes and diverse cropping patterns often thwart the precise mapping of burn scars. Recent advances in remote-sensing and availability of multimodal data offer a viable solution to this mapping problem. However, the task to segment burn marks is difficult because of its indistinguishably with similar looking land patterns, severe fragmented nature of burn marks and partially labelled noisy datasets. In this work we present AmazonNET -- a convolutional based network that allows extracting of burn patters from multimodal remote sensing images. The network consists of UNet: a well-known encoder decoder type of architecture with skip connections commonly used in biomedical segmentation. The proposed framework utilises stacked RGB-NIR channels to segment burn scars from the pastures by training on a new weakly labelled noisy dataset from Amazonia. Our model illustrates superior performance by correctly identifying partially labelled burn scars and rejecting incorrectly labelled samples, demonstrating our approach as one of the first to effectively utilise deep learning based segmentation models in multimodal burn scar identification.      
### 38.tsBNgen: A Python Library to Generate Time Series Data from an Arbitrary Dynamic Bayesian Network Structure  [ :arrow_down: ](https://arxiv.org/pdf/2009.04595.pdf)
>  Synthetic data is widely used in various domains. This is because many modern algorithms require lots of data for efficient training, and data collection and labeling usually are a time-consuming process and are prone to errors. Furthermore, some real-world data, due to its nature, is confidential and cannot be shared. Bayesian networks are a type of probabilistic graphical model widely used to model the uncertainties in real-world processes. Dynamic Bayesian networks are a special class of Bayesian networks that model temporal and time series data. In this paper, we introduce the tsBNgen, a Python library to generate time series and sequential data based on an arbitrary dynamic Bayesian network. The package, documentation, and examples can be downloaded from <a class="link-external link-https" href="https://github.com/manitadayon/tsBNgen" rel="external noopener nofollow">this https URL</a>.      
### 39.Generalized Energy Detection Under Generalized Noise Channels  [ :arrow_down: ](https://arxiv.org/pdf/2009.04564.pdf)
>  Generalized energy detection (GED) is analytically studied when operates under fast-faded channels and in the presence of generalized noise. For the first time, the McLeish distribution is used to model the underlying noise, which is suitable for both non-Gaussian (impulsive) as well as classical Gaussian noise channels. Important performance metrics are presented in closed forms, such as the false-alarm and detection probabilities as well as the decision threshold. Analytical and simulation results are cross-compared validating the accuracy of the proposed approach in the entire signal-to-noise ratio regime. Finally, useful outcomes are extracted with respect to GED system settings under versatile noise environments and when noise uncertainty is present.      
### 40.Optimal Inspection and Maintenance Planning for Deteriorating Structures through Dynamic Bayesian Networks and Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2009.04547.pdf)
>  Civil and maritime engineering systems, among others, from bridges to offshore platforms and wind turbines, must be efficiently managed as they are exposed to deterioration mechanisms throughout their operational life, such as fatigue or corrosion. Identifying optimal inspection and maintenance policies demands the solution of a complex sequential decision-making problem under uncertainty, with the main objective of efficiently controlling the risk associated with structural failures. Addressing this complexity, risk-based inspection planning methodologies, supported often by dynamic Bayesian networks, evaluate a set of pre-defined heuristic decision rules to reasonably simplify the decision problem. However, the resulting policies may be compromised by the limited space considered in the definition of the decision rules. Avoiding this limitation, Partially Observable Markov Decision Processes (POMDPs) provide a principled mathematical methodology for stochastic optimal control under uncertain action outcomes and observations, in which the optimal actions are prescribed as a function of the entire, dynamically updated, state probability distribution. In this paper, we combine dynamic Bayesian networks with POMDPs in a joint framework for optimal inspection and maintenance planning, and we provide the formulation for developing both infinite and finite horizon POMDPs in a structural reliability context. The proposed methodology is implemented and tested for the case of a structural component subject to fatigue deterioration, demonstrating the capability of state-of-the-art point-based POMDP solvers for solving the underlying planning optimization problem. Within the numerical experiments, POMDP and heuristic-based policies are thoroughly compared, and results showcase that POMDPs achieve substantially lower costs as compared to their counterparts, even for traditional problem settings.      
### 41.Attention based Writer Independent Handwriting Verification  [ :arrow_down: ](https://arxiv.org/pdf/2009.04532.pdf)
>  The task of writer verification is to provide a likelihood score for whether the queried and known handwritten image samples belong to the same writer or not. Such a task calls for the neural network to make it's outcome interpretable, i.e. provide a view into the network's decision making process. We implement and integrate cross-attention and soft-attention mechanisms to capture the highly correlated and salient points in feature space of 2D inputs. The attention maps serve as an explanation premise for the network's output likelihood score. The attention mechanism also allows the network to focus more on relevant areas of the input, thus improving the classification performance. Our proposed approach achieves a precision of 86\% for detecting intra-writer cases in CEDAR cursive "AND" dataset. Furthermore, we generate meaningful explanations for the provided decision by extracting attention maps from multiple levels of the network.      
### 42.Physics-Informed Neural Networks for Nonhomogeneous Material Identification in Elasticity Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2009.04525.pdf)
>  We apply Physics-Informed Neural Networks (PINNs) for solving identification problems of nonhomogeneous materials. We focus on the problem with a background in elasticity imaging, where one seeks to identify the nonhomogeneous mechanical properties of soft tissue based on the full-field displacement measurements under quasi-static loading. In our model, we apply two independent neural networks, one for approximating the solution of the corresponding forward problem, and the other for approximating the unknown material parameter field. As a proof of concept, we validate our model on a prototypical plane strain problem for incompressible hyperelastic tissue. The results show that the PINNs are effective in accurately recovering the unknown distribution of mechanical properties. By employing two neural networks in our model, we extend the capability of material identification of PINNs to include nonhomogeneous material parameter fields, which enables more flexibility of PINNs in representing complex material properties.      
### 43.A dataset and classification model for Malay, Hindi, Tamil and Chinese music  [ :arrow_down: ](https://arxiv.org/pdf/2009.04459.pdf)
>  In this paper we present a new dataset, with musical excepts from the three main ethnic groups in Singapore: Chinese, Malay and Indian (both Hindi and Tamil). We use this new dataset to train different classification models to distinguish the origin of the music in terms of these ethnic groups. The classification models were optimized by exploring the use of different musical features as the input. Both high level features, i.e., musically meaningful features, as well as low level features, i.e., spectrogram based features, were extracted from the audio files so as to optimize the performance of the different classification models.      
### 44.Charge-voltage relation for a universal capacitor  [ :arrow_down: ](https://arxiv.org/pdf/2007.02410.pdf)
>  Most capacitors do not satisfy the conventional assumption of a constant capacitance. They exhibit memory which is often described by a time-varying capacitance. It is shown that the classical relation, $Q\left(t\right)=CV\left(t\right)$, that relates the charge, $Q$, with the capacitance, $C$, and the voltage, $V$, is not applicable for capacitors with a time-varying capacitance. The expression for the current, $dQ/dt$, that is subsequently obtained following the substitution of $C$ by $C\left(t\right)$ in the classical relation corresponds to an inconsistent circuit. In order to address the inconsistency, I propose a charge-voltage relation according to which the charge on a capacitor is expressed by the convolution of its time-varying capacitance with the first-order time-derivative of the applied voltage, i.e., $Q\left(t\right)=C\left(t\right)\ast dV/dt$. This relation corresponds to the universal capacitor which is also known as the fractional capacitor among the fractional calculus community. Since the fractional capacitor has an inherent connection with the universal dielectric response that is expressed by the century old Curie-von Schweidler law, the finding extends to the study of dielectrics as well.      
