# ArXiv eess --Fri, 25 Sep 2020
### 1.ECOVNet: An Ensemble of Deep Convolutional Neural Networks Based on EfficientNet to Detect COVID-19 From Chest X-rays  [ :arrow_down: ](https://arxiv.org/pdf/2009.11850.pdf)
>  This paper proposed an ensemble of deep convolutional neural networks (CNN) based on EfficientNet, named ECOVNet, to detect COVID-19 using a large chest X-ray data set. At first, the open-access large chest X-ray collection is augmented, and then ImageNet pre-trained weights for EfficientNet is transferred with some customized fine-tuning top layers that are trained, followed by an ensemble of model snapshots to classify chest X-rays corresponding to COVID-19, normal, and pneumonia. The predictions of the model snapshots, which are created during a single training, are combined through two ensemble strategies, i.e., hard ensemble and soft ensemble to ameliorate classification performance and generalization in the related task of classifying chest X-rays.      
### 2.Effect of Outlier Removal from Temporal ASF Corrections on Multichain Loran Positioning Accuracy  [ :arrow_down: ](https://arxiv.org/pdf/2009.11812.pdf)
>  The widely used global navigation satellite systems (GNSSs) are vulnerable to radio frequency interference (RFI). Long-range navigation (Loran), a terrestrial navigation system, can compensate for this weakness; however, it suffers from low positioning accuracy, and studies are under way to improve its positioning performance. One such study has proposed the multichain Loran positioning method that uses the signals of transmitting stations belonging to different chains. Although the multichain Loran positioning performance is superior to the performance of conventional methods, the additional secondary factor (ASF) can still degrade its positioning accuracy. To mitigate the effects of temporal ASF, which is one of the ASF components, it is necessary to obtain temporal correction data from a nearby reference station at a known location. In this study, an experiment is performed to verify the effect of removing the outliers in the temporal correction data on the multichain Loran positioning accuracy.      
### 3.Effects of Initial Attitude Estimation Errors on Loosely Coupled Smartphone GPS/IMU Integration System  [ :arrow_down: ](https://arxiv.org/pdf/2009.11807.pdf)
>  Global Positioning System (GPS) and inertial measurement unit (IMU) sensors are commonly integrated using the extended Kalman filter (EKF), for achieving better navigation performance. However, because of nonlinearity, the performance of the EKF is affected by the initial state estimation errors, and the navigation solutions, including the attitude, diverge rapidly as the initial errors increase. This paper analyzes the data obtained from an outdoor experiment, and investigates the effect of the initial errors on the attitude estimation performance using EKF, which is used in loosely coupled low-cost smartphone GPS/IMU sensors.      
### 4.Development of Record and Management Software for GPS/Loran Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2009.11803.pdf)
>  In this paper, a software implementation that records Global Positioning System (GPS) and long-range navigation (Loran) measurement data output from an integrated GPS/Loran receiver and organizes them based on time is proposed. The purpose of the developed software is to collect measurements from multiple Loran transmitter chains for performance analysis of navigation methods using Loran, and to organize the data based on time to make it easy to use them. In addition, GPS measurements are also collected and managed as ground truth data for performance analysis. The implemented software consists of three modules: recording, classification, and conversion. The recording module records raw text data streamed from the receiver, and the classification module classifies the recorded text data according to the message format. The conversion module parses the classified text data, sorts GPS and Loran measurements based on timestamp, and outputs them according to the software platform of the user to analyze the measurements. Each module of the software runs automatically without user intervention. The functionality of the implemented software was verified using GPS and Loran measurements collected over 24 h from an actual integrated GPS/Loran receiver.      
### 5.Practical Simplified Indoor Multiwall Path-Loss Model  [ :arrow_down: ](https://arxiv.org/pdf/2009.11794.pdf)
>  Over the past few decades, attempts had been made to build a suitable channel prediction model to optimize radio transmission systems. It is particularly essential to predict the path loss due to the blockage of the signal, in indoor radio system applications. This paper proposed a multiwall path-loss propagation model for an indoor environment, operating at a transmission frequency of 2.45 GHz in the industrial, scientific, and medical (ISM) radio band. The effects of the number of the walls to be traversed along the radio propagation path are considered in the model. To propose the model, the previous works on well-known indoor path loss models are discussed. Then, the path loss produced by the intervening walls in the propagation path is measured, and the terms representing the loss factors in the theoretical pathloss model are modified. The analyzed results of the path loss factors acquired at 2.45 GHz are presented. The proposed path-loss model simplifies the loss factor term with an admissible assumption of the indoor environment and predicts the path-loss factor accurately.      
### 6.Neural Identification for Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.11782.pdf)
>  We present a new method for learning control law that stabilizes an unknown nonlinear dynamical system at an equilibrium point. We formulate a system identification task in a self-supervised learning setting that jointly learns a controller and corresponding stable closed-loop dynamics hypothesis. The open-loop input-output behavior of the underlying dynamical system is used as the supervising signal to train the neural network-based system model and controller. The method relies on the Lyapunov stability theory to generate a stable closed-loop dynamics hypothesis and corresponding control law. We demonstrate our method on various nonlinear control problems such as n-Link pendulum balancing, pendulum on cart balancing, and wheeled vehicle path following.      
### 7.A New Dataset for Amateur Vocal Percussion Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.11737.pdf)
>  The imitation of percussive instruments via the human voice is a natural way for us to communicate rhythmic ideas and, for this reason, it attracts the interest of music makers. Specifically, the automatic mapping of these vocal imitations to their emulated instruments would allow creators to realistically prototype rhythms in a faster way. The contribution of this study is two-fold. Firstly, a new Amateur Vocal Percussion (AVP) dataset is introduced to investigate how people with little or no experience in beatboxing approach the task of vocal percussion. The end-goal of this analysis is that of helping mapping algorithms to better generalise between subjects and achieve higher performances. The dataset comprises a total of 9780 utterances recorded by 28 participants with fully annotated onsets and labels (kick drum, snare drum, closed hi-hat and opened hi-hat). Lastly, we conducted baseline experiments on audio onset detection with the recorded dataset, comparing the performance of four state-of-the-art algorithms in a vocal percussion context.      
### 8.Region Growing with Convolutional Neural Networks for Biomedical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.11717.pdf)
>  In this paper we present a methodology that uses convolutional neural networks (CNNs) for segmentation by iteratively growing predicted mask regions in each coordinate direction. The CNN is used to predict class probability scores in a small neighborhood of the center pixel in a tile of an image. We use a threshold on the CNN probability scores to determine whether pixels are added to the region and the iteration continues until no new pixels are added to the region. Our method is able to achieve high segmentation accuracy and preserve biologically realistic morphological features while leveraging small amounts of training data and maintaining computational efficiency. Using retinal blood vessel images from the DRIVE database we found that our method is more accurate than a fully convolutional semantic segmentation CNN for several evaluation metrics.      
### 9.EEGdenoiseNet: A benchmark dataset for deep learning solutions of EEG denoising  [ :arrow_down: ](https://arxiv.org/pdf/2009.11662.pdf)
>  Deep learning networks are increasingly attracting attention in several fields. Among other applications, deep learning models have been used for denoising of electroencephalography (EEG) data. These models provided comparable performance with that of traditional techniques. At present, however, the lack of well-structured, standardized datasets with specific benchmark limits the development of deep learning solutions for EEG denoising. Here, we present EEGdenoiseNet, a benchmark EEG dataset that is suited for training and testing deep learning-based denoising models. This permits making valid comparisons across different models. EEGdenoiseNet contains 4514 clean EEG epochs, 3400 ocular artifact epochs and 5598 muscular artifact epochs. This allow the user to produce a large number of noisy EEG epochs with ground truth for model training and testing. We used EEGdenoiseNet to evaluate the performance of four classical deep learning networks (a fully-connected network, a simple convolution network, a complex convolution network and a recurrent neural network). Our analysis suggested that deep learning methods have great potential for EEG denoising even under high noise contamination. Through EEGdenoiseNet, we hope to accelerate the development of the emerging field of deep learning-based EEG denoising. The dataset and the code for benchmarking deep learning networks are publicly available on github (<a class="link-external link-https" href="https://github.com/ncclabsustech/EEGdenoiseNet" rel="external noopener nofollow">this https URL</a>).      
### 10.Prescribed-Time Fully Distributed Nash Equilibrium Seeking in Noncooperative Games  [ :arrow_down: ](https://arxiv.org/pdf/2009.11649.pdf)
>  In this paper, we investigate a prescribed-time and fully distributed Nash Equilibrium (NE) seeking problem for continuous-time noncooperative games. By exploiting pseudo-gradient play and consensus-based schemes, various distributed NE seeking algorithms are presented over either fixed or switching communication topologies so that the convergence to the NE is reached in a prescribed time. In particular, a prescribed-time distributed NE seeking algorithm is firstly developed under a fixed graph to find the NE in a prior-given and user-defined time, provided that a static controller gain can be selected based on certain global information such as the algebraic connectivity of the communication graph and both the Lipschitz and monotone constants of the pseudo-gradient associated with players' objective functions. Secondly, a prescribed-time and fully distributed NE seeking algorithm is proposed to remove global information by designing heterogeneous dynamic gains that turn on-line the weights of the communication topology. Further, we extend this algorithm to accommodate jointly switching topologies. It is theoretically proved that the global convergence of those proposed algorithms to the NE is rigorously guaranteed in a prescribed time based on a time function transformation approach. In the last, numerical simulation results are presented to verify the effectiveness of the designs.      
### 11.Unlocking Extra Value from Grid Batteries Using Advanced Models  [ :arrow_down: ](https://arxiv.org/pdf/2009.11615.pdf)
>  Lithium-ion batteries are increasingly being deployed in liberalised electricity systems, where their use is driven by economic optimisation in a specific market context. However, battery degradation depends strongly on operational profile, and this is particularly variable in energy trading applications. Here, we present results from a year-long experiment where pairs of batteries were cycled with profiles calculated by solving an economic optimisation problem for wholesale energy trading, including a physically-motivated degradation model as a constraint. The results show that this approach can increase revenue by 20% whilst simultaneously decreasing degradation by 30% compared to existing methods. The physics-based approach increases the lifetime both in terms of years and number of cycles, as well as the revenue per year, increasing the possible lifetime revenue by 70%. This demonstrates the potential to unlock significant extra performance using control engineering incorporating physical models of battery ageing.      
### 12.Transfer Learning by Cascaded Network to identify and classify lung nodules for cancer detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.11587.pdf)
>  Lung cancer is one of the most deadly diseases in the world. Detecting such tumors at an early stage can be a tedious task. Existing deep learning architecture for lung nodule identification used complex architecture with large number of parameters. This study developed a cascaded architecture which can accurately segment and classify the benign or malignant lung nodules on computed tomography (CT) images. The main contribution of this study is to introduce a segmentation network where the first stage trained on a public data set can help to recognize the images which included a nodule from any data set by means of transfer learning. And the segmentation of a nodule improves the second stage to classify the nodules into benign and malignant. The proposed architecture outperformed the conventional methods with an area under curve value of 95.67\%. The experimental results showed that the classification accuracy of 97.96\% of our proposed architecture outperformed other simple and complex architectures in classifying lung nodules for lung cancer detection.      
### 13.Beamforming Design for Multiuser Transmission Through Reconfigurable Intelligent Surface  [ :arrow_down: ](https://arxiv.org/pdf/2009.11560.pdf)
>  This paper investigates the problem of resource allocation for multiuser communication networks with a reconfigurable intelligent surface (RIS)-assisted wireless transmitter. In this network, the sum transmit power of the network is minimized by controlling the phase beamforming of the RIS and transmit power of the BS. This problem is posed as a joint optimization problem of transmit power and RIS control, whose goal is to minimize the sum transmit power under signal-to-interference-plus-noise ratio (SINR) constraints of the users. To solve this problem, a dual method is proposed, where the dual problem is obtained as a semidefinite programming problem. After solving the dual problem, the phase beamforming of the RIS is obtained in the closed form, while the optimal transmit power is obtained by using the standard interference function. Simulation results show that the proposed scheme can reduce up to 94% and 27% sum transmit power compared to the maximum ratio transmission (MRT) beamforming and zero-forcing (ZF) beamforming techniques, respectively.      
### 14.Robust Phase Unwrapping via Deep Image Prior for Quantitative Phase Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2009.11554.pdf)
>  Quantitative phase imaging (QPI) is an emerging label-free technique that produces images containing morphological and dynamical information without contrast agents. Unfortunately, the phase is wrapped in most imaging system. Phase unwrapping is the computational process that recovers a more informative image. It is particularly challenging with thick and complex samples such as organoids. Recent works that rely on supervised training show that deep learning is a powerful method to unwrap the phase; however, supervised approaches require large and representative datasets which are difficult to obtain for complex biological samples. Inspired by the concept of deep image priors, we propose a deep-learning-based method that does not need any training set. Our framework relies on an untrained convolutional neural network to accurately unwrap the phase while ensuring the consistency of the measurements. We experimentally demonstrate that the proposed method faithfully recovers the phase of complex samples on both real and simulated data. Our work paves the way to reliable phase imaging of thick and complex samples with QPI.      
### 15.Residual Feature Distillation Network for Lightweight Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.11551.pdf)
>  Recent advances in single image super-resolution (SISR) explored the power of convolutional neural network (CNN) to achieve a better performance. Despite the great success of CNN-based methods, it is not easy to apply these methods to edge devices due to the requirement of heavy computation. To solve this problem, various fast and lightweight CNN models have been proposed. The information distillation network is one of the state-of-the-art methods, which adopts the channel splitting operation to extract distilled features. However, it is not clear enough how this operation helps in the design of efficient SISR models. In this paper, we propose the feature distillation connection (FDC) that is functionally equivalent to the channel splitting operation while being more lightweight and flexible. Thanks to FDC, we can rethink the information multi-distillation network (IMDN) and propose a lightweight and accurate SISR model called residual feature distillation network (RFDN). RFDN uses multiple feature distillation connections to learn more discriminative feature representations. We also propose a shallow residual block (SRB) as the main building block of RFDN so that the network can benefit most from residual learning while still being lightweight enough. Extensive experimental results show that the proposed RFDN achieve a better trade-off against the state-of-the-art methods in terms of performance and model complexity. Moreover, we propose an enhanced RFDN (E-RFDN) and won the first place in the AIM 2020 efficient super-resolution challenge. Code will be available at <a class="link-external link-https" href="https://github.com/njulj/RFDN" rel="external noopener nofollow">this https URL</a>.      
### 16.Complex Convolutional Neural Networks for Ultrasound Image Reconstruction from In-Phase/Quadrature Signal  [ :arrow_down: ](https://arxiv.org/pdf/2009.11536.pdf)
>  A wide variety of studies based on deep learning have recently been investigated to improve ultrasound (US) imaging. Most of these approaches were performed on radio frequency (RF) signals. However, inphase/quadrature (I/Q) digital beamformers (IQBF) are now widely used as low-cost strategies. In this work, we leveraged complex convolutional neural networks (CCNNs) for reconstructing ultrasound images from I/Q signals. We recently described a CNN architecture called ID-Net, which exploited an inception layer devoted to the reconstruction of RF diverging-wave (DW) ultrasound images. We derived in this work the complex equivalent of this network, i.e., the complex inception for DW network (CID-Net), operating on I/Q data. We provided experimental evidence that the CID-Net yields the same image quality as that obtained from the RF-trained CNNs; i.e., by using only three I/Q images, the CID-Net produced high-quality images competing with those obtained by coherently compounding 31 RF images. Moreover, we showed that the CID-Net outperforms the straightforward architecture consisting in processing separately the real and imaginary parts of the I/Q signal, indicating thereby the importance of consistently processing the I/Q signals using a network that exploits the complex nature of such signal.      
### 17.Unpaired Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2009.11532.pdf)
>  Deep learning approaches in image processing predominantly resort to supervised learning. A majority of methods for image denoising are no exception to this rule and hence demand pairs of noisy and corresponding clean images. Only recently has there been the emergence of methods such as Noise2Void, where a deep neural network learns to denoise solely from noisy images. However, when clean images that do not directly correspond to any of the noisy images are actually available, there is room for improvement as these clean images contain useful information that fully unsupervised methods do not exploit. In this paper, we propose a method for image denoising in this setting. First, we use a flow-based generative model to learn a prior from clean images. We then use it to train a denoising network without the need for any clean targets. We demonstrate the efficacy of our method through extensive experiments and comparisons.      
### 18.Adversarial Brain Multiplex Prediction From a Single Network for High-Order Connectional Gender-Specific Brain Mapping  [ :arrow_down: ](https://arxiv.org/pdf/2009.11524.pdf)
>  Brain connectivity networks, derived from magnetic resonance imaging (MRI), non-invasively quantify the relationship in function, structure, and morphology between two brain regions of interest (ROIs) and give insights into gender-related connectional differences. However, to the best of our knowledge, studies on gender differences in brain connectivity were limited to investigating pairwise (i.e., low-order) relationship ROIs, overlooking the complex high-order interconnectedness of the brain as a network. To address this limitation, brain multiplexes have been introduced to model the relationship between at least two different brain networks. However, this inhibits their application to datasets with single brain networks such as functional networks. To fill this gap, we propose the first work on predicting brain multiplexes from a source network to investigate gender differences. Recently, generative adversarial networks (GANs) submerged the field of medical data synthesis. However, although conventional GANs work well on images, they cannot handle brain networks due to their non-Euclidean topological structure. Differently, in this paper, we tap into the nascent field of geometric-GANs (G-GAN) to design a deep multiplex prediction architecture comprising (i) a geometric source to target network translator mimicking a U-Net architecture with skip connections and (ii) a conditional discriminator which classifies predicted target intra-layers by conditioning on the multiplex source intra-layers. Such architecture simultaneously learns the latent source network representation and the deep non-linear mapping from the source to target multiplex intra-layers. Our experiments on a large dataset demonstrated that predicted multiplexes significantly boost gender classification accuracy compared with source networks and identifies both low and high-order gender-specific multiplex connections.      
### 19.Machine learning for UAV-Based networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.11522.pdf)
>  Unmanned aerial vehicles (UAVs) are considered as one of the promising technologies for the next-generation wireless communication networks. Their mobility and their ability to establish a line of sight (LOS) links with the users made them key solutions for many potential applications. In the same vein, artificial intelligence is growing rapidly nowadays and has been very successful, particularly due to the massive amount of the available data. As a result, a significant part of the research community has started to integrate intelligence at the core of UAVs networks by applying machine learning (ML) algorithms in solving several problems in relation to drones. In this article, we provide a comprehensive overview of some potential applications of ML in UAV-Based networks. We will also highlight the limits of the existing works and outline some potential future applications of ML for UAVs networks.      
### 20.Intelligent Reflecting Surface Enhanced Indoor Robot Path Planning Using Radio Maps  [ :arrow_down: ](https://arxiv.org/pdf/2009.11519.pdf)
>  An indoor robot navigation system is investigated, where an intelligent reflecting surface (IRS) is employed to enhance the connectivity between the access point (AP) and a mobile robotic user. The considered system is optimized for minimization of the travelling time/distance of the mobile robotic user from a given starting point to a predefined final location, while satisfying constraints on the communication quality. To tackle this problem, a radio map based approach is proposed to exploit location-dependent channel propagation knowledge. Specifically, a channel power gain map is constructed, which characterizes the spatial distribution of the maximum expected effective channel power gain of the mobile robotic user for the optimal IRS phase shifts. Based on the obtained channel power gain map, the communication-aware robot path planing problem is solved as a shortest path problem by exploiting graph theory. Numerical results show that: 1) Deploying an IRS can significantly extend the coverage of the AP and reduce the travelling distance of the mobile robotic user; 2) 2- or 3-bit IRS phase shifters can achieve nearly the same performance as continuous IRS phase shifters.      
### 21.Recurrent Neural Network Controllers for Signal Temporal Logic Specifications Subject to Safety Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2009.11468.pdf)
>  We propose a framework based on Recurrent Neural Networks (RNNs) to determine an optimal control strategy for a discrete-time system that is required to satisfy specifications given as Signal Temporal Logic (STL) formulae. RNNs can store information of a system over time, thus, enable us to determine satisfaction of the dynamic temporal requirements specified in STL formulae. Given a STL formula, a dataset of satisfying system executions and corresponding control policies, we can use RNNs to predict a control policy at each time based on the current and previous states of system. We use Control Barrier Functions (CBFs) to guarantee the safety of the predicted control policy. We validate our theoretical formulation and demonstrate its performance in an optimal control problem subject to partially unknown safety constraints through simulations.      
### 22.Deep Bayesian U-Nets for Efficient, Robust and Reliable Post-Disaster Damage Localization  [ :arrow_down: ](https://arxiv.org/pdf/2009.11460.pdf)
>  Post-disaster inspections are critical to emergency management after earthquakes. The availability of data on the condition of civil infrastructure immediately after an earthquake is of great importance for emergency management. Stakeholders require this information to take effective actions and to better recover from the disaster. The data-driven SHM has shown great promises to achieve this goal in near real-time. There have been several proposals to automate the inspection process from different sources of input using deep learning. The existing models in the literature only provide a final prediction output, while the risks of utilizing such models for safety-critical assessments should not be ignored. This paper is dedicated to developing deep Bayesian U-Nets where the uncertainty of predictions is a second output of the model, which is made possible through Monte Carlo dropout sampling in test time. Based on a grid-like data structure, the concept of semantic damage segmentation (SDS) is revisited. Compared to image segmentation, it is shown that a much higher level of precision is necessary for damage diagnosis. To validate and test the proposed framework, a benchmark dataset, 10,800 nonlinear response history analyses on a 10-story-10-bay 2D reinforced concrete moment frame, is utilized. Compared to the benchmark SDS model, Bayesian models exhibit superior robustness with enhanced global and mean class accuracies. Finally, the model's uncertainty output is studied by monitoring the softmax class variance of different predictions. It is shown that class variance correlates well with locations where the model makes mistakes. This output can be used in combination with the prediction results to increase the reliability of this data-driven framework in structural inspections.      
### 23.Packet Compressed Sensing Imaging (PCSI): Robust Image Transmission over Noisy Channels  [ :arrow_down: ](https://arxiv.org/pdf/2009.11455.pdf)
>  Packet Compressed Sensing Imaging (PCSI) is digital unconnected image transmission method resilient to packet loss. The goal is to develop a robust image transmission method that is computationally trivial to transmit (e.g., compatible with low-power 8-bit microcontrollers) and well suited for weak signal environments where packets are likely to be lost. In other image transmission techniques, noise and packet loss leads to parts of the image being distorted or missing. In PCSI, every packet contains random pixel information from the entire image, and each additional packet received (in any order) simply enhances image quality. Satisfactory SSTV resolution (320x240 pixel) images can be received in ~1-2 minutes when transmitted at 1200 baud AFSK, which is on par with analog SSTV transmission time. Image transmission and reception can occur simultaneously on a computer, and multiple images can be received from multiple stations simultaneously - allowing for the creation of "image nets." This paper presents a simple computer application for Windows, Mac, and Linux that implements PCSI transmission and reception on any KISS compatible hardware or software modem on any band and digital mode.      
### 24.Control Policies for Recovery of Interdependent Systems After Disruptions  [ :arrow_down: ](https://arxiv.org/pdf/2009.11453.pdf)
>  We examine a control problem where the states of the components of a system deteriorate after a disruption, if they are not being repaired by an entity. There exist a set of dependencies in the form of precedence constraints between the components, captured by a directed acyclic graph (DAG). The objective of the entity is to maximize the number of components whose states are brought back to the fully repaired state within a given time. We prove that the general problem is NP-hard, and therefore we characterize near-optimal control policies for special instances of the problem. We show that when the deterioration rates are larger than or equal to the repair rates and the precedence constraints are given by a DAG, it is optimal to continue repairing a component until its state reaches the fully recovered state before switching to repair any other component. Under the aforementioned assumptions and when the deterioration and the repair rates are homogeneous across all the components, we prove that the control policy that targets the healthiest component at each time-step while respecting the precedence and time constraints fully repairs at least half the number of components that would be fully repaired by an optimal policy. Finally, we prove that when the repair rates are sufficiently larger than the deterioration rates, the precedence constraints are given by a set of disjoint trees that each contain at most k nodes, and there is no time constraint, the policy that targets the component with the least value of health minus the deterioration rate at each time-step while respecting the precedence constraints fully repairs at least 1/k times the number of components that would be fully repaired by an optimal policy.      
### 25.Effects of Word-frequency based Pre- and Post- Processings for Audio Captioning  [ :arrow_down: ](https://arxiv.org/pdf/2009.11436.pdf)
>  The system we used for Task 6 (Automated Audio Captioning)of the Detection and Classification of Acoustic Scenes and Events(DCASE) 2020 Challenge combines three elements, namely, dataaugmentation, multi-task learning, and post-processing, for audiocaptioning. The system received the highest evaluation scores, butwhich of the individual elements most fully contributed to its perfor-mance has not yet been clarified. Here, to asses their contributions,we first conducted an element-wise ablation study on our systemto estimate to what extent each element is effective. We then con-ducted a detailed module-wise ablation study to further clarify thekey processing modules for improving accuracy. The results showthat data augmentation and post-processing significantly improvethe score in our system. In particular, mix-up data augmentationand beam search in post-processing improve SPIDEr by 0.8 and 1.6points, respectively.      
### 26.A Research Review on Detection and Classification of Power Quality Disturbances caused by Integration of Renewable Energy Sources  [ :arrow_down: ](https://arxiv.org/pdf/2009.11426.pdf)
>  With the increased interest in integrating renewable energy sources (RES) such as wind power and solar into the power systems owing to their zero greenhouse gas emissions and the involvement of power converters for integration in grid, the detection, classification and mitigation of power quality events has become indispensable. For employing an appropriate mitigation technique, it is a pre-requisite to correctly classify the various types of disturbances in power quality. This paper, therefore, presents a detailed research reviews on detection and classification of power quality disturbances caused by the integration of renewable energy sources and associated works present in literature till date. Attempts are also made to highlight the current and future issues involved in the detection, classification and mitigation of PQ disturbances. Best efforts have been made to make this paper serve as a full-fledged reference for the future work in this field. A list of 230 research publications on the subject is also appended for quick reference.      
### 27.Experimental Demonstration of 4,294,967,296-QAM Based Y-00 Quantum Stream Cipher Carrying 160-Gb/s 16-QAM Signals  [ :arrow_down: ](https://arxiv.org/pdf/2009.11412.pdf)
>  We demonstrate a 4,294,967,296-ary quadrature amplitude modulation (QAM) based Y-00 quantum stream cipher system carrying 160-Gb/s 16-QAM signal transmitted over 320-km SSMF. The ultra-dense QAM cipher template is realized by an integrated two-segment silicon photonics I/Q modulator.      
### 28.FluentNet: End-to-End Detection of Speech Disfluency with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.11394.pdf)
>  Strong presentation skills are valuable and sought-after in workplace and classroom environments alike. Of the possible improvements to vocal presentations, disfluencies and stutters in particular remain one of the most common and prominent factors of someone's demonstration. Millions of people are affected by stuttering and other speech disfluencies, with the majority of the world having experienced mild stutters while communicating under stressful conditions. While there has been much research in the field of automatic speech recognition and language models, there lacks the sufficient body of work when it comes to disfluency detection and recognition. To this end, we propose an end-to-end deep neural network, FluentNet, capable of detecting a number of different disfluency types. FluentNet consists of a Squeeze-and-Excitation Residual convolutional neural network which facilitate the learning of strong spectral frame-level representations, followed by a set of bidirectional long short-term memory layers that aid in learning effective temporal relationships. Lastly, FluentNet uses an attention mechanism to focus on the important parts of speech to obtain a better performance. We perform a number of different experiments, comparisons, and ablation studies to evaluate our model. Our model achieves state-of-the-art results by outperforming other solutions in the field on the publicly available UCLASS dataset. Additionally, we present LibriStutter: a disfluency dataset based on the public LibriSpeech dataset with synthesized stutters. We also evaluate FluentNet on this dataset, showing the strong performance of our model versus a number of benchmark techniques.      
### 29.Optimal Minimax Mobile Sensor Scheduling Over a Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.11386.pdf)
>  We investigate the problem of monitoring multiple targets using a single mobile sensor, with the goal of minimizing the maximum estimation error among all the targets over long time horizons. The sensor can move in a network-constrained structure, where it has to plan which targets to visit and for how long to dwell at each node. We prove that in an optimal observation time allocation, the peak uncertainty is the same among all the targets. By further restricting the agent policy to only visit each target once every cycle, we develop a scheme to optimize the agent's behavior that is significantly simpler computationally when compared to previous approaches for similar problems.      
### 30.ADMM-DIPTV: combining Total Variation and Deep Image Prior for image restoration  [ :arrow_down: ](https://arxiv.org/pdf/2009.11380.pdf)
>  In the last decades, unsupervised deep learning based methods have caught researchers attention, since in many applications collecting a great amount of training examples is not always feasible. Moreover, the construction of a good training set is time consuming and hard because the selected data have to be enough representative for the task. In this paper, we mainly focus on the Deep Image Prior (DIP) framework powered by adding the Total Variation regularizer which promotes gradient-sparsity of the solution. Differently from other existing approaches, we solve the arising minimization problem by using the well known Alternating Direction Method of Multipliers (ADMM) framework, decoupling the contribution of the DIP $L_{2}$-norm and Total Variation terms. The promising performances of the proposed approach, in terms of PSNR and SSIM values, are addressed by means of experiments for different image restoration tasks on synthetic as well as on real data.      
### 31.Fast Adaptation Nonlinear Observer for SLAM  [ :arrow_down: ](https://arxiv.org/pdf/2009.11374.pdf)
>  The process of simultaneously mapping the environment in three dimensional (3D) space and localizing a moving vehicle's pose (orientation and position) is termed Simultaneous Localization and Mapping (SLAM). SLAM is a core task in robotics applications. In the SLAM problem, each of the vehicle's pose and the environment are assumed to be completely unknown. This paper takes the conventional SLAM design as a basis and proposes a novel approach that ensures fast adaptation of the nonlinear observer for SLAM. Due to the fact that the true SLAM problem is nonlinear and is modeled on the Lie group of $\mathbb{SLAM}_{n}\left(3\right)$, the proposed observer for SLAM is nonlinear and modeled on $\mathbb{SLAM}_{n}\left(3\right)$. The proposed observer compensates for unknown bias attached to velocity measurements. The results of the simulation illustrate the robustness of the proposed approach.      
### 32.Low Complexity Neural Network Structures for Self-Interference Cancellation in Full-Duplex Radio  [ :arrow_down: ](https://arxiv.org/pdf/2009.11361.pdf)
>  Self-interference (SI) is considered as a main challenge in full-duplex (FD) systems. Therefore, efficient SI cancelers are required for the influential deployment of FD systems in beyond fifth-generation wireless networks. Existing methods for SI cancellation have mostly considered the polynomial representation of the SI signal at the receiver. These methods are shown to operate well in practice while requiring high computational complexity. Alternatively, neural networks (NNs) are envisioned as promising candidates for modeling the SI signal with reduced computational complexity. Consequently, in this paper, two novel low complexity NN structures, referred to as the ladder-wise grid structure (LWGS) and moving-window grid structure (MWGS), are proposed. The core idea of these two structures is to mimic the non-linearity and memory effect introduced to the SI signal in order to achieve proper SI cancellation while exhibiting low computational complexity. The simulation results reveal that the LWGS and MWGS NN-based cancelers attain the same cancellation performance of the polynomial-based canceler while providing 49.87% and 34.19% complexity reduction, respectively.      
### 33.A Deep Learning Algorithm for Objective Assessment of Hypernasality in Children with Cleft Palate  [ :arrow_down: ](https://arxiv.org/pdf/2009.11354.pdf)
>  Objectives: Evaluation of hypernasality requires extensive perceptual training by clinicians and extending this training on a large scale internationally is untenable; this compounds the health disparities that already exist among children with cleft. In this work, we present the objective hypernasality measure (OHM), a speech analytics algorithm that automatically measures hypernasality in speech, and validate it relative to a group of trained clinicians. Methods: We trained a deep neural network (DNN) on approximately 100 hours of a publicly-available healthy speech corpus to detect the presence of nasal acoustic cues generated through the production of nasal consonants and nasalized phonemes in speech. Importantly, this model does not require any clinical data for training. The posterior probabilities of the deep learning model were aggregated at the sentence and speaker-levels to compute the OHM. <br>Results: The results showed that the OHM was significantly correlated with the perceptual hypernasality ratings in the Americleft database ( r=0.797, ~p$&lt;$0.001), and with the New Mexico Cleft Palate Center (NMCPC) database (r=0.713,p&lt;$0.001). In addition, we evaluated the relationship between the OHM and articulation errors; the sensitivity of the OHM in detecting the presence of very mild hypernasality; and establishing the internal reliability of the metric. Further, the performance of OHM was compared with a DNN regression algorithm directly trained on the hypernasal speech samples. Significance: The results indicate that the OHM is able to rate the severity of hypernasality on par with Americleft-trained clinicians on this dataset.      
### 34.Generative Modelling of 3D in-silico Spongiosa with Controllable Micro-Structural Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2009.11327.pdf)
>  Research in vertebral bone micro-structure generally requires costly procedures to obtain physical scans of real bone with a specific pathology under study, since no methods are available yet to generate realistic bone structures in-silico. Here we propose to apply recent advances in generative adversarial networks (GANs) to develop such a method. We adapted style-transfer techniques, which have been largely used in other contexts, in order to transfer style between image pairs while preserving its informational content. In a first step, we trained a volumetric generative model in a progressive manner using a Wasserstein objective and gradient penalty (PWGAN-GP) to create patches of realistic bone structure in-silico. The training set contained 7660 purely spongeous bone samples from twelve human vertebrae (T12 or L1) with isotropic resolution of 164um and scanned with a high resolution peripheral quantitative CT (Scanco XCT). After training, we generated new samples with tailored micro-structure properties by optimizing a vector z in the learned latent space. To solve this optimization problem, we formulated a differentiable goal function that leads to valid samples while compromising the appearance (content) with target 3D properties (style). Properties of the learned latent space effectively matched the data distribution. Furthermore, we were able to simulate the resulting bone structure after deterioration or treatment effects of osteoporosis therapies based only on expected changes of micro-structural parameters. Our method allows to generate a virtually infinite number of patches of realistic bone micro-structure, and thereby likely serves for the development of bone-biomarkers and to simulate bone therapies in advance.      
### 35.On the Uniqueness of Inverse Problems with Fourier-domain Measurements and Generalized TV Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2009.11855.pdf)
>  We study the super-resolution problem of recovering a periodic continuous-domain function from its low-frequency information. This means that we only have access to possibly corrupted versions of its Fourier samples up to a maximum cut-off frequency. The reconstruction task is specified as an optimization problem with generalized total-variation regularization involving a pseudo-differential operator. Our special emphasis is on the uniqueness of solutions. We show that, for elliptic regularization operators (e.g., the derivatives of any order), uniqueness is always guaranteed. To achieve this goal, we provide a new analysis of constrained optimization problems over Radon measures. We demonstrate that either the solutions are always made of Radon measures of constant sign, or the solution is unique. Doing so, we identify a general sufficient condition for the uniqueness of the solution of a constrained optimization problem with TV-regularization, expressed in terms of the Fourier samples.      
### 36.Timbre Space Representation of a Subtractive Synthesizer  [ :arrow_down: ](https://arxiv.org/pdf/2009.11706.pdf)
>  In this study, we produce a geometrically scaled perceptual timbre space from dissimilarity ratings of subtractive synthesized sounds and correlate the resulting dimensions with a set of acoustic descriptors. We curate a set of 15 sounds, produced by a synthesis model that uses varying source waveforms, frequency modulation (FM) and a lowpass filter with an enveloped cutoff frequency. Pairwise dissimilarity ratings were collected within an online browser-based experiment. We hypothesized that a varied waveform input source and enveloped filter would act as the main vehicles for timbral variation, providing novel acoustic correlates for the perception of synthesized timbres.      
### 37.Kirchhoff's Circuit Law Applications to Graph Simplification in Search Problems  [ :arrow_down: ](https://arxiv.org/pdf/2009.11675.pdf)
>  This paper proposes a new analysis of graph using the concept of electric potential, and also proposes a graph simplification method based on this analysis. Suppose that each node in the weighted-graph has its respective potential value. Furthermore, suppose that the start and terminal nodes in graphs have maximum and zero potentials, respectively. When we let the level of each node be defined as the minimum number of edges/hops from the start node to the node, the proper potential of each level can be estimated based on geometric proportionality relationship. Based on the estimated potential for each level, we can re-design the graph for path-finding problems to be the electrical circuits, thus Kirchhoff's Circuit Law can be directed applicable for simplifying the graph for path-finding problems.      
### 38.The COUGHVID crowdsourcing dataset: A corpus for the study of large-scale cough analysis algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2009.11644.pdf)
>  Cough audio signal classification has been successfully used to diagnose a variety of respiratory conditions, and there has been significant interest in leveraging Machine Learning (ML) to provide widespread COVID-19 screening. However, there is currently no validated database of cough sounds with which to train such ML models. The COUGHVID dataset provides over 20,000 crowdsourced cough recordings representing a wide range of subject ages, genders, geographic locations, and COVID-19 statuses. First, we filtered the dataset using our open-sourced cough detection algorithm. Second, experienced pulmonologists labeled more than 2,000 recordings to diagnose medical abnormalities present in the coughs, thereby contributing one of the largest expert-labeled cough datasets in existence that can be used for a plethora of cough audio classification tasks. Finally, we ensured that coughs labeled as symptomatic and COVID-19 originate from countries with high infection rates, and that their expert labels are consistent. As a result, the COUGHVID dataset contributes a wealth of cough recordings for training ML models to address the world's most urgent health crises.      
### 39.Koopman Resolvent: A Laplace-Domain Analysis of Nonlinear Autonomous Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.11544.pdf)
>  The motivation of our research is to establish a Laplace-domain theory that provides principles and methodology to analyze and synthesize systems with nonlinear dynamics. A semigroup of composition operators defined for nonlinear autonomous dynamical systems---the Koopman semigroup and its associated Koopman generator---plays a central role in this study. We introduce the resolvent of the Koopman generator, which we call the Koopman resolvent, and provide its spectral characterization for three types of nonlinear dynamics: ergodic evolution on an attractor, convergence to a stable equilibrium point, and convergence to a (quasi-)stable limit cycle. This shows that the Koopman resolvent provides the Laplace-domain representation of such nonlinear autonomous dynamics. A computational aspect of the Laplace-domain representation is also discussed with emphasis on non-stationary Koopman modes.      
### 40.Robust Finite-State Controllers for Uncertain POMDPs  [ :arrow_down: ](https://arxiv.org/pdf/2009.11459.pdf)
>  Uncertain partially observable Markov decision processes (uPOMDPs) allow the probabilistic transition and observation functions of standard POMDPs to belong to a so-called uncertainty set. Such uncertainty sets capture uncountable sets of probability distributions. We develop an algorithm to compute finite-memory policies for uPOMDPs that robustly satisfy given specifications against any admissible distribution. In general, computing such policies is both theoretically and practically intractable. We provide an efficient solution to this problem in four steps. (1) We state the underlying problem as a nonconvex optimization problem with infinitely many constraints. (2) A dedicated dualization scheme yields a dual problem that is still nonconvex but has finitely many constraints. (3) We linearize this dual problem and (4) solve the resulting finite linear program to obtain locally optimal solutions to the original problem. The resulting problem formulation is exponentially smaller than those resulting from existing methods. We demonstrate the applicability of our algorithm using large instances of an aircraft collision-avoidance scenario and a novel spacecraft motion planning case study.      
### 41.Automatic identification of fossils and abiotic grains during carbonate microfacies analysis using deep convolutional neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.11429.pdf)
>  Petrographic analysis based on microfacies identification in thin sections is widely used in sedimentary environment interpretation and paleoecological reconstruction. Fossil recognition from microfacies is an essential procedure for petrographers to complete this task. Distinguishing the morphological and microstructural diversity of skeletal fragments requires extensive prior knowledge of fossil morphotypes in microfacies and long training sessions under the microscope. This requirement engenders certain challenges for sedimentologists and paleontologists, especially novices. However, a machine classifier can help address this challenge. We collected a microfacies image dataset comprising both public data from 1,149 references and our own materials (including a total of 30,815 images of 22 fossil and abiotic grain groups). We employed a high-performance workstation to implement four classic deep convolutional neural networks (DCNNs), which have proven to be highly efficient in computer vision over the last several years. Our framework uses a transfer learning technique, which reuses the pre-trained parameters that are trained on a larger ImageNet dataset as initialization for the network to achieve high accuracy with low computing costs. We obtained up to 95% of the top one and 99% of the top three test accuracies in the Inception ResNet v2 architecture. The machine classifier exhibited 0.99 precision on minerals, such as dolomite and pyrite. Although it had some difficulty on samples having similar morphologies, such as the bivalve, brachiopod, and ostracod, it nevertheless obtained 0.88 precision. Our machine learning framework demonstrated high accuracy with reproducibility and bias avoidance that was comparable to those of human classifiers. Its application can thus eliminate much of the tedious, manually intensive efforts by human experts conducting routine identification.      
### 42.A Sample-Efficient Algorithm for Episodic Finite-Horizon MDP with Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2009.11348.pdf)
>  Constrained Markov Decision Processes (CMDPs) formalize sequential decision-making problems whose objective is to minimize a cost function while satisfying constraints on various cost functions. In this paper, we consider the setting of episodic fixed-horizon CMDPs. We propose an online algorithm which leverages the linear programming formulation of finite-horizon CMDP for repeated optimistic planning to provide a probably approximately correct (PAC) guarantee on the number of episodes needed to ensure an $\epsilon$-optimal policy, i.e., with resulting objective value within $\epsilon$ of the optimal value and satisfying the constraints within $\epsilon$-tolerance, with probability at least $1-\delta$. The number of episodes needed is shown to be of the order $\tilde{\mathcal{O}}\big(\frac{|S||A|C^{2}H^{2}}{\epsilon^{2}}\log\frac{1}{\delta}\big)$, where $C$ is the upper bound on the number of possible successor states for a state-action pair. Therefore, if $C \ll |S|$, the number of episodes needed have a linear dependence on the state and action space sizes $|S|$ and $|A|$, respectively, and quadratic dependence on the time horizon $H$.      
