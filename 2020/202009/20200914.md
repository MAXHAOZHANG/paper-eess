# ArXiv eess --Mon, 14 Sep 2020
### 1.Deep Analog-to-Digital Converter for Wireless Communication  [ :arrow_down: ](https://arxiv.org/pdf/2009.05553.pdf)
>  With the advent of the 5G wireless networks, achieving tens of gigabits per second throughputs and low, milliseconds, latency has become a reality. This level of performance will fuel numerous real-time applications, such as autonomy and augmented reality, where the computationally heavy tasks can be performed in the cloud. The increase in the bandwidth along with the use of dense constellations places a significant burden on the speed and accuracy of analog-to-digital converters (ADC). A popular approach to create wideband ADCs is utilizing multiple channels each operating at a lower speed in the time-interleaved fashion. However, an interleaved ADC comes with its own set of challenges. The parallel architecture is very sensitive to the inter-channel mismatch, timing jitter, clock skew between different ADC channels as well as the nonlinearity within individual channels. Consequently, complex post-calibration is required using digital signal processing (DSP) after the ADC. The traditional DSP calibration consumes a significant amount of power and its design requires knowledge of the source and type of errors which are becoming increasingly difficult to predict in nanometer CMOS processes. In this paper, instead of individually targeting each source of error, we utilize a deep learning algorithm to learn the complete and complex ADC behavior and to compensate for it in realtime. We demonstrate this "Deep ADC" technique on an 8G Sample/s 8-channel time-interleaved ADC with the QAM-OFDM modulated data. Simulation results for different QAM symbol constellations and OFDM subcarriers show dramatic improvements of approximately 5 bits in the dynamic range with a concomitant drastic reduction in symbol error rate. We further discuss the hardware implementation including latency, power consumption, memory requirements, and chip area.      
### 2.Deep Learning Interference Cancellation in Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.05533.pdf)
>  With the crowding of the electromagnetic spectrum and the shrinking cell size in wireless networks, crosstalk between base stations and users is a major problem. Although hand-crafted functional blocks and coding schemes are proven effective to guarantee reliable data transfer, currently deep learning-based approaches have drawn increasing attention in the communication system modeling. In this paper, we propose a Neural Network (NN) based signal processing technique that works with traditional DSP algorithms to overcome the interference problem in realtime. This technique doesn't require any feedback protocol between the receiver and transmitter which makes it very suitable for low-latency and high data-rate applications such as autonomy and augmented reality. While there has been recent work on the use of Reinforcement Learning (RL) in the control layer to manage and control the interference, our approach is novel in the sense that it introduces a neural network for signal processing at baseband data rate and in the physical layer. We demonstrate this "Deep Interference Cancellation" technique using a convolutional LSTM autoencoder. When applied to QAM-OFDM modulated data, the network produces significant improvement in the symbol error rate (SER). We further discuss the hardware implementation including latency, power consumption, memory requirements, and chip area.      
### 3.Smart Jamming Attacks in 5G New Radio: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2009.05531.pdf)
>  The fifth generation of wireless cellular networks (5G) is expected to be the infrastructure for emergency services, natural disasters rescue, public safety, and military communications. 5G, as any previous wireless cellular network, is vulnerable to jamming attacks, which create deliberate interference to hinder the communication of legitimate users. Therefore, jamming 5G networks can be a real threat to public safety. Thus, there is a strong need to investigate to what extent these networks are vulnerable to jamming attacks. For this investigation, we consider the 3GPP standard released in 2017, which is widely accepted as the primary reference for the deployment of these networks. First, we describe the key elements of 5G New Radio (NR) architecture, such as different channels and signals exchanged between the base station and user equipment. Second, we conduct an in-depth review of the jamming attack models and we assess the 5G NR vulnerabilities to these jamming attacks. Then, we present the state-of-the-art detection and mitigation techniques, and we discuss their suitability to defeat smart jammers in 5G wireless networks. Finally, we provide some recommendations and future research directions at the end of this paper.      
### 4.On Multitask Loss Function for Audio Event Detection and Localization  [ :arrow_down: ](https://arxiv.org/pdf/2009.05527.pdf)
>  Audio event localization and detection (SELD) have been commonly tackled using multitask models. Such a model usually consists of a multi-label event classification branch with sigmoid cross-entropy loss for event activity detection and a regression branch with mean squared error loss for direction-of-arrival estimation. In this work, we propose a multitask regression model, in which both (multi-label) event detection and localization are formulated as regression problems and use the mean squared error loss homogeneously for model training. We show that the common combination of heterogeneous loss functions causes the network to underfit the data whereas the homogeneous mean squared error loss leads to better convergence and performance. Experiments on the development and validation sets of the DCASE 2020 SELD task demonstrate that the proposed system also outperforms the DCASE 2020 SELD baseline across all the detection and localization metrics, reducing the overall SELD error (the combined metric) by approximately 10% absolute.      
### 5.RF-Based Low-SNR Classification of UAVs Using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.05519.pdf)
>  This paper investigates the problem of classification of unmanned aerial vehicles (UAVs) from radio frequency (RF) fingerprints at the low signal-to-noise ratio (SNR) regime. We use convolutional neural networks (CNNs) trained with both RF time-series images and the spectrograms of 15 different off-the-shelf drone controller RF signals. When using time-series signal images, the CNN extracts features from the signal transient and envelope. As the SNR decreases, this approach fails dramatically because the information in the transient is lost in the noise, and the envelope is distorted heavily. In contrast to time-series representation of the RF signals, with spectrograms, it is possible to focus only on the desired frequency interval, i.e., 2.4 GHz ISM band, and filter out any other signal component outside of this band. These advantages provide a notable performance improvement over the time-series signals-based methods. To further increase the classification accuracy of the spectrogram-based CNN, we denoise the spectrogram images by truncating them to a limited spectral density interval. Creating a single model using spectrogram images of noisy signals and tuning the CNN model parameters, we achieve a classification accuracy varying from 92% to 100% for an SNR range from -10 dB to 30 dB, which significantly outperforms the existing approaches to our best knowledge.      
### 6.Analytical Voltage Sensitivity Analysis for Unbalanced Power Distribution System  [ :arrow_down: ](https://arxiv.org/pdf/2009.05513.pdf)
>  Large scale integration of distributed energy resources and electric vehicles in a transactive energy environment present new challenges in terms of voltage stability and fluctuations in a power distribution system. The impact of different level of DER/EV penetration on the voltages across the network is typically quantified through voltage sensitivity analyses. Existing methods of voltage sensitivity analysis are computationally expensive and prior efforts to develop analytical approximation lacks generality and have not been effectively validated. The objective of this work is to provide a new analytical method of voltage sensitivity analysis that has low computational cost and also allows for stochastic analysis of voltage change. This paper first derives an analytical approximation of change in voltage at a particular bus due to change in power consumption at other bus in a radial three phase unbalanced power distribution system. Then, the proposed method is shown to be valid for different load configurations, which demonstrates its generality. The results from our analytical approach is validated via classical load flow simulation of the test system based on IEEE 37 bus network. The proposed method is shown to have good accuracy, and computation complexity is of order O(1), compared to O(n3) in classical sensitivity analysis approaches.      
### 7.When Probabilistic Shaping Realizes Improper Signaling for Hardware Distortion Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2009.05510.pdf)
>  Hardware distortions (HWD) render drastic effects on the performance of communication systems. They are recently proven to bear asymmetric signatures; and hence can be efficiently mitigated using improper Gaussian signaling (IGS), thanks to its additional design degrees of freedom. Discrete asymmetric signaling (AS) can practically realize the IGS by shaping the signals' geometry or probability. In this paper, we adopt the probabilistic shaping (PS) instead of uniform symbols to mitigate the impact of HWD and derive the optimal maximum a posterior detector. Then, we design the symbols' probabilities to minimize the error rate performance while accommodating the improper nature of HWD. Although the design problem is a non-convex optimization problem, we simplified it using successive convex programming and propose an iterative algorithm. We further present a hybrid shaping (HS) design to gain the combined benefits of both PS and geometric shaping (GS). Finally, extensive numerical results and Monte-Carlo simulations highlight the superiority of the proposed PS over conventional uniform constellation and GS. Both PS and HS achieve substantial improvements over the traditional uniform constellation and GS with up to one order magnitude in error probability and throughput.      
### 8.RECOApy: Data recording, pre-processing and phonetic transcription for end-to-end speech-based applications  [ :arrow_down: ](https://arxiv.org/pdf/2009.05493.pdf)
>  Deep learning enables the development of efficient end-to-end speech processing applications while bypassing the need for expert linguistic and signal processing features. Yet, recent studies show that good quality speech resources and phonetic transcription of the training data can enhance the results of these applications. In this paper, the RECOApy tool is introduced. RECOApy streamlines the steps of data recording and pre-processing required in end-to-end speech-based applications. The tool implements an easy-to-use interface for prompted speech recording, spectrogram and waveform analysis, utterance-level normalisation and silence trimming, as well grapheme-to-phoneme conversion of the prompts in eight languages: Czech, English, French, German, Italian, Polish, Romanian and Spanish. <br>The grapheme-to-phoneme (G2P) converters are deep neural network (DNN) based architectures trained on lexicons extracted from the Wiktionary online collaborative resource. With the different degree of orthographic transparency, as well as the varying amount of phonetic entries across the languages, the DNN's hyperparameters are optimised with an evolution strategy. The phoneme and word error rates of the resulting G2P converters are presented and discussed. The tool, the processed phonetic lexicons and trained G2P models are made freely available.      
### 9.Text-Independent Speaker Verification with Dual Attention Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.05485.pdf)
>  This paper presents a novel design of attention model for text-independent speaker verification. The model takes a pair of input utterances and generates an utterance-level embedding to represent speaker-specific characteristics in each utterance. The input utterances are expected to have highly similar embeddings if they are from the same speaker. The proposed attention model consists of a self-attention module and a mutual attention module, which jointly contributes to the generation of the utterance-level embedding. The self-attention weights are computed from the utterance itself while the mutual-attention weights are computed with the involvement of the other utterance in the input pairs. As a result, each utterance is represented by a self-attention weighted embedding and a mutual-attention weighted embedding. The similarity between the embeddings is measured by a cosine distance score and a binary classifier output score. The whole model, named Dual Attention Network, is trained end-to-end on Voxceleb database. The evaluation results on Voxceleb 1 test set show that the Dual Attention Network significantly outperforms the baseline systems. The best result yields an equal error rate of 1:6%.      
### 10.Boosting the Sliding Frank-Wolfe solver for 3D deconvolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.05473.pdf)
>  In the context of gridless sparse optimization, the Sliding Frank Wolfe algorithm recently introduced has shown interesting analytical and practical properties. Nevertheless, is application to large data, such as in the case of 3D deconvolution, is computationally heavy. In this paper, we investigate a strategy for leveraging this burden, in order to make this method more tractable for 3D deconvolution. We show that a boosted SFW can achieve the same results in a significantly reduced amount of time.      
### 11.Allocation of locally generated electricity in renewable energy communities  [ :arrow_down: ](https://arxiv.org/pdf/2009.05411.pdf)
>  This paper introduces a methodology to perform an ex-post allocation of locally generated electricity among the members of a renewable energy community. Such an ex-post allocation takes place in a settlement phase where the financial exchanges of the community are based on the production and consumption profiles of each member. The proposed methodology consists of an optimisation framework which (i) minimises the sum of individual electricity costs of the community members, and (ii) can enforce minimum self-sufficiency rates --proportion of electricity consumption covered by local production-- on each member, enhancing the economic gains of some of them. The latter capability aims to ensure that members receive enough incentives to participate in the renewable energy community. This framework is designed so as to provide a practical approach that is ready to use by community managers, which is compliant with current legislation on renewable energy communities. It computes a set of optimal repartition keys, which represent the percentage of total local production given to each member -- one key per metering period per member. These keys are computed based on an initial set of keys provided in the simulation, which are typically contractual i.e., agreed upon between the member and the manager the renewable energy community. This methodology is tested in a broad range of scenarios, illustrating its ability to optimise the operational costs of a renewable energy community.      
### 12.On the estimation of spatial density from mobile network operator data  [ :arrow_down: ](https://arxiv.org/pdf/2009.05410.pdf)
>  Mobile Network Operator (MNO) data are increasingly used to infer mobility and presence patterns of human population. Whether based on Cell Detail Records (CDR) or signalling events, MNO data can be leveraged to estimate the spatial distribution of mobile devices at a given time, and from there extrapolate the distribution of humans. The process of transforming MNO data to a density map involves a chain of multiple processing blocks. A key block relates to \emph{geo-location} of individual radio cells or groups thereof, i.e., to methods for determining the spatial footprint of radio cells. Traditionally, researchers have resorted to geo-location methods based on Voronoi tessellations or variants thereof: with this approach, cell locations are mutually disjoint and the density estimation task reduces to a simple area-proportional computation. More recently, some pioneering work have started to consider more elaborated geo-location methods with partially overlapping (non-disjont) cell footprints. Estimating the spatial density from a set of overlapping cell locations is currently an open research problem, and it is the focus of this work. In this contribution we start by reviewing three different estimation methods proposed in the literature, for which we provide novel analytic results, based on formal proofs, unveiling some key aspects of their mutual relationships. Furthermore, we develop a novel estimator for which a closed-form solution can be given. Numerical results based on synthetic data are presented to assess the relative accuracy of each method.      
### 13.Non-linear fitting with joint spatial regularization in Arterial Spin Labeling  [ :arrow_down: ](https://arxiv.org/pdf/2009.05409.pdf)
>  Multi-Delay single-shot arterial spin labeling (ASL) imaging provides accurate cerebral blood flow (CBF) and, in addition, arterial transit time (ATT) maps but the inherent low SNR can be challenging. Especially standard fitting using non-linear least squares often fails in regions with poor SNR, resulting in noisy estimates of the quantitative maps. State-of-the-art fitting techniques improve the SNR by incorporating prior knowledge in the estimation process which typically leads to spatial blurring. To this end, we propose a new estimation method with a joint spatial total generalized variation regularization on CBF and ATT. This joint regularization approach utilizes shared spatial features across maps to enhance sharpness and simultaneously improves noise suppression in the final estimates. The proposed method is validated in three stages, first on synthetic phantom data, including pathologies, followed by in vivo acquisitions of healthy volunteers, and finally on patient data following an ischemic stroke. The quantitative estimates are compared to two reference methods, non-linear least squares fitting and a state-of-the-art ASL quantification algorithm based on Bayesian inference. The proposed joint regularization approach outperforms the reference implementations, substantially increasing the SNR in CBF and ATT while maintaining sharpness and quantitative accuracy in the estimates.      
### 14.TRIER: Template-Guided Neural Networks for Robust and Interpretable Sleep Stage Identification from EEG Recordings  [ :arrow_down: ](https://arxiv.org/pdf/2009.05407.pdf)
>  Neural networks often obtain sub-optimal representations during training, which degrade robustness as well as classification performances. This is a severe problem in applying deep learning to bio-medical domains, since models are vulnerable to being harmed by irregularities and scarcities in data. In this study, we propose a pre-training technique that handles this challenge in sleep staging tasks. Inspired by conventional methods that experienced physicians have used to classify sleep states from the existence of characteristic waveform shapes, or template patterns, our method introduces a cosine similarity based convolutional neural network to extract representative waveforms from training data. Afterwards, these features guide a model to construct representations based on template patterns. Through extensive experiments, we demonstrated that guiding a neural network with template patterns is an effective approach for sleep staging, since (1) classification performances are significantly enhanced and (2) robustness in several aspects are improved. Last but not least, interpretations on models showed that notable features exploited by trained experts are correctly addressed during prediction in the proposed method.      
### 15.Phase Sampling Profilometry  [ :arrow_down: ](https://arxiv.org/pdf/2009.05406.pdf)
>  Structured light 3D surface imaging is a school of techniques in which structured light patterns are used for measuring the depth map of the object. Among all the designed structured light patterns, phase pattern has become most popular because of its high resolution and high accuracy. Accordingly, phase measuring profolimetry (PMP) has become the mainstream of structured light technology. In this letter, we introduce the concept of phase sampling profilometry (PSP) that calculates the phase unambiguously in the spatial-frequency domain with only one pattern image. Therefore, PSP is capable of measuring the 3D shapes of the moving objects robustly with single-shot.      
### 16.Semantic Segmentation of Histopathological Slides for the Classification of Cutaneous Lymphoma and Eczema  [ :arrow_down: ](https://arxiv.org/pdf/2009.05403.pdf)
>  Mycosis fungoides (MF) is a rare, potentially life threatening skin disease, which in early stages clinically and histologically strongly resembles Eczema, a very common and benign skin condition. In order to increase the survival rate, one needs to provide the appropriate treatment early on. To this end, one crucial step for specialists is the evaluation of histopathological slides (glass slides), or Whole Slide Images (WSI), of the patients' skin tissue. We introduce a deep learning aided diagnostics tool that brings a two-fold value to the decision process of pathologists. First, our algorithm accurately segments WSI into regions that are relevant for an accurate diagnosis, achieving a Mean-IoU of 69% and a Matthews Correlation score of 83% on a novel dataset. Additionally, we also show that our model is competitive with the state of the art on a reference dataset. Second, using the segmentation map and the original image, we are able to predict if a patient has MF or Eczema. We created two models that can be applied in different stages of the diagnostic pipeline, potentially eliminating life-threatening mistakes. The classification outcome is considerably more interpretable than using only the WSI as the input, since it is also based on the segmentation map. Our segmentation model, which we call EU-Net, extends a classical U-Net with an EfficientNet-B7 encoder which was pre-trained on the Imagenet dataset.      
### 17.Investigation of analog signal distortion introduced by a fiber phase sensitive amplifier  [ :arrow_down: ](https://arxiv.org/pdf/2009.05399.pdf)
>  We numerically simulate the distortion of an analog signal carried in a microwave photonics link containing a phase sensitive amplifier (PSA), focusing mainly on amplitude modulation format. The numerical model is validated by comparison with experimental measurements. By using the well known two-tone test, we compare the situations in which a standard intensity modulator is used with the one where a perfectly linear modulator would be employed. We also investigate the role of gain saturation on the nonlinearity of the PSA. Finally, we establish the conditions, in which the signal nonlinearity introduced by the PSA itself can be extremely small.      
### 18.COVIDNet-CT: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.05383.pdf)
>  The coronavirus disease 2019 (COVID-19) pandemic continues to have a tremendous impact on patients and healthcare systems around the world. In the fight against this novel disease, there is a pressing need for rapid and effective screening tools to identify patients infected with COVID-19, and to this end CT imaging has been proposed as one of the key screening methods which may be used as a complement to RT-PCR testing, particularly in situations where patients undergo routine CT scans for non-COVID-19 related reasons, patients with worsening respiratory status or developing complications that require expedited care, and patients suspected to be COVID-19-positive but have negative RT-PCR test results. Motivated by this, in this study we introduce COVIDNet-CT, a deep convolutional neural network architecture that is tailored for detection of COVID-19 cases from chest CT images via a machine-driven design exploration approach. Additionally, we introduce COVIDx-CT, a benchmark CT image dataset derived from CT imaging data collected by the China National Center for Bioinformation comprising 104,009 images across 1,489 patient cases. Furthermore, in the interest of reliability and transparency, we leverage an explainability-driven performance validation strategy to investigate the decision-making behaviour of COVIDNet-CT, and in doing so ensure that COVIDNet-CT makes predictions based on relevant indicators in CT images. Both COVIDNet-CT and the COVIDx-CT dataset are available to the general public in an open-source and open access manner as part of the COVID-Net initiative. While COVIDNet-CT is not yet a production-ready screening solution, we hope that releasing the model and dataset will encourage researchers, clinicians, and citizen data scientists alike to leverage and build upon them.      
### 19.L2-Constrained RemNet for Camera Model Identification and Image Manipulation Detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.05379.pdf)
>  Source camera model identification (CMI) and image manipulation detection are of paramount importance in image forensics. In this paper, we propose an L2-constrained Remnant Convolutional Neural Network (L2-constrained RemNet) for performing these two crucial tasks. The proposed network architecture consists of a dynamic preprocessor block and a classification block. An L2 loss is applied to the output of the preprocessor block, and categorical crossentropy loss is calculated based on the output of the classification block. The whole network is trained in an end-to-end manner by minimizing the total loss, which is a combination of the L2 loss and the categorical crossentropy loss. Aided by the L2 loss, the data-adaptive preprocessor learns to suppress the unnecessary image contents and assists the classification block in extracting robust image forensics features. We train and test the network on the Dresden database and achieve an overall accuracy of 98.15%, where all the test images are from devices and scenes not used during training to replicate practical applications. The network also outperforms other state-of-the-art CNNs even when the images are manipulated. Furthermore, we attain an overall accuracy of 99.68% in image manipulation detection, which implies that it can be used as a general-purpose network for image forensic tasks.      
### 20.Distributed Density Filtering for Large-scale Systems using Mean-filed Models  [ :arrow_down: ](https://arxiv.org/pdf/2009.05366.pdf)
>  In this work, we study the problem of distributed (probability) density estimation of large-scale systems. Such problems are motivated by many density-based distributed control tasks in which the real-time global density of the swarm is required as feedback information, such as sensor deployment and city traffic scheduling. This work is built upon our previous work which presented a (centralized) density filter to estimate the dynamic density of large-scale systems through integration of mean-field models, kernel density estimation (KDE), and infinite-dimensional Kalman filters. In this work, we further study how to decentralize the density filter such that each agent can estimate the global density only based on its local observation and minimal communication with neighbors. This is achieved by noting that the global observation constructed by KDE is an average of the local kernels. Hence, dynamic average consensus algorithms are used for each agent to track the global observation in a distributed way. We present a distributed density filter which requires very little information exchange, and study its stability and optimality using the notion of input-to-state stability. Simulation results suggest that the distributed filter is able to converge to the centralized filter and remain close to it.      
### 21.Power Evolution Prediction and Optimization in a Multi-span System Based on Component-wise System Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2009.05348.pdf)
>  Cascades of a machine learning-based EDFA gain model trained on a single physical device and a fully differentiable stimulated Raman scattering fiber model are used to predict and optimize the power profile at the output of an experimental multi-span fully-loaded C-band optical communication system.      
### 22.Energy-Efficient Design of IRS-NOMA Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.05344.pdf)
>  Combining intelligent reflecting surface (IRS) and non-orthogonal multiple access (NOMA) is an effective solution to enhance communication coverage and energy efficiency. In this paper, we focus on an IRS-assisted NOMA network and propose an energy-efficient algorithm to yield a good tradeoff between the sum-rate maximization and total power consumption minimization. We aim to maximize the system energy efficiency by jointly optimizing the transmit beamforming at the BS and the reflecting beamforming at the IRS. Specifically, the transmit beamforming and the phases of the low-cost passive elements on the IRS are alternatively optimized until the convergence. Simulation results demonstrate that the proposed algorithm in IRS-NOMA can yield superior performance compared with the conventional OMA-IRS and NOMA with a random phase IRS.      
### 23.Machine learning-based EDFA Gain Model Generalizable to Multiple Physical Devices  [ :arrow_down: ](https://arxiv.org/pdf/2009.05326.pdf)
>  We report a neural-network based erbium-doped fiber amplifier (EDFA) gain model built from experimental measurements. The model shows low gain-prediction error for both the same device used for training (MSE $\leq$ 0.04 dB$^2$) and different physical units of the same make (generalization MSE $\leq$ 0.06 dB$^2$).      
### 24.End-to-end optimization of coherent optical communications over the split-step Fourier method guided by the nonlinear Fourier transform theory  [ :arrow_down: ](https://arxiv.org/pdf/2009.05324.pdf)
>  Optimizing modulation and detection strategies for a given channel is critical to maximize the throughput of a communication system. Such an optimization can be easily carried out analytically for channels that admit closed-form analytical models. However, this task becomes extremely challenging for nonlinear dispersive channels such as the optical fiber. End-to-end optimization through autoencoders (AEs) can be applied to define symbol-to-waveform (modulation) and waveform-to-symbol (detection) mappings, but so far it has been mainly shown for systems relying on approximate channel models. Here, for the first time, we propose an AE scheme applied to the full optical channel described by the nonlinear Schr\{"o}dinger equation (NLSE). Transmitter and receiver are jointly optimized through the split-step Fourier method (SSFM) which accurately models an optical fiber. In this first numerical analysis, the detection is performed by a neural network (NN), whereas the symbol-to-waveform mapping is aided by the nonlinear Fourier transform (NFT) theory in order to simplify and guide the optimization on the modulation side. This proof-of-concept AE scheme is thus benchmarked against a standard NFT-based system and a threefold increase in achievable distance (from 2000 to 6640 km) is demonstrated.      
### 25.Generalized Minimal Distortion Principle for Blind Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2009.05288.pdf)
>  We revisit the source image estimation problem from blind source separation (BSS). We generalize the traditional minimum distortion principle to maximum likelihood estimation with a model for the residual spectrograms. Because residual spectrograms typically contain other sources, we propose to use a mixed-norm model that lets us finely tune sparsity in time and frequency. We propose to carry out the minimization of the mixed-norm via majorization-minimization optimization, leading to an iteratively reweighted least-squares algorithm. The algorithm balances well efficiency and ease of implementation. We assess the performance of the proposed method as applied to two well-known determined BSS and one joint BSS-dereverberation algorithms. We find out that it is possible to tune the parameters to improve separation by up to 2 dB, with no increase in distortion, and at little computational cost. The method thus provides a cheap and easy way to boost the performance of blind source separation.      
### 26.Combining Prior Knowledge and Data for Robust Controller Design  [ :arrow_down: ](https://arxiv.org/pdf/2009.05253.pdf)
>  We present a framework for systematically combining data of an unknown linear time-invariant system with prior knowledge on the system matrices or on the uncertainty for robust controller design. Our approach leads to linear matrix inequality (LMI) based feasibility criteria which guarantee stability, $\mathcal{H}_2$-performance, or quadratic performance robustly for all closed-loop systems consistent with the prior knowledge and the available data. The design procedures rely on a simple, computationally attractive data-dependent uncertainty bound which can be employed for controller design using dualization arguments and S-procedure-based LMI relaxations. While most parts of the paper focus on input-state measurements, we also provide an extension to robust output-feedback design based on noisy input-output data. Finally, we apply sum-of-squares methods to construct relaxation hierarchies for the considered robust controller design problem which are asymptotically exact. We illustrate through various examples that our approach provides a flexible framework for simultaneously leveraging prior knowledge and available data, thereby reducing conservatism and improving performance significantly if compared to purely data-driven controller design.      
### 27.Deep Transfer Learning for Signal Detection in Ambient Backscatter Communications  [ :arrow_down: ](https://arxiv.org/pdf/2009.05231.pdf)
>  Tag signal detection is one of the key tasks in ambient backscatter communication (AmBC) systems. However, obtaining perfect channel state information (CSI) is challenging and costly, which makes AmBC systems suffer from a high bit error rate (BER). To eliminate the requirement of channel estimation and to improve the system performance, in this paper, we adopt a deep transfer learning (DTL) approach to implicitly extract the features of channel and directly recover tag symbols. To this end, we develop a DTL detection framework which consists of offline learning, transfer learning, and online detection. Specifically, a DTL-based likelihood ratio test (DTL-LRT) is derived based on the minimum error probability (MEP) criterion. As a realization of the developed framework, we then apply convolutional neural networks (CNN) to intelligently explore the features of the sample covariance matrix, which facilitates the design of a CNN-based algorithm for tag signal detection. Exploiting the powerful capability of CNN in extracting features of data in the matrix formation, the proposed method is able to further improve the system performance. In addition, an asymptotic explicit expression is also derived to characterize the properties of the proposed CNN-based method when the number of samples is sufficiently large. Finally, extensive simulation results demonstrate that the BER performance of the proposed method is comparable to that of the optimal detection method with perfect CSI.      
### 28.Research on Intelligent Traffic Control Methods at Intersections Based on Game Theory  [ :arrow_down: ](https://arxiv.org/pdf/2009.05216.pdf)
>  Based on game theory and dynamic Level-k model, this paper establishes an intelligent traffic control method for intersections, studies the influence of multi-agent vehicle joint decision-making and group behavior disturbance on system state. The dynamic Level-k model is used in emergency vehicles to analyze decision-making behavior and system performance. The simulation results show that this method has a good performance when there are more vehicles or emergency vehicles have higher priority.      
### 29.A 6.3-Nanowatt-per-Channel 96-Channel Neural Spike Processor for a Movement-Intention-Decoding Brain-Computer-Interface Implant  [ :arrow_down: ](https://arxiv.org/pdf/2009.05210.pdf)
>  This paper presents microwatt end-to-end neural signal processing hardware for deployment-stage real-time upper-limb movement intent decoding. This module features intercellular spike detection, sorting, and decoding operations for a 96-channel prosthetic implant. We design the algorithms for those operations to achieve minimal computation complexity while matching or advancing the accuracy of state-of-art Brain-Computer-Interface sorting and movement decoding. Based on those algorithms, we devise the architect of the neural signal processing hardware with the focus on hardware reuse and event-driven operation. The design achieves among the highest levels of integration, reducing wireless data rate by more than four orders of magnitude. The chip prototype in a 180-nm high-VTH, achieving the lowest power dissipation of 0.61 uW for 96 channels, 21X lower than the prior art at a comparable/better accuracy even with integration of kinematic state estimation computation.      
### 30.Consensus under Network Interruption and Effective Resistance Interdiction  [ :arrow_down: ](https://arxiv.org/pdf/2009.05208.pdf)
>  We study the problem of network robustness under consensus dynamics. We first show that maximizing the consensus time subject to removing limited network edges can be cast as an effective resistance interdiction problem. We then show that the effective resistance interdiction problem is strongly NP-hard, even for three types of resistors in the network, hence correcting some claims in the existing literature. Finally, we provide a quadratic program formulation to find a local optimum solution to the consensus interdiction problem.      
### 31.Scenario Forecast of Cross-border Electric Interconnection towards Renewables in South America  [ :arrow_down: ](https://arxiv.org/pdf/2009.05194.pdf)
>  Cross-border Electric Interconnection towards renewables is a promising solution for electric sector under the UN 2030 sustainable development goals which is widely promoted in emerging economies. This paper comprehensively investigates state of art in renewable resources and cross-border electric interconnection in South America. Based on the raw data collected from typical countries, a long-term scenario forecast methodology is applied to estimate key indicators of electric sector in target years, comparing the prospects of active promoting cross-border Interconnections Towards Renewables (ITR) scenario with Business as Usual (BAU) scenario in South America region. Key indicators including peak load, installed capacity, investment, and generation cost are forecasted and comparative analyzed by year 2035 and 2050. The comparative data analysis shows that by promoting cross-border interconnection towards renewables in South America, renewable resources can be highly utilized for energy supply, energy matrix can be optimized balanced, economics can be obviously driven and generation cost can be greatly reduced.      
### 32.STEP-GAN: A Step-by-Step Training for Multi Generator GANs with application to Cyber Security in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.05184.pdf)
>  In this study, we introduce a novel unsupervised countermeasure for smart grid power systems, based on generative adversarial networks (GANs). Given the pivotal role of smart grid systems (SGSs) in urban life, their security is of particular importance. In recent years, however, advances in the field of machine learning, have raised concerns about cyber attacks on these systems. Power systems, among the most important components of urban infrastructure, have, for example, been widely attacked by adversaries. Attackers disrupt power systems using false data injection attacks (FDIA), resulting in a breach of availability, integrity, or confidential principles of the system. Our model simulates possible attacks on power systems using multiple generators in a step-by-step interaction with a discriminator in the training phase. As a consequence, our system is robust to unseen attacks. Moreover, the proposed model considerably reduces the well-known mode collapse problem of GAN-based models. Our method is general and it can be potentially employed in a wide range of one of one-class classification tasks. The proposed model has low computational complexity and outperforms baseline systems about 14% and 41% in terms of accuracy on the highly imbalanced publicly available industrial control system (ICS) cyber attack power system dataset.      
### 33.Adaptive Distributed Transceiver Synchronization Over a 90 Meter Microwave Wireless Link  [ :arrow_down: ](https://arxiv.org/pdf/2009.05127.pdf)
>  We present an adaptive approach for synchronizing both the phase and frequency of radio-frequency transceivers over long-range wireless links to support distributed antenna array applications. To enable distributed beamforming between separate wireless nodes, the oscillators in the transceivers must operate at the same frequency, and their phases must be appropriately aligned to support phase-coherent beamsteering. Based on a spectrally-sparse waveform, a self-mixing circuit, and an adaptive control loop, we present a system capable of synchronizing the RF oscillators in separate transceivers over distances of nearly 100 m. The approach is based on a spectrally-sparse waveform for joint inter-node ranging and frequency transfer. A frequency reference is modulated onto one signal of a two-tone waveform transmitted by the primary node which is demodulated and used to lock the oscillator of the secondary node. The secondary node retransmits the two-tone signal which the primary node uses for a high-accuracy range measurement. From this range, the phase of the two transceivers can be aligned to support beamforming. We furthermore implemented an adaptive phase control approach to support high-accuracy phase coordination in changing environmental conditions. We demonstrate continuous high accuracy links over a 90 m distance in an outdoor environment for durations up to seven days, demonstrating sufficient phase coordination in changing weather conditions to support distributed beamforming at frequencies up to 3 GHz.      
### 34.Tuning of Constant in gain Lead in phase (CgLp) Reset Controller using higher-order sinusoidal input describing function (HOSIDF)  [ :arrow_down: ](https://arxiv.org/pdf/2009.05116.pdf)
>  Due to development of technology, linear controllers cannot satisfy requirements of high-tech industry. One solution is using nonlinear controllers such as reset elements to overcome this big barrier. In literature, the Constant in gain Lead in phase (CgLp) compensator is a novel reset element developed to overcome the inherent linear controller limitations. However, a tuning guideline for these controllers has not been proposed so far. In this paper, a recently developed method named higher-order sinusoidal input describing function (HOSIDF), which gives deeper insight into the frequency behaviour of non-linear controllers compared to sinusoidal input describing function (DF), is used to obtain a straight-forward tuning method for CgLp compensators. In this respect, comparative analyses on tracking performance of these compensators are carried out. Based on these analyses, tuning guidelines for CgLp compensators are developed and validated on a high-tech precision positioning stage. The results show the effectiveness of the developed tuning method.      
### 35.COVID CT-Net: Predicting Covid-19 From Chest CT Images Using Attentional Convolutional Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.05096.pdf)
>  The novel corona-virus disease (COVID-19) pandemic has caused a major outbreak in more than 200 countries around the world, leading to a severe impact on the health and life of many people globally. As of Aug 25th of 2020, more than 20 million people are infected, and more than 800,000 death are reported. Computed Tomography (CT) images can be used as a as an alternative to the time-consuming "reverse transcription polymerase chain reaction (RT-PCR)" test, to detect COVID-19. In this work we developed a deep learning framework to predict COVID-19 from CT images. We propose to use an attentional convolution network, which can focus on the infected areas of chest, enabling it to perform a more accurate prediction. We trained our model on a dataset of more than 2000 CT images, and report its performance in terms of various popular metrics, such as sensitivity, specificity, area under the curve, and also precision-recall curve, and achieve very promising results. We also provide a visualization of the attention maps of the model for several test images, and show that our model is attending to the infected regions as intended. In addition to developing a machine learning modeling framework, we also provide the manual annotation of the potentionally infected regions of chest, with the help of a board-certified radiologist, and make that publicly available for other researchers.      
### 36.Speaker Diarization Using Stereo Audio Channels: Preliminary Study on Utterance Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2009.05076.pdf)
>  Speaker diarization is one of the actively researched topics in audio signal processing and machine learning. Utterance clustering is a critical part of a speaker diarization task. In this study, we aim to improve the performance of utterance clustering by processing multichannel (stereo) audio signals. We generated processed audio signals by combining left- and right-channel audio signals in a few different ways and then extracted embedded features (also called d-vectors) from those processed audio signals. We applied the Gaussian mixture model (GMM) for supervised utterance clustering. In the training phase, we used a parameter sharing GMM to train the model for each speaker. In the testing phase, we selected the speaker with the maximum likelihood as the detected speaker. Results of experiments with real audio recordings of multi-person discussion sessions showed that our proposed method that used multichannel audio signals achieved significantly better performance than a conventional method with mono audio signals.      
### 37.Synthesis of Run-To-Completion Controllers for Discrete Event Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.05554.pdf)
>  A controller for a Discrete Event System must achieve its goals despite that its environment being capable of resolving race conditions between controlled and uncontrolled events.Assuming that the controller loses all races is sometimes unrealistic. In many cases, a realistic assumption is that the controller sometimes wins races and is fast enough to perform multiple actions without being interrupted. However, in order to model this scenario using control of DES requires introducing foreign assumptions about scheduling, that are hard to figure out correctly. We propose a more balanced control problem, named run-to-completion (RTC), to alleviate this issue. RTC naturally supports an execution assumption in which both the controller and the environment are guaranteed to initiate and perform sequences of actions, without flooding or delaying each other indefinitely. We consider control of DES in the context where specifications are given in the form of linear temporal logic. We formalize the RTC control problem and show how it can be reduced to a standard control problem.      
### 38.Fast LDPC GPU Decoder for Cloud RAN  [ :arrow_down: ](https://arxiv.org/pdf/2009.05534.pdf)
>  The GPU as a digital signal processing accelerator for cloud RAN is investigated. A new design for a 5G NR low density parity check code decoder running on a GPU is presented. The algorithm is flexibly adaptable to GPU architecture to achieve high resource utilization as well as low latency. It improves over an existing layered design that processes additional codewords in parallel to increase utilization. In comparison to a decoder implemented on a FPGA (757K gate), the new GPU (24 core) decoder has 3X higher throughput. The GPU decoder exhibits 3 to 5X lower decoding power efficiency, as typical of a general-purpose processor. Thus, GPUs may find application as cloud accelerators where rapid deployment and flexibility are prioritized over decoding power efficiency.      
### 39.Object Recognition for Economic Development from Daytime Satellite Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2009.05455.pdf)
>  Reliable data about the stock of physical capital and infrastructure in developing countries is typically very scarce. This is particular a problem for data at the subnational level where existing data is often outdated, not consistently measured or coverage is incomplete. Traditional data collection methods are time and labor-intensive costly, which often prohibits developing countries from collecting this type of data. This paper proposes a novel method to extract infrastructure features from high-resolution satellite images. We collected high-resolution satellite images for 5 million 1km $\times$ 1km grid cells covering 21 African countries. We contribute to the growing body of literature in this area by training our machine learning algorithm on ground-truth data. We show that our approach strongly improves the predictive accuracy. Our methodology can build the foundation to then predict subnational indicators of economic development for areas where this data is either missing or unreliable.      
### 40.ODIN: Automated Drift Detection and Recovery in Video Analytics  [ :arrow_down: ](https://arxiv.org/pdf/2009.05440.pdf)
>  Recent advances in computer vision have led to a resurgence of interest in visual data analytics. Researchers are developing systems for effectively and efficiently analyzing visual data at scale. A significant challenge that these systems encounter lies in the drift in real-world visual data. For instance, a model for self-driving vehicles that is not trained on images containing snow does not work well when it encounters them in practice. This drift phenomenon limits the accuracy of models employed for visual data analytics. In this paper, we present a visual data analytics system, called ODIN, that automatically detects and recovers from drift. ODIN uses adversarial autoencoders to learn the distribution of high-dimensional images. We present an unsupervised algorithm for detecting drift by comparing the distributions of the given data against that of previously seen data. When ODIN detects drift, it invokes a drift recovery algorithm to deploy specialized models tailored towards the novel data points. These specialized models outperform their non-specialized counterpart on accuracy, performance, and memory footprint. Lastly, we present a model selection algorithm for picking an ensemble of best-fit specialized models to process a given input. We evaluate the efficacy and efficiency of ODIN on high-resolution dashboard camera videos captured under diverse environments from the Berkeley DeepDrive dataset. We demonstrate that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x smaller memory footprint compared to a baseline system without automated drift detection and recovery.      
### 41.Semi-Supervised Active Learning for COVID-19 Lung Ultrasound Multi-symptom Classification  [ :arrow_down: ](https://arxiv.org/pdf/2009.05436.pdf)
>  Ultrasound (US) is a non-invasive yet effective medical diagnostic imaging technique for the COVID-19 global pandemic. However, due to complex feature behaviors and expensive annotations of US images, it is difficult to apply Artificial Intelligence (AI) assisting approaches for lung's multi-symptom (multi-label) classification. To overcome these difficulties, we propose a novel semi-supervised Two-Stream Active Learning (TSAL) method to model complicated features and reduce labeling costs in an iterative procedure. The core component of TSAL is the multi-label learning mechanism, in which label correlations information is used to design multi-label margin (MLM) strategy and confidence validation for automatically selecting informative samples and confident labels. On this basis, a multi-symptom multi-label (MSML) classification network is proposed to learn discriminative features of lung symptoms, and a human-machine interaction is exploited to confirm the final annotations that are used to fine-tune MSML with progressively labeled data. Moreover, a novel lung US dataset named COVID19-LUSMS is built, currently containing 71 clinical patients with 6,836 images sampled from 678 videos. Experimental evaluations show that TSAL using only 20% data can achieve superior performance to the baseline and the state-of-the-art. Qualitatively, visualization of both attention map and sample distribution confirms the good consistency with the clinic knowledge.      
### 42.AFP-SRC: Identification of Antifreeze Proteins Using Sparse Representation Classifier  [ :arrow_down: ](https://arxiv.org/pdf/2009.05277.pdf)
>  Species living in the extreme cold environment fight against the harsh conditions by virtue of antifreeze proteins (AFPs), that manipulates the freezing mechanism of water in more than one way. This amazing nature of AFP turns out to be extremely useful in a number of industrial and medical applications. The lack of similarity in their structure and sequence makes their prediction an arduous task and identifying them experimentally in the wet-lab is time consuming and expensive. In this research, we propose a computational framework for the prediction of AFPs which is essentially based on a sample-specific classification method using the sparse reconstruction. A linear model and an over-complete dictionary matrix of known AFPs is used to predict sparse class-label vector which provides sample-association score. Delta-rule is applied for the reconstruction of two pseudo-samples using lower and upper parts of sample-association vector and based on the minimum recovery score, class labels are assigned. We compare our approach with contemporary methods on a standard dataset and the proposed method is found to outperform in terms of Matthews correlation coefficient and Youden's index. The MATLAB implementation of proposed method is available at author's github page <a class="link-external link-https" href="https://github.com/Shujaat123/AFP-SRC" rel="external noopener nofollow">this https URL</a>.      
### 43.Capacity-Approaching Autoencoders for Communications  [ :arrow_down: ](https://arxiv.org/pdf/2009.05273.pdf)
>  The autoencoder concept has fostered the reinterpretation and the design of modern communication systems. It consists of an encoder, a channel, and a decoder block which modify their internal neural structure in an end-to-end learning fashion. However, the current approach to train an autoencoder relies on the use of the cross-entropy loss function. This approach can be prone to overfitting issues and often fails to learn an optimal system and signal representation (code). In addition, less is known about the autoencoder ability to design channel capacity-approaching codes, i.e., codes that maximize the input-output information under a certain power constraint. The task being even more formidable for an unknown channel for which the capacity is unknown and therefore it has to be learnt. <br>In this paper, we address the challenge of designing capacity-approaching codes by incorporating the presence of the communication channel into a novel loss function for the autoencoder training. In particular, we exploit the mutual information between the transmitted and received signals as a regularization term in the cross-entropy loss function, with the aim of controlling the amount of information stored. By jointly maximizing the mutual information and minimizing the cross-entropy, we propose a methodology that a) computes an estimate of the channel capacity and b) constructs an optimal coded signal approaching it. Several simulation results offer evidence of the potentiality of the proposed method.      
### 44.End-to-end Learning for OFDM: From Neural Receivers to Pilotless Communication  [ :arrow_down: ](https://arxiv.org/pdf/2009.05261.pdf)
>  Previous studies have demonstrated that end-to-end learning enables significant shaping gains over additive white Gaussian noise (AWGN) channels. However, its benefits have not yet been quantified over realistic wireless channel models. This work aims to fill this gap by exploring the gains of end-to-end learning over a frequency- and time-selective fading channel using orthogonal frequency division multiplexing (OFDM). With imperfect channel knowledge at the receiver, the shaping gains observed on AWGN channels vanish. Nonetheless, we identify two other sources of performance improvements. The first comes from a neural network (NN)-based receiver operating over a large number of subcarriers and OFDM symbols which allows to significantly reduce the number of orthogonal pilots without loss of bit error rate (BER). The second comes from entirely eliminating orthognal pilots by jointly learning a neural receiver together with either superimposed pilots (SIPs), linearly combined with conventional quadrature amplitude modulation (QAM), or an optimized constellation geometry. The learned geometry works for a wide range of signal-to-noise ratios (SNRs), Doppler and delay spreads, has zero mean and does hence not contain any form of superimposed pilots. Both schemes achieve the same BER as the pilot-based baseline with around 7% higher throughput. Thus, we believe that a jointly learned transmitter and receiver are a very interesting component for beyond-5G communication systems which could remove the need and associated control overhead for demodulation reference signals (DMRSs).      
### 45.Optimizing Convolutional Neural Network Architecture via Information Field  [ :arrow_down: ](https://arxiv.org/pdf/2009.05236.pdf)
>  CNN architecture design has attracted tremendous attention of improving model accuracy or reducing model complexity. However, existing works either introduce repeated training overhead in the search process or lack an interpretable metric to guide the design. To clear the hurdles, we propose Information Field (IF), an explainable and easy-to-compute metric, to estimate the quality of a CNN architecture and guide the search process of designs. To validate the effectiveness of IF, we build a static optimizer to improve the CNN architectures at both the stage level and the kernel level. Our optimizer not only provides a clear and reproducible procedure but also mitigates unnecessary training efforts in the architecture search process. Experiments show that the models generated by our optimizer can achieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction, compared with state-of-the-art CNN structures like MobileNet and ResNet.      
### 46.HAA500: Human-Centric Atomic Action Dataset with Curated Videos  [ :arrow_down: ](https://arxiv.org/pdf/2009.05224.pdf)
>  We contribute HAA500, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591k labeled frames. Unlike existing atomic action datasets, where coarse-grained atomic actions were labeled with action-verbs, e.g., "Throw", HAA500 contains fine-grained atomic actions where only consistent actions fall under the same label, e.g., "Baseball Pitching" vs "Free Throw in Basketball", to minimize ambiguities in action classification. HAA500 has been carefully curated to capture the movement of human figures with less spatio-temporal label noises to greatly enhance the training of deep neural networks. The advantages of HAA500 include: 1) human-centric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) each video captures the essential elements of an atomic action without irrelevant frames; 3) fine-grained atomic action classes. Our extensive experiments validate the benefits of human-centric and atomic characteristics of HAA, which enables the trained model to improve prediction by attending to atomic human poses. We detail the HAA500 dataset statistics and collection methodology, and compare quantitatively with existing action recognition datasets.      
### 47.Comments on "Generalization of the gradient method with fractional order gradient direction"  [ :arrow_down: ](https://arxiv.org/pdf/2009.05221.pdf)
>  In this paper, a detrimental mathematical mistake is pointed out in the proof of \textit{Theorem 1} presented in the paper\textit{ [Generalization of the gradient method with fractional order gradient direction, J. Franklin Inst., 357 (2020) 2514-2532]}. It is highlighted that the way the authors prove the convergence of the fractional extreme points of a real valued function to its integer order extreme points lacks correct and valid mathematical argument. Rest of the theorems contained in the paper are mostly announced without any proof relaying on that of Theorem 1.      
### 48.SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context  [ :arrow_down: ](https://arxiv.org/pdf/2009.05188.pdf)
>  We present SONYC-UST-V2, a dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the "Sounds of New York City" (SONYC) acoustic sensor network, including the timestamp of audio acquisition and location of the sensor. The dataset contains annotations by volunteers from the Zooniverse citizen science platform, as well as a two-stage verification with our team. In this article, we describe our data collection procedure and propose evaluation metrics for multilabel classification of urban sound tags. We report the results of a simple baseline model that exploits spatiotemporal information.      
### 49.Sequential Convex Programming For Non-Linear Stochastic Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.05182.pdf)
>  We introduce a sequential convex programming framework to solve general non-linear stochastic optimal control problems in finite dimension, where uncertainties are modeled by a multidimensional Wiener process. We provide sufficient conditions for the convergence of the method. Moreover, we prove that, when convergence is achieved, sequential convex programming finds a candidate locally optimal solution for the original problem in the sense of the stochastic Pontryagin Maximum Principle. We leverage those properties to design a practical numerical method to solve non-linear stochastic optimal control problems that is based on a deterministic transcription of stochastic sequential convex programming.      
### 50.Sentinel: An Onboard System for Intelligent Vehicles to Reduce Traffic Delay during Freeway Incidents  [ :arrow_down: ](https://arxiv.org/pdf/2009.05165.pdf)
>  This paper introduces Sentinel, an onboard system that guides the lane change behavior of intelligent vehicles during a freeway incident to reduce congestion and delay. Sentinel is built upon a probabilistic prediction model that uses several traffic- and driver-related parameters to estimate the probability of reaching a target position on the road using a number of lane changes. When an incident blocking the lane of an intelligent vehicle is detected, Sentinel starts estimating the probability of successfully departing the blocked lane before reaching the point of incident and alerts the vehicle to depart that lane when the probability drops below a certain threshold. To understand the impact of Sentinel on traffic flow and delay, it is used in a simulation case study of a four-lane segment of the I-66 interstate highway in the U.S. where the rightmost lane is temporarily blocked due to an incident. The results show that Sentinel can reduce average delay by up to 37%, depending on incident duration, Sentinel penetration rate, and traffic flow. In combination with Traffic Incident Management systems, Sentinel can be a valuable asset in reducing delay and saving billions of dollars in the cost of congestion on freeways.      
### 51.Simphony: An open-source photonic integrated circuit simulation framework  [ :arrow_down: ](https://arxiv.org/pdf/2009.05146.pdf)
>  We present Simphony, a free and open-source software toolbox for abstracting and simulating photonic integrated circuits, implemented in Python. The toolbox is both fast and easily extensible; plugins can be written to provide compatibility with existing layout tools, and device libraries can be easily created without a deep knowledge of programming. We include several examples of photonic circuit simulations with novel features and demonstrate a speedup of more than 20x over a leading commercially available software tool.      
### 52.SWP-Leaf NET: a novel multistage approach for plant leaf identification based on deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.05139.pdf)
>  Modern scientific and technological advances are allowing botanists to use computer vision-based approaches for plant identification tasks. These approaches have their own challenges. Leaf classification is a computer-vision task performed for the automated identification of plant species, a serious challenge due to variations in leaf morphology, including its size, texture, shape, and venation. Researchers have recently become more inclined toward deep learning-based methods rather than conventional feature-based methods due to the popularity and successful implementation of deep learning methods in image analysis, object recognition, and speech recognition. In this paper, a botanist's behavior was modeled in leaf identification by proposing a highly-efficient method of maximum behavioral resemblance developed through three deep learning-based models. Different layers of the three models were visualized to ensure that the botanist's behavior was modeled accurately. The first and second models were designed from scratch.Regarding the third model, the pre-trained architecture MobileNetV2 was employed along with the transfer-learning technique. The proposed method was evaluated on two well-known datasets: Flavia and MalayaKew. According to a comparative analysis, the suggested approach was more accurate than hand-crafted feature extraction methods and other deep learning techniques in terms of 99.67% and 99.81% accuracy. Unlike conventional techniques that have their own specific complexities and depend on datasets, the proposed method required no hand-crafted feature extraction, and also increased accuracy and distributability as compared with other deep learning techniques. It was further considerably faster than other methods because it used shallower networks with fewer parameters and did not use all three models recurrently.      
### 53.Finite-Alphabet Wiener Filter Precoding for mmWave Massive MU-MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.05133.pdf)
>  Power consumption of multi-user (MU) precoding is a major concern in all-digital massive MU multiple-input multiple-output (MIMO) base-stations with hundreds of antenna elements operating at millimeter-wave (mmWave) frequencies. We propose to replace part of the linear Wiener filter (WF) precoding matrix by a finite-alphabet WF precoding (FAWP) matrix, which enables the use of low-precision hardware that consumes low power and area. To minimize the performance loss of our approach, we present methods that efficiently compute FAWP matrices that best mimic the WF precoder. Our results show that FAWP matrices approach infinite-precision error-rate and error-vector magnitude performance with only 3-bit precoding weights, even when operating in realistic mmWave channels. Hence, FAWP is a promising approach to substantially reduce power consumption and silicon area in all-digital mmWave massive MU-MIMO systems.      
### 54.Emotion-Based End-to-End Matching Between Image and Music in Valence-Arousal Space  [ :arrow_down: ](https://arxiv.org/pdf/2009.05103.pdf)
>  Both images and music can convey rich semantics and are widely used to induce specific emotions. Matching images and music with similar emotions might help to make emotion perceptions more vivid and stronger. Existing emotion-based image and music matching methods either employ limited categorical emotion states which cannot well reflect the complexity and subtlety of emotions, or train the matching model using an impractical multi-stage pipeline. In this paper, we study end-to-end matching between image and music based on emotions in the continuous valence-arousal (VA) space. First, we construct a large-scale dataset, termed Image-Music-Emotion-Matching-Net (IMEMNet), with over 140K image-music pairs. Second, we propose cross-modal deep continuous metric learning (CDCML) to learn a shared latent embedding space which preserves the cross-modal similarity relationship in the continuous matching space. Finally, we refine the embedding space by further preserving the single-modal emotion relationship in the VA spaces of both images and music. The metric learning in the embedding space and task regression in the label space are jointly optimized for both cross-modal matching and single-modal VA prediction. The extensive experiments conducted on IMEMNet demonstrate the superiority of CDCML for emotion-based image and music matching as compared to the state-of-the-art approaches.      
### 55.Actionable Interpretation of Machine Learning Models for Sequential Data: Dementia-related Agitation Use Case  [ :arrow_down: ](https://arxiv.org/pdf/2009.05097.pdf)
>  Machine learning has shown successes for complex learning problems in which data/parameters can be multidimensional and too complex for a first-principles based analysis. Some applications that utilize machine learning require human interpretability, not just to understand a particular result (classification, detection, etc.) but also for humans to take action based on that result. Black-box machine learning model interpretation has been studied, but recent work has focused on validation and improving model performance. In this work, an actionable interpretation of black-box machine learning models is presented. The proposed technique focuses on the extraction of actionable measures to help users make a decision or take an action. Actionable interpretation can be implemented in most traditional black-box machine learning models. It uses the already trained model, used training data, and data processing techniques to extract actionable items from the model outcome and its time-series inputs. An implementation of the actionable interpretation is shown with a use case: dementia-related agitation prediction and the ambient environment. It is shown that actionable items can be extracted, such as the decreasing of in-home light level, which is triggering an agitation episode. This use case of actionable interpretation can help dementia caregivers take action to intervene and prevent agitation.      
### 56.Efficient Detectors for Telegram Splitting based Transmission in Low Power Wide Area Networks with Bursty Interference  [ :arrow_down: ](https://arxiv.org/pdf/2009.05072.pdf)
>  Low Power Wide Area (LPWA) networks are known to be highly vulnerable to external in-band interference in terms of packet collisions which may substantially degrade the system performance. In order to enhance the performance in such cases, the telegram splitting (TS) method has been proposed recently. This approach exploits the typical burstiness of the interference via forward error correction (FEC) and offers a substantial performance improvement compared to other methods for packet transmissions in LPWA networks. While it has been already demonstrated that the TS method benefits from knowledge on the current interference state at the receiver side, corresponding practical receiver algorithms of high performance are still missing. The modeling of the bursty interference via Markov chains leads to the optimal detector in terms of a-posteriori symbol error probability. However, this solution requires a high computational complexity, assumes an a-priori knowledge on the interference characteristics and lacks flexibility. We propose a further developed scheme with increased flexibility and introduce an approach to reduce its complexity while maintaining a close-to-optimum performance. In particular, the proposed low complexity solution substantially outperforms existing practical methods in terms of packet error rate and therefore is highly beneficial for practical LPWA network scenarios.      
