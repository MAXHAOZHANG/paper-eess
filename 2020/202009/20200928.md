# ArXiv eess --Mon, 28 Sep 2020
### 1.Networked control of coupled subsystems: Spectral decomposition and low-dimensional solutions  [ :arrow_down: ](https://arxiv.org/pdf/2009.12367.pdf)
>  In this paper, we investigate optimal networked control of coupled subsystems where the dynamics and the cost couplings depend on an underlying undirected weighted graph. The graph coupling matrix may be the adjacency matrix, the Laplacian matrix, or any other symmetric matrix corresponding to the underlying graph. We use the spectral decomposition of the graph coupling matrix to decompose the overall system into (L + 1) systems with decoupled dynamics and cost, where L is the rank of the coupling matrix. Consequently, the optimal control input at each subsystem can be computed by solving at most (L + 1) decoupled Riccati equations. A salient feature of the result is that the solution complexity depends on the rank of the coupling matrix rather than the size of the network (i.e., the number of nodes). Therefore, the proposed solution framework provides a scalable method for synthesizing and implementing optimal control laws for large-scale networked control systems.      
### 2.Robust Vehicle Lane Keeping Control with Proactive Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2009.12349.pdf)
>  Road condition is an important environmental factor for autonomous vehicle control. A dramatic change of the road condition from the nominal status is a source of uncertainty that can lead to a system failure. Once the vehicle encounters an uncertain environment, such as hitting an ice patch, it is too late to reduce the speed, and the vehicle can lose control. To cope with future uncertainties in advance, we study a proactive robust adaptive control architecture for autonomous vehicles' lane-keeping control problems. The data center generates a prior environmental uncertainty estimate by combining weather forecasts and measurements from anonymous vehicles through a spatio-temporal filter. The prior estimate contributes to designing a robust heading controller and nominal longitudinal velocity for a proactive adaptation to each new abnormal condition. Then the control parameters are updated based on posterior information fusion with on-board measurements.      
### 3.Fused Low-Earth-Orbit GNSS  [ :arrow_down: ](https://arxiv.org/pdf/2009.12334.pdf)
>  In addition to Internet service, new commercial broadband low-Earth-orbiting (LEO) satellites could provide a positioning, navigation, and timing (PNT) service far more robust to interference than traditional Global Navigation Satellite Systems (GNSS). Previous proposals for LEO PNT require dedicated spectrum and hardware: a transmitter, antenna, and atomic clock on board every broadband satellite. This paper proposes a high-performance, low-cost alternative which fuses the requirements of PNT service into the existing capabilities of the broadband satellite. A concept of operations for so-called fused LEO GNSS is presented and analyzed in terms of the economy of its use of constellation resources of transmitters, bandwidth, and time. This paper shows that continuous assured PNT service over $\pm$60° latitude (covering 99.8% of the world's population) with positioning performance exceeding traditional GNSS pseudoranging would cost less than 0.8% of downlink capacity for the largest of the new constellations, SpaceX's Starlink.      
### 4.Novel Visible Light Communication Assisted Perspective-Four-Line Algorithm for Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2009.12320.pdf)
>  In this paper, we propose a novel visible light communication (VLC) assisted Perspective-fourLine algorithm (V-P4L) for practical indoor localization. The basic idea of V-P4L is to joint VLC and computer vision to achieve high accuracy regardless of LED height differences. In particular, we first exploit the space-domain information to estimate the orientation and coordinate of a single rectangular LED luminaire in the camera coordinate system based on the plane and solid geometry. Then, based on the time-domain information transmitted by VLC and the estimated luminaire information, V-P4L can estimate the position and pose of the camera by the single-view geometry theory and the linear least square (LLS) method. To further mitigate the effect of height differences among LEDs on localization accuracy, we then propose a correction algorithm of V-P4L based on the LLS method and a simple optimization method. Due to the combination of time- and space-domain information, V-P4L only requires a single luminaire for localization without limitation on the correspondences between the features and their projections. Simulation results show that for V-P4L the position error is always less than 15 cm and the orientation error is always less than 3° using popular indoor luminaires.      
### 5.A consolidated view of loss functions for supervised deep learning-based speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2009.12286.pdf)
>  Deep learning-based speech enhancement for real-time applications recently made large advancements. Due to the lack of a tractable perceptual optimization target, many myths around training losses emerged, whereas the contribution to success of the loss functions in many cases has not been investigated isolated from other factors such as network architecture, features, or training procedures. In this work, we investigate a wide variety of loss spectral functions for a recurrent neural network architecture suitable to operate in online frame-by-frame processing. We relate magnitude-only with phase-aware losses, ratios, correlation metrics, and compressed metrics. Our results reveal that combining magnitude-only with phase-aware objectives always leads to improvements, even when the phase is not enhanced. Furthermore, using compressed spectral values also yields a significant improvement. On the other hand, phase-sensitive improvement is best achieved by linear domain losses such as mean absolute error.      
### 6.EEG Channel Interpolation Using Deep Encoder-decoder Netwoks  [ :arrow_down: ](https://arxiv.org/pdf/2009.12244.pdf)
>  Electrode "pop" artifacts originate from the spontaneous loss of connectivity between a surface and an electrode. Electroencephalography (EEG) uses a dense array of electrodes, hence "popped" segments are among the most pervasive type of artifact seen during the collection of EEG data. In many cases, the continuity of EEG data is critical for downstream applications (e.g. brain machine interface) and requires that popped segments be accurately interpolated. In this paper we frame the interpolation problem as a self-learning task using a deep encoder-decoder network. We compare our approach against contemporary interpolation methods on a publicly available EEG data set. Our approach exhibited a minimum of ~15% improvement over contemporary approaches when tested on subjects and tasks not used during model training. We demonstrate how our model's performance can be enhanced further on novel subjects and tasks using transfer learning. All code and data associated with this study is open-source to enable ease of extension and practical use. To our knowledge, this work is the first solution to the EEG interpolation problem that uses deep learning.      
### 7.How Much Does It Hurt: A Deep Learning Framework for Chronic Pain Score Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2009.12202.pdf)
>  Chronic pain is defined as pain that lasts or recurs for more than 3 to 6 months, often long after the injury or illness that initially caused the pain has healed. The "gold standard" for chronic pain assessment remains self report and clinical assessment via a biopsychosocial interview, since there has been no device that can measure it. A device to measure pain would be useful not only for clinical assessment, but potentially also as a biofeedback device leading to pain reduction. In this paper we propose an end-to-end deep learning framework for chronic pain score assessment. Our deep learning framework splits the long time-course data samples into shorter sequences, and uses Consensus Prediction to classify the results. We evaluate the performance of our framework on two chronic pain score datasets collected from two iterations of prototype Pain Meters that we have developed to help chronic pain subjects better understand their health condition.      
### 8.Integrating Battery Aging in the Optimization for Bidirectional Charging of Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2009.12201.pdf)
>  Smart charging of Electric Vehicles (EVs) reduces operating costs, allows more sustainable battery usage, and promotes the rise of electric mobility. In addition, bidirectional charging and improved connectivity enables efficient power grid support. Today, however, uncoordinated charging, e.g. governed by users' habits, is still the norm. Thus, the impact of upcoming smart charging applications is mostly unexplored. We aim to estimate the expenses inherent with smart charging, e.g. battery aging costs, and give suggestions for further research. Using typical on-board sensor data we concisely model and validate an EV battery. We then integrate the battery model into a realistic smart charging use case and compare it with measurements of real EV charging. The results show that i) the temperature dependence of battery aging requires precise thermal models for charging power greater than 7 kW, ii) disregarding battery aging underestimates EVs' operating costs by approx. 30%, and iii) the profitability of Vehicle-to-Grid (V2G) services based on bidirectional power flow, e.g. energy arbitrage, depends on battery aging costs and the electricity price spread.      
### 9.Grain Surface Classification via Machine Learning Methods  [ :arrow_down: ](https://arxiv.org/pdf/2009.12200.pdf)
>  In this study, radar signals were analyzed to classify grain surface types by using machine learning methods. Radar backscatter signals were recorded using a vector network analyzer between 18-40 GHz. A total of 5681 measurements of A scan signals were collected. The proposed method framework consists of two parts. First Order Statistical features are obtained by applying Fast Fourier Transform (FFT), Discrete Cosine Transform (DCT), Discrete Wavelet Transform (DWT) on backscatter signals in the first part of the framework. Classification process of these features was carried out with Support Vector Machine (SVM). In the second part of the proposed framework, two dimensional matrices in complex form were obtained by applying Short Time Fourier Transform (STFT) on the signals. Gray-Level Co-Occurrence Matrix (GLCM) and Gray-Level Run-Length Matrix (GLRLM) were obtained and feature extraction process was completed. Classification process was carried out with DVM. 10-k cross validation was applied. The highest performance was achieved with STFT+GLCM+SVM.      
### 10.End-to-End Prediction of Parcel Delivery Time with Deep Learning for Smart-City Applications  [ :arrow_down: ](https://arxiv.org/pdf/2009.12197.pdf)
>  The acquisition of massive data on parcel delivery motivates postal operators to foster the development of predictive systems to improve customer service. Predicting delivery times successive to being shipped out of the final depot, referred to as last-mile prediction, deals with complicating factors such as traffic, drivers' behaviors, and weather. This work studies the use of deep learning for solving a real-world case of last-mile parcel delivery time prediction. We present our solution under the IoT paradigm and discuss its feasibility on a cloud-based architecture as a smart city application. We focus on a large-scale parcel dataset provided by Canada Post, covering the Greater Toronto Area (GTA). We utilize an origin-destination (OD) formulation, in which routes are not available, but only the start and end delivery points. We investigate three categories of convolutional-based neural networks and assess their performances on the task. We further demonstrate how our modeling outperforms several baselines, from classical machine learning models to referenced OD solutions. Specifically, we show that a ResNet architecture with 8 residual blocks displays the best trade-off between performance and complexity. We perform a thorough error analysis across the data and visualize the deep features learned to better understand the model behavior, making interesting remarks on data predictability. Our work provides an end-to-end neural pipeline that leverages parcel OD data as well as weather to accurately predict delivery durations. We believe that our system has the potential not only to improve user experience by better modeling their anticipation but also to aid last-mile postal logistics as a whole.      
### 11.Style-invariant Cardiac Image Segmentation with Test-time Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.12193.pdf)
>  Deep models often suffer from severe performance drop due to the appearance shift in the real clinical setting. Most of the existing learning-based methods rely on images from multiple sites/vendors or even corresponding labels. However, collecting enough unknown data to robustly model segmentation cannot always hold since the complex appearance shift caused by imaging factors in daily application. In this paper, we propose a novel style-invariant method for cardiac image segmentation. Based on the zero-shot style transfer to remove appearance shift and test-time augmentation to explore diverse underlying anatomy, our proposed method is effective in combating the appearance shift. Our contribution is three-fold. First, inspired by the spirit of universal style transfer, we develop a zero-shot stylization for content images to generate stylized images that appearance similarity to the style images. Second, we build up a robust cardiac segmentation model based on the U-Net structure. Our framework mainly consists of two networks during testing: the ST network for removing appearance shift and the segmentation network. Third, we investigate test-time augmentation to explore transformed versions of the stylized image for prediction and the results are merged. Notably, our proposed framework is fully test-time adaptation. Experiment results demonstrate that our methods are promising and generic for generalizing deep segmentation models.      
### 12.Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2009.12188.pdf)
>  Automation of brain tumors in 3D magnetic resonance images (MRIs) is key to assess the diagnostic and treatment of the disease. In recent years, convolutional neural networks (CNNs) have shown improved results in the task. However, high memory consumption is still a problem in 3D-CNNs. Moreover, most methods do not include uncertainty information, which is specially critical in medical diagnosis. This work proposes a 3D encoder-decoder architecture, based on V-Net \cite{vnet} which is trained with patching techniques to reduce memory consumption and decrease the effect of unbalanced data. We also introduce voxel-wise uncertainty, both epistemic and aleatoric using test-time dropout and data-augmentation respectively. Uncertainty maps can provide extra information to expert neurologists, useful for detecting when the model is not confident on the provided segmentation.      
### 13.Intelligent Risk Alarm for Asthma Patients using Artificial Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.12175.pdf)
>  Asthma is a chronic disease of the airways of the lungs. It results in inflammation and narrowing of the respiratory passages, which prevents air flow into the airways and leads to frequent bouts of shortness of breath with wheezing accompanied by coughing and phlegm after exposure to inhalation of substances that provoke allergic reactions or irritation of the respiratory system. Data mining in healthcare system is very important in diagnosing and understanding data, so data mining aims to solve basic problems in diagnosing diseases due to the complexity of diagnosing asthma. Predicting chemicals in the atmosphere is very important and one of the most difficult problems since the last century. In this paper, the impact of chemicals on asthma patient will be presented and discussed. Sensor system called MQ5 will be used to examine the smoke and nitrogen content in the atmosphere. MQ5 will be inserted in a wristwatch that checks the smoke and nitrogen content in the patients place, the system shall issue a warning alarm if this gas affects the person with asthma. It will be based on the Artificial Neural Networks (ANN) algorithm that has been built using data that containing a set of chemicals such as carbon monoxide, NMHC (GT) acid gas, C6H6 (GT) Gasoline, NOx (GT) Nitrogen Oxide, and NO2 (GT) Nitrogen Dioxide. The temperature and humidity will be also used as they can negatively affect asthma patient. Finally, the rating model was evaluated and achieved 99.58% classification accuracy.      
### 14.Goal-Directed Occupancy Prediction for Lane-Following Actors  [ :arrow_down: ](https://arxiv.org/pdf/2009.12174.pdf)
>  Predicting the possible future behaviors of vehicles that drive on shared roads is a crucial task for safe autonomous driving. Many existing approaches to this problem strive to distill all possible vehicle behaviors into a simplified set of high-level actions. However, these action categories do not suffice to describe the full range of maneuvers possible in the complex road networks we encounter in the real world. To combat this deficiency, we propose a new method that leverages the mapped road topology to reason over possible goals and predict the future spatial occupancy of dynamic road actors. We show that our approach is able to accurately predict future occupancy that remains consistent with the mapped lane geometry and naturally captures multi-modality based on the local scene context while also not suffering from the mode collapse problem observed in prior work.      
### 15.Delay Characterization of Mobile Edge Computing for 6G Time-Sensitive Services  [ :arrow_down: ](https://arxiv.org/pdf/2009.12170.pdf)
>  Time-sensitive services (TSSs) have been widely envisioned for future sixth generation (6G) wireless communication networks. Due to its inherent low-latency advantage, mobile edge computing (MEC) will be an indispensable enabler for TSSs. The random characteristics of the delay experienced by users are key metrics reflecting the quality of service (QoS) of TSSs. Most existing studies on MEC have focused on the average delay. Only a few research efforts have been devoted to other random delay characteristics, such as the delay bound violation probability and the probability distribution of the delay, by decoupling the transmission and computation processes of MEC. However, if these two processes could not be decoupled, the coupling will bring new challenges to analyzing the random delay characteristics. In this paper, an MEC system with a limited computation buffer at the edge server is considered. In this system, the transmission process and computation process form a feedback loop and could not be decoupled. We formulate a discrete-time two-stage tandem queueing system. Then, by using the matrix-geometric method, we obtain the estimation methods for the random delay characteristics, including the probability distribution of the delay, the delay bound violation probability, the average delay and the delay standard deviation. The estimation methods are verified by simulations. The random delay characteristics are analyzed by numerical experiments, which unveil the coupling relationship between the transmission process and computation process for MEC. These results will largely facilitate elaborate allocation of communication and computation resources to improve the QoS of TSSs.      
### 16.Transient Classification in low SNR Gravitational Wave data using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.12168.pdf)
>  The recent advances in Gravitational-wave astronomy have greatly accelerated the study of Multimessenger astrophysics. There is a need for the development of fast and efficient algorithms to detect non-astrophysical transients and noises due to the rate and scale at which the data is being provided by LIGO and other gravitational wave observatories. These transients and noises can interfere with the study of gravitational waves and binary mergers and induce false positives. Here, we propose the use of deep learning algorithms to detect and classify these transient signals. Traditional statistical methods are not well designed for dealing with temporal signals but supervised deep learning techniques such as RNN-LSTM and deep CNN have proven to be effective for solving problems such as time-series forecasting and time-series classification. We also use unsupervised models such as Total variation, Principal Component Analysis, Support Vector Machine, Wavelet decomposition or Random Forests for feature extraction and noise reduction and then study the results obtained by RNN-LSTM and deep CNN for classifying the transients in low-SNR signals. We compare the results obtained by the combination of various unsupervised models and supervised models. This method can be extended to real-time detection of transients and merger signals using deep-learning optimized GPU's for early prediction and study of various astronomical events. We will also explore and compare other machine learning models such as MLP, Stacked Autoencoder, Random forests, extreme learning machine, Support Vector machine and logistic regression classifier.      
### 17.Vertical Power Flow Forecast with LSTMs Using Regular Training Update Strategies  [ :arrow_down: ](https://arxiv.org/pdf/2009.12167.pdf)
>  The strong growth of renewable energy sources and the high volatility in power generation of these sources, as well as the increasing amount of volatile energy consumption is leading to major challenges in the electrical grid. In order to ensure safety and reliability in the electricity grid, the power flow in the grid needs to be observed to prevent overloading. Furthermore, the energy supply and consumption need to be continuously balanced to ensure the security of energy supply. Therefore a high quality of power flow forecasts for the next few hours within the grid are needed. In this paper we investigate forecasts of the vertical power flow at transformer between the medium and high voltage grid. Forecasting the vertical power flow is challenging due to constantly changing characteristics of the power flow at the transformer. This is mainly a result of dynamic grid topologies, changes in the installed assets, maintenance of the transformer itself as well as the volatile generation. In this paper we present a novel approach to deal with these challenges. For the multi step time series forecasts a Long-Short Term Memory (LSTM) is used. In our presented approach an update process where the model is retrained regularly is investigated and compared to baseline models. The model is retrained as soon as a sufficient amount of new measurements are available. This retraining should capture changes in the characteristic of the transformer that the model has not yet seen in the past and therefore cannot be predicted by the model. For the regular update process we investigate different strategies where especially the number of used epochs are considered, but also different learning rates are used. We show that our new approach significantly outperforms the investigated baseline approaches.      
### 18.Integration of Roadside Camera Images and Weather Data for Monitoring Winter Road Surface Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2009.12165.pdf)
>  During the winter season, real-time monitoring of road surface conditions is critical for the safety of drivers and road maintenance operations. Previous research has evaluated the potential of image classification methods for detecting road snow coverage by processing images from roadside cameras installed in RWIS (Road Weather Information System) stations. However, there are a limited number of RWIS stations across Ontario, Canada; therefore, the network has reduced spatial coverage. In this study, we suggest improving performance on this task through the integration of images and weather data collected from the RWIS stations with images from other MTO (Ministry of Transportation of Ontario) roadside cameras and weather data from Environment Canada stations. We use spatial statistics to quantify the benefits of integrating the three datasets across Southern Ontario, showing evidence of a six-fold increase in the number of available roadside cameras and therefore improving the spatial coverage in the most populous ecoregions in Ontario. Additionally, we evaluate three spatial interpolation methods for inferring weather variables in locations without weather measurement instruments and identify the one that offers the best tradeoff between accuracy and ease of implementation.      
### 19.A Comprehensive Survey of the Tactile Internet: State of the art and Research Directions  [ :arrow_down: ](https://arxiv.org/pdf/2009.12164.pdf)
>  The Internet has made several giant leaps over the years, from a fixed to a mobile Internet, then to the Internet of Things, and now to a Tactile Internet. The Tactile Internet goes far beyond data, audio and video delivery over fixed and mobile networks, and even beyond allowing communication and collaboration among things. It is expected to enable haptic communication and allow skill set delivery over networks. Some examples of potential applications are tele-surgery, vehicle fleets, augmented reality and industrial process automation. Several papers already cover many of the Tactile Internet-related concepts and technologies, such as haptic codecs, applications, and supporting technologies. However, none of them offers a comprehensive survey of the Tactile Internet, including its architectures and algorithms. Furthermore, none of them provides a systematic and critical review of the existing solutions. To address these lacunae, we provide a comprehensive survey of the architectures and algorithms proposed to date for the Tactile Internet. In addition, we critically review them using a well-defined set of requirements and discuss some of the lessons learned as well as the most promising research directions.      
### 20.Enhancing MRI Brain Tumor Segmentation with an Additional Classification Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.12111.pdf)
>  Brain tumor segmentation plays an essential role in medical image analysis. In recent studies, deep convolution neural networks (DCNNs) are extremely powerful to tackle tumor segmentation tasks. We propose in this paper a novel training method that enhances the segmentation results by adding an additional classification branch to the network. The whole network was trained end-to-end on the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 training dataset. On the BraTS's validation set, it achieved an average Dice score of 78.43%, 89.99%, and 84.22% respectively for the enhancing tumor, the whole tumor, and the tumor core.      
### 21.EVM-Constrained and Mask-Compliant MIMO-OFDM Spectral Precoding  [ :arrow_down: ](https://arxiv.org/pdf/2009.12100.pdf)
>  Spectral precoding is a promising technique to suppress out-of-band emissions and comply with leakage constraints over adjacent frequency channels and with mask requirements on the unwanted emissions. However, spectral precoding may distort the original data vector, which is formally expressed as the error vector magnitude (EVM) between the precoded and original data vectors. Notably, EVM has a deleterious impact on the performance of multiple-input multiple-output orthogonal frequency division multiplexing-based systems. In this paper we propose a novel spectral precoding approach which constrains the EVM while complying with the mask requirements. We first formulate and solve the EVM-unconstrained mask-compliant spectral precoding problem, which serves as a springboard to the design of two EVM-constrained spectral precoding schemes. The first scheme takes into account a wideband EVM-constraint which limits the average in-band distortion. The second scheme takes into account frequency-selective EVM-constraints, and consequently, limits the signal distortion at the subcarrier level. Numerical examples illustrate that both proposed schemes outperform previously developed schemes in terms of important performance indicators such as block error rate and system-wide throughput while complying with spectral mask and EVM constraints.      
### 22.Sensor Fault Detection and Isolation via Networked Estimation: Full-Rank Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.12084.pdf)
>  This paper considers the problem of simultaneous sensor fault detection, isolation, and networked estimation of linear full-rank dynamical systems. The proposed networked estimation is a variant of single time-scale protocol and is based on (i) consensus on \textit{a-priori} estimates and (ii) measurement innovation. The necessary connectivity condition on the sensor network and stabilizing block-diagonal gain matrix is derived based on our previous works. Considering additive faults in the presence of system and measurement noise, the estimation error at sensors is derived and proper residuals are defined for fault detection. Unlike many works in the literature, no simplifying upper-bound condition on the noise is considered and we assume Gaussian system/measurement noise. A probabilistic threshold is then defined for fault detection based on the estimation error covariance norm. Finally, a graph-theoretic sensor replacement scenario is proposed to recover possible loss of networked observability due to removing the faulty sensor. We examine the proposed fault detection and isolation scheme on an illustrative academic example to verify the results and make a comparison study with related literature.      
### 23.DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels  [ :arrow_down: ](https://arxiv.org/pdf/2009.12053.pdf)
>  Retinal vessels are important biomarkers for many ophthalmological and cardiovascular diseases. It is of great significance to develop an accurate and fast vessel segmentation model for computer-aided diagnosis. Existing methods, such as U-Net follows the encoder-decoder pipeline, where detailed information is lost in the encoder in order to achieve a large field of view. Although detailed information could be recovered in the decoder via multi-scale fusion, it still contains noise. In this paper, we propose a deep segmentation model, called detail-preserving network (DPN) for efficient vessel segmentation. To preserve detailed spatial information and learn structural information at the same time, we designed the detail-preserving block (DP-Block). Further, we stacked eight DP-Blocks together to form the DPN. More importantly, there are no down-sampling operations among these blocks. As a result, the DPN could maintain a high resolution during the processing, which is helpful to locate the boundaries of thin vessels. To illustrate the effectiveness of our method, we conducted experiments over three public datasets. Experimental results show, compared to state-of-the-art methods, our method shows competitive/better performance in terms of segmentation accuracy, segmentation speed, extensibility and the number of parameters. Specifically, 1) the AUC of our method ranks first/second/third on the STARE/CHASE_DB1/DRIVE datasets, respectively. 2) Only one forward pass is required of our method to generate a vessel segmentation map, and the segmentation speed of our method is over 20-160x faster than other methods on the DRIVE dataset. 3) We conducted cross-training experiments to demonstrate the extensibility of our method, and results revealed that our method shows superior performance. 4) The number of parameters of our method is only around 96k, less then all comparison methods.      
### 24.Deep Autoencoding GMM-based Unsupervised Anomaly Detection in Acoustic Signals and its Hyper-parameter Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2009.12042.pdf)
>  Failures or breakdowns in factory machinery can be costly to companies, so there is an increasing demand for automatic machine inspection. Existing approaches to acoustic signal-based unsupervised anomaly detection, such as those using a deep autoencoder (DA) or Gaussian mixture model (GMM), have poor anomaly-detection performance. In this work, we propose a new method based on a deep autoencoding Gaussian mixture model with hyper-parameter optimization (DAGMM-HO). In our method, the DAGMM-HO applies the conventional DAGMM to the audio domain for the first time, with the idea that its total optimization on reduction of dimensions and statistical modelling will improve the anomaly-detection performance. In addition, the DAGMM-HO solves the hyper-parameter sensitivity problem of the conventional DAGMM by performing hyper-parameter optimization based on the gap statistic and the cumulative eigenvalues. Our evaluation of the proposed method with experimental data of the industrial fans showed that it significantly outperforms previous approaches and achieves up to a 20% improvement based on the standard AUC score.      
### 25.Privacy-Preserving Push-sum Average Consensus via State Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2009.12029.pdf)
>  Average consensus is extensively used in distributed networks for computing and control, where all the agents constantly communicate with each other and update their states in order to reach an agreement. Under the general average consensus algorithm, information exchanged through wireless or wired communication networks could lead to the disclosure of sensitive and private information. In this paper, we propose a privacy-preserving push-sum approach for directed networks that can maintain the privacy of all agents while achieving average consensus simultaneously. Each node {decomposes} its initial state arbitrarily into two substates, the average of which equals to the initial state, in order to guarantee the convergence to the accurate average. Only one substate is exchanged by the node with its neighbours over time, and the other one is kept private. That is to say, only the exchanged substate would be visible to an adversary, preventing the private state information from leakage. Different from the existing state-decomposition approach which only applies to undirected graphs, our proposed approach is applicable to strongly connected digraphs. In addition, in direct contrast to offset-adding based privacy-preserving push-sum algorithm, which is vulnerable to an external eavesdropper, our proposed approach can ensure privacy against both an honest-but-curious node and an external eavesdropper. A numerical simulation is provided to illustrate the effectiveness of the proposed approach.      
### 26.A Meta-learning based Distribution System Load Forecasting Model Selection Framework  [ :arrow_down: ](https://arxiv.org/pdf/2009.12001.pdf)
>  This paper presents a meta-learning based, automatic distribution system load forecasting model selection framework. The framework includes the following processes: feature extraction, candidate model labeling, offline training, and online model recommendation. Using user load forecasting needs as input features, multiple meta-learners are used to rank the available load forecast models based on their forecasting accuracy. Then, a scoring-voting mechanism weights recommendations from each meta-leaner to make the final recommendations. Heterogeneous load forecasting tasks with different temporal and technical requirements at different load aggregation levels are set up to train, validate, and test the performance of the proposed framework. Simulation results demonstrate that the performance of the meta-learning based approach is satisfactory in both seen and unseen forecasting tasks.      
### 27.Event-Driven Receding Horizon Control for Distributed Estimation in Network Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.11958.pdf)
>  This paper considers the multi-agent persistent monitoring problem defined on a network (graph) of nodes (targets) with uncertain states. The agent team's goal is to persistently observe the target states so that an overall measure of estimation error covariance evaluated over a finite period is minimized. Each agent's trajectory is fully defined by the sequence of targets it visits and the corresponding dwell times spent at each visited target. To find the optimal set of agent trajectories, we propose a distributed and on-line estimation process that requires each agent to solve a sequence of receding horizon control problems (RHCPs) in an event-driven manner. We use a novel objective function form for these RHCPs to optimize the effectiveness of this distributed estimation process and establish its unimodality under certain conditions. Moreover, we show how agents can use machine learning to efficiently and accurately solve a significant portion of each RHCP they face. Finally, extensive numerical results are provided, indicating significant improvements compared to other agent control methods.      
### 28.Compressive Spectral Image Classification using 3D Coded Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.11948.pdf)
>  Hyperspectral image classification (HIC) is an active research topic in remote sensing. However, the huge volume of three-dimensional (3D) hyperspectral images poses big challenges in data acquisition, storage, transmission and processing. To overcome these limitations, this paper develops a novel deep learning HIC approach based on the compressive measurements of coded-aperture snapshot spectral imaging (CASSI) system, without reconstructing the complete hyperspectral data cube. A new kind of deep learning strategy, namely 3D coded convolutional neural network (3D-CCNN) is proposed to efficiently solve for the HIC problem, where the hardware-based coded aperture is regarded as a pixel-wise connected network layer. An end-to-end training method is developed to jointly optimize the network parameters and the coded aperture pattern with periodic structure. The accuracy of HIC approach is effectively improved by involving the degrees of optimization freedom from the coded aperture. The superiority of the proposed method is assessed on some public hyperspectral datasets over the state-of-the-art HIC methods.      
### 29.A distributed service-matching coverage via heterogeneous mobile agents  [ :arrow_down: ](https://arxiv.org/pdf/2009.11943.pdf)
>  We propose a distributed deployment solution for a group of mobile agents that should provide a service for a dense set of targets. The agents are heterogeneous in a sense that their quality of service (QoS), modeled as a spatial Gaussian distribution, is different. To provide the best service, the objective is to deploy the agents such that their collective QoS distribution is as close as possible to the density distribution of the targets. We propose a distributed consensus-based expectation-maximization (EM) algorithm to estimate the target density distribution, modeled as a Gaussian mixture model (GMM). The GMM not only gives an estimate of the targets' distribution, but also partitions the area to subregions, each of which is represented by one of the GMM's Gaussian bases. We use the Kullback-Leibler divergence (KLD) to evaluate the similarity between the QoS distribution of each agent and each Gaussian basis/subregion. Then, a distributed assignment problem is formulated and solved as a discrete optimal mass transport problem that allocates each agent to a subregion by taking the KLD as the assignment cost. We demonstrate our results by a sensor deployment for event detection where the sensor's QoS is modeled as an anisotropic Gaussian distribution.      
### 30.Observer-based Event-triggered Boundary Control of a Class of Reaction-Diffusion PDEs  [ :arrow_down: ](https://arxiv.org/pdf/2009.11936.pdf)
>  This paper presents an observer-based event-triggered boundary control strategy for a class of reaction-diffusion PDEs with Robin actuation. The observer only requires boundary measurements. The control approach consists of a backstepping output feedback boundary controller, derived using estimated states, and a dynamic triggering condition, which determines the time instants at which the control input needs to be updated. It is shown that under the proposed observer-based event-triggered boundary control approach, there is a minimal dwell-time between two triggering instants independent of initial conditions. Furthermore, the well-posedness and the global exponential convergence of the closed-loop system to the equilibrium point are established. A simulation example is provided to validate the theoretical developments.      
### 31.RSS-based LTE Base Station Localization Using Single Receiver in Environment with Unknown Path-Loss Exponent  [ :arrow_down: ](https://arxiv.org/pdf/2009.11912.pdf)
>  With the increasing demand for location-based services, localization technology research has recently intensified. Received signal strength (RSS)-based localization has the advantage of simplicity. However, as RSS-based localization requires the path-loss model parameters, it is difficult to use in place on which those parameters are unknown. In prior research, a transmitter localization algorithm with multiple stationary receivers was proposed for use under unknown path-loss exponent (PLE) conditions. However, if a mobile receiver is utilized, the localization would be possible with a single receiver alone. In this paper, we suggest a method of RSS-based LTE base station (BS) localization with a single mobile receiver when the PLE is unknown. We also propose an efficient mobile-receiver movement method to improve the PLE estimation and BS localization accuracy. Simulation results demonstrate the performance of the proposed methods.      
### 32.A Preliminary Study of Machine-Learning-Based Ranging with LTE Channel Impulse Response in Multipath Environment  [ :arrow_down: ](https://arxiv.org/pdf/2009.11910.pdf)
>  Alternative navigation technology to global navigation satellite systems (GNSSs) is required for unmanned ground vehicles (UGVs) in multipath environments (such as urban areas). In urban areas, long-term evolution (LTE) signals can be received ubiquitously at high power without any additional infrastructure. We present a machine learning approach to estimate the range between the LTE base station and UGV based on the LTE channel impulse response (CIR). The CIR, which includes information of signal attenuation from the channel, was extracted from the LTE physical layer using a software-defined radio (SDR). We designed a convolutional neural network (CNN) that estimates ranges with the CIR as input. The proposed method demonstrated better ranging performance than a received signal strength indicator (RSSI)-based method during our field test.      
### 33.Neural Network-Based Ranging with LTE Channel Impulse Response for Localization in Indoor Environments  [ :arrow_down: ](https://arxiv.org/pdf/2009.11907.pdf)
>  A neural network (NN)-based approach for indoor localization via cellular long-term evolution (LTE) signals is proposed. The approach estimates, from the channel impulse response (CIR), the range between an LTE eNodeB and a receiver. A software-defined radio (SDR) extracts the CIR, which is fed to a long short-term memory model (LSTM) recurrent neural network (RNN) to estimate the range. Experimental results are presented comparing the proposed approach against a baseline RNN without LSTM. The results show a receiver navigating for 100 m in an indoor environment, while receiving signals from one LTE eNodeB. The ranging root-mean squared error (RMSE) and ranging maximum error along the receiver's trajectory were reduced from 13.11 m and 55.68 m, respectively, in the baseline RNN to 9.02 m and 27.40 m, respectively, with the proposed RNN-LSTM.      
### 34.Sequence-to-Sequence Load Disaggregation Using Multi-Scale Residual Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.12355.pdf)
>  With the increased demand on economy and efficiency of measurement technology, Non-Intrusive Load Monitoring (NILM) has received more and more attention as a cost-effective way to monitor electricity and provide feedback to users. Deep neural networks has been shown a great potential in the field of load disaggregation. In this paper, firstly, a new convolutional model based on residual blocks is proposed to avoid the degradation problem which traditional networks more or less suffer from when network layers are increased in order to learn more complex features. Secondly, we propose dilated convolution to curtail the excessive quantity of model parameters and obtain bigger receptive field, and multi-scale structure to learn mixed data features in a more targeted way. Thirdly, we give details about generating training and test set under certain rules. Finally, the algorithm is tested on real-house public dataset, UK-DALE, with three existing neural networks. The results are compared and analysed, the proposed model shows improvements on F1 score, MAE as well as model complexity across different appliances.      
### 35.Multilayer Minkowski Reflectarray Antenna with Improved Phase Performance  [ :arrow_down: ](https://arxiv.org/pdf/2009.12343.pdf)
>  We propose a multi-layer unit cell consisting of Minkowski fractal shaped reflector with aperture coupled phasing stubs to obtain a broad phase range for the reflectarray antenna with smaller unit cell size and inter-element spacing compared to some other studies in the literature. In the study, simulations are conducted using HFSS(TM) program with Floquet Port incidence. A unit cell with two cycles of phase range and very low reflection loss is designed. 21.9% shrinkage is achieved in patch surface area using Minkowski fractals. Subsequently, proposed unit cell is used to design and fabricate a full scale 221 element reflectarray antenna. At 10 GHz, simulation results are compared with measured data and good agreement is observed.      
### 36.SemanticVoxels: Sequential Fusion for 3D Pedestrian Detection using LiDAR Point Cloud and Semantic Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.12276.pdf)
>  3D pedestrian detection is a challenging task in automated driving because pedestrians are relatively small, frequently occluded and easily confused with narrow vertical objects. LiDAR and camera are two commonly used sensor modalities for this task, which should provide complementary information. Unexpectedly, LiDAR-only detection methods tend to outperform multisensor fusion methods in public benchmarks. Recently, PointPainting has been presented to eliminate this performance drop by effectively fusing the output of a semantic segmentation network instead of the raw image information. In this paper, we propose a generalization of PointPainting to be able to apply fusion at different levels. After the semantic augmentation of the point cloud, we encode raw point data in pillars to get geometric features and semantic point data in voxels to get semantic features and fuse them in an effective way. Experimental results on the KITTI test set show that SemanticVoxels achieves state-of-the-art performance in both 3D and bird's eye view pedestrian detection benchmarks. In particular, our approach demonstrates its strength in detecting challenging pedestrian cases and outperforms current state-of-the-art approaches.      
### 37.Database Annotation with few Examples: An Atlas-based Framework using Diffeomorphic Registration of 3D Trees  [ :arrow_down: ](https://arxiv.org/pdf/2009.12252.pdf)
>  Automatic annotation of anatomical structures can help simplify workflow during interventions in numerous clinical applications but usually involves a large amount of annotated data. The complexity of the labeling task, together with the lack of representative data, slows down the development of robust solutions. In this paper, we propose a solution requiring very few annotated cases to label 3D pelvic arterial trees of patients with benign prostatic hyperplasia. We take advantage of Large Deformation Diffeomorphic Metric Mapping (LDDMM) to perform registration based on meaningful deformations from which we build an atlas. Branch pairing is then computed from the atlas to new cases using optimal transport to ensure one-to-one correspondence during the labeling process. To tackle topological variations in the tree, which usually degrades the performance of atlas-based techniques, we propose a simple bottom-up label assignment adapted to the pelvic anatomy. The proposed method achieves 97.6\% labeling precision with only 5 cases for training, while in comparison learning-based methods only reach 82.2\% on such small training sets.      
### 38.Safe Coverage of Moving Domains for Vehicles with Second Order Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2009.12211.pdf)
>  Autonomous coverage of a specified area by robots operating in close proximity with each other has many potential applications such as real-time monitoring of rapidly changing environments, and search and rescue; however, coordination and safety are two fundamental challenges. For coordination, we propose a distributed controller for covering moving, compact domains for two types of vehicles with second order dynamics (double integrator and fixed-wing aircraft) with bounded input forces. This control policy is based on artificial potentials and alignment forces designed to promote desired vehicle-domain and inter-vehicle separations and relative velocities. We prove that certain coverage configurations are locally asymptotically stable. For safety, we establish energy conditions for collision free motion and utilize Hamilton-Jacobi (HJ) reachability theory for last-resort pairwise collision avoidance. We derive an analytical solution to the associated HJ partial differential equation corresponding to the collision avoidance problem between two double integrator vehicles. We demonstrate our approach in several numerical simulations involving the two types of vehicles covering convex and non-convex moving domains.      
### 39.Emergence of complex data from simple local rules in a network game  [ :arrow_down: ](https://arxiv.org/pdf/2009.12210.pdf)
>  As one of the main subjects of investigation in data science, network science has been demonstrated a wide range of applications to real-world networks analysis and modeling. For example, the pervasive presence of structural or topological characteristics, such as the small-world phenomenon, small-diameter, scale-free properties, or fat-tailed degree distribution were one of the underlying pillars fostering the study of complex networks. Relating these phenomena with other emergent properties in complex systems became a subject of central importance. By introducing new implications on the interface between data science and complex systems science with the purpose of tackling some of these issues, in this article we present a model for a network game played by complex networks in which nodes are computable systems. In particular, we present and discuss how some network topological properties and simple local communication rules are able to generate a phase transition with respect to the emergence of incompressible data.      
### 40.Tarsier: Evolving Noise Injection in Super-Resolution GANs  [ :arrow_down: ](https://arxiv.org/pdf/2009.12177.pdf)
>  Super-resolution aims at increasing the resolution and level of detail within an image. The current state of the art in general single-image super-resolution is held by NESRGAN+, which injects a Gaussian noise after each residual layer at training time. In this paper, we harness evolutionary methods to improve NESRGAN+ by optimizing the noise injection at inference time. More precisely, we use Diagonal CMA to optimize the injected noise according to a novel criterion combining quality assessment and realism. Our results are validated by the PIRM perceptual score and a human study. Our method outperforms NESRGAN+ on several standard super-resolution datasets. More generally, our approach can be used to optimize any method based on noise injection.      
### 41.Spatial-Temporal Demand Forecasting and Competitive Supply via Graph Convolutional Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.12157.pdf)
>  We consider a setting with an evolving set of requests for transportation from an origin to a destination before a deadline and a set of agents capable of servicing the requests. In this setting, an assignment authority is to assign agents to requests such that the average idle time of the agents is minimized. An example is the scheduling of taxis (agents) to meet incoming requests for trips while ensuring that the taxis are empty as little as possible. In this paper, we study the problem of spatial-temporal demand forecasting and competitive supply (SOUP). We address the problem in two steps. First, we build a granular model that provides spatial-temporal predictions of requests. Specifically, we propose a Spatial-Temporal Graph Convolutional Sequential Learning (ST-GCSL) algorithm that predicts the service requests across locations and time slots. Second, we provide means of routing agents to request origins while avoiding competition among the agents. In particular, we develop a demand-aware route planning (DROP) algorithm that considers both the spatial-temporal predictions and the supplydemand state. We report on extensive experiments with realworld and synthetic data that offer insight into the performance of the solution and show that it is capable of outperforming the state-of-the-art proposals.      
### 42.A Feature Importance Analysis for Soft-Sensing-Based Predictions in a Chemical Sulphonation Process  [ :arrow_down: ](https://arxiv.org/pdf/2009.12133.pdf)
>  In this paper we present the results of a feature importance analysis of a chemical sulphonation process. The task consists of predicting the neutralization number (NT), which is a metric that characterizes the product quality of active detergents. The prediction is based on a dataset of environmental measurements, sampled from an industrial chemical process. We used a soft-sensing approach, that is, predicting a variable of interest based on other process variables, instead of directly sensing the variable of interest. Reasons for doing so range from expensive sensory hardware to harsh environments, e.g., inside a chemical reactor. The aim of this study was to explore and detect which variables are the most relevant for predicting product quality, and to what degree of precision. We trained regression models based on linear regression, regression tree and random forest. A random forest model was used to rank the predictor variables by importance. Then, we trained the models in a forward-selection style by adding one feature at a time, starting with the most important one. Our results show that it is sufficient to use the top 3 important variables, out of the 8 variables, to achieve satisfactory prediction results. On the other hand, Random Forest obtained the best result when trained with all variables.      
### 43.Towards the Automation of a Chemical Sulphonation Process with Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.12125.pdf)
>  Nowadays, the continuous improvement and automation of industrial processes has become a key factor in many fields, and in the chemical industry, it is no exception. This translates into a more efficient use of resources, reduced production time, output of higher quality and reduced waste. Given the complexity of today's industrial processes, it becomes infeasible to monitor and optimize them without the use of information technologies and analytics. In recent years, machine learning methods have been used to automate processes and provide decision support. All of this, based on analyzing large amounts of data generated in a continuous manner. In this paper, we present the results of applying machine learning methods during a chemical sulphonation process with the objective of automating the product quality analysis which currently is performed manually. We used data from process parameters to train different models including Random Forest, Neural Network and linear regression in order to predict product quality values. Our experiments showed that it is possible to predict those product quality values with good accuracy, thus, having the potential to reduce time. Specifically, the best results were obtained with Random Forest with a mean absolute error of 0.089 and a correlation of 0.978.      
### 44.Training CNNs in Presence of JPEG Compression: Multimedia Forensics vs Computer Vision  [ :arrow_down: ](https://arxiv.org/pdf/2009.12088.pdf)
>  Convolutional Neural Networks (CNNs) have proved very accurate in multiple computer vision image classification tasks that required visual inspection in the past (e.g., object recognition, face detection, etc.). Motivated by these astonishing results, researchers have also started using CNNs to cope with image forensic problems (e.g., camera model identification, tampering detection, etc.). However, in computer vision, image classification methods typically rely on visual cues easily detectable by human eyes. Conversely, forensic solutions rely on almost invisible traces that are often very subtle and lie in the fine details of the image under analysis. For this reason, training a CNN to solve a forensic task requires some special care, as common processing operations (e.g., resampling, compression, etc.) can strongly hinder forensic traces. In this work, we focus on the effect that JPEG has on CNN training considering different computer vision and forensic image classification problems. Specifically, we consider the issues that rise from JPEG compression and misalignment of the JPEG grid. We show that it is necessary to consider these effects when generating a training dataset in order to properly train a forensic detector not losing generalization capability, whereas it is almost possible to ignore these effects for computer vision tasks.      
### 45.Computation Bits Maximization in a Backscatter Assisted Wirelessly Powered MEC Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.12087.pdf)
>  In this paper, we introduce a backscatter assisted wirelessly powered mobile edge computing (MEC) network, where each edge user (EU) can offload task bits to the MEC server via hybrid harvest-then-transmit (HTT) and backscatter communications. In particular, considering a practical non-linear energy harvesting (EH) model and a partial offloading scheme at each EU, we propose a scheme to maximize the weighted sum computation bits of all the EUs by jointly optimizing the backscatter reflection coefficient and time, active transmission power and time, local computing frequency and execution time of each EU. By introducing a series of auxiliary variables and using the properties of the non-linear EH model, we transform the original non-convex problem into a convex one and derive closedform expressions for parts of the optimal solutions. Simulation results demonstrate the advantage of the proposed scheme over benchmark schemes in terms of weighted sum computation bits.      
### 46.Influence of segmentation accuracy in structural MR head scans on electric field computation for TMS and tES  [ :arrow_down: ](https://arxiv.org/pdf/2009.12015.pdf)
>  In several diagnosis and therapy procedures based on electrostimulation effect, the internal physical quantity related to the stimulation is the induced electric field. To estimate the induced electric field in an individual human model, the segmentation of anatomical imaging, such as (magnetic resonance image (MRI) scans, of the corresponding body parts into tissues is required. Then, electrical properties associated with different annotated tissues are assigned to the digital model to generate a volume conductor. An open question is how segmentation accuracy of different tissues would influence the distribution of the induced electric field. In this study, we applied parametric segmentation of different tissues to exploit the segmentation of available MRI to generate different quality of head models using deep learning neural network architecture, named ForkNet. Then, the induced electric field are compared to assess the effect of model segmentation variations. Computational results indicate that the influence of segmentation error is tissue-dependent. In brain, sensitivity to segmentation accuracy is relatively high in cerebrospinal fluid (CSF), moderate in gray matter (GM) and low in white matter for transcranial magnetic stimulation (TMS) and transcranial electrical stimulation (tES). A CSF segmentation accuracy reduction of 10% in terms of Dice coefficient (DC) lead to decrease up to 4% in normalized induced electric field in both applications. However, a GM segmentation accuracy reduction of 5.6% DC leads to increase of normalized induced electric field up to 6%. Opposite trend of electric field variation was found between CSF and GM for both TMS and tES. The finding obtained here would be useful to quantify potential uncertainty of computational results.      
### 47.Predicting Parkinson's Disease with Multimodal Irregularly Collected Longitudinal Smartphone Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.11999.pdf)
>  Parkinsons Disease is a neurological disorder and prevalent in elderly people. Traditional ways to diagnose the disease rely on in-person subjective clinical evaluations on the quality of a set of activity tests. The high-resolution longitudinal activity data collected by smartphone applications nowadays make it possible to conduct remote and convenient health assessment. However, out-of-lab tests often suffer from poor quality controls as well as irregularly collected observations, leading to noisy test results. To address these issues, we propose a novel time-series based approach to predicting Parkinson's Disease with raw activity test data collected by smartphones in the wild. The proposed method first synchronizes discrete activity tests into multimodal features at unified time points. Next, it distills and enriches local and global representations from noisy data across modalities and temporal observations by two attention modules. With the proposed mechanisms, our model is capable of handling noisy observations and at the same time extracting refined temporal features for improved prediction performance. Quantitative and qualitative results on a large public dataset demonstrate the effectiveness of the proposed approach.      
### 48.An original framework for Wheat Head Detection using Deep, Semi-supervised and Ensemble Learning within Global Wheat Head Detection (GWHD) Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2009.11977.pdf)
>  In this paper, we propose an original object detection methodology applied to Global Wheat Head Detection (GWHD) Dataset. We have been through two major architectures of object detection which are FasterRCNN and EfficientDet, in order to design a novel and robust wheat head detection model. We emphasize on optimizing the performance of our proposed final architectures. Furthermore, we have been through an extensive exploratory data analysis and adapted best data augmentation techniques to our context. We use semi supervised learning to boost previous supervised models of object detection. Moreover, we put much effort on ensemble to achieve higher performance. Finally we use specific post-processing techniques to optimize our wheat head detection results. Our results have been submitted to solve a research challenge launched on the GWHD Dataset which is led by nine research institutes from seven countries. Our proposed method was ranked within the top 6% in the above mentioned challenge.      
### 49.CoFF: Cooperative Spatial Feature Fusion for 3D Object Detection on Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2009.11975.pdf)
>  To reduce the amount of transmitted data, feature map based fusion is recently proposed as a practical solution to cooperative 3D object detection by autonomous vehicles. The precision of object detection, however, may require significant improvement, especially for objects that are far away or occluded. To address this critical issue for the safety of autonomous vehicles and human beings, we propose a cooperative spatial feature fusion (CoFF) method for autonomous vehicles to effectively fuse feature maps for achieving a higher 3D object detection performance. Specially, CoFF differentiates weights among feature maps for a more guided fusion, based on how much new semantic information is provided by the received feature maps. It also enhances the inconspicuous features corresponding to far/occluded objects to improve their detection precision. Experimental results show that CoFF achieves a significant improvement in terms of both detection precision and effective detection range for autonomous vehicles, compared to previous feature fusion solutions.      
### 50.N-BEATS neural network for mid-term electricity load forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2009.11961.pdf)
>  We address the mid-term electricity load forecasting (MTLF) problem. This problem is relevant and challenging. On the one hand, MTLF supports high-level (e.g. country level) decision-making at distant planning horizons (e.g. month, quarter, year). Therefore, financial impact of associated decisions may be significant and it is desirable that they be made based on accurate forecasts. On the other hand, the country level monthly time-series typically associated with MTLF are very complex and stochastic -- including trends, seasonality and significant random fluctuations. In this paper we show that our proposed deep neural network modelling approach based on the N-BEATS neural architecture is very effective at solving MTLF problem. N-BEATS has high expressive power to solve non-linear stochastic forecasting problems. At the same time, it is simple to implement and train, it does not require signal preprocessing. We compare our approach against the set of ten baseline methods, including classical statistical methods, machine learning and hybrid approaches on 35 monthly electricity demand time series for European countries. We show that in terms of the MAPE error metric our method provides statistically significant relative gain of 25% with respect to the classical statistical methods, 28% with respect to classical machine learning methods and 14% with respect to the advanced state-of-the-art hybrid methods combining machine learning and statistical approaches.      
### 51.A Computer Vision Approach to Combat Lyme Disease  [ :arrow_down: ](https://arxiv.org/pdf/2009.11931.pdf)
>  Lyme disease is an infectious disease transmitted to humans by a bite from an infected Ixodes species (blacklegged ticks). It is one of the fastest growing vector-borne illness in North America and is expanding its geographic footprint. Lyme disease treatment is time-sensitive, and can be cured by administering an antibiotic (prophylaxis) to the patient within 72 hours after a tick bite by the Ixodes species. However, the laboratory-based identification of each tick that might carry the bacteria is time-consuming and labour intensive and cannot meet the maximum turn-around-time of 72 hours for an effective treatment. Early identification of blacklegged ticks using computer vision technologies is a potential solution in promptly identifying a tick and administering prophylaxis within a crucial window period. In this work, we build an automated detection tool that can differentiate blacklegged ticks from other ticks species using advanced deep learning and computer vision approaches. We demonstrate the classification of tick species using Convolution Neural Network (CNN) models, trained end-to-end from tick images directly. Advanced knowledge transfer techniques within teacher-student learning frameworks are adopted to improve the performance of classification of tick species. Our best CNN model achieves 92% accuracy on test set. The tool can be integrated with the geography of exposure to determine the risk of Lyme disease infection and need for prophylaxis treatment.      
### 52.Resting state-fMRI approach towards understanding impairments in mTLE  [ :arrow_down: ](https://arxiv.org/pdf/2009.11928.pdf)
>  Mesial temporal lobe epilepsy (mTLE) is the most common form of epilepsy. While it is characterized by an epileptogenic focus in the mesial temporal lobe, it is increasingly understood as a network disorder. Hence, understanding the nature of impairments on a network level is essential for its diagnosis and treatment. In this work, we review recent works that apply resting-state functional MRI to provide key insights into the impairments to the functional architecture in mTLE. We discuss changes on both regional and global scales. Finally, we describe how Machine Learning can be applied to rs-fMRI data to extract resting-state networks specific to mTLE and for automated diagnosis of this disease.      
### 53.GANs with Variational Entropy Regularizers: Applications in Mitigating the Mode-Collapse Issue  [ :arrow_down: ](https://arxiv.org/pdf/2009.11921.pdf)
>  Building on the success of deep learning, Generative Adversarial Networks (GANs) provide a modern approach to learn a probability distribution from observed samples. GANs are often formulated as a zero-sum game between two sets of functions; the generator and the discriminator. Although GANs have shown great potentials in learning complex distributions such as images, they often suffer from the mode collapse issue where the generator fails to capture all existing modes of the input distribution. As a consequence, the diversity of generated samples is lower than that of the observed ones. To tackle this issue, we take an information-theoretic approach and maximize a variational lower bound on the entropy of the generated samples to increase their diversity. We call this approach GANs with Variational Entropy Regularizers (GAN+VER). Existing remedies for the mode collapse issue in GANs can be easily coupled with our proposed variational entropy regularization. Through extensive experimentation on standard benchmark datasets, we show all the existing evaluation metrics highlighting difference of real and generated samples are significantly improved with GAN+VER.      
### 54.PK-GCN: Prior Knowledge Assisted Image Classification using Graph Convolution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.11892.pdf)
>  Deep learning has gained great success in various classification tasks. Typically, deep learning models learn underlying features directly from data, and no underlying relationship between classes are included. Similarity between classes can influence the performance of classification. In this article, we propose a method that incorporates class similarity knowledge into convolutional neural networks models using a graph convolution layer. We evaluate our method on two benchmark image datasets: MNIST and CIFAR10, and analyze the results on different data and model sizes. Experimental results show that our model can improve classification accuracy, especially when the amount of available data is small.      
### 55.Toward Adaptive Trust Calibration for Level 2 Driving Automation  [ :arrow_down: ](https://arxiv.org/pdf/2009.11890.pdf)
>  Properly calibrated human trust is essential for successful interaction between humans and automation. However, while human trust calibration can be improved by increased automation transparency, too much transparency can overwhelm human workload. To address this tradeoff, we present a probabilistic framework using a partially observable Markov decision process (POMDP) for modeling the coupled trust-workload dynamics of human behavior in an action-automation context. We specifically consider hands-off Level 2 driving automation in a city environment involving multiple intersections where the human chooses whether or not to rely on the automation. We consider automation reliability, automation transparency, and scene complexity, along with human reliance and eye-gaze behavior, to model the dynamics of human trust and workload. We demonstrate that our model framework can appropriately vary automation transparency based on real-time human trust and workload belief estimates to achieve trust calibration.      
