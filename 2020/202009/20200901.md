# ArXiv eess --Tue, 1 Sep 2020
### 1.An Efficient Algorithm To Compute Norms for Finite Horizon, Linear Time-Varying Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.13779.pdf)
>  We present an efficient algorithm to compute the induced norms of finite-horizon Linear Time-Varying (LTV) systems. The formulation includes both induced $\mathcal{L}_2$ and terminal Euclidean norm penalties. Existing computational approaches include the power iteration and Riccati Differential Equation (RDE) methods. The proposed algorithm combines the two approaches. This yields an efficient algorithm to compute provable upper and lower bounds on the induced norm within the desired tolerance. The algorithm also provides a worst-case disturbance input that achieves the lower bound on the norm. We also present a new proof which shows that the power iteration for this problem converges monotonically. Finally, we show a controllability Gramian based computational method in the special case of induced $\mathcal{L}_2$-to-Euclidean norm. This can be used to compute the reachable set at any time on the horizon. Numerical examples are provided to demonstrate the proposed algorithm.      
### 2.A Lagrangian Method for Constrained Dynamics in Tensegrity Systems with Compressible Bars  [ :arrow_down: ](https://arxiv.org/pdf/2008.13772.pdf)
>  This paper presents a Lagrangian approach to simulating multibody dynamics in a tensegrity framework with an ability to tackle holonomic constraint violations in an energy-preserving scheme. Governing equations are described using non-minimum coordinates to simplify descriptions of the structure's kinematics. To minimize constraint drift arising from this redundant system, the direct correction method has been employed in conjunction with a novel energy-correcting scheme that treats the total mechanical energy of the system as a supplementary constraint. The formulation has been extended to allow tensegrity structures with compressible bars, allowing for further discussion on potential choices for softer bar materials. The benchmark example involving a common tensegrity structure demonstrates the superiority of the presented formulation over Simscape Multibody in terms of motion accuracy as well as energy conservation. The effectiveness of the energy correction scheme is found to be increasing with the extent of deformations in the structure.      
### 3.A Derivation of Identifiable Condition for Non-Uniform Linear Array DOA Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2008.13755.pdf)
>  Phase ambiguity happens in uniform linear arrays (ULAs) when the sensor distance is greater than $\lambda/2$. This problem in direction of arrival (DOA) estimation and can be solved by designing a proper sensor configuration, such as . In this work, we derive the identifiable condition for ULA DOA estimation.      
### 4.Plug-and-Play Image Restoration with Deep Denoiser Prior  [ :arrow_down: ](https://arxiv.org/pdf/2008.13751.pdf)
>  Recent works on plug-and-play image restoration have shown that a denoiser can implicitly serve as the image prior for model-based methods to solve many inverse problems. Such a property induces considerable advantages for plug-and-play image restoration (e.g., integrating the flexibility of model-based method and effectiveness of learning-based methods) when the denoiser is discriminatively learned via deep convolutional neural network (CNN) with large modeling capacity. However, while deeper and larger CNN models are rapidly gaining popularity, existing plug-and-play image restoration hinders its performance due to the lack of suitable denoiser prior. In order to push the limits of plug-and-play image restoration, we set up a benchmark deep denoiser prior by training a highly flexible and effective CNN denoiser. We then plug the deep denoiser prior as a modular part into a half quadratic splitting based iterative algorithm to solve various image restoration problems. We, meanwhile, provide a thorough analysis of parameter setting, intermediate results and empirical convergence to better understand the working mechanism. Experimental results on three representative image restoration tasks, including deblurring, super-resolution and demosaicing, demonstrate that the proposed plug-and-play image restoration with deep denoiser prior not only significantly outperforms other state-of-the-art model-based methods but also achieves competitive or even superior performance against state-of-the-art learning-based methods. The source code is available at <a class="link-external link-https" href="https://github.com/cszn/DPIR" rel="external noopener nofollow">this https URL</a>.      
### 5.Data-driven Outer-Loop Control Using Deep Reinforcement Learning for Trajectory Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2008.13732.pdf)
>  Reference tracking systems involve a plant that is stabilized by a local feedback controller and a command center that indicates the reference set-point the plant should follow. Typically, these systems are subject to limitations such as disturbances, systems delays, constraints, uncertainties, underperforming controllers, and unmodeled parameters that do not allow them to achieve the desired performance. In situations where it is not possible to redesign the inner-loop system, it is usual to incorporate an outer-loop control that instructs the system to follow a modified reference path such that the resultant path is close to the ideal one. Typically, strategies to design the outer-loop control need to know a model of the system, which can be an unfeasible task. In this paper, we propose a framework based on deep reinforcement learning that can learn a policy to generate a modified reference that improves the system's performance in a non-invasive and model-free fashion. To illustrate the effectiveness of our approach, we present two challenging cases in engineering: a flight control with a pilot model that includes human reaction delays, and a mean-field control problem for a massive number of space-heating devices. The proposed strategy successfully designs a reference signal that works even in situations that were not seen during the learning process.      
### 6.Left atrial ejection fraction estimation using SEGANet for fully automated segmentation of CINE MRI  [ :arrow_down: ](https://arxiv.org/pdf/2008.13718.pdf)
>  Atrial fibrillation (AF) is the most common sustained cardiac arrhythmia, characterised by a rapid and irregular electrical activation of the atria. Treatments for AF are often ineffective and few atrial biomarkers exist to automatically characterise atrial function and aid in treatment selection for AF. Clinical metrics of left atrial (LA) function, such as ejection fraction (EF) and active atrial contraction ejection fraction (aEF), are promising, but have until now typically relied on volume estimations extrapolated from single-slice images. In this work, we study volumetric functional biomarkers of the LA using a fully automatic SEGmentation of the left Atrium based on a convolutional neural Network (SEGANet). SEGANet was trained using a dedicated data augmentation scheme to segment the LA, across all cardiac phases, in short axis dynamic (CINE) Magnetic Resonance Images (MRI) acquired with full cardiac coverage. Using the automatic segmentations, we plotted volumetric time curves for the LA and estimated LA EF and aEF automatically. The proposed method yields high quality segmentations that compare well with manual segmentations (Dice scores [$0.93 \pm 0.04$], median contour [$0.75 \pm 0.31$] mm and Hausdorff distances [$4.59 \pm 2.06$] mm). LA EF and aEF are also in agreement with literature values and are significantly higher in AF patients than in healthy volunteers. Our work opens up the possibility of automatically estimating LA volumes and functional biomarkers from multi-slice CINE MRI, bypassing the limitations of current single-slice methods and improving the characterisation of atrial function in AF patients.      
### 7.Unpaired Learning of Deep Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2008.13711.pdf)
>  We investigate the task of learning blind image denoising networks from an unpaired set of clean and noisy images. Such problem setting generally is practical and valuable considering that it is feasible to collect unpaired noisy and clean images in most real-world applications. And we further assume that the noise can be signal dependent but is spatially uncorrelated. In order to facilitate unpaired learning of denoising network, this paper presents a two-stage scheme by incorporating self-supervised learning and knowledge distillation. For self-supervised learning, we suggest a dilated blind-spot network (D-BSN) to learn denoising solely from real noisy images. Due to the spatial independence of noise, we adopt a network by stacking 1x1 convolution layers to estimate the noise level map for each image. Both the D-BSN and image-specific noise model (CNN\_est) can be jointly trained via maximizing the constrained log-likelihood. Given the output of D-BSN and estimated noise level map, improved denoising performance can be further obtained based on the Bayes' rule. As for knowledge distillation, we first apply the learned noise models to clean images to synthesize a paired set of training images, and use the real noisy images and the corresponding denoising results in the first stage to form another paired set. Then, the ultimate denoising model can be distilled by training an existing denoising network using these two paired sets. Experiments show that our unpaired learning method performs favorably on both synthetic noisy images and real-world noisy photographs in terms of quantitative and qualitative evaluation.      
### 8.Switchable Deep Beamformer  [ :arrow_down: ](https://arxiv.org/pdf/2008.13646.pdf)
>  Recent proposals of deep beamformers using deep neural networks have attracted significant attention as computational efficient alternatives to adaptive and compressive beamformers. Moreover, deep beamformers are versatile in that image post-processing algorithms can be combined with the beamforming. Unfortunately, in the current technology, a separate beamformer should be trained and stored for each application, demanding significant scanner resources. To address this problem, here we propose a {\em switchable} deep beamformer that can produce various types of output such as DAS, speckle removal, deconvolution, etc., using a single network with a simple switch. In particular, the switch is implemented through Adaptive Instanace Normalization (AdaIN) layers, so that various output can be generated by merely changing the AdaIN code. Experimental results using B-mode focused ultrasound confirm the flexibility and efficacy of the proposed methods for various applications.      
### 9.A Dataset of Human Motion Status Using IR-UWB Through-wall Radar  [ :arrow_down: ](https://arxiv.org/pdf/2008.13598.pdf)
>  Ultra-wideband (UWB) through-wall radar has a wide range of applications in non-contact human information detection and monitoring. With the integration of machine learning technology, its potential prospects include the physiological monitoring of patients in the hospital environment and the daily monitoring at home. Although many target detection methods of UWB through-wall radar based on machine learning have been proposed, there is a lack of an opensource dataset to evaluate the performance of the algorithm. This published dataset was measured by impulse radio UWB (IR-UWB) through-wall radar system. Three test subjects were measured in different environments and several defined motion statuses. Using the presented dataset, we propose a human-motion-status recognition method using a convolutional neural network (CNN), the detailed dataset partition method and recognition process flow is given. On the well-trained network, the recognition accuracy of testing data for three kinds of motion statuses is higher than 99.7%. The dataset presented in this paper considers a simple environment. Therefore, we call on all organizations in the UWB radar field to cooperate to build opensource datasets to further promote the development of UWB through-wall radar.      
### 10.An adaptive random experiment design method for engineering experiment  [ :arrow_down: ](https://arxiv.org/pdf/2008.13581.pdf)
>  This paper proposes an adaptive random experiment design (ARED) algorithm that can be applied to optimize the multiple factors and levels experiments. The algorithm takes real-time model error as the adaptive condition, and outputs a model that conforms to the error quantization standard based on the automatic process. According to the actual experimental scenario, the similar number of test cases were selected between the ARED method and the comparative experimental design method under the bimodal Gaussian function, the bimodal surface function and the peaks function, respectively. simultaneously, the support vector machine (SVM) algorithm is used to construct the model for the selected test cases, and the verification surface (or curve) is predicted. The qualitative and quantitative analysis is carried out at two-slice of applicability and precision. The results show that the ARED method can be applied to the experiment of multi-factor, and has better precision and applicability than the comparative experimental methods.      
### 11.Evaluating Knowledge Transfer In Neural Network for Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.13574.pdf)
>  Deep learning and knowledge transfer techniques have permeated the field of medical imaging and are considered as key approaches for revolutionizing diagnostic imaging practices. However, there are still challenges for the successful integration of deep learning into medical imaging tasks due to a lack of large annotated imaging data. To address this issue, we propose a teacher-student learning framework to transfer knowledge from a carefully pre-trained convolutional neural network (CNN) teacher to a student CNN as a way of improving the diagnostic tasks on a small data regime. In this study, we explore the performance of knowledge transfer in the medical imaging setting through a series of experiments. We investigate the proposed network's performance when the student network is trained on a small dataset (target dataset) as well as when teachers and student's domains are distinct. We also examine the proposed network's behavior on the convergence and regularization of the student network during training. The performances of the CNN models are evaluated on three medical imaging datasets including Diabetic Retinopathy, CheXpert, and ChestX-ray8. Our results indicate that the teacher-student learning framework outperforms transfer learning for small imaging datasets. Particularly, the teacher-student learning framework improves the area under the ROC Curve (AUC) of the CNN model on a small sample of CheXpert (n=5k) by 4% and on ChestX-ray8 (n=5.6k) by 9%. In addition to small training data size, we also demonstrate a clear advantage to favoring teacher-student learning framework for cross-domain knowledge transfer in the medical imaging setting compared to other knowledge transfer techniques such as transfer learning. We observe that the teacher-student network holds a great promise not only to improve the performance of diagnosis but also to reduce overfitting when the dataset is small.      
### 12.Deep Learning based Spectral CT Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2008.13570.pdf)
>  Spectral computed tomography (CT) has attracted much attention in radiation dose reduction, metal artifacts removal, tissue quantification and material discrimination. The x-ray energy spectrum is divided into several bins, each energy-bin-specific projection has a low signal-noise-ratio (SNR) than the current-integrating counterpart, which makes image reconstruction a unique challenge. Traditional wisdom is to use prior knowledge based iterative methods. However, this kind of methods demands a great computational cost. Inspired by deep learning, here we first develop a deep learning based reconstruction method; i.e., U-net with L_p^p-norm, Total variation, Residual learning, and Anisotropic adaption (ULTRA). Specifically, we emphasize the Various Multi-scale Feature Fusion and Multichannel Filtering Enhancement with a denser connection encoding architecture for residual learning and feature fusion. To address the image deblurring problem associated with the $L_2^2$-loss, we propose a general $L_p^p$-loss, $p&gt;0$ Furthermore, the images from different energy bins share similar structures of the same object, the regularization characterizing correlations of different energy bins is incorporated into the $L_p^p$-loss function, which helps unify the deep learning based methods with traditional compressed sensing based methods. Finally, the anisotropically weighted total variation is employed to characterize the sparsity in the spatial-spectral domain to regularize the proposed network. In particular, we validate our ULTRA networks on three large-scale spectral CT datasets, and obtain excellent results relative to the competing algorithms. In conclusion, our quantitative and qualitative results in numerical simulation and preclinical experiments demonstrate that our proposed approach is accurate, efficient and robust for high-quality spectral CT image reconstruction.      
### 13.Mind The Gap: Real-time Decentralized Distance Estimation using Ultrasound and Bluetooth across Multiple Smartphones  [ :arrow_down: ](https://arxiv.org/pdf/2008.13564.pdf)
>  Robust, low-cost solutions are needed to maintain social distancing guidelines during the COVID-19 pandemic. We establish a method to measure the distance between multiple phones across a large number of closely spaced smartphones with a median absolute error of 8.5~cm. The application works in real-time, using Time of Flight of near-ultrasound signals, providing alerts with sufficient responsiveness to be useful for distancing while devices are in users pockets and they are moving at walking speed. The approach is decentralized, requires no additional hardware, and can operate in the background without an internet connection. We have no device-specific requirements nor need any manual calibration or device synchronization. It has been tested with over 20 different phones models, from both the Android and iOS systems in the past 5 years. To the best of our knowledge, this is the first successful such implementation, and has 25000 users at time of publishing.      
### 14.Mode-dependent Loss and Gain Estimation in SDM Transmission Based on MMSE Equalizers  [ :arrow_down: ](https://arxiv.org/pdf/2008.13563.pdf)
>  The capacity in space division multiplexing (SDM) systems with coupled channels is fundamentally limited by mode-dependent loss (MDL) and mode-dependent gain (MDG) generated in components and amplifiers. In these systems, MDL/MDG must be accurately estimated for performance analysis and troubleshooting. Most recent demonstrations of SDM with coupled channels perform MDL/MDG estimation by digital signal processing (DSP) techniques based on the coefficients of multiple-input multiple-output (MIMO) adaptive equalizers. <br>Although these methods provide a valid indication of the order of magnitude of the accumulated MDL/MDG over the link, MIMO equalizers are usually updated according to the minimum mean square error (MMSE) criterion, which is known to depend on the channel signal-to-noise ratio (SNR). Therefore, MDL/MDG estimation techniques based on the adaptive filter coefficients are also impaired by noise. In this paper, we model analytically the influence of the SNR on DSP-based MDL/MDG estimation, and show that the technique is prone to errors. Based on the transfer function of MIMO MMSE equalizers, and assuming a known SNR, we calculate a correction factor that improves the estimation process in moderate levels of MDL/MDG and SNR. The correction factor is validated by simulation of a 6-mode long-haul transmission link, and experimentally using a 3-mode transmission link. The results confirm the limitations of the standard estimation method in scenarios of high additive noise and MDL/MDG, and indicate the correction factor as a possible solution in practical SDM scenarios.      
### 15.Convolutional Neural Network-Bagged Decision Tree: A hybrid approach to reduce electric vehicle's driver's range anxiety by estimating energy consumption in real-time  [ :arrow_down: ](https://arxiv.org/pdf/2008.13559.pdf)
>  To overcome range anxiety problem of Electric Vehicles (EVs), an accurate real-time energy consumption estimation is necessary, which can be used to provide the EV's driver with information about the remaining range in real-time. A hybrid CNN-BDT approach has been developed, in which Convolutional Neural Network (CNN) is used to provide an energy consumption estimate considering the effect of temperature, wind speed, battery's SOC, auxiliary loads, road elevation, vehicle speed and acceleration. Further, Bagged Decision Tree (BDT) is used to fine tune the estimate. Unlike existing techniques, the proposed approach doesn't require internal vehicle parameters from manufacturer and can easily learn complex patterns even from noisy data. Comparison results with existing techniques show that the developed approach provides better estimates with least mean absolute energy deviation of 0.14.      
### 16.Random Walk for modelling Multi Core Fiber cross-talk and step distribution characterisation  [ :arrow_down: ](https://arxiv.org/pdf/2008.13511.pdf)
>  A novel random walk based model for inter-core cross-talk (IC-XT) characterization of multi-core fibres capable of accurately representing both time-domain distribution and frequency-domain representation of experimental IC-XT has been proposed. It was demonstrated that this model is a generalization of the most widely used model in literature to which it will converge when the number of samples and measurement time-window tend to infinity. In addition, this model is consistent with statistical analysis such as short term average crosstalk (STAXT), keeping the same convergence properties and it showed to be almost independent to time-window. To validate this model, a new type of characterization of the IC-XT in the dB domain (based on a pseudo random walk) has been proposed and the statistical properties of its step distribution have been evaluated. The performed analysis showed that this characterization is capable of fitting every type of signal source with an accuracy above 99.3%. It also proved to be very robust to time-window length, temperature and other signal properties such as symbol rate and pseudo-random bit stream (PRBS) length. The obtained results suggest that the model was able to communicate most of the relevant information using a short observation time, making it suitable for IC-XT characterization and core-pair source signal classification. Using machine-learning (ML) techniques for source-signal classification, we empirically demonstrated that this technique carries more information regarding IC-XT than traditional statistical methods.      
### 17.Modeling the Impact of 5G Leakage on Weather Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2008.13498.pdf)
>  The 5G band allocated in the 26 GHz spectrum referred to as 3GPP band n258, has generated a lot of anxiety and concern in the meteorological data forecasting community including the National Oceanic and Atmospheric Administration (NOAA). Unlike traditional spectrum coexistence problems, the issue here stems from the leakage of n258 band transmissions impacting the observations of passive sensors (e.g. AMSU-A) operating at 23.8 GHz on weather satellites used to detect the amount of water vapor in the atmosphere, which in turn affects weather forecasting and predictions. In this paper, we study the impact of 5G leakage on the accuracy of data assimilation based weather prediction algorithms by using a first order propagation model to characterize the effect of the leakage signal on the brightness temperature (atmospheric radiance) and the induced noise temperature at the receiving antenna of the passive sensor (radiometer) on the weather observation satellite. We then characterize the resulting inaccuracies when using the Weather Research and Forecasting Data Assimilation model (WRFDA) to predict temperature and rainfall. For example, the impact of 5G leakage of -20dBW to -15dBW on the well-known Super Tuesday Tornado Outbreak data set, affects the meteorological forecasting up to 0.9 mm in precipitation and 1.3 °C in 2m-temperature. We outline future directions for both improved modeling of 5G leakage effects as well as mitigation using cross-layer antenna techniques coupled with resource allocation.      
### 18.Wireless for Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.13492.pdf)
>  As data generation increasingly takes place on devices without a wired connection, Machine Learning over wireless networks becomes critical. Many studies have shown that traditional wireless protocols are highly inefficient or unsustainable to support Distributed Machine Learning. This is creating the need for new wireless communication methods. In this survey, we give an exhaustive review of the state of the art wireless methods that are specifically designed to support Machine Learning services. Namely, over-the-air computation and radio resource allocation optimized for Machine Learning. In the over-the-air approach, multiple devices communicate simultaneously over the same time slot and frequency band to exploit the superposition property of wireless channels for gradient averaging over-the-air. In radio resource allocation optimized for Machine Learning, Active Learning metrics allow for data evaluation to greatly optimize the assignment of radio resources. This paper gives a comprehensive introduction to these methods, reviews the most important works, and highlights crucial open problems.      
### 19.Privacy-Preserving Distributed Zeroth-Order Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.13468.pdf)
>  We develop a privacy-preserving distributed algorithm to minimize a regularized empirical risk function when the first-order information is not available and data is distributed over a multi-agent network. We employ a zeroth-order method to minimize the associated augmented Lagrangian function in the primal domain using the alternating direction method of multipliers (ADMM). We show that the proposed algorithm, named distributed zeroth-order ADMM (D-ZOA), has intrinsic privacy-preserving properties. Unlike the existing privacy-preserving methods based on the ADMM where the primal or the dual variables are perturbed with noise, the inherent randomness due to the use of a zeroth-order method endows D-ZOA with intrinsic differential privacy. By analyzing the perturbation of the primal variable, we show that the privacy leakage of the proposed D-ZOA algorithm is bounded. In addition, we employ the moments accountant method to show that the total privacy leakage grows sublinearly with the number of ADMM iterations. D-ZOA outperforms the existing differentially private approaches in terms of accuracy while yielding the same privacy guarantee. We prove that D-ZOA converges to the optimal solution at a rate of $\mathcal{O}(1/M)$ where $M$ is the number of ADMM iterations. The convergence analysis also reveals a practically important trade-off between privacy and accuracy. Simulation results verify the desirable privacy-preserving properties of D-ZOA and its superiority over a state-of-the-art algorithm as well as its network-wide convergence to the optimal solution.      
### 20.Securing the Inter-Spacecraft Links: Doppler Frequency Shift based Physical Layer Key Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.13396.pdf)
>  We propose a novel physical layer secret key generation method for the inter-spacecraft communication links. By exploiting the Doppler frequency shifts of the reciprocal spacecraft links as a unique secrecy source, spacecrafts aim to obtain identical secret keys from their individual observations. We obtain theoretical expressions for the key disagreement rate (KDR). Using generalized Gauss-Laguerre quadrature, we derive closed form expressions for the KDR. Through numerical studies, the tightness of the provided approximations are shown. Both the theoretical and numerical results demonstrate the validity and the practicality of the presented physical layer key generation procedure considering the security of the communication links of spacecrafts.      
### 21.Iterative Symbol Recovery For Power Efficient DC Biased Optical OFDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.13391.pdf)
>  Orthogonal frequency division multiplexing (OFDM) has proven itself as an effective multi-carrier digital communication technique. In recent years the interest in optical OFDM has grown significantly, due to its spectral efficiency and inherent resilience to frequency-selective channels and to narrowband interference. For these reasons it is currently considered to be one of the leading candidates for deployment in short fiber links such as the ones intended for inter-data-center communications. In this paper we present a new power-efficient symbol recovery scheme for dc-biased optical OFDM (DCO-OFDM) in an intensity-modulation direct-detection (IM/DD) system. We introduce an alternative method for clipping in order to maintain a non-negative real-valued signal and still preserve information, which is lost when using clipping, and propose an iterative detection algorithm for this method. A reduction of $50\%$ in the transmitted optical power along with an increase of signal-independent noise immunity (gaining $3$[dB] in SNR), compared to traditional DCO-OFDM with a DC bias of $2$ standard deviations of the OFDM signal, is attained by our new scheme for a symbol error rate (SER) of $10^{-3}$ in a QPSK constellation additive white gaussian noise (AWGN) flat channel model.      
### 22.Impact of Battery Degradation on Market Participation of Utility-Scale Batteries: Case Studies  [ :arrow_down: ](https://arxiv.org/pdf/2008.13382.pdf)
>  The recent decrease in battery manufacturing costs stimulates the market participation of utility-scale battery energy storage systems (BESSs). However, battery degradation remains a major concern for BESS owners while determining their BESS investment and operation strategies. This paper studies the impact of battery degradation on BESS's operation and revenue/cost in real-time energy, reserve, and pay as performance regulation markets. Comparative case studies are performed on two optimization frameworks which model the participation of a price-maker BESS in energy and ancillary services markets with and without considering battery degradation cost. A synthetic test system built upon real-world data is adopted in the case studies. Analyses reveal that degradation cost plays an important role in the scheduling of BESSs and should not be neglected. Several potential enhancements to the optimization frameworks are discussed based on the performed analyses.      
### 23.Dynamics of Laser-Charged UAVs: A Battery Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2008.13316.pdf)
>  In this paper, we investigate the dynamics of a laser-charged unmanned aerial vehicle (UAV) aiming to achieve a mission under energy constraints. We study the UAV's battery dynamics by leveraging the electrical models for motors and battery. Subsequently, using these models, the path planning problem in a particular Internet-of-Things based use-case is revisited from the battery perspective. By leveraging a graph theory approach, the problem is solved optimally, and compared to benchmark trajectory approaches. Through numerical results, we show the efficiency of this novel perspective for all path planning approaches. In contrast to the battery perspective, we found that the energy perspective is very conservative and does not exploit optimally the available energy resources. However, it can be adjusted by carefully evaluating the energy as a function of the UAV motion regime. Finally, the impact of some parameters, such as turbulence and distance to charging source, is studied.      
### 24.Joint Transmission in QoE-Driven Backhaul-Aware MC-NOMA Cognitive Radio Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.13269.pdf)
>  In this paper, we develop a resource allocation framework to optimize the downlink transmission of a backhaul-aware multi-cell cognitive radio network (CRN) which is enabled with multi-carrier non-orthogonal multiple access (MC-NOMA). The considered CRN is composed of a single macro base station (MBS) and multiple small BSs (SBSs) that are referred to as the primary and secondary tiers, respectively. For the primary tier, we consider orthogonal frequency division multiple access (OFDMA) scheme and also Quality of Service (QoS) to evaluate the user satisfaction. On the other hand in secondary tier, MC-NOMA is employed and the user satisfaction for web, video and audio as popular multimedia services is evaluated by Quality-of-Experience (QoE). Furthermore, each user in secondary tier can be served simultaneously by multiple SBSs over a subcarrier via Joint Transmission (JT). In particular, we formulate a joint optimization problem of power control and scheduling (i.e., user association and subcarrier allocation) in secondary tier to maximize total achievable QoE for the secondary users. An efficient resource allocation mechanism has been developed to handle the non-linear form interference and to overcome the non-convexity of QoE serving functions. The scheduling and power control policy leverage on Augmented Lagrangian Method (ALM). Simulation results reveal that proposed solution approach can control the interference and JT-NOMA improves total perceived QoE compared to the existing schemes.      
### 25.Improved Lite Audio-Visual Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2008.13222.pdf)
>  Numerous studies have investigated the effectiveness of audio-visual multimodal learning for speech enhancement (AVSE) tasks, seeking a solution that uses visual data as auxiliary and complementary input to reduce the noise of noisy speech signals. Recently, we proposed a lite audio-visual speech enhancement (LAVSE) algorithm. Compared to conventional AVSE systems, LAVSE requires less online computation and moderately solves the user privacy problem on facial data. In this study, we extend LAVSE to improve its ability to address three practical issues often encountered in implementing AVSE systems, namely, the requirement for additional visual data, audio-visual asynchronization, and low-quality visual data. The proposed system is termed improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin speech with video dataset. Experimental results confirm that compared to conventional AVSE systems, iLAVSE can effectively overcome the aforementioned three practical issues and can improve enhancement performance. The results also confirm that iLAVSE is suitable for real-world scenarios, where high-quality audio-visual sensors may not always be available.      
### 26.Mixture of Speaker-type PLDAs for Children's Speech Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2008.13213.pdf)
>  In diarization, the PLDA is typically used to model an inference structure which assumes the variation in speech segments be induced by various speakers. The speaker variation is then learned from the training data. However, human perception can differentiate speakers by age, gender, among other characteristics. In this paper, we investigate a speaker-type informed model that explicitly captures the known variation of speakers. We explore a mixture of three PLDA models, where each model represents an adult female, male, or child category. The weighting of each model is decided by the prior probability of its respective class, which we study. The evaluation is performed on a subset of the BabyTrain corpus. We examine the expected performance gain using the oracle speaker type labels, which yields an 11.7% DER reduction. We introduce a novel baby vocalization augmentation technique and then compare the mixture model to the single model. Our experimental result shows an effective 0.9% DER reduction obtained by adding vocalizations. We discover empirically that a balanced dataset is important to train the mixture PLDA model, which outperforms the single PLDA by 1.3% using the same training data and achieving a 35.8% DER. The same setup improves over a standard baseline by 2.8% DER.      
### 27.Reinforcement Learning Based Penetration Testing of a Microgrid Control Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2008.13212.pdf)
>  Microgrids (MGs) are small-scale power systems which interconnect distributed energy resources and loads within clearly defined regions. However, the digital infrastructure used in an MG to relay sensory information and perform control commands can potentially be compromised due to a cyberattack from a capable adversary. An MG operator is interested in knowing the inherent vulnerabilities in their system and should regularly perform Penetration Testing (PT) activities to prepare for such an event. PT generally involves looking for defensive coverage blindspots in software and hardware infrastructure, however the logic in control algorithms which act upon sensory information should also be considered in PT activities. This paper demonstrates a case study of PT for an MG control algorithm by using Reinforcement Learning (RL) to uncover malicious input which compromises the effectiveness of the controller. Through trial-and-error episodic interactions with a simulated MG, we train an RL agent to find malicious input which reduces the effectiveness of the MG controller.      
### 28.The Extended "Sequentially Drilled" Joint Congruence Transformation and its Application in Gaussian Independent Vector Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.13199.pdf)
>  Independent Vector Analysis (IVA) has emerged in recent years as an extension of Independent Component Analysis (ICA) into multiple sets of mixtures, where the source signals in each set are independent, but may depend on source signals in the other sets. In a semi-blind IVA (or ICA) framework, information regarding the probability distributions of the sources may be available, giving rise to Maximum Likelihood (ML) separation. In recent work we have shown that under the multivariate Gaussian model, with arbitrary temporal covariance matrices (stationary or non-stationary) of the source signals, ML separation requires the solution of a "Sequentially Drilled" Joint Congruence (SeDJoCo) transformation of a set of matrices, which is reminiscent of (but different from) classical joint diagonalization. In this paper we extend our results to the IVA problem, showing how the ML solution for the Gaussian model (with arbitrary covariance and cross-covariance matrices) takes the form of an extended SeDJoCo problem. We formulate the extended problem, derive a condition for the existence of a solution, and propose two iterative solution algorithms. In addition, we derive the induced Cramér-Rao Lower Bound (iCRLB) on the resulting Interference-to-Source Ratios (ISR) matrices, and demonstrate by simulation how the ML separation obtained by solving the extended SeDJoCo problem indeed attains the iCRLB (asymptotically), as opposed to other separation approaches, which cannot exploit prior knowledge regarding the sources' distributions.      
### 29.Performance Analysis of the Gaussian Quasi-Maximum Likelihood Approach for Independent Vector Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.13189.pdf)
>  Maximum Likelihood (ML) estimation requires precise knowledge of the underlying statistical model. In Quasi ML (QML), a presumed model is used as a substitute to the (unknown) true model. In the context of Independent Vector Analysis (IVA), we consider the Gaussian QML Estimate (QMLE) of the demixing matrices set and present an (approximate) analysis of its asymptotic separation performance. In Gaussian QML the sources are presumed to be Gaussian, with covariance matrices specified by some "educated guess". The resulting quasi-likelihood equations of the demixing matrices take a special form, recently termed an extended "Sequentially Drilled" Joint Congruence (SeDJoCo) transformation, which is reminiscent of (though essentially different from) classical joint diagonalization. We show that asymptotically this QMLE, i.e., the solution of the resulting extended SeDJoCo transformation, attains perfect separation (under some mild conditions) regardless of the sources' true distributions and/or covariance matrices. In addition, based on the "small-errors" assumption, we present a first-order perturbation analysis of the extended SeDJoCo solution. Using the resulting closed-form expressions for the errors in the solution matrices, we provide closed-form expressions for the resulting Interference-to-Source Ratios (ISRs) for IVA. Moreover, we prove that asymptotically the ISRs depend only on the sources' covariances, and not on their specific distributions. As an immediate consequence of this result, we provide an asymptotically attainable lower bound on the resulting ISRs. We also present empirical results, corroborating our analytical derivations, of three simulation experiments concerning two possible model errors - inaccurate covariance matrices and sources' distribution mismodeling.      
### 30.Speech Pseudonymisation Assessment Using Voice Similarity Matrices  [ :arrow_down: ](https://arxiv.org/pdf/2008.13144.pdf)
>  The proliferation of speech technologies and rising privacy legislation calls for the development of privacy preservation solutions for speech applications. These are essential since speech signals convey a wealth of rich, personal and potentially sensitive information. Anonymisation, the focus of the recent VoicePrivacy initiative, is one strategy to protect speaker identity information. Pseudonymisation solutions aim not only to mask the speaker identity and preserve the linguistic content, quality and naturalness, as is the goal of anonymisation, but also to preserve voice distinctiveness. Existing metrics for the assessment of anonymisation are ill-suited and those for the assessment of pseudonymisation are completely lacking. Based upon voice similarity matrices, this paper proposes the first intuitive visualisation of pseudonymisation performance for speech signals and two novel metrics for objective assessment. They reflect the two, key pseudonymisation requirements of de-identification and voice distinctiveness.      
### 31.Enhanced Time-Frequency Representation and Mode Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2008.13136.pdf)
>  Time-frequency representation (TFR) allowing for mode reconstruction plays a significant role in interpreting and analyzing the nonstationary signal constituted of various modes. However, it is difficult for most previous methods to handle signal modes with closely-spaced or spectrally-overlapped instantaneous frequencies (IFs) especially in adverse environments. To address this issue, we propose an enhanced TFR and mode decomposition (ETFR-MD) method, which is particularly adapted to represent and decompose multi-mode signals with close or crossing IFs under low signal-to-noise ratio (SNR) conditions. The emphasis of the proposed ETFR-MD is placed on accurate IF and instantaneous amplitude (IA) extraction of each signal mode based on short-time Fourier transform (STFT). First, we design an initial IF estimation method specifically for the cases involving crossing IFs. Further, a low-complexity mode enhancement scheme is proposed so that enhanced IFs better fit underlying IF laws. Finally, the IA extraction from signal's STFT coefficients combined with the enhanced IFs enables us to reconstruct each signal mode. In addition, we derive mathematical expressions that reveal optimal window lengths and separation interference of our method. The proposed ETFR-MD is compatible with previous related methods, thus can be regarded as a step toward a more general time-frequency representation and decomposition method. Experimental results confirm the superior performance of the ETFR-MD when compared to a state-of-the-art benchmark.      
### 32.Brain Stroke Lesion Segmentation Using Consistent Perception Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2008.13109.pdf)
>  Recently, the state-of-the-art deep learning methods have demonstrated impressive performance in segmentation tasks. However, the success of these methods depends on a large amount of manually labeled masks, which are expensive and time-consuming to be collected. To reduce the dependence on full labeled samples, we propose a novel Consistent Perception Generative Adversarial Network (CPGAN) for semi-supervised stroke lesion segmentation. Specifically, we design a non-local operation named similarity connection module (SCM) to capture the information of multi-scale features. This module can selectively aggregate the features at each position by a weighted sum. Furthermore, an assistant network is constructed to encourage the discriminator to learn meaningful feature representations which are forgotten during training. The assistant network and the discriminator are used to jointly decide whether the segmentation results are real or fake. With the semi-supervised stroke lesion segmentation, we adopt a consistent perception strategy to enhance the effect of brain stroke lesion prediction for the unlabeled data. The CPGAN was evaluated on the Anatomical Tracings of Lesions After Stroke (ATLAS). The experimental results demonstrate that the proposed network achieves superior segmentation performance. In semi-supervised segmentation task, our method using only two-fifths of labeled samples outperforms some approaches using full labeled samples.      
### 33.Blind Determination of the Number of Sources Using Distance Correlation  [ :arrow_down: ](https://arxiv.org/pdf/2008.13096.pdf)
>  A novel blind estimate of the number of sources from noisy, linear mixtures is proposed. Based on Székely et al.'s distance correlation measure, we define the Sources' Dependency Criterion (SDC), from which our estimate arises. Unlike most previously proposed estimates, the SDC estimate exploits the full independence of the sources and noise, as well as the non-Gaussianity of the sources (as opposed to the Gaussianity of the noise), via implicit use of high-order statistics. This leads to a more robust, resilient and stable estimate w.r.t. the mixing matrix and the noise covariance structure. Empirical simulation results demonstrate these virtues, on top of superior performance in comparison with current state of the art estimates.      
### 34.Hierarchical Timbre-Painting and Articulation Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.13095.pdf)
>  We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at <a class="link-external link-https" href="https://github.com/mosheman5/timbre_painting" rel="external noopener nofollow">this https URL</a>.      
### 35.Parallel Rescoring with Transformer for Streaming On-Device Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.13093.pdf)
>  Recent advances of end-to-end models have outperformed conventional models through employing a two-pass model. The two-pass model provides better speed-quality trade-offs for on-device speech recognition, where a 1st-pass model generates hypotheses in a streaming fashion, and a 2nd-pass model re-scores the hypotheses with full audio sequence context. The 2nd-pass model plays a key role in the quality improvement of the end-to-end model to surpass the conventional model. One main challenge of the two-pass model is the computation latency introduced by the 2nd-pass model. Specifically, the original design of the two-pass model uses LSTMs for the 2nd-pass model, which are subject to long latency as they are constrained by the recurrent nature and have to run inference sequentially. In this work we explore replacing the LSTM layers in the 2nd-pass rescorer with Transformer layers, which can process the entire hypothesis sequences in parallel and can therefore utilize the on-device computation resources more efficiently. Compared with an LSTM-based baseline, our proposed Transformer rescorer achieves more than 50% latency reduction with quality improvement.      
### 36.Asymptotically Optimal Blind Calibration of Uniform Linear Sensor Arrays for Narrowband Gaussian Signals  [ :arrow_down: ](https://arxiv.org/pdf/2008.13091.pdf)
>  An asymptotically optimal blind calibration scheme of uniform linear arrays for narrowband Gaussian signals is proposed. Rather than taking the direct Maximum Likelihood (ML) approach for joint estimation of all the unknown model parameters, which leads to a multi-dimensional optimization problem with no closed-form solution, we revisit Paulraj and Kailath's (P-K's) classical approach in exploiting the special (Toeplitz) structure of the observations' covariance. However, we offer a substantial improvement over P-K's ordinary Least Squares (LS) estimates by using asymptotic approximations in order to obtain simple, non-iterative, (quasi-)linear Optimally-Weighted LS (OWLS) estimates of the sensors gains and phases offsets with asymptotically optimal weighting, based only on the empirical covariance matrix of the measurements. Moreover, we prove that our resulting estimates are also asymptotically optimal w.r.t. the raw data, and can therefore be deemed equivalent to the ML Estimates (MLE), which are otherwise obtained by joint ML estimation of all the unknown model parameters. After deriving computationally convenient expressions of the respective Cramér-Rao lower bounds, we also show that our estimates offer improved performance when applied to non-Gaussian signals (and/or noise) as quasi-MLE in a similar setting. The optimal performance of our estimates is demonstrated in simulation experiments, with a considerable improvement (reaching an order of magnitude and more) in the resulting mean squared errors w.r.t. P-K's ordinary LS estimates. We also demonstrate the improved accuracy in a multiple-sources directions-of-arrivals estimation task.      
### 37.MDCN: Multi-scale Dense Cross Network for Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2008.13084.pdf)
>  Convolutional neural networks have been proven to be of great benefit for single-image super-resolution (SISR). However, previous works do not make full use of multi-scale features and ignore the inter-scale correlation between different upsampling factors, resulting in sub-optimal performance. Instead of blindly increasing the depth of the network, we are committed to mining image features and learning the inter-scale correlation between different upsampling factors. To achieve this, we propose a Multi-scale Dense Cross Network (MDCN), which achieves great performance with fewer parameters and less execution time. MDCN consists of multi-scale dense cross blocks (MDCBs), hierarchical feature distillation block (HFDB), and dynamic reconstruction block (DRB). Among them, MDCB aims to detect multi-scale features and maximize the use of image features flow at different scales, HFDB focuses on adaptively recalibrate channel-wise feature responses to achieve feature distillation, and DRB attempts to reconstruct SR images with different upsampling factors in a single model. It is worth noting that all these modules can run independently. It means that these modules can be selectively plugged into any CNN model to improve model performance. Extensive experiments show that MDCN achieves competitive results in SISR, especially in the reconstruction task with multiple upsampling factors. The code will be provided at <a class="link-external link-https" href="https://github.com/MIVRC/MDCN-PyTorch" rel="external noopener nofollow">this https URL</a>.      
### 38.Centralized Coordination of Connected Vehicles at Intersections using Graphical Mixed Integer Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.13081.pdf)
>  This paper proposes a centralized multi-vehicle coordination scheme serving unsignalized intersections. The whole process consists of three stages: a) target velocity optimization: formulate the collision-free vehicle coordination as a Mixed Integer Linear Programming (MILP) problem, with each incoming lane representing an independent variable; b) dynamic vehicle selection: build a directed graph with result of the optimization, and reserve only some of the vehicle nodes to coordinate by applying a subset extraction algorithm; c) synchronous velocity profile planning: bridge the gap between current speed and optimal velocity in a synchronous manner. The problem size is essentially bounded by number of lanes instead of vehicles. Thus the optimization process is realtime with guaranteed solution quality. Simulation has verified efficiency and real-time performance of the scheme.      
### 39.Unsupervised MRI Reconstruction with Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.13065.pdf)
>  Deep learning-based image reconstruction methods have achieved promising results across multiple MRI applications. However, most approaches require large-scale fully-sampled ground truth data for supervised training. Acquiring fully-sampled data is often either difficult or impossible, particularly for dynamic contrast enhancement (DCE), 3D cardiac cine, and 4D flow. We present a deep learning framework for MRI reconstruction without any fully-sampled data using generative adversarial networks. We test the proposed method in two scenarios: retrospectively undersampled fast spin echo knee exams and prospectively undersampled abdominal DCE. The method recovers more anatomical structure compared to conventional methods.      
### 40.Secure Recovery Procedure for Manufacturing Systems using Synchronizing Automata and Supervisory Control Theory  [ :arrow_down: ](https://arxiv.org/pdf/2008.13062.pdf)
>  Manufacturing systems may be subject to external attacks and failures, so it is important to deal with the recovery of the system after these situations. This paper deals with the problem of recovering a manufacturing system, modeled as a Discrete Event System (DES) using the Supervisory Control Theory (SCT), when the control structure, called supervisor, desynchronizes from the physical plant. The desynchronization may be seen as plant and supervisor being in uncorresponding states. The recovery of the system may be attained if there is a word, the synchronizing word, that regardless the state of each one of them, brings the system and supervisor back to a known state. The concepts of synchronizing automata are used to do so. In this paper we show under what conditions a set of synchronizing plants and specifications leads to a synchronizing supervisor obtained by the Supervisory Control Theory. The problem is extended to cope with multiple supervisors, proposing a local recovery when possible. We also present a simple way to model problems, composed of machines and buffers, as synchronizing automata such that it is always possible do restore synchronization between the control (supervisor) and the plant.      
### 41.Initialization Process of a Power System Transient Simulation Scheme for Stability Studies  [ :arrow_down: ](https://arxiv.org/pdf/2008.13059.pdf)
>  The initialization process of a novel power system transient simulation scheme for stability studies is put forward, by further developing a "time-domain harmonic power-flow algorithm". The initialization process is formulated as an algebraic problem to ensure that the power system under study is in steady state and operated at a specified operating point, at the beginning of a transient simulation run. The algebraic problem is then solved efficiently by a preconditioned finite difference Newton-GMRES method. Case studies verify the validity and efficiency of the initialization process. The proposed initialization process is general-purpose and can be applied to other power system transient simulation schemes.      
### 42.Longitudinal Image Registration with Temporal-order and Subject-specificity Discrimination  [ :arrow_down: ](https://arxiv.org/pdf/2008.13002.pdf)
>  Morphological analysis of longitudinal MR images plays a key role in monitoring disease progression for prostate cancer patients, who are placed under an active surveillance program. In this paper, we describe a learning-based image registration algorithm to quantify changes on regions of interest between a pair of images from the same patient, acquired at two different time points. Combining intensity-based similarity and gland segmentation as weak supervision, the population-data-trained registration networks significantly lowered the target registration errors (TREs) on holdout patient data, compared with those before registration and those from an iterative registration algorithm. Furthermore, this work provides a quantitative analysis on several longitudinal-data-sampling strategies and, in turn, we propose a novel regularisation method based on maximum mean discrepancy, between differently-sampled training image pairs. Based on 216 3D MR images from 86 patients, we report a mean TRE of 5.6 mm and show statistically significant differences between the different training data sampling strategies.      
### 43.On Microstructure Estimation Using Flatbed Scanners for Paper Surface Based Authentication  [ :arrow_down: ](https://arxiv.org/pdf/2008.13000.pdf)
>  Paper surfaces under the microscopic view are observed to be formed by intertwisted wood fibers. Such structures of paper surfaces are unique from one location to another and are almost impossible to duplicate. Previous work used microscopic surface normals to characterize such intrinsic structures as a "fingerprint" of paper for security and forensic applications. In this work, we examine several key research questions of feature extraction in both scientific and engineering aspects to facilitate the deployment of paper surface-based authentication when flatbed scanners are used as the acquisition device. We analytically show that, under the unique optical setup of flatbed scanners, the specular reflection does not play a role in norm map estimation. We verify using a larger dataset than prior work that the scanner-acquired norm maps, although blurred, are consistent with those measured by confocal microscopes. We confirm that when choosing an authentication feature, high spatial-frequency subbands of the heightmap are more powerful than the norm map. Finally, we show that it is possible to empirically calculate the physical dimension of the paper patch needed to achieve a certain authentication performance in equal error rate (EER). We analytically show that log(EER) is decreasing linearly in the edge length of a paper patch.      
### 44.Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise  [ :arrow_down: ](https://arxiv.org/pdf/2008.12977.pdf)
>  In industrial vision, the anomaly detection problem can be addressed with an autoencoder trained to map an arbitrary image, i.e. with or without any defect, to a clean image, i.e. without any defect. In this approach, anomaly detection relies conventionally on the reconstruction residual or, alternatively, on the reconstruction uncertainty. To improve the sharpness of the reconstruction, we consider an autoencoder architecture with skip connections. In the common scenario where only clean images are available for training, we propose to corrupt them with a synthetic noise model to prevent the convergence of the network towards the identity mapping, and introduce an original Stain noise model for that purpose. We show that this model favors the reconstruction of clean images from arbitrary real-world images, regardless of the actual defects appearance. In addition to demonstrating the relevance of our approach, our validation provides the first consistent assessment of reconstruction-based methods, by comparing their performance over the MVTec AD dataset, both for pixel- and image-wise anomaly detection.      
### 45.Unpaired Deep Learning for Accelerated MRI using Optimal Transport Driven CycleGAN  [ :arrow_down: ](https://arxiv.org/pdf/2008.12967.pdf)
>  Recently, deep learning approaches for accelerated MRI have been extensively studied thanks to their high performance reconstruction in spite of significantly reduced runtime complexity. These neural networks are usually trained in a supervised manner, so matched pairs of subsampled and fully sampled k-space data are required. Unfortunately, it is often difficult to acquire matched fully sampled k-space data, since the acquisition of fully sampled k-space data requires long scan time and often leads to the change of the acquisition protocol. Therefore, unpaired deep learning without matched label data has become a very important research topic. In this paper, we propose an unpaired deep learning approach using a optimal transport driven cycle-consistent generative adversarial network (OT-cycleGAN) that employs a single pair of generator and discriminator. The proposed OT-cycleGAN architecture is rigorously derived from a dual formulation of the optimal transport formulation using a specially designed penalized least squares cost. The experimental results show that our method can reconstruct high resolution MR images from accelerated k- space data from both single and multiple coil acquisition, without requiring matched reference data.      
### 46.Optimization-driven Machine Learning for Intelligent Reflecting Surfaces Assisted Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.12938.pdf)
>  Intelligent reflecting surface (IRS) has been recently employed to reshape the wireless channels by controlling individual scattering elements' phase shifts, namely, passive beamforming. Due to the large size of scattering elements, the passive beamforming is typically challenged by the high computational complexity and inexact channel information. In this article, we focus on machine learning (ML) approaches for performance maximization in IRS-assisted wireless networks. In general, ML approaches provide enhanced flexibility and robustness against uncertain information and imprecise modeling. Practical challenges still remain mainly due to the demand for a large dataset in offline training and slow convergence in online learning. These observations motivate us to design a novel optimization-driven ML framework for IRS-assisted wireless networks, which takes both advantages of the efficiency in model-based optimization and the robustness in model-free ML approaches. By splitting the decision variables into two parts, one part is obtained by the outer-loop ML approach, while the other part is optimized efficiently by solving an approximate problem. Numerical results verify that the optimization-driven ML approach can improve both the convergence and the reward performance compared to conventional model-free learning approaches.      
### 47.Data augmentation using prosody and false starts to recognize non-native children's speech  [ :arrow_down: ](https://arxiv.org/pdf/2008.12914.pdf)
>  This paper describes AaltoASR's speech recognition system for the INTERSPEECH 2020 shared task on Automatic Speech Recognition (ASR) for non-native children's speech. The task is to recognize non-native speech from children of various age groups given a limited amount of speech. Moreover, the speech being spontaneous has false starts transcribed as partial words, which in the test transcriptions leads to unseen partial words. To cope with these two challenges, we investigate a data augmentation-based approach. Firstly, we apply the prosody-based data augmentation to supplement the audio data. Secondly, we simulate false starts by introducing partial-word noise in the language modeling corpora creating new words. Acoustic models trained on prosody-based augmented data outperform the models using the baseline recipe or the SpecAugment-based augmentation. The partial-word noise also helps to improve the baseline language model. Our ASR system, a combination of these schemes, is placed third in the evaluation period and achieves the word error rate of 18.71%. Post-evaluation period, we observe that increasing the amounts of prosody-based augmented data leads to better performance. Furthermore, removing low-confidence-score words from hypotheses can lead to further gains. These two improvements lower the ASR error rate to 17.99%.      
### 48.Ultra Lightweight Image Super-Resolution with Multi-Attention Layers  [ :arrow_down: ](https://arxiv.org/pdf/2008.12912.pdf)
>  Lightweight image super-resolution (SR) networks have the utmost significance for real-world applications. There are several deep learning based SR methods with remarkable performance, but their memory and computational cost are hindrances in practical usage. To tackle this problem, we propose a Multi-Attentive Feature Fusion Super-Resolution Network (MAFFSRN). MAFFSRN consists of proposed feature fusion groups (FFGs) that serve as a feature extraction block. Each FFG contains a stack of proposed multi-attention blocks (MAB) that are combined in a novel feature fusion structure. Further, the MAB with a cost-efficient attention mechanism (CEA) helps us to refine and extract the features using multiple attention mechanisms. The comprehensive experiments show the superiority of our model over the existing state-of-the-art. We participated in AIM 2020 efficient SR challenge with our MAFFSRN model and won 1st, 3rd, and 4th places in memory usage, floating-point operations (FLOPs) and number of parameters, respectively.      
### 49.Source-Aware Neural Speech Coding for Noisy Speech Compression  [ :arrow_down: ](https://arxiv.org/pdf/2008.12889.pdf)
>  This paper introduces a novel neural network-based speech coding system that can handle noisy speech effectively. The pro-posed source-aware neural audio coding (SANAC) system harmonizes a deep autoencoder-based source separation model and a neural coding system, so that it can explicitly perform source separation and coding in the latent space. An added benefit of this system is that the codec can allocate different amount of bits to the underlying sources, so that the more important source sounds better in the decoded signal. We target the use case where the user on the receiver side cares the quality of the non-speech components in the speech communication, while the speech source still carries the most important information. Both objective and subjective evaluation tests show that SANAC can recover the original noisy speech in a better quality than the baseline neural audio coding system, which is with no source-aware coding mechanism      
### 50.Development and Assessment of a Nearly Autonomous Management and Control System for Advanced Reactors  [ :arrow_down: ](https://arxiv.org/pdf/2008.12888.pdf)
>  This paper develops a Nearly Autonomous Management and Control (NAMAC) system for advanced reactors. The development process of NAMAC is characterized by a three layer-layer architecture: knowledge base, the Digital Twin (DT) developmental layer, and the NAMAC operational layer. The DT is described as a knowledge acquisition system from the knowledge base for intended uses in the NAMAC system. A set of DTs with different functions is developed with acceptable performance and assembled according to the NAMAC operational workflow to furnish recommendations to operators. To demonstrate the capability of the NAMAC system, a case study is designed, where a baseline NAMAC is implemented for operating a simulator of the Experimental Breeder Reactor II during a single loss of flow accident. When NAMAC is operated in the training domain, it can provide reasonable recommendations that prevent the peak fuel centerline temperature from exceeding a safety criterion.      
### 51.On analytical construction of observable functions in extended dynamic mode decomposition for nonlinear estimation and prediction  [ :arrow_down: ](https://arxiv.org/pdf/2008.12874.pdf)
>  We propose an analytical construction of observable functions in the extended dynamic mode decomposition (EDMD) algorithm. EDMD is a numerical method for approximating the spectral properties of the Koopman operator. The choice of observable functions is fundamental for the application of EDMD to nonlinear problems arising in systems and control. Existing methods either start from a set of dictionary functions and look for the subset that best fits, in a certain sense, the underlying nonlinear dynamics; or they rely on machine learning algorithms, e.g., neural networks, to "learn" observable functions that are not explicitly available. Conversely, we start from the dynamical system model and lift it through the Lie derivatives, rendering it into a polynomial form. This transformation into a polynomial form is exact, although not unique, and it provides an adequate set of observable functions. The strength of the proposed approach is its applicability to a broader class of nonlinear dynamical systems, particularly those with nonpolynomial functions and compositions thereof. Moreover, it retains the physical interpretability of the underlying dynamical system and can be readily integrated into existing numerical libraries. The proposed approach is illustrated with an application to electric power systems. The modeled system consists of a single generator connected to an infinite bus, in which case nonlinear terms include sine and cosine functions. The results demonstrate the effectiveness of the proposed procedure in off-attractor nonlinear dynamics for estimation and prediction; the observable functions obtained from the proposed construction outperformed existing methods that use dictionary functions comprising monomials or radial basis functions.      
### 52.Multi-Model Resilient Observer under False Data Injection Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2008.12859.pdf)
>  In this paper, we present the concept of boosting the resiliency of optimization-based observers for cyber-physical systems (CPS) using auxiliary sources of information. Due to the tight coupling of physics, communication and computation, a malicious agent can exploit multiple inherent vulnerabilities in order to inject stealthy signals into the measurement process. The problem setting considers the scenario in which an attacker strategically corrupts portions of the data in order to force wrong state estimates which could have catastrophic consequences. The goal of the proposed observer is to compute the true states in-spite of the adversarial corruption. In the formulation, we use a measurement prior distribution generated by the auxiliary model to refine the feasible region of a traditional compressive sensing-based regression problem. A constrained optimization-based observer is developed using l1-minimization scheme. Numerical experiments show that the solution of the resulting problem recovers the true states of the system. The developed algorithm is evaluated through a numerical simulation example of the IEEE 14-bus system.      
### 53.High Accuracy Traffic Light Controller for Increasing the Given Green Time Utilization  [ :arrow_down: ](https://arxiv.org/pdf/2008.13738.pdf)
>  Traffic congestion has become one of the major problems in the urban cities according to the increasing number of vehicles in those cities, obsolete technologies used on the roads of those cities, inappropriate road design, and many other reasons. So, that has urged the need for a more accurate traffic light controlling system; one that will help in maintaining high stability at all levels of demand. This paper introduces a dynamic traffic light phase plan protocol for Single-Isolated Intersections. The developed controlling method was compared with four other methods and showed a good performance in terms of reducing the average and maximum queue lengths, optimizing the given green time amount as needed, and increased the intersections throughput (increased the given green time utilization). In addition, it maintained a good traffic light stability at all levels of demand.      
### 54.Extracting full-field subpixel structural displacements from videos via deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.13715.pdf)
>  This paper develops a deep learning framework based on convolutional neural networks (CNNs) that enable real-time extraction of full-field subpixel structural displacements from videos. In particular, two new CNN architectures are designed and trained on a dataset generated by the phase-based motion extraction method from a single lab-recorded high-speed video of a dynamic structure. As displacement is only reliable in the regions with sufficient texture contrast, the sparsity of motion field induced by the texture mask is considered via the network architecture design and loss function definition. Results show that, with the supervision of full and sparse motion field, the trained network is capable of identifying the pixels with sufficient texture contrast as well as their subpixel motions. The performance of the trained networks is tested on various videos of other structures to extract the full-field motion (e.g., displacement time histories), which indicates that the trained networks have generalizability to accurately extract full-field subtle displacements for pixels with sufficient texture contrast.      
### 55.Control of a Nature-inspired Scorpion using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.13712.pdf)
>  A terrestrial robot that can maneuver rough terrain and scout places is very useful in mapping out unknown areas. It can also be used explore dangerous areas in place of humans. A terrestrial robot modeled after a scorpion will be able to traverse undetected and can be used for surveillance purposes. Therefore, this paper proposes modelling of a scorpion inspired robot and a reinforcement learning (RL) based controller for navigation. The robot scorpion uses serial four bar mechanisms for the legs movements. It also has an active tail and a movable claw. The controller is trained to navigate the robot scorpion to the target waypoint. The simulation results demonstrate efficient navigation of the robot scorpion.      
### 56.Power Module (PM) core-specific parameters for a detailed design-oriented inductor model  [ :arrow_down: ](https://arxiv.org/pdf/2008.13659.pdf)
>  This paper obtains shape related parameters and functions of a Power Module ferrite core which can be required in a design-oriented inductor model, which is a fundamental tool to design any electronic power converter and its control policy. Some particular modifications have been introduced into the standardized method of obtaining characteristics core areas and lengths. Also, a novel approach is taken to obtain the air gap reluctance as a function of air gap length for that specific core shape.      
### 57.A collection of definitions and fundamentals for a design-oriented inductor model  [ :arrow_down: ](https://arxiv.org/pdf/2008.13634.pdf)
>  This paper defines and develops useful concepts related to the several kinds of inductances employed in any comprehensive design-oriented ferrite-based inductor model, which is required to properly design and control high-frequency operated electronic power converters. It is also shown how to extract the necessary parameters from a ferrite material datasheet in order to get inductor models useful for a wide range of core temperatures and magnetic induction levels.      
### 58.Machine learning thermal circuit network model for thermal design optimization of electronic circuit board layout with transient heating chips  [ :arrow_down: ](https://arxiv.org/pdf/2008.13571.pdf)
>  This paper describes a method combining Bayesian optimization (BO) and a lamped-capacitance thermal circuit network model that is effective for speeding up the thermal design optimization of an electronic circuit board layout with transient heating chips. As electronic devices have become smaller and more complex, the importance of thermal design optimization to ensure heat dissipation performance has increased. However, such thermal design optimization is difficult because it is necessary to consider various trade-offs associated with packaging and transient temperature changes of heat-generating components. This study aims to improve the performance of thermal design optimization by artificial intelligence. BO using a Gaussian process was combined with the lamped-capacitance thermal circuit network model, and its performance was verified. As a result, BO successfully found the ideal circuit board layout that particle swarm optimization (PSO) and genetic algorithm (GA) could not. The CPU time for BO was 1/20 of that for PSO and GA. In addition, BO found a non-intuitive optimal solution in approximately 7 min from 10 million layout patterns. It was estimated that this was 1/1000 of the CPU time required for analyzing all layout patterns.      
### 59.Applying Deep Learning to Specific Learning Disorder Screening  [ :arrow_down: ](https://arxiv.org/pdf/2008.13525.pdf)
>  Early detection is key for treating those diagnosed with specific learning disorder, which includes problems with spelling, grammar, punctuation, clarity and organization of written expression. Intervening early can prevent potential negative consequences from this disorder. Deep convolutional neural networks (CNNs) perform better than human beings in many visual tasks such as making a medical diagnosis from visual data. The purpose of this study was to evaluate the ability of a deep CNN to detect students with a diagnosis of specific learning disorder from their handwriting. The MobileNetV2 deep CNN architecture was used by applying transfer learning. The model was trained using a data set of 497 images of handwriting samples from students with a diagnosis of specific learning disorder, as well as those without this diagnosis. The detection of a specific learning disorder yielded on the validation set a mean area under the receiver operating characteristics curve of 0.89. This is a novel attempt to detect students with the diagnosis of specific learning disorder using deep learning. Such a system as was built for this study, may potentially provide fast initial screening of students who may meet the criteria for a diagnosis of specific learning disorder.      
### 60.Torrit: A GUI-Based Power System Simulation Platform  [ :arrow_down: ](https://arxiv.org/pdf/2008.13509.pdf)
>  An adequate education on power system operations and controls requires a hands-on experience on a graphical user interface (GUI) based software. At present, most commercial software do not have free editions with high flexibility and most freeware do not have good interfaces. This paper introduces a GUI-based application called "Torrit" for executing operations of power systems, especially for transmission systems. It is written in Python for it's rapid development ability. Torrit's main window includes a single canvas with some standard graphical interactions like create, delete, copy, move, double click etc. The beta version of this application is the focus of this paper that allows executing, saving and re-opening a project in three different modes: per unit computations, power flow, and state estimation. However, it is still in a rudimentary stage and many extensions are planned for future to match the needs of both academia and industry.      
### 61.ROS-Neuro Integration of Deep Convolutional Autoencoders for EEG Signal Compression in Real-time BCIs  [ :arrow_down: ](https://arxiv.org/pdf/2008.13485.pdf)
>  Typical EEG-based BCI applications require the computation of complex functions over the noisy EEG channels to be carried out in an efficient way. Deep learning algorithms are capable of learning flexible nonlinear functions directly from data, and their constant processing latency is perfect for their deployment into online BCI systems. However, it is crucial for the jitter of the processing system to be as low as possible, in order to avoid unpredictable behaviour that can ruin the system's overall usability. In this paper, we present a novel encoding method, based on on deep convolutional autoencoders, that is able to perform efficient compression of the raw EEG inputs. We deploy our model in a ROS-Neuro node, thus making it suitable for the integration in ROS-based BCI and robotic systems in real world scenarios. The experimental results show that our system is capable to generate meaningful compressed encoding preserving to original information contained in the raw input. They also show that the ROS-Neuro node is able to produce such encodings at a steady rate, with minimal jitter. We believe that our system can represent an important step towards the development of an effective BCI processing pipeline fully standardized in ROS-Neuro framework.      
### 62.Predictor Antennas for Moving Relays: Finite Block-length Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.13444.pdf)
>  In future wireless networks, we anticipate that a large number of devices will connect to mobile networks through moving relays installed on vehicles, in particular in public transport vehicles. To provide high-speed moving relays with accurate channel state information different methods have been proposed, among which predictor antenna (PA) is one of the promising ones. Here, the PA system refers to a setup where two sets of antennas are deployed on top of a vehicle, and the front antenna(s) can be used to predict the channel state information for the antenna(s) behind. In this paper, we study the delay-limited performance of PA systems using adaptive rate allocations. We use the fundamental results on the achievable rate of finite block-length codes to study the system throughput and error probability in the presence of short packets. Particularly, we derive closed-form expressions for the error probability, the average transmit rate as well as the optimal rate allocation, and study the effect of different parameters on the performance of PA systems. The results indicate that rate adaptation under finite block-length codewords can impro      
### 63.Curb Your Normality: On the Quality Requirements of Demand Prediction for Dynamic Public Transport  [ :arrow_down: ](https://arxiv.org/pdf/2008.13443.pdf)
>  As Public Transport (PT) becomes more dynamic and demand-responsive, it increasingly depends on predictions of transport demand. But how accurate need such predictions be for effective PT operation? We address this question through an experimental case study of PT trips in Metropolitan Copenhagen, Denmark, which we conduct independently of any specific prediction models. First, we simulate errors in demand prediction through unbiased noise distributions that vary considerably in shape. Using the noisy predictions, we then simulate and optimize demand-responsive PT fleets via a commonly used linear programming formulation and measure their performance. Our results suggest that the optimized performance is mainly affected by the skew of the noise distribution and the presence of infrequently large prediction errors. In particular, the optimized performance can improve under non-Gaussian vs. Gaussian noise. We also obtain that dynamic routing can reduce trip time by at least 23% vs. static routing. This reduction is estimated at 809,000 EUR per year in terms of Value of Travel Time Savings for the case study.      
### 64.Symmetry Exploitation in Orbit Feedback Systems of Synchrotron Storage Rings  [ :arrow_down: ](https://arxiv.org/pdf/2008.13428.pdf)
>  Structural symmetries in the storage ring of synchrotrons are intentionally created during the design phase of the magnetic lattices, but they are not considered in the design of control algorithms that stabilize the beam of accelerated particles. The choice of control algorithm, however, is limited by the speed requirements of the synchrotron. Standard control algorithms for synchrotrons are based on a singular value decomposition (SVD) of the orbit response matrix. SVD controllers neither exploit the structural symmetries nor exhibit any speed advantages. Based on the periodicity and the reflection properties of the betatron function, we show that these structural symmetries are inherited by the orbit response matrix. We show that the resulting block-circulant and centrosymmetric properties of the matrix can be used for different computationally efficient decompositions of the controller. We also address the case of broken symmetry due to odd placements of magnets and monitors. Our efficient decomposition could enable the use of more advanced control techniques for synchrotrons, such as control algorithms that require real-time optimization. These advanced control techniques could in turn increase the quality of research in synchrotron light sources.      
### 65.Hydrogen Penetration and Fuel Cell Vehicle Deployment in the Carbon Constrained Future Energy System  [ :arrow_down: ](https://arxiv.org/pdf/2008.13414.pdf)
>  This research details outcomes from a global model which estimates future hydrogen penetration into a carbon constrained energy system to the year 2050. Focusing on minimum and maximum penetration scenarios, an investigation of global fuel cell vehicle (FCV) deployment is undertaken, cognizant of optimal economic deployment at the global level and stakeholder preferences in a case study of Japan. The model is mathematically formulated as a very large-scale linear optimization problem, aiming to minimize system costs, including generation type, fuel costs, conversion costs, and carbon reduction costs, subject to the constraint of carbon dioxide reductions for each nation. Results show that between approximately 0.8% and 2% of global energy consumption needs can be met by hydrogen out to the year 2050, with city gas and transport emerging as significant use cases. Passenger FCVs and hydrogen buses account for almost all of the hydrogen-based transportation sector, leading to a global deployment of approximately 120 million FCVs by 2050. Hydrogen production is reliant on fossil fuels, and OECD nations are net importers - especially Japan with a 100% import case. To underpin hydrogen production from fossil fuels, carbon capture and storage (CCS) is required in significant quantities when anticipating a large fleet of FCVs. Stakeholder engagement suggests optimism toward FCV deployment while policy issues identified include necessity for large-scale future energy system investment and rapid technical and economic feasibility progress for renewable energy technologies and electrolyzers.      
### 66.Augmented Reality-Based Advanced Driver-Assistance System for Connected Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.13381.pdf)
>  With the development of advanced communication technology, connected vehicles become increasingly popular in our transportation systems, which can conduct cooperative maneuvers with each other as well as road entities through vehicle-to-everything communication. A lot of research interests have been drawn to other building blocks of a connected vehicle system, such as communication, planning, and control. However, less research studies were focused on the human-machine cooperation and interface, namely how to visualize the guidance information to the driver as an advanced driver-assistance system (ADAS). In this study, we propose an augmented reality (AR)-based ADAS, which visualizes the guidance information calculated cooperatively by multiple connected vehicles. An unsignalized intersection scenario is adopted as the use case of this system, where the driver can drive the connected vehicle crossing the intersection under the AR guidance, without any full stop at the intersection. A simulation environment is built in Unity game engine based on the road network of San Francisco, and human-in-the-loop (HITL) simulation is conducted to validate the effectiveness of our proposed system regarding travel time and energy consumption.      
### 67.Benchmarking Metric Ground Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2008.13315.pdf)
>  Metric ground navigation addresses the problem of autonomously moving a robot from one point to another in an obstacle-occupied planar environment in a collision-free manner. It is one of the most fundamental capabilities of intelligent mobile robots. This paper presents a standardized testbed with a set of environments and metrics to benchmark difficulty of different scenarios and performance of different systems of metric ground navigation. Current benchmarks focus on individual components of mobile robot navigation, such as perception and state estimation, but the navigation performance as a whole is rarely measured in a systematic and standardized fashion. As a result, navigation systems are usually tested and compared in an ad hoc manner, such as in one or two manually chosen environments. The introduced benchmark provides a general testbed for ground robot navigation in a metric world. The dataset includes 300 navigation environments, which are ordered by a set of difficulty metrics. Navigation performance can be tested and compared in those environments in a systematic and objective fashion. This benchmark can be used to predict navigation difficulty of a new environment, compare navigation systems, and potentially serve as a cost function and a curriculum for planning-based and learning-based navigation systems. We have published our dataset and the source code to generate datasets for different robot footprints at <a class="link-external link-http" href="http://www.cs.utexas.edu/~attruong/metrics_dataset.html" rel="external noopener nofollow">this http URL</a>.      
### 68.A Meta-Learning Control Algorithm with Provable Finite-Time Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2008.13265.pdf)
>  In this work we provide provable regret guarantees for an online meta-learning control algorithm in an iterative control setting, where in each iteration the system to be controlled is a linear deterministic system that is different and unknown, the cost for the controller in an iteration is a general additive cost function and the control input is required to be constrained, which if violated incurs an additional cost. We prove (i) that the algorithm achieves a regret for the controller cost and constraint violation that are $O(T^{3/4})$ for an episode of duration $T$ with respect to the best policy that satisfies the control input control constraints and (ii) that the average of the regret for the controller cost and constraint violation with respect to the same policy vary as $O((1+\log(N)/N)T^{3/4})$ with the number of iterations $N$, showing that the worst regret for the learning within an iteration continuously improves with experience of more iterations.      
### 69.An evolutionary perspective on the design of neuromorphic shape filters  [ :arrow_down: ](https://arxiv.org/pdf/2008.13229.pdf)
>  A substantial amount of time and energy has been invested to develop machine vision using connectionist (neural network) principles. Most of that work has been inspired by theories advanced by neuroscientists and behaviorists for how cortical systems store stimulus information. Those theories call for information flow through connections among several neuron populations, with the initial connections being random (or at least non-functional). Then the strength or location of connections are modified through training trials to achieve an effective output, such as the ability to identify an object. Those theories ignored the fact that animals that have no cortex, e.g., fish, can demonstrate visual skills that outpace the best neural network models. Neural circuits that allow for immediate effective vision and quick learning have been preprogrammed by hundreds of millions of years of evolution and the visual skills are available shortly after hatching. Cortical systems may be providing advanced image processing, but most likely are using design principles that had been proven effective in simpler systems. The present article provides a brief overview of retinal and cortical mechanisms for registering shape information, with the hope that it might contribute to the design of shape-encoding circuits that more closely match the mechanisms of biological vision.      
### 70.Large-area femtosecond laser milling of silicon employing trench analysis  [ :arrow_down: ](https://arxiv.org/pdf/2008.13205.pdf)
>  A femtosecond laser is a powerful tool for micromachining of silicon. In this work, large-area laser ablation of crystalline silicon is comprehensively studied using a laser source of pulse width 300 fs at two wavelengths of 343 nm and 1030 nm. We develop a unique approach to gain insight into the laser milling process by means of detailed analysis of trenches. Laser scribed trenches and milled areas are characterized using optical profilometry to extract dimensional and roughness parameters with accuracy and repeatability. In a first step, multiple measures of the trench including the average depth, the volume of recast material, the average longitudinal profile roughness, the inner trench width and the volume removal rate are studied. This allows for delineation of ablation regimes and associated characteristics allowing to determine the impact of fluence and repetition rate on laser milling. In a second step, additional factors of debris formation and material redeposition that come into play during laser milling are further elucidated. These results are utilized for processing large-area (up to few mm2) with milling depths up to 200 {\mu}m to enable the fabrication of cavities with low surface roughness at high removal rates of up to 6.9 {\mu}m3 {\mu}s-1. Finally, laser processing in combination with XeF2 etching is applied on SOI-CMOS technology in the fabrication of radio-frequency (RF) functions standing on suspended membranes. Performance is considerably improved on different functions like RF switch (23 dB improvement in 2nd harmonic), inductors (near doubling of Q-factor) and LNA (noise figure improvement of 0.1 dB) demonstrating the applicability of milling to radio-frequency applications.      
### 71.T$^{\star}$-Lite: A Fast Time-Risk Optimal Motion Planning Algorithm for Multi-Speed Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.13048.pdf)
>  In this paper, we develop a new algorithm, called T$^{\star}$-Lite, that enables fast time-risk optimal motion planning for variable-speed autonomous vehicles. The T$^{\star}$-Lite algorithm is a significantly faster version of the previously developed T$^{\star}$ algorithm. T$^{\star}$-Lite uses the novel time-risk cost function of T$^{\star}$; however, instead of a grid-based approach, it uses an asymptotically optimal sampling-based motion planner. Furthermore, it utilizes the recently developed Generalized Multi-speed Dubins Motion-model (GMDM) for sample-to-sample kinodynamic motion planning. The sample-based approach and GMDM significantly reduce the computational burden of T$^{\star}$ while providing reasonable solution quality. The sample points are drawn from a four-dimensional configuration space consisting of two position coordinates plus vehicle heading and speed. Specifically, T$^{\star}$-Lite enables the motion planner to select the vehicle speed and direction based on its proximity to the obstacle to generate faster and safer paths. In this paper, T$^{\star}$-Lite is developed using the RRT$^{\star}$ motion planner, but adaptation to other motion planners is straightforward and depends on the needs of the planner      
### 72.$ε^*$+: An Online Coverage Path Planning Algorithm for Energy-constrained Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.13041.pdf)
>  This paper presents a novel algorithm, called $\epsilon^*$+, for online coverage path planning of unknown environments using energy-constrained autonomous vehicles. Due to limited battery size, the energy-constrained vehicles have limited duration of operation time. Therefore, while executing a coverage trajectory, the vehicle has to return to the charging station for a recharge before the battery runs out. In this regard, the $\epsilon^*$+ algorithm enables the vehicle to retreat back to the charging station based on the remaining energy which is monitored throughout the coverage process. This is followed by an advance trajectory that takes the vehicle to a near by unexplored waypoint to restart the coverage process, instead of taking it back to the previous left over point of the retreat trajectory; thus reducing the overall coverage time. The proposed $\epsilon^*$+ algorithm is an extension of the $\epsilon^*$ algorithm, which utilizes an Exploratory Turing Machine (ETM) as a supervisor to navigate the vehicle with back and forth trajectory for complete coverage. The performance of the $\epsilon^*$+ algorithm is validated on complex scenarios using Player/Stage which is a high-fidelity robotic simulator.      
### 73.On segmentation of pectoralis muscle in digital mammograms by means of deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.12904.pdf)
>  Computer-aided diagnosis (CAD) has long become an integral part of radiological management of breast disease, facilitating a number of important clinical applications, including quantitative assessment of breast density and early detection of malignancies based on X-ray mammography. Common to such applications is the need to automatically discriminate between breast tissue and adjacent anatomy, with the latter being predominantly represented by pectoralis major (or pectoral muscle). Especially in the case of mammograms acquired in the mediolateral oblique (MLO) view, the muscle is easily confusable with some elements of breast anatomy due to their morphological and photometric similarity. As a result, the problem of automatic detection and segmentation of pectoral muscle in MLO mammograms remains a challenging task, innovative approaches to which are still required and constantly searched for. To address this problem, the present paper introduces a two-step segmentation strategy based on a combined use of data-driven prediction (deep learning) and graph-based image processing. In particular, the proposed method employs a convolutional neural network (CNN) which is designed to predict the location of breast-pectoral boundary at different levels of spatial resolution. Subsequently, the predictions are used by the second stage of the algorithm, in which the desired boundary is recovered as a solution to the shortest path problem on a specially designed graph. The proposed algorithm has been tested on three different datasets (i.e., MIAS, CBIS-DDSm and InBreast) using a range of quantitative metrics. The results of comparative analysis show considerable improvement over state-of-the-art, while offering the possibility of model-free and fully automatic processing.      
### 74.Self-Organized Operational Neural Networks for Severe Image Restoration Problems  [ :arrow_down: ](https://arxiv.org/pdf/2008.12894.pdf)
>  Discriminative learning based on convolutional neural networks (CNNs) aims to perform image restoration by learning from training examples of noisy-clean image pairs. It has become the go-to methodology for tackling image restoration and has outperformed the traditional non-local class of methods. However, the top-performing networks are generally composed of many convolutional layers and hundreds of neurons, with trainable parameters in excess of several millions. We claim that this is due to the inherent linear nature of convolution-based transformation, which is inadequate for handling severe restoration problems. Recently, a non-linear generalization of CNNs, called the operational neural networks (ONN), has been shown to outperform CNN on AWGN denoising. However, its formulation is burdened by a fixed collection of well-known nonlinear operators and an exhaustive search to find the best possible configuration for a given architecture, whose efficacy is further limited by a fixed output layer operator assignment. In this study, we leverage the Taylor series-based function approximation to propose a self-organizing variant of ONNs, Self-ONNs, for image restoration, which synthesizes novel nodal transformations onthe-fly as part of the learning process, thus eliminating the need for redundant training runs for operator search. In addition, it enables a finer level of operator heterogeneity by diversifying individual connections of the receptive fields and weights. We perform a series of extensive ablation experiments across three severe image restoration tasks. Even when a strict equivalence of learnable parameters is imposed, Self-ONNs surpass CNNs by a considerable margin across all problems, improving the generalization performance by up to 3 dB in terms of PSNR.      
### 75.Convergence of a Distributed Kiefer-Wolfowitz Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2008.12856.pdf)
>  This paper proposes a proof of the convergence of a distributed and asynchronous version of the Kiefer-Wolfowitz algorithm.      
### 76.Defensive Cost-Benefit Analysis of Smart Grid Digital Functionalities  [ :arrow_down: ](https://arxiv.org/pdf/2008.12843.pdf)
>  Smart grids offer several types of digital control and monitoring of electric power transmission and distribution. The potential costs of inherent security vulnerabilities, including likelihoods of exploitation, are difficult to determine. This article presents a method for comparing the economic benefits and costs of cyber-functionalities associated with smart grids.      
