# ArXiv eess --Wed, 23 Sep 2020
### 1.Fault-Tolerant Formation Tracking of Heterogeneous Multi-Agent Systems with Time-Varying Actuator Faults and Its Application to Task-Space Cooperative Tracking of Manipulators  [ :arrow_down: ](https://arxiv.org/pdf/2009.10700.pdf)
>  This paper addresses a formation tracking problem for nonlinear multi-agent systems with time-varying actuator faults, in which only a subset of agents has access to the leader's information over the directed leader-follower network with a spanning tree. Both the amplitudes and signs of control coefficients induced by actuator faults are unknown and time-varying. The aforementioned setting improves the practical relevance of the problem to be investigated, and meanwhile, it poses technical challenges to distributed controller design and asymptotic stability analysis. By introducing a distributed estimation and control framework, a novel distributed control law based on a Nussbaum gain technique is developed to achieve robust fault-tolerant formation tracking for heterogeneous nonlinear multi-agent systems with time-varying actuator faults. It can be proved that the asymptotic convergence is guaranteed. In addition, the proposed approach is applied to task-space cooperative tracking of networked manipulators irrespective of the uncertain kinematics, dynamics, and actuator faults. Numerical simulation results are presented to verify the effectiveness of the proposed designs.      
### 2.Deep Learning based NAS Score and Fibrosis Stage Prediction from CT and Pathology Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.10687.pdf)
>  Non-Alcoholic Fatty Liver Disease (NAFLD) is becoming increasingly prevalent in the world population. Without diagnosis at the right time, NAFLD can lead to non-alcoholic steatohepatitis (NASH) and subsequent liver damage. The diagnosis and treatment of NAFLD depend on the NAFLD activity score (NAS) and the liver fibrosis stage, which are usually evaluated from liver biopsies by pathologists. In this work, we propose a novel method to automatically predict NAS score and fibrosis stage from CT data that is non-invasive and inexpensive to obtain compared with liver biopsy. We also present a method to combine the information from CT and H\&amp;E stained pathology data to improve the performance of NAS score and fibrosis stage prediction, when both types of data are available. This is of great value to assist the pathologists in computer-aided diagnosis process. Experiments on a 30-patient dataset illustrate the effectiveness of our method.      
### 3.Attack-Resilient Distributed Algorithms for Exponential Nash Equilibrium Seeking  [ :arrow_down: ](https://arxiv.org/pdf/2009.10666.pdf)
>  This paper investigates a resilient distributed Nash equilibrium (NE) seeking problem on a directed communication network subject to malicious cyber-attacks. The considered attacks, named as Denial-of-Service (DoS) attacks, are allowed to occur aperiodically, which refers to interruptions of communication channels carried out by intelligent adversaries. In such an insecure network environment, the existence of cyber-attacks may result in undesirable performance degradations or even the failures of distributed algorithm to seek the NE of noncooperative games. Hence, the aforementioned setting can improve the practical relevance of the problem to be addressed and meanwhile, it poses some technical challenges to the distributed algorithm design and exponential convergence analysis. In contrast to the existing distributed NE seeking results over a prefect communication network, an attack-resilient distributed algorithm is presented such that the NE can be exactly reached with an exponential convergence rate in the presence of DoS attacks. Inspired by the previous works in [21]-[26], an explicit analysis of the attack frequency and duration is investigated to enable exponential NE seeking with resilience against attacks.Examples and numerical simulation results are given to show the effectiveness of the proposed design.      
### 4.Channel Estimation for Distributed Intelligent Reflecting Surfaces Assisted Multi-User MISO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.10653.pdf)
>  Intelligent reflecting surfaces (IRSs)-assisted wireless communication promises improved system performance, while posing new challenges in channel estimation (CE) due to the passive nature of the reflecting elements. Although a few CE protocols for IRS-assisted multiple-input single-output (MISO) systems have appeared, they either require long channel training times or are developed under channel sparsity assumptions. Moreover, existing works focus on a single IRS, whereas in practice multiple such surfaces should be installed to truly benefit from the concept of reconfiguring propagation environments. In light of these challenges, this paper tackles the CE problem for the distributed IRSs-assisted multi-user MISO system. An optimal CE protocol requiring relatively low training overhead is developed using Bayesian techniques under the practical assumption that the BS-IRSs channels are dominated by the line-of-sight (LoS) components. An optimal solution for the phase shifts vectors required at all IRSs during CE is determined and the minimum mean square error (MMSE) estimates of the BSusers direct channels and the IRSs-users channels are derived. Simulation results corroborate the normalized MSE (NMSE) analysis and establish the advantage of the proposed protocol as compared to benchmark scheme in terms of training overhead.      
### 5.Output-Feedback Model Predictive Control with Online Identification  [ :arrow_down: ](https://arxiv.org/pdf/2009.10631.pdf)
>  Model predictive control (MPC) is a widely used modern control technique with numerous successful application in diverse areas. Much of this success is due to the ability of MPC to enforce state and control constraints, which are crucial in many applications of control. In order to avoid the need for an observer, output-feedback model predictive control with online identification (OFMPCOI) uses the block observable canonical form whose state consists of past values of the control inputs and measured outputs. Online identification is performed using recursive least squares (RLS) with variable-rate forgetting. The article describes the algorithmic details of OFMPCOI and numerically investigates its performance through a collection of numerical examples that highlight various control challenges, such as model order uncertainty, sensor noise, prediction horizon, stabilization, magnitude and move-size saturation, and stabilization. The numerical examples are used to probe the performance of OFMPCOI in terms of persistency, consistency, and exigency. Since OFMPCOI does not employ a separate control perturbation to enhance persistency, the focus is on self-generated persistency during transient operation. For closed-loop identification using RLS, sensor noise gives rise to bias in the identified model, and the goal is to determine the effect of the lack of consistency. Finally, the numerical examples reveal exigency, which is the extent to which the online identification emphasizes model characteristics that are most relevant to meeting performance objectives.      
### 6.Analytical Modeling of Nonlinear Fiber Propagation for Four Dimensional Symmetric Constellations  [ :arrow_down: ](https://arxiv.org/pdf/2009.10630.pdf)
>  Coherent optical transmission systems naturally lead to a four dimensional (4D) signal space, i.e., two polarizations each with two quadratures. In this paper we derive an analytical model to quantify the impact of Kerr nonlinearity on such 4Dspaces, taking the interpolarization dependency into account. This is in contrast to previous models such as the GN and EGN models, which are valid for polarization multiplexed (PM)formats, where the two polarizations are seen as independent channels on which data is multiplexed. The proposed model agrees with the EGN model in the special case of independent two-dimensional modulation in each polarization. The model accounts for the predominant nonlinear terms in a WDM system, namely self-phase modulation and and cross-phase modulation. Numerical results show that the EGN model may inaccurately estimate the nonlinear interference of 4D formats. This nonlinear interference discrepancy between the results of the proposed model and the EGN model could be up to 2.8 dB for a system with 80 WDM channels. The derived model is validated by split-step Fourier simulations, and it is shown to follow simulations very closely.      
### 7.Dual Encoder Fusion U-Net (DEFU-Net) for Cross-manufacturer Chest X-ray Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.10608.pdf)
>  A number of methods based on the deep learning have been applied to medical image segmentation and have achieved state-of-the-art performance. Due to the importance of chest x-ray data in studying COVID-19, there is a demand for state-of-the-art models capable of precisely segmenting chest x-rays before obtaining mask annotations about this sort of dataset. The dataset for exploring best pre-trained model is from Montgomery and Shenzhen hospital which had opened in 2014. The most famous technique is U-Net which has been used to many medical datasets including the Chest X-ray. However, most of variant U-Net mainly focus on extraction of contextual information and dense skip connection. There is still a large space for improving extraction of spatial feature. In this paper, we propose a dual encoder fusion U-Net framework for Chest X-rays based on Inception Convolutional Neural Network with dilation, Densely Connected Recurrent Convolutional Neural Network, which is named DEFU-Net. The densely connected recurrent path extends the network deeper for facilitating context feature extraction. In order to increase the width of network and enrich representation of features, the inception blocks with dilation have been used. The inception blocks can capture globally and locally spatial information by various receptive fields. Meanwhile, the features fusion of two path by summation preserve the context and the spatial information for decoding part. This multi-learning-scale model are benefiting in Chest X-ray dataset from two different manufacturers (Montgomery and Shenzhen hospital). The DEFU-Net achieves the better performance than basic U-Net, residual U-Net, BCDU-Net, modified R2U-Net and modified attention R2U-Net. This model is proved the feasibility for mixed dataset. The open source code for this proposed framework will be public soon.      
### 8.Optimizing Discrete Multi-tone Transmission for 400G Data Center Interconnects  [ :arrow_down: ](https://arxiv.org/pdf/2009.10591.pdf)
>  Discrete multi-tone transmission (DMT) is a promising candidate for future 400G data center interconnects. Eight channels, each carrying 56 Gb/s of data can be combined in a 50-GHz channel grid to form a 400 Gb/s superchannel. For a fully loaded 96 channel DWDM system this leads to a total capacity of 4.8 Tb/s. To meet the requirements of the targeted application in terms of cost efficiency and low power consumption, it is important to keep the complexity for the digital signal processing as low as possible. For DMT, the complexity is mainly determined by the length of the fast Fourier transformation (FFT). Since data center interconnects only have to bridge a distance of typically 80 km, we here investigate among other parameters the influence of the FFT length among other parameters on the achievable performance for 56 Gb/s DMT only for this distance. Transmission is performed in C-band to enable DWDM and no optical dispersion compensation is applied. We consider double sideband (DSB) as well as vestigial sideband (VSB) DMT. It can be seen that an FFT length of 128 is sufficient to reach the required performance in terms of bit error ratio, however, a higher length can significantly improve the performance.      
### 9.Practical Solutions for 400 Gbit/s Data Center Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2009.10585.pdf)
>  We review three solutions for low-cost data center interconnects with a target reach of up to 80 km. Directly detected DMT, PAM-4 and multi-band CAP are promising modulation schemes, enabling 400 Gbit/s by combining eight channels of 56 Gbit/s.      
### 10.Solutions for 80 km DWDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.10570.pdf)
>  We review currently discussed solutions for 80 km DWDM transmission targeting inter data-center connections at 100G and 400G line rates. PDM-64QAM, PAM4 and DMT are investigated, while the focus lies on directly detected solutions.      
### 11.CA-Net: Comprehensive Attention Convolutional Neural Networks for Explainable Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.10549.pdf)
>  Accurate medical image segmentation is essential for diagnosis and treatment planning of diseases. Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance for automatic medical image segmentation. However, they are still challenged by complicated conditions where the segmentation target has large variations of position, shape and scale, and existing CNNs have a poor explainability that limits their application to clinical decisions. In this work, we make extensive use of multiple attentions in a CNN architecture and propose a comprehensive attention-based CNN (CA-Net) for more accurate and explainable medical image segmentation that is aware of the most important spatial positions, channels and scales at the same time. In particular, we first propose a joint spatial attention module to make the network focus more on the foreground region. Then, a novel channel attention module is proposed to adaptively recalibrate channel-wise feature responses and highlight the most relevant feature channels. Also, we propose a scale attention module implicitly emphasizing the most salient feature maps among multiple scales so that the CNN is adaptive to the size of an object. Extensive experiments on skin lesion segmentation from ISIC 2018 and multi-class segmentation of fetal MRI found that our proposed CA-Net significantly improved the average segmentation Dice score from 87.77% to 92.08% for skin lesion, 84.79% to 87.08% for the placenta and 93.20% to 95.88% for the fetal brain respectively compared with U-Net. It reduced the model size to around 15 times smaller with close or even better accuracy compared with state-of-the-art DeepLabv3+. In addition, it has a much higher explainability than existing networks by visualizing the attention weight maps. Our code is available at <a class="link-external link-https" href="https://github.com/HiLab-git/CA-Net" rel="external noopener nofollow">this https URL</a>      
### 12.An unnoticed side effect of electric vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2009.10543.pdf)
>  We illustrate that the electrification of our transport system might impose unnecessary extra congestion and delay for daily commuting passengers. By modelling travel behaviors of these passengers, it is found that more of them tend to depart at a narrower peak-hour time window. The occurrence of this shift is mainly caused by (1) the energy consumption of electric vehicles (EVs) is much lower than that of traditional vehicles and (2) the energy consumption of EVs is less sensitive to congestion than that of traditional vehicles. We further examine the role of congestion toll in minimizing the extra congestion and delay.      
### 13.Modeling of Vertical Dipole Above Lossy Dielectric Half-Space: Characteristic Mode Theory  [ :arrow_down: ](https://arxiv.org/pdf/2009.10501.pdf)
>  This work introduces a theoretical extension of the characteristic mode formulation for analysing the vertical electric dipole lying above a lossy dielectric half-space. As the conventional characteristic formulation fails to maintain the orthogonality of the characteristic field modes over the infinite sphere, an alternate modal formulation is proposed here to maintain the orthogonality for both the current and field modes. The modal results are found to match closely with its method of moment counterparts. Later, the modes of an isolated dipole with no ground plane have been used to predict the role of the lossy ground plane through a theory of the linear combination of the eigenvectors. The proposed formulations have been studied with different heights from the ground plane and are compared with the direct modal solutions to validate its accuracy. It helps to provide a thorough understanding of how the isolated modes interact among each other to constitute the perturbed modes in the presence of the lossy half-space. It can find application to include the lossy earth effect in the study of the lightning fields and the path loss modelling of the antennas over the lossy ground.      
### 14.Classification of COVID-19 in CT Scans using Multi-Source Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.10474.pdf)
>  Since December of 2019, novel coronavirus disease COVID-19 has spread around the world infecting millions of people and upending the global economy. One of the driving reasons behind its high rate of infection is due to the unreliability and lack of RT-PCR testing. At times the turnaround results span as long as a couple of days, only to yield a roughly 70% sensitivity rate. As an alternative, recent research has investigated the use of Computer Vision with Convolutional Neural Networks (CNNs) for the classification of COVID-19 from CT scans. Due to an inherent lack of available COVID-19 CT data, these research efforts have been forced to leverage the use of Transfer Learning. This commonly employed Deep Learning technique has shown to improve model performance on tasks with relatively small amounts of data, as long as the Source feature space somewhat resembles the Target feature space. Unfortunately, a lack of similarity is often encountered in the classification of medical images as publicly available Source datasets usually lack the visual features found in medical images. In this study, we propose the use of Multi-Source Transfer Learning (MSTL) to improve upon traditional Transfer Learning for the classification of COVID-19 from CT scans. With our multi-source fine-tuning approach, our models outperformed baseline models fine-tuned with ImageNet. We additionally, propose an unsupervised label creation process, which enhances the performance of our Deep Residual Networks. Our best performing model was able to achieve an accuracy of 0.893 and a Recall score of 0.897, outperforming its baseline Recall score by 9.3%.      
### 15.Diesel Generator Model Parameterization for Microgrid Simulation Using Hybrid Box-Constrained Levenberg-Marquardt Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2009.10425.pdf)
>  Existing generator parameterization methods, typically developed for large turbine generator units, are difficult to apply to small kW-level diesel generators in microgrid applications. This paper presents a model parameterization method that estimates a complete set of kW-level diesel generator parameters simultaneously using only load-step-change tests with limited measurement points. This method provides a more cost-efficient and robust approach to achieve high-fidelity modeling of diesel generators for microgrid dynamic simulation. A two-stage hybrid box-constrained Levenberg-Marquardt (H-BCLM) algorithm is developed to search the optimal parameter set given the parameter bounds. A heuristic algorithm, namely Generalized Opposition-based Learning Genetic Algorithm (GOL-GA), is applied to identify proper initial estimates at the first stage, followed by a modified Levenberg-Marquardt algorithm designed to fine tune the solution based on the first-stage result. The proposed method is validated against dynamic simulation of a diesel generator model and field measurements from a 16kW diesel generator unit.      
### 16.In-phase and Quadrature Chirp Spread Spectrum for IoT Communications  [ :arrow_down: ](https://arxiv.org/pdf/2009.10421.pdf)
>  This paper describes a coherent chirp spread spectrum (CSS) technique based on the Long-Range (LoRa) physical layer (PHY) framework. LoRa PHY employs CSS on top of a variant of frequency shift keying (FSK), and non-coherent detection is employed at the receiver for obtaining the transmitted data symbols. In this paper, we propose a scheme that encodes information bits on both in-phase and quadrature components of the chirp signal, and rather employs a coherent detector at the receiver. Hence, channel equalization is required for compensating the channel induced phase rotation on the transmit signal. Moreover, a simple channel estimation technique exploits the LoRa reference sequences used for synchronization to obtain the complex channel coefficient used in the equalizer. Performance evaluation using numerical simulation shows that the proposed scheme achieves approximately 1 dB gain in terms of energy efficiency, and it doubles the spectral efficiency when compared to the conventional LoRa PHY scheme. This is due to the fact that the coherent receiver is able to exploit the orthogonality between in-phase and quadrature components of the transmit signal.      
### 17.A Crowdsourced Open-Source Kazakh Speech Corpus and Initial Speech Recognition Baseline  [ :arrow_down: ](https://arxiv.org/pdf/2009.10334.pdf)
>  We present an open-source speech corpus for the Kazakh language. The Kazakh speech corpus (KSC) contains around 335 hours of transcribed audio comprising over 154,000 utterances spoken by participants from different regions, age groups, and gender. It was carefully inspected by native Kazakh speakers to ensure high quality. The KSC is the largest publicly available database developed to advance various Kazakh speech and language processing applications. In this paper, we first describe the data collection and prepossessing procedures followed by the description of the database specifications. We also share our experience and challenges faced during database construction. To demonstrate the reliability of the database, we performed the preliminary speech recognition experiments. The experimental results imply that the quality of audio and transcripts are promising. To enable experiment reproducibility and ease the corpus usage, we also released the ESPnet recipe.      
### 18.Constrained Non-Linear Phase Retrieval for Single Distance X-ray Phase Contrast Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2009.10324.pdf)
>  X-ray phase contrast tomography (XPCT) is widely used for 3D imaging of objects with weak contrast in X-ray absorption index but strong contrast in refractive index decrement. To reconstruct an object imaged using XPCT, phase retrieval algorithms are first used to estimate the X-ray phase projections, which is the 2D projection of the refractive index decrement, at each view. Phase retrieval is followed by refractive index decrement reconstruction from the phase projections using an algorithm such as filtered back projection (FBP). In practice, phase retrieval is most commonly solved by approximating it as a linear inverse problem. However, this linear approximation often results in artifacts and blurring when the conditions for the approximation are violated. In this paper, we formulate phase retrieval as a non-linear inverse problem, where we solve for the transmission function, which is the negative exponential of the projections, from XPCT measurements. We use a constraint to enforce proportionality between phase and absorption projections. We do not use constraints such as large Fresnel number, slowly varying phase, or Born/Rytov approximations. Our approach also does not require any regularization parameter tuning since there is no explicit sparsity enforcing regularization function. We validate the performance of our non-linear phase retrieval (NLPR) method using both simulated and real synchrotron datasets. We compare NLPR with a popular linear phase retrieval (LPR) approach and show that NLPR achieves sharper reconstructions with higher quantitative accuracy.      
### 19.End-to-End Speech Recognition and Disfluency Removal  [ :arrow_down: ](https://arxiv.org/pdf/2009.10298.pdf)
>  Disfluency detection is usually an intermediate step between an automatic speech recognition (ASR) system and a downstream task. By contrast, this paper aims to investigate the task of end-to-end speech recognition and disfluency removal. We specifically explore whether it is possible to train an ASR model to directly map disfluent speech into fluent transcripts, without relying on a separate disfluency detection model. We show that end-to-end models do learn to directly generate fluent transcripts; however, their performance is slightly worse than a baseline pipeline approach consisting of an ASR system and a disfluency detection model. We also propose two new metrics that can be used for evaluating integrated ASR and disfluency models. The findings of this paper can serve as a benchmark for further research on the task of end-to-end speech recognition and disfluency removal in the future.      
### 20.End-to-End Learning of Speech 2D Feature-Trajectory for Prosthetic Hands  [ :arrow_down: ](https://arxiv.org/pdf/2009.10283.pdf)
>  Speech is one of the most common forms of communication in humans. Speech commands are essential parts of multimodal controlling of prosthetic hands. In the past decades, researchers used automatic speech recognition systems for controlling prosthetic hands by using speech commands. Automatic speech recognition systems learn how to map human speech to text. Then, they used natural language processing or a look-up table to map the estimated text to a trajectory. However, the performance of conventional speech-controlled prosthetic hands is still unsatisfactory. Recent advancements in general-purpose graphics processing units (GPGPUs) enable intelligent devices to run deep neural networks in real-time. Thus, architectures of intelligent systems have rapidly transformed from the paradigm of composite subsystems optimization to the paradigm of end-to-end optimization. In this paper, we propose an end-to-end convolutional neural network (CNN) that maps speech 2D features directly to trajectories for prosthetic hands. The proposed convolutional neural network is lightweight, and thus it runs in real-time in an embedded GPGPU. The proposed method can use any type of speech 2D feature that has local correlations in each dimension such as spectrogram, MFCC, or PNCC. We omit the speech to text step in controlling the prosthetic hand in this paper. The network is written in Python with Keras library that has a TensorFlow backend. We optimized the CNN for NVIDIA Jetson TX2 developer kit. Our experiment on this CNN demonstrates a root-mean-square error of 0.119 and 20ms running time to produce trajectory outputs corresponding to the voice input data. To achieve a lower error in real-time, we can optimize a similar CNN for a more powerful embedded GPGPU such as NVIDIA AGX Xavier.      
### 21.Performance Prediction for Coherent Noise Radars Using the Correlation Coefficient  [ :arrow_down: ](https://arxiv.org/pdf/2009.10271.pdf)
>  Noise radars can be understood in terms of a correlation coefficient which characterizes their detection performance. Although most results in the literature are stated in terms of the signal-to-noise ratio (SNR), we show that it is possible to carry out performance prediction in terms of the correlation coefficient. To this end, we derive the range dependence of the correlation coefficient. We then combine our result with a previously-derived expression for the receiver operating characteristic (ROC) curve of a coherent noise radar, showing that we can obtain ROC curves for varying ranges. A comparison with corresponding results for a conventional radar employing coherent integration shows that our results are sensible. The aim of our work is to show that the correlation coefficient is a viable adjunct to SNR in understanding radar performance.      
### 22.Safety-Critical Control of Compartmental Epidemiological Models with Measurement Delays  [ :arrow_down: ](https://arxiv.org/pdf/2009.10262.pdf)
>  We introduce a methodology to guarantee safety against the spread of infectious diseases by viewing epidemiological models as control systems and by considering human interventions (such as quarantining or social distancing) as control input. We consider a generalized compartmental model that represents the form of the most popular epidemiological models and we design safety-critical controllers that formally guarantee safe evolution with respect to keeping certain populations of interest under prescribed safe limits. Furthermore, we discuss how measurement delays originated from incubation period and testing delays affect safety and how delays can be compensated via predictor feedback. We demonstrate our results by synthesizing active intervention policies that bound the number of infections, hospitalizations and deaths for epidemiological models capturing the spread of COVID-19 in the USA.      
### 23.Less Manual Work for Safety Engineers: Towards an Automated Safety Reasoning with Safety Patterns  [ :arrow_down: ](https://arxiv.org/pdf/2009.10251.pdf)
>  The development of safety-critical systems requires the control of hazards that can potentially cause harm. To this end, safety engineers rely during the development phase on architectural solutions, called safety patterns, such as safety monitors, voters, and watchdogs. The goal of these patterns is to control (identified) faults that can trigger hazards. Safety patterns can control such faults by e.g., increasing the redundancy of the system. Currently, the reasoning of which pattern to use at which part of the target system to control which hazard is documented mostly in textual form or by means of models, such as GSN-models, with limited support for automation. This paper proposes the use of logic programming engines for the automated reasoning about system safety. We propose a domain-specific language for embedded system safety and specify as disjunctive logic programs reasoning principles used by safety engineers to deploy safety patterns, e.g., when to use safety monitors, or watchdogs. Our machinery enables two types of automated safety reasoning: (1) identification of which hazards can be controlled and which ones cannot be controlled by the existing safety patterns; and (2) automated recommendation of which patterns could be used at which place of the system to control potential hazards. Finally, we apply our machinery to two examples taken from the automotive domain: an adaptive cruise control system and a battery management system.      
### 24.Improving Maximal Safe Brain Tumor Resection with Photoacoustic Remote Sensing Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.10226.pdf)
>  Malignant brain tumors are among the deadliest neoplasms with the lowest survival rates of any cancer type. In considering surgical tumor resection, suboptimal extent of resection is linked to poor clinical outcomes and lower overall survival rates. Currently available tools for intraoperative histopathological assessment require an average of 20 minutes processing and are of limited diagnostic quality for guiding surgeries. Consequently, there is an unaddressed need for a rapid imaging technique to guide maximal resection of brain tumors. Working towards this goal, presented here is an all optical non-contact label-free reflection mode photoacoustic remote sensing (PARS) microscope. By using a tunable excitation laser, PARS takes advantage of the endogenous optical absorption peaks of DNA and cytoplasm to achieve virtual contrast analogous to standard hematoxylin and eosin (H and E) staining. In conjunction, a fast 266 nm excitation is used to generate large grossing scans and rapidly assess small fields in real-time with hematoxylin-like contrast. Images obtained using this technique show comparable quality and contrast to the current standard for histopathological assessment of brain tissues. Using the proposed method, rapid, high-throughput, histological-like imaging was achieved in unstained brain tissues, indicating PARS utility for intraoperative guidance to improve extent of surgical resection.      
### 25.Sensitivity of BPA SAR Image Formation to Initial Position, Velocity, and Attitude Navigation Errors  [ :arrow_down: ](https://arxiv.org/pdf/2009.10210.pdf)
>  The Back-Projection Algorithm (BPA) is a time domain matched filtering technique to form synthetic aperture radar (SAR) images. To produce high quality BPA images, precise navigation data for the radar platform must be known. Any error in position, velocity, or attitude results in improperly formed images corrupted by shifting, blurring, and distortion. This paper develops analytical expressions that characterize the relationship between navigation errors and image formation errors. These analytical expressions are verified via simulated image formation and real data image formation.      
### 26.Federated Learning for Computational Pathology on Gigapixel Whole Slide Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.10190.pdf)
>  Deep Learning-based computational pathology algorithms have demonstrated profound ability to excel in a wide array of tasks that range from characterization of well known morphological phenotypes to predicting non-human-identifiable features from histology such as molecular alterations. However, the development of robust, adaptable, and accurate deep learning-based models often rely on the collection and time-costly curation large high-quality annotated training data that should ideally come from diverse sources and patient populations to cater for the heterogeneity that exists in such datasets. Multi-centric and collaborative integration of medical data across multiple institutions can naturally help overcome this challenge and boost the model performance but is limited by privacy concerns amongst other difficulties that may arise in the complex data sharing process as models scale towards using hundreds of thousands of gigapixel whole slide images. In this paper, we introduce privacy-preserving federated learning for gigapixel whole slide images in computational pathology using weakly-supervised attention multiple instance learning and differential privacy. We evaluated our approach on two different diagnostic problems using thousands of histology whole slide images with only slide-level labels. Additionally, we present a weakly-supervised learning framework for survival prediction and patient stratification from whole slide images and demonstrate its effectiveness in a federated setting. Our results show that using federated learning, we can effectively develop accurate weakly supervised deep learning models from distributed data silos without direct data sharing and its associated complexities, while also preserving differential privacy using randomized noise generation.      
### 27.CCBlock: An Effective Use of Deep Learning for Automatic Diagnosis of COVID-19 Using X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.10141.pdf)
>  Propose: Troubling countries one after another, the COVID-19 pandemic has dramatically affected the health and well-being of the world's population. The disease may continue to persist more extensively due to the increasing number of new cases daily, the rapid spread of the virus, and delay in the PCR analysis results. Therefore, it is necessary to consider developing assistive methods for detecting and diagnosing the COVID-19 to eradicate the spread of the novel coronavirus among people. Based on convolutional neural networks (CNNs), automated detection systems have shown promising results of diagnosing patients with the COVID-19 through radiography; thus, they are introduced as a workable solution to the COVID-19 diagnosis. Materials and Methods: Based on the enhancement of the classical visual geometry group (VGG) network with the convolutional COVID block (CCBlock), an efficient screening model was proposed in this study to diagnose and distinguish patients with the COVID-19 from those with pneumonia and the healthy people through radiography. The model testing dataset included 1,828 x-ray images available on public platforms. 310 images were showing confirmed COVID-19 cases, 864 images indicating pneumonia cases, and 654 images showing healthy people. Results: According to the test results, enhancing the classical VGG network with radiography provided the highest diagnosis performance and overall accuracy of 98.52% for two classes as well as accuracy of 95.34% for three classes. Conclusions: According to the results, using the enhanced VGG deep neural network can help radiologists automatically diagnose the COVID-19 through radiography.      
### 28.Evaluating phase synchronization methods in fMRI: a comparison study and new approaches  [ :arrow_down: ](https://arxiv.org/pdf/2009.10126.pdf)
>  In recent years there has been growing interest in measuring time-varying functional connectivity between different brain regions using resting-state functional magnetic resonance imaging (rs-fMRI) data. One way to assess the relationship between signals from different brain regions is to measure their phase synchronization (PS) across time. There are several ways to perform such analyses, and here we compare methods that utilize a PS metric together with a sliding window, referred to here as windowed phase synchronization (WPS), with those that directly measure the instantaneous phase synchronization (IPS). In particular, IPS has recently gained popularity as it offers single time-point resolution of time-resolved fMRI connectivity. In this paper, we discuss the underlying assumptions required for performing PS analyses and emphasize the necessity of band-pass filtering the data to obtain valid results. We review various methods for evaluating PS and introduce a new approach within the IPS framework denoted the cosine of the relative phase (CRP). We contrast methods through a series of simulations and application to rs-fMRI data. Our results indicate that CRP outperforms other tested methods and overcomes issues related to undetected temporal transitions from positive to negative associations common in IPS analysis. Further, in contrast to phase coherence, CRP unfolds the distribution of PS measures, which benefits subsequent clustering of PS matrices into recurring brain states.      
### 29.Design and Implementation of Low Complexity Reconfigurable Filtered-OFDM based LDACS  [ :arrow_down: ](https://arxiv.org/pdf/2009.10109.pdf)
>  L-band Digital Aeronautical Communication System (LDACS) aims to exploit vacant spectrum in L-band via spectrum sharing, and orthogonal frequency division multiplexing (OFDM) is the currently accepted LDACS waveform. Recently, various works dealing with improving the spectrum utilization of LDACS via filtering/windowing are being explored. In this direction, we propose an improved and low complexity reconfigurable filtered OFDM (LRef-OFDM) based LDACS using novel interpolation and masking based multi-stage digital filter. The proposed filter is designed to meet the stringent non-uniform spectral attenuation requirements of LDACS standard. It offers significantly lower complexity as well as higher transmission bandwidth than state-of-the-art approaches. We also integrate the proposed filter in our end-to-end LDACS testbed realized using Zynq System on Chip and analyze the performance in the presence of $L$-band legacy user interference as well as LDACS specific wireless channels. Via extensive experimental results, we demonstrate the superiority of the proposed LRef-OFDM over OFDM and Filtered-OFDM based LDACS in terms of power spectral density, bit error rate, implementation complexity, and group delay parameters.      
### 30.Detection Of Concrete Cracks using Dual-channel Deep Convolutional Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.10612.pdf)
>  Due to cyclic loading and fatigue stress cracks are generated, which affect the safety of any civil infrastructure. Nowadays machine vision is being used to assist us for appropriate maintenance, monitoring and inspection of concrete structures by partial replacement of human-conducted onsite inspections. The current study proposes a crack detection method based on deep convolutional neural network (CNN) for detection of concrete cracks without explicitly calculating the defect features. In the course of the study, a database of 3200 labelled images with concrete cracks has been created, where the contrast, lighting conditions, orientations and severity of the cracks were extremely variable. In this paper, starting from a deep CNN trained with these images of 256 x 256 pixel-resolution, we have gradually optimized the model by identifying the difficulties. Using an augmented dataset, which takes into account the variations and degradations compatible to drone videos, like, random zooming, rotation and intensity scaling and exhaustive ablation studies, we have designed a dual-channel deep CNN which shows high accuracy (~ 92.25%) as well as robustness in finding concrete cracks in realis-tic situations. The model has been tested on the basis of performance and analyzed with the help of feature maps, which establishes the importance of the dual-channel structure.      
### 31.Sculpting light with paper for computational mobile phone microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.10595.pdf)
>  Many optical microscopy techniques rely on structured illumination by way of a projected image (eg. structured illumination microscopy) or a tailored angular distribution of light (eg. ptychography). Electro-optical equipment such as spatial light modulators and light emitting diode (LED) arrays are commonly used to sculpt light for these imaging schemes. However, these high-tech devices are not a requirement for light crafting. Patterned diffusely reflecting surfaces also imprint a spatio-angular structure onto reflected light. We demonstrate that paper patterned with a standard commercial printer can serve to structure the illumination light field for refocusing and stereo microscopy on a mobile phone microscope. Our results illustrate the utility of paper as a light sculpting element for low-tech computational imaging.      
### 32.The Use of AI for Thermal Emotion Recognition: A Review of Problems and Limitations in Standard Design and Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.10589.pdf)
>  With the increased attention on thermal imagery for Covid-19 screening, the public sector may believe there are new opportunities to exploit thermal as a modality for computer vision and AI. Thermal physiology research has been ongoing since the late nineties. This research lies at the intersections of medicine, psychology, machine learning, optics, and affective computing. We will review the known factors of thermal vs. RGB imaging for facial emotion recognition. But we also propose that thermal imagery may provide a semi-anonymous modality for computer vision, over RGB, which has been plagued by misuse in facial recognition. However, the transition to adopting thermal imagery as a source for any human-centered AI task is not easy and relies on the availability of high fidelity data sources across multiple demographics and thorough validation. This paper takes the reader on a short review of machine learning in thermal FER and the limitations of collecting and developing thermal FER data for AI training. Our motivation is to provide an introductory overview into recent advances for thermal FER and stimulate conversation about the limitations in current datasets.      
### 33.Solving Dynamic Optimization Problems to a Specified Accuracy: An Alternating Approach using Integrated Residuals  [ :arrow_down: ](https://arxiv.org/pdf/2009.10454.pdf)
>  We propose a novel direct transcription and solution method for solving nonlinear, continuous-time dynamic optimization problems. Instead of forcing the dynamic constraints to be satisfied only at a selected number of points as in direct collocation, the new approach alternates between minimizing and constraining the squared norm of the dynamic constraint residuals integrated along the whole solution trajectories. As a result, the method can 1) obtain solutions of higher accuracy for the same mesh compared to direct collocation methods, 2) enables a flexible trade-off between solution accuracy and optimality, 3) provides reliable solutions for challenging problems, including those with singular arcs and high-index differential algebraic equations.      
### 34.Inter-database validation of a deep learning approach for automatic sleep scoring  [ :arrow_down: ](https://arxiv.org/pdf/2009.10365.pdf)
>  In this work we describe a new deep learning approach for automatic sleep staging, and carry out its validation by addressing its generalization capabilities on a wide range of sleep staging databases. Prediction capabilities are evaluated in the context of independent local and external generalization scenarios. Effectively, by comparing both procedures it is possible to better extrapolate the expected performance of the method on the general reference task of sleep staging, regardless of data from a specific database. In addition, we examine the suitability of a novel approach based on the use of an ensemble of individual local models and evaluate its impact on the resulting inter-database generalization performance. Validation results show good general performance, as compared to the expected levels of human expert agreement, as well as state-of-the-art automatic sleep staging approaches      
### 35.Learning Task-Agnostic Action Spaces for Movement Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2009.10337.pdf)
>  We propose a novel method for exploring the dynamics of physically based animated characters, and learning a task-agnostic action space that makes movement optimization easier. Like several previous papers, we parameterize actions as target states, and learn a short-horizon goal-conditioned low-level control policy that drives the agent's state towards the targets. Our novel contribution is that with our exploration data, we are able to learn the low-level policy in a generic manner and without any reference movement data. Trained once for each agent or simulation environment, the policy improves the efficiency of optimizing both trajectories and high-level policies across multiple tasks and optimization algorithms. We also contribute novel visualizations that show how using target states as actions makes optimized trajectories more robust to disturbances; this manifests as wider optima that are easy to find. Due to its simplicity and generality, our proposed approach should provide a building block that can improve a large variety of movement optimization methods and applications.      
### 36.iWash: A Smartwatch Handwashing Quality Assessment and Reminder System with Real-time Feedback in the Context of Infectious Disease  [ :arrow_down: ](https://arxiv.org/pdf/2009.10317.pdf)
>  Washing hands properly and frequently is the simplest and most cost-effective interventions to prevent the spread of infectious diseases. People are often ignorant about proper handwashing in different situations and do not know if they wash hands properly. Smartwatches are found to be effective for assessing the quality of handwashing. However, the existing smartwatch based systems are not comprehensive enough in terms of achieving accuracy as well as reminding people to handwash and providing feedback to the user about the quality of handwashing. On-device processing is often required to provide real-time feedback to the user, and so it is important to develop a system that runs efficiently on low-resource devices like smartwatches. However, none of the existing systems for handwashing quality assessment are optimized for on-device processing. We present iWash, a comprehensive system for quality assessment and context-aware reminder for handwashing with real-time feedback using smartwatches. iWash is a hybrid deep neural network based system that is optimized for on-device processing to ensure high accuracy with minimal processing time and battery usage. Additionally, it is a context-aware system that detects when the user is entering home using a Bluetooth beacon and provides reminders to wash hands. iWash also offers touch-free interaction between the user and the smartwatch that minimizes the risk of germ transmission. We collected a real-life dataset and conducted extensive evaluations to demonstrate the performance of iWash. Compared to the existing handwashing quality assessment systems, we achieve around 12% higher accuracy for quality assessment, as well as we reduce the processing time and battery usage by around 37% and 10%, respectively.      
### 37.Design of Efficient Deep Learning models for Determining Road Surface Condition from Roadside Camera Images and Weather Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.10282.pdf)
>  Road maintenance during the Winter season is a safety critical and resource demanding operation. One of its key activities is determining road surface condition (RSC) in order to prioritize roads and allocate cleaning efforts such as plowing or salting. Two conventional approaches for determining RSC are: visual examination of roadside camera images by trained personnel and patrolling the roads to perform on-site inspections. However, with more than 500 cameras collecting images across Ontario, visual examination becomes a resource-intensive activity, difficult to scale especially during periods of snowstorms. This paper presents the results of a study focused on improving the efficiency of road maintenance operations. We use multiple Deep Learning models to automatically determine RSC from roadside camera images and weather variables, extending previous research where similar methods have been used to deal with the problem. The dataset we use was collected during the 2017-2018 Winter season from 40 stations connected to the Ontario Road Weather Information System (RWIS), it includes 14.000 labeled images and 70.000 weather measurements. We train and evaluate the performance of seven state-of-the-art models from the Computer Vision literature, including the recent DenseNet, NASNet, and MobileNet. Moreover, by following systematic ablation experiments we adapt previously published Deep Learning models and reduce their number of parameters to about ~1.3% compared to their original parameter count, and by integrating observations from weather variables the models are able to better ascertain RSC under poor visibility conditions.      
### 38.Semantic Workflows and Machine Learning for the Assessment of Carbon Storage by Urban Trees  [ :arrow_down: ](https://arxiv.org/pdf/2009.10263.pdf)
>  Climate science is critical for understanding both the causes and consequences of changes in global temperatures and has become imperative for decisive policy-making. However, climate science studies commonly require addressing complex interoperability issues between data, software, and experimental approaches from multiple fields. Scientific workflow systems provide unparalleled advantages to address these issues, including reproducibility of experiments, provenance capture, software reusability and knowledge sharing. In this paper, we introduce a novel workflow with a series of connected components to perform spatial data preparation, classification of satellite imagery with machine learning algorithms, and assessment of carbon stored by urban trees. To the best of our knowledge, this is the first study that estimates carbon storage for a region in Africa following the guidelines from the Intergovernmental Panel on Climate Change (IPCC).      
### 39.Fail-Safe Controller Architectures for Quadcopter with Motor Failures  [ :arrow_down: ](https://arxiv.org/pdf/2009.10260.pdf)
>  A fail-safe algorithm in case of motor failure was developed, simulated, and tested. For practical fail-safe flight, the quadcopter may fly with only three or two opposing propellers. Altitude for two-propeller architecture was maintained by a PID controller that is independent from the inner and outer controllers. A PID controller on propeller force deviations from equilibrium was augmented to the inner controller of the three-propeller architecture. Both architectures used LQR for the inner attitude controller and a damped second order outer controller that zeroes the error along the horizontal coordinates. The restrictiveness, stability, robustness, and symmetry of these architectures were investigated with respect to their output limits, initial conditions, and controller frequencies. Although the three-propeller architecture allows for distribution of propeller forces, the two-propeller architecture is more efficient, robust, and stable. The two-propeller architecture is also robust to model uncertainties. It was shown that higher yaw rate leads to greater stability when operating in fail-safe mode.      
### 40.DISPATCH: Design Space Exploration of Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.10214.pdf)
>  Design of Cyber-physical systems (CPSs) is a challenging task that involves searching over a large search space of various CPS configurations and possible values of components composing the system. Hence, there is a need for sample-efficient CPS design space exploration to select the system architecture and component values that meet the target system requirements. We address this challenge by formulating CPS design as a multi-objective optimization problem and propose DISPATCH, a two-step methodology for sample-efficient search over the design space. First, we use a genetic algorithm to search over discrete choices of system component values for architecture search and component selection or only component selection and terminate the algorithm even before meeting the system requirements, thus yielding a coarse design. In the second step, we use an inverse design to search over a continuous space to fine-tune the component values and meet the diverse set of system requirements. We use a neural network as a surrogate function for the inverse design of the system. The neural network, converted into a mixed-integer linear program, is used for active learning to sample component values efficiently in a continuous search space. We illustrate the efficacy of DISPATCH on electrical circuit benchmarks: two-stage and three-stage transimpedence amplifiers. Simulation results show that the proposed methodology improves sample efficiency by 5-14x compared to a prior synthesis method that relies on reinforcement learning. It also synthesizes circuits with the best performance (highest bandwidth/lowest area) compared to designs synthesized using reinforcement learning, Bayesian optimization, or humans.      
### 41.Using Inaudible Audio and Voice Assistants to Transmit Sensitive Data over Telephony  [ :arrow_down: ](https://arxiv.org/pdf/2009.10200.pdf)
>  New security and privacy concerns arise due to the growing popularity of voice assistant (VA) deployments in home and enterprise networks. A number of past research results have demonstrated how malicious actors can use hidden commands to get VAs to perform certain operations even when a person may be in their vicinity. However, such work has not explored how compromised computers that are close to VAs can leverage the phone channel to exfiltrate data with the help of VAs. After characterizing the communication channel that is set up by commanding a VA to make a call to a phone number, we demonstrate how malware can encode data into audio and send it via the phone channel. Such an attack, which can be crafted remotely, at scale and at low cost, can be used to bypass network defenses that may be deployed against leakage of sensitive data. We use Dual-Tone Multi-Frequency tones to encode arbitrary binary data into audio that can be played over computer speakers and sent through a VA mediated phone channel to a remote system. We show that modest amounts of data can be transmitted with high accuracy with a short phone call lasting a few minutes. This can be done while making the audio nearly inaudible for most people by modulating it with a carrier with frequencies that are near the higher end of the human hearing range. Several factors influence the data transfer rate, including the distance between the computer and the VA, the ambient noise that may be present and the frequency of modulating carrier. With the help of a prototype built by us, we experimentally assess the impact of these factors on data transfer rates and transmission accuracy. Our results show that voice assistants in the vicinity of computers can pose new threats to data stored on such computers. These threats are not addressed by traditional host and network defenses. We briefly discuss possible mitigation ways.      
### 42.Resilient In-Season Crop Type Classification in Multispectral Satellite Observations using Growth Stage Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2009.10189.pdf)
>  Crop type classification using satellite observations is an important tool for providing insights about planted area and enabling estimates of crop condition and yield, especially within the growing season when uncertainties around these quantities are highest. As the climate changes and extreme weather events become more frequent, these methods must be resilient to changes in domain shifts that may occur, for example, due to shifts in planting timelines. In this work, we present an approach for within-season crop type classification using moderate spatial resolution (30 m) satellite data that addresses domain shift related to planting timelines by normalizing inputs by crop growth stage. We use a neural network leveraging both convolutional and recurrent layers to predict if a pixel contains corn, soybeans, or another crop or land cover type. We evaluated this method for the 2019 growing season in the midwestern US, during which planting was delayed by as much as 1-2 months due to extreme weather that caused record flooding. We show that our approach using growth stage-normalized time series outperforms fixed-date time series, and achieves overall classification accuracy of 85.4% prior to harvest (September-November) and 82.8% by mid-season (July-September).      
### 43.A Technical Review of Wireless security for the Internet of things: Software Defined Radio perspective  [ :arrow_down: ](https://arxiv.org/pdf/2009.10171.pdf)
>  The increase of cyberattacks using IoT devices has exposed the vulnerabilities in the infrastructures that make up the IoT and have shown how small devices can affect networks and services functioning. This paper presents a review of the vulnerabilities of the wireless technologies that bear the IoT and assessing the experiences in implementing wireless attacks targeting the Internet of Things using Software-Defined Radio (SDR) technologies. A systematic literature review was conducted. The types of vulnerabilities and attacks that can affect the wireless technologies that stand the IoT ecosystem and SDR radio platforms were compared. On the IoT system model layer, perception layer was identified as the most vulnerable. Most attacks at this level occur due to limitations in hardware, physical exposure of devices, and heterogeneity of technologies. Future cybersecurity systems based on SDR radios have notable advantages due to their flexibility to adapt to new communication technologies and their potential for the development of advanced tools. However, cybersecurity challenges for the Internet of Things are so complex that it is needed to merge SDR hardware with cognitive techniques and intelligent techniques such as deep learning to adapt to rapid technological changes.      
### 44.Photoacoustic microscopical simulation platform for large volumetric imaging using Bessel beam  [ :arrow_down: ](https://arxiv.org/pdf/2009.10070.pdf)
>  We developed a Bessel-beam photoacoustic microscopical simulation platform by using the k-Wave: MATLAB toolbox. The simulation platform uses the ring slit method to generate Bessel beam. By controlling the inner and outer radius of the ring slit, the depth-of-field (DoF) of Bessel beam can be controlled. And the large volumetric image is obtained by point scanning. The simulation experiments on blood vessels was carried out to demonstrate the feasibility of the simulation plat-form. This simulation work can be used as an auxiliary tool for the research of Bessel-beam photoacoustic microscopy.      
