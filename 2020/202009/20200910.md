# ArXiv eess --Thu, 10 Sep 2020
### 1.not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget  [ :arrow_down: ](https://arxiv.org/pdf/2009.04433.pdf)
>  BigGAN is the state-of-the-art in high-resolution image generation, successfully leveraging advancements in scalable computing and theoretical understanding of generative adversarial methods to set new records in conditional image generation. A major part of BigGAN's success is due to its use of large mini-batch sizes during training in high dimensions. While effective, this technique requires an incredible amount of compute resources and/or time (256 TPU-v3 Cores), putting the model out of reach for the larger research community. In this paper, we present not-so-BigGAN, a simple and scalable framework for training deep generative models on high-dimensional natural images. Instead of modelling the image in pixel space like in BigGAN, not-so-BigGAN uses wavelet transformations to bypass the curse of dimensionality, reducing the overall compute requirement significantly. Through extensive empirical evaluation, we demonstrate that for a fixed compute budget, not-so-BigGAN converges several times faster than BigGAN, reaching competitive image quality with an order of magnitude lower compute budget (4 Telsa-V100 GPUs).      
### 2.Smooth Converse Lyapunov-Barrier Theorems for Asymptotic Stability with Safety Constraints and Reach-Avoid-Stay Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2009.04432.pdf)
>  Stability and safety are two important aspects in safety-critical control of dynamical systems. It has been a well established fact in control theory that stability properties can be characterized by Lyapunov functions. Reachability properties can also be naturally captured by Lyapunov functions for finite-time stability. Motivated by safety-critical control applications, such as in autonomous systems and robotics, there has been a recent surge of interests in characterizing safety properties using barrier functions. Lyapunov and barrier functions conditions, however, are sometimes viewed as competing objectives. In this paper, we provide a unified theoretical treatment of Lyapunov and barrier functions in terms of converse theorems for stability properties with safety guarantees and reach-avoid-stay type specifications. We show that if a system (modeled as a perturbed dynamical system) possesses a stability with safety property, then there exists a smooth Lyapunov function to certify such a property. This Lyapunov function is shown to be defined on the entire set of initial conditions from which solutions satisfy this property. A similar but slightly weaker statement is made for reach-avoid-stay specifications. We show by a simple example that the latter statement cannot be strengthened without additional assumptions.      
### 3.Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.04420.pdf)
>  Due to the lack of standardized 3D cephalometric analytic methodology, 2D cephalograms synthesized from 3D cone-beam computed tomography (CBCT) volumes are widely used for cephalometric analysis in dental CBCT systems. However, compared with conventional X-ray film based cephalograms, such synthetic cephalograms lack image contrast and resolution. In addition, the radiation dose during the scan for 3D reconstruction causes potential health risks. In this work, we propose a sigmoid-based intensity transform that uses the nonlinear optical property of X-ray films to increase image contrast of synthetic cephalograms. To improve image resolution, super resolution deep learning techniques are investigated. For low dose purpose, the pixel-to-pixel generative adversarial network (pix2pixGAN) is proposed for 2D cephalogram synthesis directly from two CBCT projections. For landmark detection in the synthetic cephalograms, an efficient automatic landmark detection method using the combination of LeNet-5 and ResNet50 is proposed. Our experiments demonstrate the efficacy of pix2pixGAN in 2D cephalogram synthesis, achieving an average peak signal-to-noise ratio (PSNR) value of 33.8 with reference to the cephalograms synthesized from 3D CBCT volumes. Pix2pixGAN also achieves the best performance in super resolution, achieving an average PSNR value of 32.5 without the introduction of checkerboard or jagging artifacts. Our proposed automatic landmark detection method achieves 86.7% successful detection rate in the 2 mm clinical acceptable range on the ISBI Test1 data, which is comparable to the state-of-the-art methods. The method trained on conventional cephalograms can be directly applied to landmark detection in the synthetic cephalograms, achieving 93.0% and 80.7% successful detection rate in 4 mm precision range for synthetic cephalograms from 3D volumes and 2D projections respectively.      
### 4.Design and Fabrication of Novel Digital Transcranial Electrical Stimulator for Medical and Psychiatry Applications  [ :arrow_down: ](https://arxiv.org/pdf/2009.04411.pdf)
>  In this article, we design a novel Transcranial Electrical Stimulator for medical applications, which is very cheap and can produce the desired signals very accurately. Our fabricated stimulator generates all current signals related to Transcranial Electrical Stimulation (TES) methods, i.e. Transcranial Direct Current Stimulation (tDCS), Transcranial Pulsed Current Stimulation (tPCS), Cranial Electrotherapy Stimulation (CES), and Microcurrent Electrical Therapy (MET). The proposed device has been constructed of an advanced digital controller which makes it tunable. One of the major advantages of the device is its ability to generate Burst pulses.      
### 5.A Lightweight CNN Model for Detecting Respiratory Diseases from Lung Auscultation Sounds using EMD-CWT-based Hybrid Scalogram  [ :arrow_down: ](https://arxiv.org/pdf/2009.04402.pdf)
>  Listening to lung sounds through auscultation is vital in examining the respiratory system for abnormalities. Automated analysis of lung auscultation sounds can be beneficial to the health systems in low-resource settings where there is a lack of skilled physicians. In this work, we propose a lightweight convolutional neural network (CNN) architecture to classify respiratory diseases using hybrid scalogram-based features of lung sounds. The hybrid scalogram features utilize the empirical mode decomposition (EMD) and continuous wavelet transform (CWT). The proposed scheme's performance is studied using a patient independent train-validation set from the publicly available ICBHI 2017 lung sound dataset. Employing the proposed framework, weighted accuracy scores of 99.20% for ternary chronic classification and 99.05% for six-class pathological classification are achieved, which outperform well-known and much larger VGG16 in terms of accuracy by 0.52% and 1.77% respectively. The proposed CNN model also outperforms other contemporary lightweight models while being computationally comparable.      
### 6.Is Third-Party Provided Travel Time Helpful to Estimate Freeway Performance Measures?  [ :arrow_down: ](https://arxiv.org/pdf/2009.04401.pdf)
>  Transportation agencies monitor freeway performance using various measures such as VMT (Vehicle Miles Traveled), VHD (Vehicle Hours of Delay), and VHT (Vehicle Hours Traveled). Public transportation agencies typically rely on point detector data to estimate these freeway performance measures. Point detectors, such as inductive loops cannot capture the travel time for a corridor, which can lead to inaccurate performance measure estimation. This research develops a hybrid method, which estimates of freeway performance measures using a mix of probe data from third-parties and data from traditional point detectors. Using a simulated I-210 model, the overall framework using multiple data sources is evaluated, and compared with the traditional point detector-based estimation method. In the traditional method, point speeds are estimated with the flow and occupancy values using the g-factors. Data from 5% of the total vehicle are used to generate the third-party vendor provided travel time data. The analysis is conducted for multiple scenarios, including peak and off-peak periods. Findings suggest that fusing data from both third-party vendors and point detectors can help estimate performance measures better, compared to the traditional method, in scenarios that have noticeable traffic demand on freeways.      
### 7.Data Augmentation for Electrocardiogram Classification with Deep Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.04398.pdf)
>  Electrocardiogram (ECG) is the most crucial monitoring modality to diagnose cardiovascular events. Precise and automatic detection of abnormal ECG patterns is beneficial to both physicians and patients. In the automatic detection of abnormal ECG patterns, deep neural networks (DNNs) have shown significant achievements. However, DNNs require large amount of labeled data, which are often expensive to obtain. On the other hand, recent research have shown by randomly combining data augmentations can improve image classification accuracy. Thus, in this work we explore data augmentation suitable for ECG data and propose ECG Augment. We show by introducing ECG Augment, we can improve classification of atrial fibrillation with single lead ECG data, without changing an architecture of DNN.      
### 8.Learning Anatomical Segmentations for Tractography from Diffusion MRI  [ :arrow_down: ](https://arxiv.org/pdf/2009.04392.pdf)
>  Deep learning approaches for diffusion MRI have so far focused primarily on voxel-based segmentation of lesions or white-matter fiber tracts. A drawback of representing tracts as volumetric labels, rather than sets of streamlines, is that it precludes point-wise analyses of microstructural or geometric features along a tract. Traditional tractography pipelines, which do allow such analyses, can benefit from detailed whole-brain segmentations to guide tract reconstruction. Here, we introduce fast, deep learning-based segmentation of 170 anatomical regions directly on diffusion-weighted MR images, removing the dependency of conventional segmentation methods on T 1-weighted images and slow pre-processing pipelines. Working natively in diffusion space avoids non-linear distortions and registration errors across modalities, as well as interpolation artifacts. We demonstrate consistent segmentation results between 0 .70 and 0 .87 Dice depending on the tissue type. We investigate various combinations of diffusion-derived inputs and show generalization across different numbers of gradient directions. Finally, integrating our approach to provide anatomical priors for tractography pipelines, such as TRACULA, removes hours of pre-processing time and permits processing even in the absence of high-quality T 1-weighted scans, without degrading the quality of the resulting tract estimates.      
### 9.A Random Antenna Subset Selection Jamming Method against Multistatic Radar System  [ :arrow_down: ](https://arxiv.org/pdf/2009.04364.pdf)
>  Multistatic radar system (MSRS) is considered an effective scheme to suppress mainlobe jamming, since it has higher spatial resolution enabling jamming cancellation from spatial domain. To develop electronic countermeasures against MSRS, a random array subset selection (RASS)jamming method is proposed in this paper. In the RASS jammer, elements of the array antenna are activated randomly, leading to stable mainlobe and random sidelobes, different from the traditional jammer that applies the complete antenna array enjoying constant mainlobe and sidelobes. We study the covariance matrix of jamming signals received by radars, and derive its rank, revealing that the covariance matrix is of full rank. We also calculate the output jamming to signal and noise ratio (JSNR) after the subspace-based jamming suppression methods used in MSRS under the proposed jamming method, which demonstrates that the full rank property invalidates such suppression methods. Numerical results verify our analytical deduction and exhibit the improved countermeasure performance of our proposed RASS jamming method compared to the traditional one.      
### 10.Reinforcement Learning in Non-Stationary Discrete-Time Linear-Quadratic Mean-Field Games  [ :arrow_down: ](https://arxiv.org/pdf/2009.04350.pdf)
>  In this paper, we study large population multi-agent reinforcement learning (RL) in the context of discrete-time linear-quadratic mean-field games (LQ-MFGs). Our setting differs from most existing work on RL for MFGs, in that we consider a non-stationary MFG over an infinite horizon. We propose an actor-critic algorithm to iteratively compute the mean-field equilibrium (MFE) of the LQ-MFG. There are two primary challenges: i) the non-stationarity of the MFG induces a linear-quadratic tracking problem, which requires solving a backwards-in-time (non-causal) equation that cannot be solved by standard (causal) RL algorithms; ii) Most RL algorithms assume that the states are sampled from the stationary distribution of a Markov chain (MC), that is, the chain is already mixed, an assumption that is not satisfied for real data sources. We first identify that the mean-field trajectory follows linear dynamics, allowing the problem to be reformulated as a linear quadratic Gaussian problem. Under this reformulation, we propose an actor-critic algorithm that allows samples to be drawn from an unmixed MC. Finite-sample convergence guarantees for the algorithm are then provided. To characterize the performance of our algorithm in multi-agent RL, we have developed an error bound with respect to the Nash equilibrium of the finite-population game.      
### 11.VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.04323.pdf)
>  We introduce VoiceFilter-Lite, a single-channel source separation model that runs on the device to preserve only the speech signals from a target user, as part of a streaming speech recognition system. Delivering such a model presents numerous challenges: It should improve the performance when the input signal consists of overlapped speech, and must not hurt the speech recognition performance under all other acoustic conditions. Besides, this model must be tiny, fast, and perform inference in a streaming fashion, in order to have minimal impact on CPU, memory, battery and latency. We propose novel techniques to meet these multi-faceted requirements, including using a new asymmetric loss, and adopting adaptive runtime suppression strength. We also show that such a model can be quantized as a 8-bit integer model and run in realtime.      
### 12.An Infrared Communication System based on Handstand Pendulum  [ :arrow_down: ](https://arxiv.org/pdf/2009.04293.pdf)
>  This paper mainly introduces an infrared optical communication system based on stable and handstand pendulum. This system adopts the method of loading the infrared light emitting end on an handstand pendulum to realize the stability and controllability of the infrared light transmission light path. In this system, 940nm infrared light is mainly used for audio signal transmission, and an handstand pendulum based on PID is used to control the angle and stability of infrared light emission. Experimental results show that the system can effectively ensure the stability of the transmission optical path and is suitable for accurate and stable signal transmission in bumpy environments.      
### 13.NTGAN: Learning Blind Image Denoising without Clean Reference  [ :arrow_down: ](https://arxiv.org/pdf/2009.04286.pdf)
>  Recent studies on learning-based image denoising have achieved promising performance on various noise reduction tasks. Most of these deep denoisers are trained either under the supervision of clean references, or unsupervised on synthetic noise. The assumption with the synthetic noise leads to poor generalization when facing real photographs. To address this issue, we propose a novel deep unsupervised image-denoising method by regarding the noise reduction task as a special case of the noise transference task. Learning noise transference enables the network to acquire the denoising ability by only observing the corrupted samples. The results on real-world denoising benchmarks demonstrate that our proposed method achieves state-of-the-art performance on removing realistic noises, making it a potential solution to practical noise reduction problems.      
### 14.Cooperative Formation of Autonomous Vehicles in Mixed Traffic Flow: Beyond Platooning  [ :arrow_down: ](https://arxiv.org/pdf/2009.04254.pdf)
>  Cooperative formation and control of autonomous vehicles (AVs) promise increased efficiency and safety on public roads. In mixed traffic flow consisting of AVs and human-driven vehicles (HDVs), the prevailing platooning of multiple AVs is not the only choice for cooperative formation. In this paper, we investigate how different formations of AVs impact traffic performance from a set-function optimization perspective. We first reveal a stability invariance property and a diminishing improvement property when AVs adopt typical Adaptive Cruise Control (ACC) strategies. Then, we re-design the control strategies of AVs in different formations and investigate the optimal formation of multiple AVs using set-function optimization. Two predominant optimal formations, i.e., uniform distribution and platoon formation, emerges from extensive numerical experiments. Interestingly, platooning might have the least potential to improve traffic performance when HDVs have poor string stability behavior. These results suggest more opportunities for cooperative formation of AVs, beyond platooning, in practical mixed traffic flow.      
### 15.Structured Equilibria for Dynamic Games with Asymmetric Information and Dependent Types  [ :arrow_down: ](https://arxiv.org/pdf/2009.04253.pdf)
>  We consider a dynamic game with asymmetric information where each player observes privately a noisy version of a (hidden) state of the world V, resulting in dependent private observations. We study structured perfect Bayesian equilibria that use private beliefs in their strategies as sufficient statistics for summarizing their observation history. The main difficulty in finding the appropriate sufficient statistic (state) for the structured strategies arises from the fact that players need to construct (private) beliefs on other players' private beliefs on V, which in turn would imply that an infinite hierarchy of beliefs on beliefs needs to be constructed, rendering the problem unsolvable. We show that this is not the case: each player's belief on other players' beliefs on V can be characterized by her own belief on V and some appropriately defined public belief. We then specialize this setting to the case of a Linear Quadratic Gaussian (LQG) non-zero-sum game and we characterize linear structured PBE that can be found through a backward/forward algorithm akin to dynamic programming for the standard LQG control problem. Unlike the standard LQG problem, however, some of the required quantities for the Kalman filter are observation-dependent and thus cannot be evaluated off-line through a forward recursion.      
### 16.Probabilistic dipole inversion for adaptive quantitativesusceptibility mapping  [ :arrow_down: ](https://arxiv.org/pdf/2009.04251.pdf)
>  A learning-based posterior distribution estimation method, Probabilistic Dipole Inversion (PDI), is proposed to solve quantitative susceptibility mapping (QSM) inverse problem in MRI with uncertainty estimation. In PDI, a deep convolutional neural network (CNN) is used to represent the multivariate Gaussian distribution as the approximated posterior distribution of susceptibility given the input measured field. Such CNN is firstly trained on healthy subjects via posterior density estimation, where the training dataset contains samples from the true posterior distribution. Domain adaptations are then deployed on patients' datasets with new pathologies not included in pre-training, where PDI updates the pre-trained CNN's weights in an unsupervised fashion by minimizing the Kullback-Leibler divergence between the approximate posterior distribution represented by CNN and the true posterior distribution given the likelihood distribution from a known physical model and pre-defined prior distribution. Based on our experiments, PDI provides additional uncertainty estimation compared to the conventional MAP approach, meanwhile addressing the potential issue of the pre-trained CNN when test data deviates from training.      
### 17.Performance Analysis of Model-Free Adaptive Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.04248.pdf)
>  Current works on model-free adaptive control (MFAC) is meaningful. However, it has not been proposed and analyzed in a right way.      
### 18.Hyperspectral Image Super-Resolution via Deep Prior Regularization with Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2009.04237.pdf)
>  Hyperspectral image (HSI) super-resolution is commonly used to overcome the hardware limitations of existing hyperspectral imaging systems on spatial resolution. It fuses a low-resolution (LR) HSI and a high-resolution (HR) conventional image of the same scene to obtain an HR HSI. In this work, we propose a method that integrates the physical model and deep prior information. A novel, yet effective two-stream fusion network is designed and its output serves as a regularization term of the fusion problem. Then, we optimize the fusion problem by solving a Sylvester equation, and simultaneously estimate the regularization parameter to automatically adjust contribution of the physical model and learnt prior to reconstruct the final HR HSI. Experimental results on two public datasets demonstrate the superiority of the proposed method over several state-of-the-art methods on both quantitative and qualitative comparisons.      
### 19.Anonymization of labeled TOF-MRA images for brain vessel segmentation using generative adversarial networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.04227.pdf)
>  Anonymization and data sharing are crucial for privacy protection and acquisition of large datasets for medical image analysis. This is a big challenge, especially for neuroimaging. Here, the brain's unique structure allows for re-identification and thus requires non-conventional anonymization. Generative adversarial networks (GANs) have the potential to provide anonymous images while preserving predictive properties. Analyzing brain vessel segmentation as a use case, we trained 3 GANs on time-of-flight (TOF) magnetic resonance angiography (MRA) patches for image-label generation: 1) Deep convolutional GAN, 2) Wasserstein-GAN with gradient penalty (WGAN-GP) and 3) WGAN-GP with spectral normalization (WGAN-GP-SN). The generated image-labels from each GAN were used to train a U-net for segmentation and tested on real data. Moreover, we applied our synthetic patches using transfer learning on a second dataset. For an increasing number of up to 15 patients we evaluated the model performance on real data with and without pre-training. The performance for all models was assessed by the Dice Similarity Coefficient (DSC) and the 95th percentile of the Hausdorff Distance (95HD). Comparing the 3 GANs, the U-net trained on synthetic data generated by the WGAN-GP-SN showed the highest performance to predict vessels (DSC/95HD 0.82/28.97) benchmarked by the U-net trained on real data (0.89/26.61). The transfer learning approach showed superior performance for the same GAN compared to no pre-training, especially for one patient only (0.91/25.68 vs. 0.85/27.36). In this work, synthetic image-label pairs retained generalizable information and showed good performance for vessel segmentation. Besides, we showed that synthetic patches can be used in a transfer learning approach with independent data. This paves the way to overcome the challenges of scarce data and anonymization in medical imaging.      
### 20.AoI Minimization in Status Update Control with Energy Harvesting Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2009.04224.pdf)
>  Information freshness is crucial for time-critical IoT applications, e.g., environment monitoring and control systems. We consider an IoT-based status update system with multiple users, multiple energy harvesting sensors, and a wireless edge node. The users are interested in time-sensitive information about physical quantities, each measured by a sensor. Users send requests to the edge node where a cache contains the most recently received measurements from each sensor. To serve a request, the edge node either commands the sensor to send a status update or retrieves the aged measurement from the cache. We aim at finding the best action of the edge node to minimize the age of information of the served measurements. We model this problem as a Markov decision process and develop reinforcement learning (RL) algorithms: a model-based value iteration method and a model-free Q-learning method. We also propose a Q-learning method for the realistic case where the edge node is informed about the sensors' battery levels only via the status updates. Furthermore, properties of an optimal policy are analytically characterized. Simulation results show that an optimal policy is a threshold-based policy and that the proposed RL methods significantly reduce the average cost as compared to several baseline methods.      
### 21.Analysis of the least sum-of-minimums estimator for switched systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.04213.pdf)
>  This paper considers a particular parameter estimator for switched systems and analyzes its properties. The estimator in question is defined as the map from the data set to the solution set of an optimization problem where the to-be-optimized cost function is a sum of pointwise infima over a finite set of sub-functions. This is a hard nonconvex problem. The paper studies some fundamental properties of this problem such as uniqueness of the solution or boundedness of the estimation error regardless of computational considerations. The interest of the analysis is to lay out the main influential properties of the data on the performance of this (ideal) estimator.      
### 22.Analysis of Hand-Crafted and Automatic-Learned Features for Glaucoma Detection Through Raw Circmpapillary OCT Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.04190.pdf)
>  Taking into account that glaucoma is the leading cause of blindness worldwide, we propose in this paper three different learning methodologies for glaucoma detection in order to elucidate that traditional machine-learning techniques could outperform deep-learning algorithms, especially when the image data set is small. The experiments were performed on a private database composed of 194 glaucomatous and 198 normal B-scans diagnosed by expert ophthalmologists. As a novelty, we only considered raw circumpapillary OCT images to build the predictive models, without using other expensive tests such as visual field and intraocular pressure measures. The results ratify that the proposed hand-driven learning model, based on novel descriptors, outperforms the automatic learning. Additionally, the hybrid approach consisting of a combination of both strategies reports the best performance, with an area under the ROC curve of 0.85 and an accuracy of 0.82 during the prediction stage.      
### 23.Small-floating Target Detection in Sea Clutter via Visual Feature Classifying in the Time-Doppler Spectra  [ :arrow_down: ](https://arxiv.org/pdf/2009.04185.pdf)
>  It is challenging to detect small-floating object in the sea clutter for a surface radar. In this paper, we have observed that the backscatters from the target brake the continuity of the underlying motion of the sea surface in the time-Doppler spectra (TDS) images. Following this visual clue, we exploit the local binary pattern (LBP) to measure the variations of texture in the TDS images. It is shown that the radar returns containing target and those only having clutter are separable in the feature space of LBP. An unsupervised one-class support vector machine (SVM) is then utilized to detect the deviation of the LBP histogram of the clutter. The outiler of the detector is classified as the target. In the real-life IPIX radar data sets, our visual feature based detector shows favorable detection rate compared to other three existing approaches.      
### 24.Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.04172.pdf)
>  This paper addresses the extraction of multiple F0 values from polyphonic and a cappella vocal performances using convolutional neural networks (CNNs). We address the major challenges of ensemble singing, i.e., all melodic sources are vocals and singers sing in harmony. We build upon an existing architecture to produce a pitch salience function of the input signal, where the harmonic constant-Q transform (HCQT) and its associated phase differentials are used as an input representation. The pitch salience function is subsequently thresholded to obtain a multiple F0 estimation output. For training, we build a dataset that comprises several multi-track datasets of vocal quartets with F0 annotations. This work proposes and evaluates a set of CNNs for this task in diverse scenarios and data configurations, including recordings with additional reverb. Our models outperform a state-of-the-art method intended for the same music genre when evaluated with an increased F0 resolution, as well as a general-purpose method for multi-F0 estimation. We conclude with a discussion on future research directions.      
### 25.Revealing Lung Affections from CTs. A Comparative Analysis of Various Deep Learning Approaches for Dealing with Volumetric Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.04160.pdf)
>  The paper presents and comparatively analyses several deep learning approaches to automatically detect tuberculosis related lesions in lung CTs, in the context of the ImageClef 2020 Tuberculosis task. Three classes of methods, different with respect to the way the volumetric data is given as input to neural network-based classifiers are discussed and evaluated. All these come with a rich experimental analysis comprising a variety of neural network architectures, various segmentation algorithms and data augmentation schemes. The reported work belongs to the SenticLab.UAIC team, which obtained the best results in the competition.      
### 26.Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth Image Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2009.04127.pdf)
>  Low-bandwidth communication, such as underwater acoustic communication, is limited by best-case data rates of 30--50 kbit/s. This renders such channels unusable or inefficient at best for single image, video, or other bandwidth-demanding sensor-data transmission. To combat data-transmission bottlenecks, we consider practical use-cases within the maritime domain and investigate the prospect of Single Image Super-Resolution methodologies. This is investigated on a large, diverse dataset obtained during years of trawl fishing where cameras have been placed in the fishing nets. We propose down-sampling images to a low-resolution low-size version of about 1 kB that satisfies underwater acoustic bandwidth requirements for even several frames per second. A neural network is then trained to perform up-sampling, trying to reconstruct the original image. We aim to investigate the quality of reconstructed images and prospects for such methods in practical use-cases in general. Our focus in this work is solely on learning to reconstruct the high-resolution images on "real-world" data. We show that our method achieves better perceptual quality and superior reconstruction than generic bicubic up-sampling and motivates further work in this area for underwater applications.      
### 27.Multi-modal Attention for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.04107.pdf)
>  Emotion represents an essential aspect of human speech that is manifested in speech prosody. Speech, visual, and textual cues are complementary in human communication. In this paper, we study a hybrid fusion method, referred to as multi-modal attention network (MMAN) to make use of visual and textual cues in speech emotion recognition. We propose a novel multi-modal attention mechanism, cLSTM-MMA, which facilitates the attention across three modalities and selectively fuse the information. cLSTM-MMA is fused with other uni-modal sub-networks in the late fusion. The experiments show that speech emotion recognition benefits significantly from visual and textual cues, and the proposed cLSTM-MMA alone is as competitive as other fusion methods in terms of accuracy, but with a much more compact network structure. The proposed hybrid network MMAN achieves state-of-the-art performance on IEMOCAP database for emotion recognition.      
### 28.Robustness of networked systems to unintended interactions with application to engineered genetic circuits  [ :arrow_down: ](https://arxiv.org/pdf/2009.04098.pdf)
>  A networked dynamical system is composed of subsystems interconnected through prescribed interactions. In many engineering applications, however, one subsystem can also affect others through "unintended" interactions that can significantly hamper the intended network's behavior. Although unintended interactions can be modeled as disturbance inputs to the subsystems, these disturbances depend on the network's states. As a consequence, a disturbance attenuation property of each isolated subsystem is, alone, insufficient to ensure that the network behavior is robust to unintended interactions. In this paper, we provide sufficient conditions on subsystem dynamics and interaction maps, such that the network's behavior is robust to unintended interactions. These conditions require that each subsystem attenuates constant external disturbances, is monotone or "near-monotone", the unintended interaction map is monotone, and the prescribed interaction map does not contain feedback loops. We employ this result to guide the design of resource-limited genetic circuits. More generally, our result provide conditions under which robustness of constituent subsystems is sufficient to guarantee robustness of the network to unintended interactions.      
### 29.GNSS Interference Monitoring from Low Earth Orbit  [ :arrow_down: ](https://arxiv.org/pdf/2009.04093.pdf)
>  Observation of terrestrial GNSS interference (jamming and spoofing) from low-earth orbit (LEO) is a uniquely effective technique for characterizing the scope, strength, and structure of interference and for estimating transmitter locations. Such details are useful for situational awareness, interference deterrence, and for developing interference-hardened GNSS receivers. This paper explores the performance of LEO interference monitoring and presents the results of a three-year study of global interference, with emphasis on a particularly powerful interference source active in Syria since 2017.      
### 30.Method for classifying a noisy Raman spectrum based on a wavelet transform and a deep neural network  [ :arrow_down: ](https://arxiv.org/pdf/2009.04078.pdf)
>  This paper proposes a new framework based on a wavelet transform and deep neural network for identifying noisy Raman spectrum since, in practice, it is relatively difficult to classify the spectrum under baseline noise and additive white Gaussian noise environments. The framework consists of two main engines. Wavelet transform is proposed as the framework front-end for transforming 1-D noise Raman spectrum to two-dimensional data. This two-dimensional data will be fed to the framework back-end which is a classifier. The optimum classifier is chosen by implementing several traditional machine learning (ML) and deep learning (DL) algorithms, and then we investigated their classification accuracy and robustness performances. The four MLs we choose included a Naive Bayes (NB), a Support Vector Machine (SVM), a Random Forest (RF) and a K-Nearest Neighbor (KNN) where a deep convolution neural network (DCNN) was chosen for a DL classifier. Noise-free, Gaussian noise, baseline noise, and mixed-noise Raman spectrums were applied to train and validate the ML and DCNN models. The optimum back-end classifier was obtained by testing the ML and DCNN models with several noisy Raman spectrums (10-30 dB noise power). Based on the simulation, the accuracy of the DCNN classifier is 9% higher than the NB classifier, 3.5% higher than the RF classifier, 1% higher than the KNN classifier, and 0.5% higher than the SVM classifier. In terms of robustness to the mixed noise scenarios, the framework with DCNN back-end showed superior performance than the other ML back-ends. The DCNN back-end achieved 90% accuracy at 3 dB SNR while NB, SVM, RF, and K-NN back-ends required 27 dB, 22 dB, 27 dB, and 23 dB SNR, respectively. In addition, in the low-noise test data set, the F-measure score of the DCNN back-end exceeded 99.1% while the F-measure scores of the other ML engines were below 98.7%.      
### 31.1-Dimensional polynomial neural networks for audio signal related problems  [ :arrow_down: ](https://arxiv.org/pdf/2009.04077.pdf)
>  In addition to being extremely non-linear, modern problems require millions if not billions of parameters to solve or at least to get a good approximation of the solution, and neural networks are known to assimilate that complexity by deepening and widening their topology in order to increase the level of non-linearity needed for a better approximation. However, compact topologies are always preferred to deeper ones as they offer the advantage of using less computational units and less parameters. This compacity comes at the price of reduced non-linearity and thus, of limited solution search space. We propose the 1-Dimensional Polynomial Neural Network (1DPNN) model that uses automatic polynomial kernel estimation for 1-Dimensional Convolutional Neural Networks (1DCNNs) and that introduces a high degree of non-linearity from the first layer which can compensate the need for deep and/or wide topologies. We show that this non-linearity introduces more computational complexity but enables the model to yield better results than a regular 1DCNN that has the same number of training parameters on various classification and regression problems related to audio signals. The experiments were conducted on three publicly available datasets and demonstrate that the proposed model can achieve a much faster convergence than a 1DCNN on the tackled regression problems.      
### 32.Noise Reduction Technique for Raman Spectrum using Deep Learning Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.04067.pdf)
>  In a normal indoor environment, Raman spectrum encounters noise often conceal spectrum peak, leading to difficulty in spectrum interpretation. This paper proposes deep learning (DL) based noise reduction technique for Raman spectroscopy. The proposed DL network is developed with several training and test sets of noisy Raman spectrum. The proposed technique is applied to denoise and compare the performance with different wavelet noise reduction methods. Output signal-to-noise ratio (SNR), root-mean-square error (RMSE) and mean absolute percentage error (MAPE) are the performance evaluation index. It is shown that output SNR of the proposed noise reduction technology is 10.24 dB greater than that of the wavelet noise reduction method while the RMSE and the MAPE are 292.63 and 10.09, which are much better than the proposed technique.      
### 33.HSMF-Net: Semantic Viewport Prediction for Immersive Telepresence and On-Demand 360-degree Video  [ :arrow_down: ](https://arxiv.org/pdf/2009.04015.pdf)
>  The acceptance of immersive telepresence systems is impeded by the latency that is present when mediating the realistic feeling of presence in a remote environment to a local human user. A disagreement between the user's ego-motion and the visual response provokes the emergence of motion sickness. Viewport or head motion (HM) prediction techniques play a key role in compensating the noticeable delay between the user and the remote site. We present a deep learning-based viewport prediction paradigm that fuses past HM trajectories with scene semantics in a late-fusion manner. Real HM profiles are used to evaluate the proposed approach. A mean compensation rate as high as 99.99% is obtained, clearly outperforming the state-of-the-art. An on-demand 360-degree video streaming framework is presented to prove its general validity. The proposed approach increases the perceived video quality while requiring a significantly lower transmission rate.      
### 34.Learning joint segmentation of tissues and brain lesions from task-specific hetero-modal domain-shifted datasets  [ :arrow_down: ](https://arxiv.org/pdf/2009.04009.pdf)
>  Brain tissue segmentation from multimodal MRI is a key building block of many neuroimaging analysis pipelines. Established tissue segmentation approaches have, however, not been developed to cope with large anatomical changes resulting from pathology, such as white matter lesions or tumours, and often fail in these cases. In the meantime, with the advent of deep neural networks (DNNs), segmentation of brain lesions has matured significantly. However, few existing approaches allow for the joint segmentation of normal tissue and brain lesions. Developing a DNN for such a joint task is currently hampered by the fact that annotated datasets typically address only one specific task and rely on task-specific imaging protocols including a task-specific set of imaging modalities. In this work, we propose a novel approach to build a joint tissue and lesion segmentation model from aggregated task-specific hetero-modal domain-shifted and partially-annotated datasets. Starting from a variational formulation of the joint problem, we show how the expected risk can be decomposed and optimised empirically. We exploit an upper bound of the risk to deal with heterogeneous imaging modalities across datasets. To deal with potential domain shift, we integrated and tested three conventional techniques based on data augmentation, adversarial learning and pseudo-healthy generation. For each individual task, our joint approach reaches comparable performance to task-specific and fully-supervised models. The proposed framework is assessed on two different types of brain lesions: White matter lesions and gliomas. In the latter case, lacking a joint ground-truth for quantitative assessment purposes, we propose and use a novel clinically-relevant qualitative assessment methodology.      
### 35.Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks On Deep COVID-19 Models  [ :arrow_down: ](https://arxiv.org/pdf/2009.04004.pdf)
>  Early identification of COVID-19 using a deep model trained on Chest X-Ray and CT images has gained considerable attention from researchers to speed up the process of identification of active COVID-19 cases. These deep models act as an aid to hospitals that suffer from the unavailability of specialists or radiologists, specifically in remote areas. Various deep models have been proposed to detect the COVID-19 cases, but few works have been performed to prevent the deep models against adversarial attacks capable of fooling the deep model by using a small perturbation in image pixels. This paper presents an evaluation of the performance of deep COVID-19 models against adversarial attacks. Also, it proposes an efficient yet effective Fuzzy Unique Image Transformation (FUIT) technique that downsamples the image pixels into an interval. The images obtained after the FUIT transformation are further utilized for training the secure deep model that preserves high accuracy of the diagnosis of COVID-19 cases and provides reliable defense against the adversarial attacks. The experiments and results show the proposed model prevents the deep model against the six adversarial attacks and maintains high accuracy to classify the COVID-19 cases from the Chest X-Ray image and CT image Datasets. The results also recommend that a careful inspection is required before practically applying the deep models to diagnose the COVID-19 cases.      
### 36.When Deep Learning Meets Digital Image Correlation  [ :arrow_down: ](https://arxiv.org/pdf/2009.03993.pdf)
>  Convolutional Neural Networks (CNNs) constitute a class of Deep Learning models which have been used in the recent past to resolve many problems in computer vision, in particular optical flow estimation. Measuring displacement and strain fields can be regarded as a particular case of this problem. However, it seems that CNNs have never been used so far to perform such measurements. This work is aimed at implementing a CNN able to retrieve displacement and strain fields from pairs of reference and deformed images of a flat speckled surface, as Digital Image Correlation (DIC) does. This paper explains how a CNN called StrainNet can be developed to reach this goal, and how specific ground truth datasets are elaborated to train this CNN. The main result is that StrainNet successfully performs such measurements, and that it achieves competing results in terms of metrological performance and computing time. The conclusion is that CNNs like StrainNet offer a viable alternative to DIC, especially for real-time applications.      
### 37.Distributed Economic Model Predictive Control -- Addressing Non-convexity Using Social Hierarchies  [ :arrow_down: ](https://arxiv.org/pdf/2009.03938.pdf)
>  This paper introduces a novel concept for addressing non-convexity in the cost functions of distributed economic model predictive control (DEMPC) systems. Specifically, the proposed algorithm enables agents to self-organize into a hierarchy which determines the order in which control decisions are made. This concept is based on the formation of social hierarchies in nature. An additional feature of the algorithm is that it does not require stationary set-points that are known a priori. Rather, agents negotiate these targets in a truly distributed and scalable manner. Upon providing a detailed description of the algorithm, guarantees of convergence, recursive feasibility, and bounded closed-loop stability are also provided. Finally, the proposed algorithm is compared against a basic parallel distributed economic model predictive controller using an academic numerical example.      
### 38.Learning-Based Distributionally Robust Model Predictive Control of Markovian Switching Systems with Guaranteed Stability and Recursive Feasibility  [ :arrow_down: ](https://arxiv.org/pdf/2009.04422.pdf)
>  We present a data-driven model predictive control scheme for chance-constrained Markovian switching systems with unknown switching probabilities. Using samples of the underlying Markov chain, ambiguity sets of transition probabilities are estimated which include the true conditional probability distributions with high probability. These sets are updated online and used to formulate a time-varying, risk-averse optimal control problem. We prove recursive feasibility of the resulting MPC scheme and show that the original chance constraints remain satisfied at every time step. Furthermore, we show that under sufficient decrease of the confidence levels, the resulting MPC scheme renders the closed-loop system mean-square stable with respect to the true-but-unknown distributions, while remaining less conservative than a fully robust approach.      
### 39.Model Fusion to Enhance the Clinical Acceptability of Long-Term Glucose Predictions  [ :arrow_down: ](https://arxiv.org/pdf/2009.04410.pdf)
>  This paper presents the Derivatives Combination Predictor (DCP), a novel model fusion algorithm for making long-term glucose predictions for diabetic people. First, using the history of glucose predictions made by several models, the future glucose variation at a given horizon is predicted. Then, by accumulating the past predicted variations starting from a known glucose value, the fused glucose prediction is computed. A new loss function is introduced to make the DCP model learn to react faster to changes in glucose variations. <br>The algorithm has been tested on 10 \textit{in-silico} type-1 diabetic children from the T1DMS software. Three initial predictors have been used: a Gaussian process regressor, a feed-forward neural network and an extreme learning machine model. The DCP and two other fusion algorithms have been evaluated at a prediction horizon of 120 minutes with the root-mean-squared error of the prediction, the root-mean-squared error of the predicted variation, and the continuous glucose-error grid analysis. <br>By making a successful trade-off between prediction accuracy and predicted-variation accuracy, the DCP, alongside with its specifically designed loss function, improves the clinical acceptability of the predictions, and therefore the safety of the model for diabetic people.      
### 40.Study of Short-Term Personalized Glucose Predictive Models on Type-1 Diabetic Children  [ :arrow_down: ](https://arxiv.org/pdf/2009.04409.pdf)
>  Research in diabetes, especially when it comes to building data-driven models to forecast future glucose values, is hindered by the sensitive nature of the data. Because researchers do not share the same data between studies, progress is hard to assess. This paper aims at comparing the most promising algorithms in the field, namely Feedforward Neural Networks (FFNN), Long Short-Term Memory (LSTM) Recurrent Neural Networks, Extreme Learning Machines (ELM), Support Vector Regression (SVR) and Gaussian Processes (GP). They are personalized and trained on a population of 10 virtual children from the Type 1 Diabetes Metabolic Simulator software to predict future glucose values at a prediction horizon of 30 minutes. The performances of the models are evaluated using the Root Mean Squared Error (RMSE) and the Continuous Glucose-Error Grid Analysis (CG-EGA). While most of the models end up having low RMSE, the GP model with a Dot-Product kernel (GP-DP), a novel usage in the context of glucose prediction, has the lowest. Despite having good RMSE values, we show that the models do not necessarily exhibit a good clinical acceptability, measured by the CG-EGA. Only the LSTM, SVR and GP-DP models have overall acceptable results, each of them performing best in one of the glycemia regions.      
### 41.Assessment of unsteady flow predictions using hybrid deep learning based reduced order models  [ :arrow_down: ](https://arxiv.org/pdf/2009.04396.pdf)
>  In this paper, we present two deep learning-based hybrid data-driven reduced order models for the prediction of unsteady fluid flows. The first model, termed as POD-RNN, projects the high-fidelity time series data from a finite element Navier-Stokes solver to a low-dimensional subspace via proper orthogonal decomposition (POD). The time-dependent coefficients in the POD subspace are propagated by the recurrent net (closed-loop encoder-decoder updates) and mapped to a high-dimensional state via the mean flow field and POD basis vectors. The second model, referred to as convolution recurrent autoencoder network (CRAN), employs convolutional neural networks (CNN) as layers of linear kernels with nonlinear activations, to extract low-dimensional features from flow field snapshots. The flattened features are advanced using a recurrent (closed-loop manner) net and up-sampled (transpose convoluted) gradually to high-dimensional snapshots. Two benchmark problems of the flow past a cylinder and flow past a side-by-side cylinder are selected as the test problems to assess the efficacy of these models. For the problem of flow past a single cylinder, the performance of both the models is satisfactory, with CRAN being a bit overkill. However, it completely outperforms the POD-RNN model for a more complicated problem of flow past side-by-side cylinders. Owing to the scalability of CRAN, we briefly introduce an observer-corrector method for the calculation of integrated pressure force coefficients on the fluid-solid boundary on a reference grid. This reference grid, typically a structured and uniform grid, is used to interpolate scattered high-dimensional field data as snapshot images. These input images are convenient in training CRAN. This motivates us to further explore the application of CRAN models for the prediction of fluid flows.      
### 42.Automated Model Selection for Time-Series Anomaly Detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.04395.pdf)
>  Time-series anomaly detection is a popular topic in both academia and industrial fields. Many companies need to monitor thousands of temporal signals for their applications and services and require instant feedback and alerts for potential incidents in time. The task is challenging because of the complex characteristics of time-series, which are messy, stochastic, and often without proper labels. This prohibits training supervised models because of lack of labels and a single model hardly fits different time series. In this paper, we propose a solution to address these issues. We present an automated model selection framework to automatically find the most suitable detection model with proper parameters for the incoming data. The model selection layer is extensible as it can be updated without too much effort when a new detector is available to the service. Finally, we incorporate a customized tuning algorithm to flexibly filter anomalies to meet customers' criteria. Experiments on real-world datasets show the effectiveness of our solution.      
### 43.A General Model of Opinion Dynamics with Tunable Sensitivity  [ :arrow_down: ](https://arxiv.org/pdf/2009.04332.pdf)
>  We introduce a general model of continuous-time opinion dynamics for an arbitrary number of agents that communicate over a network and form real-valued opinions about an arbitrary number of options. Drawing inspiration from models in biology, physics, and social psychology, we apply a sigmoidal saturating function to inter-agent and intra-agent exchanges of opinions. The saturating function is the only nonlinearity in the model, yet we prove how it yields rapid and reliable formation of consensus, dissensus, and opinion cascades as a function of just a few parameters. We further show how the network opinion dynamics exhibit both robustness to disturbance and ultrasensitivity to inputs. We design feedback dynamics for system parameters that enable active tuning of implicit thresholds in opinion formation for sensitivity to inputs, robustness to changes in input, opinion cascades, and flexible transitions between consensus and dissensus. The general model can be used for systematic control design in a range of engineering problems including network systems, multi-robot coordination, task allocation, and decision making for spatial navigation. It can also be used for systematic examination of questions in biology and social science ranging from cognitive control and networks in the brain, to resilience in collective animal behavior to changing environmental conditions, to information spreading and political polarization in social networks.      
### 44.A scalable controller synthesis method for the robust control of networked systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.04289.pdf)
>  This manuscript discusses a scalable controller synthesis method for networked systems with a large number of identical subsystems based on the H-infinity control framework. The dynamics of the individual subsystems are described by identical linear time-invariant delay differential equations and the effect of transport and communication delay is explicitly taken into account. The presented method is based on the result that, under a particular assumption on the graph describing the interconnections between the subsystems, the H-infinity norm of the overall system is upper bounded by the robust H-infinity norm of a single subsystem with an additional uncertainty. This work will therefore briefly discuss a recently developed method to compute this last quantity. The resulting controller is then obtained by directly minimizing this upper bound in the controller parameters.      
### 45.DyNODE: Neural Ordinary Differential Equations for Dynamics Modeling in Continuous Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.04278.pdf)
>  We present a novel approach (DyNODE) that captures the underlying dynamics of a system by incorporating control in a neural ordinary differential equation framework. We conduct a systematic evaluation and comparison of our method and standard neural network architectures for dynamics modeling. Our results indicate that a simple DyNODE architecture when combined with an actor-critic reinforcement learning (RL) algorithm that uses model predictions to improve the critic's target values, outperforms canonical neural networks, both in sample efficiency and predictive performance across a diverse range of continuous tasks that are frequently used to benchmark RL algorithms. This approach provides a new avenue for the development of models that are more suited to learn the evolution of dynamical systems, particularly useful in the context of model-based reinforcement learning. To assist related work, we have made code available at <a class="link-external link-https" href="https://github.com/vmartinezalvarez/DyNODE" rel="external noopener nofollow">this https URL</a> .      
### 46.Real-time Bayesian personalization via a learnable brain tumor growth model  [ :arrow_down: ](https://arxiv.org/pdf/2009.04240.pdf)
>  Modeling of brain tumor dynamics has the potential to advance therapeutic planning. Current modeling approaches resort to numerical solvers that simulate the tumor progression according to a given differential equation. Using highly-efficient numerical solvers, a single forward simulation takes up to a few minutes of compute. At the same time, clinical applications of the tumor modeling often imply solving an inverse problem, requiring up to tens of thousands forward model evaluations when used for a Bayesian model personalization via sampling. This results in a total inference time prohibitively expensive for clinical translation. Moreover, while recent data-driven approaches become capable of emulating physics simulation, they tend to fail in generalizing over the variability of the boundary conditions imposed by the patient-specific anatomy. In this paper, we propose a learnable surrogate with anatomy encoder for simulating tumor growth which maps the biophysical model parameters directly to simulation outputs, i.e. the local tumor cell densities. We test the surrogate on Bayesian tumor model personalization for a cohort of glioma patients. Bayesian inference using the proposed neural surrogate yields estimates analogous to those obtained by solving the forward model with a regular numerical solver. The near real-time computation cost renders the proposed method suitable for clinical settings. The code is available at <a class="link-external link-https" href="https://github.com/IvanEz/tumor-surrogate" rel="external noopener nofollow">this https URL</a>.      
### 47.Tactical Decision Making for Emergency Vehicles based on a Combinational Learning Method  [ :arrow_down: ](https://arxiv.org/pdf/2009.04203.pdf)
>  Increasing response time of emergency vehicles (EVs) could lead to an immensurable loss of property and life. On this account, tactical decision making for EV's microscopic control remains an indispensable issue to be improved. Our approach verifies that deep reinforcement learning could complement rule-based methods in generalization. It reveals that deterministic avoidance strategy for common vehicles at a low speed benefits EVs a lot, nevertheless, when at a high velocity, DQN breaks the deadlock of reduced safe distance and brings boldness to EVs in lane changing. Besides, a novel DQN method with speed-adaptive compact state space (SC-DQN) is put forward to fit in EVs' high-speed feature and generalize in various road topologies. All Above is implemented in SUMO emulator, where common vehicles are modeled rule-based whereas EVs are intelligently controlled.      
### 48.Traction Adaptive Motion Planning at the Limits of Handling  [ :arrow_down: ](https://arxiv.org/pdf/2009.04180.pdf)
>  In this paper we address the problem of motion planning and control at the limits of handling, under locally varying traction conditions. We propose a novel solution method where locally varying traction is represented by time-varying tire force constraints. A constrained finite time optimal control problem is solved in a receding horizon fashion, imposing these time-varying constraints. Furthermore, we employ a sampling augmentation procedure to address the problems of infeasibility and sensitivity to local minima that arises when the constraint configuration is altered. <br>We validate the proposed algorithm on a Volvo FH16 heavy-duty vehicle, in a range of critical scenarios. Experimental results indicate that traction adaptation improves the vehicle's capacity to avoid accidents, both when adapting to low and high local traction.      
### 49.Real-time Plant Health Assessment Via Implementing Cloud-based Scalable Transfer Learning On AWS DeepLens  [ :arrow_down: ](https://arxiv.org/pdf/2009.04110.pdf)
>  In the Agriculture sector, control of plant leaf diseases is crucial as it influences the quality and production of plant species with an impact on the economy of any country. Therefore, automated identification and classification of plant leaf disease at an early stage is essential to reduce economic loss and to conserve the specific species. Previously, to detect and classify plant leaf disease, various Machine Learning models have been proposed; however, they lack usability due to hardware incompatibility, limited scalability and inefficiency in practical usage. Our proposed DeepLens Classification and Detection Model (DCDM) approach deal with such limitations by introducing automated detection and classification of the leaf diseases in fruits (apple, grapes, peach and strawberry) and vegetables (potato and tomato) via scalable transfer learning on AWS SageMaker and importing it on AWS DeepLens for real-time practical usability. Cloud integration provides scalability and ubiquitous access to our approach. Our experiments on extensive image data set of healthy and unhealthy leaves of fruits and vegetables showed an accuracy of 98.78% with a real-time diagnosis of plant leaves diseases. We used forty thousand images for the training of deep learning model and then evaluated it on ten thousand images. The process of testing an image for disease diagnosis and classification using AWS DeepLens on average took 0.349s, providing disease information to the user in less than a second.      
### 50.Adaptive driver-automation shared steering control via forearm surface electromyography measurement  [ :arrow_down: ](https://arxiv.org/pdf/2009.04100.pdf)
>  Shared steering control has been developed to reduce driver workload while keeping the driver in the control loop. A driver could integrate visual sensory information from the road ahead and haptic sensory information from the steering wheel to achieve better driving performance. Previous studies suggest that, compared with adaptive automation authority, fixed automation authority is not always appropriate with respect to human factors. This paper focuses on designing an adaptive shared steering control system via sEMG (surface electromyography) measurement from the forearm of the driver, and evaluates the effect of the system on driver behavior during a double lane change task. The shared steering control was achieved through a haptic guidance system which provided active assistance torque on the steering wheel. Ten subjects participated in a high-fidelity driving simulator experiment. Two types of adaptive algorithms were investigated: haptic guidance decreases when driver grip strength increases (HG-Decrease), and haptic guidance increases when driver grip strength increases (HG-Increase). These two algorithms were compared to manual driving and two levels of fixed authority haptic guidance, for a total of five experimental conditions. Evaluation of the driving systems was based on two sets of dependent variables: objective measures of driver behavior and subjective measures of driver workload. The results indicate that the adaptive authority of HG-Decrease yielded lower driver workload and reduced the lane departure risk compared to manual driving and fixed authority haptic guidance.      
### 51.Generalizing Complex/Hyper-complex Convolutions to Vector Map Convolutions  [ :arrow_down: ](https://arxiv.org/pdf/2009.04083.pdf)
>  We show that the core reasons that complex and hypercomplex valued neural networks offer improvements over their real-valued counterparts is the weight sharing mechanism and treating multidimensional data as a single entity. Their algebra linearly combines the dimensions, making each dimension related to the others. However, both are constrained to a set number of dimensions, two for complex and four for quaternions. Here we introduce novel vector map convolutions which capture both of these properties provided by complex/hypercomplex convolutions, while dropping the unnatural dimensionality constraints they impose. This is achieved by introducing a system that mimics the unique linear combination of input dimensions, such as the Hamilton product for quaternions. We perform three experiments to show that these novel vector map convolutions seem to capture all the benefits of complex and hyper-complex networks, such as their ability to capture internal latent relations, while avoiding the dimensionality restriction.      
### 52.Template Matching and Change Point Detection by M-estimation  [ :arrow_down: ](https://arxiv.org/pdf/2009.04072.pdf)
>  We consider the fundamental problem of matching a template to a signal. We do so by M-estimation, which encompasses procedures that are robust to gross errors (i.e., outliers). Using standard results from empirical process theory, we derive the convergence rate and the asymptotic distribution of the M-estimator under relatively mild assumptions. We also discuss the optimality of the estimator, both in finite samples in the minimax sense and in the large-sample limit in terms of local minimaxity and relative efficiency. Although most of the paper is dedicated to the study of the basic shift model in the context of a random design, we consider many extensions towards the end of the paper, including more flexible templates, fixed designs, the agnostic setting, and more.      
### 53.Exploiting Multi-Modal Features From Pre-trained Networks for Alzheimer's Dementia Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.04070.pdf)
>  Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient's medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer's Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer's Dementia by providing acoustic and textual data. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. Our test results surpass baseline's accuracy by 18.75%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70%.      
### 54.Improved Trainable Calibration Method for Neural Networks on Medical Imaging Classification  [ :arrow_down: ](https://arxiv.org/pdf/2009.04057.pdf)
>  Recent works have shown that deep neural networks can achieve super-human performance in a wide range of image classification tasks in the medical imaging domain. However, these works have primarily focused on classification accuracy, ignoring the important role of uncertainty quantification. Empirically, neural networks are often miscalibrated and overconfident in their predictions. This miscalibration could be problematic in any automatic decision-making system, but we focus on the medical field in which neural network miscalibration has the potential to lead to significant treatment errors. We propose a novel calibration approach that maintains the overall classification accuracy while significantly improving model calibration. The proposed approach is based on expected calibration error, which is a common metric for quantifying miscalibration. Our approach can be easily integrated into any classification task as an auxiliary loss term, thus not requiring an explicit training round for calibration. We show that our approach reduces calibration error significantly across various architectures and datasets.      
### 55.A new architecture for hand-worn Sign language to Speech translator  [ :arrow_down: ](https://arxiv.org/pdf/2009.03988.pdf)
>  People with speech and hearing impairments often rely on sign language to communicate with others but most of the general population cannot understand sign language and sign language itself is a difficult language to learn, so there is a definite need for technologies to translate sign language to speech. In this paper, we describe the design and implementation of Smart glove, a hand-worn hardware device capable of translating American Sign Language gestures into English speech by tracking the finger's orientation, gestures and hand motion. It uses hardware sensors like Flex, Accelerometer and gyroscope and intelligent software to capture and translate the gestures into speech. This paper explains the translation of both Alphabet and Word gestures. New approaches and algorithms are proposed and implemented to address hardware-dependent issues in existing glove based designs. The whole device is designed to be modular with distributed processing units to encourage modular enhancement, reducing complexity, and interrelation between subsystems.Decision Trees are used in gesture recognition and error correction. We hope that the henceforth mentioned design and architecture would be the basis for the advancement in research related to sensor-based sign language translation along with research for smart glove and cybernetic accessories.      
### 56.Asymmetric Aging Effect on Modern Microprocessors  [ :arrow_down: ](https://arxiv.org/pdf/2009.03945.pdf)
>  Reliability is a crucial requirement in any modern microprocessor to assure correct execution over its lifetime. As mission critical components are becoming common in commodity systems; e.g., control of autonomous cars, the demand for reliable processing has even further heightened. Latest process technologies even worsened the situation; thus, microprocessors design has become highly susceptible to reliability concerns. This paper examines asymmetric aging phenomenon, which is a major reliability concern in advanced process nodes. In this phenomenon, logical elements and memory cells suffer from unequal timing degradation over time and consequently introduce reliability concerns. So far, most studies approached asymmetric aging from circuit or physical design viewpoint, but these solutions were quite limited and suboptimal. In this paper we introduce an asymmetric aging aware micro-architecture that aims at reducing its impact. The study is mainly focused on the following subsystems: execution units, register files and the memory hierarchy. Our experiments indicate that the proposed solutions incur minimal overhead while significantly mitigating the asymmetric aging stress.      
