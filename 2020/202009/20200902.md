# ArXiv eess --Wed, 2 Sep 2020
### 1.Quality-aware semi-supervised learning for CMR segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00584.pdf)
>  One of the challenges in developing deep learning algorithms for medical image segmentation is the scarcity of annotated training data. To overcome this limitation, data augmentation and semi-supervised learning (SSL) methods have been developed. However, these methods have limited effectiveness as they either exploit the existing data set only (data augmentation) or risk negative impact by adding poor training examples (SSL). Segmentations are rarely the final product of medical image analysis - they are typically used in downstream tasks to infer higher-order patterns to evaluate diseases. Clinicians take into account a wealth of prior knowledge on biophysics and physiology when evaluating image analysis results. We have used these clinical assessments in previous works to create robust quality-control (QC) classifiers for automated cardiac magnetic resonance (CMR) analysis. In this paper, we propose a novel scheme that uses QC of the downstream task to identify high quality outputs of CMR segmentation networks, that are subsequently utilised for further network training. In essence, this provides quality-aware augmentation of training data in a variant of SSL for segmentation networks (semiQCSeg). We evaluate our approach in two CMR segmentation tasks (aortic and short axis cardiac volume segmentation) using UK Biobank data and two commonly used network architectures (U-net and a Fully Convolutional Network) and compare against supervised and SSL strategies. We show that semiQCSeg improves training of the segmentation networks. It decreases the need for labelled data, while outperforming the other methods in terms of Dice and clinical metrics. SemiQCSeg can be an efficient approach for training segmentation networks for medical image data when labelled datasets are scarce.      
### 2.Frequency Domain Stability Method for Reset Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.00569.pdf)
>  Today, linear PID controllers cannot satisfy requirements of high-precision industry due to the development of technology. According to the literature, reset controllers can overcome this important barrier. However, similar to other non-linear controllers, stability analysis for these controllers is complex and needs parametric models of the systems. Consequently, the applicability of these controllers may diminish in industry. The well-known Hb method is one of the solutions of this significant problem. Nevertheless, assessing the Hb condition in the frequency-domain is complex, particularly for high dimensional plants. Furthermore, it cannot assess UBIBS stability of reset control systems in the case of reseting to non-zero values. In this paper, the aforementioned problems have been solved for the first and second order reset elements and a frequency-domain approach for assessing stability of reset control systems is developed. Finally, some practical examples are presented to demonstrate the effectiveness of the proposed approach to use the frequency response measurement directly to assess the stability.      
### 3.Analysis of memory in LSTM-RNNs for source separation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00551.pdf)
>  Long short-term memory recurrent neural networks (LSTM-RNNs) are considered state-of-the art in many speech processing tasks. The recurrence in the network, in principle, allows any input to be remembered for an indefinite time, a feature very useful for sequential data like speech. However, very little is known about which information is actually stored in the LSTM and for how long. We address this problem by using a memory reset approach which allows us to evaluate network performance depending on the allowed memory time span. We apply this approach to the task of multi-speaker source separation, but it can be used for any task using RNNs. We find a strong performance effect of short-term (shorter than 100 milliseconds) linguistic processes. Only speaker characteristics are kept in the memory for longer than 400 milliseconds. Furthermore, we confirm that performance-wise it is sufficient to implement longer memory in deeper layers. Finally, in a bidirectional model, the backward models contributes slightly more to the separation performance than the forward model.      
### 4.Effects of Correlated Noise on the Performance of Persistence Based Dynamic State Detection Methods  [ :arrow_down: ](https://arxiv.org/pdf/2009.00494.pdf)
>  The ability to characterize the state of dynamic systems has been a pertinent task in the time series analysis community. Traditional measures such as Lyapunov exponents are often times difficult to recover from noisy data, especially if the dimensionality of the system is not known. More recent binary and network based testing methods have delivered promising results for unknown deterministic systems, however noise injected into a periodic signal leads to false positives. Recently, we showed the advantage of using persistent homology as a tool for achieving dynamic state detection for systems with no known model and showed its robustness to white Gaussian noise. In this work, we explore the robustness of the persistence based methods to the influence of colored noise and show that colored noise processes of the form $1/f^{\alpha}$ lead to false positive diagnostic at lower signal to noise ratios for $\alpha&lt;0$.      
### 5.Off Policy Risk Sensitive Reinforcement Learning Based Optimal Tracking Control with Prescribe Performances  [ :arrow_down: ](https://arxiv.org/pdf/2009.00476.pdf)
>  An off policy reinforcement learning based control strategy is developed for the optimal tracking control problem to achieve the prescribed performance of full states during the learning process. The optimal tracking control problem is converted as an optimal regulation problem based on an auxiliary system. The requirements of prescribed performances are transformed into constraint satisfaction problems that are dealt with by risk sensitive state penalty terms under an optimization framework. To get approximated solutions of the Hamilton Jacobi Bellman equation, an off policy adaptive critic learning architecture is developed by using current data and experience data together. By using experience data, the proposed weight estimation update law of the critic learning agent guarantees weight convergence to the actual value. This technique enjoys practicability comparing with common methods that need to incorporate external signals to satisfy the persistence of excitation condition for weight convergence. The proofs of stability and weight convergence of the closed loop system are provided. Simulation results reveal the validity of the proposed off policy risk sensitive reinforcement learning based control strategy.      
### 6.End-to-End Hyperspectral-Depth Imaging with Learned Diffractive Optics  [ :arrow_down: ](https://arxiv.org/pdf/2009.00463.pdf)
>  To extend the capabilities of spectral imaging, hyperspectral and depth imaging have been combined to capture the higher-dimensional visual information. However, the form factor of the combined imaging systems increases, limiting the applicability of this new technology. In this work, we propose a monocular imaging system for simultaneously capturing hyperspectral-depth (HS-D) scene information with an optimized diffractive optical element (DOE). In the training phase, this DOE is optimized jointly with a convolutional neural network to estimate HS-D data from a snapshot input. To study natural image statistics of this high-dimensional visual data and to enable such a machine learning-based DOE training procedure, we record two HS-D datasets. One is used for end-to-end optimization in deep optical HS-D imaging, and the other is used for enhancing reconstruction performance with a real-DOE prototype. The optimized DOE is fabricated with a grayscale lithography process and inserted into a portable HS-D camera prototype, which is shown to robustly capture HS-D information. In extensive evaluations, we demonstrate that our deep optical imaging system achieves state-of-the-art results for HS-D imaging and that the optimized DOE outperforms alternative optical designs.      
### 7.Market Model for Demand Response under Block Rate Pricing  [ :arrow_down: ](https://arxiv.org/pdf/2009.00439.pdf)
>  Renewable sources are taking center stage in electricity generation. However, matching supply with demand in a renewable-rich system is a difficult task due to the intermittent nature of renewable resources (wind, solar, etc.). As a result, Demand Response (DR) programs are an essential part of the modern grid. An efficient DR technique is to devise different pricing schemes that encourage customers to reduce or shift the electric load. In this paper, we consider a market model for DR using Block Rate Pricing (BRP) of two blocks. We use a utility maximization approach in a competitive market. We show that when customers are price taking and the utility cost function is quadratic the resulting system achieves an equilibrium. Moreover, the equilibrium is unique and efficient, which maximizes social welfare. A distributed algorithm is proposed to find the optimal pricing of both blocks and the load. Both the customers and the utility runs the market. The proposed scheme encourages customers to curtail or shift their load. Numerical results are presented to validate our technique.      
### 8.Safety of Connected Vehicle Platoons  [ :arrow_down: ](https://arxiv.org/pdf/2009.00438.pdf)
>  Conventionally, a frequency domain condition on the spacing error transfer function is employed to assure string stability in a vehicle platoon. While this criteria guarantees that the power of spacing error signals diminish downstream, in order to avoid a collision it is more relevant to study the maximum spacing errors across the platoon. In this paper, we first re-examine the notion of string stability as it relates to safety by providing an upper bound on the maximum spacing error of any vehicle in a homogeneous platoon in terms of the lead vehicle's input. We also extend our previous work by providing a sufficient condition for minimum string stable headway for platoons experiencing burst-noise packet losses. Finally, we utilize throttle and brake maps to develop a longitudinal vehicle model and validate it against a Lincoln MKZ which is then used for numerical corroboration of the proposed lossy vehicle following algorithms.      
### 9.An algorithm for dividing quaternions  [ :arrow_down: ](https://arxiv.org/pdf/2009.00425.pdf)
>  In this work, a rationalized algorithm for calculating the quotient of two quaternions is presented which reduces the number of underlying real multiplications. Hardware for fast multiplication is much more expensive than hardware for fast addition. Therefore, reducing the number of multiplications in VLSI processor design is usually a desirable task. The performing of a quaternion division using the naive method takes 16 multiplications, 15 additions, 4 squarings and 4 divisions of real numbers while the proposed algorithm can compute the same result in only 8 multiplications (or multipliers in hardware implementation case), 31 additions, 4 squaring and 4 division of real numbers.      
### 10.Denoising Click-evoked Otoacoustic Emission Signals by Optimal Shrinkage  [ :arrow_down: ](https://arxiv.org/pdf/2009.00386.pdf)
>  Click-evoked otoacoustic emissions (CEOAEs) are clinically used as an objective way to infer whether cochlear functions are normal. However, because the sound pressure level of CEOAEs is typically much lower than the background noise, it usually takes hundreds, if not thousands of repetitions to estimate the signal with sufficient accuracy. In this paper, we propose to improve the signal-to-noise ratio (SNR) of CEOAE signals within limited measurement time by optimal shrinkage (OS) in two different settings: the covariance-based OS (cOS) and the singular value decomposition (SVD)-based OS (sOS). By simulation and analyzing human CEOAE data, the cOS consistently reduced the noise and enhanced the SNR by 1 to 2 dB from a baseline method (BM) that is based on calculating the median. The sOS achieved an SNR enhancement of 2 to 3 dB in simulation, and demonstrated capability to enhance the SNR in real recordings when the SNR achieved by the BM was below 0 dB. An appealing property of OS is that it produces an estimate of every individual column of the signal matrix. This property makes it possible to investigate CEOAE dynamics across a longer period of time when the cochlear conditions are not strictly stationary.      
### 11.Image Super-Resolution using Explicit Perceptual Loss  [ :arrow_down: ](https://arxiv.org/pdf/2009.00382.pdf)
>  This paper proposes an explicit way to optimize the super-resolution network for generating visually pleasing images. The previous approaches use several loss functions which is hard to interpret and has the implicit relationships to improve the perceptual score. We show how to exploit the machine learning based model which is directly trained to provide the perceptual score on generated images. It is believed that these models can be used to optimizes the super-resolution network which is easier to interpret. We further analyze the characteristic of the existing loss and our proposed explicit perceptual loss for better interpretation. The experimental results show the explicit approach has a higher perceptual score than other approaches. Finally, we demonstrate the relation of explicit perceptual loss and visually pleasing images using subjective evaluation.      
### 12.Free-Space Optical MISO Communications With an Array of Detectors  [ :arrow_down: ](https://arxiv.org/pdf/2009.00380.pdf)
>  Multiple-input multiple-output (MIMO) and multiple-input single-output (MISO) schemes have yielded promising results in free space optical (FSO) communications by providing diversity against fading of the received signal intensity. In this paper, we have analyzed the probability of error performance of a \emph{muliple-input single-output} (MISO) free-space optical channel that employs array(s) of detectors at the receiver. In this regard, we have considered the \emph{maximal ratio combiner} (MRC) and \emph{equal gain combiner} (EGC) fusion algorithms for the array of detectors, and we have examined the performance of these algorithms subject to phase and pointing errors for strong atmospheric turbulence conditions. It is concluded that when the variance of the phase and pointing errors are below certain thresholds, signal combining with a single array of detectors yields significantly better performance than a multiple arrays receiver. In the final part of the paper, we examine the probability of error of the single detector array receiver as a function of the beam radius, and the probability of error is minimized by (numerically) optimizing the beam radius of the received signal beams.      
### 13.PiNet: Deep Structure Learning using Feature Extraction in Trained Projection Space  [ :arrow_down: ](https://arxiv.org/pdf/2009.00378.pdf)
>  Over the last decade of machine learning, convolutional neural networks have been the most striking successes for feature extraction of rich sensory and high-dimensional data. While learning data representations via convolutions is already well studied and efficiently implemented in various deep learning libraries, one often faces limited memory capacity and an insufficient number of training data, especially for high-dimensional and large-scale tasks. To overcome these issues, we introduce a network architecture using a self-adjusting and data dependent version of the Radon-transform (projection data) to enable feature extraction via convolutions in lower-dimensional space. The resulting framework named PiNet can be trained end-to-end and shows promising performance on volumetric segmentation tasks. We also test our PiNet framework on public challenge datasets to show that our approach achieves comparable results only using a fractional amount of parameters and storage.      
### 14.Multi-Array Electron Beam Stabilization using Block-Circulant Transformation and Generalized Singular Value Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2009.00345.pdf)
>  We introduce a novel structured controller design for the electron beam stabilization problem of the UK's national synchrotron light source. Because changes to the synchrotron will not allow the application of existing control approaches, we develop a novel method to diagonalize the multi-input multi-output (MIMO) system. A generalized singular value decomposition (GSVD) is used to simultaneously diagonalize the actuator response matrices, which is applicable to an arbitrary number of actuator dynamics in a cross-directional setting. The resulting decoupled systems are regulated using mid-ranged control and the controller gains derived as a function of the generalized singular values. In addition, we exploit the inherent block-circulant symmetry of the system. The performance of our controller is demonstrated using simulations that involve machine data.      
### 15.Transaction Pricing for Maximizing Throughput in a Sharded Blockchain Ledger  [ :arrow_down: ](https://arxiv.org/pdf/2009.00319.pdf)
>  In this paper, we present a pricing mechanism that aligns incentives of agents who exchange resources on a decentralized ledger with the goal of maximizing transaction throughput. Subdividing a blockchain ledger into shards promises to greatly increase transaction throughput with minimal loss of security. However, the organization and type of the transactions also affects the ledger's efficiency, which is increased by wallet agents transacting in a single shard whenever possible while collectively distributing their transactions uniformly across the available shards. Since there is no central authority to enforce these properties, the only means of achieving them is to design the system such that it is in agents' interest to act in a way that benefits overall throughput. We show that our proposed pricing policy does exactly this by inducing a potential game for the agents, where the potential function relates directly to ledger throughput. Simulations demonstrate that this policy leads to near-optimal throughput under a variety of conditions.      
### 16.Regional Total Electron Content Map Generation based on Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2009.00305.pdf)
>  Ionosphere has an important role in long distance HF communications, satellite communications and global navigation systems. Ionosphere is a plasma medium which arises due to solar and cosmic radiation, and the amount of ionization is highly time and location dependent. Eventually, the current state of the ionosphere should be continuously monitored with high accuracy. Total Electron Content (TEC) maps are being used to investigate the state of the ionosphere. There are online services which provide TEC maps, however they mostly have low spatial and temporal resolution and the techniques used for generating these maps are generally not accessible. <br>TEC maps can also be generated from GNSS/GPS based Continuously Operating Receiver Stations (CORS) network measurements. Unfortunately, the GNSS/GPS receiver networks are not dense enough to form a map directly. Therefore, an algorithm should be used to estimate the TEC values at coordinates without a receiver. Based on the observation that TEC maps possess a high degree of sparsity, we propose a modified compressive sensing technique for generating regional TEC maps by using the sparse dataset obtained from a CORS network. <br>We evaluate the performance of the proposed technique both over synthetically generated TEC maps which mimic the common characteristics of the ionosphere, and also over actual measurements taken over the Turkish National Permanent GPS Network (TNPGN) Active. Our analysis reveals that the proposed technique can produce TEC maps with high accuracy and resolution. We also demonstrate the superiority of our technique over other TEC map generation techniques found in the literature.      
### 17.Recognition Oriented Iris Image Quality Assessment in the Feature Space  [ :arrow_down: ](https://arxiv.org/pdf/2009.00294.pdf)
>  A large portion of iris images captured in real world scenarios are poor quality due to the uncontrolled environment and the non-cooperative subject. To ensure that the recognition algorithm is not affected by low-quality images, traditional hand-crafted factors based methods discard most images, which will cause system timeout and disrupt user experience. In this paper, we propose a recognition-oriented quality metric and assessment method for iris image to deal with the problem. The method regards the iris image embeddings Distance in Feature Space (DFS) as the quality metric and the prediction is based on deep neural networks with the attention mechanism. The quality metric proposed in this paper can significantly improve the performance of the recognition algorithm while reducing the number of images discarded for recognition, which is advantageous over hand-crafted factors based iris quality assessment methods. The relationship between Image Rejection Rate (IRR) and Equal Error Rate (EER) is proposed to evaluate the performance of the quality assessment algorithm under the same image quality distribution and the same recognition algorithm. Compared with hand-crafted factors based methods, the proposed method is a trial to bridge the gap between the image quality assessment and biometric recognition.      
### 18.Intelligent Reflecting Surface Aided Multicasting with Random Passive Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2009.00274.pdf)
>  In this letter, we consider a multicast system where a single-antenna transmitter sends a common message to multiple single-antenna users, aided by an intelligent reflecting surface (IRS) equipped with $N$ passive reflecting elements. Prior works on IRS have mostly assumed the availability of channel state information (CSI) for designing its passive beamforming. However, the acquisition of CSI requires substantial training overhead that increases with $N$. In contrast, we propose in this letter a novel \emph{random passive beamforming} scheme, where the IRS performs independent random reflection for $Q\geq 1$ times in each channel coherence interval without the need of CSI acquisition. For the proposed scheme, we first derive a closed-form approximation of the outage probability, based on which the optimal $Q$ with best outage performance can be efficiently obtained. Then, for the purpose of comparison, we derive a lower bound of the outage probability with traditional CSI-based passive beamforming. Numerical results show that a small $Q$ is preferred in the high-outage regime (or with high rate target) and the optimal $Q$ becomes larger as the outage probability decreases (or as the rate target decreases). Moreover, the proposed scheme significantly outperforms the CSI-based passive beamforming scheme with training overhead taken into consideration when $N$ and/or the number of users are large, thus offering a promising CSI-free alternative to existing CSI-based schemes.      
### 19.Preventing Identity Attacks in RFID Backscatter Communication Systems: A Physical-Layer Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.00271.pdf)
>  This work considers identity attack on a radio-frequency identification (RFID)-based backscatter communication system. Specifically, we consider a single-reader, single-tag RFID system whereby the reader and the tag undergo two-way signaling which enables the reader to extract the tag ID in order to authenticate the legitimate tag (L-tag). We then consider a scenario whereby a malicious tag (M-tag)---having the same ID as the L-tag programmed in its memory by a wizard---attempts to deceive the reader by pretending to be the L-tag. To this end, we counter the identity attack by exploiting the non-reciprocity of the end-to-end channel (i.e., the residual channel) between the reader and the tag as the fingerprint of the tag. The passive nature of the tag(s) (and thus, lack of any computational platform at the tag) implies that the proposed light-weight physical-layer authentication method is implemented at the reader. To be concrete, in our proposed scheme, the reader acquires the raw data via two-way (challenge-response) message exchange mechanism, does least-squares estimation to extract the fingerprint, and does binary hypothesis testing to do authentication. We also provide closed-form expressions for the two error probabilities of interest (i.e., false alarm and missed detection). Simulation results attest to the efficacy of the proposed method.      
### 20.Design and Simulation of Voltage Amplidyne System using Robust Control Technique  [ :arrow_down: ](https://arxiv.org/pdf/2009.00232.pdf)
>  In this paper, modelling designing and simulation of a simple voltage amplidyne system is done using robust control theory. In order to increase the performance of the voltage amplidyne system with H optimal control synthesis and H optimal control synthesis via-iteration controllers are used. The open loop response of the voltage amplidyne system shows that the system can amplify the input 7 times. Comparison of the voltage amplidyne system with H optimal control synthesis and H optimal control synthesis via-iteration controllers to track a desired step input have been done. Finally, the comparative simulation results prove the effectiveness of the proposed voltage amplidyne system with H optimal control synthesis controller in improving the percentage overshoot and the settling time.      
### 21.On The Usage Of Average Hausdorff Distance For Segmentation Performance Assessment: Hidden Bias When Used For Ranking  [ :arrow_down: ](https://arxiv.org/pdf/2009.00215.pdf)
>  Average Hausdorff Distance (AVD) is a widely used performance measure to calculate the distance between two point sets. In medical image segmentation, AVD is used to compare ground truth images with segmentation results allowing their ranking. We identified, however, a ranking bias of AVD making it less suitable for segmentation ranking. To mitigate this bias, we present a modified calculation of AVD that we have coined balanced AVD (bAVD). To simulate segmentations for ranking, we manually created non-overlapping segmentation errors common in cerebral vessel segmentation as our use-case. Adding the created errors consecutively and randomly to the ground truth, we created sets of simulated segmentations with increasing number of errors. Each set of simulated segmentations was ranked using AVD and bAVD. We calculated the Kendall-rank-correlation-coefficient between the segmentation ranking and the number of errors in each simulated segmentation. The rankings produced by bAVD had a significantly higher average correlation (0.969) than those of AVD (0.847). In 200 total rankings, bAVD misranked 52 and AVD misranked 179 segmentations. Our proposed evaluation measure, bAVD, alleviates AVDs ranking bias making it more suitable for rankings and quality assessment of segmentations.      
### 22.Bearing-only formation control under persistence of excitation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00209.pdf)
>  This paper addresses the problem of bearing-only formation control in $d~(d\geq 2)$-dimensional space by exploring persistence of excitation (PE) of the desired bearing reference. By defining a desired formation that is bearing PE, distributed bearing-only control laws are proposed, which guarantee exponential stabilization of the desired formation only up to a translation vector. The key outcome of this approach relies in exploiting the bearing PE to significantly relax the conditions imposed on the graph topology to ensure exponential stabilization, when compared to the bearing rigidity conditions, and to remove the scale ambiguity introduced by bearing vectors. Simulation results are provided to illustrate the performance of the proposed control method.      
### 23.Deep unsupervised learning for Microscopy-Based Malaria detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.00197.pdf)
>  Malaria, a mosquito-borne disease caused by a parasite, kills over 1 million people globally each year. People, if left untreated, may develop severe complications, leading to death. Effective and accurate diagnosis is important for the management and control of malaria. Our research focuses on utilizing machine learning to improve the efficiency in Malaria diagnosis. We utilize a modified U-net architecture, as an unsupervised learning model, to conduct cell boundary detection. The blood cells infected by malaria are then identified in chromatic space by a Mahalanobis distance algorithm. Both the cell segmentation and Malaria detection process often requires intensive manual label, which we hope to eliminate via the unsupervised workflow.      
### 24.Neural Architecture Search For Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2009.00165.pdf)
>  Deep neural networks have recently become a popular solution to keyword spotting systems, which enable the control of smart devices via voice. In this paper, we apply neural architecture search to search for convolutional neural network models that can help boost the performance of keyword spotting based on features extracted from acoustic signals while maintaining an acceptable memory footprint. Specifically, we use differentiable architecture search techniques to search for operators and their connections in a predefined cell search space. The found cells are then scaled up in both depth and width to achieve competitive performance. We evaluated the proposed method on Google's Speech Commands Dataset and achieved a state-of-the-art accuracy of over 97% on the setting of 12-class utterance classification commonly reported in the literature.      
### 25.Optimal Solution Analysis and Decentralized Mechanisms for Peer-to-Peer Energy Markets  [ :arrow_down: ](https://arxiv.org/pdf/2009.00161.pdf)
>  This paper studies the optimal clearing problem for prosumers in peer-to-peer (P2P) energy markets. It is proved that if no trade weights are enforced and the communication structure between successfully traded peers is connected, then the optimal clearing price and total traded powers in P2P market are the same with that in the pool-based market. However, if such communication structure is unconnected, then the P2P market is clustered into smaller P2P markets. If the trade weights are imposed, then the derived P2P market solutions can be significantly changed. Next, a novel decentralized optimization approach is proposed to derive a trading mechanism for P2P markets, based on the alternating direction method of multipliers (ADMM) which naturally fits into the bidirectional trading in P2P energy systems and converges reasonably fast. Analytical formulas of variable updates reveal insightful relations for each pair of prosumers on their individually traded prices and powers with their total traded powers. Further, based on those formulas, decentralized learning schemes for tuning parameters of prosumers cost functions are proposed to attain successful trading with total traded power amount as desired. Case studies on a synthetic system and the IEEE European Low Voltage Test Feeder are then carried out to verify the proposed approaches.      
### 26.Optimal Bayesian Quickest Detection for Hidden Markov Models and Structured Generalisations  [ :arrow_down: ](https://arxiv.org/pdf/2009.00150.pdf)
>  In this paper we consider the problem of quickly detecting changes in hidden Markov models (HMMs) in a Bayesian setting, as well as several structured generalisations including changes in statistically periodic processes, quickest detection of a Markov process across a sensor array, quickest detection of a moving target in a sensor network and quickest change detection (QCD) in multistream data. Our main result establishes an optimal Bayesian HMM QCD rule with a threshold structure. This framework and proof techniques allow us to to elegantly establish optimal rules for several structured generalisations by showing that these problems are special cases of the Bayesian HMM QCD problem. We develop bounds to characterise the performance of our optimal rule and provide an efficient method for computing the test statistic. Finally, we examine the performance of our rule in several simulation examples and propose a technique for calculating the optimal threshold.      
### 27.Energy-efficient Wireless Charging and Computation Offloading In MEC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.00117.pdf)
>  Wireless charging coupled with computation offloading in edge networks offers a promising solution for realizing power-hungry and computation intensive applications on user devices. We consider a mutil-access edge computing (MEC) system with collocated MEC servers and base-stations/access points (BS/AP) supporting multiple users requesting data computation and wireless charging. We propose an integrated solution with computation offloading to satisfy the largest proportion of requested wireless charging while keeping the energy consumption at the minimum subject to the MEC-AP transmit power and latency constraints. We propose a novel algorithm to perform data partitioning, time allocation, transmit power control and design the optimal energy beamforming for wireless charging. Our resource allocation scheme offers an energy minimizing solution compared to other schemes while also delivering higher amount of transferred charge to the users.      
### 28.Data and Image Prior Integration for Image Reconstruction Using Consensus Equilibrium  [ :arrow_down: ](https://arxiv.org/pdf/2009.00092.pdf)
>  Image domain prior models have been shown to improve the quality of reconstructed images, especially when data are limited. Pre-processing of raw data, through the implicit or explicit inclusion of data domain priors have separately also shown utility in improving reconstructions. In this work, a principled approach is presented allowing the unified integration of both data and image domain priors for improved image reconstruction. The consensus equilibrium framework is extended to integrate physical sensor models, data models, and image models. In order to achieve this integration, the conventional image variables used in consensus equilibrium are augmented with variables representing data domain quantities. The overall result produces combined estimates of both the data and the reconstructed image that is consistent with the physical models and prior models being utilized. The prior models used in both domains in this work are created using deep neural networks. The superior quality allowed by incorporating both data and image domain prior models is demonstrated for two applications: limited-angle CT and accelerated MRI. The prior data model in both these applications is focused on recovering missing data. Experimental results are presented for a 90 degree limited-angle tomography problem from a real checked-bagged CT dataset and a 4x accelerated MRI problem on a simulated dataset. The new framework is very flexible and can be easily applied to other computational imaging problems with imperfect data.      
### 29.Image Reconstruction of Static and Dynamic Scenes through Anisoplanatic Turbulence  [ :arrow_down: ](https://arxiv.org/pdf/2009.00071.pdf)
>  Ground based long-range passive imaging systems often suffer from degraded image quality due to a turbulent atmosphere. While methods exist for removing such turbulent distortions, many are limited to static sequences which cannot be extended to dynamic scenes. In addition, the physics of the turbulence is often not integrated into the image reconstruction algorithms, making the physics foundations of the methods weak. In this paper, we present a unified method for atmospheric turbulence mitigation in both static and dynamic sequences. We are able to achieve better results compared to existing methods by utilizing (i) a novel space-time non-local averaging method to construct a reliable reference frame, (ii) a geometric consistency and a sharpness metric to generate the lucky frame, (iii) a physics-constrained prior model of the point spread function for blind deconvolution. Experimental results based on synthetic and real long-range turbulence sequences validate the performance of the proposed method.      
### 30.Semantic Segmentation of Neuronal Bodies in Fluorescence Microscopy Using a 2D+3D CNN Training Strategy with Sparsely Annotated Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.00029.pdf)
>  Semantic segmentation of neuronal structures in 3D high-resolution fluorescence microscopy imaging of the human brain cortexcan take advantage of bidimensional CNNs, which yield good resultsin neuron localization but lead to inaccurate surface reconstruction. 3DCNNs on the other hand would require manually annotated volumet-ric data on a large scale, and hence considerable human effort. Semi-supervised alternative strategies which make use only of sparse anno-tations suffer from longer training times and achieved models tend tohave increased capacity compared to 2D CNNs, needing more groundtruth data to attain similar results. To overcome these issues we proposea two-phase strategy for training native 3D CNN models on sparse 2Dannotations where missing labels are inferred by a 2D CNN model andcombined with manual annotations in a weighted manner during losscalculation.      
### 31.Intelligent Hotel ROS-based Service Robot  [ :arrow_down: ](https://arxiv.org/pdf/2009.00594.pdf)
>  With the advances of artificial intelligence (AI) technology, many studies and work have been carried out on how robots could replace human labor. In this paper, we present a ROS based intelligence hotel robot, which simplifies the check-in process. We use pioneer 3dx robot and considered different environment settings. The robot combined with Hokuyo Lidar and Kinect Xbox camera, can plan the routes accurately and reach rooms in different floors. In addition, we added an intelligent voice system which provides an assistant for the customers.      
### 32.Data Anomaly Detection for Structural Health Monitoring of Bridges using Shapelet Transform  [ :arrow_down: ](https://arxiv.org/pdf/2009.00470.pdf)
>  With the wider availability of sensor technology, a number of Structural Health Monitoring (SHM) systems are deployed to monitor civil infrastructure. The continuous monitoring provides valuable information about the structure that can help in providing a decision support system for retrofits and other structural modifications. However, when the sensors are exposed to harsh environmental conditions, the data measured by the SHM systems tend to be affected by multiple anomalies caused by faulty or broken sensors. Given a deluge of high-dimensional data collected continuously over time, research into using machine learning methods to detect anomalies are a topic of great interest to the SHM community. This paper contributes to this effort by proposing the use of a relatively new time series representation named Shapelet Transform in combination with a Random Forest classifier to autonomously identify anomalies in SHM data. The shapelet transform is a unique time series representation that is solely based on the shape of the time series data. In consideration of the individual characteristics unique to every anomaly, the application of this transform yields a new shape-based feature representation that can be combined with any standard machine learning algorithm to detect anomalous data with no manual intervention. For the present study, the anomaly detection framework consists of three steps: identifying unique shapes from anomalous data, using these shapes to transform the SHM data into a local-shape space and training machine learning algorithm on this transformed data to identify anomalies. The efficacy of this method is demonstrated by the identification of anomalies in acceleration data from a SHM system installed on a long-span bridge in China. The results show that multiple data anomalies in SHM data can be automatically detected with high accuracy using the proposed method.      
### 33.A Digital Twin for Reconfigurable Intelligent Surface Assisted Wireless Communication  [ :arrow_down: ](https://arxiv.org/pdf/2009.00454.pdf)
>  Reconfigurable Intelligent Surface (RIS) has emerged as one of the key technologies for 6G in recent years, which comprise a large number of low-cost passive elements that can smartly interact with the impinging electromagnetic waves for performance enhancement. However, optimally configuring massive number of RIS elements remains a challenge. In this paper, we present a novel digital-twin framework for RIS-assisted wireless networks which we name it Environment-Twin (Env-Twin). The goal of the Env-Twin framework is to enable automation of optimal control at various granularities. In this paper, we present one example of the Env-Twin models to learn the mapping function between the RIS configuration with measured attributes for the receiver location, and the corresponding achievable rate in an RIS-assisted wireless network without involving explicit channel estimation or beam training overhead. Once learned, our Env-Twin model can be used to predict optimal RIS configuration for any new receiver locations in the same wireless network. We leveraged deep learning (DL) techniques to build our model and studied its performance and robustness. Simulation results demonstrate that the proposed Env-Twin model can recommend near-optimal RIS configurations for test receiver locations which achieved close to an upper bound performance that assumes perfect channel knowledge. Our Env-Twin model was trained using less than 2% of the total receiver locations. This promising result represents great potential of the proposed Env-Twin framework for developing a practical RIS solution where the panel can automatically configure itself without requesting channel state information (CSI) from the wireless network infrastructure.      
### 34.Autonomous Formula Racecar: Overall System Design and Experimental Validation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00385.pdf)
>  This paper develops and summarizes the work of building the autonomous integrated system including perception system and vehicle dynamic controller for a formula student autonomous racecar. We propose a system framework combining X-by-wired modification, perception &amp; motion planning and vehicle dynamic control as a template of FSAC racecar which can be easily replicated. A LIDAR-vision cooperating method of detecting traffic cone which is used as track mark is proposed. Detection algorithm of the racecar also implements a precise and high rate localization method which combines the GPS-INS data and LIDAR odometry. Besides, a track map including the location and color information of the cones is built simultaneously. Finally, the system and vehicle performance on a closed loop track is tested. This paper also briefly introduces the Formula Student Autonomous Competition (FSAC).      
### 35.A Benchmark for Multi-UAV Task Assignment of an Extended Team Orienteering Problem  [ :arrow_down: ](https://arxiv.org/pdf/2009.00363.pdf)
>  A benchmark for multi-UAV task assignment is presented in order to evaluate different algorithms. An extended Team Orienteering Problem is modeled for a kind of multi-UAV task assignment problem. Three intelligent algorithms, i.e., Genetic Algorithm, Ant Colony Optimization and Particle Swarm Optimization are implemented to solve the problem. A series of experiments with different settings are conducted to evaluate three algorithms. The modeled problem and the evaluation results constitute a benchmark, which can be used to evaluate other algorithms used for multi-UAV task assignment problems.      
### 36.Parallel One-Step Control of Parametrised Boolean Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.00359.pdf)
>  Boolean network (BN) is a simple model widely used to study complex dynamic behaviour of biological systems. Nonetheless, it might be difficult to gather enough data to precisely capture the behavior of a biological system into a set of Boolean functions. These issues can be dealt with to some extent using parametrised Boolean networks (ParBNs), as it allows to leave some update functions unspecified. In this paper, we attack the control problem for ParBNs with asynchronous semantics. While there is an extensive work on controlling BNs without parameters, the problem of control for ParBNs has not been in fact addressed yet. The goal of control is to ensure the stabilisation of a system in a given state using as few interventions as possible. There are many ways to control BN dynamics. Here, we consider the one-step approach in which the system is instantaneously perturbed out of its actual state. A naive approach to handle control of ParBNs is using parameter scan and solve the control problem for each parameter valuation separately using known techniques for non-parametrised BNs. This approach is however highly inefficient as the parameter space of ParBNs grows doubly-exponentially in the worst case. In this paper, we propose a novel semi-symbolic algorithm for the one-step control problem of ParBNs, that builds on a symbolic data structures to avoid scanning individual parameters. We evaluate the performance of our approach on real biological models.      
### 37.To augment or not to augment? Data augmentation in user identification based on motion sensors  [ :arrow_down: ](https://arxiv.org/pdf/2009.00300.pdf)
>  Nowadays, commonly-used authentication systems for mobile device users, e.g. password checking, face recognition or fingerprint scanning, are susceptible to various kinds of attacks. In order to prevent some of the possible attacks, these explicit authentication systems can be enhanced by considering a two-factor authentication scheme, in which the second factor is an implicit authentication system based on analyzing motion sensor data captured by accelerometers or gyroscopes. In order to avoid any additional burdens to the user, the registration process of the implicit authentication system must be performed quickly, i.e. the number of data samples collected from the user is typically small. In the context of designing a machine learning model for implicit user authentication based on motion signals, data augmentation can play an important role. In this paper, we study several data augmentation techniques in the quest of finding useful augmentation methods for motion sensor data. We propose a set of four research questions related to data augmentation in the context of few-shot user identification based on motion sensor signals. We conduct experiments on a benchmark data set, using two deep learning architectures, convolutional neural networks and Long Short-Term Memory networks, showing which and when data augmentation methods bring accuracy improvements. Interestingly, we find that data augmentation is not very helpful, most likely because the signal patterns useful to discriminate users are too sensitive to the transformations brought by certain data augmentation techniques. This result is somewhat contradictory to the common belief that data augmentation is expected to increase the accuracy of machine learning models.      
### 38.Graph-based Model of Smart Grid Architectures  [ :arrow_down: ](https://arxiv.org/pdf/2009.00273.pdf)
>  The rising use of information and communication technology in smart grids likewise increases the risk of failures that endanger the security of power supply, e.g., due to errors in the communication configuration, faulty control algorithms, or cyber-attacks. Co-simulations can be used to investigate such effects, but require precise modeling of the energy, communication, and information domain within an integrated smart grid infrastructure model. Given the complexity and lack of detailed publicly available communication network models for smart grid scenarios, there is a need for an automated and systematic approach to creating such coupled models. In this paper, we present an approach to automatically generate smart grid infrastructure models based on an arbitrary electrical distribution grid model using a generic architectural template. We demonstrate the applicability and unique features of our approach alongside examples concerning network planning, co-simulation setup, and specification of domain-specific intrusion detection systems.      
### 39.A Samplable Multimodal Observation Model for Global Localization and Kidnapping  [ :arrow_down: ](https://arxiv.org/pdf/2009.00211.pdf)
>  Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by sampling uniformly over the state space, which is unfortunately inefficient when the environment is large. To better deal with the the problems, we present a proposal model, named Deep Multimodal Observation Model (DMOM). DMOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be much efficient. Considering that learning based Samplable Observation Model may fail to capture the true pose sometimes, we furthermore propose the Adaptive Mixture MCL, which adaptively selects updating mode for each particle to tolerate this situation. Equipped with DMOM, Adaptive Mixture MCL can achieve more accurate estimation, faster convergence and better scalability compared with previous methods in both synthetic and real scenes. Even in real environment with long-term changing, Adaptive Mixture MCL is able to localize the robot using DMON trained only on simulated observations from a SLAM map, or even a blueprint map.      
### 40.Characterizing the Probability Law on Time Until Core Damage With PRA  [ :arrow_down: ](https://arxiv.org/pdf/2009.00208.pdf)
>  Certain modeling assumptions underlying Probabilistic Risk Assessment (PRA) allow a simple computation of core damage frequency (CDF). These assumptions also guarantee that the time remaining until a core damage event follows an exponential distribution having parameter value equal to that computed for the CDF. While it is commonly understood that these modeling assumptions lead to an approximate characterization of uncertainty, we offer a simple argument that explains why the resulting exponential time until core damage distribution under-estimates risk. Our explanation will first review operational physics properties of hazard functions, and then offer a non-measure-theoretic argument to reveal the the consequences of these properties for PRA. The conclusions offered, here, hold for any possible operating history that respects the underlying assumptions of PRA. Hence, the measure-theoretic constructs on filtered probability spaces is unnecessary for our developments. We will then conclude with a brief discussion that connects intuition with our analytical development.      
### 41.Deep Ice Layer Tracking and Thickness Estimation using Fully Convolutional Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.00191.pdf)
>  Global warming is rapidly reducing glaciers and ice sheets across the world. Real time assessment of this reduction is required so as to monitor its global climatic impact. In this paper, we introduce a novel way of estimating the thickness of each internal ice layer using Snow Radar images and Fully Convolutional Networks. The estimated thickness can be analysed to understand snow accumulation each year. To understand the depth and structure of each internal ice layer, we carry out a set of image processing techniques and perform semantic segmentation on the radar images. After detecting each ice layer uniquely, we calculate its thickness and compare it with the available ground truth. Through this procedure we were able to estimate the ice layer thicknesses within a Mean Absolute Error of approximately 3.6 pixels. Such a Deep Learning based method can be used with ever-increasing datasets to make accurate assessments for cryospheric studies.      
### 42.HJB and Fokker-Planck equations for river environmental management based on stochastic impulse control with discrete and random observation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00184.pdf)
>  We formulate a new two-variable river environmental restoration problem based on jump stochastic differential equations (SDEs) governing the sediment storage and nuisance benthic algae population dynamics in a dam-downstream river. Controlling the dynamics is carried out through impulsive sediment replenishment with discrete and random observation/intervention to avoid sediment depletion and thick algae growth. We consider a cost-efficient management problem of the SDEs to achieve the objectives whose resolution reduces to solving a Hamilton-Jacobi-Bellman (HJB) equation. We also consider a Fokker-Planck (FP) equation governing the probability density function of the controlled dynamics. The HJB equation has a discontinuous solution, while the FP equation has a Dirac's delta along boundaries. We show that the value function, the optimized objective function, is governed by the HJB equation in the simplified case and further that a threshold-type control is optimal. We demonstrate that simple numerical schemes can handle these equations. Finally, we numerically analyze the optimal controls and the resulting probability density functions.      
### 43.LoRaWAN Temperature Sensors for Local Government Asset Management  [ :arrow_down: ](https://arxiv.org/pdf/2009.00172.pdf)
>  The purpose of this project is to investigate the suitability of using LoRaWAN technology to conduct temperature studies on local government assets in Australian metropolitan and residential areas. Temperature sensing devices were integrated into the existing LoRaWAN infrastructure at Curtin University with data collected and stored on a remote server. Case studies were performed for the City of Melville to address the suitability for such a system to provide insights into heat islands and urban forests. Testing was completed on the Curtin University campus, replicating the climate conditions, asset types and, dense building and tree environment found in the City of Melville.      
### 44.Learning Nash Equilibria in Zero-Sum Stochastic Games via Entropy-Regularized Policy Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00162.pdf)
>  We explore the use of policy approximation for reducing the computational cost of learning Nash equilibria in multi-agent reinforcement learning scenarios. We propose a new algorithm for zero-sum stochastic games in which each agent simultaneously learns a Nash policy and an entropy-regularized policy. The two policies help each other towards convergence: the former guides the latter to the desired Nash equilibrium, while the latter serves as an efficient approximation of the former. We demonstrate the possibility of using the proposed algorithm to transfer previous training experiences to different environments, enabling the agents to adapt quickly to new tasks. We also provide a dynamic hyper-parameter scheduling scheme for further expedited convergence. Empirical results applied to a number of stochastic games show that the proposed algorithm converges to the Nash equilibrium while exhibiting a major speed-up over existing algorithms.      
### 45.LoCUS: A multi-robot loss-tolerant algorithm for surveying volcanic plumes  [ :arrow_down: ](https://arxiv.org/pdf/2009.00156.pdf)
>  Measurement of volcanic CO2 flux by a drone swarm poses special challenges. Drones must be able to follow gas concentration gradients while tolerating frequent drone loss. We present the LoCUS algorithm as a solution to this problem and prove its robustness. LoCUS relies on swarm coordination and self-healing to solve the task. As a point of contrast we also implement the MoBS algorithm, derived from previously published work, which allows drones to solve the task independently. We compare the effectiveness of these algorithms using drone simulations, and find that LoCUS provides a reliable and efficient solution to the volcano survey problem. Further, the novel data-structures and algorithms underpinning LoCUS have application in other areas of fault-tolerant algorithm research.      
### 46.A Review of Single-Source Deep Unsupervised Visual Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00155.pdf)
>  Large-scale labeled training datasets have enabled deep neural networks to excel across a wide range of benchmark vision tasks. However, in many applications, it is prohibitively expensive and time-consuming to obtain large quantities of labeled data. To cope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain. Unfortunately, direct transfer across domains often performs poorly due to the presence of domain shift or dataset bias. Domain adaptation is a machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this paper, we review the latest single-source deep unsupervised domain adaptation methods focused on visual tasks and discuss new perspectives for future research. We begin with the definitions of different domain adaptation strategies and the descriptions of existing benchmark datasets. We then summarize and compare different categories of single-source unsupervised domain adaptation methods, including discrepancy-based methods, adversarial discriminative methods, adversarial generative methods, and self-supervision-based methods. Finally, we discuss future research directions with challenges and possible solutions.      
### 47.Nash Social Distancing Games with Equity Constraints: How Inequality Aversion Affects the Spread of Epidemics  [ :arrow_down: ](https://arxiv.org/pdf/2009.00146.pdf)
>  In this paper, we present a game-theoretic model describing the voluntary social distancing during the spread of an epidemic. The payoffs of the agents depend on the social distancing they practice and on the probability of getting infected. We consider two types of agents, the vulnerable agents who have a small cost if they get infected, and the non-vulnerable agents who have a higher cost. For the modeling of the epidemic outbreak, a variant of the SIR model is considered, involving populations of susceptible, infected, and recovered persons of vulnerable and non-vulnerable types. The Nash equilibria of this social distancing game are studied. We then analyze the case where the players, desiring to achieve a low social inequality, pose a bound on the variance of the payoffs. In this case, we introduce a notion of Generalized Nash Equilibrium (GNE) for games with a continuum of players and characterize the GNE. We then present some numerical results. It turns out that inequality constraints result in a slower spread of the epidemic and an improved cost for the vulnerable players. Furthermore, it is possible that inequality constraints are beneficial for non-vulnerable players as well.      
### 48.DeepSTCL: A Deep Spatio-temporal ConvLSTM for Travel Demand Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2009.00096.pdf)
>  Urban resource scheduling is an important part of the development of a smart city, and transportation resources are the main components of urban resources. Currently, a series of problems with transportation resources such as unbalanced distribution and road congestion disrupt the scheduling discipline. Therefore, it is significant to predict travel demand for urban resource dispatching. Previously, the traditional time series models were used to forecast travel demand, such as AR, ARIMA and so on. However, the prediction efficiency of these methods is poor and the training time is too long. In order to improve the performance, deep learning is used to assist prediction. But most of the deep learning methods only utilize temporal dependence or spatial dependence of data in the forecasting process. To address these limitations, a novel deep learning traffic demand forecasting framework which based on Deep Spatio-Temporal ConvLSTM is proposed in this paper. In order to evaluate the performance of the framework, an end-to-end deep learning system is designed and a real dataset is used. Furthermore, the proposed method can capture temporal dependence and spatial dependence simultaneously. The closeness, period and trend components of spatio-temporal data are used in three predicted branches. These branches have the same network structures, but do not share weights. Then a linear fusion method is used to get the final result. Finally, the experimental results on DIDI order dataset of Chengdu demonstrate that our method outperforms traditional models with accuracy and speed.      
### 49.Under Water Waste Cleaning by Mobile Edge Computing and Intelligent Image Processing Based Robotic Fish  [ :arrow_down: ](https://arxiv.org/pdf/2009.00072.pdf)
>  As water pollution is a serious threat to underwater resources, i.e., underwater plants and species, we focus on protecting the resources by cleaning the non-biodegradable waste from the water. The waste can be recycled for further usage. Here we design a robotic fish which mainly comprises optical biosensor, camera module, piston module, and wireless transceiver. By exploiting the LTE and 5G network architecture, the fish stores the information about the underwater waste in the nearest mobile edge computing server as well as in the centralized cloud server. Finally, when the fish clears the underwater waste, it offloads the captured image of the located object to the mobile edge computing server or sometimes to the cloud server for making a decision. The servers employ intelligent image processing technology and an adaptive learning process to make a decision. However, if the servers fail to make a decision, then the fish utilizes its optical biosensor. By this scheme, the time delay for clearing any water body is minimized and the waste collection capacity of the fish is maximized. This technique can effectively help the government or municipal personnel for making clean water without manual efforts.      
### 50.Accurate Prediction and Estimation of 3D-Repetitive-Trajectories using Kalman Filter, Machine Learning and Curve-Fitting Method  [ :arrow_down: ](https://arxiv.org/pdf/2009.00067.pdf)
>  Accurate estimation and prediction of trajectory is essential for the capture of any high speed target. In this paper, an extended Kalman filter (EKF) is used to track the target in the first loop of the trajectory to collect data points and then a combination of machine learning with least-square curve-fitting is used to accurately estimate future positions for the subsequent loops. The EKF estimates the current location of target from its visual information and then predicts its future position by using the observation sequence. We utilize noisy visual information of the target from the three dimensional trajectory to carry out the predictions. The proposed algorithm is developed in ROS-Gazebo environment and is implemented on hardware.      
### 51.Design and Simulation of a Hybrid Architecture for Edge Computing in 5G and Beyond  [ :arrow_down: ](https://arxiv.org/pdf/2009.00041.pdf)
>  Edge Computing in 5G and Beyond is a promising solution for ultra-low latency applications (e.g. Autonomous Vehicle, Augmented Reality, and Remote Surgery), which have an extraordinarily low tolerance for the delay and require fast data processing for a very high volume of data. The requirements of delay-sensitive applications (e.g. Low latency, proximity, and Location/Context-awareness) cannot be satisfied by Cloud Computing due to the high latency between User Equipment and Cloud. Nevertheless, Edge Computing in 5G and beyond can promise an ultra-high-speed caused by placing computation capabilities closer to endpoint devices, whereas 5G encourages the speed rate that is 200 times faster than 4G LTE-Advanced. This paper deeply investigates Edge Computing in 5G and characterizes it based on the requirements of ultra-low latency applications. As a contribution, we propose a hybrid architecture that takes advantage of novel and sustainable technologies (e.g. D2D communication, Massive MIMO, SDN, and NFV) and has major features such as scalability, reliability and ultra-low latency support. The proposed architecture is evaluated based on an agent-based simulation that demonstrates it can satisfy requirements and has the ability to respond to high volume demands with low latency.      
