# ArXiv eess --Tue, 22 Sep 2020
### 1.Imitation dynamics in population games on community networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.10020.pdf)
>  We study the asymptotic behavior of deterministic, continuous-time imitation dynamics for population games over networks. The basic assumption of this learning mechanism -- encompassing the replicator dynamics -- is that players belonging to a single population exchange information through pairwise interactions, whereby they get aware of the actions played by the other players and the corresponding rewards. Using this information, they can revise their current action, imitating the one of the players they interact with. The pattern of interactions regulating the learning process is determined by a community structure. First, the set of equilibrium points of such network imitation dynamics is characterized. Second, for the class of potential games and for undirected and connected community networks, global asymptotic convergence is proved. In particular, our results guarantee convergence to a Nash equilibrium from every fully supported initial population state in the special case when the Nash equilibria are isolated and fully supported. Examples and numerical simulations are offered to validate the theoretical results and counterexamples are discussed for scenarios when the assumptions on the community structure are not verified.      
### 2.Collaborative Target Tracking in Elliptic Coordinates: a Binocular Coordination Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.09915.pdf)
>  This paper concentrates on the collaborative target tracking control of a pair of tracking vehicles with formation constraints. The proposed controller requires only distance measurements between tracking vehicles and the target. Its novelty lies in two aspects: 1) the elliptic coordinates are used to represent an arbitrary tracking formation without singularity, which can be deduced from inter-agent distances, and 2) the regulation of the tracking vehicle system obeys a binocular coordination principle, which simplifies the design of the control law by leveraging rich physical meanings of elliptic coordinates. The tracking system with the proposed controller is proven to be exponentially convergent when the target is stationary. When the target drifts with a small velocity, the desired tracking formation is achieved within a small margin proportional to the magnitude of the target's drift velocity. Simulation examples are provided to demonstrate the tracking performance of the proposed controller.      
### 3.Detail reconstruction in binary ghost imaging by using point-by-point method  [ :arrow_down: ](https://arxiv.org/pdf/2009.09910.pdf)
>  We propose a new local-binary ghost imaging by using point-by-point method. This method can compensate the degradation of imaging quality due to the loss of information during binarization process. The numerical and experimental results show that the target details can be reconstructed well by this method when compared with traditional ghost imaging. By comparing the differences of the speckle patterns from different binarization methods, we also give the corresponding explanation. Our results may have the potential applications in areas with high requirements for imaging details, such as target recognition.      
### 4.End-to-End Speaker-Dependent Voice Activity Detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.09906.pdf)
>  Voice activity detection (VAD) is an essential pre-processing step for tasks such as automatic speech recognition (ASR) and speaker recognition. A basic goal is to remove silent segments within an audio, while a more general VAD system could remove all the irrelevant segments such as noise and even unwanted speech from non-target speakers. We define the task, which only detects the speech from the target speaker, as speaker-dependent voice activity detection (SDVAD). This task is quite common in real applications and usually implemented by performing speaker verification (SV) on audio segments extracted from VAD. In this paper, we propose an end-to-end neural network based approach to address this problem, which explicitly takes the speaker identity into the modeling process. Moreover, inference can be performed in an online fashion, which leads to low system latency. Experiments are carried out on a conversational telephone dataset generated from the Switchboard corpus. Results show that our proposed online approach achieves significantly better performance than the usual VAD/SV system in terms of both frame accuracy and F-score. We also used our previously proposed segment-level metric for a more comprehensive analysis.      
### 5.A Deep Learning Based Analysis-Synthesis Framework For Unison Singing  [ :arrow_down: ](https://arxiv.org/pdf/2009.09875.pdf)
>  Unison singing is the name given to an ensemble of singers simultaneously singing the same melody and lyrics. While each individual singer in a unison sings the same principle melody, there are slight timing and pitch deviations between the singers, which, along with the ensemble of timbres, give the listener a perceived sense of "unison". In this paper, we present a study of unison singing in the context of choirs; utilising some recently proposed deep-learning based methodologies, we analyse the fundamental frequency (F0) distribution of the individual singers in recordings of unison mixtures. Based on the analysis, we propose a system for synthesising a unison signal from an a cappella input and a single voice prototype representative of a unison mixture. We use subjective listening tests to evaluate perceptual factors of our proposed system for synthesis, including quality, adherence to the melody as well the degree of perceived unison.      
### 6.A Sequential Modelling Approach for Indoor Temperature Prediction and Heating Control in Smart Buildings  [ :arrow_down: ](https://arxiv.org/pdf/2009.09847.pdf)
>  The rising availability of large volume data has enabled a wide application of statistical Machine Learning (ML) algorithms in the domains of Cyber-Physical Systems (CPS), Internet of Things (IoT) and Smart Building Networks (SBN). This paper proposes a learning-based framework for sequentially applying the data-driven statistical methods to predict indoor temperature and yields an algorithm for controlling building heating system accordingly. This framework consists of a two-stage modelling effort: in the first stage, an univariate time series model (AR) was employed to predict ambient conditions; together with other control variables, they served as the input features for a second stage modelling where an multivariate ML model (XGBoost) was deployed. The models were trained with real world data from building sensor network measurements, and used to predict future temperature trajectories. Experimental results demonstrate the effectiveness of the modelling approach and control algorithm, and reveal the promising potential of the data-driven approach in smart building applications over traditional dynamics-based modelling methods. By making wise use of IoT sensory data and ML algorithms, this work contributes to efficient energy management and sustainability in smart buildings.      
### 7.Learning Safe Neural Network Controllers with Barrier Certificates  [ :arrow_down: ](https://arxiv.org/pdf/2009.09826.pdf)
>  We provide a novel approach to synthesize controllers for nonlinear continuous dynamical systems with control against safety properties. The controllers are based on neural networks (NNs). To certify the safety property we utilize barrier functions, which are represented by NNs as well. We train the controller-NN and barrier-NN simultaneously, achieving a verification-in-the-loop synthesis. We provide a prototype tool nncontroller with a number of case studies. The experiment results confirm the feasibility and efficacy of our approach.      
### 8.Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images  [ :arrow_down: ](https://arxiv.org/pdf/2009.09780.pdf)
>  The COVID-19 pandemic is undoubtedly one of the biggest public health crises our society has ever faced. This paper's main objectives are to demonstrate the impact of lung segmentation in COVID-19 automatic identification using CXR images and evaluate which contents of the image decisively contribute to the identification. We have performed lung segmentation using a U-Net CNN architecture, and the classification using three well-known CNN architectures: VGG, ResNet, and Inception. To estimate the impact of lung segmentation, we applied some Explainable Artificial Intelligence (XAI), such as LIME and Grad-CAM. To evaluate our approach, we built a database named RYDLS-20-v2, following our previous publication and the COVIDx database guidelines. We evaluated the impact of creating a COVID-19 CXR image database from different sources, called database bias, and the COVID-19 generalization from one database to another, representing our less biased scenario. The experimental results of the segmentation achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982. In the best and more realistic scenario, we achieved an F1-Score of 0.74 and an area under the ROC curve of 0.9 for COVID-19 identification using segmented CXR images. Further testing and XAI techniques suggest that segmented CXR images represent a much more realistic and less biased performance. More importantly, the experiments conducted show that even after segmentation, there is a strong bias introduced by underlying factors from the data sources, and more efforts regarding the creation of a more significant and comprehensive database still need to be done.      
### 9.Computation of Parameter Dependent Robust Invariant Sets for LPV Models with Guaranteed Performance  [ :arrow_down: ](https://arxiv.org/pdf/2009.09778.pdf)
>  This paper presents an iterative algorithm to compute a Robust Control Invariant (RCI) set, along with an invariance-inducing control law, for Linear Parameter-Varying (LPV) systems. As the real-time measurements of the scheduling parameters are typically available, in the presented formulation, we allow the RCI set description along with the invariance-inducing controller to be scheduling parameter dependent. The considered formulation thus leads to parameter-dependent conditions for the set invariance, which are replaced by sufficient Linear Matrix Inequality (LMI) conditions via Polya's relaxation. These LMI conditions are then combined with a novel volume maximization approach in a Semidefinite Programming (SDP) problem, which aims at computing the desirably large RCI set. In addition to ensuring invariance, it is also possible to guarantee performance within the RCI set by imposing a chosen quadratic performance level as an additional constraint in the SDP problem. The reported numerical example shows that the presented iterative algorithm can generate invariant sets which are larger than the maximal RCI sets computed without exploiting scheduling parameter information.      
### 10.DiffWave: A Versatile Diffusion Model for Audio Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2009.09761.pdf)
>  In this work, we propose DiffWave, a versatile Diffusion probabilistic model for conditional and unconditional Waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in Different Waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality~(MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.      
### 11.Low-Complexity Massive MIMO Tensor Precoding  [ :arrow_down: ](https://arxiv.org/pdf/2009.09729.pdf)
>  We present a novel and low-complexity massive multiple-input multiple-output (MIMO) precoding strategy based on novel findings concerning the subspace separability of Rician fading channels. Considering a uniform rectangular array at the base station, we show that the subspaces spanned by the channel vectors can be factorized as a tensor product between two lower dimensional subspaces. Based on this result, we formulate tensor maximum ratio transmit and zero-forcing precoders. We show that the proposed tensor precoders exhibit lower computational complexity and require less instantaneous channel state information than their linear counterparts. Finally, we present computer simulations that demonstrate the applicability of the proposed tensor precoders in practical communication scenarios.      
### 12.Improving Automated COVID-19 Grading with Convolutional Neural Networks in Computed Tomography Scans: An Ablation Study  [ :arrow_down: ](https://arxiv.org/pdf/2009.09725.pdf)
>  Amidst the ongoing pandemic, several studies have shown that COVID-19 classification and grading using computed tomography (CT) images can be automated with convolutional neural networks (CNNs). Many of these studies focused on reporting initial results of algorithms that were assembled from commonly used components. The choice of these components was often pragmatic rather than systematic. For instance, several studies used 2D CNNs even though these might not be optimal for handling 3D CT volumes. This paper identifies a variety of components that increase the performance of CNN-based algorithms for COVID-19 grading from CT images. We investigated the effectiveness of using a 3D CNN instead of a 2D CNN, of using transfer learning to initialize the network, of providing automatically computed lesion maps as additional network input, and of predicting a continuous instead of a categorical output. A 3D CNN with these components achieved an area under the ROC curve (AUC) of 0.934 on our test set of 105 CT scans and an AUC of 0.923 on a publicly available set of 742 CT scans, a substantial improvement in comparison with a previously published 2D CNN. An ablation study demonstrated that in addition to using a 3D CNN instead of a 2D CNN transfer learning contributed the most and continuous output contributed the least to improving the model performance.      
### 13.Stochastic Learning-Based Robust Beamforming Design for RIS-Aided Millimeter-Wave Systems in the Presence of Random Blockages  [ :arrow_down: ](https://arxiv.org/pdf/2009.09716.pdf)
>  A fundamental challenge for millimeter wave (mmWave) communications lies in its sensitivity to the presence of blockages, which impact the connectivity of the communication links and ultimately the reliability of the entire network. In this paper, we analyze a reconfigurable intelligent surface (RIS)-aided mmWave communication system for enhancing the network reliability and connectivity in the presence of random blockages. To enhance the robustness of hybrid analog-digital beamforming in the presence of random blockages, we formulate a stochastic optimization problem based on the minimization of the sum outage probability. To tackle the proposed optimization problem, we introduce a low-complexity algorithm based on the stochastic block gradient descent method, which learns sensible blockage patterns without searching for all combinations of potentially blocked links. Numerical results confirm the performance benefits of the proposed algorithm in terms of outage probability and effective data rate.      
### 14.When Healthcare Meets Off-the-Shelf WiFi: A Non-Wearable and Low-Costs Approach for In-Home Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2009.09715.pdf)
>  As elderly population grows, social and health care begin to face validation challenges, in-home monitoring is becoming a focus for professionals in the field. Governments urgently need to improve the quality of healthcare services at lower costs while ensuring the comfort and independence of the elderly. This work presents an in-home monitoring approach based on off-the-shelf WiFi, which is low-costs, non-wearable and makes all-round daily healthcare information available to caregivers. The proposed approach can capture fine-grained human pose figures even through a wall and track detailed respiration status simultaneously by off-the-shelf WiFi devices. Based on them, behavioral data, physiological data and the derived information (e.g., abnormal events and underlying diseases), of the elderly could be seen by caregivers directly. We design a series of signal processing methods and a neural network to capture human pose figures and extract respiration status curves from WiFi Channel State Information (CSI). Extensive experiments are conducted and according to the results, off-the-shelf WiFi devices are capable of capturing fine-grained human pose figures, similar to cameras, even through a wall and track accurate respiration status, thus demonstrating the effectiveness and feasibility of our approach for in-home monitoring.      
### 15.Flexible bandwidth 448 Gb/s DMT Transmission for Next Generation Data Center Inter-Connects  [ :arrow_down: ](https://arxiv.org/pdf/2009.09709.pdf)
>  We experimentally evaluate a flexible DMT system using 4 to 8 50-GHz-grid C-band channels to transmit 448 Gb/s over up to 240 km SSMF. VSB filtering enabled by detuned lasers significantly reduces the impact of chromatic dispersion.      
### 16.A 3-Dimensional Simplex Modulation Format with Improved OSNR Performance Compared to DP-BPSK  [ :arrow_down: ](https://arxiv.org/pdf/2009.09699.pdf)
>  The novel 3-dimensional modulation format 3D-Simplex offers potentially 1.2 dB higher OSNR tolerance than DP-DPSK while exhibiting the same spectral occupancy, modulating two bits per symbol. We verify this benefit experimen-tally and evaluate the transmission performance in a non-linear environment. All experimental results are confirmed by simulations. The benefit of 3D-Simplex is not maintained in the highly non-linear regime, but the performance is still comparable to that of DP-DPSK.      
### 17.Experimental Investigation of Discrete Multitone Transmission in the Presence of Optical Noise and Chromatic Dispersion  [ :arrow_down: ](https://arxiv.org/pdf/2009.09694.pdf)
>  Enabled by channel adaptive bit and power loading, we experimentally demonstrate discrete multitone transmission at 56Gb/s with simple intensity modulation and direct detection and achieve 50 km reach in the 1.55um window.      
### 18.DCASENET: A joint pre-trained deep neural network for detecting and classifying acoustic scenes and events  [ :arrow_down: ](https://arxiv.org/pdf/2009.09642.pdf)
>  Single task deep neural networks that perform a target task among diverse cross-related tasks in the acoustic scene and event literature are being developed. Few studies exist that investigate to combine such tasks, however, the work is at its preliminary stage. In this study, we propose an integrated deep neural network that can perform three tasks: acoustic scene classification, audio tagging, and sound event detection. Through vast experiments using three datasets, we show that the proposed system, DCASENet, itself can be directly used for any tasks with competitive results, or it can be further fine-tuned for the target task.      
### 19.Light Convolutional Neural Network with Feature Genuinization for Detection of Synthetic Speech Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2009.09637.pdf)
>  Modern text-to-speech (TTS) and voice conversion (VC) systems produce natural sounding speech that questions the security of automatic speaker verification (ASV). This makes detection of such synthetic speech very important to safeguard ASV systems from unauthorized access. Most of the existing spoofing countermeasures perform well when the nature of the attacks is made known to the system during training. However, their performance degrades in face of unseen nature of attacks. In comparison to the synthetic speech created by a wide range of TTS and VC methods, genuine speech has a more consistent distribution. We believe that the difference between the distribution of synthetic and genuine speech is an important discriminative feature between the two classes. In this regard, we propose a novel method referred to as feature genuinization that learns a transformer with convolutional neural network (CNN) using the characteristics of only genuine speech. We then use this genuinization transformer with a light CNN classifier. The ASVspoof 2019 logical access corpus is used to evaluate the proposed method. The studies show that the proposed feature genuinization based LCNN system outperforms other state-of-the-art spoofing countermeasures, depicting its effectiveness for detection of synthetic speech attacks.      
### 20.Detecting Acoustic Events Using Convolutional Macaron Net  [ :arrow_down: ](https://arxiv.org/pdf/2009.09632.pdf)
>  In this paper, we propose to address the issue of the lack of strongly labeled data by using pseudo strongly labeled data that is approximated using Convolutive Nonnegative Matrix Factorization (CNMF). Using this pseudo strongly labeled data, we then train a new architecture combining Convolutional Neural Network (CNN) with Macaron Net (MN), which we term it as Convolutional Macaron Net (CMN). As opposed to the Mean-Teacher approach which trains two similar models synchronously, we propose to train two different CMNs synchronously where one of the models will provide the frame-level prediction while the other will provide the clip level prediction. Based on our proposed framework, our system outperforms the baseline system of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge Task 4 by a margin of over 10%. By comparing with the first place of the challenge which utilize a combination of CNN and Conformer, our system also marginally wins it by 0.3%.      
### 21.Low-Cost Implementation of Bilinear and Bicubic Image Interpolation for Real-Time Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.09622.pdf)
>  Super-resolution imaging (S.R.) is a series of techniques that enhance the resolution of an imaging system, especially in surveillance cameras where simplicity and low cost are of great importance. S.R. image reconstruction can be viewed as a three-stage process: image interpolation, image registration, and fusion. Image interpolation is one of the most critical steps in the S.R. algorithms and has a significant influence on the quality of the output image. In this paper, two hardware-efficient interpolation methods are proposed for these platforms, mainly for the mobile application. Experiments and results on the synthetic and real image sequences clearly validate the performance of the proposed scheme. They indicate that the proposed approach is practically applicable to real-world applications. The algorithms are implemented in a Field Programmable Gate Array (FPGA) device using a pipelined architecture. The implementation results show the advantages of the proposed methods regarding area, performance, and output quality.      
### 22.End-to-End Bengali Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.09615.pdf)
>  Bengali is a prominent language of the Indian subcontinent. However, while many state-of-the-art acoustic models exist for prominent languages spoken in the region, research and resources for Bengali are few and far between. In this work, we apply CTC based CNN-RNN networks, a prominent deep learning based end-to-end automatic speech recognition technique, to the Bengali ASR task. We also propose and evaluate the applicability and efficacy of small 7x3 and 3x3 convolution kernels which are prominently used in the computer vision domain primarily because of their FLOPs and parameter efficient nature. We propose two CNN blocks, 2-layer Block A and 4-layer Block B, with the first layer comprising of 7x3 kernel and the subsequent layers comprising solely of 3x3 kernels. Using the publicly available Large Bengali ASR Training data set, we benchmark and evaluate the performance of seven deep neural network configurations of varying complexities and depth on the Bengali ASR task. Our best model, with Block B, has a WER of 13.67, having an absolute reduction of 1.39% over comparable model with larger convolution kernels of size 41x11 and 21x11.      
### 23.Reconstruct high-resolution multi-focal plane images from a single 2D wide field image  [ :arrow_down: ](https://arxiv.org/pdf/2009.09574.pdf)
>  High-resolution 3D medical images are important for analysis and diagnosis, but axial scanning to acquire them is very time-consuming. In this paper, we propose a fast end-to-end multi-focal plane imaging network (MFPINet) to reconstruct high-resolution multi-focal plane images from a single 2D low-resolution wild filed image without relying on scanning. To acquire realistic MFP images fast, the proposed MFPINet adopts generative adversarial network framework and the strategies of post-sampling and refocusing all focal planes at one time. We conduct a series experiments on cytology microscopy images and demonstrate that MFPINet performs well on both axial refocusing and horizontal super resolution. Furthermore, MFPINet is approximately 24 times faster than current refocusing methods for reconstructing the same volume images. The proposed method has the potential to greatly increase the speed of high-resolution 3D imaging and expand the application of low-resolution wide-field images.      
### 24.Open-set Short Utterance Forensic Speaker Verification using Teacher-Student Network with Explicit Inductive Bias  [ :arrow_down: ](https://arxiv.org/pdf/2009.09556.pdf)
>  In forensic applications, it is very common that only small naturalistic datasets consisting of short utterances in complex or unknown acoustic environments are available. In this study, we propose a pipeline solution to improve speaker verification on a small actual forensic field dataset. By leveraging large-scale out-of-domain datasets, a knowledge distillation based objective function is proposed for teacher-student learning, which is applied for short utterance forensic speaker verification. The objective function collectively considers speaker classification loss, Kullback-Leibler divergence, and similarity of embeddings. In order to advance the trained deep speaker embedding network to be robust for a small target dataset, we introduce a novel strategy to fine-tune the pre-trained student model towards a forensic target domain by utilizing the model as a finetuning start point and a reference in regularization. The proposed approaches are evaluated on the 1st48-UTD forensic corpus, a newly established naturalistic dataset of actual homicide investigations consisting of short utterances recorded in uncontrolled conditions. We show that the proposed objective function can efficiently improve the performance of teacher-student learning on short utterances and that our fine-tuning strategy outperforms the commonly used weight decay method by providing an explicit inductive bias towards the pre-trained model.      
### 25.State-of-Charge Estimation of a Li-Ion Battery using Deep Forward Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.09543.pdf)
>  This article presents two Deep Forward Networks with two and four hidden layers, respectively, that model the drive cycle of a Panasonic 18650PF lithium-ion (Li-ion) battery at a given temperature using the K-fold cross-validation method, in order to estimate the State of Charge (SOC) of the cell. The drive cycle power profile is calculated for an electric truck with a 35kWh battery pack scaled for a single 18650PF cell. We propose a machine learning workflow which is able to fight overfitting when developing deep learning models for SOC estimation. The contribution of this work is to present a methodology of building a Deep Forward Network for a lithium-ion battery and its performance assessment, which follows the best practices in machine learning.      
### 26.Safety-Critical Online Control with Adversarial Disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2009.09511.pdf)
>  This paper studies the control of safety-critical dynamical systems in the presence of adversarial disturbances. We seek to synthesize state-feedback controllers to minimize a cost incurred due to the disturbance, while respecting a safety constraint. The safety constraint is given by a bound on an H-inf norm, while the cost is specified as an upper bound on the H-2 norm of the system. We consider an online setting where costs at each time are revealed only after the controller at that time is chosen. We propose an iterative approach to the synthesis of the controller by solving a modified discrete-time Riccati equation. Solutions of this equation enforce the safety constraint. We compare the cost of this controller with that of the optimal controller when one has complete knowledge of disturbances and costs in hindsight. We show that the regret function, which is defined as the difference between these costs, varies logarithmically with the time horizon. We validate our approach on a process control setup that is subject to two kinds of adversarial attacks.      
### 27.Underlaid FD D2D Communications in MU-MIMO Systems via Joint Beamforming and Power Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2009.09502.pdf)
>  This paper studies the benefits of incorporating underlaid full-duplex (FD) D2D communications into multi-user multiple-input-multiple-output (MU-MIMO) cellular systems in terms of achievable network throughput. The focus is joint beamforming and power allocation design for average sum-rate (per cell) maximization while considering the effects of interference to both cellular and D2D transmission. The problem formulation leads to a nonconvex vector-variable optimization problem, where we develop an efficient solution using a fractional programming (FP) based approach. Numerical results show that, at sufficiently high self-interference cancellation (SIC) levels, the FD D2D transmission provides a significant sum-rate improvement as compared to the half-duplex (HD) counterpart and pure cellular systems in absence of D2D transmission.      
### 28.Distributed optimization with tunable learned priors for robust ptycho-tomography  [ :arrow_down: ](https://arxiv.org/pdf/2009.09498.pdf)
>  Joint ptycho-tomography is a powerful computational imaging framework to recover the refractive properties of a 3D object while relaxing the requirements for probe overlap that is common in conventional phase retrieval. We use an augmented Lagrangian scheme for formulating the constrained optimization problem and employ an alternating direction method of multipliers (ADMM) for the joint solution. ADMM allows the problem to be split into smaller and computationally more manageable subproblems: ptychographic phase retrieval, tomographic reconstruction, and regularization of the solution. We extend our ADMM framework with plug-and-play (PnP) denoisers by replacing the regularization subproblem with a general denoising operator based on machine learning. While the PnP framework enables integrating such learned priors as denoising operators, tuning of the denoiser prior remains challenging. To overcome this challenge, we propose a tuning parameter to control the effect of the denoiser and to accelerate the solution. In our simulations, we demonstrate that our proposed framework with parameter tuning and learned priors generates high-quality reconstructions under limited and noisy measurement data.      
### 29.Tight Bounds for Uncertain Time-Correlated Errors with Gauss-Markov Structure  [ :arrow_down: ](https://arxiv.org/pdf/2009.09495.pdf)
>  Safety-critical navigation applications require that estimation errors be reliably quantified and bounded. This can be challenging for linear dynamic systems if the process noise or measurement errors have uncertain time correlation. In many systems (e.g., in satellite-based or inertial navigation systems), there are sources of time-correlated sensor errors that can be well modeled using Gauss-Markov processes (GMP). However, uncertainty in the GMP parameters, particularly in the correlation time constant, can cause misleading error estimation. In this paper, we develop new time-correlated models that ensure tight upper bounds on the estimation error variance, assuming that the actual error is a stationary GMP with a time constant that is only known to reside within an interval. We first use frequency-domain analysis to derive a stationary GMP model both in continuous and discrete time domain, which outperforms models previously described in the literature. Then, we achieve an even tighter estimation error bound using a non-stationary GMP model, for which we determine the minimum initial variance that guarantees bounding conditions. In both cases, the model can easily be implemented in a linear estimator like a Kalman filter.      
### 30.Long Range Communication on Batteryless Devices  [ :arrow_down: ](https://arxiv.org/pdf/2009.09487.pdf)
>  Bulk of the existing Wireless Sensor Network (WSN) nodes are usually battery powered, stationary and mostly designed for short distance communication, with little to no consideration for constrained devices that operate solely on harvested energy. On many occasions, batteries and beefy super-capacitors are used to power these WSN, but these systems are prone to service-life degradation and current-leakages. Most of the systems implementing super capacitors do not account for leakages after exceeding the charge cycle threshold. Frequent battery maintenance and replacement at scale is non-trivial, labor-intensive and challenging task, especially on sensing nodes deployed in extreme harsh environments with limited human intervention. In this paper, we present the technique for achieving Kilometer range communication on batteryless constraint devices by harnessing the capabilities of LoRa technology.      
### 31.3D Aerial Highway: The Key Enabler of the Retail Industry Transformation  [ :arrow_down: ](https://arxiv.org/pdf/2009.09477.pdf)
>  The retail industry is already facing an inevitable and significant transformation worldwide, and with the current pandemic situation, it is even accelerating. Indeed, consumer habits are shifting from brick-and-mortar stores to online shopping. The bottleneck in the end-to-end online shopping experience remains the efficient and quick delivery of goods to consumers. In this context, unmanned aerial vehicle (UAV) technology is seen as a potential solution to address cargo delivery issues. Hence, the number of cargo-UAVs is expected to skyrocket in the next decade and the airspace to become densely crowded. To successfully deploy UAV technology for mass cargo deliveries, a challenge remains to provide seamless and reliable cellular connectivity for command and control of highly mobile and flexible aircraft. There is an urgent need for organized and "connected" routes in the sky. Like highways for cargo trucks, 3D routes in the airspace will be required for cargo-UAVs so that they can fulfill their operations safely and efficiently. We refer to these routes here as "3D aerial highways". In this paper, we thoroughly investigate the feasibility of the aerial highways paradigm. First, we discuss the motivations and concerns of the aerial highway paradigm. Then, we present our vision of the 3D aerial highway framework. Finally, we discuss related connectivity issues and their solutions.      
### 32.Multi-Objective Optimization of Distribution Networks via Daily Reconfiguration  [ :arrow_down: ](https://arxiv.org/pdf/2009.09472.pdf)
>  This paper presents a comprehensive approach to improve the daily performance of an active distribution network (ADN), which includes renewable resources and responsive load (RL), using distributed network reconfiguration (DNR). Optimization objectives in this work can be described as (i) reducing active losses, (ii) improving the voltage profile, (iii) improving the network reliability, and (iv) minimizing distribution network operation costs. The suggested approach takes into account the probability of renewable resource failure, given the information collected from their initial state at the beginning of every day, in solving the optimization problem. Furthermore, solar radiation variations are estimated based on past historical data and the impact of the performance of renewable sources such as photovoltaic (PV) is determined hourly based on the Markov model. Since the number of reconfiguration scenarios is very high, stochastic DNR (SDNR) based on the probability distance method is employed to shrink the scenarios set. At the final stage, an improved crow search algorithm (ICSA) is introduced to find the optimal scenario. The effectiveness of the suggested method is verified for the IEEE 33-bus radial distribution system as a case study.      
### 33.A Markovian Model-Driven Deep Learning Framework for Massive MIMO CSI Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2009.09468.pdf)
>  Forward channel state information (CSI) often plays a vital role in scheduling and capacity-approaching transmission optimization for massive multiple-input multiple-output (MIMO) communication systems. In frequency division duplex (FDD) massive MIMO systems, forwardlink CSI reconstruction at the transmitter relies critically on CSI feedback from receiving nodes and must carefully weigh the tradeoff between reconstruction accuracy and feedback bandwidth. Recent studies on the use of recurrent neural networks (RNNs) have demonstrated strong promises, though the cost of computation and memory remains high, for massive MIMO deployment. In this work, we exploit channel coherence in time to substantially improve the feedback efficiency. Using a Markovian model, we develop a deep convolutional neural network (CNN)-based framework MarkovNet to differentially encode forward CSI in time to effectively improve reconstruction accuracy. Furthermore, we explore important physical insights, including spherical normalization of input data and convolutional layers for feedback compression. We demonstrate substantial performance improvement and complexity reduction over the RNN-based work by our proposed MarkovNet to recover forward CSI estimates accurately. We explore additional practical consideration in feedback quantization, and show that MarkovNet outperforms RNN-based CSI estimation networks at a fraction of the computational cost.      
### 34.What Role Do Intelligent Reflecting Surfaces Play in Multi-Antenna Non-Orthogonal Multiple Access?  [ :arrow_down: ](https://arxiv.org/pdf/2009.09459.pdf)
>  Massive multiple-input multiple-output (MIMO) and non-orthogonal multiple access (NOMA) are two key techniques for enabling massive connectivity in future wireless networks. A massive MIMO-NOMA system can deliver remarkable spectral improvements and low communication latency. Nevertheless, the uncontrollable stochastic behavior of the wireless channels can still degrade its performance. In this context, intelligent reflecting surface (IRS) has arisen as a promising technology for smartly overcoming the harmful effects of the wireless environment. The disruptive IRS concept of controlling the propagation channels via software can provide attractive performance gains to the communication networks, including higher data rates, improved user fairness, and, possibly, higher energy efficiency. In this article, in contrast to the existing literature, we demonstrate the main roles of IRSs in MIMO-NOMA systems. Specifically, we identify and perform a comprehensive discussion of the main performance gains that can be achieved in IRS-assisted massive MIMO-NOMA (IRS-NOMA) networks. We outline exciting futuristic use case scenarios for IRS-NOMA and expose the main related challenges and future research directions. Furthermore, throughout the article, we support our in-depth discussions with representative numerical results.      
### 35.Physical Layer Security Over Mixture Gamma Distributed Fading Channels With Discrete Inputs: A Unified and General Analytical Framework  [ :arrow_down: ](https://arxiv.org/pdf/2009.09415.pdf)
>  Physical layer security is investigated over mixture Gamma (MG) distributed fading channels with discrete inputs. By the Gaussian quadrature rules, closed-form expressions are derived to characterize the average secrecy rate (ASR) and secrecy outage probability (SOP), whose accuracy is validated by numerical simulations. To show more properties of the finite-alphabet signaling, we perform an asymptotic analysis on the secrecy metrics in the large limit of the average signal-to-noise ratio (SNR) of the main channel. Leveraging the Mellin transform, we find that the ASR and SOP converge to some constants as the average SNR increases and we derive novel expressions to characterize the rates of convergence. This work establishes a unified and general analytical framework for the secrecy performance achieved by discrete inputs.      
### 36.Learning-Based Massive Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2009.09406.pdf)
>  Developing resource allocation algorithms with strong real-time and high efficiency has been an imperative topic in wireless networks. Conventional optimization-based iterative resource allocation algorithms often suffer from slow convergence, especially for massive multiple-input-multiple-output (MIMO) beamforming problems. This paper studies learning-based efficient massive beamforming methods for multi-user MIMO networks. The considered massive beamforming problem is challenging in two aspects. First, the beamforming matrix to be learned is quite high-dimensional in case with a massive number of antennas. Second, the objective is often time-varying and the solution space is not fixed due to some communication requirements. All these challenges make learning representation for massive beamforming an extremely difficult task. In this paper, by exploiting the structure of the most popular WMMSE beamforming solution, we propose convolutional massive beamforming neural networks (CMBNN) using both supervised and unsupervised learning schemes with particular design of network structure and input/output. Numerical results demonstrate the efficacy of the proposed CMBNN in terms of running time and system throughput.      
### 37.Accelerating Auxiliary Function-based Independent Vector Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.09402.pdf)
>  Independent Vector Analysis (IVA) is an effective approach for Blind Source Separation (BSS) of convolutive mixtures of audio signals. As a practical realization of an IVA-based BSS algorithm, the so-called AuxIVA update rules based on the Majorize-Minimize (MM) principle have been proposed which allow for fast and computationally efficient optimization of the IVA cost function. For many real-time applications, however, update rules for IVA exhibiting even faster convergence are highly desirable. To this end, we investigate techniques which accelerate the convergence of the AuxIVA update rules without extra computational cost. The efficacy of the proposed methods is verified in experiments representing real-world acoustic scenarios.      
### 38.Far-Field Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.09395.pdf)
>  The machine recognition of speech spoken at a distance from the microphones, known as far-field automatic speech recognition (ASR), has received a significant increase of attention in science and industry, which caused or was caused by an equally significant improvement in recognition accuracy. Meanwhile it has entered the consumer market with digital home assistants with a spoken language interface being its most prominent application. Speech recorded at a distance is affected by various acoustic distortions and, consequently, quite different processing pipelines have emerged compared to ASR for close-talk speech. A signal enhancement front-end for dereverberation, source separation and acoustic beamforming is employed to clean up the speech, and the back-end ASR engine is robustified by multi-condition training and adaptation. We will also describe the so-called end-to-end approach to ASR, which is a new promising architecture that has recently been extended to the far-field scenario. This tutorial article gives an account of the algorithms used to enable accurate speech recognition from a distance, and it will be seen that, although deep learning has a significant share in the technological breakthroughs, a clever combination with traditional signal processing can lead to surprisingly effective solutions.      
### 39.Stochastic Model Predictive Control with a Safety Guarantee for Automated Driving  [ :arrow_down: ](https://arxiv.org/pdf/2009.09381.pdf)
>  Automated vehicles require efficient and safe planning to maneuver in uncertain environments. Largely this uncertainty is caused by other traffic participants, e.g., surrounding vehicles. Future motion of surrounding vehicles is often difficult to predict. Whereas robust control approaches achieve safe, yet conservative motion planning for automated vehicles, Stochastic Model Predictive Control (SMPC) provides efficient planning in the presence of uncertainty. Probabilistic constraints are applied to ensure that the maximal risk remains below a predefined level. However, safety cannot be ensured as probabilistic constraints may be violated, which is not acceptable for automated vehicles. Here, we propose an efficient trajectory planning framework with safety guarantees for automated vehicles. SMPC is applied to obtain efficient vehicle trajectories for a finite horizon. Based on the first optimized SMPC input, a guaranteed safe backup trajectory is planned, using reachable sets. The SMPC input is only applied to the vehicle if a safe backup solution can be found. If no new safe backup solution can be found, the previously calculated, still valid safe backup solution is applied instead of the SMPC solution. Recursive feasibility of the safe SMPC algorithm is proved. Highway simulations show the effectiveness of the proposed method regarding performance and safety.      
### 40.Hybrid Beamforming for RIS-Empowered Multi-hop Terahertz Communications: A DRL-based Method  [ :arrow_down: ](https://arxiv.org/pdf/2009.09380.pdf)
>  Wireless communication in the TeraHertz band (0.1--10 THz) is envisioned as one of the key enabling technologies for the future six generation (6G) wireless communication systems. However, very high propagation attenuations and molecular absorptions of THz frequencies often limit the signal transmission distance and coverage range. Benefited from the recent breakthrough on the reconfigurable intelligent surfaces (RIS) for realizing smart radio propagation environment, we propose a novel hybrid beamforming scheme for the multi-hop RIS-assisted communication networks to improve the coverage range at THz-band frequencies. We investigate the joint design of digital beamforming matrix at the BS and analog beamforming matrices at the RISs, by leveraging the recent advances in deep reinforcement learning (DRL) to combat the propagation loss. Simulation results show that our proposed scheme is able to improve 50\% more coverage range of THz communications compared with the benchmarks. Furthermore, it is also shown that our proposed DRL-based method is a state-of-the-art method to solve the NP-bard beamforming problem, especially when the signals at RIS-empowered THz communication networks experience multiple hops.      
### 41.Lyapunov-Based Reinforcement Learning for Decentralized Multi-Agent Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.09361.pdf)
>  Decentralized multi-agent control has broad applications, ranging from multi-robot cooperation to distributed sensor networks. In decentralized multi-agent control, systems are complex with unknown or highly uncertain dynamics, where traditional model-based control methods can hardly be applied. Compared with model-based control in control theory, deep reinforcement learning (DRL) is promising to learn the controller/policy from data without the knowing system dynamics. However, to directly apply DRL to decentralized multi-agent control is challenging, as interactions among agents make the learning environment non-stationary. More importantly, the existing multi-agent reinforcement learning (MARL) algorithms cannot ensure the closed-loop stability of a multi-agent system from a control-theoretic perspective, so the learned control polices are highly possible to generate abnormal or dangerous behaviors in real applications. Hence, without stability guarantee, the application of the existing MARL algorithms to real multi-agent systems is of great concern, e.g., UAVs, robots, and power systems, etc. In this paper, we aim to propose a new MARL algorithm for decentralized multi-agent control with a stability guarantee. The new MARL algorithm, termed as a multi-agent soft-actor critic (MASAC), is proposed under the well-known framework of "centralized-training-with-decentralized-execution". The closed-loop stability is guaranteed by the introduction of a stability constraint during the policy improvement in our MASAC algorithm. The stability constraint is designed based on Lyapunov's method in control theory. To demonstrate the effectiveness, we present a multi-agent navigation example to show the efficiency of the proposed MASAC algorithm.      
### 42.Planning Low-carbon Distributed Power Systems: Evaluating the Role of Energy Storage  [ :arrow_down: ](https://arxiv.org/pdf/2009.09325.pdf)
>  We introduce a mathematical formulation of energy storage systems into a generation capacity expansion framework to evaluate the role of energy storage in the decarbonization of distributed power systems. The modeling framework accounts for dynamic charging/discharging efficiencies and maximum cycling powers as well as cycle and calendar degradation of a Li-ion battery system. Results from a single node case study indicate that incorporating the dynamic efficiencies and cycling powers of batteries in the generation planning problem does not significantly change the optimal generation portfolio, while adding substantial computational burden. In contrast, accounting for battery degradation leads to substantially different generation expansion outcomes, especially in deep decarbonization scenarios with larger energy storage capacities. Under the assumptions used in this study, we find that battery energy storage is economically viable for 2020 only under strict CO$_2$ emission constraints. In contrast, given the projected technology advances and corresponding cost reductions, battery energy storage exhibits an attractive option to enable deep decarbonization in 2050.      
### 43.Randomized Subspace Newton Convex Method Applied to Data-Driven Sensor Selection Problem  [ :arrow_down: ](https://arxiv.org/pdf/2009.09315.pdf)
>  The randomized subspace Newton convex methods for the sensor selection problem are proposed. The randomized subspace Newton algorithm is straightforwardly applied to the convex formulation, and the customized method in which the half of the update variables are selected to be the present best sensor candidates is also considered. In the converged solution, almost the same results are obtained by original and randomized-subspace-Newton convex methods. As expected, the randomized-subspace-Newton methods require more computational steps while they reduce the total amount of the computational time because the computational time for one step is significantly reduced by the cubic of the ratio of numbers of randomly updating variables to all the variables. The customized method shows superior performance to the straightforward implementation in terms of the quality of sensors and the computational time.      
### 44.Michelson Holography: Dual-SLM Holography with Camera-in-the-loop Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2009.09302.pdf)
>  We introduce Michelson Holography (MH), a holographic display technology that optimizes image quality for emerging holographic near-eye displays. Using two spatial light modulators, MH is capable of leveraging destructive interference to optically cancel out undiffracted light corrupting the observed image. We calibrate this system using emerging camera-in-the-loop holography techniques and demonstrate state-of-the-art holographic 2D image quality.      
### 45.Reducing false-positive biopsies with deep neural networks that utilize local and global information in screening mammograms  [ :arrow_down: ](https://arxiv.org/pdf/2009.09282.pdf)
>  Breast cancer is the most common cancer in women, and hundreds of thousands of unnecessary biopsies are done around the world at a tremendous cost. It is crucial to reduce the rate of biopsies that turn out to be benign tissue. In this study, we build deep neural networks (DNNs) to classify biopsied lesions as being either malignant or benign, with the goal of using these networks as second readers serving radiologists to further reduce the number of false positive findings. We enhance the performance of DNNs that are trained to learn from small image patches by integrating global context provided in the form of saliency maps learned from the entire image into their reasoning, similar to how radiologists consider global context when evaluating areas of interest. Our experiments are conducted on a dataset of 229,426 screening mammography exams from 141,473 patients. We achieve an AUC of 0.8 on a test set consisting of 464 benign and 136 malignant lesions.      
### 46.Bias Field Poses a Threat to DNN-based X-Ray Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.09247.pdf)
>  The chest X-ray plays a key role in screening and diagnosis of many lung diseases including the COVID-19. More recently, many works construct deep neural networks (DNNs) for chest X-ray images to realize automated and efficient diagnosis of lung diseases. However, bias field caused by the improper medical image acquisition process widely exists in the chest X-ray images while the robustness of DNNs to the bias field is rarely explored, which definitely poses a threat to the X-ray-based automated diagnosis system. In this paper, we study this problem based on the recent adversarial attack and propose a brand new attack, i.e., the adversarial bias field attack where the bias field instead of the additive noise works as the adversarial perturbations for fooling the DNNs. This novel attack posts a key problem: how to locally tune the bias field to realize high attack success rate while maintaining its spatial smoothness to guarantee high realisticity. These two goals contradict each other and thus has made the attack significantly challenging. To overcome this challenge, we propose the adversarial-smooth bias field attack that can locally tune the bias field with joint smooth &amp; adversarial constraints. As a result, the adversarial X-ray images can not only fool the DNNs effectively but also retain very high level of realisticity. We validate our method on real chest X-ray datasets with powerful DNNs, e.g., ResNet50, DenseNet121, and MobileNet, and show different properties to the state-of-the-art attacks in both image realisticity and attack transferability. Our method reveals the potential threat to the DNN-based X-ray automated diagnosis and can definitely benefit the development of bias-field-robust automated diagnosis system.      
### 47.Detailed Dynamic Model of Antagonistic PAM System and its Experimental Validation: Sensor-less Angle and Torque Control with UKF  [ :arrow_down: ](https://arxiv.org/pdf/2009.09229.pdf)
>  This study proposes a detailed nonlinear mathematical model of an antagonistic pneumatic artificial muscle (PAM) actuator system for estimating the joint angle and torque using an unscented Kalman filter (UKF). The proposed model is described in a hybrid state-space representation. It includes the contraction force of the PAM, joint dynamics, fluid dynamics of compressed air, mass flows of a valve, and friction models. A part of the friction models is modified to obtain a novel form of Coulomb friction depending on the inner pressure of the PAM. For model validation, offline and online UKF estimations and sensor-less tracking control of the joint angle and torque are conducted to evaluate the estimation accuracy and tracking control performance. The estimation accuracy is less than 7.91 %, and the steady-state tracking control performance is more than 94.75 %. These results confirm that the proposed model is detailed and could be used as the state estimator of an antagonistic PAM system.      
### 48.Improving Spiking Sparse Recovery via Non-Convex Penalties  [ :arrow_down: ](https://arxiv.org/pdf/2009.09163.pdf)
>  Compared with digital methods, sparse recovery based on spiking neural networks has great advantages like high computational efficiency and low power-consumption. However, current spiking algorithms cannot guarantee more accurate estimates since they are usually designed to solve the classical optimization with convex penalties, especially the $\ell_{1}$-norm. In fact, convex penalties are observed to underestimate the true solution in practice, while non-convex ones can avoid the underestimation. Inspired by this, we propose an adaptive version of spiking sparse recovery algorithm to solve the non-convex regularized optimization, and provide an analysis on its global asymptotic convergence. Through experiments, the accuracy is greatly improved under different adaptive ways.      
### 49.Lossless White Balance For Improved Lossless CFA Image and Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2009.09137.pdf)
>  Color filter array is spatial multiplexing of pixel-sized filters placed over pixel detectors in camera sensors. The state-of-the-art lossless coding techniques of raw sensor data captured by such sensors leverage spatial or cross-color correlation using lifting schemes. In this paper, we propose a lifting-based lossless white balance algorithm. When applied to the raw sensor data, the spatial bandwidth of the implied chrominance signals decreases. We propose to use this white balance as a pre-processing step to lossless CFA subsampled image/video compression, improving the overall coding efficiency of the raw sensor data.      
### 50.Morphological Reconstruction Improves Microvessel Mapping in Super-Resolution Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2009.09129.pdf)
>  Generation of super-resolution (SR) ultrasound (US) images, created from the successive local-ization of individual microbubbles in the circulation, has enabled the visualization of microvascular structure and flow at a level of detail that was not possible previously. Despite rapid progress, tradeoffs between spatial and temporal resolution may challenge the translation of this promising technology to the clinic. To temper these trade-offs, we propose a method based on morphological image reconstriction. This method can extract from ultrafast contrast-enhanced ultrasound (CEUS) images hundreds of microbubble peaks per image (312-by-180 pixels) with intensity values varying by an order of magnitude. Specifically, it offers a fourfold increase in the number of peaks detected per frame, requires on the order of 100 ms for processing, and is robust to additive electronic noise (down to 3.6 dB CNR in CEUS images). By integrating this method to a SR framework we demonstrate a 6-fold improvement in spatial resolution, as compared to CEUS, in imaging chicken embryo microvessels. This method that is computationally efficient and, thus, scalable to large data sets, may augment the abilities of SR-US in imaging microvascular structure and function.      
### 51.Analysis of artifacts in EEG signals for building BCIs  [ :arrow_down: ](https://arxiv.org/pdf/2009.09116.pdf)
>  Brain-Computer Interface (BCI) is an essential mechanism that interprets the human brain signal. It provides an assistive technology that enables persons with motor disabilities to communicate with the world and also empowers them to lead independent lives. The common BCI devices use Electroencephalography (EEG) electrical activity recorded from the scalp. EEG signals are noisy owing to the presence of many artifacts, namely, eye blink, head movement, and jaw movement. Such artifacts corrupt the EEG signal and make EEG analysis challenging. This issue is addressed by locating the artifacts and excluding the EEG segment from the analysis, which could lead to a loss of useful information. However, we propose a practical BCI that uses the artifacts which has a low signal to noise ratio. <br>The objective of our work is to classify different types of artifacts, namely eye blink, head nod, head turn, and jaw movements in the EEG signal. The occurrence of the artifacts is first located in the EEG signal. The located artifacts are then classified using linear time and dynamic time warping techniques. The located artifacts can be used by a person with a motor disability to control a smartphone. A speech synthesis application that uses eyeblinks in a single channel EEG system and jaw clinches in four channels EEG system are developed. Word prediction models are used for word completion, thus reducing the number of artifacts required.      
### 52.A Convex Neural Network Solver for DCOPF with Generalization Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2009.09109.pdf)
>  The DC optimal power flow (DCOPF) problem is a fundamental problem in power systems operations and planning. With high penetration of uncertain renewable resources in power systems, DCOPF needs to be solved repeatedly for a large amount of scenarios, which can be computationally challenging. As an alternative to iterative solvers, neural networks are often trained and used to solve DCOPF. These approaches can offer orders of magnitude reduction in computational time, but they cannot guarantee generalization, and small training error does not imply small testing errors. In this work, we propose a novel algorithm for solving DCOPF that guarantees the generalization performance. First, by utilizing the convexity of DCOPF problem, we train an input convex neural network. Second, we construct the training loss based on KKT optimality conditions. By combining these two techniques, the trained model has provable generalization properties, where small training error implies small testing errors. In experiments, our algorithm improves the optimality ratio of the solutions by a factor of five in comparison to end-to-end models.      
### 53.Safety-Critical Kinematic Control of Robotic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.09100.pdf)
>  Over the decades, kinematic controllers have proven to be practically useful for applications like set-point and trajectory tracking in robotic systems. To this end, we formulate a novel safety-critical paradigm for kinematic control in this paper. In particular, we extend the methodology of control barrier functions (CBFs) to kinematic equations governing robotic systems. We demonstrate a purely kinematic implementation of a velocity-based CBF, and subsequently introduce a formulation that guarantees safety at the level of dynamics. This is achieved through a new form CBFs that incorporate kinetic energy with the classical forms, thereby minimizing model dependence and conservativeness. The approach is then extended to underactuated systems. This method and the purely kinematic implementation are demonstrated in simulation on two robotic platforms: a 6-DOF robotic manipulator, and a cart-pole system.      
### 54.Referenceless Rate-Distortion Modeling with Learning from Bitstream and Pixel Features  [ :arrow_down: ](https://arxiv.org/pdf/2009.09056.pdf)
>  Generally, adaptive bitrates for variable Internet bandwidths can be obtained through multi-pass coding. Referenceless prediction-based methods show practical benefits compared with multi-pass coding to avoid excessive computational resource consumption, especially in low-latency circumstances. However, most of them fail to predict precisely due to the complex inner structure of modern codecs. Therefore, to improve the fidelity of prediction, we propose a referenceless prediction-based R-QP modeling (PmR-QP) method to estimate bitrate by leveraging a deep learning algorithm with only one-pass coding. It refines the global rate-control paradigm in modern codecs on flexibility and applicability with few adjustments as possible. By exploring the potentials of bitstream and pixel features from the prerequisite of one-pass coding, it can reach the expectation of bitrate estimation in terms of precision. To be more specific, we first describe the R-QP relationship curve as a robust quadratic R-QP modeling function derived from the Cauchy-based distribution. Second, we simplify the modeling function by fastening one operational point of the relationship curve received from the coding process. Third, we learn the model parameters from bitstream and pixel features, named them hybrid referenceless features, comprising texture information, hierarchical coding structure, and selected modes in intra-prediction. Extensive experiments demonstrate the proposed method significantly decreases the proportion of samples' bitrate estimation error within 10\% by 24.60\% on average over the state-of-the-art.      
### 55.Prediction and Optimal Feedback Steering of Probability Density Functions for Safe Automated Driving  [ :arrow_down: ](https://arxiv.org/pdf/2009.09055.pdf)
>  We propose a stochastic prediction-control framework to promote safety in automated driving by directly controlling the joint state probability density functions (PDFs) subject to the vehicle dynamics via trajectory-level state feedback. To illustrate the main ideas, we focus on a multi-lane highway driving scenario although the proposed framework can be adapted to other contexts. The computational pipeline consists of a PDF prediction layer, followed by a PDF control layer. The prediction layer performs moving horizon nonparametric forecasts for the ego and the non-ego vehicles' stochastic states, and thereby derives safe target PDF for the ego. The latter is based on the forecasted collision probabilities, and promotes the probabilistic safety for the ego. The PDF control layer designs a feedback that optimally steers the joint state PDF subject to the controlled ego dynamics while satisfying the endpoint PDF constraints. Our computation for the PDF prediction layer leverages the structure of the controlled Liouville PDE to evolve the joint PDF values, as opposed to empirically approximating the PDFs. Our computation for the PDF control layer leverages the differential flatness structure in vehicle dynamics. We harness recent theoretical and algorithmic advances in optimal mass transport, and the Schrödinger bridge. The numerical simulations illustrate the efficacy of the proposed framework.      
### 56.Combined approach for automatic and robust calculation of dominant frequency of electrogastrogram  [ :arrow_down: ](https://arxiv.org/pdf/2009.09023.pdf)
>  We present a novel method for automatic and robust detection of dominant frequency (DF) in the electrogastrogram (EGG). Our new approach combines Fast Fourier Transform (FFT), Welch's method for spectral density estimation, and autocorrelation. The proposed combined method as well as other separate procedures were tested on a freely available dataset consisted of EGG recordings in 20 healthy individuals. DF was calculated in relation (1) to the fasting and postprandial states, (2) to the three recording locations, and (3) to the subjects' body mass index. For the estimation of algorithms performance in the presence of noise, we created a synthetic dataset by adding white Gaussian noise to the artifact-free EGG waveform in one subject. The individual algorithms and novel combined approach were evaluated in relation to the signal-to-noise ratio (SNR) in range from -40 dB to 20 dB. Our results showed that the novel combined method significantly outperformed the commonly used approach for DF calculation - FFT in noise presence when compared to the benchmark data being was manually corrected by an expert. The novel method outperformed autocorrelation and Welch's method in accuracy. Additionally, we presented a method for optimal window width selection when using Welch's spectrogram that showed that for DF detection, window length of N/4 (300 s), where N is the length of EGG waveform in samples, performed the best when compared to the benchmark data. The combined approach proved efficient for automatic and robust calculation of dominant frequency on openly available EGG dataset recorded in healthy individuals and is promising approach for DF detection.      
### 57.Natural Graph Wavelet Packet Dictionaries  [ :arrow_down: ](https://arxiv.org/pdf/2009.09020.pdf)
>  We introduce a set of novel multiscale basis transforms for signals on graphs that utilize their "dual" domains by incorporating the "natural" distances between graph Laplacian eigenvectors, rather than simply using the eigenvalue ordering. These basis dictionaries can be seen as generalizations of the classical Shannon wavelet packet dictionary and of classical time-frequency adapted wavelet packets to arbitrary graphs, and does not rely on the frequency interpretation of Laplacian eigenvalues. We describe the algorithms (involving either vector rotations or orthogonalizations) to efficiently approximate and compress signals through the best-basis algorithm, and demonstrate the strengths of these basis dictionaries for graph signals on sunflower graphs and road traffic networks.      
### 58.Reflection-mode virtual histology using photoacoustic remote sensing microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.10010.pdf)
>  Histological visualizations are critical to clinical disease management and are fundamental to biological understanding. However, current approaches that rely on bright-field microscopy require extensive tissue preparation prior to imaging. These processes are labor intensive and contribute to delays in clinical feedback that can extend to two to three weeks for standard paraffin-embedded tissue preparation and interpretation. Here, we present a label-free reflection-mode imaging modality that reveals cellular-scale morphology by detecting intrinsic endogenous contrast. We accomplish this with the novel photoacoustic remote sensing (PARS) detection system that permits non-contact optical absorption contrast to be extracted from thick and opaque biological targets with optical resolution. PARS was examined both as a rapid assessment tool that is capable of managing large samples (&gt;1 cm2) in under 10 minutes, and as a high contrast imaging modality capable of extracting specific biological contrast to simulate conventional histological stains such as hematoxylin and eosin (H&amp;E). The capabilities of the proposed method are demonstrated in a variety of human tissue preparations including formalin-fixed paraffin-embedded tissue blocks and unstained slides sectioned from these blocks, including normal and neoplastic human brain, and breast epithelium involved with breast cancer. Similarly, PARS images of human skin prepared by frozen section clearly demonstrated basal cell carcinoma and normal human skin tissue. Finally, we imaged unprocessed murine kidney and achieved histologically relevant subcellular morphology in fresh tissue. This represents a vital step towards an effective real-time clinical microscope that overcomes the limitations of standard histopathologic tissue preparations and enables real-time pathology assessment.      
### 59.Optimal Targeting in Super-Modular Games  [ :arrow_down: ](https://arxiv.org/pdf/2009.09946.pdf)
>  We study an optimal targeting problem for super-modular games with binary actions and finitely many players. The considered problem consists in the selection of a subset of players of minimum size such that, when the actions of these players are forced to a controlled value while the others are left to repeatedly play a best response action, the system will converge to the greatest Nash equilibrium of the game. Our main contributions consist in showing that the problem is NP-complete and in proposing an efficient iterative algorithm with provable convergence properties for its solution. We discuss in detail the special case of network coordination games and its relation with the notion of cohesiveness. Finally, we show with simulations the strength of our approach with respect to naive heuristics based on classical network centrality measures.      
### 60.Automatic Target Recognition (ATR) from SAR Imaginary by Using Machine Learning Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2009.09939.pdf)
>  Automatic Target Recognition (ATR) in Synthetic aperture radar (SAR) images becomes a very challenging problem owing to containing high level noise. In this study, a machine learning-based method is proposed to detect different moving and stationary targets using SAR images. First Order Statistical (FOS) features were obtained from Fast Fourier Transform (FFT), Discrete Cosine Transform (DCT) and Discrete Wavelet Transform (DWT) on gray level SAR images. Gray Level Co-occurrence Matrix (GLCM), Gray Level Run Length Matrix (GLRLM) and Gray Level Size Zone Matrix (GLSZM) algorithms are also used. These features are provided as input for the training and testing stage Support Vector Machine (SVM) model with Gaussian kernels. 4-fold cross-validations were implemented in performance evaluation. Obtained results showed that GLCM + SVM algorithm is the best model with 95.26% accuracy. This proposed method shows that moving and stationary targets in MSTAR database could be recognized with high performance.      
### 61.Applying a random projection algorithm to optimize machine learning model for breast lesion classification  [ :arrow_down: ](https://arxiv.org/pdf/2009.09937.pdf)
>  Machine learning is widely used in developing computer-aided diagnosis (CAD) schemes of medical images. However, CAD usually computes large number of image features from the targeted regions, which creates a challenge of how to identify a small and optimal feature vector to build robust machine learning models. In this study, we investigate feasibility of applying a random projection algorithm to build an optimal feature vector from the initially CAD-generated large feature pool and improve performance of machine learning model. We assemble a retrospective dataset involving 1,487 cases of mammograms in which 644 cases have confirmed malignant mass lesions and 843 have benign lesions. A CAD scheme is first applied to segment mass regions and initially compute 181 features. Then, support vector machine (SVM) models embedded with several feature dimensionality reduction methods are built to predict likelihood of lesions being malignant. All SVM models are trained and tested using a leave-one-case-out cross-validation method. SVM generates a likelihood score of each segmented mass region depicting on one-view mammogram. By fusion of two scores of the same mass depicting on two-view mammograms, a case-based likelihood score is also evaluated. Comparing with the principle component analyses, nonnegative matrix factorization, and Chi-squared methods, SVM embedded with the random projection algorithm yielded a significantly higher case-based lesion classification performance with the area under ROC curve of 0.84+0.01 (p&lt;0.02). The study demonstrates that the random project algorithm is a promising method to generate optimal feature vectors to help improve performance of machine learning models of medical images.      
### 62.Multi-species Seagrass Detection and Classification from Underwater Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.09924.pdf)
>  Underwater surveys conducted using divers or robots equipped with customized camera payloads can generate a large number of images. Manual review of these images to extract ecological data is prohibitive in terms of time and cost, thus providing strong incentive to automate this process using machine learning solutions. In this paper, we introduce a multi-species detector and classifier for seagrasses based on a deep convolutional neural network (achieved an overall accuracy of 92.4%). We also introduce a simple method to semi-automatically label image patches and therefore minimize manual labelling requirement. We describe and release publicly the dataset collected in this study as well as the code and pre-trained models to replicate our experiments at: <a class="link-external link-https" href="https://github.com/csiro-robotics/deepseagrass" rel="external noopener nofollow">this https URL</a>      
### 63.A Deep Neural Network Tool for Automatic Segmentation of Human Body Parts in Natural Scenes  [ :arrow_down: ](https://arxiv.org/pdf/2009.09900.pdf)
>  This short article describes a deep neural network trained to perform automatic segmentation of human body parts in natural scenes. More specifically, we trained a Bayesian SegNet with concrete dropout on the Pascal-Parts dataset to predict whether each pixel in a given frame was part of a person's hair, head, ear, eyebrows, legs, arms, mouth, neck, nose, or torso.      
### 64.Clustering COVID-19 Lung Scans  [ :arrow_down: ](https://arxiv.org/pdf/2009.09899.pdf)
>  With the recent outbreak of COVID-19, creating a means to stop it's spread and eventually develop a vaccine are the most important and challenging tasks that the scientific community is facing right now. The first step towards these goals is to correctly identify a patient that is infected with the virus. Our group applied an unsupervised machine learning technique to identify COVID-19 cases. This is an important topic as COVID-19 is a novel disease currently being studied in detail and our methodology has the potential to reveal important differences between it and other viral pneumonia. This could then, in turn, enable doctors to more confidently help each patient. Our experiments utilize Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and the recently developed Robust Continuous Clustering algorithm (RCC). We display the performance of RCC in identifying COVID-19 patients and its ability to compete with other unsupervised algorithms, namely K-Means++ (KM++). Using a COVID-19 Radiography dataset, we found that RCC outperformed KM++; we used the Adjusted Mutual Information Score (AMI) in order to measure the effectiveness of both algorithms. The AMI for the two and three class cases of KM++ were 0.0250 and 0.054, respectively. In comparison, RCC scored 0.5044 in the two class case and 0.267 in the three class case, clearly showing RCC as the superior algorithm. This not only opens new possible applications of RCC, but it could potentially aid in the creation of a new tool for COVID-19 identification.      
### 65.Spatio-Temporal Hybrid Graph Convolutional Network for Traffic Forecasting in Telecommunication Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.09849.pdf)
>  Telecommunication networks play a critical role in modern society. With the arrival of 5G networks, these systems are becoming even more diversified, integrated, and intelligent. Traffic forecasting is one of the key components in such a system, however, it is particularly challenging due to the complex spatial-temporal dependency. In this work, we consider this problem from the aspect of a cellular network and the interactions among its base stations. We thoroughly investigate the characteristics of cellular network traffic and shed light on the dependency complexities based on data collected from a densely populated metropolis area. Specifically, we observe that the traffic shows both dynamic and static spatial dependencies as well as diverse cyclic temporal patterns. To address these complexities, we propose an effective deep-learning-based approach, namely, Spatio-Temporal Hybrid Graph Convolutional Network (STHGCN). It employs GRUs to model the temporal dependency, while capturing the complex spatial dependency through a hybrid-GCN from three perspectives: spatial proximity, functional similarity, and recent trend similarity. We conduct extensive experiments on real-world traffic datasets collected from telecommunication networks. Our experimental results demonstrate the superiority of the proposed model in that it consistently outperforms both classical methods and state-of-the-art deep learning models, while being more robust and stable.      
### 66.Deep learning achieves radiologist-level performance of tumor segmentation in breast MRI  [ :arrow_down: ](https://arxiv.org/pdf/2009.09827.pdf)
>  Purpose: The goal of this research was to develop a deep network architecture that achieves fully-automated radiologist-level segmentation of breast tumors in MRI. <br>Materials and Methods: We leveraged 38,229 clinical MRI breast exams collected retrospectively from women aged 12-94 (mean age 54) who presented between 2002 and 2014 at a single clinical site. The training set for the network consisted of 2,555 malignant breasts that were segmented in 2D by experienced radiologists, as well as 60,108 benign breasts that served as negative controls. The test set consisted of 250 exams with tumors segmented independently by four radiologists. We selected among several 3D deep convolutional neural network architectures, input modalities and harmonization methods. The outcome measure was the Dice score for 2D segmentation, and was compared between the network and radiologists using the Wilcoxon signed-rank test and the TOST procedure. <br>Results: The best-performing network on the training set was a volumetric U-Net with contrast enhancement dynamic as input and with intensity normalized for each exam. In the test set the median Dice score of this network was 0.77. The performance of the network was equivalent to that of the radiologists (TOST procedure with radiologist performance of 0.69-0.84 as equivalence bounds: p = 5e-10 and p = 2e-5, respectively; N = 250) and compares favorably with published state of the art (0.6-0.77). <br>Conclusion: When trained on a dataset of over 60 thousand breasts, a volumetric U-Net performs as well as expert radiologists at segmenting malignant breast lesions in MRI.      
### 67.Structure-Guided Processing Path Optimization with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.09706.pdf)
>  A major goal of material design is the inverse optimization of processing-structure-property relationships. In this paper, we propose and investigate a deep reinforcement learning approach for the optimization of processing paths. The goal is to find optimal processing paths in the material structure space that lead to target structures, which have been identified beforehand to yield desired material properties. The contribution completes the desired inversion of the processing-structure-property chain in a flexible and generic way. As the relation between properties and structures is generally nonunique, typically a whole set of goal structures can be identified, that lead to desired properties. Our proposed method optimizes processing paths from a start structure to one of the equivalent goal-structures. The algorithm learns to find near-optimal paths by interacting with the structure-generating process. It is guided by structure descriptors as process state features and a reward signal, which is formulated based on a distance function in the structure space. The model-free reinforcement learning algorithm learns through trial and error while interacting with the process and does not rely on a priori sampled processing data. We instantiate and evaluate the proposed method by optimizing paths of a generic metal forming process to reach near-optimal structures, which are represented by one-point statistics of crystallographic textures.      
### 68.Accent Estimation of Japanese Words from Their Surfaces and Romanizations for Building Large Vocabulary Accent Dictionaries  [ :arrow_down: ](https://arxiv.org/pdf/2009.09679.pdf)
>  In Japanese text-to-speech (TTS), it is necessary to add accent information to the input sentence. However, there are a limited number of publicly available accent dictionaries, and those dictionaries e.g. UniDic, do not contain many compound words, proper nouns, etc., which are required in a practical TTS system. In order to build a large scale accent dictionary that contains those words, the authors developed an accent estimation technique that predicts the accent of a word from its limited information, namely the surface (e.g. kanji) and the yomi (simplified phonetic information). It is experimentally shown that the technique can estimate accents with high accuracies, especially for some categories of words. The authors applied this technique to an existing large vocabulary Japanese dictionary NEologd, and obtained a large vocabulary Japanese accent dictionary. Many cases have been observed in which the use of this dictionary yields more appropriate phonetic information than UniDic.      
### 69.Correlating Subword Articulation with Lip Shapes for Embedding Aware Audio-Visual Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2009.09561.pdf)
>  In this paper, we propose a visual embedding approach to improving embedding aware speech enhancement (EASE) by synchronizing visual lip frames at the phone and place of articulation levels. We first extract visual embedding from lip frames using a pre-trained phone or articulation place recognizer for visual-only EASE (VEASE). Next, we extract audio-visual embedding from noisy speech and lip videos in an information intersection manner, utilizing a complementarity of audio and visual features for multi-modal EASE (MEASE). Experiments on the TCD-TIMIT corpus corrupted by simulated additive noises show that our proposed subword based VEASE approach is more effective than conventional embedding at the word level. Moreover, visual embedding at the articulation place level, leveraging upon a high correlation between place of articulation and lip shapes, shows an even better performance than that at the phone level. Finally the proposed MEASE framework, incorporating both audio and visual embedding, yields significantly better speech quality and intelligibility than those obtained with the best visual-only and audio-only EASE systems.      
### 70.Interpretable-AI Policies using Evolutionary Nonlinear Decision Trees for Discrete Action Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.09521.pdf)
>  Black-box artificial intelligence (AI) induction methods such as deep reinforcement learning (DRL) are increasingly being used to find optimal policies for a given control task. Although policies represented using a black-box AI are capable of efficiently executing the underlying control task and achieving optimal closed-loop performance -- controlling the agent from initial time step until the successful termination of an episode, the developed control rules are often complex and neither interpretable nor explainable. In this paper, we use a recently proposed nonlinear decision-tree (NLDT) approach to find a hierarchical set of control rules in an attempt to maximize the open-loop performance for approximating and explaining the pre-trained black-box DRL (oracle) agent using the labelled state-action dataset. Recent advances in nonlinear optimization approaches using evolutionary computation facilitates finding a hierarchical set of nonlinear control rules as a function of state variables using a computationally fast bilevel optimization procedure at each node of the proposed NLDT. Additionally, we propose a re-optimization procedure for enhancing closed-loop performance of an already derived NLDT. We evaluate our proposed methodologies on four different control problems having two to four discrete actions. In all these problems our proposed approach is able to find simple and interpretable rules involving one to four non-linear terms per rule, while simultaneously achieving on par closed-loop performance when compared to a trained black-box DRL agent. The obtained results are inspiring as they suggest the replacement of complicated black-box DRL policies involving thousands of parameters (making them non-interpretable) with simple interpretable policies. Results are encouraging and motivating to pursue further applications of proposed approach in solving more complex control tasks.      
### 71.Ultrashort Pulse Detection and Response Time Analysis Using Plasma-wave Terahertz Field Effect Transistors  [ :arrow_down: ](https://arxiv.org/pdf/2009.09456.pdf)
>  We report on the response characteristics of plasmonic terahertz field-effect transistors (TeraFETs) fed with femtosecond and picosecond pulses. Varying the pulse width (tpw) from 10-15 s to 10-10 s under a constant input power condition revealed two distinctive pulse detection modes. In the short pulse mode (tpw &lt;&lt; L/s, where L is the gated channel length, s is the plasma velocity), the source-to-drain voltage response is a sharp pulse oscillatory decay preceded by a delay time on the order of L/s. The plasma wave travels along the channel like the shallow water wave with a relatively narrow wave package. In the long pulse mode (tpw &gt; L/s), the response profile has two oscillatory decay processes and the propagation of plasma wave is analogues to oscillating rod with one side fixed. The ultimate response time at the long pulse mode is significantly higher than that under the short pulse conditions. The detection conditions under the long pulse mode are close to the step response condition, and the response time conforms well to the analytical theory for the step function response. The simulated waveform agrees well with the measured pulse response. Our results show that the measurements of the pulse response enable the material parameter extraction from the pulse response data (including the effective mass, kinematic viscosity and momentum relaxation time).      
### 72.An enhanced performance for H.265/SHVC based on combined AEGBM3D filter and back-propagation neural network  [ :arrow_down: ](https://arxiv.org/pdf/2009.09428.pdf)
>  This paper deals with the latest video coding standard H265 SHVC, a scalable extension to High Efficiency Video Coding (HEVC). HEVC introduces new coding tools compared to its predecessor and is backward compatible with all types of electronic gadgets. The gadgets with different display capabilities cannot be offered the same quality video due to the constraints in transmission bandwidth is a major problem. One solution to this problem will be the compression of the video sequence which is focused in this paper to preserve or increase PSNR while reducing bit-rate besides a novel method implemented in SHVC encoder. The novel method undergoes a combined AEGBM3D (adaptive edge guided block-matching and 3D) filtering and back-propagation technique. The technique includes an AEGBM3D filter which avoids spatial redundancy and de-noise frames; hence enhancement in PSNR is achieved. The obtained PSNR of the video is compared with the set threshold PSNR to maintain PSNR above the threshold by repeated AEGBM3D filtering. The BP technique based on the neural network machine learning approach continually restrains the output if the input block does not contain a feature they were trained to recognize. This frequent control over the output produces few bits; hence reduction in bit-rate is achieved. The simulation results show that the proposed technique delivers an average increment of 0.16 and 0.25dB in PSNR and an average decrement of 28 and 37% in bit-rate for 1.5 and 2 times spatial ratios respectively, compared with the existing methods.      
### 73.A Hybrid Simulation-based Duopoly Game Framework for Analysis of Supply Chain and Marketing Activities  [ :arrow_down: ](https://arxiv.org/pdf/2009.09352.pdf)
>  A hybrid simulation-based framework involving system dynamics and agent-based simulation is proposed to address duopoly game considering multiple strategic decision variables and rich payoff, which cannot be addressed by traditional approaches involving closed-form equations. While system dynamics models are used to represent integrated production, logistics, and pricing determination activities of duopoly companies, agent-based simulation is used to mimic enhanced consumer purchasing behavior considering advertisement, promotion effect, and acquaintance recommendation in the consumer social network. The payoff function of the duopoly companies is assumed to be the net profit based on the total revenue and various cost items such as raw material, production, transportation, inventory and backorder. A unique procedure is proposed to solve and analyze the proposed simulation-based game, where the procedural components include strategy refinement, data sampling, gaming solving, and performance evaluation. First, design of experiment and estimated conformational value of information techniques are employed for strategy refinement and data sampling, respectively. Game solving then focuses on pure strategy equilibriums, and performance evaluation addresses game stability, equilibrium strictness, and robustness. A hypothetical case scenario involving soft-drink duopoly on Coke and Pepsi is considered to illustrate and demonstrate the proposed approach. Final results include P-values of statistical tests, confidence intervals, and simulation steady state analysis for different pure equilibriums.      
### 74.Features based Mammogram Image Classification using Weighted Feature Support Vector Machine  [ :arrow_down: ](https://arxiv.org/pdf/2009.09300.pdf)
>  In the existing research of mammogram image classification, either clinical data or image features of a specific type is considered along with the supervised classifiers such as Neural Network (NN) and Support Vector Machine (SVM). This paper considers automated classification of breast tissue type as benign or malignant using Weighted Feature Support Vector Machine (WFSVM) through constructing the precomputed kernel function by assigning more weight to relevant features using the principle of maximizing deviations. Initially, MIAS dataset of mammogram images is divided into training and test set, then the preprocessing techniques such as noise removal and background removal are applied to the input images and the Region of Interest (ROI) is identified. The statistical features and texture features are extracted from the ROI and the clinical features are obtained directly from the dataset. The extracted features of the training dataset are used to construct the weighted features and precomputed linear kernel for training the WFSVM, from which the training model file is created. Using this model file the kernel matrix of test samples is classified as benign or malignant. This analysis shows that the texture features have resulted in better accuracy than the other features with WFSVM and SVM. However, the number of support vectors created in WFSVM is less than the SVM classifier.      
### 75.A Review of Visual Odometry Methods and Its Applications for Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2009.09193.pdf)
>  The research into autonomous driving applications has observed an increase in computer vision-based approaches in recent years. In attempts to develop exclusive vision-based systems, visual odometry is often considered as a key element to achieve motion estimation and self-localisation, in place of wheel odometry or inertial measurements. This paper presents a recent review to methods that are pertinent to visual odometry with an emphasis on autonomous driving. This review covers visual odometry in their monocular, stereoscopic and visual-inertial form, individually presenting them with analyses related to their applications. Discussions are drawn to outline the problems faced in the current state of research, and to summarise the works reviewed. This paper concludes with future work suggestions to aid prospective developments in visual odometry.      
### 76.Dynamic Scheduling and Workforce Assignment in Open Source Software Development  [ :arrow_down: ](https://arxiv.org/pdf/2009.09168.pdf)
>  A novel modeling framework is proposed for dynamic scheduling of projects and workforce assignment in open source software development (OSSD). The goal is to help project managers in OSSD distribute workforce to multiple projects to achieve high efficiency in software development (e.g. high workforce utilization and short development time) while ensuring the quality of deliverables (e.g. code modularity and software security). The proposed framework consists of two models: 1) a system dynamic model coupled with a meta-heuristic to obtain an optimal schedule of software development projects considering their attributes (e.g. priority, effort, duration) and 2) an agent based model to represent the development community as a social network, where development managers form an optimal team for each project and balance the workload among multiple scheduled projects based on the optimal schedule obtained from the system dynamic model. To illustrate the proposed framework, a software enhancement request process in Kuali foundation is used as a case study. Survey data collected from the Kuali development managers, project managers and actual historical enhancement requests have been used to construct the proposed models. Extensive experiments are conducted to demonstrate the impact of varying parameters on the considered efficiency and quality.      
### 77.Population Susceptibility Variation and Its Effect on Contagion Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2009.09150.pdf)
>  Susceptibility governs the dynamics of contagion. The classical SIR model is one of the simplest compartmental models of contagion spread, assuming a single shared susceptibility level. However, variation in susceptibility over a population can fundamentally affect the dynamics of contagion and thus the ultimate outcome of a pandemic. We develop mathematical machinery which explicitly considers susceptibility variation, illuminates how the susceptibility distribution is sculpted by contagion, and thence how such variation affects the SIR differential questions that govern contagion. Our methods allow us to derive closed form expressions for herd immunity thresholds as a function of initial susceptibility distributions and suggests an intuitively satisfying approach to inoculation when only a fraction of the population is accessible to such intervention. Of particular interest, if we assume static susceptibility of individuals in the susceptible pool, ignoring susceptibility diversity {\em always} results in overestimation of the herd immunity threshold and that difference can be dramatic. Therefore, we should develop robust measures of susceptibility variation as part of public health strategies for handling pandemics.      
### 78.A Survey on Smart Metering Systems using Blockchain for E-Mobility  [ :arrow_down: ](https://arxiv.org/pdf/2009.09075.pdf)
>  Electricity is an essential comfort to support our daily activities. With the competitive increase and energy costs by the industry, new values and opportunities for delivering electricity to customers are produced. One of these new opportunities is electric vehicles. With the arrival of electric vehicles, various challenges and opportunities are being presented in the electric power system worldwide. For example, under the traditional electric power billing scheme, electric power has to be consumed where it is needed so that end-users could not charge their electric vehicles at different points (e.g. a relative's house) if this the correct user is not billed (this due to the high consumption of electrical energy that makes it expensive). To achieve electric mobility, they must solve new challenges, such as the smart metering of energy consumption and the cybersecurity of these measurements. The present work shows a study of the different smart metering technologies that use blockchain and other security mechanisms to achieve e-mobility.      
### 79.A Machine Learning Approach to Detect Suicidal Ideation in US Veterans Based on Acoustic and Linguistic Features of Speech  [ :arrow_down: ](https://arxiv.org/pdf/2009.09069.pdf)
>  Preventing Veteran suicide is a national priority. The US Department of Veterans Affairs (VA) collects, analyzes, and publishes data to inform suicide prevention strategies. Current approaches for detecting suicidal ideation mostly rely on patient self report which are inadequate and time consuming. In this research study, our goal was to automate suicidal ideation detection from acoustic and linguistic features of an individual's speech using machine learning (ML) algorithms. Using voice data collected from Veterans enrolled in a large interventional study on Gulf War Illness at the Washington DC VA Medical Center, we conducted an evaluation of the performance of different ML approaches in achieving our objective. By fitting both classical ML and deep learning models to the dataset, we identified the algorithms that were most effective for each feature set. Among classical machine learning algorithms, the Support Vector Machine (SVM) trained on acoustic features performed best in classifying suicidal Veterans. Among deep learning methods, the Convolutional Neural Network (CNN) trained on the linguistic features performed best. Our study shows that speech analysis in a machine learning pipeline is a promising approach for detecting suicidality among Veterans.      
