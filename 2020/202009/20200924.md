# ArXiv eess --Thu, 24 Sep 2020
### 1.Multi-Agent Deep Reinforcement Learning Based Trajectory Planning for Multi-UAV Assisted Mobile Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2009.11277.pdf)
>  An unmanned aerial vehicle (UAV)-aided mobile edge computing (MEC) framework is proposed, where several UAVs having different trajectories fly over the target area and support the user equipments (UEs) on the ground. We aim to jointly optimize the geographical fairness among all the UEs, the fairness of each UAV' UE-load and the overall energy consumption of UEs. The above optimization problem includes both integer and continues variables and it is challenging to solve. To address the above problem, a multi-agent deep reinforcement learning based trajectory control algorithm is proposed for managing the trajectory of each UAV independently, where the popular Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method is applied. Given the UAVs' trajectories, a low-complexity approach is introduced for optimizing the offloading decisions of UEs. We show that our proposed solution has considerable performance over other traditional algorithms, both in terms of the fairness for serving UEs, fairness of UE-load at each UAV and energy consumption for all the UEs.      
### 2.Edge Intelligence Empowered UAVs for Automated Wind Farm Monitoring in Smart Grids  [ :arrow_down: ](https://arxiv.org/pdf/2009.11256.pdf)
>  With the exploitation of wind power, more turbines will be deployed at remote areas possibly with harsh working conditions (e.g., offshore wind farm). The adverse working environment may lead to massive operating and maintenance costs of turbines. Deploying unmanned aerial vehicles (UAVs) for turbine inspection is considered as a viable alternative to manual inspections. An important objective of automated UAV inspection is to minimize the flight time of the UAVs to inspect all the turbines. A first contribution of this paper is thus formulating an optimization problem to compute the optimal routes for turbine inspection satisfying the above goal. On the other hand, the limited computational capability on UAVs can be used to increase the power generation of wind turbine. Power generation from the turbines can be optimized by controlling the yaw angle of the turbines. Forecasting wind conditions such as wind speed and wind direction is crucial for solving both optimization problems. Therefore, UAVs can utilize their limited computational capability to perform wind forecasting. In this way, UAVs form edge intelligence in offshore wind farm. With the forecasted wind conditions, we design two algorithms to solve the formulated problems, and then evaluate the proposed methods with realworld data. The results reveal that the proposed methods offer an improvement of 44% of the power generation from the turbine compared to hour-ahead forecasting and 25% reduction of the flight time of the UAVs compared to the chosen baseline method.      
### 3.An electronic neuromorphic system for real-time detection of High Frequency Oscillations (HFOs) in intracranial EEG  [ :arrow_down: ](https://arxiv.org/pdf/2009.11245.pdf)
>  In this work, we present a neuromorphic system that combines for the first time a neural recording headstage with a signal-to-spike conversion circuit and a multi-core spiking neural network (SNN) architecture on the same die for recording, processing, and detecting High Frequency Oscillations (HFO), which are biomarkers for the epileptogenic zone. The device was fabricated using a standard 0.18$\mu$m CMOS technology node and has a total area of 99mm$^{2}$. We demonstrate its application to HFO detection in the iEEG recorded from 9 patients with temporal lobe epilepsy who subsequently underwent epilepsy surgery. The total average power consumption of the chip during the detection task was 614.3$\mu$W. We show how the neuromorphic system can reliably detect HFOs: the system predicts postsurgical seizure outcome with state-of-the-art accuracy, specificity and sensitivity (78%, 100%, and 33% respectively). This is the first feasibility study towards identifying relevant features in intracranial human data in real-time, on-chip, using event-based processors and spiking neural networks. By providing "neuromorphic intelligence" to neural recording circuits the approach proposed will pave the way for the development of systems that can detect HFO areas directly in the operation room and improve the seizure outcome of epilepsy surgery.      
### 4.Disconnection-aware Attack Detection and Isolation with Separation-based Detector Reconfiguration  [ :arrow_down: ](https://arxiv.org/pdf/2009.11205.pdf)
>  This study addresses incident handling during an adverse event for dynamical networked control systems. Incident handling can be divided into five steps: detection, analysis, containment, eradication, and recovery. For networked control systems, the containment step can be conducted through physical disconnection of an attacked subsystem. In accordance with the disconnection, the equipped attack detection unit should be reconfigured to maintain its detection capability. In particular, separating the detection subunit associated with the disconnected subsystem is considered as a specific reconfiguration scheme in this study. This paper poses the problem of disconnection-aware attack detection and isolation with the separation-based detector reconfiguration. The objective is to find an attack detection unit that preserves its detection and isolation capability even under any possible disconnection and separation. The difficulty arises from network topology variation caused by disconnection that can possibly lead to stability loss of the distributed observer inside the attack detection unit. A solution is proposed based on an existing controller design technique referred to as retrofit control. Furthermore, an application to low-voltage power distribution networks with distributed generation is exhibited. Numerical examples evidence the practical use of the proposed method through a benchmark distribution network.      
### 5.Perceptual Video Quality Prediction Emphasizing Chroma Distortions  [ :arrow_down: ](https://arxiv.org/pdf/2009.11203.pdf)
>  Measuring the quality of digital videos viewed by human observers has become a common practice in numerous multimedia applications, such as adaptive video streaming, quality monitoring, and other digital TV applications. Here we explore a significant, yet relatively unexplored problem: measuring perceptual quality on videos arising from both luma and chroma distortions from compression. Toward investigating this problem, it is important to understand the kinds of chroma distortions that arise, how they relate to luma compression distortions, and how they can affect perceived quality. We designed and carried out a subjective experiment to measure subjective video quality on both luma and chroma distortions, introduced both in isolation as well as together. Specifically, the new subjective dataset comprises a total of $210$ videos afflicted by distortions caused by varying levels of luma quantization commingled with different amounts of chroma quantization. The subjective scores were evaluated by $34$ subjects in a controlled environmental setting. Using the newly collected subjective data, we were able to demonstrate important shortcomings of existing video quality models, especially in regards to chroma distortions. Further, we designed an objective video quality model which builds on existing video quality algorithms, by considering the fidelity of chroma channels in a principled way. We also found that this quality analysis implies that there is room for reducing bitrate consumption in modern video codecs by creatively increasing the compression factor on chroma channels. We believe that this work will both encourage further research in this direction, as well as advance progress on the ultimate goal of jointly optimizing luma and chroma compression in modern video encoders.      
### 6.56 Gb/s DMT Transmission with VCSELs in 1.5 um Wavelength Range over up to 12 km for DWDM Intra-Data Center Connects  [ :arrow_down: ](https://arxiv.org/pdf/2009.11192.pdf)
>  We demonstrate up to 12 km, 56 Gb/s DMT transmission using high-speed VCSELs in the 1.5 um wavelength range for future 400Gb/s intra-data center connects, enabled by vestigial sideband filtering of the transmit signal.      
### 7.Solutions for 400 Gbit/s Inter Data Center WDM Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2009.11184.pdf)
>  We review some currently discussed solutions for 400 Gbit/s inter-data center WDM transmission for up to 100 km. We focus on direct detected solutions, namely PAM4 and DMT, and present two WDM systems based on these formats.      
### 8.Modular Multilevel Converter with Sensorless Diode-Clamped Balancing through Level-Adjusted Phase-Shifted Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2009.11177.pdf)
>  Cascaded H-bridge and modular multilevel converters (MMC) are on the rise with emerging applications in renewable energy generation, energy storage, and electric motor drives. However, their well-known advantages come at the price of complicated balancing, high-bandwidth isolated monitoring, and numerous sensors that can prevent MMCs from expanding into highly cost driven markets. Therefore, an obvious trend in research is developing control and topologies that depend less on measurements and benefit from simpler control. Diode-clamped topologies are considered among the more applicable solutions. The main problem with a diode-clamped topology is that it can only balance the module voltages of a string in one direction; therefore, it cannot provide a completely balanced operation. This paper proposes an effective balancing technique for the diode-clamped topology. The proposed solution exploits the dc component of the arm current by introducing a symmetrically level-adjusted phase-shifted modulation scheme, and ensures the balancing current flow is always in the correct direction. The main advantages of this method are sensorless operation, no added computation and control effort, and low overall cost. Analysis and detailed simulations provide insight into the operation of the system as well as the new balancing technique and the experimental results confirm the provided discussions.      
### 9.Whole Slide Images based Cancer Survival Prediction using Attention Guided Deep Multiple Instance Learning Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.11169.pdf)
>  Traditional image-based survival prediction models rely on discriminative patch labeling which make those methods not scalable to extend to large datasets. Recent studies have shown Multiple Instance Learning (MIL) framework is useful for histopathological images when no annotations are available in classification task. Different to the current image-based survival models that limit to key patches or clusters derived from Whole Slide Images (WSIs), we propose Deep Attention Multiple Instance Survival Learning (DeepAttnMISL) by introducing both siamese MI-FCN and attention-based MIL pooling to efficiently learn imaging features from the WSI and then aggregate WSI-level information to patient-level. Attention-based aggregation is more flexible and adaptive than aggregation techniques in recent survival models. We evaluated our methods on two large cancer whole slide images datasets and our results suggest that the proposed approach is more effective and suitable for large datasets and has better interpretability in locating important patterns and features that contribute to accurate cancer survival predictions. The proposed framework can also be used to assess individual patient's risk and thus assisting in delivering personalized medicine. Codes are available at <a class="link-external link-https" href="https://github.com/uta-smile/DeepAttnMISL_MEDIA" rel="external noopener nofollow">this https URL</a>.      
### 10.Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial Network Normalizer  [ :arrow_down: ](https://arxiv.org/pdf/2009.11166.pdf)
>  Foreseeing the brain evolution as a complex highly inter-connected system, widely modeled as a graph, is crucial for mapping dynamic interactions between different anatomical regions of interest (ROIs) in health and disease. Interestingly, brain graph evolution models remain almost absent in the literature. Here we design an adversarial brain network normalizer for representing each brain network as a transformation of a fixed centered population-driven connectional template. Such graph normalization with respect to a fixed reference paves the way for reliably identifying the most similar training samples (i.e., brain graphs) to the testing sample at baseline timepoint. The testing evolution trajectory will be then spanned by the selected training graphs and their corresponding evolution trajectories. We base our prediction framework on geometric deep learning which naturally operates on graphs and nicely preserves their topological properties. Specifically, we propose the first graph-based Generative Adversarial Network (gGAN) that not only learns how to normalize brain graphs with respect to a fixed connectional brain template (CBT) (i.e., a brain template that selectively captures the most common features across a brain population) but also learns a high-order representation of the brain graphs also called embeddings. We use these embeddings to compute the similarity between training and testing subjects which allows us to pick the closest training subjects at baseline timepoint to predict the evolution of the testing brain graph over time. A series of benchmarks against several comparison methods showed that our proposed method achieved the lowest brain disease evolution prediction error using a single baseline timepoint. Our gGAN code is available at <a class="link-external link-http" href="http://github.com/basiralab/gGAN" rel="external noopener nofollow">this http URL</a>.      
### 11.Fractional-Order Single State Reset Element  [ :arrow_down: ](https://arxiv.org/pdf/2009.11131.pdf)
>  This paper proposes a fractional-order reset element whose architecture allows for the suppression of nonlinear effects for a range of frequencies. Suppressing the nonlinear effects of a reset element for the desired frequency range while maintaining it for the rest is beneficial, especially when it is used in the framework of a "Constant in gain, Lead in phase" (CgLp) filter. CgLp is a newly introduced nonlinear filter, bound to circumvent the well-known linear control limitation -- the waterbed effect. The ideal behaviour of such a filter in the frequency domain is unity gain while providing a phase lead for a broad range of frequencies. However, CgLp's ideal behaviour is based on the describing function, which is a first-order approximation that neglects the effects of the higher-order harmonics in the output of the filter. Although CgLp is fundamentally a nonlinear filter, its nonlinearity is not required for all frequencies. Thus, it is shown in this paper that using the proposed reset element architecture, CgLp gets closer to its ideal behaviour for a range of frequencies, and its performance will be improved accordingly.      
### 12.Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from Multi-Planar MRI  [ :arrow_down: ](https://arxiv.org/pdf/2009.11120.pdf)
>  Background and Objective: Accurate and reliable segmentation of the prostate gland in MR images can support the clinical assessment of prostate cancer, as well as the planning and monitoring of focal and loco-regional therapeutic interventions. Despite the availability of multi-planar MR scans due to standardized protocols, the majority of segmentation approaches presented in the literature consider the axial scans only. Methods: We propose an anisotropic 3D multi-stream CNN architecture, which processes additional scan directions to produce a higher-resolution isotropic prostate segmentation. We investigate two variants of our architecture, which work on two (dual-plane) and three (triple-plane) image orientations, respectively. We compare them with the standard baseline (single-plane) used in literature, i.e., plain axial segmentation. To realize a fair comparison, we employ a hyperparameter optimization strategy to select optimal configurations for the individual approaches. Results: Training and evaluation on two datasets spanning multiple sites obtain statistical significant improvement over the plain axial segmentation ($p&lt;0.05$ on the Dice similarity coefficient). The improvement can be observed especially at the base ($0.898$ single-plane vs. $0.906$ triple-plane) and apex ($0.888$ single-plane vs. $0.901$ dual-plane). Conclusion: This study indicates that models employing two or three scan directions are superior to plain axial segmentation. The knowledge of precise boundaries of the prostate is crucial for the conservation of risk structures. Thus, the proposed models have the potential to improve the outcome of prostate cancer diagnosis and therapies.      
### 13.Estimation of Motion Parameters for Ultrasound Images Using Motion Blur Invariants  [ :arrow_down: ](https://arxiv.org/pdf/2009.11117.pdf)
>  The quality of fetal ultrasound images is significantly affected by motion blur while the imaging system requires low motion quality in order to capture accurate data. This can be achieved with a mathematical model of motion blur in time or frequency domain. We propose a new model of linear motion blur in both frequency and moment domain to analyse the invariant features of blur convolution for ultrasound images. Moreover, the model also helps to provide an estimation of motion parameters for blur length and angle. These outcomes might imply great potential of this invariant method in ultrasound imaging application.      
### 14.Device for ECG prediction based on retinal vasculature analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.11099.pdf)
>  Pulsatile changes in retinal vascular geometry over the cardiac cycle have clinical implications for the diagnosis of ocular and systemic vascular diseases, including ischemia, coronary heart diseases, and diabetes mellitus and its complications. Thus, analysis of the pulsatile changes over the cardiac cycle is a potential non-invasive assessment for the presence of ocular and systemic vascular diseases. The cardiac rhythm influences these pulsatile changes in the retina, is a result of the change in blood volumetric flow entering the ophthalmic-vascular system under a certain level of intraocular pressure during the peak systolic and diastolic phases of the cardiac cycle. Assessment of fundus images generally requires ophthalmologic expertise. However, the availability of an expert is not always guaranteed, and even if an expert is available, the assessment is performed manually. Thus, there is also a need for an automated device that can analyze the fundus images and keep track of the pulsations. Such a device can be synchronized with the Electrocardiogram and can be programmed to assess the presence of various ocular and systemic vascular diseases. In this project, we proposed and worked on a portable embedded system device that automatically captures retinal images and analyzes them for the estimation of vessel diameters. The device is hand-held and attaches to the ophthalmoscope as an extension. Using the device, the time variation of the vessel diameters can be estimated, which can further be used to predict ECG and the presence of various other systemic vascular diseases.      
### 15.Factor Graph-Based Smoothing Without Matrix Inversion for Highly Precise Localization  [ :arrow_down: ](https://arxiv.org/pdf/2009.11097.pdf)
>  We consider the problem of localizing a manned, semi-autonomous, or autonomous vehicle in the environment using information coming from the vehicle's sensors, a problem known as navigation or simultaneous localization and mapping (SLAM) depending on the context. To infer knowledge from sensors' measurements, while drawing on a priori knowledge about the vehicle's dynamics, modern approaches solve an optimization problem to compute the most likely trajectory given all past observations, an approach known as smoothing. Improving smoothing solvers is an active field of research in the SLAM community. Most work is focused on reducing computation load by inverting the involved linear system while preserving its sparsity. The present paper raises an issue which, to the knowledge of the authors, has not been addressed yet: standard smoothing solvers require explicitly using the inverse of sensor noise covariance matrices. This means the parameters that reflect the noise magnitude must be sufficiently large for the smoother to properly function. When matrices are close to singular, which is the case when using high precision modern inertial measurement units (IMU), numerical issues necessarily arise, especially with 32-bits implementation demanded by most industrial aerospace applications. We discuss these issues and propose a solution that builds upon the Kalman filter to improve smoothing algorithms. We then leverage the results to devise a localization algorithm based on fusion of IMU and vision sensors. Successful real experiments using an actual car equipped with a tactical grade high performance IMU and a LiDAR illustrate the relevance of the approach to the field of autonomous vehicles.      
### 16.Robustification of Segmentation Models Against Adversarial Perturbations In Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2009.11090.pdf)
>  This paper presents a novel yet efficient defense framework for segmentation models against adversarial attacks in medical imaging. In contrary to the defense methods against adversarial attacks for classification models which widely are investigated, such defense methods for segmentation models has been less explored. Our proposed method can be used for any deep learning models without revising the target deep learning models, as well as can be independent of adversarial attacks. Our framework consists of a frequency domain converter, a detector, and a reformer. The frequency domain converter helps the detector detects adversarial examples by using a frame domain of an image. The reformer helps target models to predict more precisely. We have experiments to empirically show that our proposed method has a better performance compared to the existing defense method.      
### 17.GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution from Low-Resolution Functional Brain Connectomes  [ :arrow_down: ](https://arxiv.org/pdf/2009.11080.pdf)
>  Catchy but rigorous deep learning architectures were tailored for image super-resolution (SR), however, these fail to generalize to non-Euclidean data such as brain connectomes. Specifically, building generative models for super-resolving a low-resolution (LR) brain connectome at a higher resolution (HR) (i.e., adding new graph nodes/edges) remains unexplored although this would circumvent the need for costly data collection and manual labelling of anatomical brain regions (i.e. parcellation). To fill this gap, we introduce GSR-Net (Graph Super-Resolution Network), the first super-resolution framework operating on graph-structured data that generates high-resolution brain graphs from low-resolution graphs. First, we adopt a U-Net like architecture based on graph convolution, pooling and unpooling operations specific to non-Euclidean data. However, unlike conventional U-Nets where graph nodes represent samples and node features are mapped to a low-dimensional space (encoding and decoding node attributes or sample features), our GSR-Net operates directly on a single connectome: a fully connected graph where conventionally, a node denotes a brain region, nodes have no features, and edge weights denote brain connectivity strength between two regions of interest (ROIs). In the absence of original node features, we initially assign identity feature vectors to each brain ROI (node) and then leverage the learned local receptive fields to learn node feature representations. Second, inspired by spectral theory, we break the symmetry of the U-Net architecture by topping it up with a graph super-resolution (GSR) layer and two graph convolutional network layers to predict a HR graph while preserving the characteristics of the LR input. Our proposed GSR-Net framework outperformed its variants for predicting high-resolution brain functional connectomes from low-resolution connectomes.      
### 18.Stochastic output feedback MPC with intermittent observations  [ :arrow_down: ](https://arxiv.org/pdf/2009.11071.pdf)
>  This paper considers constrained linear systems with stochastic additive disturbances and noisy measurements transmitted over a lossy communication channel. We propose a model predictive control (MPC) law that minimises a discounted cost subject to a discounted expectation constraint. Sensor data is assumed to be lost with known probability, and data losses are accounted for by expressing the predicted control policy as an affine function of future observations, which results in a convex optimal control problem. An online constraint-tightening technique ensures recursive feasibility of the online optimisation problem and satisfaction of the expectation constraint without imposing bounds on the distributions of the noise and disturbance inputs. The discounted cost evaluated along trajectories of the closed loop system is shown to be bounded by the initial optimal predicted cost. We also provide conditions under which the averaged undiscounted closed loop cost accumulated over an infinite horizon is bounded. Numerical simulations are described to illustrate these results.      
### 19.Zero-inertia Offshore Grids: N-1 Security and Active Power Sharing  [ :arrow_down: ](https://arxiv.org/pdf/2009.11039.pdf)
>  With Denmark dedicated to maintaining its leading position in the integration of massive shares of wind energy, the construction of new offshore energy islands has been recently approved by the Danish government. These new islands will be zero-inertia systems, meaning that no synchronous generation will be installed in the island and that power imbalances will be shared only among converters. To this end, this paper proposes a telecommunication-free frequency droop controller to maintain the active power balance in the offshore system and guarantee N-1 security. Although offshore systems are the main focus of this paper, the presented methodology could be applied to any other zero- or low-inertia system. The frequency droop gains are calculated solving an optimization problem which takes into consideration the small-signal and transient stability of the system. As a consequence, the proposed controller allows for greater loadability of the offshore converters at pre-fault state and guarantees their safe operation in the event of any power imbalance.      
### 20.Automatic Breast Lesion Classification by Joint Neural Analysis of Mammography and Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2009.11009.pdf)
>  Mammography and ultrasound are extensively used by radiologists as complementary modalities to achieve better performance in breast cancer diagnosis. However, existing computer-aided diagnosis (CAD) systems for the breast are generally based on a single modality. In this work, we propose a deep-learning based method for classifying breast cancer lesions from their respective mammography and ultrasound images. We present various approaches and show a consistent improvement in performance when utilizing both modalities. The proposed approach is based on a GoogleNet architecture, fine-tuned for our data in two training steps. First, a distinct neural network is trained separately for each modality, generating high-level features. Then, the aggregated features originating from each modality are used to train a multimodal network to provide the final classification. In quantitative experiments, the proposed approach achieves an AUC of 0.94, outperforming state-of-the-art models trained over a single modality. Moreover, it performs similarly to an average radiologist, surpassing two out of four radiologists participating in a reader study. The promising results suggest that the proposed method may become a valuable decision support tool for breast radiologists.      
### 21.Attention with Multiple Sources Knowledges for COVID-19 from CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.11008.pdf)
>  Until now, Coronavirus SARS-CoV-2 has caused more than 850,000 deaths and infected more than 27 million individuals in over 120 countries. Besides principal polymerase chain reaction (PCR) tests, automatically identifying positive samples based on computed tomography (CT) scans can present a promising option in the early diagnosis of COVID-19. Recently, there have been increasing efforts to utilize deep networks for COVID-19 diagnosis based on CT scans. While these approaches mostly focus on introducing novel architectures, transfer learning techniques, or construction large scale data, we propose a novel strategy to improve the performance of several baselines by leveraging multiple useful information sources relevant to doctors' judgments. Specifically, infected regions and heat maps extracted from learned networks are integrated with the global image via an attention mechanism during the learning process. This procedure not only makes our system more robust to noise but also guides the network focusing on local lesion areas. Extensive experiments illustrate the superior performance of our approach compared to recent baselines. Furthermore, our learned network guidance presents an explainable feature to doctors as we can understand the connection between input and output in a grey-box model.      
### 22.Attention Driven Fusion for Multi-Modal Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.10991.pdf)
>  Deep learning has emerged as a powerful alternative to hand-crafted methods for emotion recognition on combined acoustic and text modalities. Baseline systems model emotion information in text and acoustic modes independently using Deep Convolutional Neural Networks (DCNN) and Recurrent Neural Networks (RNN), followed by applying attention, fusion, and classification. In this paper, we present a deep learning-based approach to exploit and fuse text and acoustic data for emotion classification. We utilize a SincNet layer, based on parameterized sinc functions with band-pass filters, to extract acoustic features from raw audio followed by a DCNN. This approach learns filter banks tuned for emotion recognition and provides more effective features compared to directly applying convolutions over the raw speech signal. For text processing, we use two branches (a DCNN and a Bi-direction RNN followed by a DCNN) in parallel where cross attention is introduced to infer the N-gram level correlations on hidden representations received from the Bi-RNN. Following existing state-of-the-art, we evaluate the performance of the proposed system on the IEMOCAP dataset. Experimental results indicate that the proposed system outperforms existing methods, achieving 3.5% improvement in weighted accuracy.      
### 23.Learning Non-Unique Segmentation with Reward-Penalty Dice Loss  [ :arrow_down: ](https://arxiv.org/pdf/2009.10987.pdf)
>  Semantic segmentation is one of the key problems in the field of computer vision, as it enables computer image understanding. However, most research and applications of semantic segmentation focus on addressing unique segmentation problems, where there is only one gold standard segmentation result for every input image. This may not be true in some problems, e.g., medical applications. We may have non-unique segmentation annotations as different surgeons may perform successful surgeries for the same patient in slightly different ways. To comprehensively learn non-unique segmentation tasks, we propose the reward-penalty Dice loss (RPDL) function as the optimization objective for deep convolutional neural networks (DCNN). RPDL is capable of helping DCNN learn non-unique segmentation by enhancing common regions and penalizing outside ones. Experimental results show that RPDL improves the performance of DCNN models by up to 18.4% compared with other loss functions on our collected surgical dataset.      
### 24.A data-driven method for computing polyhedral invariant sets of black-box switched linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.10984.pdf)
>  In this paper, we consider the problem of invariant set computation for black-box switched linear systems using merely a finite set of observations of system simulations. In particular, this paper focuses on polyhedral invariant sets. We propose a data-driven method based on the one step forward reachable set. For formal verification of the proposed method, we introduce the concept of almost-invariant sets for switched linear systems. The convexity-preserving property of switched linear systems allows us to conduct contraction analysis on almost-invariant sets and derive an a priori probabilistic guarantee. In the spirit of non-convex scenario optimization, we also establish a posteriori the level of violation on the computed set. The performance of our method is then illustrated by a switched system under arbitrary switching between two modes.      
### 25.Improved gradient descent-based chroma subsampling method for color images in VVC  [ :arrow_down: ](https://arxiv.org/pdf/2009.10934.pdf)
>  Prior to encoding color images for RGB full-color, Bayer color filter array (CFA), and digital time delay integration (DTDI) CFA images, performing chroma subsampling on their converted chroma images is necessary and important. <br>In this paper, we propose an effective general gradient descent-based chroma subsampling method for the above three kinds of color images, achieving substantial quality and quality-bitrate tradeoff improvement of the reconstructed color images when compared with the related methods. First, a bilinear interpolation based 2$\times$2 $t$ ($\in \{RGB, Bayer, DTDI\}$) color block-distortion function is proposed at the server side, and then in real domain, we prove that our general 2$\times$2 $t$ color block-distortion function is a convex function. Furthermore, a general closed form is derived to determine the initially subsampled chroma pair for each 2$\times$2 chroma block. Finally, an effective iterative method is developed to improve the initially subsampled $(U, V)$-pair. Based on the Kodak and IMAX datasets, the comprehensive experimental results demonstrated that on the newly released versatile video coding (VVC) platform VTM-8.0, for the above three kinds of color images, our chroma subsampling method clearly outperforms the existing chroma subsampling methods.      
### 26.Joint routing and pricing control of autonomous vehicles in mixed equilibrium simulation-based dynamic traffic assignment  [ :arrow_down: ](https://arxiv.org/pdf/2009.10907.pdf)
>  Routing controllability of autonomous vehicles (AVs) has been shown to reduce the impact of selfish routing on network efficiency. However, the assumption that AVs would readily allow themselves to be controlled externally by a central agency is unrealistic. In this paper, we propose a joint routing and pricing control scheme that aims to incentivize AVs to seek centrally controlled system optimal (SO) routing by saving on tolls while user equilibrium (UE) seeking AVs and human-driven vehicles (HVs) are subject to a congestion charge. The problem is formulated as a bi-level optimization, in which dynamic tolls are optimized in the upper level, whereas the lower level is a mixed equilibrium simulation-based dynamic traffic assignment model considering mixed fleet of AVs and HVs. We develop a feedback-based controller to implement a second-based pricing scheme from which SO-seeking AVs are exempt; but UEseeking vehicles, including both AVs and HVs, are subject to a distance-based charge for entering a pricing zone. This control strategy encourages controllable AVs to adopt SO routing while discouraging UE-seeking users from entering the pricing zone. We demonstrate the performance of the proposed framework using the Nguyen network and a large-scale network model of Melbourne, Australia.      
### 27.A Multi-Agent Deep Reinforcement Learning Approach for a Distributed Energy Marketplace in Smart Grids  [ :arrow_down: ](https://arxiv.org/pdf/2009.10905.pdf)
>  This paper presents a Reinforcement Learning (RL) based energy market for a prosumer dominated microgrid. The proposed market model facilitates a real-time and demanddependent dynamic pricing environment, which reduces grid costs and improves the economic benefits for prosumers. Furthermore, this market model enables the grid operator to leverage prosumers storage capacity as a dispatchable asset for grid support applications. Simulation results based on the Deep QNetwork (DQN) framework demonstrate significant improvements of the 24-hour accumulative profit for both prosumers and the grid operator, as well as major reductions in grid reserve power utilization.      
### 28.Demand Responsive Dynamic Pricing Framework for Prosumer Dominated Microgrids using Multiagent Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.10890.pdf)
>  Demand Response (DR) has a widely recognized potential for improving grid stability and reliability while reducing customers energy bills. However, the conventional DR techniques come with several shortcomings, such as inability to handle operational uncertainties and incurring customer disutility, impeding their wide spread adoption in real-world applications. This paper proposes a new multiagent Reinforcement Learning (RL) based decision-making environment for implementing a Real-Time Pricing (RTP) DR technique in a prosumer dominated microgrid. The proposed technique addresses several shortcomings common to traditional DR methods and provides significant economic benefits to the grid operator and prosumers. To show its better efficacy, the proposed DR method is compared to a baseline traditional operation scenario in a small-scale microgrid system. Finally, investigations on the use of prosumers energy storage capacity in this microgrid highlight the advantages of the proposed method in establishing a balanced market setup.      
### 29.Schizophrenia-mimicking layers outperform conventional neural network layers  [ :arrow_down: ](https://arxiv.org/pdf/2009.10887.pdf)
>  We have reported nanometer-scale three-dimensional studies of brain networks of schizophrenia cases and found that their neurites are thin and tortuous compared to healthy controls. This suggests that connections between distal neurons are impaired in microcircuits of the schizophrenia cases. In this study, we applied this biological findings to designing schizophrenia-mimicking artificial neural network to simulate the connection impairment in the disorder. Neural networks having the schizophrenia connection layer in place of fully connected layer were subjected to image classification tasks using MNIST and CIFAR-10 datasets. The obtained results revealed that the schizophrenia connection layer is tolerant to overfitting and outperforms fully connected layer. Schizophrenia-mimicking convolution layer was also tested with the VGG configuration, showing that 60% of kernel weights of the last convolution layer can be eliminated while keeping competitive performance. Schizophrenia-mimicking layers can be used instead of fully-connected or convolution layers without any change in the network configuration and training procedures, hence the outperformance of the schizophrenia-mimicking layer is easily incorporated in neural networks. The results of this study indicate that the connection impairment in schizophrenia is not a burden to the brain, but has some functional roles to attain a better brain performance. We suggest that the seemingly neuropathological alterations observed in schizophrenia have been rationally implemented in our brain during the process of biological evolution.      
### 30.Unfolding WMMSE using Graph Neural Networks for Efficient Power Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2009.10812.pdf)
>  We study the problem of optimal power allocation in a single-hop ad hoc wireless network. In solving this problem, we depart from classical purely model-based approaches and propose a hybrid method that retains key modeling elements in conjunction with data-driven components. More precisely, we put forth a neural network architecture inspired by the algorithmic unfolding of the iterative weighted minimum mean squared error (WMMSE) method, that we denote by unfolded WMMSE (UWMMSE). The learnable weights within UWMMSE are parameterized using graph neural networks (GNNs), where the time-varying underlying graphs are given by the fading interference coefficients in the wireless network. These GNNs are trained through a gradient descent approach based on multiple instances of the power allocation problem. We show that the proposed architecture is permutation equivariant, thus facilitating generalizability across network topologies. Comprehensive numerical experiments illustrate the performance attained by UWMMSE along with its robustness to hyper-parameter selection and generalizability to unseen scenarios such as different network densities and network sizes.      
### 31.Adaptive Debanding Filter  [ :arrow_down: ](https://arxiv.org/pdf/2009.10804.pdf)
>  Banding artifacts, which manifest as staircase-like color bands on pictures or video frames, is a common distortion caused by compression of low-textured smooth regions. These false contours can be very noticeable even on high-quality videos, especially when displayed on high-definition screens. Yet, relatively little attention has been applied to this problem. Here we consider banding artifact removal as a visual enhancement problem, and accordingly, we solve it by applying a form of content-adaptive smoothing filtering followed by dithered quantization, as a post-processing module. The proposed debanding filter is able to adaptively smooth banded regions while preserving image edges and details, yielding perceptually enhanced gradient rendering with limited bit-depths. Experimental results show that our proposed debanding filter outperforms state-of-the-art false contour removing algorithms both visually and quantitatively.      
### 32.Resource Aware Pricing for Electric Vehicle Charging  [ :arrow_down: ](https://arxiv.org/pdf/2009.10771.pdf)
>  Electric vehicle charging facilities offer electric charge and parking to users for a fee. Both parking availability and electric charge capacity are constrained resources, and as the demand for charging facilities grows with increasing electric vehicle adoption, so too does the potential for exceeding these resource limitations. In this paper, we study how prices set by the charging facility impact the likelihood that resource constraints are exceeded. Specifically, we present probabilistic bounds on the number of charging spots and the total power supply needed at a facility based on the characteristics of the arriving vehicles. We assume the charging facility either offers a set of distinct and fixed charging rates to each user or allows the user to decide a charging deadline, from which a charging rate is determined. Users arrive randomly, requiring a random amount of charge. Additionally, each user has a random impatience factor that quantifies their value of time, and a random desired time to stay at a particular location. Assuming rational user behavior, and with knowledge of the probability distribution of the random parameters, we present high-confidence bounds on the total number of vehicles parked at the station and the aggregate power use of all vehicles actively charging. We demonstrate how these bounds can be used by a charging facility to determine appropriate prices and investigate through a Monte-Carlo simulation case study the tightness of the bounds.      
### 33.Cranial Implant Prediction using Low-Resolution 3D Shape Completion and High-Resolution 2D Refinement  [ :arrow_down: ](https://arxiv.org/pdf/2009.10769.pdf)
>  Designing of a cranial implant needs a 3D understanding of the complete skull shape. Thus, taking a 2D approach is sub-optimal, since a 2D model lacks a holistic 3D view of both the defective and healthy skulls. Further, loading the whole 3D skull shapes at its original image resolution is not feasible in commonly available GPUs. To mitigate these issues, we propose a fully convolutional network composed of two subnetworks. The first subnetwork is designed to complete the shape of the downsampled defective skull. The second subnetwork upsamples the reconstructed shape slice-wise. We train the 3D and 2D networks together end-to-end, with a hierarchical loss function. Our proposed solution accurately predicts a high-resolution 3D implant in the challenge test case in terms of dice-score and the Hausdorff distance.      
### 34.Age-Net: An MRI-Based Iterative Framework for Biological Age Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2009.10765.pdf)
>  The concept of biological age (BA) - although important in clinical practice - is hard to grasp mainly due to lack of a clearly defined reference standard. For specific applications, especially in pediatrics, medical image data are used for BA estimation in a routine clinical context. Beyond this young age group, BA estimation is restricted to whole-body assessment using non-imaging indicators such as blood biomarkers, genetic and cellular data. However, various organ systems may exhibit different aging characteristics due to lifestyle and genetic factors. Thus, a whole-body assessment of the BA does not reflect the deviations of aging behavior between organs. To this end, we propose a new imaging-based framework for organ-specific BA estimation. As a first step, we introduce a chronological age (CA) estimation framework using deep convolutional neural networks (Age-Net). We quantitatively assess the performance of this framework in comparison to existing CA estimation approaches. Furthermore, we expand upon Age-Net with a novel iterative data-cleaning algorithm to segregate atypical-aging patients (BA $\not \approx$ CA) from the given population. In this manner, we hypothesize that the remaining population should approximate the true BA behaviour. For this initial study, we apply the proposed methodology on a brain magnetic resonance image (MRI) dataset containing healthy individuals as well as Alzheimer's patients with different dementia ratings. We demonstrate the correlation between the predicted BAs and the expected cognitive deterioration in Alzheimer's patients. A statistical and visualization-based analysis has provided evidence regarding the potential and current challenges of the proposed methodology.      
### 35.Learning Mixtures of Low-Rank Models  [ :arrow_down: ](https://arxiv.org/pdf/2009.11282.pdf)
>  We study the problem of learning mixtures of low-rank models, i.e. reconstructing multiple low-rank matrices from unlabelled linear measurements of each. This problem enriches two widely studied settings -- low-rank matrix sensing and mixed linear regression -- by bringing latent variables (i.e. unknown labels) and structural priors (i.e. low-rank structures) into consideration. To cope with the non-convexity issues arising from unlabelled heterogeneous data and low-complexity structure, we develop a three-stage meta-algorithm that is guaranteed to recover the unknown matrices with near-optimal sample and computational complexities under Gaussian designs. In addition, the proposed algorithm is provably stable against random noise. We complement the theoretical studies with empirical evidence that confirms the efficacy of our algorithm.      
### 36.Deep Learning-Based Reconstruction of Interventional Tools from Four X-Ray Projections for Tomographic Interventional Guidance  [ :arrow_down: ](https://arxiv.org/pdf/2009.10993.pdf)
>  Image guidance for minimally invasive interventions is usually performed by acquiring fluoroscopic images using a C-arm system. However, the projective data provide only limited information about the spatial structure and position of interventional tools such as stents, guide wires or coils. In this work we propose a deep learning-based pipeline for real-time tomographic (four-dimensional) interventional guidance at acceptable dose levels. In the first step, interventional tools are extracted from four cone-beam CT projections using a deep convolutional neural network (CNN). These projections are then reconstructed and fed into a second CNN, which maps this highly undersampled reconstruction to a segmentation of the interventional tools. Our pipeline is capable of reconstructing interventional tools from only four x-ray projections without the need for a patient prior with very high accuracy. Therefore, the proposed approach is capable of overcoming the drawbacks of today's interventional guidance and could enable the development of new minimally invasive radiological interventions by providing full spatiotemporal information about the interventional tools.      
### 37.Terahertz Massive MIMO with Holographic Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2009.10963.pdf)
>  We propose a holographic version of a reconfigurable intelligent surface (RIS) and investigate its application to terahertz (THz) massive multiple-input multiple-output systems. Capitalizing on the miniaturization of THz electronic components, RISs can be implemented by densely packing subwavelength unit cells, so as to realize continuous or quasi-continuous apertures and to enable holographic communications. In this paper, in particular, we derive the beam pattern of a holographic RIS. Our analysis reveals that the beam pattern of an ideal holographic RIS can be well approximated by that of an ultra-dense RIS, which has a more practical hardware architecture. In addition, we propose a closedloop channel estimation (CE) scheme to effectively estimate the broadband channels that characterize THz massive MIMO systems aided by holographic RISs. The proposed CE scheme includes a downlink coarse CE stage and an uplink finer-grained CE stage. The uplink pilot signals are judiciously designed for obtaining good CE performance. Moreover, to reduce the pilot overhead, we introduce a compressive sensing-based CE algorithm, which exploits the dual sparsity of THz MIMO channels in both the angular and delay domain. Simulation results demonstrate the superiority of holographic RISs over the nonholographic ones, and the effectiveness of the proposed CE scheme.      
### 38.Improving Medical Annotation Quality to Decrease Labeling Burden Using Stratified Noisy Cross-Validation  [ :arrow_down: ](https://arxiv.org/pdf/2009.10858.pdf)
>  As machine learning has become increasingly applied to medical imaging data, noise in training labels has emerged as an important challenge. Variability in diagnosis of medical images is well established; in addition, variability in training and attention to task among medical labelers may exacerbate this issue. Methods for identifying and mitigating the impact of low quality labels have been studied, but are not well characterized in medical imaging tasks. For instance, Noisy Cross-Validation splits the training data into halves, and has been shown to identify low-quality labels in computer vision tasks; but it has not been applied to medical imaging tasks specifically. In this work we introduce Stratified Noisy Cross-Validation (SNCV), an extension of noisy cross validation. SNCV can provide estimates of confidence in model predictions by assigning a quality score to each example; stratify labels to handle class imbalance; and identify likely low-quality labels to analyze the causes. We assess performance of SNCV on diagnosis of glaucoma suspect risk from retinal fundus photographs, a clinically important yet nuanced labeling task. Using training data from a previously-published deep learning model, we compute a continuous quality score (QS) for each training example. We relabel 1,277 low-QS examples using a trained glaucoma specialist; the new labels agree with the SNCV prediction over the initial label &gt;85% of the time, indicating that low-QS examples mostly reflect labeler errors. We then quantify the impact of training with only high-QS labels, showing that strong model performance may be obtained with many fewer examples. By applying the method to randomly sub-sampled training dataset, we show that our method can reduce labelling burden by approximately 50% while achieving model performance non-inferior to using the full dataset on multiple held-out test sets.      
### 39.My Health Sensor, my Classifier: Adapting a Trained Classifier to Unlabeled End-User Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.10799.pdf)
>  In this work, we present an approach for unsupervised domain adaptation (DA) with the constraint, that the labeled source data are not directly available, and instead only access to a classifier trained on the source data is provided. Our solution, iteratively labels only high confidence sub-regions of the target data distribution, based on the belief of the classifier. Then it iteratively learns new classifiers from the expanding high-confidence dataset. The goal is to apply the proposed approach on DA for the task of sleep apnea detection and achieve personalization based on the needs of the patient. In a series of experiments with both open and closed sleep monitoring datasets, the proposed approach is applied to data from different sensors, for DA between the different datasets. The proposed approach outperforms in all experiments the classifier trained in the source domain, with an improvement of the kappa coefficient that varies from 0.012 to 0.242. Additionally, our solution is applied to digit classification DA between three well established digit datasets, to investigate the generalizability of the approach, and to allow for comparison with related work. Even without direct access to the source data, it achieves good results, and outperforms several well established unsupervised DA methods.      
### 40.Efficient DWT-based fusion techniques using genetic algorithm for optimal parameter estimation  [ :arrow_down: ](https://arxiv.org/pdf/2009.10777.pdf)
>  Image fusion plays a vital role in medical imaging. Image fusion aims to integrate complementary as well as redundant information from multiple modalities into a single fused image without distortion or loss of information. In this research work, discrete wavelet transform (DWT)and undecimated discrete wavelet transform (UDWT)-based fusion techniques using genetic algorithm (GA)foroptimalparameter(weight)estimationinthefusionprocessareimplemented and analyzed with multi-modality brain images. The lack of shift variance while performing image fusion using DWT is addressed using UDWT. The proposed fusion model uses an efficient, modified GA in DWT and UDWT for optimal parameter estimation, to improve the image quality and contrast. The complexity of the basic GA (pixel level) has been reduced in the modified GA (feature level), by limiting the search space. It is observed from our experiments that fusion using DWT and UDWT techniques with GA for optimal parameter estimation resulted in a better fused image in the aspects of retaining the information and contrast without error, both in human perception as well as evaluation using objective metrics. The contributions of this research work are (1) reduced time and space complexity in estimating the weight values using GA for fusion (2) system is scalable for input image of any size with similar time complexity, owing to feature level GA implementation and (3) identification of source image that contributes more to the fused image, from the weight values estimated.      
### 41.URLLC with Massive MIMO: Analysis and Design at Finite Blocklength  [ :arrow_down: ](https://arxiv.org/pdf/2009.10550.pdf)
>  The fast adoption of Massive MIMO for high-throughput communications was enabled by many research contributions mostly relying on infinite-blocklength information-theoretic bounds. This makes it hard to assess the suitability of Massive MIMO for ultra-reliable low-latency communications (URLLC) operating with short blocklength codes. This paper provides a rigorous framework for the characterization and numerical evaluation (using the saddlepoint approximation) of the error probability achievable in the uplink and downlink of Massive MIMO at finite blocklength. The framework encompasses imperfect channel state information, pilot contamination, spatially correlated channels, and arbitrary linear spatial processing. In line with previous results based on infinite-blocklength bounds, we prove that, with minimum mean-square error (MMSE) processing and spatially correlated channels, the error probability at finite blocklength goes to zero as the number $M$ of antennas grows to infinity, even under pilot contamination. On the other hand, numerical results for a practical URLLC network setup involving a base station with $M=100$ antennas, show that a target error probability of $10^{-5}$ can be achieved with MMSE processing, uniformly over each cell, only if orthogonal pilot sequences are assigned to all the users in the network. Maximum ratio processing does not suffice.      
