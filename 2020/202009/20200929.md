# ArXiv eess --Tue, 29 Sep 2020
### 1.A Wideband Sliding Correlator based Channel Sounder in 65 nm CMOS: An Evaluation Board Design  [ :arrow_down: ](https://arxiv.org/pdf/2009.13490.pdf)
>  Wide swaths of bandwidth at millimeter-wave (mmWave) and Terahertz (THz) frequencies stimulate diverse applications in wireless sensing, imaging, position location, cloud computing, and much more. These emerging applications motivate wireless communications hardware to operate with multi Gigahertz (GHz) bandwidth, at nominal costs, minimal size, and power consumption. Channel sounding system implementations currently used to study and measure wireless channels utilize numerous commercially available components from multiple manufacturers that result in a complex and large assembly with many costly and fragile cable interconnections between the constituents and commonly achieve a system bandwidth under one GHz. This paper presents an evaluation board (EVB) design that features a sliding correlator-based channel sounder with 2 GHz null-to-null RF bandwidth in a single monolithic integrated circuit (IC) fabricated in 65 nm CMOS technology. The EVB landscape provides necessary peripherals for signal interfacing, amplification, buffering, and enables integration into both the transmitter and receiver of a channel sounding system, thereby reducing complexity, size, and cost through integrated design. The channel sounder IC on the EVB is the worlds first to report gigabit-per-second baseband operation using low-cost CMOS technology, allowing the global research community to now have an inexpensive and compact channel sounder system with nanosecond time resolution capability for the detection of multipath signals in a wireless channel.      
### 2.Siamese Capsule Network for End-to-End Speaker Recognition In The Wild  [ :arrow_down: ](https://arxiv.org/pdf/2009.13480.pdf)
>  We propose an end-to-end deep model for speaker verification in the wild. Our model uses thin-ResNet for extracting speaker embeddings from utterances and a Siamese capsule network and dynamic routing as the Back-end to calculate a similarity score between the embeddings. We conduct a series of experiments and comparisons on our model to state-of-the-art solutions, showing that our model outperforms all the other models using substantially less amount of training data. We also perform additional experiments to study the impact of different speaker embeddings on the Siamese capsule network. We show that the best performance is achieved by using embeddings obtained directly from the feature aggregation module of the Front-end and passing them to higher capsules using dynamic routing.      
### 3.Topology Learning in Radial Dynamical Systems with Unreliable Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.13458.pdf)
>  Many complex engineering systems admit bidirectional and linear couplings between their agents. Blind and passive methods to identify such influence pathways/couplings from data are central to many applications. However, dynamically related data-streams originating at different sources are prone to corruption caused by asynchronous time-stamps of different streams, packet drops and noise. Such imperfect information may be present in the entire observation period, and hence not detected by change-detection algorithms that require an initial clean observation period. Prior work has shown that spurious links are inferred in the graph structure due to the corrupted data-streams, which prevents consistent learning. In this article, we provide a novel approach to detect the location of corrupt agents as well as present an algorithm to learn the structure of radial dynamical systems despite corrupted data streams. In particular, we show that our approach provably learns the true radial structure if the unknown corrupted nodes are at least three hops away from each other. Our theoretical results are further validated in test dynamical network.      
### 4.Universal Physiological Representation Learning with Soft-Disentangled Rateless Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2009.13453.pdf)
>  Human computer interaction (HCI) involves a multidisciplinary fusion of technologies, through which the control of external devices could be achieved by monitoring physiological status of users. However, physiological biosignals often vary across users and recording sessions due to unstable physical/mental conditions and task-irrelevant activities. To deal with this challenge, we propose a method of adversarial feature encoding with the concept of a Rateless Autoencoder (RAE), in order to exploit disentangled, nuisance-robust, and universal representations. We achieve a good trade-off between user-specific and task-relevant features by making use of the stochastic disentanglement of the latent representations by adopting additional adversarial networks. The proposed model is applicable to a wider range of unknown users and tasks as well as different classifiers. Results on cross-subject transfer evaluations show the advantages of the proposed framework, with up to an 11.6% improvement in the average subject-transfer classification accuracy.      
### 5.Robust Model Predictive Longitudinal Position Tracking Control for an Autonomous Vehicle Based on Multiple Models  [ :arrow_down: ](https://arxiv.org/pdf/2009.13406.pdf)
>  The aim of this work is to control the longitudinal position of an autonomous vehicle with an internal combustion engine. The powertrain has an inherent dead-time characteristic and constraints on physical states apply since the vehicle is neither able to accelerate arbitrarily strong, nor to drive arbitrarily fast. A model predictive controller (MPC) is able to cope with both of the aforementioned system properties. MPC heavily relies on a model and therefore a strategy on how to obtain multiple linear state space prediction models of the nonlinear system via input/output data system identification from acceleration data is given. The models are identified in different regions of the vehicle dynamics in order to obtain more accurate predictions. The still remaining plant-model mismatch can be expressed as an additive disturbance which can be handled through robust control theory. Therefore modifications to the models for applying robust MPC tracking control theory are described. Then a controller which guarantees robust constraint satisfaction and recursive feasibility is designed. As a next step, modifications to apply the controller on multiple models are discussed. In this context, a model switching strategy is provided and theoretical and computational limitations are pointed out. Lastly, simulation results are presented and discussed, including computational load when switching between systems.      
### 6.Latency-Robust Control of High-Speed Signal-Free Intersections  [ :arrow_down: ](https://arxiv.org/pdf/2009.13373.pdf)
>  High-speed signal-free intersections are a novel urban traffic operations enabled by connected and autonomous vehicles. However, the impact of communication latency on intersection performance has not been well understood. In this paper, we consider vehicle coordination at signal-free intersections with latency. We focus on two questions: (i) how to ensure latency-resiliency of the coordination algorithm, and (ii) how latency affects the intersection's capacity. We consider a trajectory-based model with bounded speed uncertainties. Latency leads to uncertain state observation. We propose a piecewise-linear control law that ensures safety (avoidance of interference) as long as the initial condition is safe. We also analytically quantify the throughput that the proposed control can attain in the face of latency.      
### 7.Characterizing Inter-Numerology Interference in Mixed-Numerology OFDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.13348.pdf)
>  The advent of mixed-numerology multi-carrier (MN-MC) techniques adds flexibilities in supporting heterogeneous services in fifth generation (5G) communication systems and beyond. However, the coexistence of mixed numerologies destroys the orthogonality principle defined for single-numerology orthogonal frequency division multiplexing (SN-OFDM) systems with overlapping subcarriers of uniform subcarrier spacing. Consequently, the loss of orthogonality leads to inter-numerology interference (INI), which complicates signal generation and impedes signal isolation. In this paper, the INI in MN-OFDM systems is characterized through mathematical modeling and is shown to primarily rely on system parameters with regard to the pulse shape, the relative distance between subcarriers and the numerology scaling factor. Reduced-form formulas for the INI in continuous-time and discrete-time MN systems are derived. The derived mathematical framework facilitates the study of the effect of discretization on the INI and partial orthogonality existing in subsets of the subcarriers. The reduced-form formulas can also be used in developing interference metrics and designing mitigation techniques.      
### 8.Frequency-Domain Modelling of Reset Control Systems using an Impulsive Description  [ :arrow_down: ](https://arxiv.org/pdf/2009.13341.pdf)
>  The ever-increasing industry desire for improved performance makes linear controller design run into fundamental limitations. Nonlinear control methods such as Reset Control (RC) are needed to overcome these. RC is a promising candidate since, unlike other nonlinear methods, it easily integrates into the industry-preferred PID design framework. Thus far, RC has been analysed in the frequency domain either through describing function analysis or by direct closed-loop numerical computation. The former computes a simplified closed-loop RC response by assuming a sufficient low-pass behaviour. In doing so it ignores all harmonics, which literature has found to cause significant modelling prediction errors. The latter gives a precise solution, but by its direct closed-loop computation does not clearly show how open-loop RC design translates to closed-loop performance. The main contribution of this work is aimed at overcoming these limitations by considering an alternative approach for modelling RC using state-dependent impulse inputs. This permits accurately computing closed-loop RC behaviour starting from the underlying linear system, improving system understanding. A frequency-domain description for closed-loop RC is obtained, which is solved analytically by using several well-defined assumptions. This analytical solution is verified using a simulated high-precision stage, critically examining sources of modelling errors. The accuracy of the proposed method is further substantiated using controllers designed for various specifications.      
### 9.Improving Radio Systems Efficiency via Employing SCRO-SOA (Squared Cosine Roll off Filter -- Semiconductor Optical Amplifier Technology) in DWDM-RoF System  [ :arrow_down: ](https://arxiv.org/pdf/2009.13340.pdf)
>  The increasing demand on the internet trafficking to meet the demands of video streaming and mobile communication has exerted too much pressure. Accordingly, this demand will result in providing high bandwidth which in turn increase the use of cells on the network. However, the current networks do not meet the requirements of the necessary data rate. Therefore, the Dense Wavelength Division Multiplexing (DWDM) network and the Radio over Fiber (RoF) technology are the ideal solution for providing the necessary data rate needed in the current networks. The DWDM, will increase the transmission distance and will increase the data rate; nonetheless, the DWDM network will be compromised especially by the Non-linear effects. This study in intended to propose a system to find solutions for the issues of increasing the data rate and for reducing the nonlinear effects. There are number of technologies that could be adopted to fix such issues, which includes; Optical Phase Conjugation (OPC), Semiconductor Optical Amplifier (SOA),      
### 10.Database Assisted Nonlinear Least Squares Algorithm for Visible Light Positioning in NLOS Environments  [ :arrow_down: ](https://arxiv.org/pdf/2009.13326.pdf)
>  We propose an indoor localization algorithm for visible light systems by considering effects of non-line-of-sight (NLOS) propagation. The proposed algorithm, named database assisted nonlinear least squares (DA-NLS), utilizes ideas from both the classical NLS algorithm and the fingerprinting algorithm to achieve accurate and robust localization performance in NLOS environments. In particular, a database is used to learn NLOS related parameters, which are employed in an NLS algorithm to estimate the position. The performance of the proposed algorithm is compared against that of the fingerprinting and NLS algorithms.      
### 11.AI Progress in Skin Lesion Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.13323.pdf)
>  We examine progress in the use of AI for detecting skin lesions, with particular emphasis on the erythema migrans rash of acute Lyme disease, and other lesions, such as those from conditions like herpes zoster (shingles), tinea corporis, erythema multiforme, cellulitis, insect bites, or tick bites. We discuss important challenges for these applications, including the problem of AI bias especially regarding the lack of skin images in dark skinned individuals, being able to accurately detect, delineate, and segment lesions or regions of interest compared to normal skin in images, and the challenge of low shot learning (addressing classification with a paucity of training images). Solving these problems ranges from being highly desirable requirements, e.g. for delineation, which may be useful to disambiguate between similar types of lesions, and perform improved diagnostics, or required, as is the case for AI de-biasing, to allow for the deployment of fair AI techniques in the clinic for skin lesion analysis. For the problem of low shot learning in particular, we report skin analysis algorithms that gracefully degrade and still perform well at low shots, when compared to baseline algorithms: when using a little as 10 training exemplars per class, the baseline DL algorithm performance significantly degrades, with accuracy of 56.41%, close to chance, whereas the best performing low shot algorithm yields an accuracy of 83.33%.      
### 12.High-throughput molecular imaging via deep learning enabled Raman spectroscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.13318.pdf)
>  Raman spectroscopy enables non-destructive, label-free imaging with unprecedented molecular contrast but is limited by slow data acquisition, largely preventing high-throughput imaging applications. Here, we present a comprehensive framework for higher-throughput molecular imaging via deep learning enabled Raman spectroscopy, termed DeepeR, trained on a large dataset of hyperspectral Raman images, with over 1.5 million spectra (400 hours of acquisition) in total. We firstly perform denoising and reconstruction of low signal-to-noise ratio Raman molecular signatures via deep learning, with a 9x improvement in mean squared error over state-of-the-art Raman filtering methods. Next, we develop a neural network for robust 2-4x super-resolution of hyperspectral Raman images that preserves molecular cellular information. Combining these approaches, we achieve Raman imaging speed-ups of up to 160x, enabling high resolution, high signal-to-noise ratio cellular imaging in under one minute. Finally, transfer learning is applied to extend DeepeR from cell to tissue-scale imaging. DeepeR provides a foundation that will enable a host of higher-throughput Raman spectroscopy and molecular imaging applications across biomedicine.      
### 13.Characterization of Covid-19 Dataset using Complex Networks and Image Processing  [ :arrow_down: ](https://arxiv.org/pdf/2009.13302.pdf)
>  This paper aims to explore the structure of pattern behind covid-19 dataset. The dataset includes medical images with positive and negative cases. A sample of 100 sample is chosen, 50 per each class. An histogram frequency is calculated to get features using statistical measurements, besides a feature extraction using Grey Level Co-Occurrence Matrix (GLCM). Using both features are build Complex Networks respectively to analyze the adjacency matrices and check the presence of patterns. Initial experiments introduces the evidence of hidden patterns in the dataset for each class, which are visible using Complex Networks representation.      
### 14.Communicate to Learn at the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2009.13269.pdf)
>  Bringing the success of modern machine learning (ML) techniques to mobile devices can enable many new services and businesses, but also poses significant technical and research challenges. Two factors that are critical for the success of ML algorithms are massive amounts of data and processing power, both of which are plentiful, yet highly distributed at the network edge. Moreover, edge devices are connected through bandwidth- and power-limited wireless links that suffer from noise, time-variations, and interference. Information and coding theory have laid the foundations of reliable and efficient communications in the presence of channel imperfections, whose application in modern wireless networks have been a tremendous success. However, there is a clear disconnect between the current coding and communication schemes, and the ML algorithms deployed at the network edge. In this paper, we challenge the current approach that treats these problems separately, and argue for a joint communication and learning paradigm for both the training and inference stages of edge learning.      
### 15.Robust MIMO Radar Waveform-Filter Design for Extended Target Detection in the Presence of Multipath  [ :arrow_down: ](https://arxiv.org/pdf/2009.13261.pdf)
>  The existence of multipath brings extra "looks" of targets. This paper considers the extended target detection problem with a narrow band Multiple-Input Multiple-Output(MIMO) radar in the presence of multipath from the view of waveform-filter design. The goal is to maximize the worst-case Signal-to-Interference-pulse-Noise Ratio(SINR) at the receiver against the uncertainties of the target and multipath reflection coefficients. Moreover, a Constant Modulus Constraint(CMC) is imposed on the transmit waveform to meet the actual demands of radar. Two types of uncertainty sets are taken into consideration. One is the spherical uncertainty set. In this case, the max-min waveform-filter design problem belongs to the non-convex concave minimax problems, and the inner minimization problem is converted to a maximization problem based on Lagrange duality with the strong duality property. Then the optimal waveform is optimized with Semi-Definite Relaxation(SDR) and randomization schemes. Therefore, we call the optimization algorithm Duality Maximization Semi-Definite Relaxation(DMSDR). Additionally, we further study the case of annular uncertainty set which belongs to non-convex non-concave minimax problems. In order to address it, the SDR is utilized to approximate the inner minimization problem with a convex problem, then the inner minimization problem is reformulated as a maximization problem based on Lagrange duality. We resort to a sequential optimization procedure alternating between two SDR problems to optimize the covariance matrix of transmit waveform and receive filter, so we call the algorithm Duality Maximization Double Semi-Definite Relaxation(DMDSDR). The convergences of DMDSDR are proved theoretically. Finally, numerical results highlight the effectiveness and competitiveness of the proposed algorithms as well as the optimized waveform-filter pair.      
### 16.Reference Signal-Aided Channel Estimation in Spatial Media-Based Modulation Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.13231.pdf)
>  In this letter, we study the reference signal-aided channel estimation concept which is a crucial requirement to address the realistic performance of spatial media-based modulation (SMBM) systems where the radio frequency mirrors are deployed along with the multiple transmit antennas. Accordingly, least squares and linear minimum mean square error-based channel estimation schemes are proposed for MBM-based systems for the first time in the literature where former studies mainly assume either perfect channel state information or an error model on channel coefficients. In addition, corresponding symbol detection performance is studied. To measure the efficiency of the proposed channel estimation approaches, the theoretical upper bounds on average bit error rate are derived and shown to be well overlapped with the computer simulations for the medium and high signal-to-noise ratio regions. This study is important due to the implementation of channel estimation as well as the theoretical derivation of detection bounds for MBM-based communication systems.      
### 17.Deep EvoGraphNet Architecture For Time-Dependent Brain Graph Data Synthesis From a Single Timepoint  [ :arrow_down: ](https://arxiv.org/pdf/2009.13217.pdf)
>  Learning how to predict the brain connectome (i.e. graph) development and aging is of paramount importance for charting the future of within-disorder and cross-disorder landscape of brain dysconnectivity evolution. Indeed, predicting the longitudinal (i.e., time-dependent ) brain dysconnectivity as it emerges and evolves over time from a single timepoint can help design personalized treatments for disordered patients in a very early stage. Despite its significance, evolution models of the brain graph are largely overlooked in the literature. Here, we propose EvoGraphNet, the first end-to-end geometric deep learning-powered graph-generative adversarial network (gGAN) for predicting time-dependent brain graph evolution from a single timepoint. Our EvoGraphNet architecture cascades a set of time-dependent gGANs, where each gGAN communicates its predicted brain graphs at a particular timepoint to train the next gGAN in the cascade at follow-up timepoint. Therefore, we obtain each next predicted timepoint by setting the output of each generator as the input of its successor which enables us to predict a given number of timepoints using only one single timepoint in an end- to-end fashion. At each timepoint, to better align the distribution of the predicted brain graphs with that of the ground-truth graphs, we further integrate an auxiliary Kullback-Leibler divergence loss function. To capture time-dependency between two consecutive observations, we impose an l1 loss to minimize the sparse distance between two serialized brain graphs. A series of benchmarks against variants and ablated versions of our EvoGraphNet showed that we can achieve the lowest brain graph evolution prediction error using a single baseline timepoint. Our EvoGraphNet code is available at <a class="link-external link-http" href="http://github.com/basiralab/EvoGraphNet" rel="external noopener nofollow">this http URL</a>.      
### 18.Index and Composition Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2009.13214.pdf)
>  In this paper, we propose a novel modulation concept which we call \emph{index and composition modulation (ICM)}. In the proposed concept, we use indices of active/deactive codeword elements and compositions of an integer to encode information. In this regard, we first determine the activated codeword elements, then we exploit energy levels of these elements to identify the compositions. We depict a practical scheme for using ICM with orthogonal frequency division multiplexing (OFDM) and show that OFDM with ICM (OFDM-ICM) can enhance the spectral efficiency (SE) and error performance of OFDM-IM. We design an efficient low-complexity detector for the proposed technique. Moreover, we analyze the error and SE performance of the OFDM-ICM technique and show that it is capable of outperforming existing OFDM benchmarks in terms of error and SE performance.      
### 19.Large-Scale Parameters of Spatio-Temporal Short-Range Indoor Backhaul Channels at 140 GHz  [ :arrow_down: ](https://arxiv.org/pdf/2009.13209.pdf)
>  The use of above-100 GHz radio frequencies would be one of promising approaches to enhance the fifth-generation cellular further. Any air interface and cellular network designs require channel models, for which measured evidence of largescale parameters such as pathloss, delay and angular spreads, is crucial. This paper provides the evidence from quasi-static spatiotemporal channel sounding campaigns at two indoor hotspot (InH) scenarios at 140 GHz band, assuming short-range backhaul connectivity. The measured two InH sites are shopping mall and airport check-in hall. Our estimated omni-directional large-scale parameters from the measurements are found in good match with those of the Third Generation Partnership Project (3GPP) for new radios (NR) channel model in InH scenario, despite the difference of assumed link types and radio frequency range. The 3GPP NR channel model is meant for access links and said to be valid up to 100 GHz, while our measurements cover shortrange backhaul scenarios at 140 GHz. We found more deviation between our estimated large-scale parameters and those of the 3GPP NR channel model in the airport than in the shopping mall.      
### 20.A Comparative Study Between a Classical and Optimal Controller for a Quadrotor  [ :arrow_down: ](https://arxiv.org/pdf/2009.13175.pdf)
>  This paper presents a simulation-based comparison between the two controllers, Proportional Integral Derivative (PID), a classical controller and Linear Quadratic Regulator (LQR), an optimal controller, for a linearized quadrotor model. To simplify an otherwise complicated dynamic model of a quadrotor, we derive a linear mathematical model using Newtonian and Euler's laws and applying basic principles of physics. This derivation gives the equations that govern the motion of a quadrotor, both concerning the body frame and the inertial frame. A state-space model is developed, which is then used to simulate the control algorithms for the quadrotor. Apart from the classic PID control algorithm, LQR is an optimal control regulator, and it is more robust for a quadrotor. Both the controllers are simulated in Simulink under the same initial conditions and show a satisfactory response.      
### 21.Automated Pancreas Segmentation Using Multi-institutional Collaborative Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.13148.pdf)
>  The performance of deep learning-based methods strongly relies on the number of datasets used for training. Many efforts have been made to increase the data in the medical image analysis field. However, unlike photography images, it is hard to generate centralized databases to collect medical images because of numerous technical, legal, and privacy issues. In this work, we study the use of federated learning between two institutions in a real-world setting to collaboratively train a model without sharing the raw data across national boundaries. We quantitatively compare the segmentation models obtained with federated learning and local training alone. Our experimental results show that federated learning models have higher generalizability than standalone training.      
### 22.Integrated Communication and Localization in mmWave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.13135.pdf)
>  As the fifth-generation (5G) mobile communication system is being commercialized, extensive studies on the evolution of 5G and sixth-generation mobile communication systems have been conducted. Future mobile communication systems are evidently evolving towards a more intelligent and software-reconfigurable functionality paradigm that can provide ubiquitous communication and also sense, control, and optimize wireless environments. Thus, integrating communication and localization by utilizing the highly directional transmission characteristics of millimeter-wave (mmWave) is a promising route. This approach not only expands the localization capabilities of a communication system but also provides new concepts and opportunities to enhance communication. In this paper, we explain the integrated communication and localization in mmWave systems, in which these processes share the same set of hardware architecture and algorithm. We also perform an overview of the key enabling technologies and the basic knowledge on localization. Then, we provide two promising directions for studies on localization with extremely large antenna array and model-based neural networks. We also discuss a comprehensive guidance for location-assisted mmWave communications in terms of channel estimation, channel state information feedback, beam tracking, synchronization, interference control, resource allocation, and user selection. Finally, this paper outlines the future trends on the mutual assistance and enhancement of communication and localization in integrated systems.      
### 23.A Novel Approach for Ridge Detection and Mode Retrieval of Multicomponent Signals Based on STFT  [ :arrow_down: ](https://arxiv.org/pdf/2009.13123.pdf)
>  Time-frequency analysis is often used to study non stationary multicomponent signals, which can be viewed as the surperimposition of modes, associated with ridges in the TF plane. To understand such signals, it is essential to identify their constituent modes. This is often done by performing ridge detection in the time-frequency plane which is then followed by mode retrieval. Unfortunately, existing ridge detectors are often not enough robust to noise therefore hampering mode retrieval. In this paper, we therefore develop a novel approach to ridge detection and mode retrieval based on the analysis of the short-time Fourier transform of multicomponent signals in the presence of noise, which will prove to be much more robust than state-of-the-art methods based on the same time-frequency representation.      
### 24.Medical Image Segmentation Using Deep Learning: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2009.13120.pdf)
>  Deep learning has been widely used for medical image segmentation and a large number of papers has been presented recording the success of deep learning in the field. In this paper, we present a comprehensive thematic survey on medical image segmentation using deep learning techniques. This paper makes two original contributions. Firstly, compared to traditional surveys that directly divide literatures of deep learning on medical image segmentation into many groups and introduce literatures in detail for each group, we classify currently popular literatures according to a multi-level structure from coarse to fine. Secondly, this paper focuses on supervised and weakly supervised learning approaches, without including unsupervised approaches since they have been introduced in many old surveys and they are not popular currently. For supervised learning approaches, we analyze literatures in three aspects: the selection of backbone networks, the design of network blocks, and the improvement of loss functions. For weakly supervised learning approaches, we investigate literature according to data augmentation, transfer learning, and interactive segmentation, separately. Compared to existing surveys, this survey classifies the literatures very differently from before and is more convenient for readers to understand the relevant rationale and will guide them to think of appropriate improvements in medical image segmentation based on deep learning approaches.      
### 25.Deep Reinforcement Learning for DER Cyber-Attack Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2009.13088.pdf)
>  The increasing penetration of DER with smart-inverter functionality is set to transform the electrical distribution network from a passive system, with fixed injection/consumption, to an active network with hundreds of distributed controllers dynamically modulating their operating setpoints as a function of system conditions. This transition is being achieved through standardization of functionality through grid codes and/or international standards. DER, however, are unique in that they are typically neither owned nor operated by distribution utilities and, therefore, represent a new emerging attack vector for cyber-physical attacks. Within this work we consider deep reinforcement learning as a tool to learn the optimal parameters for the control logic of a set of uncompromised DER units to actively mitigate the effects of a cyber-attack on a subset of network DER.      
### 26.The Geometric Unscented Kalman Filter  [ :arrow_down: ](https://arxiv.org/pdf/2009.13079.pdf)
>  Many filters have been proposed in recent decades for the nonlinear state estimation problem. The linearization-based extended Kalman filter (EKF) is widely applied to nonlinear industrial systems. As EKF is limited in accuracy and reliability, sequential Monte-Carlo methods or particle filters (PF) can obtain superior accuracy at the cost of a huge number of random samples. The unscented Kalman filter (UKF) can achieve adequate accuracy more efficiently by using deterministic samples, but its weights may be negative, which might cause instability problem. For Gaussian filters, the cubature Kalman filter (CKF) and Gauss Hermit filter (GHF) employ cubature and respectively Gauss-Hermite rules to approximate statistic information of random variables and exhibit impressive performances in practical problems. Inspired by this work, this paper presents a new nonlinear estimation scheme named after geometric unscented Kalman filter (GUF). The GUF chooses the filtering framework of CKF for updating data and develops a geometric unscented sampling (GUS) strategy for approximating random variables. The main feature of GUS is selecting uniformly distributed samples according to the probability and geometric location similar to UKF and CKF, and having positive weights like PF. Through such way, GUF can maintain adequate accuracy as GHF with reasonable efficiency and good stability. The GUF does not suffer from the exponential increase of sample size as for PF or failure to converge resulted from non-positive weights as for high order CKF and UKF.      
### 27.Learned Variable-Rate Multi-Frequency Image Compression using Modulated Generalized Octave Convolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.13074.pdf)
>  In this proposal, we design a learned multi-frequency image compression approach that uses generalized octave convolutions to factorize the latent representations into high-frequency (HF) and low-frequency (LF) components, and the LF components have lower resolution than HF components, which can improve the rate-distortion performance, similar to wavelet transform. Moreover, compared to the original octave convolution, the proposed generalized octave convolution (GoConv) and octave transposed-convolution (GoTConv) with internal activation layers preserve more spatial structure of the information, and enable more effective filtering between the HF and LF components, which further improve the performance. In addition, we develop a variable-rate scheme using the Lagrangian parameter to modulate all the internal feature maps in the auto-encoder, which allows the scheme to achieve the large bitrate range of the JPEG AI with only three models. Experiments show that the proposed scheme achieves much better Y MS-SSIM than VVC. In terms of YUV PSNR, our scheme is very similar to HEVC.      
### 28.A spatial algorithm for the analysis of transportation systems using statistical model checking  [ :arrow_down: ](https://arxiv.org/pdf/2009.13053.pdf)
>  We present an automated methodology for using Automatic Vehicle Location measurements of public transportation vehicles to construct a probabilistic model. The model not only allows for accurate evaluation of service performance, but also makes it possible to study the effects of system modifications a priori. The methodology is almost entirely agnostic to otherwise important details of the service -- in particular its route and the location of stops. Instead, it infers this from the data using automated map generation techniques. The behaviour of vehicles in the model is analysed using computer simulation combined with statistical model checking. We present two case studies involving the Airlink service in Edinburgh and the Bellevue Express in Seattle. To demonstrate the usefulness of the approach, we analyse the impact of the scheduling strategies of bus holding and speed modification on the Airlink's performance. The data and code used to create the figures are publicly available online.      
### 29.Mixture of Spectral Generative Adversarial Networks for Imbalanced Hyperspectral Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2009.13037.pdf)
>  We propose a three-player spectral generative adversarial network (GAN) architecture to afford GAN with the ability to manage minority classes under imbalance conditions. A class-dependent mixture generator spectral GAN (MGSGAN) has been developed to force generated samples remain within the domain of the actual distribution of the data. MGSGAN is able to generate minority classes even when the imbalance ratio of majority to minority classes is high. A classifier based on lower features is adopted with a sequential discriminator to form a three-player GAN game. The generator networks perform data augmentation to improve the classifier's performance. The proposed method has been validated through two hyperspectral images datasets and compared with state-of-the-art methods under two class-imbalance settings corresponding to real data distributions.      
### 30.Cloud Removal for Remote Sensing Imagery via Spatial Attention Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.13015.pdf)
>  Optical remote sensing imagery has been widely used in many fields due to its high resolution and stable geometric properties. However, remote sensing imagery is inevitably affected by climate, especially clouds. Removing the cloud in the high-resolution remote sensing satellite image is an indispensable pre-processing step before analyzing it. For the sake of large-scale training data, neural networks have been successful in many image processing tasks, but the use of neural networks to remove cloud in remote sensing imagery is still relatively small. We adopt generative adversarial network to solve this task and introduce the spatial attention mechanism into the remote sensing imagery cloud removal task, proposes a model named spatial attention generative adversarial network (SpA GAN), which imitates the human visual mechanism, and recognizes and focuses the cloud area with local-to-global spatial attention, thereby enhancing the information recovery of these areas and generating cloudless images with better quality...      
### 31.3-D Statistical Indoor Channel Model for Millimeter-Wave and Sub-Terahertz Bands  [ :arrow_down: ](https://arxiv.org/pdf/2009.12971.pdf)
>  Millimeter-wave (mmWave) and Terahertz (THz) will be used in the sixth-generation (6G) wireless systems, especially for indoor scenarios. This paper presents an indoor three-dimensional (3-D) statistical channel model for mmWave and sub-THz frequencies, which is developed from extensive channel propagation measurements conducted in an office building at 28 GHz and 140 GHz in 2014 and 2019. Over 15,000 power delay profiles (PDPs) were recorded to study channel statistics such as the number of time clusters, cluster delays, and cluster powers. All the parameters required in the channel generation procedure are derived from empirical measurement data for 28 GHz and 140 GHz line-of-sight (LOS) and non-line-of-sight (NLOS) scenarios. The channel model is validated by showing that the simulated root mean square (RMS) delay spread and RMS angular spread yield good agreements with measured values. An indoor channel simulation software is built upon the popular NYUSIM outdoor channel simulator, which can generate realistic channel impulse response, PDP, and power angular spectrum.      
### 32.Decentralized Age-of-Information Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2009.12961.pdf)
>  Age-of-Information (AoI) is a performance metric for scheduling systems that measures the freshness of the data available at the intended destination. AoI is formally defined as the time elapsed since the destination received the recent most update from the source. We consider the problem of scheduling to minimize the cumulative AoI in a multi-source multi-channel setting. Our focus is on the setting where channel statistics are unknown and we model the problem as a distributed multi-arm bandit problem. For an appropriately defined AoI regret metric, we provide analytical performance guarantees of an existing UCB-based policy for the distributed multi-arm bandit problem. In addition, we propose a novel policy based on Thomson Sampling and a hybrid policy that tries to balance the trade-off between the aforementioned policies. Further, we develop AoI-aware variants of these policies in which each source takes its current AoI into account while making decisions. We compare the performance of various policies via simulations.      
### 33.Control oriented modeling of TCLs  [ :arrow_down: ](https://arxiv.org/pdf/2009.12960.pdf)
>  Thermostatically controlled loads (TCLs) have the potential to be a valuable resource for the Balancing Authority (BA) of the future. Examples of TCLs include household appliances such as air conditioners, water heaters, and refrigerators. Since the rated power of each TCL is on the order of kilowatts, to provide meaningful service for the BA, it is necessary to control large collections of TCLs. To perform design of a distributed coordination/control algorithm, the BA requires a control oriented model that describes the relevant dynamics of an ensemble. Works focusing on solely modeling the ensemble date back to the 1980's, while works focusing on control oriented modeling are more recent. In this work, we contribute to the control oriented modeling literature. We leverage techniques from computational fluid dynamics (CFD) to discretize a pair of Fokker-Planck equations derived in earlier work [1]. The discretized equations are shown to admit a certain factorization, which makes the developed model useful for control design. In particular, the effects of weather and control are shown to independently effect the system dynamics.      
### 34.Analysis of a Markovian Queuing Model for Autonomous Signal-Free Intersection  [ :arrow_down: ](https://arxiv.org/pdf/2009.12938.pdf)
>  We consider a novel, analytical queuing model for vehicle coordination at signal-free intersections. Vehicles arrive at an intersection according to Poisson processes, and the crossing times are constants dependent of vehicle types. We use this model to quantitatively relate key operational parameters (vehicle speed/acceleration, inter-vehicle headway) to key performance metrics (throughput and delay) under the first-come-first-serve rule. We use the Foster-Lyapunov drift condition to obtain stability criteria and an upper bound for average time delay. Based on these results, we compare the efficiency of signal free intersections with conventional vehicles and with connected and autonomous vehicles. We also validate our results in Simulation of Urban Mobility (SUMO).      
### 35.Classification and understanding of cloud structures via satellite images with EfficientUNet  [ :arrow_down: ](https://arxiv.org/pdf/2009.12931.pdf)
>  Climate change has been a common interest and the forefront of crucial political discussion and decision-making for many years. Shallow clouds play a significant role in understanding the Earth's climate, but they are challenging to interpret and represent in a climate model. By classifying these cloud structures, there is a better possibility of understanding the physical structures of the clouds, which would improve the climate model generation, resulting in a better prediction of climate change or forecasting weather update. Clouds organise in many forms, which makes it challenging to build traditional rule-based algorithms to separate cloud features. In this paper, classification of cloud organization patterns was performed using a new scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet as the encoder and UNet as decoder where they worked as feature extractor and reconstructor of fine grained feature map and was used as a classifier, which will help experts to understand how clouds will shape the future climate. By using a segmentation model in a classification task, it was shown that with a good encoder alongside UNet, it is possible to obtain good performance from this dataset. Dice coefficient has been used for the final evaluation metric, which gave the score of 66.26% and 66.02% for public and private leaderboard on Kaggle competition respectively.      
### 36.Learning to Improve Image Compression without Changing the Standard Decoder  [ :arrow_down: ](https://arxiv.org/pdf/2009.12927.pdf)
>  In recent years we have witnessed an increasing interest in applying Deep Neural Networks (DNNs) to improve the rate-distortion performance in image compression. However, the existing approaches either train a post-processing DNN on the decoder side, or propose learning for image compression in an end-to-end manner. This way, the trained DNNs are required in the decoder, leading to the incompatibility to the standard image decoders (\eg, JPEG) in personal computers and mobiles. Therefore, we propose learning to improve the encoding performance with the standard decoder. In this paper, We work on JPEG as an example. Specifically, a frequency-domain pre-editing method is proposed to optimize the distribution of DCT coefficients, aiming at facilitating the JPEG compression. Moreover, we propose learning the JPEG quantization table jointly with the pre-editing network. Most importantly, we do not modify the JPEG decoder and therefore our approach is applicable when viewing images with the widely used standard JPEG decoder. The experiments validate that our approach successfully improves the rate-distortion performance of JPEG in terms of various quality metrics, such as PSNR, MS-SSIM and LPIPS. Visually, this translates to better overall color retention especially when strong compression is applied.      
### 37.Formal Verification of Safety Critical Autonomous Systems via Bayesian Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2009.12909.pdf)
>  As control systems become increasingly more complex, there exists a pressing need to find systematic ways of verifying them. To address this concern, there has been significant work in developing test generation schemes for black-box control architectures. These schemes test a black-box control architecture's ability to satisfy its control objectives, when these objectives are expressed as operational specifications through temporal logic formulae. Our work extends these prior, model based results by lower bounding the probability by which the black-box system will satisfy its operational specification, when subject to a pre-specified set of environmental phenomena. We do so by systematically generating tests to minimize a Lipschitz continuous robustness measure for the operational specification. We demonstrate our method with experimental results, wherein we show that our framework can reasonably lower bound the probability of specification satisfaction.      
### 38.ESTAN: Enhanced Small Tumor-Aware Network for Breast Ultrasound Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.12894.pdf)
>  Breast tumor segmentation is a critical task in computer-aided diagnosis (CAD) systems for breast cancer detection because accurate tumor size, shape and location are important for further tumor quantification and classification. However, segmenting small tumors in ultrasound images is challenging, due to the speckle noise, varying tumor shapes and sizes among patients, and the existence of tumor-like image regions. Recently, deep learning-based approaches have achieved great success for biomedical image analysis, but current state-of-the-art approaches achieve poor performance for segmenting small breast tumors. In this paper, we propose a novel deep neural network architecture, namely Enhanced Small Tumor-Aware Network (ESTAN), to accurately and robustly segment breast tumors. ESTAN introduces two encoders to extract and fuse image context information at different scales and utilizes row-column-wise kernels in the encoder to adapt to breast anatomy. We validate the proposed approach and compare it to nine state-of-the-art approaches on three public breast ultrasound datasets using seven quantitative metrics. The results demonstrate that the proposed approach achieves the best overall performance and outperforms all other approaches on small tumor segmentation.      
### 39.RRA-U-Net: a Residual Encoder to Attention Decoder by Residual Connections Framework for Spine Segmentation under Noisy Labels  [ :arrow_down: ](https://arxiv.org/pdf/2009.12873.pdf)
>  Segmentation algorithms of medical image volumes are widely studied for many clinical and research purposes. We propose a novel and efficient framework for medical image segmentation. The framework functions under a deep learning paradigm, incorporating four novel contributions. Firstly, a residual interconnection is explored in different scale encoders. Secondly, four copy and crop connections are replaced to residual-block-based concatenation to alleviate the disparity between encoders and decoders, respectively. Thirdly, convolutional attention modules for feature refinement are studied on all scale decoders. Finally, an adaptive clean noisy label learning strategy(ACNLL) based on the training process from underfitting to overfitting is studied. Experimental results are illustrated on a publicly available benchmark database of spine CTs. Our segmentation framework achieves competitive performance with other state-of-the-art methods over a variety of different evaluation measures.      
### 40.Grant-Free Access via Bilinear Inference for Cell-Free MIMO with Low-Coherent Pilots  [ :arrow_down: ](https://arxiv.org/pdf/2009.12863.pdf)
>  We propose a novel joint activity, channel, and data estimation (JACDE) scheme for cell-free multiple-input multiple-output (MIMO) systems compliant with fifth-generation (5G) new radio (NR) orthogonal frequency-division multiplexing (OFDM) signaling. The contribution aims to allow significant overhead reduction of cell-free MIMO systems by enabling grant-free access, while maintaining moderate throughput per user. To that end, we extend the conventional MIMO OFDM protocol so as to incorporate activity detection capability without resorting to spreading informative data symbols, in contrast with related work which typically relies on signal spreading. Our method leverages a Bayesian message passing scheme based on Gaussian approximation, which jointly performs active user detection (AUD), channel estimation (CE), and multi-user detection (MUD), incorporating also a well-structured low-coherent pilot design based on frame theory, which mitigates pilot contamination, and finally complemented with a detector empowered by bilinear message passing. The efficacy of the resulting JACDE-based grant-free access scheme without spreading data sequences is demonstrated by simulation results, which are shown to significantly outperform the current state-of-the-art and approach the performance of an idealized (genie-aided) scheme in which user activity and channel coefficients are perfectly known.      
### 41.A Machine Learning Approach to DoA Estimation and Model Order Selection for Antenna Arrays with Subarray Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2009.12858.pdf)
>  In this paper, we study the problem of direction of arrival estimation and model order selection for systems employing subarray sampling. Thereby, we focus on scenarios, where the number of active sources may exceed the number of simultaneously sampled antenna elements. For this purpose, we propose new schemes based on neural networks and estimators that combine neural networks with gradient steps on the likelihood function. These methods are able to outperform existing estimators in terms of mean squared error and model selection accuracy, especially in the low snapshot domain, at a drastically lower computational complexity.      
### 42.Learning event-driven switched linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.12831.pdf)
>  We propose an automata theoretic learning algorithm for the identification of black-box switched linear systems whose switching logics are event-driven. A switched system is expressed by a deterministic finite automaton (FA) whose node labels are the subsystem matrices. With information about the dimensions of the matrices and the set of events, and with access to two oracles, that can simulate the system on a given input, and provide counter-examples when given an incorrect hypothesis automaton, we provide an algorithm that outputs the unknown FA. Our algorithm first uses the oracle to obtain the node labels of the system run on a given input sequence of events, and then extends Angluin's \(L^*\)-algorithm to determine the FA that accepts the language of the given FA. We demonstrate the performance of our learning algorithm on a set of benchmark examples.      
### 43.Intelligent Reflecting Surface Enhanced Indoor Robot Path Planning: A Radio Map based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.12804.pdf)
>  An indoor robot navigation system is investigated, where an intelligent reflecting surface (IRS) is employed to enhance the connectivity between the access point (AP) and robotic users. Both single-user and multiple-user scenarios are considered. In the single-user scenario, one mobile robotic user communicates with the AP. In the multiple-user scenario, the AP serves one mobile robotic user and one static robotic user employing either NOMA or OMA transmission. The considered system is optimized for minimization of the travelling time/distance of the mobile robotic user from a given starting point to a predefined final location, while satisfying constraints on the communication quality of the robotic users. To tackle this problem, a radio map based approach is proposed to exploit location-dependent channel propagation knowledge. For the single-user scenario, a channel power gain map is constructed, which characterizes the spatial distribution of the maximum expected effective channel power gain of the mobile robotic user for the optimal IRS phase shifts. Based on the obtained channel power gain map, the communication-aware robot path planing problem is solved by exploiting graph theory. For the multiple-user scenario, an achievable communication rate map is constructed. It characterizes the spatial distribution of the maximum expected rate of the mobile robotic user for the optimal power allocation at the AP and the optimal IRS phase shifts subject to a minimum rate requirement for the static robotic user. The joint optimization problem is efficiently solved by invoking bisection search and successive convex approximation methods. Then, a graph theory based solution for the robot path planning problem is derived by exploiting the obtained communication rate map. Our numerical results verify the effectiveness of the proposed designs.      
### 44.Solid-State Inrush Current Limiter Controller Based on Inrush Prediction for Large Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2009.12800.pdf)
>  Inrush current problems of the industrial transformers are occasionally more serious than the utility transformers. A new Solid-State Inrush Current Limiter (SSICL) is proposed for medium voltage power transformers. The SSICL consists of three similar sets. Each set includes a bidirectional semiconductor switch and an inrush current limiter resistor. Using a control strategy based on Kalman filtering, transformer inrush current is suppressed during magnetization process. In fact, built up profile of the inrush current is dictated in accordance with the estimated current by a slow dynamic Kalman filter. A single-phase prototype SSICL is developed and tested. The proposed control scheme is simulated in MATLAB-SIMULINK. By using Real Time (RT) toolbox of the MATLAB software and a PCI_1711U, interrelationship between the SSICL and its controlling system is realized. Some experiments are carried out to evaluate the performance of the SSICL. The results show that the proposed SSICL can considerably suppress inrush current of the transformer      
### 45.Machine Learning in Event-Triggered Control: Recent Advances and Open Issues  [ :arrow_down: ](https://arxiv.org/pdf/2009.12783.pdf)
>  Network Control Systems (NCSs) have attracted much interest over the past decade as part of a move towards more decentralised control applications and the rise of cyberphysical system applications. Many practical NCSs face the challenges of limited communication bandwidth resources, reliability and lack of knowledge of network dynamics, particularly when wireless networks are involved. Machine learning (ML) combined with event-triggered control (ETC) has the potential to ease some of these challenges. For example, ML can be used to overcome the problem of a lack of network models by learning system behaviour or adapt to dynamically changing models by continually learning model dynamics. ETC can help to conserve bandwidth resources by communicating only when needed or when resources are available. Here, we present a review of the literature on work that combines ML and ETC. The literature on supervised, semi-supervised, unsupervised and reinforcement learning based approaches such as deep reinforcement learning and statistical learning in combination with ETC is explored. Furthermore, the difference between the application of these learning algorithms on model-based and model-free systems are discussed. Following the analysis of the literature, we highlight open research questions and challenges related to ML-based ETC and propose approaches to possible solutions to these challenges.      
### 46.Iterative Reconstruction for Low-Dose CT using Deep Gradient Priors of Generative Model  [ :arrow_down: ](https://arxiv.org/pdf/2009.12760.pdf)
>  Dose reduction in computed tomography (CT) is essential for decreasing radiation risk in clinical applications. Iterative reconstruction is one of the most promising ways to compensate for the increased noise due to reduction of photon flux. Rather than most existing prior-driven algorithms that benefit from manually designed prior functions or supervised learning schemes, in this work we integrate the data-consistency as a conditional term into the iterative generative model for low-dose CT. At first, a score-based generative network is used for unsupervised distribution learning and the gradient of generative density prior is learned from normal-dose images. Then, the annealing Langevin dynamics is employed to update the trained priors with conditional scheme, i.e., the distance between the reconstructed image and the manifold is minimized along with data fidelity during reconstruction. Experimental comparisons demonstrated the noise reduction and detail preservation abilities of the proposed method.      
### 47.Secure distributed adaptive optimal coordination of nonlinear cyber-physical systems with attack diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2009.12739.pdf)
>  This paper studies the problem of distributed optimal coordination (DOC) for a class of nonlinear large-scale cyber-physical systems (CPSs) in the presence of cyber attacks. A secure DOC architecture with attack diagnosis is proposed that guarantees the attack-free subsystems to achieve the output consensus which minimizes the sum of their objective functions, while the attacked subsystems converge to preset secure states. A two-layer DOC structure is established with emphasis on the interactions between cyber and physical layers, where a command-driven control law is designed that generates provable optimal output consensus. Differing from the existing fault diagnosis methods which are generally applicable to given failure types, the focus of the attack diagnosis is to achieve detection and isolation for arbitrary malicious behaviors. To this end, double coupling residuals are generated by a carefully-designed distributed filter. The adaptive thresholds with prescribed performance are designed to enhance the detectability and isolability. It is theoretically guaranteed that any attack signal cannot bypass the designed attack diagnosis methodology to destroy the convergence of the DOC algorithm, and the locally-occurring detectable attack can be isolated from the propagating attacks from neighboring subsystems. Simulation results for the motion coordination of multiple remotely operated underwater vehicles illustrate the effectiveness of the proposed architecture.      
### 48.Resilient Networking in Formation Flying UAVs  [ :arrow_down: ](https://arxiv.org/pdf/2009.12738.pdf)
>  The threats on cyber-physical system have changed much into a level of sophistication that elude the traditional security and protection methods. This work addresses a proactive approaches to the cyber security of a formation flying UAVs. A resilient formation control of UAVs in the presence of non-cooperative (defective or malicious) UAVs is presented. Based on local information a resilient consensus in the presence of misbehaving nodes is dealt with fault-tolerant consensus algorithm. In the proposed framework, a graph-theoretic property of network robustness conveying the notion of a direct information exchange between two sets of UAVs in the network is introduced to analyze the behavior and convergence of the distributed consensus algorithm. A distributed control policy is developed to maintain the network connectivity threshold to satisfy the topological requirement put forward for the resiliency of the consensus algorithm. Numerical examples are presented to show the applicability of the proactive approach used in dealing with the cyber attack treat on a formation flying UAVs      
### 49.On-The-Fly Control of Unknown Smooth Systems from Limited Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.12733.pdf)
>  We investigate the problem of data-driven, on-the-fly control of systems with unknown nonlinear dynamics where data from only a single finite-horizon trajectory and possibly side information on the dynamics are available. Such side information may include knowledge of the regularity of the underlying dynamics, monotonicity, or decoupling in the dynamics between the states. Specifically, we propose two algorithms, $\texttt{DaTaReach}$ and $\texttt{DaTaControl}$, to over-approximate the reachable set and design control signals for the system on the fly. $\texttt{DaTaReach}$ constructs a differential inclusion that contains the unknown dynamics. Then, it computes an over-approximation of the reachable set based on interval Taylor-based methods applied to systems with dynamics described as differential inclusions. $\texttt{DaTaControl}$ enables convex-optimization-based, near-optimal control using the computed over-approximation and the receding-horizon control framework. We provide a bound on its suboptimality and show that more data and side information enable $\texttt{DaTaControl}$ to achieve tighter suboptimality bounds. Finally, we demonstrate the efficacy of $\texttt{DaTaControl}$ over existing approaches on the problems of controlling a unicycle and quadrotor systems.      
### 50.COVID-19 Infection Map Generation and Detection from Chest X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.12698.pdf)
>  Computer-aided diagnosis has become a necessity for accurate and immediate coronavirus disease 2019 (COVID-19) detection to aid treatment and prevent the spread of the virus. Compared to other diagnosis methodologies, chest X-ray (CXR) imaging is an advantageous tool since it is fast, low-cost, and easily accessible. Thus, CXR has a great potential not only to help diagnose COVID-19 but also to track the progression of the disease. Numerous studies have proposed to use Deep Learning techniques for COVID-19 diagnosis. However, they have used very limited CXR image repositories for evaluation with a small number, a few hundreds, of COVID-19 samples. Moreover, these methods can neither localize nor grade the severity of COVID-19 infection. For this purpose, recent studies proposed to explore the activation maps of deep networks. However, they remain inaccurate for localizing the actual infestation making them unreliable for clinical use. This study proposes a novel method for the joint localization, severity grading, and detection of COVID-19 from CXR images by generating the so-called infection maps that can accurately localize and grade the severity of COVID-19 infection. To accomplish this, we have compiled the largest COVID-19 dataset up to date with 2951 COVID-19 CXR images, where the annotation of the ground-truth segmentation masks is performed on CXRs by a novel collaborative expert human-machine approach. Furthermore, we publicly release the first CXR dataset with the ground-truth segmentation masks of the COVID-19 infected regions. A detailed set of experiments show that state-of-the-art segmentation networks can learn to localize COVID-19 infection with an F1-score of 85.81%, that is significantly superior to the activation maps created by the previous methods. Finally, the proposed approach achieved a COVID-19 detection performance with 98.37% sensitivity and 99.16% specificity.      
### 51.Characterization of the Link Function in GN and EGN methods for Nonlinearity Assessment of Ultrawideband Coherent Fiber Optic Communication Systems with Raman Effect  [ :arrow_down: ](https://arxiv.org/pdf/2009.12687.pdf)
>  In this paper, we present an accurate and numerically efficient method to implement the GN and EGN nonlinearity prediction methods when the power evolution along the fiber is in an arbitrary form. This approach will provide us with a reliable tool to efficiently use GN and Enhanced GN (EGN) methods for analysis of the nonlinearity for ultrawideband coherent fiber optic WDM systems (C+L band or even wider bandwidth systems) in the presence of the Inter-channel Stimulated Raman Scattering (ISRS) and systems using forward-pumped or/and backward-pumped Raman amplification.      
### 52.Characterizing and Detecting Freezing of Gait using Multi-modal Physiological Signals  [ :arrow_down: ](https://arxiv.org/pdf/2009.12660.pdf)
>  Freezing-of-gait a mysterious symptom of Parkinsons disease and defined as a sudden loss of ability to move forward. Common treatments of freezing episodes are currently of moderate efficacy and can likely be improved through a reliable freezing evaluation. Basic-science studies about the characterization of freezing episodes and a 24/7 evidence-support freezing detection system can contribute to the reliability of the evaluation in daily life. In this study, we analyzed multi-modal features from brain, eye, heart, motion, and gait activity from 15 participants with idiopathic Parkinsons disease and 551 freezing episodes induced by turning in place. Statistical analysis was first applied on 248 of the 551 to determine which multi-modal features were associated with freezing episodes. Features significantly associated with freezing episodes were ranked and used for the freezing detection. We found that eye-stabilization speed during turning and lower-body trembling measure significantly associated with freezing episodes and used for freezing detection. Using a leave-one-subject-out cross-validation, we obtained a sensitivity of 97%+/-3%, a specificity of 96%+/-7%, a precision of 73%+/-21%, a Matthews correlation coefficient of 0.82+/-0.15, and an area under the Precision-Recall curve of 0.94+/-0.05. According to the Precision-Recall curves, the proposed freezing detection method using the multi-modal features performed better than using single-modal features.      
### 53.ADMM-Net for Communication Interference Removal in Stepped-Frequency Radar  [ :arrow_down: ](https://arxiv.org/pdf/2009.12651.pdf)
>  Complex ADMM-Net, a complex-valued neural network architecture inspired by the alternating direction method of multipliers (ADMM), is designed for interference removal in super-resolution stepped frequency radar angle-range-doppler imaging. Tailored to an uncooperative scenario wherein a MIMO radar shares spectrum with communications, the ADMM-Net recovers the radar image---which is assumed to be sparse---and simultaneously removes the communication interference, which is modeled as sparse in the frequency domain owing to spectrum underutilization. The scenario motivates an $\ell_1$-minimization problem whose ADMM iteration, in turn, undergirds the neural network design, yielding a set of generalized ADMM iterations that have learnable hyperparameters and operations. To train the network we use random data generated according to the radar and communication signal models. In numerical experiments ADMM-Net exhibits markedly lower error and computational cost than ADMM and CVX.      
### 54.Quantitative and Qualitative Evaluation of Explainable Deep Learning Methods for Ophthalmic Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2009.12648.pdf)
>  Background: The lack of explanations for the decisions made by algorithms such as deep learning has hampered their acceptance by the clinical community despite highly accurate results on multiple problems. Recently, attribution methods have emerged for explaining deep learning models, and they have been tested on medical imaging problems. The performance of attribution methods is compared on standard machine learning datasets and not on medical images. In this study, we perform a comparative analysis to determine the most suitable explainability method for retinal OCT diagnosis. <br>Methods: A commonly used deep learning model known as Inception v3 was trained to diagnose 3 retinal diseases - choroidal neovascularization (CNV), diabetic macular edema (DME), and drusen. The explanations from 13 different attribution methods were rated by a panel of 14 clinicians for clinical significance. Feedback was obtained from the clinicians regarding the current and future scope of such methods. <br>Results: An attribution method based on a Taylor series expansion, called Deep Taylor was rated the highest by clinicians with a median rating of 3.85/5. It was followed by two other attribution methods, Guided backpropagation and SHAP (SHapley Additive exPlanations). <br>Conclusion: Explanations of deep learning models can make them more transparent for clinical diagnosis. This study compared different explanations methods in the context of retinal OCT diagnosis and found that the best performing method may not be the one considered best for other deep learning tasks. Overall, there was a high degree of acceptance from the clinicians surveyed in the study. <br>Keywords: explainable AI, deep learning, machine learning, image processing, Optical coherence tomography, retina, Diabetic macular edema, Choroidal Neovascularization, Drusen      
### 55.A practical method for pupil segmentation in challenging conditions  [ :arrow_down: ](https://arxiv.org/pdf/2009.12639.pdf)
>  Various methods have been proposed for authentication, including password or pattern drawing, which is clearly visible on personal electronic devices. However, these methods of authentication are more vulnerable, as passwords and cards can be forgotten, lost, or stolen. Therefore, a great curiosity has developed in individual authentication using biometric methods that are based on physical and behavioral features not possible to forget or be stolen. Authentication methods are used widely in portable devices since the lifetime of battery and time response are essential concerns in these devices. Due to the fact that these systems need to be fast and low power, designing efficient methods is still critical. We, in this paper, proposed a new low power and fast method for pupil segmentation based on approximate computing that under trading a minor level of accuracy, significant improvement in power assumption and time saving can be obtained and makes this algorithm suitable for hardware implementation. Furthermore, the experimental results of PSNR and SSIM show that the error rate in this method is negligible.      
### 56.Image quality enhancement in wireless capsule endoscopy with adaptive fraction gamma transformation and unsharp masking filter  [ :arrow_down: ](https://arxiv.org/pdf/2009.12631.pdf)
>  Wireless Capsule Endoscopy (WCE) presented in 2001 as one of the key approaches to observe the entire gastrointestinal (GI) tract, generally the small bowels. It has been used to detect diseases in the gastrointestinal tract. Endoscopic image analysis is still a required field with many open problems. The quality of many images it produced is rather unacceptable due to the nature of this imaging system, which causes some issues to prognosticate by physicians and computer-aided diagnosis. In this paper, a novel technique is proposed to improve the quality of images captured by the WCE. More specifically, it enhanced the brightness, contrast, and preserve the color information while reducing its computational complexity. Furthermore, the experimental results of PSNR and SSIM confirm that the error rate in this method is near to the ground and negligible. Moreover, the proposed method improves intensity restricted average local entropy (IRMLE) by 22%, color enhancement factor (CEF) by 10%, and can keep the lightness of image effectively. The performances of our method have better visual quality and objective assessments in compare to the state-of-art methods.      
### 57.Deep Learning-based Four-region Lung Segmentation in Chest Radiography for COVID-19 Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2009.12610.pdf)
>  Purpose. Imaging plays an important role in assessing severity of COVID 19 pneumonia. However, semantic interpretation of chest radiography (CXR) findings does not include quantitative description of radiographic opacities. Most current AI assisted CXR image analysis framework do not quantify for regional variations of disease. To address these, we proposed a four region lung segmentation method to assist accurate quantification of COVID 19 pneumonia. Methods. A segmentation model to separate left and right lung is firstly applied, and then a carina and left hilum detection network is used, which are the clinical landmarks to separate the upper and lower lungs. To improve the segmentation performance of COVID 19 images, ensemble strategy incorporating five models is exploited. Using each region, we evaluated the clinical relevance of the proposed method with the Radiographic Assessment of the Quality of Lung Edema (RALE). Results. The proposed ensemble strategy showed dice score of 0.900, which is significantly higher than conventional methods (0.854 0.889). Mean intensities of segmented four regions indicate positive correlation to the extent and density scores of pulmonary opacities under the RALE framework. Conclusion. A deep learning based model in CXR can accurately segment and quantify regional distribution of pulmonary opacities in patients with COVID 19 pneumonia.      
### 58.fMRI Multiple Missing Values Imputation Regularized by a Recurrent Denoiser  [ :arrow_down: ](https://arxiv.org/pdf/2009.12602.pdf)
>  Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging technique with pivotal importance due to its scientific and clinical applications. As with any widely used imaging modality, there is a need to ensure the quality of the same, with missing values being highly frequent due to the presence of artifacts or sub-optimal imaging resolutions. Our work focus on missing values imputation on multivariate signal data. To do so, a new imputation method is proposed consisting on two major steps: spatial-dependent signal imputation and time-dependent regularization of the imputed signal. A novel layer, to be used in deep learning architectures, is proposed in this work, bringing back the concept of chained equations for multiple imputation. Finally, a recurrent layer is applied to tune the signal, such that it captures its true patterns. Both operations yield an improved robustness against state-of-the-art alternatives.      
### 59.Contract-based Time-of-use Pricing for Energy Storage Investment  [ :arrow_down: ](https://arxiv.org/pdf/2009.12599.pdf)
>  Time-of-use (ToU) pricing is widely used by the electricity utility. A carefully designed ToU pricing can incentivize end-users' energy storage deployment, which helps shave the system peak load and reduce the system social cost. However, the optimization of ToU pricing is highly non-trivial, and an improperly designed ToU pricing may lead to storage investments that are far from the social optimum. In this paper, we aim at designing the optimal ToU pricing, jointly considering the social cost of the utility and the storage investment decisions of users. Since the storage investment costs are users' private information, we design low-complexity contracts to elicit the necessary information and induce the proper behavior of users' storage investment. The proposed contracts only specify three contract items, which guides users of arbitrarily many different storage-cost types to invest in full, partial, or no storage capacity with respect to their peak demands. Our contracts can achieve the social optimum when the utility knows the aggregate demand of each storage-cost type (but not the individual user's type). When the utility only knows the distribution of each storage-cost type's demand, our contracts can lead to a near-optimal solution. The gap with the social optimum is as small as 1.5% based on the simulations using realistic data. We also show that the proposed contracts can reduce the system social cost by over 30%, compared with no storage investment benchmark.      
### 60.Potential Features of ICU Admission in X-ray Images of COVID-19 Patients  [ :arrow_down: ](https://arxiv.org/pdf/2009.12597.pdf)
>  X-ray images may present non-trivial features with predictive information of patients that develop severe symptoms of COVID-19. If true, this hypothesis may have practical value in allocating resources to particular patients while using a relatively inexpensive imaging technique. The difficulty of testing such a hypothesis comes from the need for large sets of labelled data, which not only need to be well-annotated but also should contemplate the post-imaging severity outcome. On this account, this paper presents a methodology for extracting features from a limited data set with outcome label (patient required ICU admission or not) and correlating its significance to an additional, larger data set with hundreds of images. The methodology employs a neural network trained to recognise lung pathologies to extract the semantic features, which are then analysed with a shallow decision tree to limit overfitting while increasing interpretability. This analysis points out that only a few features explain most of the variance between patients that developed severe symptoms. When applied to an unrelated, larger data set with labels extracted from clinical notes, the method classified distinct sets of samples where there was a much higher frequency of labels such as `Consolidation', `Effusion', and `alveolar'. A further brief analysis on the locations of such labels also showed a significant increase in the frequency of words like `bilateral', `middle', and `lower' in patients classified as with higher chances of going severe. The methodology for dealing with the lack of specific ICU label data while attesting correlations with a data set containing text notes is novel; its results suggest that some pathologies should receive higher weights when assessing disease severity.      
### 61.Quantifying the effect of image compression on supervised learning applications in optical microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.12570.pdf)
>  The impressive growth of data throughput in optical microscopy has triggered a widespread use of supervised learning (SL) models running on compressed image datasets for efficient automated analysis. However, since lossy image compression risks to produce unpredictable artifacts, quantifying the effect of data compression on SL applications is of pivotal importance to assess their reliability, especially for clinical use. We propose an experimental method to evaluate the tolerability of image compression distortions in 2D and 3D cell segmentation SL tasks: predictions on compressed data are compared to the raw predictive uncertainty, which is numerically estimated from the raw noise statistics measured through sensor calibration. We show that predictions on object- and image-specific segmentation parameters can be altered by up to 15% and more than 10 standard deviations after 16-to-8 bits downsampling or JPEG compression. In contrast, a recently developed lossless compression algorithm provides a prediction spread which is statistically equivalent to that stemming from raw noise, while providing a compression ratio of up to 10:1. By setting a lower bound to the SL predictive uncertainty, our technique can be generalized to validate a variety of data analysis pipelines in SL-assisted fields.      
### 62.Gradients of Connectivity as Graph Fourier Bases of Brain Activity  [ :arrow_down: ](https://arxiv.org/pdf/2009.12567.pdf)
>  The application of graph theory to model the complex structure and function of the brain has shed new light on its organization and function, prompting the emergence of network neuroscience. Despite the tremendous progress that has been achieved in this field, still relatively few methods exploit the topology of brain networks to analyze brain activity. Recent attempts in this direction have leveraged on graph spectral analysis and graph signal processing to decompose brain activity in connectivity eigenmodes or gradients. If results are promising in terms of interpretability and functional relevance, methodologies and terminology are sometimes confusing. The goals of this paper are twofold. First, we summarize recent contributions related to connectivity gradients and graph signal processing, and attempt a clarification of the terminology and methods used in the field, while pointing out current methodological limitations. Second, we discuss the perspective that the functional relevance of connectivity gradients could be fruitfully exploited by considering them as graph Fourier bases of brain activity.      
### 63.Deep Selective Combinatorial Embedding and Consistency Regularization for Light Field Super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.12537.pdf)
>  Light field (LF) images acquired by hand-held devices usually suffer from low spatial resolution as the limited detector resolution has to be shared with the angular dimension. LF spatial super-resolution (SR) thus becomes an indispensable part of the LF camera processing pipeline. The high-dimensionality characteristic and complex geometrical structure of LF images make the problem more challenging than traditional single-image SR. The performance of existing methods is still limited as they fail to thoroughly explore the coherence among LF sub-aperture images (SAIs) and are insufficient in accurately preserving the scene's parallax structure. To tackle this challenge, we propose a novel learning-based LF spatial SR framework. Specifically, each SAI of an LF image is first coarsely and individually super-resolved by exploring the complementary information among SAIs with selective combinatorial geometry embedding. To achieve efficient and effective selection of the complementary information, we propose two novel sub-modules conducted hierarchically: the patch selector provides an option of retrieving similar image patches based on offline disparity estimation to handle large-disparity correlations; and the SAI selector adaptively and flexibly selects the most informative SAIs to improve the embedding efficiency. To preserve the parallax structure among the reconstructed SAIs, we subsequently append a consistency regularization network trained over a structure-aware loss function to refine the parallax relationships over the coarse estimation. In addition, we extend the proposed method to irregular LF data. To the best of our knowledge, this is the first learning-based SR method for irregular LF data. Experimental results over both synthetic and real-world LF datasets demonstrate the significant advantage of our approach over state-of-the-art methods.      
### 64.Enhanced 3D Myocardial Strain Estimation from Multi-View 2D CMR Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2009.12466.pdf)
>  In this paper, we propose an enhanced 3D myocardial strain estimation procedure which combines complementary displacement information from multiple orientations of a single imaging modality (untagged CMR SSFP images). To estimate myocardial strain across the left ventricle, we register the sets of short-axis, four-chamber and twochamber views via a 2D non-rigid registration algorithm implemented in a commercial software (Segment, Medviso). We then create a series of interpolating functions for the three orthogonal directions of motion and use them to deform a tetrahedral mesh representation of a patient-specific left ventricle. Additionally, we correct for overestimation of displacement by introducing a weighting scheme that is based on displacement along the long axis. The procedure was evaluated on the STACOM 2011 dataset containing CMR SSFP images for 16 healthy volunteers. We show increased accuracy in estimating the three strain components (radial, circumferential, longitudinal) compared to reported results in the challenge, for the imaging modality of interest (SSFP). Our peak strain estimates are also significantly closer to reported measurements from studies of a larger cohort in the literature. Our proposed procedure provides a fast way to accurately reconstruct a deforming patient-specific model of the left ventricle using the commonest imaging modality routinely administered in clinical settings, without requiring additional or specialized imaging protocols.      
### 65.Lateral Force Prediction using Gaussian Process Regression for Intelligent Tire Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.12463.pdf)
>  Understanding the dynamic behavior of tires and their interactions with road plays an important role in designing integrated vehicle control strategies. Accordingly, having access to reliable information about the tire-road interactions through tire embedded sensors is very demanding for developing enhanced vehicle control systems. Thus, the main objectives of the present research work are i. to analyze data from an experimental accelerometer-based intelligent tire acquired over a wide range of maneuvers, with different vertical loads, velocities, and high slip angles; and ii. to develop a lateral force predictor based on a machine learning tool, more specifically the Gaussian Process Regression (GPR) technique. It is delineated that the proposed intelligent tire system can provide reliable information about the tire-road interactions even in the case of high slip angles. Besides, the lateral forces model based on GPR can predict forces with acceptable accuracy and provide level of uncertainties that can be very useful for designing vehicle control strategies.      
### 66.Blind Image Super-Resolution with Spatial Context Hallucination  [ :arrow_down: ](https://arxiv.org/pdf/2009.12461.pdf)
>  Deep convolution neural networks (CNNs) play a critical role in single image super-resolution (SISR) since the amazing improvement of high performance computing. However, most of the super-resolution (SR) methods only focus on recovering bicubic degradation. Reconstructing high-resolution (HR) images from randomly blurred and noisy low-resolution (LR) images is still a challenging problem. In this paper, we propose a novel Spatial Context Hallucination Network (SCHN) for blind super-resolution without knowing the degradation kernel. We find that when the blur kernel is unknown, separate deblurring and super-resolution could limit the performance because of the accumulation of error. Thus, we integrate denoising, deblurring and super-resolution within one framework to avoid such a problem. We train our model on two high quality datasets, DIV2K and Flickr2K. Our method performs better than state-of-the-art methods when input images are corrupted with random blur and noise.      
### 67.Democratizing Artificial Intelligence in Healthcare: A Study of Model Development Across Two Institutions Incorporating Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.12437.pdf)
>  The training of deep learning models typically requires extensive data, which are not readily available as large well-curated medical-image datasets for development of artificial intelligence (AI) models applied in Radiology. Recognizing the potential for transfer learning (TL) to allow a fully trained model from one institution to be fine-tuned by another institution using a much small local dataset, this report describes the challenges, methodology, and benefits of TL within the context of developing an AI model for a basic use-case, segmentation of Left Ventricular Myocardium (LVM) on images from 4-dimensional coronary computed tomography angiography. Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance. This process reduces the time required to build a new model in the clinical environment at a different institution.      
### 68.FLC tuned with Gravitational Search Algorithm for Nonlinear Pose Filter  [ :arrow_down: ](https://arxiv.org/pdf/2009.12436.pdf)
>  Nonlinear pose (\textit{i.e,} attitude and position) filters are characterized with simpler structure and better tracking performance in comparison with other methods of pose estimation. A critical factor when designing a nonlinear pose filter is the selection of the error function. Conventional design of nonlinear pose filter design trade-off between fast adaptation and robustness. This paper introduces a new practical approach based on fuzzy rules for on-line continuous tuning of the nonlinear pose filter. Each of input and output membership functions are optimally tuned using graphical search algorithm optimization considering both pose error and its rate of change. The proposed approach is characterized with high adaptation features and strong level of robustness. Therefore, the proposed approach results of robust and fast convergence properties. The simulation results show the effectiveness of the proposed approach considering uncertain measurements and large error in initialization.      
### 69.Deep Artifact-Free Residual Network for Single Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.12433.pdf)
>  Recently, convolutional neural networks have shown promising performance for single-image super-resolution. In this paper, we propose Deep Artifact-Free Residual (DAFR) network which uses the merits of both residual learning and usage of ground-truth image as target. Our framework uses a deep model to extract the high-frequency information which is necessary for high-quality image reconstruction. We use a skip-connection to feed the low-resolution image to the network before the image reconstruction. In this way, we are able to use the ground-truth images as target and avoid misleading the network due to artifacts in difference image. In order to extract clean high-frequency information, we train the network in two steps. The first step is a traditional residual learning which uses the difference image as target. Then, the trained parameters of this step are transferred to the main training in the second step. Our experimental results show that the proposed method achieves better quantitative and qualitative image quality compared to the existing methods.      
### 70.Pareto-Optimal Bit Allocation for Collaborative Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2009.12430.pdf)
>  In recent studies, collaborative intelligence (CI) has emerged as a promising framework for deployment of Artificial Intelligence (AI)-based services on mobile/edge devices. In CI, the AI model (a deep neural network) is split between the edge and the cloud, and intermediate features are sent from the edge sub-model to the cloud sub-model. In this paper, we study bit allocation for feature coding in multi-stream CI systems. We model task distortion as a function of rate using convex surfaces similar to those found in distortion-rate theory. Using such models, we are able to provide closed-form bit allocation solutions for single-task systems and scalarized multi-task systems. Moreover, we provide analytical characterization of the full Pareto set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream 2-task systems. Analytical results are examined on a variety of DNN models from the literature to demonstrate wide applicability of the results      
### 71.Fundamental limitations to no-jerk gearshift in electric vehicles: the importance of transmission architectures  [ :arrow_down: ](https://arxiv.org/pdf/2009.12410.pdf)
>  Multi-speed transmissions can enhance the performance and reduce the overall cost of an electric vehicle, but they also introduce a challenge: avoiding gearshift jerk, which may sometimes prove to be impossible in the presence of motor and clutch saturation. In this article, we introduce three theorems that explicitly define the fundamental limitations to no-jerk gearshifts resulting from motor or actuator saturation. We compare gearshifts that consist of transferring transmission torque from one friction clutch to another, to the case in which one of the clutches is a one-way clutch. We show that systems with a one-way clutch are more prone to motor saturation, thus gearshift jerk is more often inevitable. We also study the influence of planetary gearsets on the gearshift dynamical trajectories, and expose the impact on the no-jerk limitations. This work offers tools to compare transmission architectures during the conceptual design phase of a new electric vehicle.      
### 72.An Optimal Computing Budget Allocation Tree Policy for Monte Carlo Tree Search  [ :arrow_down: ](https://arxiv.org/pdf/2009.12407.pdf)
>  We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to widely used Upper Confidence Bound (UCB) tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation trade-off when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption. Numerical experiments demonstrate the efficiency of our algorithm in selecting the best action at the root.      
### 73.Super-Resolution Ultrasound Localization Microscopy Based on a High Frame-rate Clinical Ultrasound Scanner: An In-human Feasibility Study  [ :arrow_down: ](https://arxiv.org/pdf/2009.13477.pdf)
>  Non-invasive detection of microvascular alterations in deep tissues in vivo provides critical information for clinical diagnosis and evaluation of a broad-spectrum of pathologies. Recently, the emergence of super-resolution ultrasound localization microscopy (ULM) offers new possibilities for clinical imaging of microvasculature at capillary level. Currently, the clinical utility of ULM on clinical ultrasound scanners is hindered by the technical limitations, such as long data acquisition time, and compromised tracking performance associated with low imaging frame-rate. Here we present an in-human ULM on a high frame-rate (HFR) clinical ultrasound scanner to achieve super-resolution microvessel imaging using a short acquisition time (&lt;10s). Ultrasound MB data were acquired from different human tissues, (liver, kidney, pancreatic, and breast tumor) using an HFR clinical scanner. By leveraging the HFR and advanced processing techniques including sub-pixel motion registration, MB signal separation, and Kalman filter-based tracking, MBs can be robustly localized and tracked for successful ULM under the circumstances of relatively high MB concentration and limited data acquisition time in humans. Subtle morphological and hemodynamic information were demonstrated on data acquired with single breath-hold and free-hand scanning. Compared with contrast-enhanced power Doppler generated based on the same MB dataset, ULM showed a 5.7-fold resolution improvement in a vessel, and provided a wide-range flow speed measurement that is Doppler angle-independent. This study demonstrated the feasibility of ultrafast in-human ULM in various human tissues based on a clinical scanner that supports HFR imaging, and showed a great potential for the implementation of super-resolution ultrasound microvessel imaging in a myriad of clinical applications involving microvascular abnormalities and pathologies.      
### 74.Partially Observable Minimum-Age Scheduling: The Greedy Policy  [ :arrow_down: ](https://arxiv.org/pdf/2009.13441.pdf)
>  This paper studies the minimum-age scheduling problem in a wireless sensor network where an access point (AP) monitors the state of an object via a set of sensors. The freshness of the sensed state, measured by the age-of-information (AoI), varies at different sensors and is not directly observable to the AP. The AP has to decide which sensor to query/sample in order to get the most updated state information of the object (i.e., the state information with the minimum AoI). In this paper, we formulate the minimum-age scheduling problem as a multi-armed bandit problem with partially observable arms and explore the greedy policy to minimize the expected AoI sampled over an infinite horizon. To analyze the performance of the greedy policy, we 1) put forth a relaxed greedy policy that decouples the sampling processes of the arms, 2) formulate the sampling process of each arm as a partially observable Markov decision process (POMDP), and 3) derive the average sampled AoI under the relaxed greedy policy as a sum of the average AoI sampled from individual arms. Numerical and simulation results validate that the relaxed greedy policy is a good approximation to the greedy policy in terms of the expected AoI sampled over an infinite horizon.      
### 75.A Study on Lip Localization Techniques used for Lip reading from a Video  [ :arrow_down: ](https://arxiv.org/pdf/2009.13420.pdf)
>  In this paper some of the different techniques used to localize the lips from the face are discussed and compared along with its processing steps. Lip localization is the basic step needed to read the lips for extracting visual information from the video input. The techniques could be applied on asymmetric lips and also on the mouth with visible teeth, tongue &amp; mouth with moustache. In the process of Lip reading the following steps are generally used. They are, initially locating lips in the first frame of the video input, then tracking the lips in the following frames using the resulting pixel points of initial step and at last converting the tracked lip model to its corresponding matched letter to give the visual information. A new proposal is also initiated from the discussed techniques. The lip reading is useful in Automatic Speech Recognition when the audio is absent or present low with or without noise in the communication systems. Human Computer communication also will require speech recognition.      
### 76.Neural Networks based approaches for Major Depressive Disorder and Bipolar Disorder Diagnosis using EEG signals: A review  [ :arrow_down: ](https://arxiv.org/pdf/2009.13402.pdf)
>  Mental disorders represent critical public health challenges as they are leading contributors to the global burden of disease and intensely influence social and financial welfare of individuals. The present comprehensive review concentrate on the two mental disorders: Major depressive Disorder (MDD) and Bipolar Disorder (BD) with noteworthy publications during the last ten years. There's a big need nowadays for phenotypic characterization of psychiatric disorders with biomarkers. Electroencephalography (EEG) signals could offer a rich signature for MDD and BD and then they could improve understanding of pathophysiological mechanisms underling these mental disorders. In this work, we focus on the literature works adopting neural networks fed by EEG signals. Among those studies using EEG and neural networks, we have discussed a variety of EEG based protocols, biomarkers and public datasets for depression and bipolar disorder detection. We conclude with a discussion and valuable recommendations that will help to improve the reliability of developed models and for more accurate and more deterministic computational intelligence based systems in psychiatry. This review will prove to be a structured and valuable initial point for the researchers working on depression and bipolar disorders recognition by using EEG signals.      
### 77.A Content Driven Resource Allocation Scheme for Video Transmission in Vehicular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.13379.pdf)
>  With the growing computer vision applications, lots of videos are transmitted for content analysis, the way to allocate resources can affect the performance of video content analysis. For this purpose, the traditional resource allocation schemes for video transmission in vehicular networks, such as qualityof-service (QoS) based or quality-of-experience (QoE) based schemes, are no longer optimal anymore. In this paper, we propose an efficient content driven resource allocation scheme for vehicles equipped with cameras under bandwidth constraints in order to improve the video content analysis performance. The proposed resource allocation scheme is based on maximizing the quality-of-content (QoC), which is related to the content analysis performance. A QoC based assessment model is first proposed. Then, the resource allocation problem is converted to a solvable convex optimization problem. Finally, simulation results show the better performance of our proposed scheme than the existing schemes like QoE based schemes.      
### 78.Analysis of IoT-Based Load Altering Attacks Against Power Grids Using the Theory of Second-Order Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.13352.pdf)
>  Recent research has shown that large-scale Internet of Things (IoT)-based load altering attacks can have a serious impact on power grid operations such as causing unsafe frequency excursions and destabilizing the grid's control loops. In this work, we present an analytical framework to investigate the impact of IoT-based static/dynamic load altering attacks (S/DLAAs) on the power grid's dynamic response. Existing work on this topic has mainly relied on numerical simulations and, to date, there is no analytical framework to identify the victim nodes from which that attacker can launch the most impactful attacks. To address these shortcomings, we use results from second-order dynamical systems to analyze the power grid frequency control loop under S/DLAAs. We use parametric sensitivity of the system's eigensolutions to identify victim nodes that correspond to the least-effort destabilizing DLAAs. Further, to analyze the SLAAs, we present closed-form expression for the system's frequency response in terms of the attacker's inputs, helping us characterize the minimum load change required to cause unsafe frequency excursions. Using these results, we formulate the defense against S/DLAAs as a linear programming problem in which we determine the minimum amount of load that needs to be secured at the victim nodes to ensure system safety/stability. Extensive simulations conducted using benchmark IEEE-bus systems validate the accuracy and efficacy of our approach.      
### 79.ECG Classification with a Convolutional Recurrent Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.13320.pdf)
>  We developed a convolutional recurrent neural network to classify 12-lead ECG signals for the challenge of PhysioNet/ Computing in Cardiology 2020 as team Pink Irish Hat. The model combines convolutional and recurrent layers, takes sliding windows of ECG signals as input and yields the probability of each class as output. The convolutional part extracts features from each sliding window. The bi-directional gated recurrent unit (GRU) layer and an attention layer aggregate these features from all windows into a single feature vector. Finally, a dense layer outputs class probabilities. The final decision is made using test time augmentation (TTA) and an optimized decision threshold. Several hyperparameters of our architecture were optimized, the most important of which turned out to be the choice of optimizer and the number of filters per convolutional layer. Our network achieved a challenge score of 0.511 on the hidden validation set and 0.167 on the full hidden test set, ranking us 23rd out of 41 in the official ranking.      
### 80.Cuid: A new study of perceived image quality and its subjective assessment  [ :arrow_down: ](https://arxiv.org/pdf/2009.13304.pdf)
>  Research on image quality assessment (IQA) remains limited mainly due to our incomplete knowledge about human visual perception. Existing IQA algorithms have been designed or trained with insufficient subjective data with a small degree of stimulus variability. This has led to challenges for those algorithms to handle complexity and diversity of real-world digital content. Perceptual evidence from human subjects serves as a grounding for the development of advanced IQA algorithms. It is thus critical to acquire reliable subjective data with controlled perception experiments that faithfully reflect human behavioural responses to distortions in visual signals. In this paper, we present a new study of image quality perception where subjective ratings were collected in a controlled lab environment. We investigate how quality perception is affected by a combination of different categories of images and different types and levels of distortions. The database will be made publicly available to facilitate calibration and validation of IQA algorithms.      
### 81.Embedding and generation of indoor climbing routes with variational autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2009.13271.pdf)
>  Recent increase in popularity of indoor climbing allows possible applications of deep learning algorthms to classify and generate climbing routes. In this work, we employ a variational autoencoder to climbing routes in a standardized training apparatus MoonBoard, a well-known training tool within the climbing community. By sampling the encoded latent space, it is observed that the algorithm can generate high quality climbing routes. 22 generated problems are uploaded to the Moonboard app for user review. This algorithm could serve as a first step to facilitate indoor climbing route setting.      
### 82.Deep Reinforcement Learning for Process Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2009.13265.pdf)
>  This paper demonstrates the application of reinforcement learning (RL) to process synthesis by presenting Distillation Gym, a set of RL environments in which an RL agent is tasked with designing a distillation train, given a user defined multi-component feed stream. Distillation Gym interfaces with a process simulator (COCO and ChemSep) to simulate the environment. A demonstration of two distillation problem examples are discussed in this paper (a Benzene, Toluene, P-xylene separation problem and a hydrocarbon separation problem), in which a deep RL agent is successfully able to learn within Distillation Gym to produce reasonable designs. Finally, this paper proposes the creation of Chemical Engineering Gym, an all-purpose reinforcement learning software toolkit for chemical engineering process synthesis.      
### 83.Texture Memory-Augmented Deep Patch-Based Image Inpainting  [ :arrow_down: ](https://arxiv.org/pdf/2009.13240.pdf)
>  Patch-based methods and deep networks have been employed to tackle image inpainting problem, with their own strengths and weaknesses. Patch-based methods are capable of restoring a missing region with high-quality texture through searching nearest neighbor patches from the unmasked regions. However, these methods bring problematic contents when recovering large missing regions. Deep networks, on the other hand, show promising results in completing large regions. Nonetheless, the results often lack faithful and sharp details that resemble the surrounding area. By bringing together the best of both paradigms, we propose a new deep inpainting framework where texture generation is guided by a texture memory of patch samples extracted from unmasked regions. The framework has a novel design that allows texture memory retrieval to be trained end-to-end with the deep inpainting network. In addition, we introduce a patch distribution loss to encourage high-quality patch synthesis. The proposed method shows superior performance both qualitatively and quantitatively on three challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris Street-View datasets.      
### 84.Discrimination of attractors with noisy nodes in Boolean networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.13198.pdf)
>  Observing the internal state of the whole system using a small number of sensor nodes is important in analysis of complex networks. Here, we study the problem of determining the minimum number of sensor nodes to discriminate attractors under the assumption that each attractor has at most K noisy nodes. We present exact and approximation algorithms for this minimization problem. The effectiveness of the algorithms is also demonstrated by computational experiments using both synthetic data and realistic biological data.      
### 85.Conditions for Regional Frequency Stability in Power Systems -- Part II: Applications  [ :arrow_down: ](https://arxiv.org/pdf/2009.13164.pdf)
>  In Part I of this paper we have introduced the closed-form conditions for guaranteeing regional frequency stability in a power system. Here we propose a methodology to represent these conditions in the form of linear constraints and demonstrate their applicability by implementing them in a generation-scheduling model. This model simultaneously optimises energy production and ancillary services for maintaining frequency stability in the event of a generation outage, by solving a frequency-secured Stochastic Unit Commitment (SUC). We consider the Great Britain system, characterised by two regions that create a non-uniform distribution of inertia: England in the South, where most of the load is located, and Scotland in the North, containing significant wind resources. Through several case studies, it is shown that inertia and frequency response cannot be considered as system-wide magnitudes in power systems that exhibit inter-area oscillations in frequency, as their location in a particular region is key to guarantee stability. In addition, securing against a medium-sized loss in the low-inertia region proves to cause significant wind curtailment, which could be alleviated through reinforced transmission corridors. In this context, the proposed constraints allow to find the optimal volume of ancillary services to be procured in each region.      
### 86.Conditions for Regional Frequency Stability in Power Systems -- Part I: Theory  [ :arrow_down: ](https://arxiv.org/pdf/2009.13163.pdf)
>  This paper considers the phenomenon of distinct regional frequencies recently observed in some power systems. We first study the causes through dynamic simulations and then develop the mathematical model describing this behaviour. Then, techniques to solve the model are discussed, demonstrating that the post-fault frequency evolution in any given region is equal to the frequency evolution of the Centre Of Inertia plus certain inter-area oscillations. This finding leads to the deduction of conditions for guaranteeing frequency stability in all regions of a power system, a deduction performed using a mixed analytical-numerical approach that combines mathematical analysis with regression methods on simulation samples. The proposed stability conditions are linear inequalities that can be implemented in any optimisation routine allowing the co-optimisation of all existing ancillary services for frequency support: inertia, multi-speed frequency response, load damping and an optimised largest power infeed. This is the first reported mathematical framework with explicit conditions to maintain frequency stability in a power system exhibiting inter-area oscillations in frequency.      
### 87.Trainable Structure Tensors for Autonomous Baggage Threat Detection Under Extreme Occlusion  [ :arrow_down: ](https://arxiv.org/pdf/2009.13158.pdf)
>  Detecting baggage threats is one of the most difficult tasks, even for expert officers. Many researchers have developed computer-aided screening systems to recognize these threats from the baggage X-ray scans. However, all of these frameworks are limited in recognizing contraband items under extreme occlusion. This paper presents a novel instance detector that utilizes a trainable structure tensor scheme to highlight the contours of the occluded and cluttered contraband items (obtained from multiple predominant orientations) while simultaneously suppressing all the other baggage content within the scan, leading to robust detection. The proposed framework has been rigorously tested on four publicly available X-ray datasets where it outperforms the state-of-the-art frameworks in terms of mean average precision scores. Furthermore, to the best of our knowledge, it is the only framework that has been rigorously tested on combined grayscale and colored scans obtained from four different types of X-ray scanners.      
### 88.Decisiveness of Stochastic Systems and its Application to Hybrid Models (Full Version)  [ :arrow_down: ](https://arxiv.org/pdf/2009.13152.pdf)
>  In [ABM07], Abdulla et al. introduced the concept of decisiveness, an interesting tool for lifting good properties of finite Markov chains to denumerable ones. Later, this concept was extended to more general stochastic transition systems (STSs), allowing the design of various verification algorithms for large classes of (infinite) STSs. We further improve the understanding and utility of decisiveness in two ways. First, we provide a general criterion for proving decisiveness of general STSs. This criterion, which is very natural but whose proof is rather technical, (strictly) generalizes all known criteria from the literature. Second, we focus on stochastic hybrid systems (SHSs), a stochastic extension of hybrid systems. We establish the decisiveness of a large class of SHSs and, under a few classical hypotheses from mathematical logic, we show how to decide reachability problems in this class, even though they are undecidable for general SHSs. This provides a decidable stochastic extension of o-minimal hybrid systems. <br>[ABM07] Parosh A. Abdulla, Noomene Ben Henda, and Richard Mayr. 2007. Decisive Markov Chains. Log. Methods Comput. Sci. 3, 4 (2007).      
### 89.Segmentation and Analysis of a Sketched Truss Frame Using Morphological Image Processing Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2009.13144.pdf)
>  Development of computational tools to analyze and assess the building capacities has had a major impact in civil engineering. The interaction with the structural software packages is becoming easier and the modeling tools are becoming smarter by automating the users role during their interaction with the software. One of the difficulties and the most time consuming steps involved in the structural modeling is defining the geometry of the structure to provide the analysis. This paper is dedicated to the development of a methodology to automate analysis of a hand sketched or computer generated truss frame drawn on a piece of paper. First, we focus on the segmentation methodologies for hand sketched truss components using the morphological image processing techniques, and then we provide a real time analysis of the truss. We visualize and augment the results on the input image to facilitate the public understanding of the truss geometry and internal forces. MATLAB is used as the programming language for the image processing purposes, and the truss is analyzed using Sap2000 API to integrate with MATLAB to provide a convenient structural analysis. This paper highlights the potential of the automation of the structural analysis using image processing to quickly assess the efficiency of structural systems. Further development of this framework is likely to revolutionize the way that structures are modeled and analyzed.      
### 90.Hamilton-Jacobi-Bellman Equations for Maximum Entropy Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.13097.pdf)
>  Maximum entropy reinforcement learning (RL) methods have been successfully applied to a range of challenging sequential decision-making and control tasks. However, most of existing techniques are designed for discrete-time systems. As a first step toward their extension to continuous-time systems, this paper considers continuous-time deterministic optimal control problems with entropy regularization. Applying the dynamic programming principle, we derive a novel class of Hamilton-Jacobi-Bellman (HJB) equations and prove that the optimal value function of the maximum entropy control problem corresponds to the unique viscosity solution of the HJB equation. Our maximum entropy formulation is shown to enhance the regularity of the viscosity solution and to be asymptotically consistent as the effect of entropy regularization diminishes. A salient feature of the HJB equations is computational tractability. Generalized Hopf-Lax formulas can be used to solve the HJB equations in a tractable grid-free manner without the need for numerically optimizing the Hamiltonian. We further show that the optimal control is uniquely characterized as Gaussian in the case of control affine systems and that, for linear-quadratic problems, the HJB equation is reduced to a Riccati equation, which can be used to obtain an explicit expression of the optimal control. Lastly, we discuss how to extend our results to continuous-time model-free RL by taking an adaptive dynamic programming approach. To our knowledge, the resulting algorithms are the first data-driven control methods that use an information theoretic exploration mechanism in continuous time.      
### 91.The value chain of Industrial IoT and its reference framework for digitalization  [ :arrow_down: ](https://arxiv.org/pdf/2009.13039.pdf)
>  Nowadays, we are rapidly moving beyond bespoke detailed solutions tailored for very specific problems, and we already build upon reusable and more general purpose infrastructures and tools, referring to them as IoT, Industrial IoT/Industry 4.0[1-3], etc. These are what will be discussed in this paper. When Industrial IoT (IIoT) is concerned about, the enormous innovation potential of IoT technologies are not only in the production of physical devices, but also in all activities performed by manufacturing industries, both in the pre-production (ideation, design, prototyping) and in the post-production (sales, training, maintenance, recycling) phases . It is also known that IIoT acquire and analyze data from connected devices, Cyber-Physical Systems (CPS), locations and people (e.g. operator); along with its contemporary new terms, such as 5G, Edge computing, and other ICT technologies with their applications[4] . More or less it is drawn upon on its combination with relative monitoring devices and actuators from operational technology (OT). IIoT helps regulate and monitor industrial systems [2], and it integrates/re-organize production resources flexibly, enhanced OT capability in the smart value chains enabling distributed decision-making of production.      
### 92.Finding Trajectories with High Asymptotic Growth Rate for Linear Constrained Switching Systems via a Lift Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.12948.pdf)
>  This paper investigates how to generate a sequence of matrices with an asymptotic growth rate close to the constrained joint spectral radius (CJSR) of the constrained switching system whose switching sequences are constrained by a deterministic finite automaton. Based on a matrix-form expression, the dynamics of a constrained switching system are proved to be equivalent to the dynamics of a lifted arbitrary switching system. By using the dual solution of a sum-of-squares optimization program, an algorithm is designed to produce a sequence of matrices with an asymptotic growth rate that can be made arbitrarily close to the joint spectral radius (JSR) of the lifted arbitrary switching system, or equivalently the CJSR of the original constrained switching system. Several numerical examples are provided to illustrate the better performance of the proposed algorithm compared with existing ones.      
### 93.Benchmarking deep inverse models over time, and the neural-adjoint method  [ :arrow_down: ](https://arxiv.org/pdf/2009.12919.pdf)
>  We consider the task of solving generic inverse problems, where one wishes to determine the hidden parameters of a natural system that will give rise to a particular set of measurements. Recently many new approaches based upon deep learning have arisen generating impressive results. We conceptualize these models as different schemes for efficiently, but randomly, exploring the space of possible inverse solutions. As a result, the accuracy of each approach should be evaluated as a function of time rather than a single estimated solution, as is often done now. Using this metric, we compare several state-of-the-art inverse modeling approaches on four benchmark tasks: two existing tasks, one simple task for visualization and one new task from metamaterial design. Finally, inspired by our conception of the inverse problem, we explore a solution that uses a deep learning model to approximate the forward model, and then uses backpropagation to search for good inverse solutions. This approach, termed the neural-adjoint, achieves the best performance in many scenarios.      
### 94.Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2009.12829.pdf)
>  Recently, we have witnessed great progress in the field of medical imaging classification by adopting deep neural networks. However, the recent advanced models still require accessing sufficiently large and representative datasets for training, which is often unfeasible in clinically realistic environments. When trained on limited datasets, the deep neural network is lack of generalization capability, as the trained deep neural network on data within a certain distribution (e.g. the data captured by a certain device vendor or patient population) may not be able to generalize to the data with another distribution. <br>In this paper, we introduce a simple but effective approach to improve the generalization capability of deep neural networks in the field of medical imaging classification. Motivated by the observation that the domain variability of the medical images is to some extent compact, we propose to learn a representative feature space through variational encoding with a novel linear-dependency regularization term to capture the shareable information among medical data collected from different domains. As a result, the trained neural network is expected to equip with better generalization capability to the "unseen" medical data. Experimental results on two challenging medical imaging classification tasks indicate that our method can achieve better cross-domain generalization capability compared with state-of-the-art baselines.      
### 95.TernaryBERT: Distillation-aware Ultra-low Bit BERT  [ :arrow_down: ](https://arxiv.org/pdf/2009.12812.pdf)
>  Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks.However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by the lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.      
### 96.AIM 2020: Scene Relighting and Illumination Estimation Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2009.12798.pdf)
>  We review the AIM 2020 challenge on virtual image relighting and illumination estimation. This paper presents the novel VIDIT dataset used in the challenge and the different proposed solutions and final evaluation results over the 3 challenge tracks. The first track considered one-to-one relighting; the objective was to relight an input photo of a scene with a different color temperature and illuminant orientation (i.e., light source position). The goal of the second track was to estimate illumination settings, namely the color temperature and orientation, from a given image. Lastly, the third track dealt with any-to-any relighting, thus a generalization of the first track. The target color temperature and orientation, rather than being pre-determined, are instead given by a guide image. Participants were allowed to make use of their track 1 and 2 solutions for track 3. The tracks had 94, 52, and 56 registered participants, respectively, leading to 20 confirmed submissions in the final competition stage.      
### 97.Proximity Inference with Wifi-Colocation during the COVID-19 Pandemic  [ :arrow_down: ](https://arxiv.org/pdf/2009.12699.pdf)
>  In this work we propose using WiFi signals recorded on the phone for performing digital contact tracing. The approach works by scanning the access point information on the device and storing it for future purposes of privacy preserving digital contact tracing. We make our approach resilient to different practical scenarios by configuring a device to turn into hotspot if the access points are unavailable. This makes our proposed approach to be feasible in both dense urban areas as well as sparse rural places. We compare and discuss various shortcomings and advantages of this work over other conventional ways of doing digital contact tracing. Preliminaries results indicate the feasibility and efficacy of our approach for the task of proximity sensing which could be relevant and accurate for its relevance to contact tracing and exposure notifications.      
### 98.Adaptive Non-reversible Stochastic Gradient Langevin Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2009.12690.pdf)
>  It is well known that adding any skew symmetric matrix to the gradient of Langevin dynamics algorithm results in a non-reversible diffusion with improved convergence rate. This paper presents a gradient algorithm to adaptively optimize the choice of the skew symmetric matrix. The resulting algorithm involves a non-reversible diffusion algorithm cross coupled with a stochastic gradient algorithm that adapts the skew symmetric matrix. The algorithm uses the same data as the classical Langevin algorithm. A weak convergence proof is given for the optimality of the choice of the skew symmetric matrix. The improved convergence rate of the algorithm is illustrated numerically in Bayesian learning and tracking examples.      
### 99.MicroAnalyzer: A Python Tool for Automated Bacterial Analysis with Fluorescence Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.12684.pdf)
>  Fluorescence microscopy is a widely used method among cell biologists for studying the localization and co-localization of fluorescent protein. For microbial cell biologists, these studies often include tedious and time-consuming manual segmentation of bacteria and of the fluorescence clusters or working with multiple programs. Here, we present MicroAnalyzer - a tool that automates these tasks by providing an end-to-end platform for microscope image analysis. While such tools do exist, they are costly, black-boxed programs. Microanalyzer offers an open-source alternative to these tools, allowing flexibility and expandability by advanced users. MicroAnalyzer provides accurate cell and fluorescence cluster segmentation based on state-of-the-art deep-learning segmentation models, combined with ad-hoc post-processing and Colicoords - an open-source cell image analysis tool for calculating general cell and fluorescence measurements. Using these methods, it performs better than generic approaches since the dynamic nature of neural networks allows for a quick adaptation to experiment restrictions and assumptions. Other existing tools do not consider experiment assumptions, nor do they provide fluorescence cluster detection without the need for any specialized equipment. The key goal of MicroAnalyzer is to automate the entire process of cell and fluorescence image analysis "from microscope to database", meaning it does not require any further input from the researcher except for the initial deep-learning model training. In this fashion, it allows the researchers to concentrate on the bigger picture instead of granular, eye-straining labor      
### 100.Complementary Meta-Reinforcement Learning for Fault-Adaptive Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.12634.pdf)
>  Faults are endemic to all systems. Adaptive fault-tolerant control maintains degraded performance when faults occur as opposed to unsafe conditions or catastrophic events. In systems with abrupt faults and strict time constraints, it is imperative for control to adapt quickly to system changes to maintain system operations. We present a meta-reinforcement learning approach that quickly adapts its control policy to changing conditions. The approach builds upon model-agnostic meta learning (MAML). The controller maintains a complement of prior policies learned under system faults. This "library" is evaluated on a system after a new fault to initialize the new policy. This contrasts with MAML, where the controller derives intermediate policies anew, sampled from a distribution of similar systems, to initialize a new policy. Our approach improves sample efficiency of the reinforcement learning process. We evaluate our approach on an aircraft fuel transfer system under abrupt faults.      
### 101.Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2009.12576.pdf)
>  A fundamental question in neuroscience is how the brain creates an internal model of the world to guide actions using sequences of ambiguous sensory information. This is naturally formulated as a reinforcement learning problem under partial observations, where an agent must estimate relevant latent variables in the world from its evidence, anticipate possible future states, and choose actions that optimize total expected reward. This problem can be solved by control theory, which allows us to find the optimal actions for a given system dynamics and objective function. However, animals often appear to behave suboptimally. Why? We hypothesize that animals have their own flawed internal model of the world, and choose actions with the highest expected subjective reward according to that flawed model. We describe this behavior as rational but not optimal. The problem of Inverse Rational Control (IRC) aims to identify which internal model would best explain an agent's actions. Our contribution here generalizes past work on Inverse Rational Control which solved this problem for discrete control in partially observable Markov decision processes. Here we accommodate continuous nonlinear dynamics and continuous actions, and impute sensory observations corrupted by unknown noise that is private to the animal. We first build an optimal Bayesian agent that learns an optimal policy generalized over the entire model space of dynamics and subjective rewards using deep reinforcement learning. Crucially, this allows us to compute a likelihood over models for experimentally observable action trajectories acquired from a suboptimal agent. We then find the model parameters that maximize the likelihood using gradient ascent.      
### 102.DT-Net: A novel network based on multi-directional integrated convolution and threshold convolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.12569.pdf)
>  Since medical image data sets contain few samples and singular features, lesions are viewed as highly similar to other tissues. The traditional neural network has a limited ability to learn features. Even if a host of feature maps is expanded to obtain more semantic information, the accuracy of segmenting the final medical image is slightly improved, and the features are excessively redundant. To solve the above problems, in this paper, we propose a novel end-to-end semantic segmentation algorithm, DT-Net, and use two new convolution strategies to better achieve end-to-end semantic segmentation of medical images. 1. In the feature mining and feature fusion stage, we construct a multi-directional integrated convolution (MDIC). The core idea is to use the multi-scale convolution to enhance the local multi-directional feature maps to generate enhanced feature maps and to mine the generated features that contain more semantics without increasing the number of feature maps. 2. We also aim to further excavate and retain more meaningful deep features reduce a host of noise features in the training process. Therefore, we propose a convolution thresholding strategy. The central idea is to set a threshold to eliminate a large number of redundant features and reduce computational complexity. Through the two strategies proposed above, the algorithm proposed in this paper produces state-of-the-art results on two public medical image datasets. We prove in detail that our proposed strategy plays an important role in feature mining and eliminating redundant features. Compared with the existing semantic segmentation algorithms, our proposed algorithm has better robustness.      
### 103.Latency Analysis for IMT-2020 Radio Interface Technology Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2009.12533.pdf)
>  The International Telecommunication Union (ITU) is currently deliberating on the finalization of candidate radio interface technologies (RITs) for IMT-2020 (International Mobile Telecommunications) suitability. The candidate technologies are currently being evaluated and after a couple of ITU-Radiocommunication sector (ITU-R) working party (WP) meetings, they will become official. Although, products based on the candidate technology from 3GPP (5G new radio (NR)) is already commercial in several operator networks, the ITU is yet to officially declare it as IMT-2020 qualified. Along with evaluation of the 3GPP 5G NR specifications, our group has evaluated many other proponent technologies. 3GPP entire specifications were examined and evaluated through simulation using Matlab and using own developed simulator which is based on the Go-language. The simulator can evaluate complete 5G NR performance using the IMT-2020 evaluation framework. In this work, we are presenting latency parameters which has shown some minor differences from the 3GPP report. Especially, for time division duplexing (TDD) mode of operation, the differences are observed. It might be possible that the differences are due to assumptions made outside the scope of the evaluation. However, we considered the worst case parameter. Although, the report is submitted to ITU but it is also important for the research community to understand why the differences and what were the assumptions in scenario for which differences are observed.      
### 104.Cross-individual Recognition of Emotions by a Dynamic Entropy based on Pattern Learning with EEG features  [ :arrow_down: ](https://arxiv.org/pdf/2009.12525.pdf)
>  Use of the electroencephalogram (EEG) and machine learning approaches to recognize emotions can facilitate affective human computer interactions. However, the type of EEG data constitutes an obstacle for cross-individual EEG feature modelling and classification. To address this issue, we propose a deep-learning framework denoted as a dynamic entropy-based pattern learning (DEPL) to abstract informative indicators pertaining to the neurophysiological features among multiple individuals. DEPL enhanced the capability of representations generated by a deep convolutional neural network by modelling the interdependencies between the cortical locations of dynamical entropy based features. The effectiveness of the DEPL has been validated with two public databases, commonly referred to as the DEAP and MAHNOB-HCI multimodal tagging databases. Specifically, the leave one subject out training and testing paradigm has been applied. Numerous experiments on EEG emotion recognition demonstrate that the proposed DEPL is superior to those traditional machine learning (ML) methods, and could learn between electrode dependencies w.r.t. different emotions, which is meaningful for developing the effective human-computer interaction systems by adapting to human emotions in the real world applications.      
### 105.Improving electric power generation of a standalone wave energy converter via optimal electric load control  [ :arrow_down: ](https://arxiv.org/pdf/2009.12485.pdf)
>  This paper aims to investigate electric dynamics and improve electric power generation of an isolated wave energy converter that uses a linear permanent magnet generator as the power take-off system, excited by regular or irregular waves. This is of significant concern when considering actual operating conditions of an offshore wave energy converter, where the device will encounter different sea states and its electric load needs to be tuned on a sea-state-to-sea-state basis. To that end, a fully coupled fluid-mechanical-electric-magnetic-electronic mathematical model and an optimization routine are developed. This proposed time-domain wave-to-wire model is used to simulate the hydrodynamic and electric response of a wave energy converter connected to specific electric loads and also used in an optimization routine that searches optimal resistive load value for a wave energy converter under specific sea states. Sample results are presented for a point-absorber type wave energy converter, showing that the electric power generation of a device under irregular waves can be significantly improved. <br>Keywords: wave energy converter, wave power system, wave-to-wire model, linear permanent magnet generator, electric load optimization.      
### 106.Bandwidth-Agile Image Transmission with Deep Joint Source-Channel Coding  [ :arrow_down: ](https://arxiv.org/pdf/2009.12480.pdf)
>  We introduce deep learning based communication methods for adaptive-bandwidth transmission of images over wireless channels. We consider the scenario in which images are transmitted progressively in discrete layers over time or frequency, and such layers can be aggregated by receivers in order to increase the quality of their reconstructions. We investigate two scenarios, one in which the layers are sent sequentially, and incrementally contribute to the refinement of a reconstruction, and another in which the layers are independent and can be retrieved in any order. Those scenarios correspond to the well known problems of successive refinement and multiple descriptions, respectively, in the context of joint source-channel coding (JSCC). We propose DeepJSCC-$l$, an innovative solution that uses convolutional autoencoders, and present three different architectures with different complexity trade-offs. To the best of our knowledge, this is the first practical multiple-description JSCC scheme developed and tested for practical information sources and channels. Numerical results show that DeepJSCC-$l$ can learn different strategies to divide the sources into a layered representation with negligible losses to the end-to-end performance when compared to a single transmission. Moreover, compared to state-of-the-art digital communication schemes, DeepJSCC-$l$ performs well in the challenging low signal-to-noise ratio (SNR) and small bandwidth regimes, and provides graceful degradation with channel SNR.      
### 107.Generating Realistic COVID19 X-rays with a Mean Teacher + Transfer Learning GAN  [ :arrow_down: ](https://arxiv.org/pdf/2009.12478.pdf)
>  COVID-19 is a novel infectious disease responsible for over 800K deaths worldwide as of August 2020. The need for rapid testing is a high priority and alternative testing strategies including X-ray image classification are a promising area of research. However, at present, public datasets for COVID19 x-ray images have low data volumes, making it challenging to develop accurate image classifiers. Several recent papers have made use of Generative Adversarial Networks (GANs) in order to increase the training data volumes. But realistic synthetic COVID19 X-rays remain challenging to generate. We present a novel Mean Teacher + Transfer GAN (MTT-GAN) that generates COVID19 chest X-ray images of high quality. In order to create a more accurate GAN, we employ transfer learning from the Kaggle Pneumonia X-Ray dataset, a highly relevant data source orders of magnitude larger than public COVID19 datasets. Furthermore, we employ the Mean Teacher algorithm as a constraint to improve stability of training. Our qualitative analysis shows that the MTT-GAN generates X-ray images that are greatly superior to a baseline GAN and visually comparable to real X-rays. Although board-certified radiologists can distinguish MTT-GAN fakes from real COVID19 X-rays. Quantitative analysis shows that MTT-GAN greatly improves the accuracy of both a binary COVID19 classifier as well as a multi-class Pneumonia classifier as compared to a baseline GAN. Our classification accuracy is favourable as compared to recently reported results in the literature for similar binary and multi-class COVID19 screening tasks.      
### 108.A Complex Stiffness Human Impedance Model with Customizable Exoskeleton Control  [ :arrow_down: ](https://arxiv.org/pdf/2009.12446.pdf)
>  The natural impedance, or dynamic relationship between force and motion, of a human operator can determine the stability of exoskeletons that use interaction-torque feedback to amplify human strength. While human impedance is typically modelled as a linear system, our experiments on a single-joint exoskeleton testbed involving 10 human subjects show evidence of nonlinear behavior: a low-frequency asymptotic phase for the dynamic stiffness of the human that is different than the expected zero, and an unexpectedly consistent damping ratio as the stiffness and inertia vary. To explain these observations, this paper considers a new frequency-domain model of the human joint dynamics featuring complex value stiffness comprising a real stiffness term and a hysteretic damping term. Using a statistical F-test we show that the hysteretic damping term is not only significant but is even more significant than the linear damping term. Further analysis reveals a linear trend linking hysteretic damping and the real part of the stiffness, which allows us to simplify the complex stiffness model down to a 1-parameter system. Then, we introduce and demonstrate a customizable fractional-order controller that exploits this hysteretic damping behavior to improve strength amplification bandwidth while maintaining stability, and explore a tuning approach which ensures that this stability property is robust to muscle co-contraction for each individual.      
### 109.Towards General Purpose and Geometry Preserving Single-View Depth Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2009.12419.pdf)
>  Single-view depth estimation plays a crucial role in scene understanding for AR applications and 3D modelling as it allows to retrieve the geometry of a scene. However, it is only possible if the inverse depth estimates are unbiased, i.e. they are either absolute or Up-to-Scale (UTS). In recent years, great progress has been made in general-purpose single-view depth estimation. Nevertheless, the latest general-purpose models were trained using ranking or on Up-to-Shift-Scale (UTSS) data. As a result, they provide UTSS predictions that cannot be used to reconstruct scene geometry. In this work, we strive to build a general-purpose single-view UTS depth estimation model. Following Ranftl et. al., we train our model on a mixture of datasets and test it on several previously unseen datasets. We show that our method outperforms previous state-of-the-art UTS models. We train several light-weight models following the proposed training scheme and prove that our ideas are applicable for computationally efficient depth estimation.      
### 110.Robust Trajectory Optimization over Uncertain Terrain with Stochastic Complementarity  [ :arrow_down: ](https://arxiv.org/pdf/2009.12409.pdf)
>  Trajectory optimization with contact-rich behaviors has recently gained attention for generating diverse locomotion behaviors without pre-specified ground contact sequences. However, these approaches rely on precise models of robot dynamics and the terrain and are susceptible to uncertainty. Recent works have attempted to handle uncertainties in the system model, but few have investigated uncertainty in contact dynamics. In this study, we model uncertainty stemming from the terrain and design corresponding risk-sensitive objectives under the framework of contact-implicit trajectory optimization. In particular, we parameterize uncertainties from the terrain contact distance and friction coefficients using probability distributions and propose a corresponding expected residual minimization cost design approach. We evaluate our method in three simple robotic examples, including a legged hopping robot, and we benchmark one of our examples in simulation against a robust worst-case solution. We show that our risk-sensitive method produces contact-averse trajectories that are robust to terrain perturbations. Moreover, we demonstrate that the resulting trajectories converge to those generated by a traditional, non-robust method as the terrain model becomes more certain. Our study marks an important step towards a fully robust, contact-implicit approach suitable for deploying robots on real-world terrain.      
