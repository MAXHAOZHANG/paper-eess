# ArXiv eess --Thu, 3 Sep 2020
### 1.American Sign Language Recognition Using RF Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2009.01224.pdf)
>  Many technologies for human-computer interaction have been designed for hearing individuals and depend upon vocalized speech, precluding users of American Sign Language (ASL) in the Deaf community from benefiting from these advancements. While great strides have been made in ASL recognition with video or wearable gloves, the use of video in homes has raised privacy concerns, while wearable gloves severely restrict movement and infringe on daily life. Methods: This paper proposes the use of RF sensors for HCI applications serving the Deaf community. A multi-frequency RF sensor network is used to acquire non-invasive, non-contact measurements of ASL signing irrespective of lighting conditions. The unique patterns of motion present in the RF data due to the micro-Doppler effect are revealed using time-frequency analysis with the Short-Time Fourier Transform. Linguistic properties of RF ASL data are investigated using machine learning (ML). Results: The information content, measured by fractal complexity, of ASL signing is shown to be greater than that of other upper body activities encountered in daily living. This can be used to differentiate daily activities from signing, while features from RF data show that imitation signing by non-signers is 99\% differentiable from native ASL signing. Feature-level fusion of RF sensor network data is used to achieve 72.5\% accuracy in classification of 20 native ASL signs. Implications: RF sensing can be used to study dynamic linguistic properties of ASL and design Deaf-centric smart environments for non-invasive, remote recognition of ASL. ML algorithms should be benchmarked on native, not imitation, ASL data.      
### 2.Determining the Number of Sinusoids Measured with Errors  [ :arrow_down: ](https://arxiv.org/pdf/2009.01199.pdf)
>  This paper describes how a priori information about the signal parameters can influence the accuracy of estimating the number of these signals. This study considers sinusoidal signals and it is supposed that the parameters (amplitudes, frequencies and phases) of the received signals are known up to a certain error. The error probability of the maximum likelihood estimation of the number of sinusoids is calculated under this condition.      
### 3.Safe Optimal Control Using Stochastic Barrier Functions and Deep Forward-Backward SDEs  [ :arrow_down: ](https://arxiv.org/pdf/2009.01196.pdf)
>  This paper introduces a new formulation for stochastic optimal control and stochastic dynamic optimization that ensures safety with respect to state and control constraints. The proposed methodology brings together concepts such as Forward-Backward Stochastic Differential Equations, Stochastic Barrier Functions, Differentiable Convex Optimization and Deep Learning. Using the aforementioned concepts, a Neural Network architecture is designed for safe trajectory optimization in which learning can be performed in an end-to-end fashion. Simulations are performed on three systems to show the efficacy of the proposed methodology.      
### 4.The Effect of Various Strengths of Noises and Data Augmentations on Classification of Short Single-Lead ECG Signals Using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.01192.pdf)
>  Due to the multiple imperfections during the signal acquisition, Electrocardiogram (ECG) datasets are typically contaminated with numerous types of noise, like salt and pepper and baseline drift. These datasets may contain different recordings with various types of noise [1] and thus, denoising may not be the easiest task. Furthermore, usually, the number of labeled bio-signals is very limited for a proper classification task.      
### 5.A General Framework for RIS-Aided mmWave Communication Networks: Channel Estimation and Mobile User Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2009.01180.pdf)
>  Reconfigurable intelligent surface (RIS) has been widely discussed as new technology to improve wireless communication performance. Based on the unique design of RIS, its elements can reflect, refract, absorb, or focus the incoming waves toward any desired direction. These functionalities turned out to be a major solution to overcome millimeter-wave (mmWave)'s high propagation conditions including path attenuation and blockage. However, channel estimation in RIS-aided communication is still a major concern due to the passive nature of RIS elements, and estimation overhead that arises with multiple-input multiple-output (MIMO) system. As a consequence, user tracking has not been analyzed yet. This paper is the first work that addresses channel estimation, beamforming, and user tracking under practical mmWave RIS-MIMO systems. By providing the mathematical relation of RIS design with a MIMO system, a three-stage framework is presented. Starting with estimating the channel between a base station (BS) and RIS using hierarchical beam searching, followed by estimating the channel between RIS and user using an iterative resolution algorithm. Lastly, a popular tracking algorithm is employed to track channel parameters between the RIS and the user. System analysis demonstrates the robustness and the effectiveness of the proposed framework in real-time scenarios.      
### 6.SURF-SVM Based Identification and Classification of Gastrointestinal Diseases in Wireless Capsule Endoscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.01179.pdf)
>  Endoscopy provides a major contribution to the diagnosis of the Gastrointestinal Tract (GIT) diseases. With Colon Endoscopy having its certain limitations, Wireless Capsule Endoscopy is gradually taking over it in the terms of ease and efficiency. WCE is performed with a miniature optical endoscope which is swallowed by the patient and transmits colour images wirelessly during its journey through the GIT, inside the body of the patient. These images are used to implement an effective and computationally efficient approach which aims to detect the abnormal and normal tissues in the GIT automatically, and thus helps in reducing the manual work of the reviewers. The algorithm further aims to classify the diseased tissues into various GIT diseases that are commonly known to be affecting the tract. In this manuscript, the descriptor used for the detection of the interest points is Speeded Up Robust Features (SURF), which uses the colour information contained in the images which is converted to CIELAB space colours for better identification. The features extracted at the interest points are then used to train and test a Support Vector Machine (SVM), so that it automatically classifies the images into normal or abnormal and further detects the specific abnormalities. SVM, along with a few parameters, gives a very high accuracy of 94.58% while classifying normal and abnormal images and an accuracy of 82.91% while classifying into multi-class. The present work is an improvement on the previously reported analyses which were only limited to the bi-class classification using this approach.      
### 7.The LoRa Modulation Over Rapidly-Varying Channels: Are the Higher Spreading Factors Necessarily More Robust?  [ :arrow_down: ](https://arxiv.org/pdf/2009.01176.pdf)
>  The chirp spread spectrum (CSS) modulation scheme is the basic building block of the physical layer of the Long Range (LoRa) communication technology. In this paper, we present some results from our investigation of CSS communications over fading channels whose gain may change within the duration of a LoRa frame. Specifically, we investigate the effects of exponentially correlated Rayleigh fading on the frame-error rate of CSS. Our primary observation is that in certain fading environments, the robustness benefits of the larger spreading factors tend to disappear as the payload size grows. This observation, which is contrary to the common perception that higher spreading factors necessarily provide lower frame-error rates, highlights the need to consider channel characteristics and payload sizes in allocating the spreading factor for reliable and energy-efficient LoRa communications.      
### 8.Teaching a Machine to Diagnose a Heart Disease; Beginning from digitizing scanned ECGs to detecting the Brugada Syndrome (BrS)  [ :arrow_down: ](https://arxiv.org/pdf/2009.01076.pdf)
>  Medical diagnoses can shape and change the life of a person drastically. Therefore, it is always best advised to collect as much evidence as possible to be certain about the diagnosis. Unfortunately, in the case of the Brugada Syndrome (BrS), a rare and inherited heart disease, only one diagnostic criterion exists, namely, a typical pattern in the Electrocardiogram (ECG). In the following treatise, we question whether the investigation of ECG strips by the means of machine learning methods improves the detection of BrS positive cases and hence, the diagnostic process. We propose a pipeline that reads in scanned images of ECGs, and transforms the encaptured signals to digital time-voltage data after several processing steps. Then, we present a long short-term memory (LSTM) classifier that is built based on the previously extracted data and that makes the diagnosis. The proposed pipeline distinguishes between three major types of ECG images and recreates each recorded lead signal. Features and quality are retained during the digitization of the data, albeit some encountered issues are not fully removed (Part I). Nevertheless, the results of the aforesaid program are suitable for further investigation of the ECG by a computational method such as the proposed classifier which proves the concept and could be the architectural basis for future research (Part II). This thesis is divided into two parts as they are part of the same process but conceptually different. It is hoped that this work builds a new foundation for computational investigations in the case of the BrS and its diagnosis.      
### 9.SAR Tomography via Nonlinear Blind Scatterer Separation  [ :arrow_down: ](https://arxiv.org/pdf/2009.01009.pdf)
>  Layover separation has been fundamental to many synthetic aperture radar applications, such as building reconstruction and biomass estimation. Retrieving the scattering profile along the mixed dimension (elevation) is typically solved by inversion of the SAR imaging model, a process known as SAR tomography. This paper proposes a nonlinear blind scatterer separation method to retrieve the phase centers of the layovered scatterers, avoiding the computationally expensive tomographic inversion. We demonstrate that conventional linear separation methods, e.g., principle component analysis (PCA), can only partially separate the scatterers under good conditions. These methods produce systematic phase bias in the retrieved scatterers due to the nonorthogonality of the scatterers' steering vectors, especially when the intensities of the sources are similar or the number of images is low. The proposed method artificially increases the dimensionality of the data using kernel PCA, hence mitigating the aforementioned limitations. In the processing, the proposed method sequentially deflates the covariance matrix using the estimate of the brightest scatterer from kernel PCA. Simulations demonstrate the superior performance of the proposed method over conventional PCA-based methods in various respects. Experiments using TerraSAR-X data show an improvement in height reconstruction accuracy by a factor of one to three, depending on the used number of looks.      
### 10.Classification of Diabetic Retinopathy Using Unlabeled Data and Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00982.pdf)
>  Knowledge distillation allows transferring knowledge from a pre-trained model to another. However, it suffers from limitations, and constraints related to the two models need to be architecturally similar. Knowledge distillation addresses some of the shortcomings associated with transfer learning by generalizing a complex model to a lighter model. However, some parts of the knowledge may not be distilled by knowledge distillation sufficiently. In this paper, a novel knowledge distillation approach using transfer learning is proposed. The proposed method transfers the entire knowledge of a model to a new smaller one. To accomplish this, unlabeled data are used in an unsupervised manner to transfer the maximum amount of knowledge to the new slimmer model. The proposed method can be beneficial in medical image analysis, where labeled data are typically scarce. The proposed approach is evaluated in the context of classification of images for diagnosing Diabetic Retinopathy on two publicly available datasets, including Messidor and EyePACS. Simulation results demonstrate that the approach is effective in transferring knowledge from a complex model to a lighter one. Furthermore, experimental results illustrate that the performance of different small models is improved significantly using unlabeled data and knowledge distillation.      
### 11.Adaptive Scale Factor Compensation for Missiles with Strapdown Seekers via Predictive Coding  [ :arrow_down: ](https://arxiv.org/pdf/2009.00975.pdf)
>  In this work we present a method to adaptively compensate for scale factor errors in both rotational velocity and seeker angle measurements. The adaptation scheme estimates the scale factor errors using a predictive coding model implemented as a deep neural network with recurrent layer, and then uses these estimates to compensate for the error. During training, the model learns over a wide range of scale factor errors that ideally bound the expected errors that can occur during deployment, allowing the deployed model to quickly adapt in real time to the ground truth error. We demonstrate in a realistic six degrees-of-freedom simulation of an exoatmospheric intercept that our method effectively compensates for concurrent rotational velocity and seeker angle scale factor errors. The compensation method is general in that it is independent of a given guidance, navigation, and control system implementation. Although demonstrated using an exoatmospheric missile with strapdown seeker, the method is also applicable to endoatmospheric missiles with both gimbaled and strapdown seekers, as well as general purpose inertial measurement unit rate gyro compensation.      
### 12.Application-Based Coexistence of Different Waveforms on Non-orthogonal Multiple Access  [ :arrow_down: ](https://arxiv.org/pdf/2009.00973.pdf)
>  The coexistence of different wireless communication systems such as LTE and Wi-Fi by sharing the unlicensed band is well studied in the literature. In these studies, various methods are proposed to support the coexistence of systems, including listen-before-talk mechanism, joint user association and resource allocation. However, in this study, the coexistence of different waveform structures in the same resource elements are studied under the theory of non-orthogonal multiple access. This study introduces a paradigm-shift on NOMA towards the application-centric waveform coexistence. Throughout the paper, the coexistence of different waveforms is explained with two specific use cases, which are power-balanced NOMA and joint radar-sensing and communication with NOMA. In addition, some of the previous works in the literature regarding non-orthogonal waveform coexistence are reviewed. However, the concept is not limited to these use cases. With the rapid development of wireless technology, next-generation wireless systems are proposed to be flexible and hybrid, having different kinds of capabilities such as sensing, security, intelligence, control, and computing. Therefore, the concept of different waveforms' coexistence to meet these concerns are becoming impressive for researchers.      
### 13.A Flux and Speed Observer for Induction Motors with Unknown Rotor Resistance and Load Torque and no Persistent Excitation Requirement  [ :arrow_down: ](https://arxiv.org/pdf/2009.00966.pdf)
>  In this paper we address the problems of flux and speed observer design for voltage-fed induction motors with unknown rotor resistance and load torque. The only measured signals are stator current and control voltage. Invoking the recently reported Dynamic Regressor Extension and Mixing-Based Adaptive Observer (DREMBAO) we provide the first global solution to this problem. The proposed DREMBAO achieve asymptotic convergence under an excitation condition that is strictly weaker than persistent excitation. If the latter condition is assumed the convergence is exponential.      
### 14.DARWIN: A Highly Flexible Platform for Imaging Research in Radiology  [ :arrow_down: ](https://arxiv.org/pdf/2009.00908.pdf)
>  To conduct a radiomics or deep learning research experiment, the radiologists or physicians need to grasp the needed programming skills, which, however, could be frustrating and costly when they have limited coding experience. In this paper, we present DARWIN, a flexible research platform with a graphical user interface for medical imaging research. Our platform is consists of a radiomics module and a deep learning module. The radiomics module can extract more than 1000 dimension features(first-, second-, and higher-order) and provided many draggable supervised and unsupervised machine learning models. Our deep learning module integrates state of the art architectures of classification, detection, and segmentation tasks. It allows users to manually select hyperparameters, or choose an algorithm to automatically search for the best ones. DARWIN also offers the possibility for users to define a custom pipeline for their experiment. These flexibilities enable radiologists to carry out various experiments easily.      
### 15.Physical Layer Security of Terahertz and Infrared Wireless Links in Atmospheric Turbulence  [ :arrow_down: ](https://arxiv.org/pdf/2009.00894.pdf)
>  The future applications of terahertz (THz) wireless communication require investigations on link secrecy performance in all kinds of atmospheric conditions, including fog, snow, rain and atmospheric turbulence. Here, we present theoretical studies on physical layer security of point-to-point THz and infrared (IR) wireless links in atmospheric turbulence with a potential eavesdropper outside of the link path. Attenuations due to turbulence, gaseous absorption and beam divergence are included in the model to predict the propagation of both links. Secrecy capacity and outage probability of the THz links are calculated and compared with that of an IR (1550 nm) link. Dependences of link security on eavesdropper's position, atmospheric visibility, turbulence strength and intended data transmission rate are also presented and analyzed. We find that the THz link owns higher security at physical layer than the IR link.      
### 16.Reconfigurable Intelligent Surfaces for Smart Cities: Research Challenges and Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2009.00891.pdf)
>  The concept of Smart Cities has been introduced as a way to benefit from the digitization of various ecosystems at a city level. To support this concept, future communication networks need to be carefully designed with respect to the city infrastructure and utilization of resources. Recently, the idea of 'smart' environment, which takes advantage of the infrastructure for better performance of wireless networks, has been proposed. This idea is aligned with the recent advances in design of reconfigurable intelligent surfaces (RISs), which are planar structures with the capability to reflect impinging electromagnetic waves toward preferred directions. Thus, RISs are expected to provide the necessary flexibility for the design of the 'smart' communication environment, which can be optimally shaped to enable cost- and energy-efficient signal transmissions where needed. Upon deployment of RISs, the ecosystem of the Smart Cities would become even more controllable and adaptable, which would subsequently ease the implementation of future communication networks in urban areas and boost the interconnection among private households and public services. In this paper, we describe our vision of the application of RISs in future Smart Cities. In particular, the research challenges and opportunities are addressed. The contribution paves the road to a systematic design of RIS-assisted communication networks for Smart Cities in the years to come.      
### 17.Efficient, high-performance pancreatic segmentation using multi-scale feature extraction  [ :arrow_down: ](https://arxiv.org/pdf/2009.00872.pdf)
>  For artificial intelligence-based image analysis methods to reach clinical applicability, the development of high-performance algorithms is crucial. For example, existent segmentation algorithms based on natural images are neither efficient in their parameter use nor optimized for medical imaging. Here we present MoNet, a highly optimized neural-network-based pancreatic segmentation algorithm focused on achieving high performance by efficient multi-scale image feature utilization.      
### 18.Radio Frequency Speed Bumps for Near-Zero Speed Zone Control in Nigeria  [ :arrow_down: ](https://arxiv.org/pdf/2009.00869.pdf)
>  Over speeding is an important problem in Nigeria. The Nigerian Federal Road Safety Commission report shows that over speeding is directly responsible for over 50 percent of road crashes in the country. Speed bumps particularly on highways have not proved to be the solution; they rather contribute more to the problem. Based the advent of the dedicated short range communications (DSRC) systems defined by the IEEE 802.11p standard, we proposed a radio frequency based speed bump to mimic the operation of traditional speed breakers. The model consists of a roadside unit and an in-vehicle unit that works with the already installed speed limiter in vehicles. In our work, we showed that the RF based model is able to decelerate the vehicle at a recommended rate of 10 m/s2 over a convenient stopping distance of 80 m. Our work also showed that signal strength based adaptive speed limiter systems can be used as a reliable substitute to GPS thus eradicating the need for additional hardware in the design.      
### 19.Power Management of Nanogrid Cluster with P2P Electricity Trading Based on Future Trends of Load Demand and PV Power Production  [ :arrow_down: ](https://arxiv.org/pdf/2009.00863.pdf)
>  Power management of nanogrid cluster assisted by a novel peer-to-peer(P2P) electricity trading is presented. In our work, several nanogrids are integrated into a nanogrid cluster and unbalance of power consumption among clusters is mitigated by the proposed P2P trading method. For power management of individual clusters, multi-objective optimization simultaneously minimizing total power consumption, portion of grid power in total power consumption, and total delay incurred by scheduling is attempted. Types of loads involved in power management of nanogrid cluster are flexible loads allowing scheduling and non-flexible loads. Renewable power source photovoltaic(PV) system is adopted for each cluster as a secondary source. Temporal surplus of self-supply PV power of a cluster can be sold through P2P trading to other cluster(s) experiencing temporal shortage. The cluster in temporal shortage of electric power buys the surplus PV power and uses it to reduce peak load and total delay. In P2P trading, a cooperative game model is used for buyers and sellers to maximize their welfare. To increase the efficiency of P2P trading measured by reduction of peak load and total delay, future trends of load demand and PV power production are taken into account for power management of each cluster unlike conventional P2P trading to resolve instantaneous unbalance between load demand and PV power production. To this end, a gated recurrent unit network is used to forecast future load demand and future PV power production. The effectiveness of the proposed P2P trading method for nanogrid cluster is verified by simulations. According to simulation results, proposed P2P trading causes peak load reduction in peak hours, reduction of grid power consumption, and total delay.      
### 20.Efficient Multi-Robot Exploration with Energy Constraint based on Optimal Transport Theory  [ :arrow_down: ](https://arxiv.org/pdf/2009.00862.pdf)
>  This paper addresses an Optimal Transport (OT)-based efficient multi-robot exploration problem, considering the energy constraints of a multi-robot system. The efficiency in this problem implies how a team of robots (agents) covers a given domain, reflecting a priority of areas of interest represented by a density distribution, rather than simply following a preset of uniform patterns. To achieve an efficient multi-robot exploration, the optimal transport theory that quantifies a distance between two density distributions is employed as a tool, which also serves as a means of performance measure. The energy constraints for the multi-robot system is then incorporated into the OT-based multi-robot exploration scheme. <br>The proposed scheme is decoupled from robot dynamics, broadening the applicability of the multi-robot exploration plan to heterogeneous robot platforms. Not only the centralized but also decentralized algorithms are provided to cope with more realistic scenarios such as communication range limits between agents. To measure the exploration efficiency, the upper bound of the performance is developed for both the centralized and decentralized cases based on the optimal transport theory, which is computationally tractable as well as efficient. The proposed multi-robot exploration scheme is also applicable to a time-varying distribution, where the spatio-temporal evolution of the given reference distribution is desired. To validate the proposed method, multiple simulation results are provided.      
### 21.Breast mass detection in digital mammography based on anchor-free architecture  [ :arrow_down: ](https://arxiv.org/pdf/2009.00857.pdf)
>  Background and Objective: Accurate detection of breast masses in mammography images is critical to diagnose early breast cancer, which can greatly improve the patients survival rate. However, it is still a big challenge due to the heterogeneity of breast masses and the complexity of their surrounding environment.Methods: To address these problems, we propose a one-stage object detection architecture, called Breast Mass Detection Network (BMassDNet), based on anchor-free and feature pyramid which makes the detection of breast masses of different sizes well adapted. We introduce a truncation normalization method and combine it with adaptive histogram equalization to enhance the contrast between the breast mass and the surrounding environment. Meanwhile, to solve the overfitting problem caused by small data size, we propose a natural deformation data augmentation method and mend the train data dynamic updating method based on the data complexity to effectively utilize the limited data. Finally, we use transfer learning to assist the training process and to improve the robustness of the model ulteriorly.Results: On the INbreast dataset, each image has an average of 0.495 false positives whilst the recall rate is 0.930; On the DDSM dataset, when each image has 0.599 false positives, the recall rate reaches 0.943.Conclusions: The experimental results on datasets INbreast and DDSM show that the proposed BMassDNet can obtain competitive detection performance over the current top ranked methods.      
### 22.A reinforcement learning approach to hybrid control design  [ :arrow_down: ](https://arxiv.org/pdf/2009.00821.pdf)
>  In this paper we design hybrid control policies for hybrid systems whose mathematical models are unknown. Our contributions are threefold. First, we propose a framework for modelling the hybrid control design problem as a single Markov Decision Process (MDP). This result facilitates the application of off-the-shelf algorithms from Reinforcement Learning (RL) literature towards designing optimal control policies. Second, we model a set of benchmark examples of hybrid control design problem in the proposed MDP framework. Third, we adapt the recently proposed Proximal Policy Optimisation (PPO) algorithm for the hybrid action space and apply it to the above set of problems. It is observed that in each case the algorithm converges and finds the optimal policy.      
### 23.Analysis of Dilation in Children and its Impact on Iris Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.00777.pdf)
>  The dilation of the pupil and it's variation between a mated pair of irides has been found to be an important factor in the performance of iris recognition systems. Studies on adult irides indicated significant impact of dilation on iris recognition performance at different ages. However, the results of adults may not necessarily translate to children. This study analyzes dilation as a factor of age and over time in children, from data collected from same 209 subjects in the age group of four to 11 years at enrollment, longitudinally over three years spaced by six months. The performance of iris recognition is also analyzed in presence of dilation variation.      
### 24.Speaker Representation Learning using Global Context Guided Channel and Time-Frequency Transformations  [ :arrow_down: ](https://arxiv.org/pdf/2009.00768.pdf)
>  In this study, we propose the global context guided channel and time-frequency transformations to model the long-range, non-local time-frequency dependencies and channel variances in speaker representations. We use the global context information to enhance important channels and recalibrate salient time-frequency locations by computing the similarity between the global context and local features. The proposed modules, together with a popular ResNet based model, are evaluated on the VoxCeleb1 dataset, which is a large scale speaker verification corpus collected in the wild. This lightweight block can be easily incorporated into a CNN model with little additional computational costs and effectively improves the speaker verification performance compared to the baseline ResNet-LDE model and the Squeeze&amp;Excitation block by a large margin. Detailed ablation studies are also performed to analyze various factors that may impact the performance of the proposed modules. We find that by employing the proposed L2-tf-GTFC transformation block, the Equal Error Rate decreases from 4.56% to 3.07%, a relative 32.68% reduction, and a relative 27.28% improvement in terms of the DCF score. The results indicate that our proposed global context guided transformation modules can efficiently improve the learned speaker representations by achieving time-frequency and channel-wise feature recalibration.      
### 25.Depth Range Reduction for 3D Range Geometry Compression  [ :arrow_down: ](https://arxiv.org/pdf/2009.00763.pdf)
>  Three-dimensional (3D) shape measurement devices and techniques are being rapidly adopted within a variety of industries and applications. As acquiring 3D range data becomes faster and more accurate it becomes more challenging to efficiently store, transmit, or stream this data. One prevailing approach to compressing 3D range data is to encode it within the color channels of regular 2D images. This paper presents a novel method for reducing the depth range of a 3D geometry such that it can be stored within a 2D image using lower encoding frequencies (or a fewer number of encoding periods). This allows for smaller compressed file sizes to be achieved without a proportional increase in reconstruction errors. Further, as the proposed method occurs prior to encoding, it is readily compatible with a variety of existing image-based 3D range geometry compression methods.      
### 26.Disaggregating Customer-level Behind-the-Meter PV Generation Using Smart Meter Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.00734.pdf)
>  Customer-level rooftop photovoltaic (PV) has been widely integrated into distribution systems. In most cases, PVs are installed behind-the-meter (BTM) and only the net demand is recorded. Therefore, the native demand and PV generation are unknown to utilities. Separating native demand and solar generation from net demand is critical for improving grid-edge observability. In this paper, a novel approach is proposed for disaggregating customer-level BTM PV generation using low-resolution but widely available smart meter data. The proposed approach exploits the high correlation between monthly nocturnal and diurnal native demands. First, a joint probability density function (PDF) of monthly nocturnal and diurnal native demands is constructed for customers without PVs, using Gaussian mixture modeling (GMM). Deviation from the constructed PDF is leveraged to probabilistically assess the monthly solar generation of customers with PVs. Then, to identify hourly BTM solar generation for these customers, their estimated monthly solar generation is decomposed into an hourly timescale; to do this, we have proposed a maximum likelihood estimation (MLE)-based technique that takes advantage of hourly typical solar exemplars. Unlike previous disaggregation methods, our approach does not require native demand exemplars or knowledge of PV model parameters, which makes it robust against volatility of customers' load and enables highly-accurate disaggregation. The proposed approach has been verified using real smart meter data.      
### 27.Performance Analysis and Non-Quadratic Lyapunov Functions for Linear Time-Varying Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.00727.pdf)
>  Performance analysis for linear time-invariant (LTI) systems has been closely tied to quadratic Lyapunov functions ever since it was shown that LTI system stability is equivalent to the existence of such a Lyapunov function. Some metrics for LTI systems, however, have resisted treatment via means of quadratic Lyapunov functions. Among these, point-wise-in-time metrics, such as peak norms, are not captured accurately using these techniques, and this shortcoming has prevented the development of tools to analyze system behavior by means other than e.g. time-domain simulations. This work demonstrates how the more general class of homogeneous polynomial Lyapunov functions can be used to approximate point-wise-in-time behavior for LTI systems with greater accuracy, and we extend this to the case of linear time-varying (LTV) systems as well. Our findings rely on the recent observation that the search for homogeneous polynomial Lyapunov functions for LTV systems can be recast as a search for quadratic Lyapunov functions for a related hierarchy of time-varying Lyapunov differential equations; thus, performance guarantees for LTV systems are attainable without heavy computation. Numerous examples are provided to demonstrate the findings of this work.      
### 28.WaveGrad: Estimating Gradients for Waveform Generation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00713.pdf)
>  This paper introduces WaveGrad, a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive, and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. WaveGrad is simple to train, and implicitly optimizes for the weighted variational lower-bound of the log-likelihood. Empirical experiments reveal WaveGrad to generate high fidelity audio samples matching a strong likelihood-based autoregressive baseline with less sequential operations.      
### 29.A new electromechanical analogy approach based on electrostatic coupling for vertical dynamic analysis of planar vehicle models  [ :arrow_down: ](https://arxiv.org/pdf/2009.00701.pdf)
>  Analogies between mechanical and electrical systems have been developed and applied for almost a century, and they have proved their usefulness in the study of mechanical and electrical systems. The development of new elements such as the inerter or the memristor is a clear example. However, new applications and possibilities of using these analogues still remain to be explored. In this work, the electrical analogues of different vehicle models are presented. A new and not previously reported analogy between inertial coupling and electrostatic capacitive coupling is found and described. Several examples are provided to highlight the benefits of this analogy. Well-known mechanical systems like the half-car or three three-axle vehicle models are discussed and some numerical results are presented. To the best of the author's knowledge, such systems were never dealt with by using a full electromechanical analogy. The mechanical equations are also derived and compared with those of the electrical domain for harmonic steady state analysis.      
### 30.Multimodal Inductive Transfer Learning for Detection of Alzheimer's Dementia and its Severity  [ :arrow_down: ](https://arxiv.org/pdf/2009.00700.pdf)
>  Alzheimer's disease is estimated to affect around 50 million people worldwide and is rising rapidly, with a global economic burden of nearly a trillion dollars. This calls for scalable, cost-effective, and robust methods for detection of Alzheimer's dementia (AD). We present a novel architecture that leverages acoustic, cognitive, and linguistic features to form a multimodal ensemble system. It uses specialized artificial neural networks with temporal characteristics to detect AD and its severity, which is reflected through Mini-Mental State Exam (MMSE) scores. We first evaluate it on the ADReSS challenge dataset, which is a subject-independent and balanced dataset matched for age and gender to mitigate biases, and is available through DementiaBank. Our system achieves state-of-the-art test accuracy, precision, recall, and F1-score of 83.3% each for AD classification, and state-of-the-art test root mean squared error (RMSE) of 4.60 for MMSE score regression. To the best of our knowledge, the system further achieves state-of-the-art AD classification accuracy of 88.0% when evaluated on the full benchmark DementiaBank Pitt database. Our work highlights the applicability and transferability of spontaneous speech to produce a robust inductive transfer learning model, and demonstrates generalizability through a task-agnostic feature-space. The source code is available at <a class="link-external link-https" href="https://github.com/wazeerzulfikar/alzheimers-dementia" rel="external noopener nofollow">this https URL</a>      
### 31.Distributed Locally Non-interfering Connectivity via Linear Temporal Logic  [ :arrow_down: ](https://arxiv.org/pdf/2009.00669.pdf)
>  In this paper, we consider networks of static sensors with integrated sensing and communication capabilities. The goal of the sensors is to propagate their collected information to every other agent in the network and possibly a human operator. Such a task requires constant communication among all agents which may result in collisions and congestion in wireless communication. To mitigate this issue, we impose locally non-interfering communication constraints that must be respected by every agent. We show that these constraints along with the requirement of propagating information in the network can be captured by a Linear Temporal Logic (LTL) framework. Existing temporal logic control synthesis algorithms can be used to design correct-by-construction communication schedules that satisfy the considered LTL formula. Nevertheless, such approaches are centralized and scale poorly with the size of the network. We propose a distributed LTL-based algorithm that designs communication schedules that determine which agents should communicate while maximizing network usage. We show that the proposed algorithm is complete and demonstrate its efficiency and scalability through numerical experiments.      
### 32.Channel Estimation and Detection in FBMC/OQAM System with Affine Precoding and Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2009.00662.pdf)
>  We derive the mathematical equations required for channel estimation and data detection of filter bank multi-carrier (FBMC) offset quadrature amplitude modulation (OQAM) systems with affine precoding and decoding. The mean square error (MSE) in least square (LS) channel estimation and bit error rate (BER) of the system is found for different training power coefficients. The proposed system gives better BER performance compared to other cutting edge FBMC systems. The optimum training power coefficient $(\sigma_{c_{opt}}^2)$ is also found from the simulation results. The band width efficiency of the system is also calculated.      
### 33.Covid 19 and A Wavelet Analysis of the Total Deaths per Month in Brazil since 2015  [ :arrow_down: ](https://arxiv.org/pdf/2009.00648.pdf)
>  We investigate the historical series of the total number of deaths per month in Brazil since 2015 using the wavelet transform, in order to assess whether the COVID-19 pandemic caused any change point in that series. Our wavelet analysis shows that the series has a change point in the variance. However, it occurred long before the pandemic began.      
### 34.Operational vs Convolutional Neural Networks for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2009.00612.pdf)
>  Convolutional Neural Networks (CNNs) have recently become a favored technique for image denoising due to its adaptive learning ability, especially with a deep configuration. However, their efficacy is inherently limited owing to their homogenous network formation with the unique use of linear convolution. In this study, we propose a heterogeneous network model which allows greater flexibility for embedding additional non-linearity at the core of the data transformation. To this end, we propose the idea of an operational neuron or Operational Neural Networks (ONN), which enables a flexible non-linear and heterogeneous configuration employing both inter and intra-layer neuronal diversity. Furthermore, we propose a robust operator search strategy inspired by the Hebbian theory, called the Synaptic Plasticity Monitoring (SPM) which can make data-driven choices for non-linearities in any architecture. An extensive set of comparative evaluations of ONNs and CNNs over two severe image denoising problems yield conclusive evidence that ONNs enriched by non-linear operators can achieve a superior denoising performance against CNNs with both equivalent and well-known deep configurations.      
### 35.Towards Non-Contact Glucose Sensing in Aqueous Turbid Medium at ~1.1 Meters Distance  [ :arrow_down: ](https://arxiv.org/pdf/2009.01208.pdf)
>  This work demonstrates a non-contact diffuse reflectance approach with a working distance of ~1.1 meters for the potential of glucose sensing. Non-contact diffuse reflectance over 1.1-1.3 micrometers was developed according to a center-illumination-area-detection (CIAD) geometry. The modeled response of diffuse reflectance in the CIAD geometry was examined with phantoms by altering independently the size of the collection geometry and the reduced scattering and absorption properties of the medium. When applied to aqueous turbid medium containing glucose control solutions with the cumulative volume varying over three orders of magnitude, a linear relationship expected for the diffuse reflectance as a function of the medium absorption/reduced-scattering property was observed for four conditions of the glucose-medium composition that differed either in the effective glucose concentration or the host medium scattering property. The cumulation of glucose up to 17.8mg/dL and 8.9mg/dL in the host medium having the same optical properties resulted in linear regression slopes of 0.0032 and 0.0030, respectively. The cumulation of the glucose up to 17.8mg/dL in an aqueous host medium that differed two folds in the reduced scattering property caused the linear regression slope to differ between 0.0032 and 0.0019. The R^2 values of all cases were all greater than 0.987. A provisional patent (US#63/053,004) has been obtained for this work.      
### 36.A single inverse-designed photonic structure that performs parallel computing  [ :arrow_down: ](https://arxiv.org/pdf/2009.01187.pdf)
>  In the search for improved computational capabilities, conventional microelectronic computers are facing various problems arising from the miniaturization and concentration of active electronics devices (1-2). Therefore, researchers have been exploring several paths for the next generation of computing platforms, which could exploit various physical phenomena for solving mathematical problems at higher speeds and larger capacities. Among others, physical systems described by waves, such as photonic and quantum devices, have been utilized to compute the solution of mathematical problems (1-18). However, previous devices have not fully exploited the linearity of the wave equation, which as we show here, allows for the simultaneous parallel solution of several independent mathematical problems within the same device. In this Letter, we demonstrate, theoretically and experimentally, that a transmissive cavity filled with a judiciously tailored dielectric distribution and embedded in a multi-frequency feedback loop can calculate the solutions of an arbitrary number of mathematical problems simultaneously. We design, build, and test a computing structure at microwave frequencies that solves two independent integral equations with any two arbitrary inputs. We offer another design that can invert four arbitrary 5x5 matrices, confirming its functionality with numerical simulations. We believe our results presented here can provide "coincident computing" and pave the way for the design of low-power, ultrafast, parallel photonic analog computing devices for sensing and signal processing in embedded computing applications.      
### 37.Transform Quantization for CNN Compression  [ :arrow_down: ](https://arxiv.org/pdf/2009.01174.pdf)
>  In this paper, we compress convolutional neural network (CNN) weights post-training via transform quantization. Previous CNN quantization techniques tend to ignore the joint statistics of weights and activations, producing sub-optimal CNN performance at a given quantization bit-rate, or consider their joint statistics during training only and do not facilitate efficient compression of already trained CNN models. We optimally transform (decorrelate) and quantize the weights post-training using a rate-distortion framework to improve compression at any given quantization bit-rate. Transform quantization unifies quantization and dimensionality reduction (decorrelation) techniques in a single framework to facilitate low bit-rate compression of CNNs and efficient inference in the transform domain. We first introduce a theory of rate and distortion for CNN quantization, and pose optimum quantization as a rate-distortion optimization problem. We then show that this problem can be solved using optimal bit-depth allocation following decorrelation by the optimal End-to-end Learned Transform (ELT) we derive in this paper. Experiments demonstrate that transform quantization advances the state of the art in CNN compression in both retrained and non-retrained quantization scenarios. In particular, we find that transform quantization with retraining is able to compress CNN models such as AlexNet, ResNet and DenseNet to very low bit-rates (1-2 bits).      
### 38.Adversarial Attacks on Deep Learning Systems for User Identification based on Motion Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2009.01109.pdf)
>  For the time being, mobile devices employ implicit authentication mechanisms, namely, unlock patterns, PINs or biometric-based systems such as fingerprint or face recognition. While these systems are prone to well-known attacks, the introduction of an explicit and unobtrusive authentication layer can greatly enhance security. In this study, we focus on deep learning methods for explicit authentication based on motion sensor signals. In this scenario, attackers could craft adversarial examples with the aim of gaining unauthorized access and even restraining a legitimate user to access his mobile device. To our knowledge, this is the first study that aims at quantifying the impact of adversarial attacks on machine learning models used for user identification based on motion sensors. To accomplish our goal, we study multiple methods for generating adversarial examples. We propose three research questions regarding the impact and the universality of adversarial examples, conducting relevant experiments in order to answer our research questions. Our empirical results demonstrate that certain adversarial example generation methods are specific to the attacked classification model, while others tend to be generic. We thus conclude that deep neural networks trained for user identification tasks based on motion sensors are subject to a high percentage of misclassification when given adversarial input.      
### 39.Privacy-Preserving Distributed Processing: Metrics, Bounds, and Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2009.01098.pdf)
>  Privacy-preserving distributed processing has recently attracted considerable attention. It aims to design solutions for conducting signal processing tasks over networks in a decentralized fashion without violating privacy. Many algorithms can be adopted to solve this problem such as differential privacy, secure multiparty computation, and the recently proposed distributed optimization based subspace perturbation. However, how these algorithms relate to each other is not fully explored yet. In this paper, we therefore first propose information-theoretic metrics based on mutual information. Using the proposed metrics, we are able to compare and relate a number of existing well-known algorithms. We then derive a lower bound on individual privacy that gives insights on the nature of the problem. To validate the above claims, we investigate a concrete example and compare a number of state-of-the-art approaches in terms of different aspects such as output utility, individual privacy and algorithm robustness against the number of corrupted parties, using not only theoretical analysis but also numerical validation. Finally, we discuss and provide principles for designing appropriate algorithms for different applications.      
### 40.Decentralized Source Localization Using Wireless Sensor Networks from Noisy Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.01062.pdf)
>  In this paper, the source (event) localization problem is studied in decentralized wireless sensor networks under the fault model where the sensor nodes observe the source and report their decisions to the Fusion Center (FC) for estimating source location. Due to fault model, sensor nodes may provide false positive or false negative decisions to the FC. Event localizations have many applications such as localizing intruder, pollutant sources like biological and chemical weapons, enemies positions in combat monitoring, and faults in power systems. We propose two methods to estimate the source location under the fault model: hitting set approach and feature selection method, which utilize the noisy data set at the FC for estimation of the source location. We have shown that these methods are more fault tolerant in estimating the source location and are not complex as well. We also study the lower bound on the sample complexity requirement for hitting set method. These methods have also been extended for multiple sources localization. Finally, extensive simulations are carried out for different parameters (i.e., the number of sensor nodes and sample complexity) to validate our proposed methods, which show that the proposed methods achieve better performances under the fault model.      
### 41.Travel time prediction for congested freeways with a dynamic linear model  [ :arrow_down: ](https://arxiv.org/pdf/2009.01016.pdf)
>  Accurate prediction of travel time is an essential feature to support Intelligent Transportation Systems (ITS). The non-linearity of traffic states, however, makes this prediction a challenging task. Here we propose to use dynamic linear models (DLMs) to approximate the non-linear traffic states. Unlike a static linear regression model, the DLMs assume that their parameters are changing across time. We design a DLM with model parameters defined at each time unit to describe the spatio-temporal characteristics of time-series traffic data. Based on our DLM and its model parameters analytically trained using historical data, we suggest an optimal linear predictor in the minimum mean square error (MMSE) sense. We compare our prediction accuracy of travel time for freeways in California (I210-E and I5-S) under highly congested traffic conditions with those of other methods: the instantaneous travel time, k-nearest neighbor, support vector regression, and artificial neural network. We show significant improvements in the accuracy, especially for short-term prediction.      
### 42.Cross-Utterance Language Models with Acoustic Error Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2009.01008.pdf)
>  The effective exploitation of richer contextual information in language models (LMs) is a long-standing research problem for automatic speech recognition (ASR). A cross-utterance LM (CULM) is proposed in this paper, which augments the input to a standard long short-term memory (LSTM) LM with a context vector derived from past and future utterances using an extraction network. The extraction network uses another LSTM to encode surrounding utterances into vectors which are integrated into a context vector using either a projection of LSTM final hidden states, or a multi-head self-attentive layer. In addition, an acoustic error sampling technique is proposed to reduce the mismatch between training and test-time. This is achieved by considering possible ASR errors into the model training procedure, and can therefore improve the word error rate (WER). Experiments performed on both AMI and Switchboard datasets show that CULMs outperform the LSTM LM baseline WER. In particular, the CULM with a self-attentive layer-based extraction network and acoustic error sampling achieves 0.6% absolute WER reduction on AMI, 0.3% WER reduction on the Switchboard part and 0.9% WER reduction on the Callhome part of Eval2000 test set over the respective baselines.      
### 43.ALANET: Adaptive Latent Attention Network forJoint Video Deblurring and Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2009.01005.pdf)
>  Existing works address the problem of generating high frame-rate sharp videos by separately learning the frame deblurring and frame interpolation modules. Most of these approaches have a strong prior assumption that all the input frames are blurry whereas in a real-world setting, the quality of frames varies. Moreover, such approaches are trained to perform either of the two tasks - deblurring or interpolation - in isolation, while many practical situations call for both. Different from these works, we address a more realistic problem of high frame-rate sharp video synthesis with no prior assumption that input is always blurry. We introduce a novel architecture, Adaptive Latent Attention Network (ALANET), which synthesizes sharp high frame-rate videos with no prior knowledge of input frames being blurry or not, thereby performing the task of both deblurring and interpolation. We hypothesize that information from the latent representation of the consecutive frames can be utilized to generate optimized representations for both frame deblurring and frame interpolation. Specifically, we employ combination of self-attention and cross-attention module between consecutive frames in the latent space to generate optimized representation for each frame. The optimized representation learnt using these attention modules help the model to generate and interpolate sharp frames. Extensive experiments on standard datasets demonstrate that our method performs favorably against various state-of-the-art approaches, even though we tackle a much more difficult problem.      
### 44.Variational Inference-Based Dropout in Recurrent Neural Networks for Slot Filling in Spoken Language Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2009.01003.pdf)
>  This paper proposes to generalize the variational recurrent neural network (RNN) with variational inference (VI)-based dropout regularization employed for the long short-term memory (LSTM) cells to more advanced RNN architectures like gated recurrent unit (GRU) and bi-directional LSTM/GRU. The new variational RNNs are employed for slot filling, which is an intriguing but challenging task in spoken language understanding. The experiments on the ATIS dataset suggest that the variational RNNs with the VI-based dropout regularization can significantly improve the naive dropout regularization RNNs-based baseline systems in terms of F-measure. Particularly, the variational RNN with bi-directional LSTM/GRU obtains the best F-measure score.      
### 45.Neural Crossbreed: Neural Based Image Metamorphosis  [ :arrow_down: ](https://arxiv.org/pdf/2009.00905.pdf)
>  We propose Neural Crossbreed, a feed-forward neural network that can learn a semantic change of input images in a latent space to create the morphing effect. Because the network learns a semantic change, a sequence of meaningful intermediate images can be generated without requiring the user to specify explicit correspondences. In addition, the semantic change learning makes it possible to perform the morphing between the images that contain objects with significantly different poses or camera views. Furthermore, just as in conventional morphing techniques, our morphing network can handle shape and appearance transitions separately by disentangling the content and the style transfer for rich usability. We prepare a training dataset for morphing using a pre-trained BigGAN, which generates an intermediate image by interpolating two latent vectors at an intended morphing value. This is the first attempt to address image morphing using a pre-trained generative model in order to learn semantic transformation. The experiments show that Neural Crossbreed produces high quality morphed images, overcoming various limitations associated with conventional approaches. In addition, Neural Crossbreed can be further extended for diverse applications such as multi-image morphing, appearance transfer, and video frame interpolation.      
### 46.GAIT: Gradient Adjusted Unsupervised Image-to-Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00878.pdf)
>  Image-to-image translation (IIT) has made much progress recently with the development of adversarial learning. In most of the recent work, an adversarial loss is utilized to match the distributions of the translated and target image sets. However, this may create artifacts if two domains have different marginal distributions, for example, in uniform areas. In this work, we propose an unsupervised IIT method that preserves the uniform regions after the translation. The gradient adjustment loss, which is the L2 norm between the Sobel response of the target image and the adjusted Sobel response of the source images, is utilized. The proposed method is validated on the jellyfish-to-Haeckel dataset, which is prepared to demonstrate the mentioned problem, which contains images with different background distributions. We demonstrate that our method obtained a performance gain compared to the baseline method qualitatively and quantitatively, showing the effectiveness of the proposed method.      
### 47.Online system identification in a Duffing oscillator by free energy minimisation  [ :arrow_down: ](https://arxiv.org/pdf/2009.00845.pdf)
>  Online system identification is the estimation of parameters of a dynamical system, such as mass or friction coefficients, for each measurement of the input and output signals. Here, the nonlinear stochastic differential equation of a Duffing oscillator is cast to a generative model and dynamical parameters are inferred using variational message passing on a factor graph of the model. The approach is validated with an experiment on data from an electronic implementation of a Duffing oscillator. The proposed inference procedure performs as well as offline prediction error minimisation in a state-of-the-art nonlinear model.      
### 48.Application of LSTM architectures for next frame forecasting in Sentinel-1 images time series  [ :arrow_down: ](https://arxiv.org/pdf/2009.00841.pdf)
>  L'analyse pr{é}dictive permet d'estimer les tendances des {é}v{è}nements futurs. De nos jours, les algorithmes Deep Learning permettent de faire de bonnes pr{é}dictions. Cependant, pour chaque type de probl{è}me donn{é}, il est n{é}cessaire de choisir l'architecture optimale. Dans cet article, les mod{è}les Stack-LSTM, CNN-LSTM et ConvLSTM sont appliqu{é}s {à} une s{é}rie temporelle d'images radar sentinel-1, le but {é}tant de pr{é}dire la prochaine occurrence dans une s{é}quence. Les r{é}sultats exp{é}rimentaux {é}valu{é}s {à} l'aide des indicateurs de performance tels que le RMSE et le MAE, le temps de traitement et l'index de similarit{é} SSIM, montrent que chacune des trois architectures peut produire de bons r{é}sultats en fonction des param{è}tres utilis{é}s.      
### 49.On the Structures of Representation for the Robustness of Semantic Segmentation to Input Corruption  [ :arrow_down: ](https://arxiv.org/pdf/2009.00817.pdf)
>  Semantic segmentation is a scene understanding task at the heart of safety-critical applications where robustness to corrupted inputs is essential. Implicit Background Estimation (IBE) has demonstrated to be a promising technique to improve the robustness to out-of-distribution inputs for semantic segmentation models for little to no cost. In this paper, we provide analysis comparing the structures learned as a result of optimization objectives that use Softmax, IBE, and Sigmoid in order to improve understanding their relationship to robustness. As a result of this analysis, we propose combining Sigmoid with IBE (SCrIBE) to improve robustness. Finally, we demonstrate that SCrIBE exhibits superior segmentation performance aggregated across all corruptions and severity levels with a mIOU of 42.1 compared to both IBE 40.3 and the Softmax Baseline 37.5.      
### 50.Receiver Design for OTFS with Fractionally Spaced Sampling Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.00806.pdf)
>  The recent emergence of orthogonal time frequency space (OTFS) modulation as a novel PHY-layer mechanism is more suitable in high-mobility wireless communication scenarios than traditional orthogonal frequency division multiplexing (OFDM). Although multiple studies have analyzed OTFS performance using theoretical and ideal baseband pulseshapes, a challenging and open problem is the development of effective receivers for practical OTFS systems that must rely on non-ideal pulseshapes for transmission. This work focuses on the design of practical receivers for OTFS. We consider a fractionally spaced sampling (FSS) receiver in which the sampling rate is an integer multiple of the symbol rate. For rectangular pulses used in OTFS transmission, we derive a general channel input-output relationship of OTFS in delay-Doppler domain without the common reliance on impractical assumptions such as ideal bi-orthogonal pulses and on-the-grid delay/Doppler shifts. We propose two equalization algorithms: iterative combining message passing (ICMP) and turbo message passing (TMP) for symbol detection by exploiting delay-Doppler channel sparsity and the frequency diversity gain via FSS. We analyze the convergence performance of TMP receiver and propose simplified message passing (MP) receivers to further reduce complexity. Our FSS receivers demonstrate stronger performance than traditional receivers and robustness to the imperfect channel state information knowledge.      
### 51.Non-asymptotic Identification of Linear Dynamical Systems Using Multiple Trajectories  [ :arrow_down: ](https://arxiv.org/pdf/2009.00739.pdf)
>  This paper considers the problem of linear time-invariant (LTI) system identification using input/output data. Recent work has provided non-asymptotic results on partially observed LTI system identification using a single trajectory but is only suitable for stable systems. We provide finite-time analysis for learning Markov parameters based on the ordinary least-squares (OLS) estimator using multiple trajectories, which covers both stable and unstable systems. For unstable systems, our results suggest that the Markov parameters are harder to estimate in the presence of process noise. Without process noise, our upper bound on the estimation error is independent of the spectral radius of system dynamics with high probability. These two features are different from fully observed LTI systems for which recent work has shown that unstable systems with a bigger spectral radius are easier to estimate. Extensive numerical experiments demonstrate the performance of our OLS estimator.      
### 52.A Deontic Logic Analysis of Autonomous Systems' Safety  [ :arrow_down: ](https://arxiv.org/pdf/2009.00738.pdf)
>  We consider the pressing question of how to model, verify, and ensure that autonomous systems meet certain \textit{obligations} (like the obligation to respect traffic laws), and refrain from impermissible behavior (like recklessly changing lanes). Temporal logics are heavily used in autonomous system design; however, as we illustrate here, temporal (alethic) logics alone are inappropriate for reasoning about obligations of autonomous systems. This paper proposes the use of Dominance Act Utilitarianism (DAU), a deontic logic of agency, to encode and reason about obligations of autonomous systems. We use DAU to analyze Intel's Responsibility-Sensitive Safety (RSS) proposal as a real-world case study. We demonstrate that DAU can express well-posed RSS rules, formally derive undesirable consequences of these rules, illustrate how DAU could help design systems that have specific obligations, and how to model-check DAU obligations.      
### 53.Short-term Traffic Prediction with Deep Neural Networks: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2009.00712.pdf)
>  In modern transportation systems, an enormous amount of traffic data is generated every day. This has led to rapid progress in short-term traffic prediction (STTP), in which deep learning methods have recently been applied. In traffic networks with complex spatiotemporal relationships, deep neural networks (DNNs) often perform well because they are capable of automatically extracting the most important features and patterns. In this study, we survey recent STTP studies applying deep networks from four perspectives. 1) We summarize input data representation methods according to the number and type of spatial and temporal dependencies involved. 2) We briefly explain a wide range of DNN techniques from the earliest networks, including Restricted Boltzmann Machines, to the most recent, including graph-based and meta-learning networks. 3) We summarize previous STTP studies in terms of the type of DNN techniques, application area, dataset and code availability, and the type of the represented spatiotemporal dependencies. 4) We compile public traffic datasets that are popular and can be used as the standard benchmarks. Finally, we suggest challenging issues and possible future research directions in STTP.      
### 54.Applying a random projection algorithm to optimize machine learning model for predicting peritoneal metastasis in gastric cancer patients using CT images  [ :arrow_down: ](https://arxiv.org/pdf/2009.00675.pdf)
>  Background and Objective: Non-invasively predicting the risk of cancer metastasis before surgery plays an essential role in determining optimal treatment methods for cancer patients (including who can benefit from neoadjuvant chemotherapy). Although developing radiomics based machine learning (ML) models has attracted broad research interest for this purpose, it often faces a challenge of how to build a highly performed and robust ML model using small and imbalanced image datasets. Methods: In this study, we explore a new approach to build an optimal ML model. A retrospective dataset involving abdominal computed tomography (CT) images acquired from 159 patients diagnosed with gastric cancer is assembled. Among them, 121 cases have peritoneal metastasis (PM), while 38 cases do not have PM. A computer-aided detection (CAD) scheme is first applied to segment primary gastric tumor volumes and initially computes 315 image features. Then, two Gradient Boosting Machine (GBM) models embedded with two different feature dimensionality reduction methods, namely, the principal component analysis (PCA) and a random projection algorithm (RPA) and a synthetic minority oversampling technique, are built to predict the risk of the patients having PM. All GBM models are trained and tested using a leave-one-case-out cross-validation method. Results: Results show that the GBM embedded with RPA yielded a significantly higher prediction accuracy (71.2%) than using PCA (65.2%) (p&lt;0.05). Conclusions: The study demonstrated that CT images of the primary gastric tumors contain discriminatory information to predict the risk of PM, and RPA is a promising method to generate optimal feature vector, improving the performance of ML models of medical images.      
### 55.Fed-Sim: Federated Simulation for Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2009.00668.pdf)
>  Labelling data is expensive and time consuming especially for domains such as medical imaging that contain volumetric imaging data and require expert knowledge. Exploiting a larger pool of labeled data available across multiple centers, such as in federated learning, has also seen limited success since current deep learning approaches do not generalize well to images acquired with scanners from different manufacturers. We aim to address these problems in a common, learning-based image simulation framework which we refer to as Federated Simulation. We introduce a physics-driven generative approach that consists of two learnable neural modules: 1) a module that synthesizes 3D cardiac shapes along with their materials, and 2) a CT simulator that renders these into realistic 3D CT Volumes, with annotations. Since the model of geometry and material is disentangled from the imaging sensor, it can effectively be trained across multiple medical centers. We show that our data synthesis framework improves the downstream segmentation performance on several datasets. Project Page: <a class="link-external link-https" href="https://nv-tlabs.github.io/fed-sim/" rel="external noopener nofollow">this https URL</a> .      
### 56.Large Intelligent Surface Aided Physical Layer Security Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2009.00473.pdf)
>  In this paper, we investigate a large intelligent surface-enhanced (LIS-enhanced) system, where a LIS is deployed to assist secure transmission. Our design aims to maximize the achievable secrecy rates in different channel models, i.e., Rician fading and (or) independent and identically distributed Gaussian fading for the legitimate and eavesdropper channels. In addition, we take into consideration an artificial noise-aided transmission structure for further improving system performance. The difficulties of tackling the aforementioned problems are the structure of the expected secrecy rate expressions and the non-convex phase shift constraint. To facilitate the design, we propose two frameworks, namely the sample average approximation based (SAA-based) algorithm and the hybrid stochastic projected gradient-convergent policy (hybrid SPG-CP) algorithm, to calculate the expectation terms in the secrecy rate expressions. Meanwhile, majorization minimization (MM) is adopted to address the non-convexity of the phase shift constraint. In addition, we give some analyses on two special scenarios by making full use of the expectation terms. Simulation results show that the proposed algorithms effectively optimize the secrecy communication rate for the considered setup, and the LIS-enhanced system greatly improves secrecy performance compared to conventional architectures without LIS.      
### 57.A robust, low-cost approach to Face Detection and Face Recognition  [ :arrow_down: ](https://arxiv.org/pdf/1111.1090.pdf)
>  In the domain of Biometrics, recognition systems based on iris, fingerprint or palm print scans etc. are often considered more dependable due to extremely low variance in the properties of these entities with respect to time. However, over the last decade data processing capability of computers has increased manifold, which has made real-time video content analysis possible. This shows that the need of the hour is a robust and highly automated Face Detection and Recognition algorithm with credible accuracy rate. The proposed Face Detection and Recognition system using Discrete Wavelet Transform (DWT) accepts face frames as input from a database containing images from low cost devices such as VGA cameras, webcams or even CCTV's, where image quality is inferior. Face region is then detected using properties of L*a*b* color space and only Frontal Face is extracted such that all additional background is eliminated. Further, this extracted image is converted to grayscale and its dimensions are resized to 128 x 128 pixels. DWT is then applied to entire image to obtain the coefficients. Recognition is carried out by comparison of the DWT coefficients belonging to the test image with those of the registered reference image. On comparison, Euclidean distance classifier is deployed to validate the test image from the database. Accuracy for various levels of DWT Decomposition is obtained and hence, compared.      
### 58.Audio Watermarking with Error Correction  [ :arrow_down: ](https://arxiv.org/pdf/1110.1209.pdf)
>  In recent times, communication through the internet has tremendously facilitated the distribution of multimedia data. Although this is indubitably a boon, one of its repercussions is that it has also given impetus to the notorious issue of online music piracy. Unethical attempts can also be made to deliberately alter such copyrighted data and thus, misuse it. Copyright violation by means of unauthorized distribution, as well as unauthorized tampering of copyrighted audio data is an important technological and research issue. Audio watermarking has been proposed as a solution to tackle this issue. The main purpose of audio watermarking is to protect against possible threats to the audio data and in case of copyright violation or unauthorized tampering, authenticity of such data can be disputed by virtue of audio watermarking.      
