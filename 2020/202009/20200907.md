# ArXiv eess --Mon, 7 Sep 2020
### 1.Deep data compression for approximate ultrasonic image formation  [ :arrow_down: ](https://arxiv.org/pdf/2009.02293.pdf)
>  In many ultrasonic imaging systems, data acquisition and image formation are performed on separate computing devices. Data transmission is becoming a bottleneck, thus, efficient data compression is essential. Compression rates can be improved by considering the fact that many image formation methods rely on approximations of wave-matter interactions, and only use the corresponding part of the data. Tailored data compression could exploit this, but extracting the useful part of the data efficiently is not always trivial. In this work, we tackle this problem using deep neural networks, optimized to preserve the image quality of a particular image formation method. The Delay-And-Sum (DAS) algorithm is examined which is used in reflectivity-based ultrasonic imaging. We propose a novel encoder-decoder architecture with vector quantization and formulate image formation as a network layer for end-to-end training. Experiments demonstrate that our proposed data compression tailored for a specific image formation method obtains significantly better results as opposed to compression agnostic to subsequent imaging. We maintain high image quality at much higher compression rates than the theoretical lossless compression rate derived from the rank of the linear imaging operator. This demonstrates the great potential of deep ultrasonic data compression tailored for a specific image formation method.      
### 2.Flow Field Reconstructions with GANs based on Radial Basis Functions  [ :arrow_down: ](https://arxiv.org/pdf/2009.02285.pdf)
>  Nonlinear sparse data regression and generation have been a long-term challenge, to cite the flow field reconstruction as a typical example. The huge computational cost of computational fluid dynamics (CFD) makes it much expensive for large scale CFD data producing, which is the reason why we need some cheaper ways to do this, of which the traditional reduced order models (ROMs) were promising but they couldn't generate a large number of full domain flow field data (FFD) to realize high-precision flow field reconstructions. Motivated by the problems of existing approaches and inspired by the success of the generative adversarial networks (GANs) in the field of computer vision, we prove an optimal discriminator theorem that the optimal discriminator of a GAN is a radial basis function neural network (RBFNN) while dealing with nonlinear sparse FFD regression and generation. Based on this theorem, two radial basis function-based GANs (RBF-GAN and RBFC-GAN), for regression and generation purposes, are proposed. Three different datasets are applied to verify the feasibility of our models. The results show that the performance of the RBF-GAN and the RBFC-GAN are better than that of GANs/cGANs by means of both the mean square error (MSE) and the mean square percentage error (MSPE). Besides, compared with GANs/cGANs, the stability of the RBF-GAN and the RBFC-GAN improve by 34.62% and 72.31%, respectively. Consequently, our proposed models can be used to generate full domain FFD from limited and sparse datasets, to meet the requirement of high-precision flow field reconstructions.      
### 3.A Effective Carrier Phase Recovery Method in Tigth Time-Packing Fast than Nyquist Optical Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2009.02282.pdf)
>  We propose a new scheme that combines polybinary transformaton and corrected-BPS to compensate noise for PDM-FTN-QPSK when its accelerated factor is 0.5,which has 3.3 dB OSNR gain when phase noise is 800 kHz.      
### 4.Communication and networking technologies for UAVs: A survey  [ :arrow_down: ](https://arxiv.org/pdf/2009.02280.pdf)
>  With the advancement in drone technology, in just a few years, drones will be assisting humans in every domain. But there are many challenges to be tackled, communication being the chief one. This paper aims at providing insights into the latest UAV (Unmanned Aerial Vehicle) communication technologies through investigation of suitable task modules, antennas, resource handling platforms, and network architectures. Additionally, we explore techniques such as machine learning and path planning to enhance existing drone communication methods. Encryption and optimization techniques for ensuring long lasting and secure communications, as well as for power management, are discussed. Moreover, applications of UAV networks for different contextual uses ranging from navigation to surveillance, URLLC (Ultra reliable and low latency communications), edge computing and work related to artificial intelligence are examined. In particular, the intricate interplay between UAV, advanced cellular communication, and internet of things constitutes one of the focal points of this paper. The survey encompasses lessons learned, insights, challenges, open issues, and future directions in UAV communications. Our literature review reveals the need for more research work on drone to drone and drone to device communications.      
### 5.A Review of Wind Speed and Wind Power Forecasting Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2009.02279.pdf)
>  Forecasting a particular variable can depend upon temporal or spatial scale. Temporal variations that indicate variations with time, reflect the stochasticity present in the variable. Spatial variation usually are dominant in climatology and meteorology. Temporal scale for a variable can be modeled in terms of time-series. A time series is a successively ordered sequence of numerical data points, and can be taken on any variable changing with time. Wind speed forecasting applications lie majorly in the area of electricity market clearing, economic load dispatch and scheduling, and sometimes to provide ancillary support. Thus, a proper classification based on the prediction horizon i.e. the duration of prediction becomes important for various transmission system operators.      
### 6.Improving axial resolution in SIM using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.02264.pdf)
>  Structured Illumination Microscopy is a widespread methodology to image live and fixed biological structures smaller than the diffraction limits of conventional optical microscopy. Using recent advances in image up-scaling through deep learning models, we demonstrate a method to reconstruct 3D SIM image stacks with twice the axial resolution attainable through conventional SIM reconstructions. We further evaluate our method for robustness to noise &amp; generalisability to varying observed specimens, and discuss potential adaptions of the method to further improvements in resolution.      
### 7.Towards Secure Localization in Randomly Deployed Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.02259.pdf)
>  Being able to accurately locate wireless devices, while guaranteeing high-level of security against spoofing attacks, benefits all participants in the localization chain (e.g., end users, network operators, and location service providers). On the one hand, most of existing localization systems are designed for innocuous environments, where no malicious adversaries are present. On the other hand, existing secure localization solutions make certain assumptions regarding the network topology, which restrict their applicability. Therefore, this work addresses the problem of target localization in randomly deployed wireless networks in the presence of a malicious attacker, whose goal is to manipulate (spoof) the estimation process in order to disable accurate localization. This is an important problem since strengthening the security of current non-secure systems or generalization of existing secure localization systems to ad hoc scenarios will enable additional reliable safety parameter (location) to be employed for digital interactions in more general contexts (such as social media, health monitoring or surveillance systems). We propose a low-complex solution based on clustering and weighted central mass to detect the attacker, using only the bare minimum of reference points, after which we solve the localization problem by a bisection procedure. The proposed method is studied from both localization accuracy and success in attacker detection point of views, where closed-form expressions for upper and lower bounds on the probability of attacker detection are derived. Its performance is validated through computer simulations, which corroborate the effectiveness of the proposed scheme, outperforming the state-of-the-art method.      
### 8.Fast Zonotope-Tube-based LPV-MPC for Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2009.02248.pdf)
>  In this paper, we present an effective online tube-based model predictive control (T-MPC) solution for autonomous driving that aims at improving the computational load while ensuring robust stability and performance in fast and disturbed scenarios. We focus on reformulating the non-linear original problem into a pseudo-linear problem by transforming the non-linear vehicle equations to be expressed in a Linear Parameter Varying (LPV) form. An scheme composed by a nominal controller and a corrective local controller is propossed. First, the local controller is designed as a polytopic LPV-H$_{\infty}$ controller able to reject external disturbances. Moreover, a finite number of accurate reachable sets, also called tube, are computed online using zonotopes taking into account the system dynamics, the local controller and the diturbance-uncertainty bounds considered. Second, the nominal controller is designed as an MPC where the LPV vehicle model is used to speed up the computational time while keeping accurate vehicle representation. Employing reachability theory with zonotopes, the MPC changes online its state and input constraints to ensure robust feasibility and stability under exhogenous disturbances. Finally, we test the presented scheme and compare the local controller performance against the LQR design as state of the art approach. We demonstrate its effectiveness in a disturbed fast driving scenario being able to reject strong exogenous disturbances and fulfilling imposed constraints at a very reduced computational cost.      
### 9.Evolving Intelligent Reflector Surface towards 6G for Public Health: Application in Airborne Virus Detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.02224.pdf)
>  While metasurface based intelligent reflecting surfaces (IRS) are an important emerging technology for future generations of wireless connectivity in its own right, the plans for the mass deployment of these surfaces motivate the question of their integration with other new and emerging technologies that would require mass proliferation. This question of integration and the vision of future communication systems as an invaluable component for public health motivated our new concept of Intelligent Reflector-Viral Detectors (IR-VD). In this novel scheme, we propose deployment of intelligent reflectors with strips of receptor-based viral detectors placed between the reflective surface tiles. Our proposed approach encodes information of the virus by flicking the angle of the reflected beams, using time variations between the beam deviations to represent the messages. This information includes the presence of the virus, its location and load size. The paper presents simulation to demonstrate the encoding process based on varying quantity of virus that have bound onto the IR-VD.      
### 10.Lorentzian Peak Sharpening and Sparse Blind Source Separation for NMR Spectroscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.02200.pdf)
>  In this paper, we introduce a preprocessing technique for blind source separation (BSS) of nonnegative and overlapped data. For Nuclear Magnetic Resonance spectroscopy (NMR), the classical method of Naanaa and Nuzillard (NN) requires the condition that source signals to be non-overlapping at certain locations while they are allowed to overlap with each other elsewhere. NN's method works well with data signals that possess stand alone peaks (SAP). The SAP does not hold completely for realistic NMR spectra however. Violation of SAP often introduces errors or artifacts in the NN's separation results. To address this issue, a preprocessing technique is developed here based on Lorentzian peak shapes and weighted peak sharpening. The idea is to superimpose the original peak signal with its weighted negative second order derivative. The resulting sharpened (narrower and taller) peaks enable NN's method to work with a more relaxed SAP condition, the so called dominant peaks condition (DPS), and deliver improved results. To achieve an optimal sharpening while preserving the data nonnegativity, we prove the existence of an upper bound of the weight parameter and propose a selection criterion. Numerical experiments on NMR spectroscopy data show satisfactory performance of our proposed method.      
### 11.Fast ultrasonic imaging using end-to-end deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.02194.pdf)
>  Ultrasonic imaging algorithms used in many clinical and industrial applications consist of three steps: A data pre-processing, an image formation and an image post-processing step. For efficiency, image formation often relies on an approximation of the underlying wave physics. A prominent example is the Delay-And-Sum (DAS) algorithm used in reflectivity-based ultrasonic imaging. Recently, deep neural networks (DNNs) are being used for the data pre-processing and the image post-processing steps separately. In this work, we propose a novel deep learning architecture that integrates all three steps to enable end-to-end training. We examine turning the DAS image formation method into a network layer that connects data pre-processing layers with image post-processing layers that perform segmentation. We demonstrate that this integrated approach clearly outperforms sequential approaches that are trained separately. While network training and evaluation is performed only on simulated data, we also showcase the potential of our approach on real data from a non-destructive testing scenario.      
### 12.Degradation effects of water immersion on earbud audio quality  [ :arrow_down: ](https://arxiv.org/pdf/2009.02151.pdf)
>  Earbuds are subjected to constant use and scenarios that may degrade sound quality. Indeed, a common fate of earbuds is being forgotten in pockets and faced with a laundry cycle (LC). Manufacturers' accounts of the extent to which LCs affect earbud sound quality are vague at best, leaving users to their own devices in assessing the damage caused. This paper offers a systematic, empirical approach to measure the effects of laundering earbuds on sound quality. Three earbud pairs were subjected to LCs spaced 24 hours apart. After each LC, a professional microphone as well as a mid-market smartphone were used to record i) a test tone ii) a frequency sweep and iii) a music signal played through the earbuds. We deployed mixed effects models and found significant degradation in terms of RMS noise loudness, Total Harmonic Distortion (THD), as well as measures of change in the frequency responses of the earbuds. All transducers showed degradation already after the first cycle, and no transducers produced a measurable signal after the sixth LC. The degradation effects were detectable in both, the professional microphone as well as the smartphone recordings. We hope that the present work is a first step in establishing a practical, and ecologically valid method for everyday users to assess the degree of degradation of their personal earbuds.      
### 13.On the Inherent Dose-Reduction Potential of Classical Ghost Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2009.02139.pdf)
>  Classical ghost imaging is a computational imaging technique that employs patterned illumination. It is very similar in concept to the single-pixel camera in that an image may be reconstructed from a set of measurements even though all imaging quanta that pass through that sample are never recorded with a position resolving detector. The method was first conceived and applied for visible-wavelength photons and was subsequently translated to other probes such as x rays, atomic beams, electrons and neutrons. In the context of ghost imaging using penetrating probes that enable transmission measurement, we here consider several questions relating to the achievable signal-to-noise ratio (SNR). This is compared with the SNR for conventional imaging under scenarios of constant radiation dose and constant experiment time, considering both photon shot-noise and per-measurement electronic read-out noise. We show that inherent improved SNR capabilities of ghost imaging are limited to a subset of these scenarios and are actually due to increased dose (Fellgett advantage). An explanation is also presented for recent results published in the literature that are not consistent with these findings.      
### 14.Multi-Attention-Network for Semantic Segmentation of High-Resolution Remote Sensing Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.02130.pdf)
>  Semantic segmentation of remote sensing images plays an important role in land resource management, yield estimation, and economic assessment. Even though the semantic segmentation of remote sensing images has been prominently improved by convolutional neural networks, there are still several limitations contained in standard models. First, for encoder-decoder architectures like U-Net, the utilization of multi-scale features causes overuse of information, where similar low-level features are exploited at multiple scales for multiple times. Second, long-range dependencies of feature maps are not sufficiently explored, leading to feature representations associated with each semantic class are not optimal. Third, despite the dot-product attention mechanism has been introduced and harnessed widely in semantic segmentation to model long-range dependencies, the high time and space complexities of attention impede the usage of attention in application scenarios with large input. In this paper, we proposed a Multi-Attention-Network (MANet) to remedy these drawbacks, which extracts contextual dependencies by multi efficient attention mechanisms. A novel attention mechanism named kernel attention with linear complexity is proposed to alleviate the high computational demand of attention. Based on kernel attention and channel attention, we integrate local feature maps extracted by ResNeXt-101 with their corresponding global dependencies, and adaptively signalize interdependent channel maps. Experiments conducted on two remote sensing image datasets captured by variant satellites demonstrate that the performance of our MANet transcends the DeepLab V3+, PSPNet, FastFCN, and other baseline algorithms.      
### 15.Silent Speech Interfaces for Speech Restoration: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2009.02110.pdf)
>  This review summarises the status of silent speech interface (SSI) research. SSIs rely on non-acoustic biosignals generated by the human body during speech production to enable communication whenever normal verbal communication is not possible or not desirable. In this review, we focus on the first case and present latest SSI research aimed at providing new alternative and augmentative communication methods for persons with severe speech disorders. SSIs can employ a variety of biosignals to enable silent communication, such as electrophysiological recordings of neural activity, electromyographic (EMG) recordings of vocal tract movements or the direct tracking of articulator movements using imaging techniques. Depending on the disorder, some sensing techniques may be better suited than others to capture speech-related information. For instance, EMG and imaging techniques are well suited for laryngectomised patients, whose vocal tract remains almost intact but are unable to speak after the removal of the vocal folds, but fail for severely paralysed individuals. From the biosignals, SSIs decode the intended message, using automatic speech recognition or speech synthesis algorithms. Despite considerable advances in recent years, most present-day SSIs have only been validated in laboratory settings for healthy users. Thus, as discussed in this paper, a number of challenges remain to be addressed in future research before SSIs can be promoted to real-world applications. If these issues can be addressed successfully, future SSIs will improve the lives of persons with severe speech impairments by restoring their communication capabilities.      
### 16.SEANet: A Multi-modal Speech Enhancement Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.02095.pdf)
>  We explore the possibility of leveraging accelerometer data to perform speech enhancement in very noisy conditions. Although it is possible to only partially reconstruct user's speech from the accelerometer, the latter provides a strong conditioning signal that is not influenced from noise sources in the environment. Based on this observation, we feed a multi-modal input to SEANet (Sound EnhAncement Network), a wave-to-wave fully convolutional model, which adopts a combination of feature losses and adversarial losses to reconstruct an enhanced version of user's speech. We trained our model with data collected by sensors mounted on an earbud and synthetically corrupted by adding different kinds of noise sources to the audio signal. Our experimental results demonstrate that it is possible to achieve very high quality results, even in the case of interfering speech at the same level of loudness. A sample of the output produced by our model is available at <a class="link-external link-https" href="https://google-research.github.io/seanet/multimodal/speech" rel="external noopener nofollow">this https URL</a>.      
### 17.Massive Machine Type Communication Pilot-Hopping Sequence Detection Architectures Based on Non-Negative Least Squares for Grant-Free Random Access  [ :arrow_down: ](https://arxiv.org/pdf/2009.02089.pdf)
>  User activity detection in grant-free random access massive machine type communication (mMTC) using pilot-hopping sequences can be formulated as solving a non-negative least squares (NNLS) problem. In this work, two architectures using different algorithms to solve the NNLS problem is proposed. The algorithms are implemented using a fully parallel approach and fixed-point arithmetic, leading to high detection rates and low power consumption. The first algorithm, fast projected gradients, converges faster to the optimal value. The second algorithm, multiplicative updates, is partially implemented in the logarithmic domain, and provides a smaller chip area and lower power consumption. For a detection rate of about one million detections per second, the chip area for the fast algorithm is about 0.7 mm$^2$ compared to about 0.5 mm$^2$ for the multiplicative algorithm when implemented in a 28 nm FD-SOI standard cell process at 1 V power supply voltage. The energy consumption is about 300 nJ/detection for the fast projected gradient algorithm using 256 iterations, leading to a convergence close to the theoretical. With 128 iterations, about 250 nJ/detection is required, with a detection performance on par with 192 iterations of the multiplicative algorithm for which about 100 nJ/detection is required.      
### 18.Coordination Between TSOs and DSOs: Flexibility Domain Identification  [ :arrow_down: ](https://arxiv.org/pdf/2009.02088.pdf)
>  The enormous technological potential accumulated over the past two decades would make possible to change the operating principles of power systems entirely. The consequent technological evolution is not only affecting the structure of the electricity markets, but also the interactions between Transmission System Operators (TSOs) and Distribution System Operators (DSOs). New practical solutions are needed to improve the coordination between the grid operators at the national, TSOs, and local level, DSOs. In this paper, we define the flexibility range of coordination between TSOs and DSOs. By doing so, we propose an algorithm based on epsilon-constrained methods by means of mathematical programming and power systems principles. We evaluate and compare different classical optimal power flow formulations (AC-OPF, DISTFLOW, DISTFLOW-SOCP, and LINDISTFLOW) for building the flexible TSO-DSO flexible domain. The presented approaches in this paper are analyzed in an IEEE 33-bus test radial distribution system. We show that for this particular problem, the DISTFLOW-SOCP has the worst accuracy, despite the popularity among the academic community of convex relaxation approaches.      
### 19.Spatio-Temporal Hierarchical Adaptive Dispatching for Ridesharing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.02080.pdf)
>  Nowadays, ridesharing has become one of the most popular services offered by online ride-hailing platforms (e.g., Uber and Didi Chuxing). Existing ridesharing platforms adopt the strategy that dispatches orders over the entire city at a uniform time interval. However, the uneven spatio-temporal order distributions in real-world ridesharing systems indicate that such an approach is suboptimal in practice. Thus, in this paper, we exploit adaptive dispatching intervals to boost the platform's profit under a guarantee of the maximum passenger waiting time. Specifically, we propose a hierarchical approach, which generates clusters of geographical areas suitable to share the same dispatching intervals, and then makes online decisions of selecting the appropriate time instances for order dispatch within each spatial cluster. Technically, we prove the impossibility of designing constant-competitive-ratio algorithms for the online adaptive interval problem, and propose online algorithms under partial or even zero future order knowledge that significantly improve the platform's profit over existing approaches. We conduct extensive experiments with a large-scale ridesharing order dataset, which contains all of the over 3.5 million ridesharing orders in Beijing, China, received by Didi Chuxing from October 1st to October 31st, 2018. The experimental results demonstrate that our proposed algorithms outperform existing approaches.      
### 20.Algorithm and VLSI Design for 1-bit Data Detection in Massive MIMO-OFDM  [ :arrow_down: ](https://arxiv.org/pdf/2009.02068.pdf)
>  The use of low-resolution data converters in the radio-frequency (RF) chains of all-digital massive multiple-input multiple-output (MIMO) basestations promises significant reductions in power consumption, hardware costs, and interconnect bandwidth. We propose a quantization-aware data-detection algorithm which mitigates the performance loss of 1-bit quantized massive MIMO orthogonal frequency-division multiplexing (OFDM) systems. Since the system performance heavily depends on the quality of channel estimates, we also develop a nonlinear 1-bit channel estimation algorithm that builds upon the proposed data detection algorithm. We show that the proposed algorithms significantly outperform linear data detectors and channel estimators in terms of bit error rate. For the proposed nonlinear data detection algorithm, we develop a very large scale integration (VLSI) architecture and present implementation results on a Xilinx Virtex-7 field programmable gate array (FPGA). Our implementation results are, to the best of our knowledge, the first for 1-bit massive MU-MIMO-OFDM systems and demonstrate comparable hardware efficiency with respect to state-of-the-art linear data detectors designed for systems with high-resolution data converters, while achieving lower bit error rate.      
### 21.Delay Compensation for Regular Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.02046.pdf)
>  This is the third part of four series papers, aiming at the delay compensation for the abstract linear system (A,B,C). Both the input delay and output delay are investigated. We first propose a full state feedback control to stabilize the system (A,B) with input delay and then design a Luenberger-like observer for the system (A,C) in terms of the delayed output. We formulate the delay compensation in the framework of regular linear systems. The developed approach builds upon an upper-block-triangle transform that is associated with a Sylvester operator equation. It is found that the controllability/observability map of system (-A,B)/(-A,-C) happens to be the solution of the corresponding Sylvester equation. As an immediate consequence, both the feedback law and the state observer can be expressed explicitly in the operator form. The exponential stability of the resulting closed-loop system and the exponential convergence of the observation error are established without using the Lyapunov functional approach. The theoretical results are validated through the delay compensation for a benchmark one-dimensional wave equation.      
### 22.What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS  [ :arrow_down: ](https://arxiv.org/pdf/2009.02035.pdf)
>  In incremental text to speech synthesis (iTTS), the synthesizer produces an audio output before it has access to the entire input sentence. In this paper, we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token n, the system has access to n + k tokens from the text sequence. We first analyze the impact of this incremental policy on the evolution of the encoder representations of token n for different values of k (the lookahead parameter). The results show that, on average, tokens travel 88% of the way to their full context representation with a one-word lookahead and 94% after 2 words. We then investigate which text features are the most influential on the evolution towards the final representation using a random forest analysis. The results show that the most salient factors are related to token length. We finally evaluate the effects of lookahead k at the decoder level, using a MUSHRA listening test. This test shows results that contrast with the above high figures: speech synthesis quality obtained with 2 word-lookahead is significantly lower than the one obtained with the full sentence.      
### 23.Learning Constellation Map with Deep CNN for Accurate Modulation Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.02026.pdf)
>  Modulation classification, recognized as the intermediate step between signal detection and demodulation, is widely deployed in several modern wireless communication systems. Although many approaches have been studied in the last decades for identifying the modulation format of an incoming signal, they often reveal the obstacle of learning radio characteristics for most traditional machine learning algorithms. To overcome this drawback, we propose an accurate modulation classification method by exploiting deep learning for being compatible with constellation diagram. Particularly, a convolutional neural network is developed for proficiently learning the most relevant radio characteristics of gray-scale constellation image. The deep network is specified by multiple processing blocks, where several grouped and asymmetric convolutional layers in each block are organized by a flow-in-flow structure for feature enrichment. These blocks are connected via skip-connection to prevent the vanishing gradient problem while effectively preserving the information identify throughout the network. Regarding several intensive simulations on the constellation image dataset of eight digital modulations, the proposed deep network achieves the remarkable classification accuracy of approximately 87% at 0 dB signal-to-noise ratio (SNR) under a multipath Rayleigh fading channel and further outperforms some state-of-the-art deep models of constellation-based modulation classification.      
### 24.Chain-Net: Learning Deep Model for Modulation Classification Under Synthetic Channel Impairment  [ :arrow_down: ](https://arxiv.org/pdf/2009.02023.pdf)
>  Modulation classification, an intermediate process between signal detection and demodulation in a physical layer, is now attracting more interest to the cognitive radio field, wherein the performance is powered by artificial intelligence algorithms. However, most existing conventional approaches pose the obstacle of effectively learning weakly discriminative modulation patterns. This paper proposes a robust modulation classification method by taking advantage of deep learning to capture the meaningful information of modulation signal at multi-scale feature representations. To this end, a novel architecture of convolutional neural network, namely Chain-Net, is developed with various asymmetric kernels organized in two processing flows and associated via depth-wise concatenation and element-wise addition for optimizing feature utilization. The network is evaluated on a big dataset of 14 challenging modulation formats, including analog and high-order digital techniques. The simulation results demonstrate that Chain-Net robustly classifies the modulation of radio signals suffering from a synthetic channel deterioration and further performs better than other deep networks.      
### 25.Adversarial Learning of Robust and Safe Controllers for Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.02019.pdf)
>  We introduce a novel learning-based approach to synthesize safe and robust controllers for autonomous Cyber-Physical Systems and, at the same time, to generate challenging tests. This procedure combines formal methods for model verification with Generative Adversarial Networks. The method learns two Neural Networks: the first one aims at generating troubling scenarios for the controller, while the second one aims at enforcing the safety constraints. We test the proposed method on a variety of case studies.      
### 26.Including steady-state information in nonlinear models: an application to the development of soft-sensors  [ :arrow_down: ](https://arxiv.org/pdf/2009.01978.pdf)
>  When the dynamical data of a system only convey dynamic information over a limited operating range, the identification of models with good performance over a wider operating range is very unlikely. Nevertheless, models with such characteristic are desirable to implement modern control systems. To overcome such a shortcoming, this paper describes a methodology to train models from dynamical data and steady-state information, which is assumed available. The novelty is that the procedure can be applied to models with rather complex structures such as multilayer perceptron neural networks in a bi-objective fashion without the need to compute fixed points neither analytically nor numerically. As a consequence, the required computing time is greatly reduced. The capabilities of the proposed method are explored in numerical examples and the development of soft-sensors for downhole pressure estimation for a real deep-water offshore oil well. The results indicate that the procedure yields suitable soft-sensors with good dynamical and static performance and, in the case of models that are nonlinear in the parameters, the gain in computation time is about three orders of magnitude considering existing approaches.      
### 27.Automatic segmentation of CT images for ventral body composition analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.01965.pdf)
>  Purpose: Body composition is known to be associated with many diseases including diabetes, cancers and cardiovascular diseases. In this paper, we developed a fully automatic body tissue decomposition procedure to segment three major compartments that are related to body composition analysis - subcutaneous adipose tissue (SAT), visceral adipose tissue (VAT) and muscle. Three additional compartments - the ventral cavity, lung and bones were also segmented during the segmentation process to assist segmentation of the major compartments. <br>Methods: A convolutional neural network (CNN) model with densely connected layers was developed to perform ventral cavity segmentation. An image processing workflow was developed to segment the ventral cavity in any patient's CT using the CNN model, then further segment the body tissue into multiple compartments using hysteresis thresholding followed by morphological operations. It is important to segment ventral cavity firstly to allow accurate separation of compartments with similar Hounsfield unit (HU) inside and outside the ventral cavity. <br>Results: The ventral cavity segmentation CNN model was trained and tested with manually labelled ventral cavities in 60 CTs. Dice scores (mean +/- standard deviation) for ventral cavity segmentation were 0.966+/-0.012. Tested on CT datasets with intravenous (IV) and oral contrast, the Dice scores were 0.96+/-0.02, 0.94+/-0.06, 0.96+/-0.04, 0.95+/-0.04 and 0.99+/-0.01 for bone, VAT, SAT, muscle and lung, respectively. The respective Dice scores were 0.97+/-0.02, 0.94+/-0.07, 0.93+/-0.06, 0.91+/-0.04 and 0.99+/-0.01 for non-contrast CT datasets. <br>Conclusion: A body tissue decomposition procedure was developed to automatically segment multiple compartments of the ventral body. The proposed method enables fully automated quantification of 3D ventral body composition metrics from CT images.      
### 28.Dense CNN with Self-Attention for Time-Domain Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2009.01941.pdf)
>  Speech enhancement in the time domain is becoming increasingly popular in recent years, due to its capability to jointly enhance both the magnitude and the phase of speech. In this work, we propose a dense convolutional network (DCN) with self-attention for speech enhancement in the time domain. DCN is an encoder and decoder based architecture with skip connections. Each layer in the encoder and the decoder comprises a dense block and an attention module. Dense blocks and attention modules help in feature extraction using a combination of feature reuse, increased network depth, and maximum context aggregation. Furthermore, we reveal previously unknown problems with a loss based on the spectral magnitude of enhanced speech. To alleviate these problems, we propose a novel loss based on magnitudes of enhanced speech and a predicted noise. Even though the proposed loss is based on magnitudes only, a constraint imposed by noise prediction ensures that the loss enhances both magnitude and phase. Experimental results demonstrate that DCN trained with the proposed loss substantially outperforms other state-of-the-art approaches to causal and non-causal speech enhancement.      
### 29.Sparse Sensing and Optimal Precision: Robust $\mathcal{H}_{\infty}$ Optimal Observer Design with Model Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2009.01930.pdf)
>  We present a framework which incorporates three aspects of the estimation problem, namely, sparse sensor configuration, optimal precision, and robustness in the presence of model uncertainty. The problem is formulated in the $\mathcal{H}_{\infty}$ optimal observer design framework. We consider two types of uncertainties in the system, i.e. structured affine and unstructured uncertainties. The objective is to design an observer with a given $\mathcal{H}_{\infty}$ performance index with minimal number of sensors and minimal precision values, while guaranteeing the performance for all admissible uncertainties. The problem is posed as a convex optimization problem subject to linear matrix inequalities. Numerical simulations demonstrate the application of the theoretical results presented in this work.      
### 30.Introduction to Medical Image Registration with DeepReg, Between Old and New  [ :arrow_down: ](https://arxiv.org/pdf/2009.01924.pdf)
>  This document outlines a tutorial to get started with medical image registration using the open-source package DeepReg. The basic concepts of medical image registration are discussed, linking classical methods to newer methods using deep learning. Two iterative, classical algorithms using optimisation and one learning-based algorithm using deep learning are coded step-by-step using DeepReg utilities, all with real, open-accessible, medical data.      
### 31.The Little W-Net That Could: State-of-the-Art Retinal Vessel Segmentation with Minimalistic Models  [ :arrow_down: ](https://arxiv.org/pdf/2009.01907.pdf)
>  The segmentation of the retinal vasculature from eye fundus images represents one of the most fundamental tasks in retinal image analysis. Over recent years, increasingly complex approaches based on sophisticated Convolutional Neural Network architectures have been slowly pushing performance on well-established benchmark datasets. In this paper, we take a step back and analyze the real need of such complexity. Specifically, we demonstrate that a minimalistic version of a standard U-Net with several orders of magnitude less parameters, carefully trained and rigorously evaluated, closely approximates the performance of current best techniques. In addition, we propose a simple extension, dubbed W-Net, which reaches outstanding performance on several popular datasets, still using orders of magnitude less learnable weights than any previously published approach. Furthermore, we provide the most comprehensive cross-dataset performance analysis to date, involving up to 10 different databases. Our analysis demonstrates that the retinal vessel segmentation problem is far from solved when considering test images that differ substantially from the training data, and that this task represents an ideal scenario for the exploration of domain adaptation techniques. In this context, we experiment with a simple self-labeling strategy that allows us to moderately enhance cross-dataset performance, indicating that there is still much room for improvement in this area. Finally, we also test our approach on the Artery/Vein segmentation problem, where we again achieve results well-aligned with the state-of-the-art, at a fraction of the model complexity in recent literature. All the code to reproduce the results in this paper is released.      
### 32.Federated Learning for Breast Density Classification: A Real-World Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2009.01871.pdf)
>  Building robust deep learning-based models requires large quantities of diverse training data. In this study, we investigate the use of federated learning (FL) to build medical imaging classification models in a real-world collaborative setting. Seven clinical institutions from across the world joined this FL effort to train a model for breast density classification based on Breast Imaging, Reporting &amp; Data System (BI-RADS). We show that despite substantial differences among the datasets from all sites (mammography system, class distribution, and data set size) and without centralizing data, we can successfully train AI models in federation. The results show that models trained using FL perform 6.3% on average better than their counterparts trained on an institute's local data alone. Furthermore, we show a 45.8% relative improvement in the models' generalizability when evaluated on the other participating sites' testing data.      
### 33.Computed extended depth of field optical-resolution photoacoustic microscope  [ :arrow_down: ](https://arxiv.org/pdf/2009.01840.pdf)
>  Photoacoustic microscopy with large depth of focus is significant to the biomedical research. The conventional optical-resolution photoacoustic microscope (OR-PAM) suffers from limited depth of field (DoF) since the employed focused Gaussian beam only has a narrow depth range in focus, little details in depth direction can be revealed. Here, we developed a computed extended depth of field method for photoacoustic microscope by using wavelet transform image fusion rules. Wavelet transform is performed on the max amplitude projection (MAP) images acquired at different axial positions by OR-PAM to separate the low and high frequencies, respectively. The fused low frequency coefficients is taking the average of the low-frequency coefficients of the low-frequency part of the images. And maximum selection rule is used in high frequency coefficients. Wavelet coefficient of the MAP images are compared and select the maximum value coefficient is taken as fused high-frequency coefficients. And finally the wavelet inverse transform is performed to achieve large DoF. Simulation was performed to demonstrate that this method can extend the depth of field of PAM two times without the sacrifice of lateral resolution. And the in vivo imaging of the mouse cerebral vasculature with intact skull further demonstrates the feasibility of our method.      
### 34.Intelligent Reflecting Surface Aided Multi-User Communication: Capacity Region and Deployment Strategy  [ :arrow_down: ](https://arxiv.org/pdf/2009.02324.pdf)
>  Intelligent reflecting surface (IRS) is a new promising technology that is able to reconfigure the wireless propagation channel via smart and passive signal reflection. In this paper, we investigate the capacity region of a communication network with two users served by an access point (AP), aided by $M$ IRS reflecting elements. In particular, we consider two practical IRS deployment strategies that lead to different effective channels between the users and AP, namely, the distributed deployment where the $M$ reflecting elements form two IRSs, each deployed in the vicinity of one user, versus the centralized deployment where all the $M$ reflecting elements are deployed in the vicinity of the AP. First, we consider the uplink multiple access channel (MAC) and derive the capacity/achievable rate regions for both deployment strategies under different multiple access schemes. It is shown that the centralized deployment generally outperforms the distributed deployment under symmetric channel setups in terms of achievable user rates. Next, we extend the results to the downlink broadcast channel (BC) by leveraging the celebrated uplink-downlink (or MAC-BC) duality framework, and show that the superior rate performance of centralized over distributed deployment also holds. Numerical results are presented that validate our analysis, and reveal new and useful insights for optimal IRS deployment in wireless networks.      
### 35.Proposing a two-step Decision Support System (TPIS) based on Stacked ensemble classifier for early and low cost (step-1) and final (step-2) differential diagnosis of Mycobacterium Tuberculosis from non-tuberculosis Pneumonia  [ :arrow_down: ](https://arxiv.org/pdf/2009.02316.pdf)
>  Background: Mycobacterium Tuberculosis (TB) is an infectious bacterial disease presenting similar symptoms to pneumonia; therefore, differentiating between TB and pneumonia is challenging. Therefore, the main aim of this study is proposing an automatic method for differential diagnosis of TB from Pneumonia. Methods: In this study, a two-step decision support system named TPIS is proposed for differential diagnosis of TB from pneumonia based on stacked ensemble classifiers. The first step of our proposed model aims at early diagnosis based on low-cost features including demographic characteristics and patient symptoms (including 18 features). TPIS second step makes the final decision based on the meta features extracted in the first step, the laboratory tests and chest radiography reports. This retrospective study considers 199 patient medical records for patients suffering from TB or pneumonia, which has been registered in a hospital in Arak, Iran. Results: Experimental results show that TPIS outperforms the compared machine learning methods for early differential diagnosis of pulmonary tuberculosis from pneumonia with AUC of 90.26 and accuracy of 91.37 and final decision making with AUC of 92.81 and accuracy of 93.89. Conclusions: The main advantage of early diagnosis is beginning the treatment procedure for confidently diagnosed patients as soon as possible and preventing latency in treatment. Therefore, early diagnosis reduces the maturation of late treatment of both diseases.      
### 36.AIMx: An Extended Adaptive Integral Method for the Fast Electromagnetic Modeling of Complex Structures  [ :arrow_down: ](https://arxiv.org/pdf/2009.02281.pdf)
>  Surface integral equation (SIE) methods are of great interest for the efficient electromagnetic modeling of various devices, from integrated circuits to antenna arrays. Existing acceleration algorithms for SIEs, such as the adaptive integral method (AIM), enable the fast approximation of interactions between well-separated mesh elements. Nearby interactions involve the singularity of the kernel, and must instead be computed accurately with direct integration at each frequency of interest, which can be computationally expensive. In this work, a novel algorithm is proposed for reducing the cost-per-frequency associated with near-region computations for both homogeneous and layered background media. In the proposed extended AIM (AIMx), the SIE operators are decomposed into a frequency-independent term, which contains the singularity of the kernel, and a frequency-dependent term, which is a smooth function. The expensive near-region computations are only required for the frequency-independent term, and can be reused at each frequency point, leading to significantly faster frequency sweeps. The frequency-dependent term is accurately captured via the AIM even in the near region, as confirmed through error analysis. The accuracy and efficiency of the proposed method are demonstrated through numerical examples drawn from several applications, and CPU times are significantly reduced by factors ranging from three to 16.      
### 37.Zero-Bias Deep Learning for Accurate Identification of Internet of Things (IoT) Devices  [ :arrow_down: ](https://arxiv.org/pdf/2009.02267.pdf)
>  The Internet of Things (IoT) provides applications and services that would otherwise not be possible. However, the open nature of IoT make it vulnerable to cybersecurity threats. Especially, identity spoofing attacks, where an adversary passively listens to existing radio communications and then mimic the identity of legitimate devices to conduct malicious activities. Existing solutions employ cryptographic signatures to verify the trustworthiness of received information. In prevalent IoT, secret keys for cryptography can potentially be disclosed and disable the verification mechanism. Non-cryptographic device verification is needed to ensure trustworthy IoT. In this paper, we propose an enhanced deep learning framework for IoT device identification using physical layer signals. Specifically, we enable our framework to report unseen IoT devices and introduce the zero-bias layer to deep neural networks to increase robustness and interpretability. We have evaluated the effectiveness of the proposed framework using real data from ADS-B (Automatic Dependent Surveillance-Broadcast), an application of IoT in aviation. The proposed framework has the potential to be applied to accurate identification of IoT devices in a variety of IoT applications and services. Codes and data are available in IEEE Dataport.      
### 38.Dynamic Standalone Drone-Mounted Small Cells  [ :arrow_down: ](https://arxiv.org/pdf/2009.02192.pdf)
>  This paper investigates the feasibility of Dynamic Horizontal Opportunistic Positioning (D-HOP) use in Drone Small Cells (DSCs), with a central analysis on the impact of antenna equipment efficiency onto the optimal DSC altitude that has been chosen in favor of maximizing coverage. We extend the common urban propagation model of an isotropic antenna to account for a directional antenna, making it dependent on the antenna's ability to fit the ideal propagation pattern. This leads us to define a closed-form expression for calculating the Rate improvement of D-HOP implementations that maintain constant coverage through antenna tilting. Assuming full knowledge of the uniformly distributed active users' locations, three D-HOP techniques were tested: in the center of the Smallest Bounding Circle (SBC); the point of Maximum Aggregated Rate (MAR); and the Center-Most Point (CMP) out of the two aforementioned. Through analytic study and simulation we infer that DSC D-HOP implementations are feasible when using electrically small and tiltable antennas. Nonetheless, it is possible to achieve average per user average rate increases of up to 20-35% in low user density scenarios, or 3-5% in user-dense scenarios, even when using efficient antennas in a DSC that has been designed for standalone coverage.      
### 39.Over-the-Air Computing for 6G -- Turning Air into a Computer  [ :arrow_down: ](https://arxiv.org/pdf/2009.02181.pdf)
>  Wireless data aggregation (WDA), referring to aggregating data distributed at devices, is a common operation in 5G-and-beyond networks for supporting applications related to distributed sensing, learning, and consensus. Conventional WDA techniques that are designed based on a separated-communication-and-computation principle encounter difficulty in meeting the stringent throughput/latency requirements imposed by emerging applications (e.g., auto-driving). To address this issue, over-the-air computation (AirComp) is being developed as a new WDA solution by integrating computation and communication. By exploiting the waveform superposition property of a multiple-access channel, AirComp turns the air into a computer for computing and communicating functions of distributed data at many devices, thereby dramatically accelerating WDA. In view of growing interests on AirComp, this article provides a timely overview of the technology by discussing basic principles, advanced techniques, applications, answering frequently asked questions, and identifying promising research opportunities.      
### 40.Collaboratively Optimizing Power Scheduling and Mitigating Congestion using Local Pricing in a Receding Horizon Market  [ :arrow_down: ](https://arxiv.org/pdf/2009.02166.pdf)
>  A distributed, hierarchical, market based approach is introduced to solve the economic dispatch problem. The approach requires only a minimal amount of information to be shared between a central market operator and the end-users. Price signals from the market operator are sent down to end-user device agents, which in turn respond with power schedules. Intermediate congestion agents make sure that local power constraints are satisfied and any potential congestion is avoided by adding local pricing differences. Our results show that in 20% of the evaluated scenarios the solutions are identical to the global optimum when perfect knowledge is available. In the other 80% the results are not significantly worse, while providing a higher level of scalability and increasing the consumer's privacy.      
### 41.Towards Musically Meaningful Explanations Using Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2009.02051.pdf)
>  Deep neural networks (DNNs) are successfully applied in a wide variety of music information retrieval (MIR) tasks. Such models are usually considered "black boxes", meaning that their predictions are not interpretable. Prior work on explainable models in MIR has generally used image processing tools to produce explanations for DNN predictions, but these are not necessarily musically meaningful, or can be listened to (which, arguably, is important in music). We propose audioLIME, a method based on Local Interpretable Model-agnostic Explanation (LIME), extended by a musical definition of locality. LIME learns locally linear models on perturbations of an example that we want to explain. Instead of extracting components of the spectrogram using image segmentation as part of the LIME pipeline, we propose using source separation. The perturbations are created by switching on/off sources which makes our explanations listenable. We first validate audioLIME on a classifier that was deliberately trained to confuse the true target with a spurious signal, and show that this can easily be detected using our method. We then show that it passes a sanity check that many available explanation methods fail. Finally, we demonstrate the general applicability of our (model-agnostic) method on a third-party music tagger.      
### 42.Markovian Traffic Equilibrium Assignment based on Network Generalized Extreme Value Model  [ :arrow_down: ](https://arxiv.org/pdf/2009.02033.pdf)
>  This study establishes a novel framework of Markovian traffic equilibrium assignment based on the network generalized extreme value (NGEV) model, which we call NGEV equilibrium assignment. The use of the NGEV model in traffic assignment has recently been proposed and enables capturing the path correlation without explicit path enumeration. However, the NGEV equilibrium assignment has never been investigated in the literature, which has limited the practical applicability of the NGEV-based models. We address this gap by providing the necessary development for the NGEV equilibrium assignment. We first show that the NGEV assignment can be formulated and solved under the same path algebra with the Markovian traffic assignment models. We then provide the equivalent optimization formulations to the NGEV equilibrium assignment, from which both primal and dual types of solution algorithms are derived. In particular, we are the first to propose an efficient algorithm based on an accelerated gradient method in the traffic assignment field. The convergence and complementary relationship of the proposed primal-dual algorithms are shown through numerical experiments.      
### 43.ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.02010.pdf)
>  DNN accelerators provide efficiency by leveraging reuse of activations/weights/outputs during the DNN computations to reduce data movement from DRAM to the chip. The reuse is captured by the accelerator's dataflow. While there has been significant prior work in exploring and comparing various dataflows, the strategy for assigning on-chip hardware resources (i.e., compute and memory) given a dataflow that can optimize for performance/energy while meeting platform constraints of area/power for DNN(s) of interest is still relatively unexplored. The design-space of choices for balancing compute and memory explodes combinatorially, as we show in this work (e.g., as large as O(10^(72)) choices for running \mobilenet), making it infeasible to do manual-tuning via exhaustive searches. It is also difficult to come up with a specific heuristic given that different DNNs and layer types exhibit different amounts of reuse. <br>In this paper, we propose an autonomous strategy called ConfuciuX to find optimized HW resource assignments for a given model and dataflow style. ConfuciuX leverages a reinforcement learning method, REINFORCE, to guide the search process, leveraging a detailed HW performance cost model within the training loop to estimate rewards. We also augment the RL approach with a genetic algorithm for further fine-tuning. ConfuciuX demonstrates the highest sample-efficiency for training compared to other techniques such as Bayesian optimization, genetic algorithm, simulated annealing, and other RL methods. It converges to the optimized hardware configuration 4.7 to 24 times faster than alternate techniques.      
### 44.Multimodal Autonomous Late Mile Delivery System Design and Application  [ :arrow_down: ](https://arxiv.org/pdf/2009.01960.pdf)
>  With the rapid increase in congestion, alternative solutions are needed to efficiently use the capacity of our existing networks. This paper focuses on exploring the emerging autonomous technologies for on-demand food delivery in congested urban cities. Three different last mile food delivery systems are proposed in this study employing aerial and ground autonomous vehicles technologies. The three proposed systems are: robot delivery system, drone delivery system and a hybrid delivery system. In the hybrid system the concept of hub-and-spoke network is explored in order to consolidate orders and reach more destinations in less time. To investigate the performance of the three proposed delivery systems, they are applied to the city of Mississauga network, in an in-house agent-based simulation in MATLAB. 18 Scenarios are tested differing in terms of demand and fleet size. The results show that the hybrid robot-drone delivery system performs the best with a fleet side of 25 robots and 15 drones and with an average preparation and delivery time less than the individual robot and drone system by 48% and 42% respectively.      
### 45.System-wide safety staffing and stabilizability of large-scale parallel server networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.01942.pdf)
>  We introduce a "system-wide safety staffing" (SWSS) parameter for multiclass multi-pool networks of any tree topology, Markovian or non-Markovian, in the Halfin--Whitt regime. The SWSS parameter can be regarded as the optimal reallocation of the capacity fluctuations (positive or negative) of order $\sqrt{n}$ when each server pool employs a square-root staffing rule. We provide an explicit form of the SWSS as a function of the system papameters, which is derived using a graph theoretic approach based on Gaussian elimination. <br>For Markovian networks, we give an equivalent characterization of the SWSS parameter via the drift parameters of the limiting diffusion. We show that if the SWSS parameter is negative, the limiting diffusion and the diffusion-scaled processes are transient under any Markov control, and cannot have a stationary distribution when this parameter is zero. If it is positive, we show that these processes are stabilizable, that is, there exists a control (scheduling policy) under which the stationary distributions of the controlled processes are tight. Thus, we have identified a necessary and sufficient condition for stabilizability of such networks. <br>We use a constant control resulting from the leaf elimination algorithm to stabilize the limiting controlled diffusion, while a family of Markov scheduling policies which are easy to compute are used to stabilize the diffusion-scaled processes. Finally, we show that under these controls the processes are exponentially ergodic and the stationary distributions have exponential tails.      
### 46.Detection of AI-Synthesized Speech Using Cepstral &amp; Bispectral Statistics  [ :arrow_down: ](https://arxiv.org/pdf/2009.01934.pdf)
>  Digital technology has made possible unimaginable applications come true. It seems exciting to have a handful of tools for easy editing and manipulation, but it raises alarming concerns that can propagate as speech clones, duplicates, or maybe deep fakes. Validating the authenticity of a speech is one of the primary problems of digital audio forensics. We propose an approach to distinguish human speech from AI synthesized speech exploiting the Bi-spectral and Cepstral analysis. Higher-order statistics have less correlation for human speech in comparison to a synthesized speech. Also, Cepstral analysis revealed a durable power component in human speech that is missing for a synthesized speech. We integrate both these analyses and propose a machine learning model to detect AI synthesized speech.      
### 47.RAMSES: A full-stack application for detecting seizures and reducing data during continuous EEG monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2009.01920.pdf)
>  Objective: Continuous EEG (cEEG) monitoring is associated with lower mortality in critically ill patients, however it is underutilized due to the difficulty of manually interpreting prolonged streams of cEEG data. Here we present a novel real-time, machine learning-based alerting and monitoring system for epilepsy and seizures (RAMSES) that dramatically reduces the amount of manual EEG review. Methods: We developed a custom data reduction algorithm using a random forest, and deployed it within an online cloud-based platform which streams data and communicates interactively with caregivers via a web interface to display algorithm results. We validate RAMSES on cEEG recordings from 77 patients undergoing routine scalp ICU EEG monitoring. Results: On subjects with seizures we achieved &gt;80% overall data reduction, while detecting a mean of 84% of seizures across all validation patients, with 19/27 patients achieving 100% seizure detection. On seizure free-patients, the majority of cEEG records, we reduced data requiring manual review by &gt;83%. Conclusion: This study validates a platform for machine-learning assisted data reduction. Significance: This work represents a meaningful step towards improving utility and decreasing cost for cEEG monitoring We also make our high-quality annotated dataset of 77 ICU cEEG recordings public for others to validate and improve upon our methods.      
### 48.Numerical differentiation of noisy data: A unifying multi-objective optimization framework  [ :arrow_down: ](https://arxiv.org/pdf/2009.01911.pdf)
>  Computing derivatives of noisy measurement data is ubiquitous in the physical, engineering, and biological sciences, and it is often a critical step in developing dynamic models or designing control. Unfortunately, the mathematical formulation of numerical differentiation is typically ill-posed, and researchers often resort to an \textit{ad hoc} process for choosing one of many computational methods and its parameters. In this work, we take a principled approach and propose a multi-objective optimization framework for choosing parameters that minimize a loss function to balance the faithfulness and smoothness of the derivative estimate. Our framework has three significant advantages. First, the task of selecting multiple parameters is reduced to choosing a single hyper-parameter. Second, where ground-truth data is unknown, we provide a heuristic for automatically selecting this hyper-parameter based on the power spectrum and temporal resolution of the data. Third, the optimal value of the hyper-parameter is consistent across different differentiation methods, thus our approach unifies vastly different numerical differentiation methods and facilitates unbiased comparison of their results. Finally, we provide an extensive open-source Python library \texttt{pynumdiff} to facilitate easy application to diverse datasets (<a class="link-external link-https" href="https://github.com/florisvb/PyNumDiff" rel="external noopener nofollow">this https URL</a>).      
### 49.Depth Completion via Inductive Fusion of Planar LIDAR and Monocular Camera  [ :arrow_down: ](https://arxiv.org/pdf/2009.01875.pdf)
>  Modern high-definition LIDAR is expensive for commercial autonomous driving vehicles and small indoor robots. An affordable solution to this problem is fusion of planar LIDAR with RGB images to provide a similar level of perception capability. Even though state-of-the-art methods provide approaches to predict depth information from limited sensor input, they are usually a simple concatenation of sparse LIDAR features and dense RGB features through an end-to-end fusion architecture. In this paper, we introduce an inductive late-fusion block which better fuses different sensor modalities inspired by a probability model. The proposed demonstration and aggregation network propagates the mixed context and depth features to the prediction network and serves as a prior knowledge of the depth completion. This late-fusion block uses the dense context features to guide the depth prediction based on demonstrations by sparse depth features. In addition to evaluating the proposed method on benchmark depth completion datasets including NYUDepthV2 and KITTI, we also test the proposed method on a simulated planar LIDAR dataset. Our method shows promising results compared to previous approaches on both the benchmark datasets and simulated dataset with various 3D densities.      
### 50.Seeing wake words: Audio-visual Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2009.01225.pdf)
>  The goal of this work is to automatically determine whether and when a word of interest is spoken by a talking face, with or without the audio. We propose a zero-shot method suitable for in the wild videos. Our key contributions are: (1) a novel convolutional architecture, KWS-Net, that uses a similarity map intermediate representation to separate the task into (i) sequence matching, and (ii) pattern detection, to decide whether the word is there and when; (2) we demonstrate that if audio is available, visual keyword spotting improves the performance both for a clean and noisy audio signal. Finally, (3) we show that our method generalises to other languages, specifically French and German, and achieves a comparable performance to English with less language specific data, by fine-tuning the network pre-trained on English. The method exceeds the performance of the previous state-of-the-art visual keyword spotting architecture when trained and tested on the same benchmark, and also that of a state-of-the-art lip reading method.      
### 51.Dual-Layer Video Encryption using RSA Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/1509.04387.pdf)
>  This paper proposes a video encryption algorithm using RSA and Pseudo Noise (PN) sequence, aimed at applications requiring sensitive video information transfers. The system is primarily designed to work with files encoded using the Audio Video Interleaved (AVI) codec, although it can be easily ported for use with Moving Picture Experts Group (MPEG) encoded files. The audio and video components of the source separately undergo two layers of encryption to ensure a reasonable level of security. Encryption of the video component involves applying the RSA algorithm followed by the PN-based encryption. Similarly, the audio component is first encrypted using PN and further subjected to encryption using the Discrete Cosine Transform. Combining these techniques, an efficient system, invulnerable to security breaches and attacks with favorable values of parameters such as encryption/decryption speed, encryption/decryption ratio and visual degradation; has been put forth. For applications requiring encryption of sensitive data wherein stringent security requirements are of prime concern, the system is found to yield negligible similarities in visual perception between the original and the encrypted video sequence. For applications wherein visual similarity is not of major concern, we limit the encryption task to a single level of encryption which is accomplished by using RSA, thereby quickening the encryption process. Although some similarity between the original and encrypted video is observed in this case, it is not enough to comprehend the happenings in the video.      
