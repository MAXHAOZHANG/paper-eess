# ArXiv eess --Fri, 18 Sep 2020
### 1.Knowledge-Assisted Deep Reinforcement Learning in 5G Scheduler Design: From Theoretical Framework to Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2009.08346.pdf)
>  In this paper, we develop a knowledge-assisted deep reinforcement learning (DRL) algorithm to design wireless schedulers in the fifth-generation (5G) cellular networks with time-sensitive traffic. Since the scheduling policy is a deterministic mapping from channel and queue states to scheduling actions, it can be optimized by using deep deterministic policy gradient (DDPG). We show that a straightforward implementation of DDPG converges slowly, has a poor quality-of-service (QoS) performance, and cannot be implemented in real-world 5G systems, which are non-stationary in general. To address these issues, we propose a theoretical DRL framework, where theoretical models from wireless communications are used to formulate a Markov decision process in DRL. To reduce the convergence time and improve the QoS of each user, we design a knowledge-assisted DDPG (K-DDPG) that exploits expert knowledge of the scheduler deign problem, such as the knowledge of the QoS, the target scheduling policy, and the importance of each training sample, determined by the approximation error of the value function and the number of packet losses. Furthermore, we develop an architecture for online training and inference, where K-DDPG initializes the scheduler off-line and then fine-tunes the scheduler online to handle the mismatch between off-line simulations and non-stationary real-world systems. Simulation results show that our approach reduces the convergence time of DDPG significantly and achieves better QoS than existing schedulers (reducing 30% ~ 50% packet losses). Experimental results show that with off-line initialization, our approach achieves better initial QoS than random initialization and the online fine-tuning converges in few minutes.      
### 2.Novel Parameter Estimation and Radar Detection Approaches for Multiple Point-like Targets: Designs and Comparisons  [ :arrow_down: ](https://arxiv.org/pdf/2009.08329.pdf)
>  In this work, we develop and compare two innovative strategies for parameter estimation and radar detection of multiple point-like targets. The first strategy, which appears here for the first time, jointly exploits the maximum likelihood approach and Bayesian learning to estimate targets' parameters including their positions in terms of range bins. The second strategy relies on the intuition that for high signal-to-interference plus-noise ratio values, the energy of data containing target components projected onto the nominal steering direction should be higher than the energy of data affected by interference only. The adaptivity with respect to the interference covariance matrix is also considered exploiting a training data set collected in the proximity of the window under test. Finally, another important innovation aspect concerns the adaptive estimation of the unknown number of targets by means of the model order selection rules.      
### 3.Review: Deep Learning in Electron Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.08328.pdf)
>  Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.      
### 4.Effect Of Weather Conditions On FSO Link  [ :arrow_down: ](https://arxiv.org/pdf/2009.08317.pdf)
>  Free Space Optics (FSO) is a developing technology for Line of Sight communication that uses light propagation in free space that provides various advantages like high bandwidth, high data rate, ease of installation, free licensing and secure communication. Thus, FSO is a developing technology that can be used in numerous applications for Line of Sight Communication. But the diverse effects like attenuation on FSO communication link due to environmental factors and weather conditions like fog, rain, dust, sand storms, clouds, temperature and the other factors like range, effects of physical obstructions are an essential topic for study which is discussed in this paper. We have done the simulation for the effects of fog and rain on the FSO communication link in Opti system software [1]. This is submitted in leu of FOC assignment at Nirma University.      
### 5.High Performance Low Complexity Multitarget Tracking Filter for a Array of Non-directional Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2009.08310.pdf)
>  This paper develops an accurate, efficient filter (called the `TT filter') for tracking multiple targets using a spatially-distributed network of amplitude sensors that estimate distance but not direction. Several innovations are included in the algorithm that increase accuracy and reduce complexity. For initial target acquisition once tracking begins, a constrained Hessian search is used to find the maximum likelihood (ML) target vector, based on the measurement model and a Gaussian approximation of the prior. The Hessian at the ML vector is used to give an initial approximation of the negative log likelihood for the target vector distribution: corrections are applied if the Hessian is not positive definite due to the near-far problem. Further corrections are made by applying a transformation that matches the known nonlinearity introduced by distance-only sensors. A set of integration points is constructed using this information, which are used to estimate the mean and moments of the target vector distribution. Results show that the TT filter gives superior accuracy and lower complexity than previous alternatives such as Kalman-based or particle filters.      
### 6.Improving in-home appliance identification using fuzzy-neighbors-preserving analysis based QR-decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2009.08282.pdf)
>  This paper proposes a new appliance identification scheme by introducing a novel approach for extracting highly discriminative characteristic sets that can considerably distinguish between various appliance footprints. In this context, a precise and powerful characteristic projection technique depending on fuzzy-neighbors-preserving analysis based QR-decomposition (FNPA-QR) is applied on the extracted energy consumption time-domain features. The FNPA-QR aims to diminish the distance among the between class features and increase the gap among features of dissimilar categories. Following, a novel bagging decision tree (BDT) classifier is also designed to further improve the classification accuracy. The proposed technique is then validated on three appliance energy consumption datasets, which are collected at both low and high frequency. The practical results obtained point out the outstanding classification rate of the time-domain based FNPA-QR and BDT.      
### 7.Coordinated PV re-phasing: a novel method to maximize renewable energy integration in LV networks by mitigating network unbalances  [ :arrow_down: ](https://arxiv.org/pdf/2009.08260.pdf)
>  As combating climate change has become a top priority and as many countries are taking steps to make their power generation sustainable, there is a marked increase in the use of renewable energy sources (RESs) for electricity generation. Among these RESs, solar photovoltaics (PV) is one of the most popular sources of energy connected to LV distribution networks. With the greater integration of solar PV into LV distribution networks, utility providers impose caps to solar penetration in order to operate their network safely and within acceptable norms. One parameter that restricts solar PV penetration is unbalances created by loads and single-phase rooftop schemes connected to LV distribution grids. In this paper, a novel method is proposed to mitigate voltage unbalance in LV distribution grids by optimally re-phasing grid-connected rooftop PV systems. A modified version of the discrete bacterial foraging optimization algorithm (DBFOA) is introduced as the optimization technique to minimize the overall voltage unbalance of the network as the objective function, subjected to various network and operating parameters. The impact of utilizing the proposed PV re-phasing technique as opposed to a fixed phase configuration are compared based on overall voltage unbalance, which was observed hourly throughout the day. The case studies show that the proposed approach can significantly mitigate the overall voltage unbalance during the daytime and can facilitate to increase the usable PV capacity of the considered network by 77%.      
### 8.A Two-stage Stochastic Programming DSO Framework for Comprehensive Market Participation of DER Aggregators under Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2009.08248.pdf)
>  In this paper, a distribution system operator (DSO) framework is proposed for comprehensive retail and wholesale markets participation of distributed energy resource (DER) aggregators under uncertainty based on two-stage stochastic programming. Different kinds of DER aggregators including energy storage aggregators (ESAGs), demand response aggregators (DRAGs), electric vehicle (EV) aggregating charging stations (EVCSs), dispatchable distributed generation (DDG) aggregators (DDGAGs), and renewable energy aggregators (REAGs) are modeled. Distribution network operation constraints are considered using a linearized power flow. The problem is modeled using mixed-integer linear programming (MILP) which can be solved by using commercial solvers. Case studies are conducted to investigate the performance of the proposed DSO framework.      
### 9.Single Frame Deblurring with Laplacian Filters  [ :arrow_down: ](https://arxiv.org/pdf/2009.08182.pdf)
>  Blind single image deblurring has been a challenge over many decades due to the ill-posed nature of the problem. In this paper, we propose a single-frame blind deblurring solution with the aid of Laplacian filters. Utilized Residual Dense Network has proven its strengths in superresolution task, thus we selected it as a baseline architecture. We evaluated the proposed solution with state-of-art DNN methods on a benchmark dataset. The proposed method shows significant improvement in image quality measured objectively and subjectively.      
### 10.Online Speaker Diarization with Relation Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.08162.pdf)
>  In this paper, we propose an online speaker diarization system based on Relation Network, named RenoSD. Unlike conventional diariztion systems which consist of several independently-optimized modules, RenoSD implements voice-activity-detection (VAD), embedding extraction, and speaker identity association using a single deep neural network. The most striking feature of RenoSD is that it adopts a meta-learning strategy for speaker identity association. In particular, the relation network learns to learn a deep distance metric in a data-driven way and it can determine through a simple forward pass whether two given segments belong to the same speaker. As such, RenoSD can be performed in an online manner with low latency. Experimental results on AMI and CALLHOME datasets show that the proposed RenoSD system achieves consistent improvements over the state-of-the-art x-vector baseline. Compared with an existing online diarization system named UIS-RNN, RenoSD achieves a better performance using much fewer training data and at a lower time complexity.      
### 11.Logical Signal Processing: a Fourier Analysis of Temporal Logic  [ :arrow_down: ](https://arxiv.org/pdf/2009.08090.pdf)
>  What is the frequency content of temporal logic formulas? That is, when we monitor a signal against a formula, which frequency bands of the signal are relevant to the logic and should be preserved, and which can be safely discarded? This question is relevant whenever signals are filtered or compressed before being monitored, which is almost always the case for analog signals. To answer this question, we focus on monitors that measure the robustness of a signal relative to a specification in Signal Temporal Logic. We prove that robustness monitors can be modeled using Volterra series. We then study the Fourier transforms of these Volterra representations, and provide a method to derive the Fourier transforms of entire formulas. We also make explicit the measurement process in temporal logic and re-define it on the basis of distributions to make it compatible with measurements in signal processing. Experiments illustrate these results. Beyond compression, this work enables the integration of temporal logic monitoring into common signal processing toolchains as just another signal processing operation, and enables a common formalism to study both logical and non-logical operations in the frequency domain, which we refer to as Logical Signal Processing.      
### 12.Resource Allocation and Dithering of Bayesian Parameter Estimation Using Mixed-Resolution Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.08081.pdf)
>  Quantization of signals is an integral part of modern signal processing applications, such as sensing, communication, and inference. While signal quantization provides many physical advantages, it usually degrades the subsequent estimation performance that is based on quantized data. In order to maintain physical constraints and simultaneously bring substantial performance gain, in this work we consider systems with mixed-resolution, 1-bit quantized and continuous-valued, data. First, we describe the linear minimum mean-squared error (LMMSE) estimator and its associated mean-squared error (MSE) for the general mixed-resolution model. However, the MSE of the LMMSE requires matrix inversion in which the number of measurements defines the matrix dimensions and thus, is not a tractable tool for optimization and system design. Therefore, we present the linear Gaussian orthonormal (LGO) measurement model and derive a closed-form analytic expression for the MSE of the LMMSE estimator under this model. In addition, we present two common special cases of the LGO model: 1) scalar parameter estimation and 2) channel estimation in mixed-ADC multiple-input multiple-output (MIMO) communication systems. We then solve the resource allocation optimization problem of the LGO model with the proposed tractable form of the MSE as an objective function and under a power constraint using a one-dimensional search. Moreover, we present the concept of dithering for mixed-resolution models and optimize the dithering noise as part of the resource allocation optimization problem for two dithering schemes: 1) adding noise only to the quantized measurements and 2) adding noise to both measurement types. Finally, we present simulations that demonstrate the advantages of using mixed-resolution measurements and the possible improvement introduced with dithering and resource allocation.      
### 13.Utterance-level Intent Recognition from Keywords  [ :arrow_down: ](https://arxiv.org/pdf/2009.08064.pdf)
>  This paper focuses on wake on intent (WOI) techniques for platforms with limited compute and memory. Our approach of utterance-level intent classification is based on a sequence of keywords in the utterance instead of a single fixed key phrase. The keyword sequence is transformed into four types of input features, namely acoustics, phones, word2vec and speech2vec for individual intent learning and then fused decision making. If a wake intent is detected, it will trigger the power-costly ASR afterwards. The system is trained and tested on a newly collected internal dataset in Intel called AMIE, which will be reported in this paper for the first time. It is demonstrated that our novel technique with the representation of the key-phrases successfully achieved a noise robust intent classification in different domains including in-car human-machine communications. The wake on intent system will be low-power and low-complexity, which makes it suitable for always on operations in real life hardware-based applications.      
### 14.Empirical Fourier Decomposition: An Accurate Adaptive Signal Decomposition Method  [ :arrow_down: ](https://arxiv.org/pdf/2009.08047.pdf)
>  Signal decomposition is an effective tool to assist the identification of modal information in time-domain signals. Two signal decomposition methods, including the empirical wavelet transform (EWT) and Fourier decomposition method (FDM), have been developed based on Fourier theory. However, the EWT can suffer from a mode mixing problem for time-domain signals with closely-spaced modes and decomposition results by FDM can suffer from an inconsistency problem. An accurate adaptive signal decomposition method is proposed to solve the problems in this work; it is called the empirical Fourier decomposition (EFD). The proposed EFD combines the uses of an improved frequency spectrum segmentation technique and an ideal filter bank. The segmentation technique can solve the inconsistency problem by predefining the number of modes in a time-domain signal to be decomposed and filter functions in the ideal filter bank have no transition phases, which can solve the mode mixing problem. Numerical investigations are conducted to study the accuracy of the EFD. It is shown that the EFD can yield accurate and consistent decomposition results for time-domain signals with multiple non-stationary modes and those with closely-spaced modes, compared with decomposition results by the EWT, FDM, variational mode decomposition, and empirical mode decomposition. It is also shown that the EFD can yield accurate time-frequency representation results and has the highest computational efficiency among the compared methods.      
### 15.Noise-Aware Merging of High Dynamic Range Image Stacks without Camera Calibration  [ :arrow_down: ](https://arxiv.org/pdf/2009.07975.pdf)
>  A near-optimal reconstruction of the radiance of a High Dynamic Range scene from an exposure stack can be obtained by modeling the camera noise distribution. The latent radiance is then estimated using Maximum Likelihood Estimation. But this requires a well-calibrated noise model of the camera, which is difficult to obtain in practice. We show that an unbiased estimation of comparable variance can be obtained with a simpler Poisson noise estimator, which does not require the knowledge of camera-specific noise parameters. We demonstrate this empirically for four different cameras, ranging from a smartphone camera to a full-frame mirrorless camera. Our experimental results are consistent for simulated as well as real images, and across different camera settings.      
### 16.Detecting and monitoring long-term landslides in urbanized areas with nighttime light data and multi-seasonal Landsat imagery across Taiwan from 1998 to 2017  [ :arrow_down: ](https://arxiv.org/pdf/2009.07954.pdf)
>  Monitoring long-term landslide activity is important for risk assessment and land management. Despite the widespread use of open-access 30m Landsat imagery, their utility for landslide detection is often limited when separating landslides from other anthropogenic disturbances. Here, we produce landslide maps retrospectively from 1998 to 2017 for landslide-prone and highly populated Taiwan (35,874 km2). To improve classification accuracy of landslides, we integrate nighttime light imagery from the Defense Meteorological Satellite Program (DMSP) and the Visible Infrared Imaging Radiometer Suite (VIIRS), with multi-seasonal daytime optical Landsat time-series, and digital elevation data from the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER). We employed a non-parametric machine-learning classifier, random forest, to classify the satellite imagery. The classifier was trained with data from three years (2005, 2010, and 2015), and was validated with an independent reference sample from twelve years. Our results demonstrated that combining nighttime light data and multi-seasonal imagery significantly improved the classification (p&lt;0.001), compared to conventional methods based on single-season optical imagery. The results confirmed that the developed classification model enabled mapping of landslides across Taiwan over a long period with annual overall accuracy varying between 96% and 97%, user's and producer's accuracies between 73% and 86%. Spatiotemporal analysis of the landslide inventories from 1998 to 2017 revealed different temporal patterns of landslide activities, showing those areas where landslides were persistent and other areas where landslides tended to reoccur after vegetation regrowth. In sum, we provide a robust method to detect long-term landslide activities based on freely available satellite imagery, which can be applied elsewhere.      
### 17.Control coordination between DFIG-based wind turbines and synchronous generators for optimal primary frequency response  [ :arrow_down: ](https://arxiv.org/pdf/2009.07890.pdf)
>  This paper proposes a novel coordinating mechanism between synchronous generators (SGs) and wind turbines (WTs) based on doubly-fed induction generators (DFIGs) for enhanced primary frequency regulation. WTs are urged to participate on frequency regulation, specially if wind power penetration keeps increasing. WTs control support is possible, but it is transient due to the WTs lack of energy storage. This drawback can result in either a further delayed response from the governors of SGs or further frequency decay when WTs support is over. The proposed coordination attempt to tackle this issue. An artificial neural network (ANN) is used to obtain an optimal coordination signal to improve frequency response. As a proof of concept, the proposed coordination is tested on a 9-bus test system that includes a wind farm with 5 WTs. Simulation results show that frequency nadir is reduced in about 22\% and rates of change of the system frequency (RoCoF) in about 29.5\%. Further work is needed to validate this concept in large-scale systems, but the development and results obtained so far are promising to strengthen power systems.      
### 18.Image Separation with Side Information: A Connected Auto-Encoders Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.07889.pdf)
>  X-radiography (X-ray imaging) is a widely used imaging technique in art investigation. It can provide information about the condition of a painting as well as insights into an artist's techniques and working methods, often revealing hidden information invisible to the naked eye. In this paper, we deal with the problem of separating mixed X-ray images originating from the radiography of double-sided paintings. Using the visible color images (RGB images) from each side of the painting, we propose a new Neural Network architecture, based upon 'connected' auto-encoders, designed to separate the mixed X-ray image into two simulated X-ray images corresponding to each side. In this proposed architecture, the convolutional auto encoders extract features from the RGB images. These features are then used to (1) reproduce both of the original RGB images, (2) reconstruct the hypothetical separated X-ray images, and (3) regenerate the mixed X-ray image. The algorithm operates in a totally self-supervised fashion without requiring a sample set that contains both the mixed X-ray images and the separated ones. The methodology was tested on images from the double-sided wing panels of the \textsl{Ghent Altarpiece}, painted in 1432 by the brothers Hubert and Jan van Eyck. These tests show that the proposed approach outperforms other state-of-the-art X-ray image separation methods for art investigation applications.      
### 19.Energy and Flow Effects of Optimal Automated Driving in Mixed Traffic: Vehicle-in-the-Loop Experimental Results  [ :arrow_down: ](https://arxiv.org/pdf/2009.07872.pdf)
>  This paper experimentally demonstrates the effectiveness of an anticipative car-following algorithm in reducing energy use of gasoline engine and electric Connected and Automated Vehicles (CAV), without sacrificing safety and traffic flow. We propose a Vehicle-in-the-Loop (VIL) testing environment in which experimental CAVs driven on a track interact with surrounding virtual traffic in real-time. We explore the energy savings when following city and highway drive cycles, as well as in emergent highway traffic created from microsimulations. Model predictive control handles high level velocity planning and benefits from communicated intentions of a preceding CAV or estimated probable motion of a preceding human driven vehicle. A combination of classical feedback control and data-driven nonlinear feedforward control of pedals achieve acceleration tracking at the low level. The controllers are implemented in ROS and energy is measured via calibrated OBD-II readings. We report up to 30% improved energy economy compared to realistically calibrated human driver car-following without sacrificing following headway.      
### 20.Elastica: A compliant mechanics environment for soft robotic control  [ :arrow_down: ](https://arxiv.org/pdf/2009.08422.pdf)
>  Soft robots are notoriously hard to control. This is partly due to the scarcity of models able to capture their complex continuum mechanics, resulting in a lack of control methodologies that take full advantage of body compliance. Currently available simulation methods are either too computational demanding or overly simplistic in their physical assumptions, leading to a paucity of available simulation resources for developing such control schemes. To address this, we introduce Elastica, a free, open-source simulation environment for soft, slender rods that can bend, twist, shear and stretch. We demonstrate how Elastica can be coupled with five state-of-the-art reinforcement learning algorithms to successfully control a soft, compliant robotic arm and complete increasingly challenging tasks.      
### 21.Microtubule Tracking in Electron Microscopy Volumes  [ :arrow_down: ](https://arxiv.org/pdf/2009.08371.pdf)
>  We present a method for microtubule tracking in electron microscopy volumes. Our method first identifies a sparse set of voxels that likely belong to microtubules. Similar to prior work, we then enumerate potential edges between these voxels, which we represent in a candidate graph. Tracks of microtubules are found by selecting nodes and edges in the candidate graph by solving a constrained optimization problem incorporating biological priors on microtubule structure. For this, we present a novel integer linear programming formulation, which results in speed-ups of three orders of magnitude and an increase of 53% in accuracy compared to prior art (evaluated on three 1.2 x 4 x 4$\mu$m volumes of Drosophila neural tissue). We also propose a scheme to solve the optimization problem in a block-wise fashion, which allows distributed tracking and is necessary to process very large electron microscopy volumes. Finally, we release a benchmark dataset for microtubule tracking, here used for training, testing and validation, consisting of eight 30 x 1000 x 1000 voxel blocks (1.2 x 4 x 4$\mu$m) of densely annotated microtubules in the CREMI data set (<a class="link-external link-https" href="https://github.com/nilsec/micron" rel="external noopener nofollow">this https URL</a>).      
### 22.Face Mask Detection using Transfer Learning of InceptionV3  [ :arrow_down: ](https://arxiv.org/pdf/2009.08369.pdf)
>  The world is facing a huge health crisis due to the rapid transmission of coronavirus (COVID-19). Several guidelines were issued by the World Health Organization (WHO) for protection against the spread of coronavirus. According to WHO, the most effective preventive measure against COVID-19 is wearing a mask in public places and crowded areas. It is very difficult to monitor people manually in these areas. In this paper, a transfer learning model is proposed to automate the process of identifying the people who are not wearing mask. The proposed model is built by fine-tuning the pre-trained state-of-the-art deep learning model, InceptionV3. The proposed model is trained and tested on the Simulated Masked Face Dataset (SMFD). Image augmentation technique is adopted to address the limited availability of data for better training and testing of the model. The model outperformed the other recently proposed approaches by achieving an accuracy of 99.9% during training and 100% during testing.      
### 23.Histopathology for Mohs Micrographic Surgery with Photoacoustic Remote Sensing Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.08337.pdf)
>  Mohs micrographic surgery (MMS) is a precise technique where layers of tissue are resected and examined with intraoperative histopathology to minimize the removal of normal tissue while completely excising the cancer. To achieve intraoperative pathology, the tissue is frozen, sectioned and stained over a 20- to 60-minute period, then analyzed by the MMS surgeon. Surgery is continued one layer at a time until no cancerous cells remain, meaning MMS can take several hours to complete. Ideally, it would be desirable to circumvent or augment frozen sectioning methods and directly visualize subcellular morphology on the unprocessed excised tissues. Employing photoacoustic remote sensing (PARS) microscopy, we present a non-contact label-free reflection-mode method of performing such visualizations in frozen sections of human skin. PARS leverages endogenous optical absorption contrast within cell nuclei to provide visualizations reminiscent of histochemical staining techniques. Here, we demonstrate the ability of PARS microscopy to provide large grossing scans (&gt;1 cm^2, sufficient to visualize entire MMS sections) and regional scans with subcellular lateral resolution (400 +- 150 nm).      
### 24.Unsupervised Image Classification Through Time-Multiplexed Photonic Multi-Layer Spiking Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.08309.pdf)
>  We present results of a deep photonic spiking convolutional neural network, based on two-section VCSELs, targeting image classification. Training is based on unsupervised spike-timing dependent plasticity, whereas neuron time-multiplexing and ultra-fast response are exploited towards a a reduction of the physical neuron count by 90%      
### 25.Adaptive Neuro-Fuzzy Inference System and a Multilayer Perceptron Model Trained with Grey Wolf Optimizer for Predicting Solar Diffuse Fraction  [ :arrow_down: ](https://arxiv.org/pdf/2009.08275.pdf)
>  The accurate prediction of the solar Diffuse Fraction (DF), sometimes called the Diffuse Ratio, is an important topic for solar energy research. In the present study, the current state of Diffuse Irradiance research is discussed and then three robust, Machine Learning (ML) models, are examined using a large dataset (almost 8 years) of hourly readings from Almeria, Spain. The ML models used herein, are a hybrid Adaptive Network-based Fuzzy Inference System (ANFIS), a single Multi-Layer Perceptron (MLP) and a hybrid Multi-Layer Perceptron-Grey Wolf Optimizer (MLP-GWO). These models were evaluated for their predictive precision, using various Solar and Diffuse Fraction (DF) irradiance data, from Spain. The results were then evaluated using two frequently used evaluation criteria, the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE). The results showed that the MLP-GWO model, followed by the ANFIS model, provided a higher performance, in both the training and the testing procedures.      
### 26.Deploying machine learning to assist digital humanitarians: making image annotation in OpenStreetMap more efficient  [ :arrow_down: ](https://arxiv.org/pdf/2009.08188.pdf)
>  Locating populations in rural areas of developing countries has attracted the attention of humanitarian mapping projects since it is important to plan actions that affect vulnerable areas. Recent efforts have tackled this problem as the detection of buildings in aerial images. However, the quality and the amount of rural building annotated data in open mapping services like OpenStreetMap (OSM) is not sufficient for training accurate models for such detection. Although these methods have the potential of aiding in the update of rural building information, they are not accurate enough to automatically update the rural building maps. In this paper, we explore a human-computer interaction approach and propose an interactive method to support and optimize the work of volunteers in OSM. The user is asked to verify/correct the annotation of selected tiles during several iterations and therefore improving the model with the new annotated data. The experimental results, with simulated and real user annotation corrections, show that the proposed method greatly reduces the amount of data that the volunteers of OSM need to verify/correct. The proposed methodology could benefit humanitarian mapping projects, not only by making more efficient the process of annotation but also by improving the engagement of volunteers.      
### 27.An Algorithm to Attack Neural Network Encoder-based Out-Of-Distribution Sample Detector  [ :arrow_down: ](https://arxiv.org/pdf/2009.08016.pdf)
>  Deep neural network (DNN), especially convolutional neural network, has achieved superior performance on image classification tasks. However, such performance is only guaranteed if the input to a trained model is similar to the training samples, i.e., the input follows the probability distribution of the training set. Out-Of-Distribution (OOD) samples do not follow the distribution of training set, and therefore the predicted class labels on OOD samples become meaningless. Classification-based methods have been proposed for OOD detection; however, in this study we show that this type of method is theoretically ineffective and practically breakable because of dimensionality reduction in the model. We also show that Glow likelihood-based OOD detection is ineffective as well. Our analysis is demonstrated on five open datasets, including a COVID-19 CT dataset. At last, we present a simple theoretical solution with guaranteed performance for OOD detection.      
### 28.Temporally Guided Music-to-Body-Movement Generation  [ :arrow_down: ](https://arxiv.org/pdf/2009.08015.pdf)
>  This paper presents a neural network model to generate virtual violinist's 3-D skeleton movements from music audio. Improved from the conventional recurrent neural network models for generating 2-D skeleton data in previous works, the proposed model incorporates an encoder-decoder architecture, as well as the self-attention mechanism to model the complicated dynamics in body movement sequences. To facilitate the optimization of self-attention model, beat tracking is applied to determine effective sizes and boundaries of the training examples. The decoder is accompanied with a refining network and a bowing attack inference mechanism to emphasize the right-hand behavior and bowing attack timing. Both objective and subjective evaluations reveal that the proposed model outperforms the state-of-the-art methods. To the best of our knowledge, this work represents the first attempt to generate 3-D violinists' body movements considering key features in musical body movement.      
### 29.Arbitrary Video Style Transfer via Multi-Channel Correlation  [ :arrow_down: ](https://arxiv.org/pdf/2009.08003.pdf)
>  Video style transfer is getting more attention in AI community for its numerous applications such as augmented reality and animation productions. Compared with traditional image style transfer, performing this task on video presents new challenges: how to effectively generate satisfactory stylized results for any specified style, and maintain temporal coherence across frames at the same time. Towards this end, we propose Multi-Channel Correction network (MCCNet), which can be trained to fuse the exemplar style features and input content features for efficient style transfer while naturally maintaining the coherence of input videos. Specifically, MCCNet works directly on the feature space of style and content domain where it learns to rearrange and fuse style features based on their similarity with content features. The outputs generated by MCC are features containing the desired style patterns which can further be decoded into images with vivid style textures. Moreover, MCCNet is also designed to explicitly align the features to input which ensures the output maintains the content structures as well as the temporal continuity. To further improve the performance of MCCNet under complex light conditions, we also introduce the illumination loss during training. Qualitative and quantitative evaluations demonstrate that MCCNet performs well in both arbitrary video and image style transfer tasks.      
### 30.Efficiency-optimized design of PCB-integrated magnetorquers for CubeSats  [ :arrow_down: ](https://arxiv.org/pdf/2009.07981.pdf)
>  CubeSats are miniature satellites used to carry experimental payloads into orbit, where it is often critical to precisely control their spatial orientation. One way to do this is through the use of magnetorquers, which can be integrated into PCBs. This technique saves considerable space and capital when compared with more common torque-rod magnetorquer systems. Here we derive a method of analyzing different PCB-integrated magnetorquer geometries, parametrizing them such that the moment and efficiency are maximized. Furthermore, by modulating the trace width, the trace number, and other electrical characteristics of the magnetorquer coil, this paper optimizes the generated magnetic moment. Both constant voltage and constant current sources are analyzed as inputs. These optimizations are then simulated in COMSOL for multiple geometries, and it is found that there exists an optimal geometry, given a specified power dissipation. Simulations verify the general trend and maxima of these derivations, barring small, consistent re-scaling in the magnitude of the coil resistance. It is also found that these PCB-magnetorquers provide a sufficient alternative to commercial coil magnetorquers - particularly in volume-restricted configurations. Optimizations for common PCB-implementable geometries on small satellites are tabulated in the Appendix.      
