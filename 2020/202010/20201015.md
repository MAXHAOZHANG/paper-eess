# ArXiv eess --Thu, 15 Oct 2020
### 1.Spatial-Slepian Transform on the Sphere  [ :arrow_down: ](https://arxiv.org/pdf/2010.07266.pdf)
>  We present spatial-Slepian transform~(SST) for the representation of signals on the sphere to support localized signal analysis. We use well-optimally concentrated Slepian functions, obtained by solving the Slepian spatial-spectral concentration problem of finding bandlimited and spatially optimally concentrated functions on the sphere, to formulate the proposed transform and obtain the joint spatial-Slepian domain representation of the signal. Due to the optimal energy concentration of the Slepian functions in the spatial domain, the proposed spatial-Slepian transform allows us to probe spatially localized content of the signal. Furthermore, we present an inverse transform to recover the signal from the spatial-Slepian coefficients, and show that well-optimally concentrated rotated Slepian functions form a tight frame on the sphere. We develop an algorithm for the fast computation of the spatial-Slepian transform and carry out computational complexity analysis. We present the formulation of SST for zonal Slepian functions, which are spatially optimally concentrated in the polar cap~(axisymmetric) region, and provide an illustration using the Earth topography map. To demonstrate the utility of the proposed transform, we carry out localized variation analysis; employing SST for detecting hidden localized variations in the signal.      
### 2.Joint SO(3)-Spectral Domain Filtering of Spherical Signals in the Presence of Anisotropic Noise  [ :arrow_down: ](https://arxiv.org/pdf/2010.07260.pdf)
>  We present a joint SO(3)-spectral domain filtering framework using directional spatially localized spherical harmonic transform (DSLSHT), for the estimation and enhancement of random anisotropic signals on the sphere contaminated by random anisotropic noise. We design an optimal filter for filtering the DSLSHT representation of the noise-contaminated signal in the joint SO(3)-spectral domain. The filter is optimal in the sense that the filtered representation in the joint domain is the minimum mean square error estimate of the DSLSHT representation of the underlying (noise-free) source signal. We also derive a least-square solution for the estimate of the source signal from the filtered representation in the joint domain. We demonstrate the capability of the proposed filtering framework using the Earth topography map in the presence of anisotropic, zero-mean, uncorrelated Gaussian noise, and compare its performance with the joint spatial-spectral domain filtering framework.      
### 3.Robust path-following control design of heavy vehicles based on multiobjective evolutionary optimization  [ :arrow_down: ](https://arxiv.org/pdf/2010.07255.pdf)
>  The ability to deal with systems parametric uncertainties is an essential issue for heavy self-driving vehicles in unconfined environments. In this sense, robust controllers prove to be efficient for autonomous navigation. However, uncertainty matrices for this class of systems are usually defined by algebraic methods which demand prior knowledge of the system dynamics. In this case, the control system designer depends, on the quality of the uncertain model to obtain an optimal control performance. This work proposes a robust recursive controller designed via multiobjective optimization to overcome these shortcomings. Furthermore, a local search approach for multiobjective optimization problems is presented. The proposed method applies to any multiobjective evolutionary algorithm already established in the literature. The results presented show that this combination of model-based controller and machine learning improves the effectiveness of the system in terms of robustness, stability and smoothness.      
### 4.Handling plant-model mismatch in Koopman Lyapunov-based model predictive control via offset-free control framework  [ :arrow_down: ](https://arxiv.org/pdf/2010.07239.pdf)
>  Koopman operator theory enables a global linear representation of a given nonlinear dynamical system by transforming the nonlinear dynamics into a higher dimensional observable function space where the evolution of observable functions is governed by an infinite-dimensional linear operator. For practical application of Koopman operator theory, various data-driven methods have been developed to derive lifted state-space models via approximation to the Koopman operator. Based on approximate models, several Koopman-based model predictive control (KMPC) schemes have been proposed. However, since a finite-dimensional approximation to the infinite-dimensional Koopman operator cannot fully represent a nonlinear dynamical system, plant-model mismatch inherently exists in these KMPC schemes and negatively influences the performance of control systems. In this work, we present offset-free Koopman Lyapunov-based model predictive control (KLMPC) framework that addresses the inherent plant-model mismatch in KMPC schemes using an offset-free control framework based on a disturbance estimator approach and ensures feasibility and stability of the control system by applying Lyapunov constraints to the optimal control problem. The zero steady-state offset condition of the developed framework is mathematically examined. The effectiveness of the developed framework is also demonstrated by comparing the closed-loop results of the proposed offset-free KLMPC and the nominal KLMPC.      
### 5.Fader Networks for domain adaptation on fMRI: ABIDE-II study  [ :arrow_down: ](https://arxiv.org/pdf/2010.07233.pdf)
>  ABIDE is the largest open-source autism spectrum disorder database with both fMRI data and full phenotype description. These data were extensively studied based on functional connectivity analysis as well as with deep learning on raw data, with top models accuracy close to 75\% for separate scanning sites. Yet there is still a problem of models transferability between different scanning sites within ABIDE. In the current paper, we for the first time perform domain adaptation for brain pathology classification problem on raw neuroimaging data. We use 3D convolutional autoencoders to build the domain irrelevant latent space image representation and demonstrate this method to outperform existing approaches on ABIDE data.      
### 6.Domain Shift in Computer Vision models for MRI data analysis: An Overview  [ :arrow_down: ](https://arxiv.org/pdf/2010.07222.pdf)
>  Machine learning and computer vision methods are showing good performance in medical imagery analysis. Yetonly a few applications are now in clinical use and one of the reasons for that is poor transferability of themodels to data from different sources or acquisition domains. Development of new methods and algorithms forthe transfer of training and adaptation of the domain in multi-modal medical imaging data is crucial for thedevelopment of accurate models and their use in clinics. In present work, we overview methods used to tackle thedomain shift problem in machine learning and computer vision. The algorithms discussed in this survey includeadvanced data processing, model architecture enhancing and featured training, as well as predicting in domaininvariant latent space. The application of the autoencoding neural networks and their domain-invariant variationsare heavily discussed in a survey. We observe the latest methods applied to the magnetic resonance imaging(MRI) data analysis and conclude on their performance as well as propose directions for further research.      
### 7.Physical Layer Security for V2I Communications: Reflecting Surfaces Vs. Relaying  [ :arrow_down: ](https://arxiv.org/pdf/2010.07216.pdf)
>  Wireless vehicular network (WVN) is exponentially gaining attention from industries and researchers since it is the Keystone of intelligent transportation systems (ITS). Despite the sophisticated features and services that it can offer, it is susceptible to networking attacks such as eavesdropping threats where the confidential transmitted signal could be overheard by a malicious entity. In this paper, we intend to study the physical layer security (PLS) where we consider the eavesdropping attack for vehicle-to-infrastructure (V2I) communications. We analyze the average secrecy capacity, under different scenarios by comparing the performances of employing the decode-and-forward (D relay, the amplify-and-forward fixed gain (AFFG) relay, and the intelligent reflecting surface (IRS). Actually, this comparison investigates the efficiency of IRS comparing to the traditional relaying systems, since it was introduced as a novel paradigm in wireless technology with highly promising potential, especially in 5G and 6G.      
### 8.Riemannian geometry-based decoding of the directional focus of auditory attention using EEG  [ :arrow_down: ](https://arxiv.org/pdf/2010.07171.pdf)
>  Auditory attention decoding (AAD) algorithms decode the auditory attention from electroencephalography (EEG) signals which capture the neural activity of the listener. Such AAD methods are believed to be an important ingredient towards so-called neuro-steered assistive hearing devices. For example, traditional AAD decoders allow to detect to which of multiple speakers a listener is attending to by reconstructing the amplitude envelope of the attended speech signal from the EEG signals. Recently, an alternative paradigm to this stimulus reconstruction approach was proposed, in which the directional focus of auditory attention is determined instead, solely based on the EEG, using common spatial pattern filters (CSP). Here, we propose Riemannian geometry-based classification (RGC) as an alternative for this CSP approach, in which the covariance matrix of a new EEG segment is directly classified while taking its Riemannian structure into account. While the proposed RGC method performs similarly to the CSP method for short decision lengths (i.e., the amount of EEG samples used to make a decision), we show that it significantly outperforms it for longer decision window lengths.      
### 9.Wireless Channel Type Recognition via Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.07052.pdf)
>  In this work, we propose two novel deep-learning-based algorithms to solve the wireless channel type (WCT) determination problem. Specifically, the WCT determination problem is recast as a classification problem in deep learning due to their similarities, where a deep neural network (DNN) is trained offline with a diversity of WCTs usually encountered in wireless communications, which is then utilized to perform online WCT determination. In the first algorithm, one WCT is regarded as a single task, while in the second scheme, one WCT is jointly characterized by several independent features, each of which is regarded as a task and is classified respectively by training a DNN in a multi-task-learning manner, and the final WCT is identified by the combination of those channel features. Simulation results show that the proposed algorithms can classify various WCTs instantaneously with high accuracy.      
### 10.A Data Driven End-to-end Approach for In-the-wild Monitoring of Eating Behavior Using Smartwatches  [ :arrow_down: ](https://arxiv.org/pdf/2010.07051.pdf)
>  The increased worldwide prevalence of obesity has sparked the interest of the scientific community towards tools that objectively and automatically monitor eating behavior. Despite the study of obesity being in the spotlight, such tools can also be used to study eating disorders (e.g. anorexia nervosa) or provide a personalized monitoring platform for patients or athletes. This paper presents a complete framework towards the automated i) modeling of in-meal eating behavior and ii) temporal localization of meals, from raw inertial data collected in-the-wild using commercially available smartwatches. Initially, we present an end-to-end Neural Network which detects food intake events (i.e. bites). The proposed network uses both convolutional and recurrent layers that are trained simultaneously. Subsequently, we show how the distribution of the detected bites throughout the day can be used to estimate the start and end points of meals, using signal processing algorithms. We perform extensive evaluation on each framework part individually. Leave-one-subject-out (LOSO) evaluation shows that our bite detection approach outperforms four state-of-the-art algorithms towards the detection of bites during the course of a meal (0.923 F1 score). Furthermore, LOSO and held-out set experiments regarding the estimation of meal start/end points reveal that the proposed approach outperforms a relevant approach found in the literature (Jaccard Index of 0.820 and 0.821 for the LOSO and heldout experiments, respectively). Experiments are performed using our publicly available FIC and the newly introduced FreeFIC datasets.      
### 11.Predictive Visual Analytics System for Studying Neurodegenerative Disease based on DTI Fiber Tracts  [ :arrow_down: ](https://arxiv.org/pdf/2010.07047.pdf)
>  Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce an intelligent visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system's AI-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space, and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using real data from the research database of Parkinson's Progression Markers Initiative.      
### 12.3D Segmentation Networks for Excessive Numbers of Classes: Distinct Bone Segmentation in Upper Bodies  [ :arrow_down: ](https://arxiv.org/pdf/2010.07045.pdf)
>  Segmentation of distinct bones plays a crucial role in diagnosis, planning, navigation, and the assessment of bone metastasis. It supplies semantic knowledge to visualisation tools for the planning of surgical interventions and the education of health professionals. Fully supervised segmentation of 3D data using Deep Learning methods has been extensively studied for many tasks but is usually restricted to distinguishing only a handful of classes. With 125 distinct bones, our case includes many more labels than typical 3D segmentation tasks. For this reason, the direct adaptation of most established methods is not possible. This paper discusses the intricacies of training a 3D segmentation network in a many-label setting and shows necessary modifications in network architecture, loss function, and data augmentation. As a result, we demonstrate the robustness of our method by automatically segmenting over one hundred distinct bones simultaneously in an end-to-end learnt fashion from a CT-scan.      
### 13.Soil moisture map construction using microwave remote sensors and sequential data assimilation  [ :arrow_down: ](https://arxiv.org/pdf/2010.07037.pdf)
>  Microwave remote sensors mounted on center pivot irrigation systems provide a feasible approach to obtain soil moisture information, in the form of water content maps, for the implementation of closed-loop irrigation. Major challenges such as significant time delays in the soil moisture measurements, the inability of the sensors to provide soil moisture information in instances where the center pivot is stationary, and the inability of the sensors to provide soil moisture information in the root zone reduce the usability of the water content maps in the effective implementation of closed-loop irrigation. In this paper, we seek to address the aforementioned challenges and consequently describe a water content map construction procedure that is suitable for the implementation of closed-loop irrigation. Firstly, we propose the cylindrical coordinates version of the Richards equation (field model) which naturally models fields equipped with a center pivot irrigation system. Secondly, measurements obtained from the microwave sensors are assimilated into the field model using the extended Kalman filter to form an information fusion system, which will provide frequent soil moisture estimates and predictions in the form of moisture content maps. The utility of the proposed information fusion system is first investigated with simulated microwave sensor measurements. The information fusion system is then applied to a real large-scale agriculture field where we demonstrate the its ability to address the challenges. Three performance evaluation criteria are used to validate the soil moisture estimates and predictions provided by the proposed information fusion system.      
### 14.Hybrid Modeling Application in Control Valve  [ :arrow_down: ](https://arxiv.org/pdf/2010.07014.pdf)
>  In view of the serious nonlinearity, time-varying and parameter uncertainty in the physical model of regulating valve, a prediction model of flow rate and pressure of regulating valve based on mixed model was proposed.According to the physical model of the regulator, the parameters that can represent the operation state of the regulator are analyzed, and the relevant parameters are identified by unbiased LSSVM method.The DAMADICS simulation results show that the model can predict the output of flow rate and pressure with better accuracy, which can provide guidance for the design of automatic control valve or fault diagnosis system.      
### 15.Fast signal recovery from quadratic measurements  [ :arrow_down: ](https://arxiv.org/pdf/2010.07012.pdf)
>  We present a novel approach for recovering a sparse signal from cross-correlated data. Cross-correlations naturally arise in many fields of imaging, such as optics, holography and seismic interferometry. Compared to the sparse signal recovery problem that uses linear measurements, the unknown is now a matrix formed by the cross correlation of the unknown signal. Hence, the bottleneck for inversion is the number of unknowns that grows quadratically. The main idea of our proposed approach is to reduce the dimensionality of the problem by recovering only the diagonal of the unknown matrix, whose dimension grows linearly with the size of the problem. The keystone of the methodology is the use of an efficient {\em Noise Collector} that absorbs the data that come from the off-diagonal elements of the unknown matrix and that do not carry extra information about the support of the signal. This results in a linear problem whose cost is similar to the one that uses linear measurements. Our theory shows that the proposed approach provides exact support recovery when the data is not too noisy, and that there are no false positives for any level of noise. Moreover, our theory also demonstrates that when using cross-correlated data, the level of sparsity that can be recovered increases, scaling almost linearly with the number of data. The numerical experiments presented in the paper corroborate these findings.      
### 16.Optimization of loading factor preventing target cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2010.07010.pdf)
>  Adaptive algorithms based on sample matrix inversion belong to an important class of algorithms used in radar target detection to overcome prior uncertainty of interference covariance. Sample matrix inversion problem is generally ill conditioned. Moreover, the contamination of the empirical covariance matrix by the useful signal leads to significant degradation of performance of this class of adaptive algorithms. Regularization, also known in radar literature as sample covariance loading, can be used to combat both ill conditioning of the original problem and contamination of the empirical covariance by the desired signal. However, the optimum value of loading factor cannot be derived unless strong assumptions are made regarding the structure of covariance matrix and useful signal penetration model. In this paper an iterative algorithm for loading factor optimization based on the maximization of empirical signal to interference plus noise ratio (SINR) is proposed. The proposed solution does not rely on any assumptions regarding the structure of empirical covariance matrix and signal penetration model. The paper also presents simulation examples showing the effectiveness of the proposed solution.      
### 17.Binarization Methods for Motor-Imagery Brain-Computer Interface Classification  [ :arrow_down: ](https://arxiv.org/pdf/2010.07004.pdf)
>  Successful motor-imagery brain-computer interface (MI-BCI) algorithms either extract a large number of handcrafted features and train a classifier, or combine feature extraction and classification within deep convolutional neural networks (CNNs). Both approaches typically result in a set of real-valued weights, that pose challenges when targeting real-time execution on tightly resource-constrained devices. We propose methods for each of these approaches that allow transforming real-valued weights to binary numbers for efficient inference. Our first method, based on sparse bipolar random projection, projects a large number of real-valued Riemannian covariance features to a binary space, where a linear SVM classifier can be learned with binary weights too. By tuning the dimension of the binary embedding, we achieve almost the same accuracy in 4-class MI ($\leq$1.27% lower) compared to models with float16 weights, yet delivering a more compact model with simpler operations to execute. Second, we propose to use memory-augmented neural networks (MANNs) for MI-BCI such that the augmented memory is binarized. Our method replaces the fully connected layer of CNNs with a binary augmented memory using bipolar random projection, or learned projection. Our experimental results on EEGNet, an already compact CNN for MI-BCI, show that it can be compressed by 1.28x at iso-accuracy using the random projection. On the other hand, using the learned projection provides 3.89% higher accuracy but increases the memory size by 28.10x.      
### 18.Fast meningioma segmentation in T1-weighted MRI volumes using a lightweight 3D deep learning architecture  [ :arrow_down: ](https://arxiv.org/pdf/2010.07002.pdf)
>  Automatic and consistent meningioma segmentation in T1-weighted MRI volumes and corresponding volumetric assessment is of use for diagnosis, treatment planning, and tumor growth evaluation. In this paper, we optimized the segmentation and processing speed performances using a large number of both surgically treated meningiomas and untreated meningiomas followed at the outpatient clinic. We studied two different 3D neural network architectures: (i) a simple encoder-decoder similar to a 3D U-Net, and (ii) a lightweight multi-scale architecture (PLS-Net). In addition, we studied the impact of different training schemes. For the validation studies, we used 698 T1-weighted MR volumes from St. Olav University Hospital, Trondheim, Norway. The models were evaluated in terms of detection accuracy, segmentation accuracy and training/inference speed. While both architectures reached a similar Dice score of 70% on average, the PLS-Net was more accurate with an F1-score of up to 88%. The highest accuracy was achieved for the largest meningiomas. Speed-wise, the PLS-Net architecture tended to converge in about 50 hours while 130 hours were necessary for U-Net. Inference with PLS-Net takes less than a second on GPU and about 15 seconds on CPU. Overall, with the use of mixed precision training, it was possible to train competitive segmentation models in a relatively short amount of time using the lightweight PLS-Net architecture. In the future, the focus should be brought toward the segmentation of small meningiomas (less than 2ml) to improve clinical relevance for automatic and early diagnosis as well as speed of growth estimates.      
### 19.Passive RIS vs.Hybrid RIS: A Comparative Study on Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2010.06981.pdf)
>  The reconfigurable intelligent surface (RIS) plays an important role in maintaining the connectivity in millimeter wave (mmWave) MIMO systems when the direct channel between the transceivers is blocked. However, it is difficult to acquire the channel state information (CSI), which is essential for the design of RIS phase control matrix and beamforming vectors at the transceivers. In this paper, we compare the channel estimation (CE) performance and achieved spectral efficiency (SE) of the purely passive and hybrid RIS architectures. CE is done via atomic norm minimization (ANM). For the purely passive RIS, we follow a two-stage procedure to sequentially estimate the channel parameters, while for the hybrid RIS we estimate the individual channels at the RIS based on the observations from active RIS elements assuming alternating uplink and downlink training. The simulation results show that the purely passive RIS brings better CE and SE performance compared to the hybrid RIS under the same training overhead. We further consider different setups for the hybrid RIS and study the tradeoffs among them.      
### 20.Practical Deep Raw Image Denoising on Mobile Devices  [ :arrow_down: ](https://arxiv.org/pdf/2010.06935.pdf)
>  Deep learning-based image denoising approaches have been extensively studied in recent years, prevailing in many public benchmark datasets. However, the stat-of-the-art networks are computationally too expensive to be directly applied on mobile devices. In this work, we propose a light-weight, efficient neural network-based raw image denoiser that runs smoothly on mainstream mobile devices, and produces high quality denoising results. Our key insights are twofold: (1) by measuring and estimating sensor noise level, a smaller network trained on synthetic sensor-specific data can out-perform larger ones trained on general data; (2) the large noise level variation under different ISO settings can be removed by a novel k-Sigma Transform, allowing a small network to efficiently handle a wide range of noise levels. We conduct extensive experiments to demonstrate the efficiency and accuracy of our approach. Our proposed mobile-friendly denoising model runs at ~70 milliseconds per megapixel on Qualcomm Snapdragon 855 chipset, and it is the basis of the night shot feature of several flagship smartphones released in 2019.      
### 21.Efficient and high accuracy 3-D OCT angiography motion correction in pathology  [ :arrow_down: ](https://arxiv.org/pdf/2010.06931.pdf)
>  We propose a novel method for non-rigid 3-D motion correction of orthogonally raster-scanned optical coherence tomography angiography volumes. This is the first approach that aligns predominantly axial structural features like retinal layers and transverse angiographic vascular features in a joint optimization. Combined with the use of orthogonal scans and favorization of kinematically more plausible displacements, the approach allows subpixel alignment and micrometer-scale distortion correction in all 3 dimensions. As no specific structures or layers are segmented, the approach is by design robust to pathologic changes. It is furthermore designed for highly parallel implementation and brief runtime, allowing its integration in clinical routine even for high density or wide-field scans. We evaluated the algorithm with metrics related to clinically relevant features in a large-scale quantitative evaluation based on 204 volumetric scans of 17 subjects including both a wide range of pathologies and healthy controls. Using this method, we achieve state-of-the-art axial performance and show significant advances in both transverse co-alignment and distortion correction, especially in the pathologic subgroup.      
### 22.A Semi-Blind Multiuser SIMO GFDM System in the Presence of CFOs and IQ Imbalances  [ :arrow_down: ](https://arxiv.org/pdf/2010.06886.pdf)
>  In this paper, we investigate an open topic of a multiuser single-input-multiple-output (SIMO) generalized frequency division multiplexing (GFDM) system in the presence of carrier frequency offsets (CFOs) and in-phase/quadrature-phase (IQ) imbalances. A low-complexity semi-blind joint estimation scheme of multiple channels, CFOs and IQ imbalances is proposed. By utilizing the subspace approach, CFOs and channels corresponding to U users are first separated into U groups. For each individual user, CFO is extracted by minimizing the smallest eigenvalue whose corresponding eigenvector is utilized to estimate channel blindly. The IQ imbalance parameters are estimated jointly with channel ambiguities by very few pilots. The proposed scheme is feasible for a wider range of receive antennas number and has no constraints on the assignment scheme of subsymbols and subcarriers, modulation type, cyclic prefix length and the number of subsymbols per GFDM symbol. Simulation results show that the proposed scheme significantly outperforms the existing methods in terms of bit error rate, outage probability, mean-square-errors of CFO estimation, channel and IQ imbalance estimation, while at much higher spectral efficiency and lower computational complexity. The Cramer-Rao lower bound is derived to verify the effectiveness of the proposed scheme, which is shown to be close to simulation results.      
### 23.Chance-constrained Stochastic MPC of Astlingen Urban Drainage Benchmark Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.06883.pdf)
>  In urban drainage systems (UDS), a proven method for reducing the combined sewer overflow (CSO) pollution is real-time control (RTC) based on model predictive control (MPC). MPC methodologies for RTC of UDSs in the literature rely on the computation of the optimal control strategies based on deterministic rain forecast. However, in reality, uncertainties exist in rainfall forecasts which affect severely accuracy of computing the optimal control strategies. Under this context, this work aims to focus on the uncertainty associated with the rainfall forecasting and its effects. One option is to use stochastic information about the rain events in the controller; in the case of using MPC methods, the class called stochastic MPC is available, including several approaches such as the chance-constrained MPC method. In this study, we apply stochastic MPC to the UDS using the chance-constrained method. Moreover, we also compare the operational behavior of both the classical MPC with perfect forecast and the chance-constrained MPC based on different stochastic scenarios of the rain forecast. The application and comparison have been based on simulations using a SWMM model of the Astlingen urban drainage benchmark network.      
### 24.Towards the Transportation Internet: Concepts, Architectures and Requirements  [ :arrow_down: ](https://arxiv.org/pdf/2010.06880.pdf)
>  The Internet has become a general network design paradigm. The Energy Internet is a successful case applied in the energy field. Disruptive transportation changes have been driven by the development of technologies such as cloud computing, 5G, AI, autonomous driving, and mobility services. We propose the Transportation Internet by introducing the paradigm into the transportation field. The Transportation Internet brings dynamics, efficiency, intelligence, scalability, and openness to transportation. It also provides a valuable reference system for the future transportation automation. By comparing with the internet, this paper draws out concepts, proposes architectures and gives requirements of the Transportation Internet. A small-scale verification has been carried out.      
### 25.Robust Parametrization of a Model Predictive Controller for a CNC Machining Center Using Bayesian Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2010.06869.pdf)
>  Control algorithms such as model predictive control (MPC) and state estimators rely on a number of different parameters. The performance of the closed loop usually depends on the correct setting of these parameters. Tuning is often done manually by experts based on a simulation model of the system. Two problems arise with this procedure. Firstly, experts need to be skilled and still may not be able to find the optimal parametrization. Secondly, the performance of the simulation model might not be able to be carried over to the real world application due to model inaccuracies within the simulation. With this contribution, we demonstrate on an industrial milling process how Bayesian optimization can automate the tuning process and help to solve the mentioned problems. Robust parametrization is ensured by perturbing the simulation with arbitrarily distributed model plant mismatches. The objective is to minimize the expected integral reference tracking error, guaranteeing acceptable worst case behavior while maintaining real-time capability. These verbal requirements are translated into a constrained stochastic mixed-integer black-box optimization problem. A two stage min-max-type Bayesian optimization procedure is developed and compared to benchmark algorithms in a simulation study of a CNC machining center. It is showcased how the empirical performance model obtained through Bayesian optimization can be used to analyze and visualize the results. Results indicate superior performance over the case where only the nominal model is used for controller synthesis. The optimized parametrization improves the initial hand-tuned parametrization notably.      
### 26.Stochastic Model Predictive Control and Sewer Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.06859.pdf)
>  In this work, an evaluation of Chance-Constrained Model Predictive Control (CC-MPC) in sewer systems over the use of the classical deterministic Model Predictive Control (MPC) is presented. The focus of this evaluation is on the avoidance of weir overflow when uncertainty is present. Furthermore, the design formulation of CC-MPC is presented with a comparison to the design of MPC. For the evaluation, a simplified model of the Barcelona sewer network case study is utilized. Our comparison shows that for sewer systems with uncertain inflows, a CC-MPC allows for better statistical guarantees for avoiding weir overflow, than relying on a deterministic MPC. A simple back-up strategy in case of infeasible optimization program was also apparent for the CC-MPC based on the results of the analysis.      
### 27.Extended Koopman Models  [ :arrow_down: ](https://arxiv.org/pdf/2010.06845.pdf)
>  We introduce two novel generalizations of the Koopman operator method of nonlinear dynamic modeling. Each of these generalizations leads to greatly improved predictive performance without sacrificing a unique trait of Koopman methods: the potential for fast, globally optimal control of nonlinear, nonconvex systems. The first generalization, Convex Koopman Models, uses convex rather than linear dynamics in the lifted space. The second, Extended Koopman Models, additionally introduces an invertible transformation of the control signal which contributes to the lifted convex dynamics. We describe a deep learning architecture for parameterizing these classes of models, and show experimentally that each significantly outperforms traditional Koopman models in trajectory prediction for two nonlinear, nonconvex dynamic systems.      
### 28.Rapid Generation of Stochastic Signals with Specified Statistics  [ :arrow_down: ](https://arxiv.org/pdf/2010.06840.pdf)
>  We demonstrate a novel algorithm for generating stationary stochastic signals with a specified power spectral density (or equivalently, via the Wiener-Khinchin relation, a specified autocorrelation function) while satisfying constraints on the signal's probability density function. A tightly related problem has already been essentially solved by methods involving nonlinear filtering, however we use a fundamentally different approach involving optimization and stochastic interchange which immediately generalizes to generating signals with a broader range of statistics. This combination of optimization and stochastic interchange eliminates drawbacks associated with either method in isolation, improving the best-case scaling in runtime to generate a signal of length $n$ from $\mathcal{O}(n^2)$ for stochastic interchange on its own to $\mathcal{O}(n \: \text{log} \: n)$ without parallelization or $\mathcal{O}(n)$ with full parallelization. We demonstrate this speedup experimentally, and furthermore show that the signals we generate match the desired autocorrelation more accurately than those generated by stochastic interchange on its own. We observe that the signals we produce, unlike those generated by optimization on its own, are stationary.      
### 29.Coarse-Grained Nonlinear System Identification  [ :arrow_down: ](https://arxiv.org/pdf/2010.06830.pdf)
>  We introduce Coarse-Grained Nonlinear Dynamics, an efficient and universal parameterization of nonlinear system dynamics based on the Volterra series expansion. These models require a number of parameters only quasilinear in the system's memory regardless of the order at which the Volterra expansion is truncated; this is a superpolynomial reduction in the number of parameters as the order becomes large. This efficient parameterization is achieved by coarse-graining parts of the system dynamics that depend on the product of temporally distant input samples; this is conceptually similar to the coarse-graining that the fast multipole method uses to achieve $\mathcal{O}(n)$ simulation of n-body dynamics. Our efficient parameterization of nonlinear dynamics can be used for regularization, leading to Coarse-Grained Nonlinear System Identification, a technique which requires very little experimental data to identify accurate nonlinear dynamic models. We demonstrate the properties of this approach on a simple synthetic problem. We also demonstrate this approach experimentally, showing that it identifies an accurate model of the nonlinear voltage to luminosity dynamics of a tungsten filament with less than a second of experimental data.      
### 30.Differential diagnosis and molecular stratification of gastrointestinal stromal tumors on CT images using a radiomics approach  [ :arrow_down: ](https://arxiv.org/pdf/2010.06824.pdf)
>  Distinguishing gastrointestinal stromal tumors (GISTs) from other intra-abdominal tumors and GISTs molecular analysis is necessary for treatment planning, but challenging due to its rarity. The aim of this study was to evaluate radiomics for distinguishing GISTs from other intra-abdominal tumors, and in GISTs, predict the \textit{c-KIT}, \textit{PDGFRA}, \textit{BRAF} mutational status and mitotic index (MI). All 247 included patients (125 GISTS, 122 non-GISTs) underwent a contrast-enhanced venous phase CT. The GIST vs. non-GIST radiomics model, including imaging, age, sex and location, had a mean area under the curve (AUC) of 0.82. Three radiologists had an AUC of 0.69, 0.76, and 0.84, respectively. The radiomics model had an AUC of 0.52 for \textit{c-KIT}, 0.56 for \textit{c-KIT} exon 11, and 0.52 for the MI. Hence, our radiomics model was able to distinguish GIST from non-GISTS with a performance similar to three radiologists, but was not able to predict the c-KIT mutation or MI.      
### 31.On the Mutual Interference between Spaceborne SARs: Modeling, Characterization, and Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2010.06819.pdf)
>  Since by the International Telecommunications Union (ITU) regulatory the radio spectrum available to spaceborne synthetic aperture radar (SAR) is restricted to certain limited frequency intervals, there are many different spaceborne SAR systems sharing common frequency bands. Due to this reason, it is reported that two spaceborne SARs at orbit cross positions can potentially cause severe mutual interference. Specifically, the transmitting signal of a SAR, typically linear frequency modulated (LFM), can be directly received by the side or back lobes of another SAR's antenna, causing radiometric artefacts in the focused image. This paper tries to model and characterize the artefacts, and study efficient methods for mitigating them. To this end, we formulate an analytical model for describing the artefact, which reveals that the mutual interference can introduce a two-dimensional LFM radiometric artefact in image domain with a limited spatial extent. We show that the artefact is low-rank based on range-azimuth decoupling analysis and two-dimensional high-order Taylor expansion. Based on the low rank model, we show that two methods, i.e., principle component analysis and its robust variant, can be adopted to efficiently mitigate the artefact via processing in image domain. The former method has the advantage of fast processing speed, for example, a sub-swath of Sentinel-1 interferometric wide swath image can be processed within 70 seconds via block-wise operation, whereas the latter provides improved accuracy for sparse point-like scatterers. Experiment results demonstrate that the radiometric artefacts caused by mutual interference in Sentinel-1 level-1 images can be efficiently mitigated via the proposed methods.      
### 32.Tire Slip Angle Estimation based on the Intelligent Tire Technology  [ :arrow_down: ](https://arxiv.org/pdf/2010.06803.pdf)
>  Tire slip angle is a vital parameter in tire/vehicle dynamics and control. This paper proposes an accurate estimation method by the fusion of intelligent tire technology and machine-learning techniques. The intelligent tire is equipped by MEMS accelerometers attached to its inner liner. First, we describe the intelligent tire system along with the implemented testing apparatus. Second, experimental results under different loading and velocity conditions are provided. Then,~we show the procedure of data processing, which will be used for training three different machine learning techniques to estimate tire slip angles. The results show that the machine learning techniques, especially in frequency domain, can accurately estimate tire slip angles up to 10 degrees. More importantly, with the accurate tire slip angle estimation, all other states and parameters can be easily and precisely obtained, which is significant to vehicle advanced control, and thus this study has a high potential to obviously improve the vehicle safety especially in extreme maneuvers.      
### 33.Designing Massive MIMO Detector via PS-ADMM approach  [ :arrow_down: ](https://arxiv.org/pdf/2010.06798.pdf)
>  In this paper, we develop an efficient detector for massive multiple-input multiple-output (MIMO) communication systems via penalty-sharing alternating direction method of multipliers (PS-ADMM). Its main content are as follows: first, we formulate the MIMO detection as a maximum-likelihood optimization problem with bound relaxation constraints. Then, the higher modulation signals are decomposed into a sum of multiple binary variables through their inherent structures, by exploiting introduced binary variables as penalty functions, the detection optimization model is equivalent to a nonconvex sharing minimization problem. Second, a customized ADMM algorithm is presented to solve the formulated nonconvex optimization problem. In the implementation, all variables can be solved analytically and parallelly. Third, it is proved that the proposed PS-ADMM algorithm converges if proper parameters are chosen. Simulation results demonstrate the effectiveness of the proposed approach.      
### 34.Low-rank Convex/Sparse Thermal Matrix Approximation for Infrared-based Diagnostic System  [ :arrow_down: ](https://arxiv.org/pdf/2010.06784.pdf)
>  Active and passive thermography are two efficient techniques extensively used to measure heterogeneous thermal patterns leading to subsurface defects for diagnostic evaluations. This study conducts a comparative analysis on low-rank matrix approximation methods in thermography with applications of semi-, convex-, and sparse- non-negative matrix factorization (NMF) methods for detecting subsurface thermal patterns. These methods inherit the advantages of principal component thermography (PCT) and sparse PCT, whereas tackle negative bases in sparse PCT with non-negative constraints, and exhibit clustering property in processing data. The practicality and efficiency of these methods are demonstrated by the experimental results for subsurface defect detection in three specimens (for different depth and size defects) and preserving thermal heterogeneity for distinguishing breast abnormality in breast cancer screening dataset (accuracy of 74.1%, 75.8%, and 77.8%).      
### 35.AlphaZero Based Post-Storm Vehicle Routing for Distribution Grid Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2010.06764.pdf)
>  Natural disasters such as storms usually bring significant damages to distribution grids. However, plenty of distribution grids are not installed sensors that can pinpoint the location of damaged equipments (lines, transforms, etc.), which greatly increased the difficulty of the outage restoration. This paper investigates the optimal routing of repair vehicles to restore outages in distribution grid as fast as possible after a storm. First, the vehicle routing problem is formulated as a sequential stochastic optimization problem. In the formulated optimization model, the belief state of the power grid is updated according to the phone calls from customers and the information collected by the repair vehicles. Second, an AlphaZero based utility vehicle routing (AlphaZero-UVR) approach is developed to achieve the real-time dispatching of the repair vehicle. The proposed AlphaZero-UVR approach combines deep neural networks with a Monte-Carlo tree search (MCTS) to give a lookahead search decisions, which can learn to navigate the repair vehicle without human guidance. Simulation results show that the proposed approach can effectively navigate the vehicle to repair all outages.      
### 36.Grid-Interactive Multi-Zone Building Control Using Reinforcement Learning with Global-Local Policy Search  [ :arrow_down: ](https://arxiv.org/pdf/2010.06718.pdf)
>  In this paper, we develop a grid-interactive multi-zone building controller based on a deep reinforcement learning (RL) approach. The controller is designed to facilitate building operation during normal conditions and demand response events, while ensuring occupants comfort and energy efficiency. We leverage a continuous action space RL formulation, and devise a two-stage global-local RL training framework. In the first stage, a global fast policy search is performed using a gradient-free RL algorithm. In the second stage, a local fine-tuning is conducted using a policy gradient method. In contrast to the state-of-the-art model predictive control (MPC) approach, the proposed RL controller does not require complex computation during real-time operation and can adapt to non-linear building models. We illustrate the controller performance numerically using a five-zone commercial building.      
### 37.On Front-end Gain Invariant Modeling for Wake Word Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2010.06676.pdf)
>  Wake word (WW) spotting is challenging in far-field due to the complexities and variations in acoustic conditions and the environmental interference in signal transmission. A suite of carefully designed and optimized audio front-end (AFE) algorithms help mitigate these challenges and provide better quality audio signals to the downstream modules such as WW spotter. Since the WW model is trained with the AFE-processed audio data, its performance is sensitive to AFE variations, such as gain changes. In addition, when deploying to new devices, the WW performance is not guaranteed because the AFE is unknown to the WW model. To address these issues, we propose a novel approach to use a new feature called $\Delta$LFBE to decouple the AFE gain variations from the WW model. We modified the neural network architectures to accommodate the delta computation, with the feature extraction module unchanged. We evaluate our WW models using data collected from real household settings and showed the models with the $\Delta$LFBE is robust to AFE gain changes. Specifically, when AFE gain changes up to $\pm$12dB, the baseline CNN model lost up to relative 19.0% in false alarm rate or 34.3% in false reject rate, while the model with $\Delta$LFBE demonstrates no performance loss.      
### 38.Towards Data-efficient Modeling for Wake Word Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2010.06659.pdf)
>  Wake word (WW) spotting is challenging in far-field not only because of the interference in signal transmission but also the complexity in acoustic environments. Traditional WW model training requires large amount of in-domain WW-specific data with substantial human annotations therefore it is hard to build WW models without such data. In this paper we present data-efficient solutions to address the challenges in WW modeling, such as domain-mismatch, noisy conditions, limited annotation, etc. Our proposed system is composed of a multi-condition training pipeline with a stratified data augmentation, which improves the model robustness to a variety of predefined acoustic conditions, together with a semi-supervised learning pipeline to accurately extract the WW and confusable examples from untranscribed speech corpus. Starting from only 10 hours of domain-mismatched WW audio, we are able to enlarge and enrich the training dataset by 20-100 times to capture the acoustic complexity. Our experiments on real user data show that the proposed solutions can achieve comparable performance of a production-grade model by saving 97\% of the amount of WW-specific data collection and 86\% of the bandwidth for annotation.      
### 39.Frequency Domain Multi-user Detection for Single Carrier Modulation with Cyclic Prefix  [ :arrow_down: ](https://arxiv.org/pdf/2010.06658.pdf)
>  Frequency domain (FD) multi-user detection (MUD) has been shown to be an effective means of approaching the theoretical per-user capacity for single carrier modulation (SCM) schemes in massive MIMO scenarios with highly dispersive channels. When a cyclic prefix is added to the SCM waveform, the circulant structures of the resulting convolutional channel matrix allows for relatively simple expressions for the FD detection. In this paper, we develop a computationally efficient minimum mean squared error (MMSE) FD-MUD technique in a time-division duplexing (TDD) massive MIMO setup and show how processing steps can be shared between uplink and downlink.      
### 40.Deep Delay Loop Reservoir Computing for Specific Emitter Identification  [ :arrow_down: ](https://arxiv.org/pdf/2010.06649.pdf)
>  Current AI systems at the tactical edge lack the computational resources to support in-situ training and inference for situational awareness, and it is not always practical to leverage backhaul resources due to security, bandwidth, and mission latency requirements. We propose a solution through Deep delay Loop Reservoir Computing (DLR), a processing architecture supporting general machine learning algorithms on compact mobile devices by leveraging delay-loop (DL) reservoir computing in combination with innovative photonic hardware exploiting the inherent speed, and spatial, temporal and wavelength-based processing diversity of signals in the optical domain. DLR delivers reductions in form factor, hardware complexity, power consumption and latency, compared to State-of-the-Art . DLR can be implemented with a single photonic DL and a few electro-optical components. In certain cases multiple DL layers increase learning capacity of the DLR with no added latency. We demonstrate the advantages of DLR on the application of RF Specific Emitter Identification.      
### 41.Finite-Time Model Inference From A Single Noisy Trajectory  [ :arrow_down: ](https://arxiv.org/pdf/2010.06616.pdf)
>  This paper proposes a novel model inference procedure to identify system matrix from a single noisy trajectory over a finite-time interval. The proposed inference procedure comprises an observation data processor, a redundant data processor and an ordinary least-square estimator, wherein the data processors mitigate the influence of observation noise on inference error. We first systematically investigate the comparisons with naive least-square-regression based model inference and uncover that 1) the same observation data has identical influence on the feasibility of the proposed and the naive model inferences, 2) the naive model inference uses all of the redundant data, while the proposed model inference optimally uses the basis and the redundant data. We then study the sample complexity of the proposed model inference in the presence of observation noise, which leads to the dependence of the processed bias in the observed system trajectory on time and coordinates. Particularly, we derive the sample-complexity upper bound (on the number of observations sufficient to infer a model with prescribed levels of accuracy and confidence) and the sample-complexity lower bound (high-probability lower bound on model error). Finally, the proposed model inference is numerically validated and analyzed.      
### 42.Regret Guarantees for Online Receding Horizon Control  [ :arrow_down: ](https://arxiv.org/pdf/2010.07269.pdf)
>  In this paper we provide provable regret guarantees for an online receding horizon type control policy in a setting where the system to be controlled is an unknown linear dynamical system, the cost for the controller is a general additive function over a finite period $T$, and there exist control input constraints that when violated incur an additional cost. We show that the learning based receding horizon control policy achieves the regret of $O(T^{3/4})$ for both the controller's cost and cumulative constraint violation w.r.t the baseline receding horizon control policy that has full knowledge of the system.      
### 43.Cost-optimal V2X Service Placement in Distributed Cloud/Edge Environment  [ :arrow_down: ](https://arxiv.org/pdf/2010.07223.pdf)
>  Deploying V2X services has become a challenging task. This is mainly due to the fact that such services have strict latency requirements. To meet these requirements, one potential solution is adopting mobile edge computing (MEC). However, this presents new challenges including how to find a cost efficient placement that meets other requirements such as latency. In this work, the problem of cost-optimal V2X service placement (CO-VSP) in a distributed cloud/edge environment is formulated. Additionally, a cost-focused delay-aware V2X service placement (DA-VSP) heuristic algorithm is proposed. Simulation results show that both CO-VSP model and DA-VSP algorithm guarantee the QoS requirements of all such services and illustrates the trade-off between latency and deployment cost.      
### 44.Emergent Jaw Predominance in Vocal Development through Stochastic Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2010.07208.pdf)
>  Infant vocal babbling strongly relies on jaw oscillations, especially at the stage of canonical babbling, which underlies the syllabic structure of world languages. In this paper, we propose, model and analyze an hypothesis to explain this predominance of the jaw in early babbling. This hypothesis states that general stochastic optimization principles, when applied to learning sensorimotor control, automatically generate ordered babbling stages with a predominant exploration of jaw movements in early stages. The reason is that those movements impact the auditory effects more than other articulators. In previous computational models, such general principles were shown to selectively freeze and free degrees of freedom in a model reproducing the proximo-distal development observed in infant arm reaching. The contribution of this paper is to show how, using the same methods, we are able to explain such patterns in vocal development. We present three experiments. The two first ones show that the recruitment order of articulators emerging from stochastic optimization depends on the target sound to be achieved but that on average the jaw is largely chosen as the first recruited articulator. The third experiment analyses in more detail how the emerging recruitment order is shaped by the dynamics of the optimization process.      
### 45.Towards Resistant Audio Adversarial Examples  [ :arrow_down: ](https://arxiv.org/pdf/2010.07190.pdf)
>  Adversarial examples tremendously threaten the availability and integrity of machine learning-based systems. While the feasibility of such attacks has been observed first in the domain of image processing, recent research shows that speech recognition is also susceptible to adversarial attacks. However, reliably bridging the air gap (i.e., making the adversarial examples work when recorded via a microphone) has so far eluded researchers. We find that due to flaws in the generation process, state-of-the-art adversarial example generation methods cause overfitting because of the binning operation in the target speech recognition system (e.g., Mozilla Deepspeech). We devise an approach to mitigate this flaw and find that our method improves generation of adversarial examples with varying offsets. We confirm the significant improvement with our approach by empirical comparison of the edit distance in a realistic over-the-air setting. Our approach states a significant step towards over-the-air attacks. We publish the code and an applicable implementation of our approach.      
### 46.A Geometric Approach to On-road Motion Planning for Long and Multi-Body Heavy-Duty Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2010.07133.pdf)
>  Driving heavy-duty vehicles, such as buses and tractor-trailer vehicles, is a difficult task in comparison to passenger cars. Most research on motion planning for autonomous vehicles has focused on passenger vehicles, and many unique challenges associated with heavy-duty vehicles remain open. However, recent works have started to tackle the particular difficulties related to on-road motion planning for buses and tractor-trailer vehicles using numerical optimization approaches. In this work, we propose a framework to design an optimization objective to be used in motion planners. Based on geometric derivations, the method finds the optimal trade-off between the conflicting objectives of centering different axles of the vehicle in the lane. For the buses, we consider the front and rear axles trade-off, whereas for articulated vehicles, we consider the tractor and trailer rear axles trade-off. Our results show that the proposed design strategy results in planned paths that considerably improve the behavior of heavy-duty vehicles by keeping the whole vehicle body in the center of the lane.      
### 47.Exploiting Spectral Augmentation for Code-Switched Spoken Language Identification  [ :arrow_down: ](https://arxiv.org/pdf/2010.07130.pdf)
>  Spoken language Identification (LID) systems are needed to identify the language(s) present in a given audio sample, and typically could be the first step in many speech processing related tasks such as automatic speech recognition (ASR). Automatic identification of the languages present in a speech signal is not only scientifically interesting, but also of practical importance in a multilingual country such as India. In many of the Indian cities, when people interact with each other, as many as three languages may get mixed. These may include the official language of that province, Hindi and English (at times the languages of the neighboring provinces may also get mixed during these interactions). This makes the spoken LID task extremely challenging in Indian context. While quite a few LID systems in the context of Indian languages have been implemented, most such systems have used small scale speech data collected internally within an organization. In the current work, we perform spoken LID on three Indian languages (Gujarati, Telugu, and Tamil) code-mixed with English. This task was organized by the Microsoft research team as a spoken LID challenge. In our work, we modify the usual spectral augmentation approach and propose a language mask that discriminates the language ID pairs, which leads to a noise robust spoken LID system. The proposed method gives a relative improvement of approximately 3-5% in the LID accuracy over a baseline system proposed by Microsoft on the three language pairs for two shared tasks suggested in the challenge.      
### 48.Online Anomaly Detection in Surveillance Videos with Asymptotic Bounds on False Alarm Rate  [ :arrow_down: ](https://arxiv.org/pdf/2010.07110.pdf)
>  Anomaly detection in surveillance videos is attracting an increasing amount of attention. Despite the competitive performance of recent methods, they lack theoretical performance analysis, particularly due to the complex deep neural network architectures used in decision making. Additionally, online decision making is an important but mostly neglected factor in this domain. Much of the existing methods that claim to be online, depend on batch or offline processing in practice. Motivated by these research gaps, we propose an online anomaly detection method in surveillance videos with asymptotic bounds on the false alarm rate, which in turn provides a clear procedure for selecting a proper decision threshold that satisfies the desired false alarm rate. Our proposed algorithm consists of a multi-objective deep learning module along with a statistical anomaly detection module, and its effectiveness is demonstrated on several publicly available data sets where we outperform the state-of-the-art algorithms. All codes are available at <a class="link-external link-https" href="https://github.com/kevaldoshi17/Prediction-based-Video-Anomaly-Detection-" rel="external noopener nofollow">this https URL</a>.      
### 49.Learned Greedy Method (LGM): A Novel Neural Architecture for Sparse Coding and Beyond  [ :arrow_down: ](https://arxiv.org/pdf/2010.07069.pdf)
>  The fields of signal and image processing have been deeply influenced by the introduction of deep neural networks. These are successfully deployed in a wide range of real-world applications, obtaining state of the art results and surpassing well-known and well-established classical methods. Despite their impressive success, the architectures used in many of these neural networks come with no clear justification. As such, these are usually treated as "black box" machines that lack any kind of interpretability. A constructive remedy to this drawback is a systematic design of such networks by unfolding well-understood iterative algorithms. A popular representative of this approach is the Iterative Shrinkage-Thresholding Algorithm (ISTA) and its learned version -- LISTA, aiming for the sparse representations of the processed signals. In this paper we revisit this sparse coding task and propose an unfolded version of a greedy pursuit algorithm for the same goal. More specifically, we concentrate on the well-known Orthogonal-Matching-Pursuit (OMP) algorithm, and introduce its unfolded and learned version. Key features of our Learned Greedy Method (LGM) are the ability to accommodate a dynamic number of unfolded layers, and a stopping mechanism based on representation error, both adapted to the input. We develop several variants of the proposed LGM architecture and test some of them in various experiments, demonstrating their flexibility and efficiency.      
### 50.GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music  [ :arrow_down: ](https://arxiv.org/pdf/2010.07061.pdf)
>  Symbolic music datasets are important for music information retrieval and musical analysis. However, there is a lack of large-scale symbolic dataset for classical piano music. In this article, we create a GiantMIDI-Piano dataset containing 10,854 unique piano solo pieces composed by 2,786 composers. The dataset is collected as follows, we extract music piece names and composer names from the International Music Score Library Project (IMSLP). We search and download their corresponding audio recordings from the internet. We apply a convolutional neural network to detect piano solo pieces. Then, we transcribe those piano solo recordings to Musical Instrument Digital Interface (MIDI) files using our recently proposed high-resolution piano transcription system. Each transcribed MIDI file contains onset, offset, pitch and velocity attributes of piano notes, and onset and offset attributes of sustain pedals. GiantMIDI-Piano contains 34,504,873 transcribed notes, and contains metadata information of each music piece. To our knowledge, GiantMIDI-Piano is the largest classical piano MIDI dataset so far. We analyses the statistics of GiantMIDI-Piano including the nationalities, the number and duration of works of composers. We show the chroma, interval, trichord and tetrachord frequencies of six composers from different eras to show that GiantMIDI-Piano can be used for musical analysis. Our piano solo detection system achieves an accuracy of 89\%, and the piano note transcription achieves an onset F1 of 96.72\% evaluated on the MAESTRO dataset. GiantMIDI-Piano achieves an alignment error rate (ER) of 0.154 to the manually input MIDI files, comparing to MAESTRO with an alignment ER of 0.061 to the manually input MIDI files. We release the source code of acquiring the GiantMIDI-Piano dataset at <a class="link-external link-https" href="https://github.com/bytedance/GiantMIDI-Piano" rel="external noopener nofollow">this https URL</a>.      
### 51.Croatian public companies for energy distribution and supply: integration of information subsystems  [ :arrow_down: ](https://arxiv.org/pdf/2010.07055.pdf)
>  This research is about integration of information subsystems from:information system procurement, financial information system, information system security, technical information systems and legal information systems, and about their mutual dependence and close connections in Croatian public companies for energy distribution and supply. Also, herewe research the main goals of procurement information system which must be achieved in every organization because procurement process takes place in every public organization. Based on the model of the business technology matrix their processes can be executed by other companies engaged in similar activities. This research paper describes the timing of the sub processes, also. The timing of sub processes needs to be reduced as much as possible to achieve the planned results at the exit of the sub processes so that the costs of running sub-processes are equal to or lower than they had been so far, but with a higher quality output. We discuss possible threats to the information systems organization, and how to protect electronic information in the process of restoration of electronic data base for the financial information system. At the end the research paper we explain the advantages and disadvantages of cloud computing, information security in the event of possible applications of computing in the clouds and activities for technical information system in the flow diagram.      
### 52.Privacy Concerns Regarding Occupant Tracking in Smart Buildings  [ :arrow_down: ](https://arxiv.org/pdf/2010.07028.pdf)
>  Tracking of occupants within buildings has become a topic of interest in the past decade. Occupant tracking has been used in the public safety, energy conservation, and marketing fields. Various methods have been demonstrated which can track people outside of and inside buildings; including GPS, visual-based tracking using surveillance cameras, and vibration-based tracking using sensors such as accelerometers. In this work, those main systems for tracking occupants are compared and contrasted for the levels of detail they give about where occupants are, as well as their respective privacy concerns and how identifiable the tracking information collected is to a specific person. We discuss a case study using vibrations sensors mounted in Virginia Tech's Goodwin Hall that was recently conducted, demonstrating that similar levels of accuracy in occupant localization can be achieved to current methods, and highlighting the amount of identifying information in the vibration signals dataset. Finally, a method of transforming the vibration data to preserve occupant privacy was proposed and tested on the dataset. The results indicate that our proposed method has successfully resulted in anonymizing the occupant's gender information which was previously identifiable from the vibration data, while minimally impacting the localization accuracy achieved without anonymization.      
### 53.A Pathologist-Annotated Dataset for Validating Artificial Intelligence: A Project Description and Pilot Study  [ :arrow_down: ](https://arxiv.org/pdf/2010.06995.pdf)
>  Purpose: In this work, we present a collaboration to create a validation dataset of pathologist annotations for algorithms that process whole slide images (WSIs). We focus on data collection and evaluation of algorithm performance in the context of estimating the density of stromal tumor infiltrating lymphocytes (sTILs) in breast cancer. Methods: We digitized 64 glass slides of hematoxylin- and eosin-stained ductal carcinoma core biopsies prepared at a single clinical site. We created training materials and workflows to crowdsource pathologist image annotations on two modes: an optical microscope and two digital platforms. The workflows collect the ROI type, a decision on whether the ROI is appropriate for estimating the density of sTILs, and if appropriate, the sTIL density value for that ROI. Results: The pilot study yielded an abundant number of cases with nominal sTIL infiltration. Furthermore, we found that the sTIL densities are correlated within a case, and there is notable pathologist variability. Consequently, we outline plans to improve our ROI and case sampling methods. We also outline statistical methods to account for ROI correlations within a case and pathologist variability when validating an algorithm. Conclusion: We have built workflows for efficient data collection and tested them in a pilot study. As we prepare for pivotal studies, we will consider what it will take for the dataset to be fit for a regulatory purpose: study size, patient population, and pathologist training and qualifications. To this end, we will elicit feedback from the FDA via the Medical Device Development Tool program and from the broader digital pathology and AI community. Ultimately, we intend to share the dataset, statistical methods, and lessons learned.      
### 54.Online Shaping for ISI Channels with a Limited Number of ADC Bits  [ :arrow_down: ](https://arxiv.org/pdf/2010.06963.pdf)
>  An online shaping technique for high performance communication over Gaussian channels with Inter-Symbol Interference (ISI) and receiver Analog to Digital Converter (ADC) noise is presented. The technique uses online transmitter precoding over Pulse Amplitude Modulation (PAM) constellation, designed to shape the symbols distribution so that peak power constraint at the channel output is satisfied. An iterative decoder shares information between a modified M-BCJR module, which computes online the trellis transition probabilities of the shaped distribution, and turbo decoder. The result is a reduction in the required ADC Effective Number Of Bits (ENOB), which is in particular attractive in modern high-speed wireline links. Theoretical bounds are analytically derived which enable to assess the possible gain using shaping. On practical scenarios aim to transmit 200 Gbps and 400 Gbps over printed circuit board, we demonstrate in simulations an overall ENOB gains as high as 1.43 bit and 1.78 bit, respectively, compared to uniform 4-PAM transmission with turbo equalization at the receiver side.      
### 55.PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training  [ :arrow_down: ](https://arxiv.org/pdf/2010.06932.pdf)
>  Road network and building footprint extraction is essential for many applications such as updating maps, traffic regulations, city planning, ride-hailing, disaster response \textit{etc}. Mapping road networks is currently both expensive and labor-intensive. Recently, improvements in image segmentation through the application of deep neural networks has shown promising results in extracting road segments from large scale, high resolution satellite imagery. However, significant challenges remain due to lack of enough labeled training data needed to build models for industry grade applications. In this paper, we propose a two-stage transfer learning technique to improve robustness of semantic segmentation for satellite images that leverages noisy pseudo ground truth masks obtained automatically (without human labor) from crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation that uses focal loss, poly learning rate, and context module. We demonstrate the strengths of our approach through evaluations done on three popular datasets over two tasks, namely, road extraction and building foot-print detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet and DeepGlobe road extraction dataset, respectively.      
### 56.A Relaxation of the Stochastic Ruler Method for Discrete Simulation Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2010.06909.pdf)
>  In this paper, we propose a relaxation to the stochastic ruler method originally described by Yan and Mukai in 1992 for asymptotically determining the global optima of discrete simulation optimization problems. We show that our proposed variant of the stochastic ruler method provides accelerated convergence to the optimal solution by providing computational results for two example problems, each of which support the better performance of the variant of the stochastic ruler over the original. We then provide the theoretical grounding for the asymptotic convergence in probability of the variant to the global optimal solution under the same set of assumptions as those underlying the original stochastic ruler method.      
### 57.AMPA-Net: Optimization-Inspired Attention Neural Network for Deep Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2010.06907.pdf)
>  Compressed sensing (CS) is a challenging problem in image processing due to reconstructing an almost complete image from a limited measurement. To achieve fast and accurate CS reconstruction, we synthesize the advantages of two well-known methods (neural network and optimization algorithm) to propose a novel optimization inspired neural network which dubbed AMP-Net. AMP-Net realizes the fusion of the Approximate Message Passing (AMP) algorithm and neural network. All of its parameters are learned automatically. Furthermore, we propose an AMPA-Net which uses three attention networks to improve the representation ability of AMP-Net. Finally, We demonstrate the effectiveness of AMP-Net and AMPA-Net on four CS reconstruction benchmark data sets.      
### 58.A Review of Cyber-Ranges and Test-Beds: Current and Future Trends  [ :arrow_down: ](https://arxiv.org/pdf/2010.06850.pdf)
>  Cyber situational awareness has been proven to be of value in forming a comprehensive understanding of threats and vulnerabilities within organisations, as the degree of exposure is governed by the prevailing levels of cyber-hygiene and established processes. A more accurate assessment of the security provision informs on the most vulnerable environments that necessitate more diligent management. The rapid proliferation in the automation of cyber-attacks is reducing the gap between information and operational technologies and the need to review the current levels of robustness against new sophisticated cyber-attacks, trends, technologies and mitigation countermeasures has become pressing. A deeper characterisation is also the basis with which to predict future vulnerabilities in turn guiding the most appropriate deployment technologies. Thus, refreshing established practices and the scope of the training to support the decision making of users and operators. The foundation of the training provision is the use of Cyber-Ranges (CRs) and Test-Beds (TBs), platforms/tools that help inculcate a deeper understanding of the evolution of an attack and the methodology to deploy the most impactful countermeasures to arrest breaches. In this paper, an evaluation of documented CR and TB platforms is evaluated. CRs and TBs are segmented by type, technology, threat scenarios, applications and the scope of attainable training. To enrich the analysis of documented CR and TB research and cap the study, a taxonomy is developed to provide a broader comprehension of the future of CRs and TBs. The taxonomy elaborates on the CRs/TBs different dimensions, as well as, highlighting a diminishing differentiation between application areas.      
### 59.Fusing electrical and elasticity imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.06847.pdf)
>  Electrical and elasticity imaging are promising modalities for a suite of different applications including medical tomography, non-destructive testing, and structural health monitoring. These emerging modalities are capable of providing remote, non-invasive, and low cost opportunities. Unfortunately, both modalities are severely ill-posed nonlinear inverse problems, susceptive to noise and modelling errors. Nevertheless, the ability to incorporate complimentary data sets obtained simultaneously offers mutually-beneficial information. By fusing electrical and elastic modalities as a joint problem we are afforded the possibility to stabilise the inversion process via the utilisation of auxiliary information from both modalities as well as joint structural operators. In this study, we will discuss a possible approach to combine electrical and elasticity imaging in a joint reconstruction problem giving rise to novel multi-modality applications for use in both medical and structural engineering.      
### 60.Full-stack Hybrid Beamforming in mmWave 5G Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.06836.pdf)
>  This paper analyzes Hybrid Beamforming (HBF) and Multi-User Multiple-Input Multiple-Output (MU-MIMO) in millimeter wave (mmWave) 5th generation (5G) cellular networks considering the full protocol stack with TCP/IP traffic and MAC scheduling. Prior work on HBF and MU-MIMO has assumed full-buffer transmissions and studied link-level performance. We report non-trivial interactions between the HBF technique, the front-loaded channel estimation pilot scheme in NR, and the constraints of MU-MIMO scheduling. We also report that joint multi-user beamforming design is imperative, in the sense that the MU-MIMO system cannot be fully exploited when implemented as a mere collection of single-user analog beams working in parallel. By addressing these issues, throughput can be dramatically increased in mmWave 5G networks by means of Spatial Division Multiple Access (SDMA).      
### 61.Polynomial Approximation of Value Functions and Nonlinear Controller Design with Performance Bounds  [ :arrow_down: ](https://arxiv.org/pdf/2010.06828.pdf)
>  For any suitable Optimal Control Problem (OCP) which satisfies the Principle of Optimality, there exists a value function, defined as the unique viscosity solution to a Hamilton Jacobi Bellman (HJB) equation, and which can be used to design an optimal feedback controller for the given OCP. Unfortunately, solving the HJB analytically is rarely possible, and existing numerical approximation schemes largely rely on discretization - implying that the resulting approximate value functions may be hard to represent and may not be sub-value functions. Furthermore, controllers obtained from such schemes currently have no associated bound on performance. To address these issues, for a given OCP, we propose a sequence of Sum-OfSquares (SOS) programming problems, each of which yields a polynomial sub-solution to the HJB PDE, and show that the resulting sequence of polynomial sub-solutions converges to the value function of the OCP (convergence in the $L^1$ norm). Furthermore, for each polynomial sub-solution in this sequence we define an associated sublevel set, and show that the resulting sequence of sublevel sets converges to the sub-level set of the value function of the OCP (convergence in the volume metric). Next, for any approximate value function, obtained from an SOS program or any other method (e.g. discretization), we construct an associated feedback controller, and show that sub-optimality of this controller as applied to the OCP is bounded by the distance between the approximate and true value function of the OCP in the $W^{1, \infty}$ (Sobolev) norm. Finally, we demonstrate through several numerical examples how by solving our proposed SOS programing problem we are able to accurately approximate value functions, design controllers and estimate reachable sets.      
### 62.Exploiting wavelength diversity for high resolution time-of-flight 3D imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.06799.pdf)
>  State-of-the-art time-of-flight (ToF) based 3D sensors suffer from poor lateral and depth resolutions. In this work, we introduce a novel sensor concept that provides ToF-based 3D measurements of real world objects with depth precisions up to 35 micrometers and point cloud densities at the native sensor-resolutions of state-of-the-art CMOS/CCD cameras (up to several megapixels). Unlike other continuous-wave amplitude-modulated ToF principles, our approach exploits wavelength diversity for an interferometric surface measurement of macroscopic objects with rough or specular surfaces. Based on this principle, we introduce three different embodiments of prototype sensors, exploiting three different sensor architectures.      
### 63.Data-driven Distributionally Robust Optimal Stochastic Control Using theWasserstein Metric  [ :arrow_down: ](https://arxiv.org/pdf/2010.06794.pdf)
>  Optimal control of a stochastic dynamical system usually requires a good dynamical model with probability distributions, which is difficult to obtain due to limited measurements and/or complicated dynamics. To solve it, this work proposes a data-driven distributionally robust control framework with the Wasserstein metric via a constrained two-player zero-sum Markov game, where the adversarial player selects the probability distribution from a Wasserstein ball centered at an empirical distribution. Then, the game is approached by its penalized version, an optimal stabilizing solution of which is derived explicitly in a linear structure under the Riccati-type iterations. Moreover, we design a model-free Q-learning algorithm with global convergence to learn the optimal controller. Finally, we verify the effectiveness of the proposed learning algorithm and demonstrate its robustness to the probability distribution errors via numerical examples.      
### 64.Are all negatives created equal in contrastive instance discrimination?  [ :arrow_down: ](https://arxiv.org/pdf/2010.06682.pdf)
>  Self-supervised learning has recently begun to rival supervised learning on computer vision tasks. Many of the recent approaches have been based on contrastive instance discrimination (CID), in which the network is trained to recognize two augmented versions of the same instance (a query and positive) while discriminating against a pool of other instances (negatives). The learned representation is then used on downstream tasks such as image classification. Using methodology from MoCo v2 (Chen et al., 2020), we divided negatives by their difficulty for a given query and studied which difficulty ranges were most important for learning useful representations. We found a minority of negatives -- the hardest 5% -- were both necessary and sufficient for the downstream task to reach nearly full accuracy. Conversely, the easiest 95% of negatives were unnecessary and insufficient. Moreover, the very hardest 0.1% of negatives were unnecessary and sometimes detrimental. Finally, we studied the properties of negatives that affect their hardness, and found that hard negatives were more semantically similar to the query, and that some negatives were more consistently easy or hard than we would expect by chance. Together, our results indicate that negatives vary in importance and that CID may benefit from more intelligent negative treatment.      
### 65.Adaptive Testing for Specification Coverage  [ :arrow_down: ](https://arxiv.org/pdf/2010.06674.pdf)
>  Ensuring correctness of cyber-physical systems (CPS) is an extremely challenging task that is in practice often addressed with simulation based testing. Formal specification languages, such as Signal Temporal Logic (STL), are used to mathematically express CPS requirements and thus render the simulation activity more systematic and principled. We propose a novel method for adaptive generation of tests with specification coverage for STL. To achieve this goal, we devise cooperative reachability games that we combine with numerical optimization to create tests that explore the system in a way that exercise various parts of the specification. To the best of our knowledge our approach is the first adaptive testing approach that can be applied directly to MATLAB\texttrademark\; Simulink/Stateflow models. We implemented our approach in a prototype tool and evaluated it on several illustrating examples and a case study from the avionics domain, demonstrating the effectiveness of adaptive testing to (1) incrementally build a test case that reaches a test objective, (2) generate a test suite that increases the specification coverage, and (3) infer what part of the specification is actually implemented.      
### 66.The Role of Damping in Second-Order Dynamical Systems with Applications to Power Grid Stability  [ :arrow_down: ](https://arxiv.org/pdf/2010.06662.pdf)
>  We consider a broad class of second-order dynamical systems and study the role of damping as a system parameter in the stability, hyperbolicity, and bifurcation in such systems. We prove a monotonic effect of damping on the hyperbolicity of the equilibrium points of the corresponding first-order system. This provides a rigorous formulation and theoretical justification for some intuitive notions that damping increases stability. To establish this result, we prove a matrix perturbation result for complex symmetric matrices with positive semidefinite perturbations to their imaginary parts, which may be of independent interest. Furthermore, we establish necessary and sufficient conditions for the breakdown of hyperbolicity of the first-order system under damping variations in terms of observability of a pair of matrices relating damping, inertia, and Jacobian matrices, and propose sufficient conditions for Hopf bifurcation resulting from such hyperbolicity breakdown. The developed theory has significant applications in the stability of electric power systems, which are one of the most complex and important engineering systems. In particular, we characterize the impact of damping on the hyperbolicity of swing equations, which is the fundamental dynamical model of power systems, and demonstrate Hopf bifurcations resulting from damping variations.      
### 67.Motivations and Preliminary Design for Mid-Air Deployment of a Science Rotorcraft on Mars  [ :arrow_down: ](https://arxiv.org/pdf/2010.06630.pdf)
>  Mid-Air Deployment (MAD) of a rotorcraft during Entry, Descent and Landing (EDL) on Mars eliminates the need to carry a propulsion or airbag landing system. This reduces the total mass inside the aeroshell by more than 100 kg and simplifies the aeroshell architecture. MAD's lighter and simpler design is likely to bring the risk and cost associated with the mission down. Moreover, the lighter entry mass enables landing in the Martian highlands, at elevations inaccessible to current EDL technologies. This paper proposes a novel MAD concept for a Mars helicopter. We suggest a minimum science payload package to perform relevant science in the highlands. A variant of the Ingenuity helicopter is proposed to provide increased deceleration during MAD, and enough lift to fly the science payload in the highlands. We show in simulation that the lighter aeroshell results in a lower terminal velocity (30 m/s) at the end of the parachute phase of the EDL, and at higher altitudes than other approaches. After discussing the aerodynamics, controls, guidance, and mechanical challenges associated with deploying at such speed, we propose a backshell architecture that addresses them to release the helicopter in the safest conditions. Finally, we implemented the helicopter model and aerodynamic descent perturbations in the JPL Dynamics and Real-Time Simulation (DARTS)framework. Preliminary performance evaluation indicates landing and helicopter operation scan be achieved up to 5 km MOLA (Mars Orbiter Laser Altimeter reference).      
### 68.Intelligent Reflecting Surface-Assisted Wireless Key Generation for Low-Entropy Environments  [ :arrow_down: ](https://arxiv.org/pdf/2010.06613.pdf)
>  Physical layer key generation is a promising candidate for cryptographic key establishment between two wireless communication parties. It offers information-theoretic security and is an attractive alternative to public-key techniques. Here, the inherent randomness of wireless radio channels is used as a shared entropy source to generate cryptographic key material. However, practical implementations often suffer from static channel conditions which exhibit a limited amount of randomness. In the past, considerable research efforts have been made to address this fundamental limitation. However, current solutions are not generic or require dedicated hardware extensions such as reconfigurable antennas. In this paper, we propose a novel wireless key generation architecture based on randomized channel responses from Intelligent Reflecting Surfaces (IRS). Due to its passive nature, a cooperative IRS is well-suited to provide randomness for conventional low-resource radios. We conduct the first practical studies to successfully demonstrate IRS-based physical-layer key generation with a 2x2 MIMO-OFDM system. In a static environment, using a single subcarrier only, our IRS-assisted prototype system achieves a KGR of 20.75 bps with 9.3% KDR after quantization, while passing standard randomness tests.      
