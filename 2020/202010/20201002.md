# ArXiv eess --Fri, 2 Oct 2020
### 1.Utilizing Transfer Learning and a Customized Loss Function for Optic Disc Segmentation from Retinal Images  [ :arrow_down: ](https://arxiv.org/pdf/2010.00583.pdf)
>  Accurate segmentation of the optic disc from a retinal image is vital to extracting retinal features that may be highly correlated with retinal conditions such as glaucoma. In this paper, we propose a deep-learning based approach capable of segmenting the optic disc given a high-precision retinal fundus image. Our approach utilizes a UNET-based model with a VGG16 encoder trained on the ImageNet dataset. This study can be distinguished from other studies in the customization made for the VGG16 model, the diversity of the datasets adopted, the duration of disc segmentation, the loss function utilized, and the number of parameters required to train our model. Our approach was tested on seven publicly available datasets augmented by a dataset from a private clinic that was annotated by two Doctors of Optometry through a web portal built for this purpose. We achieved an accuracy of 99.78\% and a Dice coefficient of 94.73\% for a disc segmentation from a retinal image in 0.03 seconds. The results obtained from comprehensive experiments demonstrate the robustness of our approach to disc segmentation of retinal images obtained from different sources.      
### 2.High Accuracy VLP based on Image Sensor using Error Calibration Method  [ :arrow_down: ](https://arxiv.org/pdf/2010.00529.pdf)
>  In this paper, visible light positioning (VLP) where the receiver adopts a commercial image sensor is considered. We firstly analyze the theoretical limits and error source of the VLP system using image sensor. And then, we develop a VLP positioning model on the receiver movement and further propose two novel error calibration algorithms, namely Rotation Calibration Method and dispersion circle calibration method. The rotation algorithm estimates the rotation center in the image instead of treating the image center as the rotation center, leading to reduced positioning error. For the dispersion circle, it can offset the shift error created by the conversation between different coordinate during the positioning calculation. According to the experimental results, the average positioning error of the proposed methods can be reduced to 0.82cm, which achieve state-of-the-art in the VLP field.      
### 3.Channel Modeling for IRS-Assisted FSO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.00528.pdf)
>  In this paper, we develop an analytical channel model for intelligent reflecting surface (IRS)-assisted free space optical (FSO) systems. Unlike IRS-assisted radio frequency systems, where it is typically assumed that a plane wave is incident on the IRS, in FSO systems, the incident wave is a Gaussian beam with non-uniform power distribution across the IRS. Taking this property into account, we develop an analytical end-to-end channel model for IRS-assisted FSO systems based on the Huygens-Fresnel principle. Our analytical model reveals the impact of the size, position, orientation, and phase-shift configuration of the IRS on the end-to-end channel. Furthermore, we show that results obtained based on geometric optics under the far-field approximation are only valid for a specific range of IRS-receiver lens distances depending on the IRS size, incident beam width, and wavelength. Simulation results validate the accuracy of the proposed analytical results for the FSO beam reflected from the IRS and compare the bit error rate performance obtained for the proposed analytical channel model with that obtained for geometric optics under the far-field approximation.      
### 4.On the Compression of Translation Operator Tensors in FMM-FFT-Accelerated SIE Simulators via Tensor Decompositions  [ :arrow_down: ](https://arxiv.org/pdf/2010.00520.pdf)
>  Tensor decomposition methodologies are proposed to reduce the memory requirement of translation operator tensors arising in the fast multipole method-fast Fourier transform (FMM-FFT)-accelerated surface integral equation (SIE) simulators. These methodologies leverage Tucker, hierarchical Tucker (H-Tucker), and tensor train (TT) decompositions to compress the FFT'ed translation operator tensors stored in three-dimensional (3D) and four-dimensional (4D) array formats. Extensive numerical tests are performed to demonstrate the memory saving achieved by and computational overhead introduced by these methodologies for different simulation parameters. Numerical results show that the H-Tucker-based methodology for 4D array format yields the maximum memory saving while Tucker-based methodology for 3D array format introduces the minimum computational overhead. For many practical scenarios, all methodologies yield a significant reduction in the memory requirement of translation operator tensors while imposing negligible/acceptable computational overhead.      
### 5.A Wavelet-CNN-LSTM Model for Tailings Pond Risk Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2010.00518.pdf)
>  Tailings ponds are places for storing industrial waste. Once the tailings pond collapses, the villages nearby will be destroyed and the harmful chemicals will cause serious environmental pollution. There is an urgent need for a reliable forecast model, which could investigate the variation trend of stability coefficient of tailing dam and issue early warnings. In order to fill the gap, this work presents an hybrid network - Wavelet-based Long-Short-Term Memory (LSTM) and Convolutional Neural Network (CNN), namely Wavelet-CNN-LSTM netwrok for predicting the tailings pond risk. Firstly, we construct the especial nonlinear data processing method to impute the missing value with the numerical inversion (NI) method, which combines correlation analysis, sensitivity analysis, and Random Forest (RF) algorithms. Secondly, a new forecasting model was proposed to monitor the saturation line, which is the lifeline of the tailings pond and can directly reflect the stability of the tailings pond. After using the discrete wavelet transform (DWT) to decompose the original saturation line data into 4-layer wavelets and de-noise the data, the CNN was used to identify and learn the spatial structures in the time series, followed by LSTM cells for detecting the long-short-term dependence. Finally, different experiments were conducted to evaluate the effectiveness of our model by comparing it with other state-of-the-art algorithms. The results show that Wavelet-CNN-LSTM achieves the best score both in mean absolute percentage error (MAPE), root-mean-square error (RMSE) and R 2 .      
### 6.On the Advantage of Coherent LoRa Detection in the Presence of Interference  [ :arrow_down: ](https://arxiv.org/pdf/2010.00507.pdf)
>  It has been shown that the coherent detection of LoRa signals only provides marginal gains of around 0.7 dB on the additive white Gaussian noise (AWGN) channel. However, ALOHA-based massive Internet of Things systems, including LoRa, often operate in the interference-limited regime. Therefore, in this work, we examine the performance of the LoRa modulation with coherent detection in the presence of interference from another LoRa user with the same spreading factor. We derive rigorous symbol- and frame error rate expressions as well as bounds and approximations for evaluating the error rates. The error rates predicted by these approximations are compared against error rates found by Monte Carlo simulations and shown to be very accurate. We also compare the performance of LoRa with coherent and non-coherent receivers and we show that the coherent detection of LoRa is significantly more beneficial in interference scenarios than in the presence of only AWGN. For example, we show that coherent detection leads to a 2.5 dB gain over the standard non-coherent detection for a signal-to-interference ratio (SIR) of 3 dB and up to a 10 dB gain for an SIR of 0 dB.      
### 7.High Quality Remote Sensing Image Super-Resolution Using Deep Memory Connected Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.00472.pdf)
>  Single image super-resolution is an effective way to enhance the spatial resolution of remote sensing image, which is crucial for many applications such as target detection and image classification. However, existing methods based on the neural network usually have small receptive fields and ignore the image detail. We propose a novel method named deep memory connected network (DMCN) based on a convolutional neural network to reconstruct high-quality super-resolution images. We build local and global memory connections to combine image detail with environmental information. To further reduce parameters and ease time-consuming, we propose downsampling units, shrinking the spatial size of feature maps. We test DMCN on three remote sensing datasets with different spatial resolution. Experimental results indicate that our method yields promising improvements in both accuracy and visual performance over the current state-of-the-art.      
### 8.The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep Learning to Radio Frequency Applications  [ :arrow_down: ](https://arxiv.org/pdf/2010.00432.pdf)
>  While deep machine learning technologies are now pervasive in state-of-the-art image recognition and natural language processing applications, only in recent years have these technologies started to sufficiently mature in applications related to wireless communications. In particular, recent research has shown deep machine learning to be an enabling technology for cognitive radio applications as well as a useful tool for supplementing expertly defined algorithms for spectrum sensing applications such as signal detection, estimation, and classification (termed here as Radio Frequency Machine Learning, or RFML). A major driver for the usage of deep machine learning in the context of wireless communications is that little, to no, a priori knowledge of the intended spectral environment is required, given that there is an abundance of representative data to facilitate training and evaluation. However, in addition to this fundamental need for sufficient data, there are other key considerations, such as trust, security, and hardware/software issues, that must be taken into account before deploying deep machine learning systems in real-world wireless communication applications. This paper provides an overview and survey of prior work related to these major research considerations. In particular, we present their unique considerations in the RFML application space, which are not generally present in the image, audio, and/or text application spaces.      
### 9.Forced Variational Integrators for the Formation Control of Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.00425.pdf)
>  Formation control of autonomous agents can be seen as a physical system of individuals interacting with local potentials, and whose evolution can be described by a Lagrangian function. In this paper, we construct and implement forced variational integrators for the formation control of autonomous agents modeled by double integrators. In particular, we provide an accurate numerical integrator with a lower computational cost than traditional solutions. We find error estimations for the rate of the energy dissipated along with the agents' motion to achieve desired formations. Consequently, this permits to provide sufficient conditions on the simulation's time step for the convergence of discrete formation control systems such as the consensus problem in discrete systems. We present practical applications such as the rapid estimation of regions of attraction to desired shapes in distance-based formation control.      
### 10.A computationally efficient reconstruction algorithm for circular cone-beam computed tomography using shallow neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.00421.pdf)
>  Circular cone-beam (CCB) Computed Tomography (CT) has become an integral part of industrial quality control, materials science and medical imaging. The need to acquire and process each scan in a short time naturally leads to trade-offs between speed and reconstruction quality, creating a need for fast reconstruction algorithms capable of creating accurate reconstructions from limited data. <br>In this paper we introduce the Neural Network Feldkamp-Davis-Kress (NN-FDK) algorithm. This algorithm adds a machine learning component to the FDK algorithm to improve its reconstruction accuracy while maintaining its computational efficiency. Moreover, the NN-FDK algorithm is designed such that it has low training data requirements and is fast to train. This ensures that the proposed algorithm can be used to improve image quality in high throughput CT scanning settings, where FDK is currently used to keep pace with the acquisition speed using readily available computational resources. <br>We compare the NN-FDK algorithm to two standard CT reconstruction algorithms and to two popular deep neural networks trained to remove reconstruction artifacts from the 2D slices of an FDK reconstruction. We show that the NN-FDK reconstruction algorithm is substantially faster in computing a reconstruction than all the tested alternative methods except for the standard FDK algorithm and we show it can compute accurate CCB CT reconstructions in cases of high noise, a low number of projection angles or large cone angles. Moreover, we show that the training time of an NN-FDK network is orders of magnitude lower than the considered deep neural networks, with only a slight reduction in reconstruction accuracy.      
### 11.Centrality in Epidemic Networks with Time-Delay: A Decision-Support Framework for Epidemic Containment  [ :arrow_down: ](https://arxiv.org/pdf/2010.00398.pdf)
>  During an epidemic, infectious individuals might not be detectable until some time after becoming infected. The studies show that carriers with mild or no symptoms are the main contributors to the transmission of a virus within the population. The average time it takes to develop the symptoms causes a delay in the spread dynamics of the disease. When considering the influence of delay on the disease propagation in epidemic networks, depending on the value of the time-delay and the network topology, the peak of epidemic could be considerably different in time, duration, and intensity. Motivated by the recent worldwide outbreak of the COVID-19 virus and the topological extent in which this virus has spread over the course of a few months, this study aims to highlight the effect of time-delay in the progress of such infectious diseases in the meta-population networks rather than individuals or a single population. In this regard, the notions of epidemic network centrality in terms of the underlying interaction graph of the network, structure of the uncertainties, and symptom development duration are investigated to establish a centrality-based analysis of the disease evolution. A traffic volume convex optimization method is then developed to control the outbreak by identifying the sub-populations with the highest centrality and then isolating them while maintaining the same overall traffic volume (motivated by economic considerations) in the meta-population level. The numerical results, along with the theoretical expectations, highlight the impact of time-delay as well as the importance of considering the worst-case scenarios in investigating the most effective methods of epidemic containment.      
### 12.SESQA: semi-supervised learning for speech quality assessment  [ :arrow_down: ](https://arxiv.org/pdf/2010.00368.pdf)
>  Automatic speech quality assessment is an important, transversal task whose progress is hampered by the scarcity of human annotations, poor generalization to unseen recording conditions, and a lack of flexibility of existing approaches. In this work, we tackle these problems with a semi-supervised learning approach, combining available annotations with programmatically generated data, and using 3 different optimization criteria together with 5 complementary auxiliary tasks. Our results show that such a semi-supervised approach can cut the error of existing methods by more than 36%, while providing additional benefits in terms of reusable features or auxiliary outputs. Improvement is further corroborated with an out-of-sample test showing promising generalization capabilities.      
### 13.Distributed two-time-scale methods over clustered networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.00355.pdf)
>  In this paper, we consider consensus problems over a network of nodes, where the network is divided into a number of clusters. We are interested in the case where the communication topology within each cluster is dense as compared to the sparse communication across the clusters. Moreover, each cluster has one leader which can communicate with other leaders in different clusters. The goal of the nodes is to agree at some common value under the presence of communication delays across the clusters. <br>Our main contribution is to propose a novel distributed two-time-scale consensus algorithm, which pertains to the separation in network topology of clustered networks. In particular, one scale is to model the dynamic of the agents in each cluster, which is much faster (due to the dense communication) than the scale describing the slowly aggregated evolution between the clusters (due to the sparse communication). We prove the convergence of the proposed method in the presence of uniform, but possibly arbitrarily large, communication delays between the leaders. In addition, we provided an explicit formula for the convergence rate of such algorithm, which characterizes the impact of delays and the network topology. Our results shows that after a transient time characterized by the topology of each cluster, the convergence of the two-time-scale consensus method only depends on the connectivity of the leaders. Finally, we validate our theoretical results by a number of numerical simulations on different clustered networks.      
### 14.Reinforcement Learning Using Expectation Maximization Based Guided Policy Search for Stochastic Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2010.00304.pdf)
>  Guided policy search algorithms have been proven to work with incredible accuracy for not only controlling a complicated dynamical system, but also learning optimal policies from various unseen instances. One assumes true nature of the states in almost all of the well known policy search and learning algorithms. This paper deals with a trajectory optimization procedure for an unknown dynamical system subject to measurement noise using expectation maximization and extends it to learning (optimal) policies which have less noise because of lower variance in the optimal trajectories. Theoretical and empirical evidence of learnt optimal policies of the new approach is depicted in comparison to some well known baselines which are evaluated on an autonomous system with widely used performance metrics.      
### 15.Improving spatial domain based image formation through compressed sensing  [ :arrow_down: ](https://arxiv.org/pdf/2010.00295.pdf)
>  In this paper, we improve image reconstruction in a single-pixel scanning system by selecting an detector optimal field of view. Image reconstruction is based on compressed sensing and image quality is compared to interpolated staring arrays. The image quality comparisons use a "dead leaves" data set, Bayesian estimation and the Peak-Signal-to-Noise Ratio (PSNR) measure. <br>Compressed sensing is explored as an interpolation algorithm and shows with high probability an improved performance compared to Lanczos interpolation. Furthermore, multi-level sampling in a single-pixel scanning system is simulated by dynamically altering the detector field of view. It was shown that multi-level sampling improves the distribution of the Peak-Signal-to-Noise Ratio. <br>We further explore the expected sampling level distributions and PSNR distributions for multi-level sampling. The PSNR distribution indicates that there is a small set of levels which will improve image quality over interpolated staring arrays. We further conclude that multi-level sampling will outperform single-level uniform random sampling on average.      
### 16.Encrypted control for networked systems -- An illustrative introduction and current challenges  [ :arrow_down: ](https://arxiv.org/pdf/2010.00268.pdf)
>  Cloud computing and distributed computing are becoming ubiquitous in many modern control systems such as smart grids, building automation, robot swarms or intelligent transportation systems. Compared to "isolated" control systems, the advantages of cloud-based and distributed control systems are, in particular, resource pooling and outsourcing, rapid scalability, and high performance. However, these capabilities do not come without risks. In fact, the involved communication and processing of sensitive data via public networks and on third-party platforms promote, among other cyberthreats, eavesdropping and manipulation of data. Encrypted control addresses this security gap and provides confidentiality of the processed data in the entire control loop. This paper presents a tutorial-style introduction to this young but emerging field in the framework of secure control for networked dynamical systems.      
### 17.Deep Group-wise Variational Diffeomorphic Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2010.00231.pdf)
>  Deep neural networks are increasingly used for pair-wise image registration. We propose to extend current learning-based image registration to allow simultaneous registration of multiple images. To achieve this, we build upon the pair-wise variational and diffeomorphic VoxelMorph approach and present a general mathematical framework that enables both registration of multiple images to their geodesic average and registration in which any of the available images can be used as a fixed image. In addition, we provide a likelihood based on normalized mutual information, a well-known image similarity metric in registration, between multiple images, and a prior that allows for explicit control over the viscous fluid energy to effectively regularize deformations. We trained and evaluated our approach using intra-patient registration of breast MRI and Thoracic 4DCT exams acquired over multiple time points. Comparison with Elastix and VoxelMorph demonstrates competitive quantitative performance of the proposed method in terms of image similarity and reference landmark distances at significantly faster registration.      
### 18.An Enhanced Energy Management System Including a Real-Time Load-Redistribution Threat Analysis Tool and Cyber-Physical SCED  [ :arrow_down: ](https://arxiv.org/pdf/2010.00223.pdf)
>  It is possible to launch undetectable load-redistribution (LR) attacks against power systems, even in systems with protection schemes. Therefore, detecting LR attacks in power systems and establishing a corrective action to provide secured operating points are imperative. In this paper, we develop a systematic real-time LR threat analysis (RTLRTA) tool, which can flag LR attacks and identify all affected transmission assets. Since attackers might use random deviations to create LR attacks, we introduce an optimization model to generate random LR attacks. Hence, we can determine accurate thresholds for our detection index and test the tool's functionality when there are random LR attacks. Additionally, based on an estimation for the actual loads in the post-attack stage, we design a set of physical line flow security constraints (PLFSCs) and add it to the security-constrained economic dispatch (SCED) model. We call the new model cyber-physical SCED (CPSCED), which can appropriately respond to the identified LR attacks and provide secured dispatch points. We generate multiple scenarios of random LR attacks and noise errors for different target lines in the $2383$-bus Polish test system to validate our proposed methods' accuracy and functionality in detecting LR attacks and responding to them.      
### 19.Robust Stochastic Optimal Control for Multivariable Dynamical Systems Using Expectation Maximization  [ :arrow_down: ](https://arxiv.org/pdf/2010.00207.pdf)
>  Trajectory optimization is a fundamental stochastic optimal control problem. This paper deals with a trajectory optimization approach for unknown complicated systems subjected to stochastic sensor noise. The proposed methodology assimilates the benefits of conventional optimal control procedure with the advantages of maximum likelihood approaches to deliver a novel iterative trajectory optimization paradigm to be called as Stochastic Optimal Control - Expectation Maximization (SOC-EM). This trajectory optimization procedure exhibits theoretical results which prove that the optimal policy parameters produced by the maximum likelihood technique produce better performance in terms of reduction of cumulative cost-to-go and less stochasticity in the states and actions. Furthermore, the paper provides empirical results which support the superiority of the new technique when applied to a system in presence of measurement noise, compared to some of widely known and extensively employed methodologies.      
### 20.Fast Uplink Grant-Free NOMA with Sinusoidal Spreading Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2010.00199.pdf)
>  Uplink (UL) dominated sporadic transmission and stringent latency requirement of massive machine type communication (mMTC) forces researchers to abandon complicated grant-acknowledgment based legacy networks. UL grant-free non-orthogonal multiple access (NOMA) provides an array of features which can be harnessed to efficiently solve the problem of massive random connectivity and latency. Because of the inherent sparsity in user activity pattern in mMTC, the trend of existing literature specifically revolves around compressive sensing based multi user detection (CS-MUD) and Bayesian framework paradigm which employs either random or Zadoff-Chu spreading sequences for non-orthogonal multiple access. In this work, we propose sinusoidal code as candidate spreading sequences. We show that, sinusoidal codes allow some non-iterative algorithms to be employed in context of active user detection, channel estimation and data detection in a UL grant-free mMTC system. This relaxes the requirement of several impractical assumptions considered in the state-of-art algorithms with added advantages of performance guarantees and lower computational cost. Extensive simulation results validate the performance potential of sinusoidal codes in realistic mMTC environments.      
### 21.BiLiMO: Bit-Limited MIMO Radar via Task-Based Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2010.00195.pdf)
>  Recent years have witnessed growing interest in reduced cost radar systems operating with low power. Multiple-input multiple-output (MIMO) radar technology is known to achieve high performance sensing by probing with multiple orthogonal waveforms. However, implementing a low cost low power MIMO radar is challenging. One of the reasons for this difficulty stems from the increased cost and power consumption required by analog-to-digital convertors (ADCs) in acquiring the multiple waveforms at the radar receiver. In this work we study reduced cost MIMO radar receivers restricted to operate with low resolution ADCs. We design bit-limited MIMO radar (BiLiMO) receivers which are capable of accurately recovering their targets while operating under strict resolution constraints. This is achieved by applying an additional analog filter to the acquired waveforms, and designing the overall hybrid analog-digital system to facilitate target identification using task-based quantization methods. In particular, we exploit the fact that the target parameters can be recovered from a compressed representation of the received waveforms. We thus tune the acquisition system to recover this representation with minimal distortion, from which the targets can be extracted in digital, and characterize the achievable error in identifying the targets. Our numerical results demonstrate that the proposed BiLiMO receiver operating with a bit budget of one bit per sample achieves target recovery performance which approaches that of costly MIMO radars operating with unlimited resolution ADCs, while substantially outperforming MIMO receivers operating only in the digital domain under the same bit limitations.      
### 22.System Design and Analysis for Energy-Efficient Passive UAV Radar Imaging System using Illuminators of Opportunity  [ :arrow_down: ](https://arxiv.org/pdf/2010.00179.pdf)
>  Unmanned ariel vehicle (UAV) can provide superior flexibility and cost-efficiency for modern radar imaging systems, which is an ideal platform for advanced remote sensing applications using synthetic aperture radar (SAR) technology. In this paper, an energy-efficient passive UAV radar imaging system using illuminators of opportunity is first proposed and investigated. Equipped with a SAR receiver, the UAV platform passively reuses the backscattered signal of the target scene from an external illuminator, such as SAR satellite, GNSS or ground-based stationary commercial illuminators, and achieves bi-static SAR imaging and data communication. The system can provide instant accessibility to the radar image of the interested targets with enhanced platform concealment, which is an essential tool for stealth observation and scene monitoring. The mission concept and system block diagram are first presented with justifications on the advantages of the system. Then, the prospective imaging performance and system feasibility are analyzed for the typical illuminators based on signal and spatial resolution model. With different illuminators, the proposed system can achieve distinct imaging performance, which offers more alternatives for various mission requirements. A set of mission performance evaluators is established to quantitatively assess the capability of the system in a comprehensive manner, including UAV navigation, passive SAR imaging and communication. Finally, the validity of the proposed performance evaluators are verified by numerical simulations.      
### 23.DEEPMIR: A DEEP convolutional neural network for differential detection of cerebral Microbleeds and IRon deposits in MRI  [ :arrow_down: ](https://arxiv.org/pdf/2010.00148.pdf)
>  Background: Cerebral microbleeds (CMBs) and non-hemorrhage iron deposits in the basal ganglia have been associated with brain aging, vascular disease and neurodegenerative disorders. Recent advances using quantitative susceptibility mapping (QSM) make it possible to differentiate iron content from mineralization in-vivo using magnetic resonance imaging (MRI). However, automated detection of such lesions is still challenging, making quantification in large cohort bases studies rather limited. Purpose: Development of a fully automated method using deep learning for detecting CMBs and basal ganglia iron deposits using multimodal MRI. Materials and Methods: We included a convenience sample of 24 participants from the MESA cohort and used T2-weighted images, susceptibility weighted imaging (SWI), and QSM to segment the lesions. We developed a protocol for simultaneous manual annotation of CMBs and non-hemorrhage iron deposits in the basal ganglia, which resulted in defining the gold standard. This gold standard was then used to train a deep convolution neural network (CNN) model. Specifically, we adapted the U-Net model with a higher number of resolution layers to be able to detect small lesions such as CMBs from standard resolution MRI which are used in cohort-based studies. The detection performance was then evaluated using the cross-validation principle in order to ensure generalization of the results. Results: With multi-class CNN models, we achieved an average sensitivity and precision of about 0.8 and 0.6, respectively for detecting CMBs. The same framework detected non-hemorrhage iron deposits reaching an average sensitivity and precision of about 0.8. Conclusions: Our results showed that deep learning could automate the detection of small vessel disease lesions and including multimodal MR data such as QSM can improve the detection of CMB and non-hemorrhage iron deposits.      
### 24.Event-Independent Network for Polyphonic Sound Event Localization and Detection  [ :arrow_down: ](https://arxiv.org/pdf/2010.00140.pdf)
>  Polyphonic sound event localization and detection is not only detecting what sound events are happening but localizing corresponding sound sources. This series of tasks was first introduced in DCASE 2019 Task 3. In 2020, the sound event localization and detection task introduces additional challenges in moving sound sources and overlapping-event cases, which include two events of the same type with two different direction-of-arrival (DoA) angles. In this paper, a novel event-independent network for polyphonic sound event localization and detection is proposed. Unlike the two-stage method we proposed in DCASE 2019 Task 3, this new network is fully end-to-end. Inputs to the network are first-order Ambisonics (FOA) time-domain signals, which are then fed into a 1-D convolutional layer to extract acoustic features. The network is then split into two parallel branches. The first branch is for sound event detection (SED), and the second branch is for DoA estimation. There are three types of predictions from the network, SED predictions, DoA predictions, and event activity detection (EAD) predictions that are used to combine the SED and DoA features for on-set and off-set estimation. All of these predictions have the format of two tracks indicating that there are at most two overlapping events. Within each track, there could be at most one event happening. This architecture introduces a problem of track permutation. To address this problem, a frame-level permutation invariant training method is used. Experimental results show that the proposed method can detect polyphonic sound events and their corresponding DoAs. Its performance on the Task 3 dataset is greatly increased as compared with that of the baseline method.      
### 25.Light Field Compression by Residual CNN Assisted JPEG  [ :arrow_down: ](https://arxiv.org/pdf/2010.00062.pdf)
>  Light field (LF) imaging has gained significant attention due to its recent success in 3-dimensional (3D) displaying and rendering as well as augmented and virtual reality usage. Nonetheless, because of the two extra dimensions, LFs are much larger than conventional images. We develop a JPEG-assisted learning-based technique to reconstruct an LF from a JPEG bitstream with a bit per pixel ratio of 0.0047 on average. For compression, we keep the LF's center view and use JPEG compression with 50\% quality. Our reconstruction pipeline consists of a small JPEG enhancement network (JPEG-Hance), a depth estimation network (Depth-Net), followed by view synthesizing by warping the enhanced center view. Our pipeline is significantly faster than using video compression on pseudo-sequences extracted from an LF, both in compression and decompression, while maintaining effective performance. We show that with a 1\% compression time cost and 18x speedup for decompression, our methods reconstructed LFs have better structural similarity index metric (SSIM) and comparable peak signal-to-noise ratio (PSNR) compared to the state-of-the-art video compression techniques used to compress LFs.      
### 26.Sampling possible reconstructions of undersampled acquisitions in MR imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.00042.pdf)
>  Undersampling the k-space during MR acquisitions saves time, however results in an ill-posed inversion problem, leading to an infinite set of images as possible solutions. Traditionally, this is tackled as a reconstruction problem by searching for a single "best" image out of this solution set according to some chosen regularization or prior. This approach, however, misses the possibility of other solutions and hence ignores the uncertainty in the inversion process. In this paper, we propose a method that instead returns multiple images which are possible under the acquisition model and the chosen prior. To this end, we introduce a low dimensional latent space and model the posterior distribution of the latent vectors given the acquisition data in k-space, from which we can sample in the latent space and obtain the corresponding images. We use a variational autoencoder for the latent model and the Metropolis adjusted Langevin algorithm for the sampling. This approach allows us to obtain multiple possible images and capture the uncertainty in the inversion process under the used prior. We evaluate our method on images from the Human Connectome Project dataset as well as in-house measured multi-coil images and compare to two different methods. The results indicate that the proposed method is capable of producing images that match the ground truth in regions where acquired k-space data is informative and construct different possible reconstructions, which show realistic structural variations, in regions where acquired k-space data is not informative. <br>Keywords: Magnetic Resonance image reconstruction, uncertainty estimation, inverse problems, sampling, MCMC, deep learning, unsupervised learning.      
### 27.Robustness Analysis of Neural Networks via Efficient Partitioning: Theory and Applications in Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.00540.pdf)
>  Neural networks (NNs) are now routinely implemented on systems that must operate in uncertain environments, but the tools for formally analyzing how this uncertainty propagates to NN outputs are not yet commonplace. Computing tight bounds on NN output sets (given an input set) provides a measure of confidence associated with the NN decisions and is essential to deploy NNs on safety-critical systems. Recent works approximate the propagation of sets through nonlinear activations or partition the uncertainty set to provide a guaranteed outer bound on the set of possible NN outputs. However, the bound looseness causes excessive conservatism and/or the computation is too slow for online analysis. This paper unifies propagation and partition approaches to provide a family of robustness analysis algorithms that give tighter bounds than existing works for the same amount of computation time (or reduced computational effort for a desired accuracy level). Moreover, we provide new partitioning techniques that are aware of their current bound estimates and desired boundary shape (e.g., lower bounds, weighted $\ell_\infty$-ball, convex hull), leading to further improvements in the computation-tightness tradeoff. The paper demonstrates the tighter bounds and reduced conservatism of the proposed robustness analysis framework with examples from model-free RL and forward kinematics learning.      
### 28.Performance of Intelligent Reconfigurable Surface-Based Wireless Communications Using QAM Signaling  [ :arrow_down: ](https://arxiv.org/pdf/2010.00519.pdf)
>  Intelligent reconfigurable surface (IRS) is being seen as a promising technology for 6G wireless networks. The IRS can reconfigure the wireless propagation environment, which results in significant performance improvement of wireless communications. In this paper, we analyze the performance of bandwidth-efficient quadrature amplitude modulation (QAM) techniques for IRS-assisted wireless communications over Rayleigh fading channels. New closed-form expressions of the generic average symbol error rate (ASER) for rectangular QAM, square QAM and cross QAM schemes are derived. Moreover, simplified expressions of the ASER for low signal-to-noise-ratio (SNR) and high SNR regions are also presented, which are useful to provide insights analytically. We comprehensively analyze the impact of modulation parameters and the number of IRS elements employed. We also verify our theoretical results through simulations. Our results demonstrate that employing IRS significantly enhances the ASER performance in comparison to additive white Gaussian noise channel at a low SNR regime. Thus, IRS-assisted wireless communications can be a promising candidate for various low powered communication applications such as internet-of-things (IoT).      
### 29.Mini-DDSM: Mammography-based Automatic Age Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2010.00494.pdf)
>  Age estimation has attracted attention for its various medical applications. There are many studies on human age estimation from biomedical images. However, there is no research done on mammograms for age estimation, as far as we know. The purpose of this study is to devise an AI-based model for estimating age from mammogram images. Due to lack of public mammography data sets that have the age attribute, we resort to using a web crawler to download thumbnail mammographic images and their age fields from the public data set; the Digital Database for Screening Mammography. The original images in this data set unfortunately can only be retrieved by a software which is broken. Subsequently, we extracted deep learning features from the collected data set, by which we built a model using Random Forests regressor to estimate the age automatically. The performance assessment was measured using the mean absolute error values. The average error value out of 10 tests on random selection of samples was around 8 years. In this paper, we show the merits of this approach to fill up missing age values. We ran logistic and linear regression models on another independent data set to further validate the advantage of our proposed work. This paper also introduces the free-access Mini-DDSM data set.      
### 30.FSD50K: an Open Dataset of Human-Labeled Sound Events  [ :arrow_down: ](https://arxiv.org/pdf/2010.00475.pdf)
>  Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset---its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems' benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.      
### 31.Learning Set Functions that are Sparse in Non-Orthogonal Fourier Bases  [ :arrow_down: ](https://arxiv.org/pdf/2010.00439.pdf)
>  Many applications of machine learning on discrete domains, such as learning preference functions in recommender systems or auctions, can be reduced to estimating a set function that is sparse in the Fourier domain. In this work, we present a new family of algorithms for learning Fourier-sparse set functions. They require at most $nk - k \log_2 k + k$ queries (set function evaluations), under mild conditions on the Fourier coefficients, where $n$ is the size of the ground set and $k$ the number of non-zero Fourier coefficients. In contrast to other work that focused on the orthogonal Walsh-Hadamard transform, our novel algorithms operate with recently introduced non-orthogonal Fourier transforms that offer different notions of Fourier-sparsity. These naturally arise when modeling, e.g., sets of items forming substitutes and complements. We demonstrate effectiveness on several real-world applications.      
### 32.Medical imaging data structure extended to multiple modalities and anatomical regions  [ :arrow_down: ](https://arxiv.org/pdf/2010.00434.pdf)
>  Brain Imaging Data Structure (BIDS) allows the user to organise brain imaging data into a clear and easy standard directory structure. BIDS is widely supported by the scientific community and is considered a powerful standard for management. The original BIDS is limited to images or data related to the brain. Medical Imaging Data Structure (MIDS) was therefore conceived with the objective of extending this methodology to other anatomical regions and other types of imaging systems in these areas.      
### 33.A Direct-Indirect Hybridization Approach to Control-Limited DDP  [ :arrow_down: ](https://arxiv.org/pdf/2010.00411.pdf)
>  Optimal control is a widely used tool for synthesizing motions and controls for user-defined tasks under physical constraints. A common approach is to formulate it using direct multiple-shooting and then to use off-the-shelf nonlinear programming solvers that can easily handle arbitrary constraints on the controls and states. However, these methods are not fast enough for many robotics applications such as real-time humanoid motor control. Exploiting the sparse structure of optimal control problem, such as in Differential DynamicProgramming (DDP), has proven to significantly boost the computational efficiency, and recent works have been focused on handling arbitrary constraints. Despite that, DDP has been associated with poor numerical convergence, particularly when considering long time horizons. One of the main reasons is due to system instabilities and poor warm-starting (only controls). This paper presents control-limited Feasibility-driven DDP (Box-FDDP), a solver that incorporates a direct-indirect hybridization of the control-limited DDP algorithm. Concretely, the forward and backward passes handle feasibility and control limits. We showcase the impact and importance of our method on a set of challenging optimal control problems against the Box-DDP and squashing-function approach.      
### 34.Quantum Annealing Approaches to the Phase-Unwrapping Problem in Synthetic-Aperture Radar Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.00220.pdf)
>  The focus of this work is to explore the use of quantum annealing solvers for the problem of phase unwrapping of synthetic aperture radar (SAR) images. Although solutions to this problem exist based on network programming, these techniques do not scale well to larger-sized images. Our approach involves formulating the problem as a quadratic unconstrained binary optimization (QUBO) problem, which can be solved using a quantum annealer. Given that present embodiments of quantum annealers remain limited in the number of qubits they possess, we decompose the problem into a set of subproblems that can be solved individually. These individual solutions are close to optimal up to an integer constant, with one constant per sub-image. In a second phase, these integer constants are determined as a solution to yet another QUBO problem. We test our approach with a variety of software-based QUBO solvers and on a variety of images, both synthetic and real. Additionally, we experiment using D-Wave Systems's quantum annealer, the D-Wave 2000Q. The software-based solvers obtain high-quality solutions comparable to state-of-the-art phase-unwrapping solvers. We are currently working on optimally mapping the problem onto the restricted topology of the quantum annealer to improve the quality of the solution.      
### 35.Robust Model-Free Learning and Control without Prior Knowledge  [ :arrow_down: ](https://arxiv.org/pdf/2010.00204.pdf)
>  We present a simple model-free control algorithm that is able to robustly learn and stabilize an unknown discrete-time linear system with full control and state feedback subject to arbitrary bounded disturbance and noise sequences. The controller does not require any prior knowledge of the system dynamics, disturbances, or noise, yet it can guarantee robust stability and provides asymptotic and worst-case bounds on the state and input trajectories. To the best of our knowledge, this is the first model-free algorithm that comes with such robust stability guarantees without the need to make any prior assumptions about the system. We would like to highlight the new convex geometry-based approach taken towards robust stability analysis which served as a key enabler in our results. We will conclude with simulation results that show that despite the generality and simplicity, the controller demonstrates good closed-loop performance.      
### 36.Training Data Augmentation for Deep Learning RF Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.00178.pdf)
>  Applications of machine learning are subject to three major components that contribute to the final performance metrics. Within the specifics of neural networks, and deep learning specifically, the first two are the architecture for the model being trained and the training approach used. This work focuses on the third component, the data being used during training. The questions that arise are then "what is in the data" and "what within the data matters?" Looking into the Radio Frequency Machine Learning (RFML) field of Modulation Classification, the use of synthetic, captured, and augmented data are examined and compared to provide insights about the quantity and quality of the available data presented. In general, all three data types have useful contributions to a final application, but captured data germane to the intended use case will always provide more significant information and enable the greatest performance. Despite the benefit of captured data, the difficulties that arise from collection often make the quantity of data needed to achieve peak performance impractical. This paper helps quantify the balance between real and synthetic data, offering concrete examples where training data is parametrically varied in size and source.      
### 37.C-Arm Non-Circular Orbits: Geometric Calibration, Image Quality, and Avoidance of Metal Artifacts  [ :arrow_down: ](https://arxiv.org/pdf/2010.00175.pdf)
>  Metal artifacts present a frequent challenge to cone-beam CT (CBCT) in image-guided surgery, obscuring visualization of metal instruments and adjacent anatomy. Recent advances in mobile C-arm systems have enabled 3D imaging capacity with non-circular orbits. We extend a previously proposed metal artifacts avoidance (MAA) method to reduce the influence of metal artifacts by prospectively defining a non-circular orbit that avoids metal-induced biases in projection domain. Accurate geometric calibration is an important challenge to accurate 3D image reconstruction for such orbits. We investigate the performance of interpolation-based calibration from a library of circular orbits for any non-circular orbit. We apply the method to non-circular scans acquired for MAA, which involves: (i) coarse 3D localization of metal objects via only two scout views using an end-to-end trained neural network; (ii) calculation of the metal-induced x-ray spectral shift for all possible views; and (iii) identification of the non-circular orbit that minimizes the variations in spectral shift. Non-circular orbits with interpolation-based geometric calibration yielded reasonably accurate 3D image reconstruction. The end-to-end neural network accurately localized metal implants with just two scout views even in complex anatomical scenes, improving Dice coefficient by ~42% compared to a more conventional cascade of separately trained U-nets. In a spine phantom with pedicle screw instrumentation, non-circular orbits identified by the MAA method reduced the magnitude of metal "blomming" artifacts (apparent width of the screw shaft) in CBCT reconstructions by ~70%. The proposed imaging and calibration methods present a practical means to improve image quality in mobile C-arm CBCT by identifying non-circular scan protocols that improve sampling and reduce metal-induced biases in the projection data.      
### 38.GeoD: Consensus-based Geodesic Distributed Pose Graph Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2010.00156.pdf)
>  We present a consensus-based distributed pose graph optimization algorithm for obtaining an estimate of the 3D translation and rotation of each pose in a pose graph, given noisy relative measurements between poses. The algorithm, called GeoD, implements a continuous time distributed consensus protocol to minimize the geodesic pose graph error. GeoD is distributed over the pose graph itself, with a separate computation thread for each node in the graph, and messages are passed only between neighboring nodes in the graph. We leverage tools from Lyapunov theory and multi-agent consensus to prove the convergence of the algorithm. We identify two new consistency conditions sufficient for convergence: pairwise consistency of relative rotation measurements, and minimal consistency of relative translation measurements. GeoD incorporates a simple one step distributed initialization to satisfy both conditions. We demonstrate GeoD on simulated and real world SLAM datasets. We compare to a centralized pose graph optimizer with an optimality certificate (SE-Sync) and a Distributed Gauss-Seidel (DGS) method. On average, GeoD converges 20 times more quickly than DGS to a value with 3.4 times less error when compared to the global minimum provided by SE-Sync. GeoD scales more favorably with graph size than DGS, converging over 100 times faster on graphs larger than 1000 poses. Lastly, we test GeoD on a multi-UAV vision-based SLAM scenario, where the UAVs estimate their pose trajectories in a distributed manner using the relative poses extracted from their on board camera images. We show qualitative performance that is better than either the centralized SE-Sync or the distributed DGS methods.      
### 39.Self-Guided Multiple Instance Learning for Weakly Supervised Disease Classification and Localization in Chest Radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2010.00127.pdf)
>  The lack of fine-grained annotations hinders the deployment of automated diagnosis systems, which require human-interpretable justification for their decision process. In this paper, we address the problem of weakly supervised identification and localization of abnormalities in chest radiographs. To that end, we introduce a novel loss function for training convolutional neural networks increasing the \emph{localization confidence} and assisting the overall \emph{disease identification}. The loss leverages both image- and patch-level predictions to generate auxiliary supervision. Rather than forming strictly binary from the predictions as done in previous loss formulations, we create targets in a more customized manner, which allows the loss to account for possible misclassification. We show that the supervision provided within the proposed learning scheme leads to better performance and more precise predictions on prevalent datasets for multiple-instance learning as well as on the NIH~ChestX-Ray14 benchmark for disease recognition than previously used losses.      
### 40.CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG  [ :arrow_down: ](https://arxiv.org/pdf/2010.00104.pdf)
>  Electrocardiogram (ECG) is the electrical measurement of cardiac activity, whereas Photoplethysmogram (PPG) is the optical measurement of volumetric changes in blood circulation. While both signals are used for heart rate monitoring, from a medical perspective, ECG is more useful as it carries additional cardiac information. Despite many attempts toward incorporating ECG sensing in smartwatches or similar wearable devices for continuous and reliable cardiac monitoring, PPG sensors are the main feasible sensing solution available. In order to tackle this problem, we propose CardioGAN, an adversarial model which takes PPG as input and generates ECG as output. The proposed network utilizes an attention-based generator to learn local salient features, as well as dual discriminators to preserve the integrity of generated data in both time and frequency domains. Our experiments show that the ECG generated by CardioGAN provides more reliable heart rate measurements compared to the original input PPG, reducing the error from 9.74 beats per minute (measured from the PPG) to 2.89 (measured from the generated ECG).      
### 41.Channel Estimation for Reconfigurable Intelligent Surface-Assisted Wireless Communications Considering Doppler Effect  [ :arrow_down: ](https://arxiv.org/pdf/2010.00101.pdf)
>  In wireless systems aided by reconfigurable intelligent surfaces (RISs), channel state information (CSI) plays a pivotal role in achieving the performance gain of RISs. In this letter, we propose a novel Doppler shift adjustment (DSA) method to improve CSI estimation accuracy in the presence of motions at the communication ends for RIS-assisted systems. Specifically, the estimated channel frequency response is transformed into the time domain to facilitate phase shift adjustment, which is then converted back to the frequency domain. The proposed DSA necessitates only one more symbol which incurs negligible extra overhead compared with the large number of symbols originally required for channel estimation. Further, low-complexity channel estimation and DSA schemes are designed for approximately single-path scenarios applicable to millimeter-wave and terahertz systems. Simulation results demonstrate the effectiveness of the proposed DSA and channel estimation methods.      
### 42.Massive Uncoordinated Multiple Access for Beyond 5G  [ :arrow_down: ](https://arxiv.org/pdf/2010.00098.pdf)
>  Existing wireless communication systems have been mainly designed to provide substantial gain in terms of data rates. However, 5G and Beyond will depart from this scheme, with the objective not only to provide services with higher data rates. One of the main goals is to support massive machine-type communications (mMTC) in the IoT applications. Supporting massive uplink (UP) communications for devices with sporadic traffic pattern and short-packet size, as it is in many mMTC use cases, is a challenging task, particularly when the control signaling is not negligible in size compared to the payload. Also, channel estimation is challenging for sporadic and short-packet transmission due to the limited number of employed pilots. In this paper, a new UP multiple access (MA) scheme is proposed for mMTC, which can support a large number of uncoordinated IoT devices with short-packet and sporadic traffic. The proposed UP MA scheme removes the overheads associated with the device identifier as well as pilots related to channel estimation. An alternative mechanism for device identification is proposed, where a unique spreading code is dedicated to each IoT device. This unique code is simultaneously used for the spreading purpose and device identification. Two IoT device identification algorithms which employ sparse signal reconstruction methods are proposed to determine the active IoT devices prior to data detection. Specifically, the BIC model order selection method is employed to develop an IoT device identification algorithm for unknown and time-varying probability of device activity. Our proposed MA scheme benefits from a non-coherent multiuser detection algorithm based on machine learning to enable data detection without a priori knowledge on channel state information. The effectiveness of the proposed MA scheme for known and unknown probability of activity is supported by simulation results.      
### 43.Stage-wise Conservative Linear Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2010.00081.pdf)
>  We study stage-wise conservative linear stochastic bandits: an instance of bandit optimization, which accounts for (unknown) safety constraints that appear in applications such as online advertising and medical trials. At each stage, the learner must choose actions that not only maximize cumulative reward across the entire time horizon but further satisfy a linear baseline constraint that takes the form of a lower bound on the instantaneous reward. For this problem, we present two novel algorithms, stage-wise conservative linear Thompson Sampling (SCLTS) and stage-wise conservative linear UCB (SCLUCB), that respect the baseline constraints and enjoy probabilistic regret bounds of order O(\sqrt{T} \log^{3/2}T) and O(\sqrt{T} \log T), respectively. Notably, the proposed algorithms can be adjusted with only minor modifications to tackle different problem variations, such as constraints with bandit-feedback, or an unknown sequence of baseline actions. We discuss these and other improvements over the state-of-the-art. For instance, compared to existing solutions, we show that SCLTS plays the (non-optimal) baseline action at most O(\log{T}) times (compared to O(\sqrt{T})). Finally, we make connections to another studied form of safety constraints that takes the form of an upper bound on the instantaneous reward. While this incurs additional complexity to the learning process as the optimal action is not guaranteed to belong to the safe set at each round, we show that SCLUCB can properly adjust in this setting via a simple modification.      
### 44.The MIDI Degradation Toolkit: Symbolic Music Augmentation and Correction  [ :arrow_down: ](https://arxiv.org/pdf/2010.00059.pdf)
>  In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a "degraded" version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difficulty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK's degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community.      
### 45.A general approach for identifying hierarchical symmetry constraints for analog circuit layout  [ :arrow_down: ](https://arxiv.org/pdf/2010.00051.pdf)
>  Analog layout synthesis requires some elements in the circuit netlist to be matched and placed symmetrically. However, the set of symmetries is very circuit-specific and a versatile algorithm, applicable to a broad variety of circuits, has been elusive. This paper presents a general methodology for the automated generation of symmetry constraints, and applies these constraints to guide automated layout synthesis. While prior approaches were restricted to identifying simple symmetries, the proposed method operates hierarchically and uses graph-based algorithms to extract multiple axes of symmetry within a circuit. An important ingredient of the algorithm is its ability to identify arrays of repeated structures. In some circuits, the repeated structures are not perfect replicas and can only be found through approximate graph matching. A fast graph neural network based methodology is developed for this purpose, based on evaluating the graph edit distance. The utility of this algorithm is demonstrated on a variety of circuits, including operational amplifiers, data converters, equalizers, and low-noise amplifiers.      
### 46.Spectral Decomposition in Deep Networks for Segmentation of Dynamic Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2010.00003.pdf)
>  Dynamic contrast-enhanced magnetic resonance imaging (DCE- MRI) is a widely used multi-phase technique routinely used in clinical practice. DCE and similar datasets of dynamic medical data tend to contain redundant information on the spatial and temporal components that may not be relevant for detection of the object of interest and result in unnecessarily complex computer models with long training times that may also under-perform at test time due to the abundance of noisy heterogeneous data. This work attempts to increase the training efficacy and performance of deep networks by determining redundant information in the spatial and spectral components and show that the performance of segmentation accuracy can be maintained and potentially improved. Reported experiments include the evaluation of training/testing efficacy on a heterogeneous dataset composed of abdominal images of pediatric DCE patients, showing that drastic data reduction (higher than 80%) can preserve the dynamic information and performance of the segmentation model, while effectively suppressing noise and unwanted portion of the images.      
