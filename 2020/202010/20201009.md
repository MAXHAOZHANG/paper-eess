# ArXiv eess --Fri, 9 Oct 2020
### 1.Estimating load points of a motor-pump system using pressure and inverter drive data  [ :arrow_down: ](https://arxiv.org/pdf/2010.04090.pdf)
>  We propose a novel method for the estimation of rotor position, speed, and torque of a motor-pump system consisting of a progressive cavity pump (PCP) driven by an induction motor which operates under V/f open-loop control. We compute the speed and rotor position of the PCP by applying a phase locked loop to the pressure signal at the pressure side of the pump. An extended Kalman filter is used to estimate the torque of the PCP based on the speed, effective value of the stator current of the induction motor and a nonlinear motor model. Furthermore, we derive a tractable condition under which the convergence of the observer is guaranteed. We use a laboratory experiment to verify our results.      
### 2.Classifying Songs with EEG  [ :arrow_down: ](https://arxiv.org/pdf/2010.04087.pdf)
>  This research study aims to use machine learning methods to characterize the EEG response to music. Specifically, we investigate how resonance in the EEG response correlates with individual aesthetic enjoyment. Inspired by the notion of musical processing as resonance, we hypothesize that the intensity of an aesthetic experience is based on the degree to which a participants EEG entrains to the perceptual input. To test this and other hypotheses, we have built an EEG dataset from 20 subjects listening to 12 two minute-long songs in random order. After preprocessing and feature construction, we used this dataset to train and test multiple machine learning models.      
### 3.Displaced Sensor Automotive Radar Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.04085.pdf)
>  Displaced automotive sensor imaging exploits joint processing of the data acquired from multiple radar units, each of which may have limited individual resources, to enhance the localization accuracy. Prior works either consider perfect synchronization among the sensors, employ single antenna radars, entail high processing cost, or lack performance analyses. Contrary to these works, we develop a displaced multiple-input multiple-output (MIMO) frequency-modulated continuous-wave (FMCW) radar signal model under coarse synchronization with only frame-level alignment. We derive Bayesian performance bounds for the common automotive radar processing modes such as point-cloud-based fusion as well as raw-signal-based non-coherent and coherent imaging. For the non-coherent mode, which offers a compromise between low computational load and improved localization, we exploit the block sparsity of range profiles for signal reconstruction to avoid direct computational imaging with massive data. For the high-resolution coherent imaging, we develop a method that automatically estimates the synchronization error and performs displaced radar imaging by exploiting sparsity-driven recovery models. Our extensive numerical experiments demonstrate these advantages. Our proposed non-coherent processing of displaced MIMO FMCW radars improves position estimation by an order over the conventional point-cloud fusion.      
### 4.Nonlinear Model Predictive Control of Permanent Magnet Synchronous Generators in DC Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2010.04069.pdf)
>  A new strategy is proposed to control interior permanent magnet generators in dc microgrids interfaced through an active rectifier. The controller design is based on the decomposition of the system dynamics into slow and fast modes using singular perturbation theory. An inner current controller is developed based on output regulation techniques and an outer voltage controller is proposed using Nonlinear Model Predictive Control (NMPC). The NMPC regulates the dc bus voltage and minimizes the ac side losses. Simulation results are then presented based on realistic conditions for aircraft power systems.      
### 5.Regularized Compression of MRI Data: Modular Optimization of Joint Reconstruction and Coding  [ :arrow_down: ](https://arxiv.org/pdf/2010.04065.pdf)
>  The Magnetic Resonance Imaging (MRI) processing chain starts with a critical acquisition stage that provides raw data for reconstruction of images for medical diagnosis. This flow usually includes a near-lossless data compression stage that enables digital storage and/or transmission in binary formats. In this work we propose a framework for joint optimization of the MRI reconstruction and lossy compression, producing compressed representations of medical images that achieve improved trade-offs between quality and bit-rate. Moreover, we demonstrate that lossy compression can even improve the reconstruction quality compared to settings based on lossless compression. Our method has a modular optimization structure, implemented using the alternating direction method of multipliers (ADMM) technique and the state-of-the-art image compression technique (BPG) as a black-box module iteratively applied. This establishes a medical data compression approach compatible with a lossy compression standard of choice. A main novelty of the proposed algorithm is in the total-variation regularization added to the modular compression process, leading to decompressed images of higher quality without any additional processing at/after the decompression stage. Our experiments show that our regularization-based approach for joint MRI reconstruction and compression often achieves significant PSNR gains between 4 to 9 dB at high bit-rates compared to non-regularized solutions of the joint task. Compared to regularization-based solutions, our optimization method provides PSNR gains between 0.5 to 1 dB at high bit-rates, which is the range of interest for medical image compression.      
### 6.Hierarchical Classification of Pulmonary Lesions: A Large-Scale Radio-Pathomics Study  [ :arrow_down: ](https://arxiv.org/pdf/2010.04049.pdf)
>  Diagnosis of pulmonary lesions from computed tomography (CT) is important but challenging for clinical decision making in lung cancer related diseases. Deep learning has achieved great success in computer aided diagnosis (CADx) area for lung cancer, whereas it suffers from label ambiguity due to the difficulty in the radiological diagnosis. Considering that invasive pathological analysis serves as the clinical golden standard of lung cancer diagnosis, in this study, we solve the label ambiguity issue via a large-scale radio-pathomics dataset containing 5,134 radiological CT images with pathologically confirmed labels, including cancers (e.g., invasive/non-invasive adenocarcinoma, squamous carcinoma) and non-cancer diseases (e.g., tuberculosis, hamartoma). This retrospective dataset, named Pulmonary-RadPath, enables development and validation of accurate deep learning systems to predict invasive pathological labels with a non-invasive procedure, i.e., radiological CT scans. A three-level hierarchical classification system for pulmonary lesions is developed, which covers most diseases in cancer-related diagnosis. We explore several techniques for hierarchical classification on this dataset, and propose a Leaky Dense Hierarchy approach with proven effectiveness in experiments. Our study significantly outperforms prior arts in terms of data scales (6x larger), disease comprehensiveness and hierarchies. The promising results suggest the potentials to facilitate precision medicine.      
### 7.Frequency and Spatial domain based Saliency for Pigmented Skin Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.04022.pdf)
>  Skin lesion segmentation can be rather a challenging task owing to the presence of artifacts, low contrast between lesion and boundary, color variegation, fuzzy skin lesion borders and heterogeneous background in dermoscopy images. In this paper, we propose a simple yet effective saliency-based approach derived in the frequency and spatial domain to detect pigmented skin lesion. Two color models are utilized for the construction of these maps. We suggest a different metric for each color model to design map in the spatial domain via color features. The map in the frequency domain is generated from aggregated images. We adopt a separate fusion scheme to combine salient features in their respective domains. Finally, two-phase saliency integration scheme is devised to combine these maps using pixelwise multiplication. Performance of the proposed method is assessed on PH2 and ISIC 2016 datasets. The outcome of the experiments suggests that the proposed scheme generate better segmentation result as compared to state-of-the-art methods.      
### 8.Spatially-Variant CNN-based Point Spread Function Estimation for Blind Deconvolution and Depth Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2010.04011.pdf)
>  Optical microscopy is an essential tool in biology and medicine. Imaging thin, yet non-flat objects in a single shot (without relying on more sophisticated sectioning setups) remains challenging as the shallow depth of field that comes with high-resolution microscopes leads to unsharp image regions and makes depth localization and quantitative image interpretation difficult. <br>Here, we present a method that improves the resolution of light microscopy images of such objects by locally estimating image distortion while jointly estimating object distance to the focal plane. Specifically, we estimate the parameters of a spatially-variant Point-Spread function (PSF) model using a Convolutional Neural Network (CNN), which does not require instrument- or object-specific calibration. Our method recovers PSF parameters from the image itself with up to a squared Pearson correlation coefficient of 0.99 in ideal conditions, while remaining robust to object rotation, illumination variations, or photon noise. When the recovered PSFs are used with a spatially-variant and regularized Richardson-Lucy deconvolution algorithm, we observed up to 2.1 dB better signal-to-noise ratio compared to other blind deconvolution techniques. Following microscope-specific calibration, we further demonstrate that the recovered PSF model parameters permit estimating surface depth with a precision of 2 micrometers and over an extended range when using engineered PSFs. Our method opens up multiple possibilities for enhancing images of non-flat objects with minimal need for a priori knowledge about the optical setup.      
### 9.Tractography filtering using autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2010.04007.pdf)
>  Current brain white matter fiber tracking techniques show a number of problems, including: generating large proportions of streamlines that do not accurately describe the underlying anatomy; extracting streamlines that are not supported by the underlying diffusion signal; and under-representing some fiber populations, among others. In this paper, we describe a novel unsupervised learning method to filter streamlines from diffusion MRI tractography, and hence, to obtain more reliable tractograms. <br>We show that a convolutional neural network autoencoder provides a straightforward and elegant way to learn a robust representation of brain streamlines, which can be used to filter undesired samples with a nearest neighbor algorithm. Our method, dubbed FINTA (Filtering in Tractography using Autoencoders) comes with several key advantages: training does not need labeled data, as it uses raw tractograms, it is fast and easily reproducible, it does not rely on the input diffusion MRI data, and thus, does not suffer from domain adaptation issues. We demonstrate the ability of FINTA to discriminate between "plausible" and "implausible" streamlines as well as to recover individual streamline group instances from a raw tractogram, from both synthetic and real human brain diffusion MRI tractography data, including partial tractograms. Results reveal that FINTA has a superior filtering performance compared to state-of-the-art methods. <br>Together, this work brings forward a new deep learning framework in tractography based on autoencoders, and shows how it can be applied for filtering purposes. It sets the foundations for opening up new prospects towards more accurate and robust tractometry and connectivity diffusion MRI analyses, which may ultimately lead to improve the imaging of the white matter anatomy.      
### 10.Free annotated data for deep learning in microscopy? A hitchhiker's guide  [ :arrow_down: ](https://arxiv.org/pdf/2010.03988.pdf)
>  In microscopy, the time burden and cost of acquiring and annotating large datasets that many deep learning models take as a prerequisite, often appears to make these methods impractical. Can this requirement for annotated data be relaxed? Is it possible to borrow the knowledge gathered from datasets in other application fields and leverage it for microscopy? Here, we aim to provide an overview of methods that have recently emerged to successfully train learning-based methods in bio-microscopy.      
### 11.Synthesising clinically realistic Chest X-rays using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.03975.pdf)
>  Chest x-rays are one of the most commonly performed medical investigations globally and are vital to identifying a number of conditions. These images are however protected under patient confidentiality and as such require the removal of identifying information as well as ethical clearance to be released. Generative adversarial networks (GANs) are a branch of deep learning which are capable of producing synthetic samples of a desired distribution. Image generation is one such application with recent advances enabling the production of high-resolution images, a feature vital to the utility of x-rays given the scale of various pathologies. We apply the Progressive Growing GAN (PGGAN) to the task of chest x-ray generation with the goal of being able to produce images without any ethical concerns that may be used for medical education or in other machine learning work. We evaluate the properties of the generated x-rays with a practicing radiologist and demonstrate that high-quality, realistic images can be produced with global features consistent with pathologies seen in the NIH dataset. Improvements in the reproduction of small-scale details remains for future work. We train a classification model on the NIH images and evaluate the distribution of disease labels across the generated samples. We find that the model is capable of reproducing all the abnormalities in a similar proportion to the source image distribution as labelled by the classifier. We additionally demonstrate that the latent space can be optimised to produce images of a particular class despite unconditional training, with the model producing related features and complications for the class of interest. We also validate the application of the Fr'echet Inception Distance (FID) to x-ray images and determine that the PGGAN reproduces x-ray images with an FID of 8.02, which is similar to other high resolution tasks.      
### 12.A Variational Auto-Encoder Approach for Image Transmission in Wireless Channel  [ :arrow_down: ](https://arxiv.org/pdf/2010.03967.pdf)
>  Recent advancements in information technology and the widespread use of the Internet have led to easier access to data worldwide. As a result, transmitting data through noisy channels is inevitable. Reducing the size of data and protecting it during transmission from corruption due to channel noises are two classical problems in communication and information theory. Recently, inspired by deep neural networks' success in different tasks, many works have been done to address these two problems using deep learning techniques. <br>In this paper, we investigate the performance of variational auto-encoders and compare the results with standard auto-encoders. Our findings suggest that variational auto-encoders are more robust to channel degradation than auto-encoders. Furthermore, we have tried to excel in the human perceptual quality of reconstructed images by using perception-based error metrics as our network's loss function. To this end, we use the structural similarity index (SSIM) as a perception-based metric to optimize the proposed neural network. Our experiments demonstrate that the SSIM metric visually improves the quality of the reconstructed images at the receiver.      
### 13.High Definition image classification in Geoscience using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.03965.pdf)
>  High Definition (HD) digital photos taken with drones are widely used in the study of Geoscience. However, blurry images are often taken in collected data, and it takes a lot of time and effort to distinguish clear images from blurry ones. In this work, we apply Machine learning techniques, such as Support Vector Machine (SVM) and Neural Network (NN) to classify HD images in Geoscience as clear and blurry, and therefore automate data cleaning in Geoscience. We compare the results of classification based on features abstracted from several mathematical models. Some of the implementation of our machine learning tool is freely available at: <a class="link-external link-https" href="https://github.com/zachgolden/geoai" rel="external noopener nofollow">this https URL</a>.      
### 14.Neurodevelopmental Age Estimation of Infants Using a 3D-Convolutional Neural Network Model based on Fusion MRI Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2010.03963.pdf)
>  The ability to determine if the brain is developing normally is a key component of pediatric neuroradiology and neurology. Brain magnetic resonance imaging (MRI) of infants demonstrates a specific pattern of development beyond simply myelination. While radiologists have used myelination patterns, brain morphology and size characteristics in determining if brain maturity matches the chronological age of the patient, this requires years of experience with pediatric neuroradiology. Due to the lack of standardized criteria, estimation of brain maturity before age three remains fraught with interobserver and intraobserver variability. An objective measure of brain developmental age estimation (BDAE) could be a useful tool in helping physicians identify developmental delay as well as other neurological diseases. We investigated a three-dimensional convolutional neural network (3D CNN) to rapidly classify brain developmental age using common MRI sequences. MRI datasets from normal newborns were obtained from the National Institute of Mental Health Data Archive from birth to 3 years. We developed a BDAE method using T1-weighted, as well as a fusion of T1-weighted, T2-weighted, and proton density (PD) sequences from 112 individual subjects using 3D CNN. We achieved a precision of 94.8% and a recall of 93.5% in utilizing multiple MRI sequences in determining BDAE.      
### 15.Emotion Invariant Speaker Embeddings for Speaker Identification with Emotional Speech  [ :arrow_down: ](https://arxiv.org/pdf/2010.03909.pdf)
>  Emotional state of a speaker is found to have significant effect in speech production, which can deviate speech from that arising from neutral state. This makes identifying speakers with different emotions a challenging task as generally the speaker models are trained using neutral speech. In this work, we propose to overcome this problem by creation of emotion invariant speaker embedding. We learn an extractor network that maps the test embeddings with different emotions obtained using i-vector based system to an emotion invariant space. The resultant test embeddings thus become emotion invariant and thereby compensate the mismatch between various emotional states. The studies are conducted using four different emotion classes from IEMOCAP database. We obtain an absolute improvement of 2.6% in accuracy for speaker identification studies using emotion invariant speaker embedding against average speaker model based framework with different emotions.      
### 16.Classification of Speech with and without Face Mask using Acoustic Features  [ :arrow_down: ](https://arxiv.org/pdf/2010.03907.pdf)
>  The understanding and interpretation of speech can be affected by various external factors. The use of face masks is one such factors that can create obstruction to speech while communicating. This may lead to degradation of speech processing and affect humans perceptually. Knowing whether a speaker wears a mask may be useful for modeling speech for different applications. With this motivation, finding whether a speaker wears face mask from a given speech is included as a task in Computational Paralinguistics Evaluation (ComParE) 2020. We study novel acoustic features based on linear filterbanks, instantaneous phase and long-term information that can capture the artifacts for classification of speech with and without face mask. These acoustic features are used along with the state-of-the-art baselines of ComParE functionals, bag-of-audio-words, DeepSpectrum and auDeep features for ComParE 2020. The studies reveal the effectiveness of acoustic features, and their score level fusion with the ComParE 2020 baselines leads to an unweighted average recall of 73.50% on the test set.      
### 17.HLT-NUS Submission for NIST 2019 Multimedia Speaker Recognition Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2010.03905.pdf)
>  This work describes the speaker verification system developed by Human Language Technology Laboratory, National University of Singapore (HLT-NUS) for 2019 NIST Multimedia Speaker Recognition Evaluation (SRE). The multimedia research has gained attention to a wide range of applications and speaker recognition is no exception to it. In contrast to the previous NIST SREs, the latest edition focuses on a multimedia track to recognize speakers with both audio and visual information. We developed separate systems for audio and visual inputs followed by a score level fusion of the systems from the two modalities to collectively use their information. The audio systems are based on x-vector based speaker embedding, whereas the face recognition systems are based on ResNet and InsightFace based face embeddings. With post evaluation studies and refinements, we obtain an equal error rate (EER) of 0.88% and an actual detection cost function (actDCF) of 0.026 on the evaluation set of 2019 NIST multimedia SRE corpus.      
### 18.CSTNet: A Dual-Branch Convolutional Network for Imaging of Reactive Flows using Chemical Species Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2010.03868.pdf)
>  Chemical Species Tomography (CST) has been widely used for in situ imaging of critical parameters, e.g. species concentration and temperature, in reactive flows. However, even with state-of-the-art computational algorithms the method is limited due to the inherently ill-posed and rank-deficient tomographic data inversion, and by high computational cost. These issues hinder its application for real-time flow diagnosis. To address them, we present here a novel CST-based convolutional neural Network (CSTNet) for high-fidelity, rapid, and simultaneous imaging of species concentration and temperature. CSTNet introduces a shared feature extractor that incorporates the CST measurement and sensor layout into the learning network. In addition, a dual-branch architecture is proposed for image reconstruction with crosstalk decoders that automatically learn the naturally correlated distributions of species concentration and temperature. The proposed CSTNet is validated both with simulated datasets, and with measured data from real flames in experiments using an industry-oriented sensor. Superior performance is found relative to previous approaches, in terms of robustness to measurement noise and millisecond-level computing time. This is the first time, to the best of our knowledge, that a deep learning-based algorithm for CST has been experimentally validated for simultaneous imaging of multiple critical parameters in reactive flows using a low-complexity optical sensor with severely limited number of laser beams.      
### 19.CS-MCNet:A Video Compressive Sensing Reconstruction Network with Interpretable Motion Compensation  [ :arrow_down: ](https://arxiv.org/pdf/2010.03780.pdf)
>  In this paper, a deep neural network with interpretable motion compensation called CS-MCNet is proposed to realize high-quality and real-time decoding of video compressive sensing. Firstly, explicit multi-hypothesis motion compensation is applied in our network to extract correlation information of adjacent frames(as shown in Fig. 1), which improves the recover performance. And then, a residual module further narrows down the gap between reconstruction result and original signal. The overall architecture is interpretable by using algorithm unrolling, which brings the benefits of being able to transfer prior knowledge about the conventional algorithms. As a result, a PSNR of 22dB can be achieved at 64x compression ratio, which is about 4% to 9% better than state-of-the-art methods. In addition, due to the feed-forward architecture, the reconstruction can be processed by our network in real time and up to three orders of magnitude faster than traditional iterative methods.      
### 20.Integrity-Based Path Planning Strategy for Urban Autonomous Vehicular Navigation Using GPS and Cellular Signals  [ :arrow_down: ](https://arxiv.org/pdf/2010.03761.pdf)
>  An integrity-based path planning strategy for autonomous ground vehicles (AGVs) in urban areas is suggested. The vehicle in this paper is assumed to navigate utilizing long-term evolution (LTE) signals in addition to the Global Positioning System (GPS) signals. Given a destination, an optimal path that minimizes the cost function considering both the horizontal protection levels (HPLs) and travel distance is calculated. Besides, the candidate paths in which the HPL or the ratio of nodes with fault signals to total nodes exceeding its threshold are excluded to maintain the reliability of navigation. To predict the faults and HPLs before the vehicle is driven, GPS and LTE pseudoranges along the candidate paths are predicted by utilizing a commercial ray-tracing software and 3D terrain and building maps, which simulates pseudorange biases owing to refections by buildings in an urban environment. The simulation results show that the optimal path chosen from the proposed path planning strategy has the minimum average HPL among the four candidate paths considered.      
### 21.Ultra-wideband free-space optical phase stabilisation  [ :arrow_down: ](https://arxiv.org/pdf/2010.03745.pdf)
>  Free-space optical (FSO) communications has the potential to revolutionize wireless communications due to its advantages of inherent security, high-directionality, high available bandwidth and small physical footprint. The effects of atmospheric turbulence currently limit the performance of FSO communications. In this letter, we demonstrate a system capable of indiscriminately suppressing the atmospheric phase noise encountered by independent optical signals spread over a range of 7.2 THz (encompassing the full optical C-Band), by actively phase stabilizing a primary optical signal at 193.1 THz (1552 nm). We show ~30 dB of indiscriminate phase stabilization over the full range, down to average phase noise at 10 Hz of -39.6 dBc/Hz when using an acousto-optic modulator (AOM) as a Doppler actuator, and -39.9 dBc/Hz when using a fiber-stretcher as group-delay actuator to provide the phase-stabilization system's feedback. We demonstrate that this suppression is limited by the noise of the independent optical signals, and that the expected achievable suppression is more than 40 dB greater, reaching around -90 dB/Hz at 10 Hz. We conclude that 40 Tbps ground-to-space FSO transmission would be made possible with the combination of our stabilization system and other demonstrated technologies.      
### 22.Bone Feature Segmentation in Ultrasound Spine Image with Robustness to Speckle and Regular Occlusion Noise  [ :arrow_down: ](https://arxiv.org/pdf/2010.03740.pdf)
>  3D ultrasound imaging shows great promise for scoliosis diagnosis thanks to its low-costing, radiation-free and real-time characteristics. The key to accessing scoliosis by ultrasound imaging is to accurately segment the bone area and measure the scoliosis degree based on the symmetry of the bone features. The ultrasound images tend to contain many speckles and regular occlusion noise which is difficult, tedious and time-consuming for experts to find out the bony feature. In this paper, we propose a robust bone feature segmentation method based on the U-net structure for ultrasound spine Volume Projection Imaging (VPI) images. The proposed segmentation method introduces a total variance loss to reduce the sensitivity of the model to small-scale and regular occlusion noise. The proposed approach improves 2.3% of Dice score and 1% of AUC score as compared with the u-net model and shows high robustness to speckle and regular occlusion noise.      
### 23.3D Convolutional Sequence to Sequence Model for Vertebral Compression Fractures Identification in CT  [ :arrow_down: ](https://arxiv.org/pdf/2010.03739.pdf)
>  An osteoporosis-related fracture occurs every three seconds worldwide, affecting one in three women and one in five men aged over 50. The early detection of at-risk patients facilitates effective and well-evidenced preventative interventions, reducing the incidence of major osteoporotic fractures. In this study, we present an automatic system for identification of vertebral compression fractures on Computed Tomography images, which are often an undiagnosed precursor to major osteoporosis-related fractures. The system integrates a compact 3D representation of the spine, utilizing a Convolutional Neural Network (CNN) for spinal cord detection and a novel end-to-end sequence to sequence 3D architecture. We evaluate several model variants that exploit different representation and classification approaches and present a framework combining an ensemble of models that achieves state of the art results, validated on a large data set, with a patient-level fracture identification of 0.955 Area Under the Curve (AUC). The system proposed has the potential to support osteoporosis clinical management, improve treatment pathways, and to change the course of one of the most burdensome diseases of our generation.      
### 24.Latent linguistic embedding for cross-lingual text-to-speech and voice conversion  [ :arrow_down: ](https://arxiv.org/pdf/2010.03717.pdf)
>  As the recently proposed voice cloning system, NAUTILUS, is capable of cloning unseen voices using untranscribed speech, we investigate the feasibility of using it to develop a unified cross-lingual TTS/VC system. Cross-lingual speech generation is the scenario in which speech utterances are generated with the voices of target speakers in a language not spoken by them originally. This type of system is not simply cloning the voice of the target speaker, but essentially creating a new voice that can be considered better than the original under a specific framing. By using a well-trained English latent linguistic embedding to create a cross-lingual TTS and VC system for several German, Finnish, and Mandarin speakers included in the Voice Conversion Challenge 2020, we show that our method not only creates cross-lingual VC with high speaker similarity but also can be seamlessly used for cross-lingual TTS without having to perform any extra steps. However, the subjective evaluations of perceived naturalness seemed to vary between target speakers, which is one aspect for future improvement.      
### 25.Array Resource Allocation for Radar and Communication Integration Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.03690.pdf)
>  A radar and communication integration (RCI) system has great flexibility in allocating antenna resources to guarantee both radar and communication performance. This paper considers the array allocation problems for multiple target localization and multiple platforms communication in an RCI network. The objective of array allocation is to maximize the communication capacity for each channel and to minimize the localization error for each target. In this paper, we firstly build a localization and communication model for array allocation in an RCI network. Minorization maximization (MM) is then applied to create surrogate functions for multiple objective optimization problems. The projected gradient descent (PGD) method is further employed to solve two array allocation problems with and without a certain communication capacity constraint. Computer simulations are conducted to evaluate the performance of the proposed algorithms. The results show that the proposed algorithms have improved localization and communication performance after efficiently allocating the array resource in the RCI network.      
### 26.Islanding microgrid with solar and fuel cell for providing load  [ :arrow_down: ](https://arxiv.org/pdf/2010.03686.pdf)
>  Solar and Fuel cell energy resources are two major of Renewable Energy Resources (RES) which is using to power electrical grids. They can provide clean energy and help to reduce greenhouse emissions. RES is changing the structure of electrical systems day by day. They have provided to supply the loads and have distributed systems. The concept of Microgrid has been defined by connecting distributed generation sources and load which can run in islanding and interconnected modes. In this research, solar and fuel cell are used as a source for providing variable loads. They try to deliver defined power to the load. The solar system created by one diode corresponding circuit is given and a Perturbation and observation (P&amp;O) approach is applied to obtain full power from the solar panels. Direct Methanol Fuel Cell Model (DFMC) is another renewable energy resource that is utilized in this microgrid to provide the load. This model has included two Gibbs reactors that considered for the anode and cathode respectively and a splitter among the anode and cathode. The droop controller is used to control the injected power from the resources to the loads. The effectiveness of the designed microgrid is justified in MATLAB/SIMULINK environment      
### 27.Designing Sequence with Minimum PSL Using Chebyshev Distance and its Application for Chaotic MIMO Radar Waveform Design  [ :arrow_down: ](https://arxiv.org/pdf/2010.03674.pdf)
>  Controlling peak side-lobe level (PSL) is of great importance in high-resolution applications of multiple-input multiple-output (MIMO) radars. In this paper, designing sequences with good autocorrelation properties are studied. The PSL of the autocorrelation is regarded as the main merit and is optimized through newly introduced cyclic algorithms, namely; PSL Minimization Quadratic Approach (PMQA), PSL Minimization Algorithm, the smallest Rectangular (PMAR), and PSL Optimization Cyclic Algorithm (POCA). It is revealed that minimizing PSL results in better sequences in terms of autocorrelation side-lobes when compared with traditional integrated side-lobe level (ISL) minimization. In order to improve the performance of these algorithms, fast-randomized Singular Value Decomposition (SVD) is utilized. To achieve waveform design for MIMO radars, this algorithm is applied to the waveform generated from a modified Bernoulli chaotic system. The numerical experiments confirm the superiority of the newly developed algorithms compared to high-performance algorithms in mono-static and MIMO radars.      
### 28.Sliding Mode Control Barrier Function  [ :arrow_down: ](https://arxiv.org/pdf/2010.03673.pdf)
>  This work proposes a sliding mode control barrier function to robustly deal with high relative-degree safety constraints in safety-critical control systems. Stability/tracking objectives, expressed as a nominal control law, and safety constraints, expressed as control barrier functions are unified through quadratic programming. The proposed control framework is numerically validated considering a Furuta pendulum and a magnetic levitation system. For the first system, a linear quadratic regulator is considered as a nominal control law, and a safety constraint is considered to guarantee that the pendulum angular position never exceeds a predetermined value. For the second one, a sliding mode controller is considered as a nominal control law and multiple safety constraints are considered to guarantee that the magnetic levitation system positions never exceed predetermined values. For both systems, we consider high relative-degree safety constraints robust against model uncertainties. The numerical results indicate that the stability/tracking objectives are reached and the safety constraints are respected even with model uncertainties.      
### 29.COHORT: Coordination of Heterogeneous Thermostatically Controlled Loads for Demand Flexibility  [ :arrow_down: ](https://arxiv.org/pdf/2010.03659.pdf)
>  Demand flexibility is increasingly important for power grids. Careful coordination of thermostatically controlled loads (TCLs) can modulate energy demand, decrease operating costs, and increase grid resiliency. We propose a novel distributed control framework for the Coordination Of HeterOgeneous Residential Thermostatically controlled loads (COHORT). COHORT is a practical, scalable, and versatile solution that coordinates a population of TCLs to jointly optimize a grid-level objective, while satisfying each TCL's end-use requirements and operational constraints. To achieve that, we decompose the grid-scale problem into subproblems and coordinate their solutions to find the global optimum using the alternating direction method of multipliers (ADMM). The TCLs' local problems are distributed to and computed in parallel at each TCL, making COHORT highly scalable and privacy-preserving. While each TCL poses combinatorial and non-convex constraints, we characterize these constraints as a convex set through relaxation, thereby making COHORT computationally viable over long planning horizons. After coordination, each TCL is responsible for its own control and tracks the agreed-upon power trajectory with its preferred strategy. In this work, we translate continuous power back to discrete on/off actuation, using pulse width modulation. COHORT is generalizable to a wide range of grid objectives, which we demonstrate through three distinct use cases: generation following, minimizing ramping, and peak load curtailment. In a notable experiment, we validated our approach through a hardware-in-the-loop simulation, including a real-world air conditioner (AC) controlled via a smart thermostat, and simulated instances of ACs modeled after real-world data traces. During the 15-day experimental period, COHORT reduced daily peak loads by an average of 12.5% and maintained comfortable temperatures.      
### 30.pymia: A Python package for data handling and evaluation in deep learning-based medical image analysis  [ :arrow_down: ](https://arxiv.org/pdf/2010.03639.pdf)
>  Background and Objective: Deep learning enables tremendous progress in medical image analysis. One driving force of this progress are open-source frameworks like TensorFlow and PyTorch. However, these frameworks rarely address issues specific to the domain of medical image analysis, such as 3-D data handling and distance metrics for evaluation. pymia, an open-source Python package, tries to address these issues by providing flexible data handling and evaluation independent of the deep learning framework. <br>Methods: The pymia package provides data handling and evaluation functionalities. The data handling allows flexible medical image handling in every commonly used format (e.g., 2-D, 2.5-D, and 3-D; full- or patch-wise). Even data beyond images like demographics or clinical reports can easily be integrated into deep learning pipelines. The evaluation allows stand-alone result calculation and reporting, as well as performance monitoring during training using a vast amount of domain-specific metrics for segmentation, reconstruction, and regression. <br>Results: The pymia package is highly flexible, allows for fast prototyping, and reduces the burden of implementing data handling routines and evaluation methods. While data handling and evaluation are independent of the deep learning framework used, they can easily be integrated into TensorFlow and PyTorch pipelines. The developed package was successfully used in a variety of research projects for segmentation, reconstruction, and regression. <br>Conclusions: The pymia package fills the gap of current deep learning frameworks regarding data handling and evaluation in medical image analysis. It is available at <a class="link-external link-https" href="https://github.com/rundherum/pymia" rel="external noopener nofollow">this https URL</a> and can directly be installed from the Python Package Index using pip install pymia.      
### 31.Integrated Approximate Dynamic Programming and Equivalent Consumption Minimization Strategy for Eco-Driving in a Connected and Automated Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2010.03620.pdf)
>  This paper focuses on the velocity planning and energy management problems for Connected and Automated Vehicles (CAVs) with hybrid electric powertrains. The eco-driving problem is formulated in the spatial domain as a nonlinear dynamic optimization problem, in which information about the upcoming speed limits and road topography is assumed to be known a priori. To solve this problem, a novel Dynamic Programming (DP) based optimization method is proposed, in which a causal Equivalent Consumption Minimization Strategy (ECMS) is embedded. The underlying vehicle model to predict energy consumption over real-world routes is validated using experimental data. <br>Further, a multi-layer hierarchical control architecture is proposed as a pathway to real-time implementation in a vehicle. The DP-ECMS algorithm is introduced for a long-horizon optimization problem, and then adapted for a receding horizon implementation using principles in Approximate Dynamic Programming (ADP). This computationally economical alternative to the traditional DP solution is then benchmarked and evaluated.      
### 32.Electrocardiogram Heartbeat Classification Using Convolutional Neural Networks for the Detection of Cardiac Arrhythmia  [ :arrow_down: ](https://arxiv.org/pdf/2010.04086.pdf)
>  The classification of the electrocardiogram (ECG) signal has a vital impact on identifying heart-related diseases. This can ensure the premature finding of heart disease and the proper selection of the patient's customized treatment. However, the detection of arrhythmia is a challenging task to perform manually. This justifies the necessity of a technique for automatic detection of abnormal heart signals. Therefore, our work is based on the classification of five classes of ECG arrhythmic signals from Physionet's MIT-BIH Arrhythmia Dataset. Artificial Neural Networks (ANN) have demonstrated significant success in ECG signal classification. Our proposed model is a Convolutional Neural Network (CNN) customized to categorize the ECG signals. Our result testifies that the planned CNN model can successfully categorize arrhythmia with an overall accuracy of 95.2%. The average precision and recall of the proposed model are 95.2% and 95.4%, respectively. This model can effectively be used to detect irregularities of heart rhythm at an early stage.      
### 33.Single-molecule orientation localization microscopy II: a performance comparison  [ :arrow_down: ](https://arxiv.org/pdf/2010.04064.pdf)
>  Various techniques have been developed to measure the 2D and 3D positions and 2D and 3D orientations of fluorescent molecules with improved precision over standard epifluorescence microscopes. Due to the challenging signal-to-background ratio in typical single-molecule experiments, it is essential to choose an imaging system optimized for the specific target sample. In this work, we compare the performance of multiple state-of-the-art and commonly used methods for orientation localization microscopy against the fundamental limits of measurement precision. Our analysis reveals optimal imaging methods for various experiment conditions and sample geometries. Interestingly, simple modifications to the standard fluorescence microscope exhibit superior performance in many imaging scenarios.      
### 34.Single-molecule orientation localization microscopy I: fundamental limits  [ :arrow_down: ](https://arxiv.org/pdf/2010.04060.pdf)
>  Precisely measuring the three-dimensional position and orientation of individual fluorophores is challenging due to the substantial photon shot noise in single-molecule experiments. Facing this limited photon budget, numerous techniques have been developed to encode 2D and 3D position and 2D and 3D orientation information into fluorescence images. In this work, we adapt classical and quantum estimation theory and propose a mathematical framework to derive the best possible precision for measuring the position and orientation of dipole-like emitters for any fixed imaging system. We find that it is impossible to design an instrument that achieves the maximum sensitivity limit for measuring all possible rotational motions. Further, our vectorial dipole imaging model shows that the best quantum-limited localization precision is ~4-8% worse than that suggested by a scalar monopole model. Overall, we conclude that no single instrument can be optimized for maximum precision across all possible 2D and 3D localization and orientation measurement tasks.      
### 35.Low-Complexity ZF/MMSE Receivers for MIMO-OTFS Systems With Imperfect CSI  [ :arrow_down: ](https://arxiv.org/pdf/2010.04057.pdf)
>  Orthogonal time-frequency space (OTFS) scheme, which transforms a time and frequency selective channel into an almost non-selective channel in the delay-Doppler domain, establishes reliable wireless communication for high-speed moving devices. This work designs and analyzes low-complexity zero-forcing (LZ) and minimum mean square error (LM) receivers for multiple-input multiple-output (MIMO)-OTFS systems with perfect and imperfect receive channel state information (CSI). The proposed receivers provide exactly the same solution as that of the conventional counterparts, and reduce the complexity by exploiting the doubly-circulant nature of the MIMO-OTFS channel matrix, the block-wise inverse, and Schur complement. We also derive, by exploiting the Taylor expansion and results from random matrix theory, a tight approximation of the post-processing signal-to-noise-plus-interference-ratio (SINR) expressions in closed-form for both LZ and LM receivers. We show that the derived SINR expressions, when averaged over multiple channel realizations, accurately characterize their respective bit error rate (BER) of both perfect and imperfect receive CSI. We numerically show the lower BER and lower complexity of the proposed designs over state-of-the-art exiting solutions.      
### 36.Texture-based Presentation Attack Detection for Automatic Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2010.04038.pdf)
>  Biometric systems are nowadays employed across a broad range of applications. They provide high security and efficiency and, in many cases, are user friendly. Despite these and other advantages, biometric systems in general and Automatic speaker verification (ASV) systems in particular can be vulnerable to attack presentations. The most recent ASVSpoof 2019 competition showed that most forms of attacks can be detected reliably with ensemble classifier-based presentation attack detection (PAD) approaches. These, though, depend fundamentally upon the complementarity of systems in the ensemble. With the motivation to increase the generalisability of PAD solutions, this paper reports our exploration of texture descriptors applied to the analysis of speech spectrogram images. In particular, we propose a common fisher vector feature space based on a generative model. Experimental results show the soundness of our approach: at most, 16 in 100 bona fide presentations are rejected whereas only one in 100 attack presentations are accepted.      
### 37.Learning Partially Observed Linear Dynamical Systems from Logarithmic Number of Samples  [ :arrow_down: ](https://arxiv.org/pdf/2010.04015.pdf)
>  In this work, we study the problem of learning partially observed linear dynamical systems from a single sample trajectory. A major practical challenge in the existing system identification methods is the undesirable dependency of their required sample size on the system dimension: roughly speaking, they presume and rely on sample sizes that scale linearly with respect to the system dimension. Evidently, in high-dimensional regime where the system dimension is large, it may be costly, if not impossible, to collect as many samples from the unknown system. In this paper, we will remedy this undesirable dependency on the system dimension by introducing an $\ell_1$-regularized estimation method that can accurately estimate the Markov parameters of the system, provided that the number of samples scale logarithmically with the system dimension. Our result significantly improves the sample complexity of learning partially observed linear dynamical systems: it shows that the Markov parameters of the system can be learned in the high-dimensional setting, where the number of samples is significantly smaller than the system dimension. Traditionally, the $\ell_1$-regularized estimators have been used to promote sparsity in the estimated parameters. By resorting to the notion of "weak sparsity", we show that, irrespective of the true sparsity of the system, a similar regularized estimator can be used to reduce the sample complexity of learning partially observed linear systems, provided that the true system is inherently stable.      
### 38.Color night vision correlation imaging without an infrared focal plane array  [ :arrow_down: ](https://arxiv.org/pdf/2010.03977.pdf)
>  Night vision is the ability to see in low-light conditions. However, conventional night vision imaging technology is limited by the requisite high-performance infrared focal plane array. In this article, we propose a novel scheme of color night vision imaging without the use of an infrared focal plane array. In the experimental device, the two-wavelength infrared laser beam reflected by the target is modulated by a spatial light modulator, and the output light is detected by a photomultiplier tube. Two infrared night vision images are reconstructed by measuring the second-order intensity correlation function between two light fields. Thus, the processing mode of optical electric detection in conventional night vision imaging is transformed into the processing mode of light field control. Furthermore, two gray images with different spectra are processed to form a color night vision image. We show that a high-quality color night vision image can be obtained by this method.      
### 39.A Survey on Deep Neural Network Compression: Challenges, Overview, and Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2010.03954.pdf)
>  Deep Neural Network (DNN) has gained unprecedented performance due to its automated feature extraction capability. This high order performance leads to significant incorporation of DNN models in different Internet of Things (IoT) applications in the past decade. However, the colossal requirement of computation, energy, and storage of DNN models make their deployment prohibitive on resource constraint IoT devices. Therefore, several compression techniques were proposed in recent years for reducing the storage and computation requirements of the DNN model. These techniques on DNN compression have utilized a different perspective for compressing DNN with minimal accuracy compromise. It encourages us to make a comprehensive overview of the DNN compression techniques. In this paper, we present a comprehensive review of existing literature on compressing DNN model that reduces both storage and computation requirements. We divide the existing approaches into five broad categories, i.e., network pruning, sparse representation, bits precision, knowledge distillation, and miscellaneous, based upon the mechanism incorporated for compressing the DNN model. The paper also discussed the challenges associated with each category of DNN compression techniques. Finally, we provide a quick summary of existing work under each category with the future direction in DNN compression.      
### 40.Population Based Training for Data Augmentation and Regularization in Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.03899.pdf)
>  Varying data augmentation policies and regularization over the course of optimization has led to performance improvements over using fixed values. We show that population based training is a useful tool to continuously search those hyperparameters, within a fixed budget. This greatly simplifies the experimental burden and computational cost of finding such optimal schedules. We experiment in speech recognition by optimizing SpecAugment this way, as well as dropout. It compares favorably to a baseline that does not change those hyperparameters over the course of training, with an 8% relative WER improvement. We obtain 5.18% word error rate on LibriSpeech's test-other.      
### 41.Clinically Verified Hybrid Deep Learning System for Retinal Ganglion Cells Aware Grading of Glaucomatous Progression  [ :arrow_down: ](https://arxiv.org/pdf/2010.03872.pdf)
>  Objective: Glaucoma is the second leading cause of blindness worldwide. Glaucomatous progression can be easily monitored by analyzing the degeneration of retinal ganglion cells (RGCs). Many researchers have screened glaucoma by measuring cup-to-disc ratios from fundus and optical coherence tomography scans. However, this paper presents a novel strategy that pays attention to the RGC atrophy for screening glaucomatous pathologies and grading their severity. Methods: The proposed framework encompasses a hybrid convolutional network that extracts the retinal nerve fiber layer, ganglion cell with the inner plexiform layer and ganglion cell complex regions, allowing thus a quantitative screening of glaucomatous subjects. Furthermore, the severity of glaucoma in screened cases is objectively graded by analyzing the thickness of these regions. Results: The proposed framework is rigorously tested on publicly available Armed Forces Institute of Ophthalmology (AFIO) dataset, where it achieved the F1 score of 0.9577 for diagnosing glaucoma, a mean dice coefficient score of 0.8697 for extracting the RGC regions and an accuracy of 0.9117 for grading glaucomatous progression. Furthermore, the performance of the proposed framework is clinically verified with the markings of four expert ophthalmologists, achieving a statistically significant Pearson correlation coefficient of 0.9236. Conclusion: An automated assessment of RGC degeneration yields better glaucomatous screening and grading as compared to the state-of-the-art solutions. Significance: An RGC-aware system not only screens glaucoma but can also grade its severity and here we present an end-to-end solution that is thoroughly evaluated on a standardized dataset and is clinically validated for analyzing glaucomatous pathologies.      
### 42.Cascaded WLAN-FWA Networking and Computing Architecture for Pervasive In-Home Healthcare  [ :arrow_down: ](https://arxiv.org/pdf/2010.03805.pdf)
>  Pervasive healthcare is a promising assisted-living solution for chronic patients. However, current cutting-edge communication technologies are not able to strictly meet the requirements of these applications, especially in the case of life-threatening events. To bridge this gap, this paper proposes a new architecture to support indoor healthcare monitoring, with a focus on epileptic patients. Several novel elements are introduced. The first element is the cascading of a WLAN and a cellular network, where IEEE 802.11ax is used for the wireless local area network to collect physiological and environmental data in-home and 5G-enabled Fixed Wireless Access links transfer them to a remote hospital. The second element is the extension of the network slicing concept to the WLAN, and the introduction of two new slice types to support both regular monitoring and emergency handling. Moreover, the inclusion of local computing capabilities at the WLAN router, together with a mobile edge computing resource, represents a further architectural enhancement. Local computation is required to trigger not only health-related alarms, but also the network slicing change in case of emergency: in fact, proper radio resource scheduling is necessary for the cascaded networks to handle healthcare traffic together with other promiscuous everyday communication services. Numerical results demonstrate the effectiveness of the proposed approach while highlighting the performance gain achieved with respect to baseline solutions.      
### 43.Intensity interferometry-based 3D imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.03797.pdf)
>  The development of single-photon counting detectors and arrays has made tremendous steps in recent years, not the least because of various new applications in, e.g., LIDAR devices. In this work, a 3D imaging device based on real thermal light intensity interferometry is presented. By using gated SPAD technology, a basic 3D scene is imaged in reasonable measurement time. Compared to conventional approaches, the proposed synchronized photon counting allows using more light modes to enhance 3D ranging performance. Advantages like robustness to atmospheric scattering or autonomy by exploiting external light sources can make this ranging approach interesting for future applications.      
### 44.Vrengt: A Shared Body-Machine Instrument for Music-Dance Performance  [ :arrow_down: ](https://arxiv.org/pdf/2010.03779.pdf)
>  This paper describes the process of developing a shared instrument for music-dance performance, with a particular focus on exploring the boundaries between standstill vs motion, and silence vs sound. The piece Vrengt grew from the idea of enabling a true partnership between a musician and a dancer, developing an instrument that would allow for active co-performance. Using a participatory design approach, we worked with sonification as a tool for systematically exploring the dancer's bodily expressions. The exploration used a "spatiotemporal matrix", with a particular focus on sonic microinteraction. In the final performance, two Myo armbands were used for capturing muscle activity of the arm and leg of the dancer, together with a wireless headset microphone capturing the sound of breathing. In the paper we reflect on multi-user instrument paradigms, discuss our approach to creating a shared instrument using sonification as a tool for the sound design, and reflect on the performers' subjective evaluation of the instrument.      
### 45.Tatum-Level Drum Transcription Based on a Convolutional Recurrent Neural Network with Language Model-Based Regularized Training  [ :arrow_down: ](https://arxiv.org/pdf/2010.03749.pdf)
>  This paper describes a neural drum transcription method that detects from music signals the onset times of drums at the $\textit{tatum}$ level, where tatum times are assumed to be estimated in advance. In conventional studies on drum transcription, deep neural networks (DNNs) have often been used to take a music spectrogram as input and estimate the onset times of drums at the $\textit{frame}$ level. The major problem with such frame-to-frame DNNs, however, is that the estimated onset times do not often conform with the typical tatum-level patterns appearing in symbolic drum scores because the long-term musically meaningful structures of those patterns are difficult to learn at the frame level. To solve this problem, we propose a regularized training method for a frame-to-tatum DNN. In the proposed method, a tatum-level probabilistic language model (gated recurrent unit (GRU) network or repetition-aware bi-gram model) is trained from an extensive collection of drum scores. Given that the musical naturalness of tatum-level onset times can be evaluated by the language model, the frame-to-tatum DNN is trained with a regularizer based on the pretrained language model. The experimental results demonstrate the effectiveness of the proposed regularized training method.      
### 46.An Audio-Video Deep and Transfer Learning Framework for Multimodal Emotion Recognition in the wild  [ :arrow_down: ](https://arxiv.org/pdf/2010.03692.pdf)
>  In this paper, we present our contribution to ABAW facial expression challenge. We report the proposed system and the official challenge results adhering to the challenge protocol. Using end-to-end deep learning and benefiting from transfer learning approaches, we reached a validation set challenge performance measure of 56.56%.      
### 47.Domain Adversarial Neural Networks for Dysarthric Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.03623.pdf)
>  Speech recognition systems have improved dramatically over the last few years, however, their performance is significantly degraded for the cases of accented or impaired speech. This work explores domain adversarial neural networks (DANN) for speaker-independent speech recognition on the UAS dataset of dysarthric speech. The classification task on 10 spoken digits is performed using an end-to-end CNN taking raw audio as input. The results are compared to a speaker-adaptive (SA) model as well as speaker-dependent (SD) and multi-task learning models (MTL). The experiments conducted in this paper show that DANN achieves an absolute recognition rate of 74.91% and outperforms the baseline by 12.18%. Additionally, the DANN model achieves comparable results to the SA model's recognition rate of 77.65%. We also observe that when labelled dysarthric speech data is available DANN and MTL perform similarly, but when they are not DANN performs better than MTL.      
