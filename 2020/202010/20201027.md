# ArXiv eess --Tue, 27 Oct 2020
### 1.On The Joint Effects of HPA Nonlinearities and IQ Imbalance On Mixed RF/FSO Cooperative Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.13739.pdf)
>  In this work, we provide a framework analysis of dual-hop hybrid Millimeter Wave Radio Frequency (RF)/Free Space Optical (FSO) MIMO relaying system. The source is equipped with multiple antennas and employs conjugate beamforming while the destination consists of multiple apertures with selection combining. The system also consists of a relay operating at amplify-and-forward mode. The RF channels are subject to Nakagami-m fading while the optical links experience the MÃ¡laga distribution. In addition, we introduce the impairments to the relay and receiver. In fact, the relay is impaired by the High Power Amplifier (HPA) nonlinearities while the receiver suffers from the In phase and Quadrature Imbalance. Moreover, we assume two types of HPA nonlinearities impairments called Soft Envelope Limiter (SEL) and Traveling Wave Tube Amplifier (TWTA). Closed-forms of the outage probability, the bit error probability, and the ergodic capacity are derived. Capitalizing on these performances, we derive the high SNR asymptotes to unpack insightful metrics such as the diversity gain. We also address the impacts of some key factors on the system performance such as the impairments, the interferers, the number of antennas and apertures and the pointing errors, etc. Finally, the analytical expressions are confirmed by Monte Carlo simulation.      
### 2.Dyslexia detection from EEG signals using SSA component correlation and Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.13731.pdf)
>  Objective dyslexia diagnosis is not a straighforward task since it is traditionally performed by means of the intepretation of different behavioural tests. Moreover, these tests are only applicable to readers. This way, early diagnosis requires the use of specific tasks not only related to reading. Thus, the use of Electroencephalography (EEG) constitutes an alternative for an objective and early diagnosis that can be used with pre-readers. In this way, the extraction of relevant features in EEG signals results crucial for classification. However, the identification of the most relevant features is not straighforward, and predefined statistics in the time or frequency domain are not always discriminant enough. On the other hand, classical processing of EEG signals based on extracting EEG bands frequency descriptors, usually make some assumptions on the raw signals that could cause indormation loosing. In this work we propose an alternative for analysis in the frequency domain based on Singluar Spectrum Analysis (SSA) to split the raw signal into components representing different oscillatory modes. Moreover, correlation matrices obtained for each component among EEG channels are classfied using a Convolutional Neural network.      
### 3.A Hierarchical Attack Identification Method for Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.13719.pdf)
>  Many autonomous control systems are frequently exposed to attacks, so methods for attack identification are crucial for a safe operation. To preserve the privacy of the subsystems and achieve scalability in large-scale systems, identification algorithms should not require global model knowledge. We analyze a previously presented method for hierarchical attack identification, that is embedded in a distributed control setup for systems of systems with coupled nonlinear dynamics. It is based on the exchange of local sensitivity information and ideas from sparse signal recovery. In this paper, we prove sufficient conditions under which the method is guaranteed to identify all components affected by some unknown attack. Even though a general class of nonlinear dynamic systems is considered, our rigorous theoretical guarantees are applicable to practically relevant examples, which is underlined by numerical experiments with the IEEE~30 bus power system.      
### 4.Self-supervised Wearable-based Activity Recognition by Learning to Forecast Motion  [ :arrow_down: ](https://arxiv.org/pdf/2010.13713.pdf)
>  We propose the use of self-supervised learning for human activity recognition. Our proposed solution consists of two steps. First, the representations of unlabeled input signals are learned by training a deep convolutional neural network to predict the values of accelerometer signals in future time-steps. Then, we freeze the convolution blocks of this network and transfer the weights to our next network aimed at human activity recognition. For this task, we add a number of fully connected layers to the end of the frozen network and train the added layers with labeled accelerometer signals to learn to classify human activities. We evaluate the performance of our method on two publicly available human activity datasets: UCI HAR and MotionSense. The results show that our self-supervised approach outperforms the existing supervised and self-supervised methods to set new state-of-the-art values.      
### 5.Multilabel 12-Lead Electrocardiogram Classification Using Gradient Boosting Tree Ensemble  [ :arrow_down: ](https://arxiv.org/pdf/2010.13712.pdf)
>  The 12-lead electrocardiogram (ECG) is a commonly used tool for detecting cardiac abnormalities such as atrial fibrillation, blocks, and irregular complexes. For the PhysioNet/CinC 2020 Challenge, we built an algorithm using gradient boosted tree ensembles fitted on morphology and signal processing features to classify ECG diagnosis. <br>For each lead, we derive features from heart rate variability, PQRST template shape, and the full signal waveform. We join the features of all 12 leads to fit an ensemble of gradient boosting decision trees to predict probabilities of ECG instances belonging to each class. We train a phase one set of feature importance determining models to isolate the top 1,000 most important features to use in our phase two diagnosis prediction models. We use repeated random sub-sampling by splitting our dataset of 43,101 records into 100 independent runs of 85:15 training/validation splits for our internal evaluation results. <br>Our methodology generates us an official phase validation set score of 0.476 and test set score of -0.080 under the team name, CVC, placing us 36 out of 41 in the rankings.      
### 6.Optimizing Coverage and Capacity in Cellular Networks using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.13710.pdf)
>  Wireless cellular networks have many parameters that are normally tuned upon deployment and re-tuned as the network changes. Many operational parameters affect reference signal received power (RSRP), reference signal received quality (RSRQ), signal-to-interference-plus-noise-ratio (SINR), and, ultimately, throughput. In this paper, we develop and compare two approaches for maximizing coverage and minimizing interference by jointly optimizing the transmit power and downtilt (elevation tilt) settings across sectors. To evaluate different parameter configurations offline, we construct a realistic simulation model that captures geographic correlations. Using this model, we evaluate two optimization methods: deep deterministic policy gradient (DDPG), a reinforcement learning (RL) algorithm, and multi-objective Bayesian optimization (BO). Our simulations show that both approaches significantly outperform random search and converge to comparable Pareto frontiers, but that BO converges with two orders of magnitude fewer evaluations than DDPG. Our results suggest that data-driven techniques can effectively self-optimize coverage and capacity in cellular networks.      
### 7.Learning from Heterogeneous EEG Signals with Differentiable Channel Reordering  [ :arrow_down: ](https://arxiv.org/pdf/2010.13694.pdf)
>  We propose CHARM, a method for training a single neural network across inconsistent input channels. Our work is motivated by Electroencephalography (EEG), where data collection protocols from different headsets result in varying channel ordering and number, which limits the feasibility of transferring trained systems across datasets. Our approach builds upon attention mechanisms to estimate a latent reordering matrix from each input signal and map input channels to a canonical order. CHARM is differentiable and can be composed further with architectures expecting a consistent channel ordering to build end-to-end trainable classifiers. We perform experiments on four EEG classification datasets and demonstrate the efficacy of CHARM via simulated shuffling and masking of input channels. Moreover, our method improves the transfer of pre-trained representations between datasets collected with different protocols.      
### 8.Deep Low-rank plus Sparse Network for Dynamic MR Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.13677.pdf)
>  In dynamic MR imaging, L+S decomposition, or robust PCA equivalently, has achieved stunning performance. However, the selection of parameters of L+S is empirical, and the acceleration rate is limited, which are the common failings of iterative CS-MRI reconstruction methods. Many deep learning approaches were proposed to address these issues, but few of them used the low-rank prior. In this paper, a model-based low-rank plus sparse network, dubbed as L+S-Net, is proposed for dynamic MR reconstruction. In particular, we use an alternating linearized minimization method to solve the optimization problem with low-rank and sparse regularization. A learned soft singular value thresholding is introduced to make sure the clear separation of L component and S component. Then the iterative steps is unrolled into a network whose regularization parameters are learnable. Experiments on retrospective and prospective cardiac cine dataset show that the proposed model outperforms the state-of-the-art CS and existing deep learning methods.      
### 9.Social learning under inferential attacks  [ :arrow_down: ](https://arxiv.org/pdf/2010.13660.pdf)
>  A common assumption in the social learning literature is that agents exchange information in an unselfish manner. In this work, we consider the scenario where a subset of agents aims at driving the network beliefs to the wrong hypothesis. The adversaries are unaware of the true hypothesis. However, they will "blend in" by behaving similarly to the other agents and will manipulate the likelihood functions used in the belief update process to launch inferential attacks. We will characterize the conditions under which the network is misled. Then, we will explain that it is possible for such attacks to succeed by showing that strategies exist that can be adopted by the malicious agents for this purpose. We examine both situations in which the agents have minimal or no information about the network model.      
### 10.Improving Sound Event Detection Metrics: Insights from DCASE 2020  [ :arrow_down: ](https://arxiv.org/pdf/2010.13648.pdf)
>  The ranking of sound event detection (SED) systems may be biased by assumptions inherent to evaluation criteria and to the choice of an operating point. This paper compares conventional event-based and segment-based criteria against the Polyphonic Sound Detection Score (PSDS)'s intersection-based criterion, over a selection of systems from DCASE 2020 Challenge Task 4. It shows that, by relying on collars , the conventional event-based criterion introduces different strictness levels depending on the length of the sound events, and that the segment-based criterion may lack precision and be application dependent. Alternatively, PSDS's intersection-based criterion overcomes the dependency of the evaluation on sound event duration and provides robustness to labelling subjectivity, by allowing valid detections of interrupted events. Furthermore, PSDS enhances the comparison of SED systems by measuring sound event modelling performance independently from the systems' operating points.      
### 11.A Systematic Evaluation of Coding Strategies for Sparse Binary Images  [ :arrow_down: ](https://arxiv.org/pdf/2010.13634.pdf)
>  Inpainting-based compression represents images in terms of a sparse subset of its pixel data. Storing the carefully optimised positions of known data creates a lossless compression problem on sparse and often scattered binary images. This central issue is crucial for the performance of such codecs. Since it has only received little attention in the literature, we conduct the first systematic investigation of this problem so far. To this end, we first review and compare a wide range of existing methods from image compression and general purpose coding in terms of their coding efficiency and runtime. Afterwards, an ablation study enables us to identify and isolate the most useful components of existing methods. With context mixing, we combine those ingredients into new codecs that offer either better compression ratios or a more favourable trade-off between speed and performance.      
### 12.Wind Power Transmission System Integration -- a Case Study of China Wind Power Base  [ :arrow_down: ](https://arxiv.org/pdf/2010.13624.pdf)
>  Due to a series of supporting policies in recent years, China wind power has developed rapidly through a large-scale and centralized mode. This paper analyzes the two major concerns faced by wind power development in China: wind generation reliability and wind energy balancing. More specifically, wind farm tripping-off-grid incidents and wind power curtailment issues, which caused huge economical loss, are investigated in details. Based on operation experience of large wind power bases, technical recommendations and economic incentives are proposed to improve wind power integration and power grid reliability. As a summary and outlook of wind power development in China, this paper provides a reference on future wind power development for other countries.      
### 13.Frequency Response Characteristic (FRC) Curve and Fast Frequency Response Assessment in High Renewable Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.13623.pdf)
>  This paper introduces a frequency response characteristic (FRC) curve and its application in high renewable power systems. In addition, the letter presents a method for fast frequency response assessment and frequency nadir prediction without performing dynamic simulations using detailed models. The proposed FRC curve and fast frequency response assessment method are useful for operators to understand frequency response performance of high renewable systems in real time.      
### 14.Exploiting Cell-Free Massive MIMO for Enabling Simultaneous Wireless Information and Power Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2010.13545.pdf)
>  The performance of simultaneous wireless information and power transfer (SWIPT) in downlink (DL) cell-free massive multiple-input multiple-output (MIMO) is investigated. Tight approximations to the DL harvested energy and the DL/uplink (UL) achievable rates are derived for two practical channel state information (CSI) cases by using a non-linear energy harvesting model for time-switching and power-splitting protocols. Max-min fairness-based transmit power control policies are employed to mitigate the deleterious near-far effects caused by distributed transmissions/receptions in cell-free massive MIMO. The achievable common DL energy-rate trade-off is derived, and thereby, it is shown that the proposed max-min power control guarantees user-fairness regardless of near-far effects in terms of both harvested energy and achievable rate. The benefits of user estimated DL CSI to boost the SWIPT performance are explored. These performance metrics are compared against those of the conventional co-located massive MIMO, and thereby, it is revealed that the reduction of path-losses and lower average transmit powers offered by cell-free massive MIMO can be exploited to boost the energy-rate trade-off of SWIPT at the expense of increased backhaul requirements.      
### 15.Power Scaling Law Analysis and Phase Shift Optimization of RIS-aided Massive MIMO Systems with Statistical CSI  [ :arrow_down: ](https://arxiv.org/pdf/2010.13525.pdf)
>  This paper considers an uplink reconfigurable intelligent surface (RIS)-aided massive multiple-input multiple-output (MIMO) system with statistical channel state information (CSI). The RIS is deployed to help conventional massive MIMO networks serve the users in the dead zone. We consider the Rician channel model and exploit the long-time statistical CSI to design the phase shifts of the RIS, while the maximum ratio combination (MRC) technique is applied for the active beamforming at the base station (BS) relying on the instantaneous CSI. Firstly, we reveal the power scaling laws and derive the closed-form expressions for the uplink achievable rate which holds for arbitrary numbers of base station (BS) antennas. Based on the theoretical expressions, we discuss the rate performance under some special cases and provide the average asymptotic rate when using random phase shifts. Then, we consider the sum-rate maximization problem and the minimum user rate maximization problem by optimizing the phase shifts at the RIS. However, these two optimization problems are challenging to solve due to the complicated data rate expression. To solve these problem, we propose a novel genetic algorithm (GA) with low complexity but can achieve good performance. Finally, extensive numerical simulations are provided to validate the benefits by integrating RIS into conventional massive MIMO systems. Besides, our simulations demonstrate the feasibility of deploying RIS with large-size but low-resolution hardware in the massive MIMO systems.      
### 16.Li-ion Battery Fault Detection in Large Packs Using Force and Gas Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2010.13519.pdf)
>  Internal short circuits are a leading cause of battery thermal runaway, and hence a major safety issue for electric vehicles. An internal short circuit with low resistance is called a hard internal short, which causes a high internal current flow that leads to an extremely fast temperature rise, gas generation, cell swelling, and ultimately battery rupture and failure. Thus it is crucial to detect these faults immediately after they get triggered. In large battery packs with many cells in parallel, detecting an internal short circuit event using voltage is difficult due to suppression of the voltage signal from the faulty cell by the other healthy cells connected in parallel. In contrast, analyzing the gas composition in the pack enclosure can provide a robust single cell failure detection method. At elevated temperature, decomposition of the battery materials results in gas generation and cell swelling. The cell structure is designed to rupture at a critical gas pressure and vent the accumulated $CO_2$ gas, in order to prevent explosive forces. In this paper, we extend our previous work by combining the models of cell thermal dynamics, swelling, and $CO_2$ gas generation. In particular, we developed a fast and high confidence level detection method of hard internal short circuit events for a battery pack by measuring cell expansion force and monitoring $CO_2$ concentrations in a pack enclosure.      
### 17.Unleashing the Potential of Tethered Networked Flying Platforms: Prospects, Challenges, and Applications  [ :arrow_down: ](https://arxiv.org/pdf/2010.13509.pdf)
>  Researchers are currently speculating about what the six generation (6G) of wireless systems will be. Several applications are proposed ranging from enhancing the conventional mobile communications to holographic and tactile communications. However, a crucial and pivotal aspect of 6G is worldwide connectivity. In fact, nearly 4 billion people do not have internet connection. Hence, 6G intends to bridge the digital divide and connecting the unconnected. For that purpose, 6G relies on terrestrial communications, satellite communications, and airborne communications. Terrestrial and satellite communications are expensive and take time to deploy. Airborne communications overcome these limitations and have shown promising success, such as, unmanned aerial vehicles (UAVs) and high altitude platforms (HAPS). However, even though these free flying platforms are a promising solution, they have issues with endurance and backhaul capacity. Tethered flying platforms overcome these limitations by providing continuous supply of power and data via the tether. They are also cost efficient and have good green credential. Not limited to communications, tether platforms have a wide rang of applications, such as, energy harvesting, entertainment, science, research, public safety, disaster relief, Government, and defence. In this survey, we intend to provide an extensive and comprehensive overview of tethered platforms. We provide all the types of tethered platforms, their components, applications, advantages as well as their challenges. We also show several case studies in various fields and applications. Finally, we present the start of art of tethered platforms from a wireless communications perspective.      
### 18.Optimization for Medical Image Segmentation: Theory and Practice when evaluating with Dice Score or Jaccard Index  [ :arrow_down: ](https://arxiv.org/pdf/2010.13499.pdf)
>  In many medical imaging and classical computer vision tasks, the Dice score and Jaccard index are used to evaluate the segmentation performance. Despite the existence and great empirical success of metric-sensitive losses, i.e. relaxations of these metrics such as soft Dice, soft Jaccard and Lovasz-Softmax, many researchers still use per-pixel losses, such as (weighted) cross-entropy to train CNNs for segmentation. Therefore, the target metric is in many cases not directly optimized. We investigate from a theoretical perspective, the relation within the group of metric-sensitive loss functions and question the existence of an optimal weighting scheme for weighted cross-entropy to optimize the Dice score and Jaccard index at test time. We find that the Dice score and Jaccard index approximate each other relatively and absolutely, but we find no such approximation for a weighted Hamming similarity. For the Tversky loss, the approximation gets monotonically worse when deviating from the trivial weight setting where soft Tversky equals soft Dice. We verify these results empirically in an extensive validation on six medical segmentation tasks and can confirm that metric-sensitive losses are superior to cross-entropy based loss functions in case of evaluation with Dice Score or Jaccard Index. This further holds in a multi-class setting, and across different object sizes and foreground/background ratios. These results encourage a wider adoption of metric-sensitive loss functions for medical segmentation tasks where the performance measure of interest is the Dice score or Jaccard index.      
### 19.Adaptive Optimal Trajectory Tracking Control Applied to a Large-Scale Ball-on-Plate System  [ :arrow_down: ](https://arxiv.org/pdf/2010.13486.pdf)
>  While many theoretical works concerning Adaptive Dynamic Programming (ADP) have been proposed, application results are scarce. Therefore, we design an ADP-based optimal trajectory tracking controller and apply it to a large-scale ball-on-plate system. Our proposed method incorporates an approximated reference trajectory instead of using setpoint tracking and allows to automatically compensate for constant offset terms. Due to the off-policy characteristics of the algorithm, the method requires only a small amount of measured data to train the controller. Our experimental results show that this tracking mechanism significantly reduces the control cost compared to setpoint controllers. Furthermore, a comparison with a model-based optimal controller highlights the benefits of our model-free data-based ADP tracking controller, where no system model and manual tuning are required but the controller is tuned automatically using measured data.      
### 20.VoteNet++: Registration Refinement for Multi-Atlas Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13484.pdf)
>  Multi-atlas segmentation (MAS) is a popular image segmentation technique for medical images. In this work, we improve the performance of MAS by correcting registration errors before label fusion. Specifically, we use a volumetric displacement field to refine registrations based on image anatomical appearance and predicted labels. We show the influence of the initial spatial alignment as well as the beneficial effect of using label information for MAS performance. Experiments demonstrate that the proposed refinement approach improves MAS performance on a 3D magnetic resonance dataset of the knee.      
### 21.Does contextual information improve 3D U-Net based brain tumor segmentation?  [ :arrow_down: ](https://arxiv.org/pdf/2010.13460.pdf)
>  Effective, robust and automatic tools for brain tumor segmentation are needed for extraction of information useful in treatment planning. In recent years, convolutional neural networks have shown state-of-the-art performance in the identification of tumor regions in magnetic resonance (MR) images. A large portion of the current research is devoted to the development of new network architectures to improve segmentation accuracy. In this work it is instead investigated if the addition of contextual information in the form of white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF) masks improves U-Net based brain tumor segmentation. The BraTS 2020 dataset was used to train and test a standard 3D U-Net model that, in addition to the conventional MR image modalities, used the contextual information as extra channels. For comparison, a baseline model that only used the conventional MR image modalities was also trained. Dice scores of 80.76 and 79.58 were obtained for the baseline and the contextual information models, respectively. Results show that there is no statistically significant difference when comparing Dice scores of the two models on the test dataset p &gt; 0.5. In conclusion, there is no improvement in segmentation performance when using contextual information as extra channels.      
### 22.Matthews Correlation Coefficient Loss for Deep Convolutional Networks: Application to Skin Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13454.pdf)
>  The segmentation of skin lesions is a crucial task in clinical decision support systems for the computer aided diagnosis of skin lesions. Although deep learning based approaches have improved segmentation performance, these models are often susceptible to class imbalance in the data, particularly, the fraction of the image occupied by the background healthy skin. Despite variations of the popular Dice loss function being proposed to tackle the class imbalance problem, the Dice loss formulation does not penalize misclassifications of the background pixels. We propose a novel metric-based loss function using the Matthews correlation coefficient, a metric that has been shown to be efficient in scenarios with skewed class distributions, and use it to optimize deep segmentation models. Evaluations on three dermoscopic image datasets: the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset, the DermoFit Image Library, and the PH2 dataset show that models trained using the proposed loss function outperform those trained using Dice loss by 11.25%, 4.87%, and 0.76% respectively in the mean Jaccard index. We plan to release the code on GitHub at <a class="link-external link-https" href="https://github.com/kakumarabhishek/MCC-Loss" rel="external noopener nofollow">this https URL</a> upon publication of this paper.      
### 23.Optimal transport-based metric for SMLM  [ :arrow_down: ](https://arxiv.org/pdf/2010.13423.pdf)
>  We propose the use of Flat Metric to assess the performance of reconstruction methods for single-molecule localization microscopy (SMLM)in scenarios where the ground-truth is available. Flat Metric is intimately related to the concept of optimal transport between measures of different mass, providing solid mathematical foundations for SMLM evaluation and integrating both localization and detection performance. In this paper, we introduce the foundations of Flat Metric and validate this measure by applying it to controlled synthetic examples and to data from the SMLM 2016 Challenge.      
### 24.TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2010.13421.pdf)
>  In this paper, we propose a text-to-speech (TTS)-driven data augmentation method for improving the quality of a non-autoregressive (AR) TTS system. Recently proposed non-AR models, such as FastSpeech 2, have successfully achieved fast speech synthesis system. However, their quality is not satisfactory, especially when the amount of training data is insufficient. To address this problem, we propose an effective data augmentation method using a well-designed AR TTS system. In this method, large-scale synthetic corpora including text-waveform pairs with phoneme duration are generated by the AR TTS system and then used to train the target non-AR model. Perceptual listening test results showed that the proposed method significantly improved the quality of the non-AR TTS system. In particular, we augmented five hours of a training database to 179 hours of a synthetic one. Using these databases, our TTS system consisting of a FastSpeech 2 acoustic model with a Parallel WaveGAN vocoder achieved a mean opinion score of 3.74, which is 40% higher than that achieved by the conventional method.      
### 25.Robust stabilization of multiport networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.13419.pdf)
>  This paper formulates and solves the problem of robust compensation of multiport active network. This is an important engineering problem as networks designed differ in parameter values due to tolerance during manufacture from their actual realizations in chips and hardware. Parameters also undergo changes due to environmental factors. Hence, practical use of networks requires compensation which is only possible by connecting compensating network at the ports. The resulting interconnection is then required to be stable over a range of parameter values. This is called robust compensation. This paper formulates such a problem using an extension of the coprime factorization theory well known in feedback control theory to the situation of multiport network interconnection developed in \cite{msm1} and formulates the robust stabilization problem as an $H_{\infty}$ optimization problem. The port interconnection of networks does not confirm with computation of the function of the interconnected network analogous to that of the feedback interconnection using signal flow graph. Hence the well known stabilization and stability theory of feedback systems cannot be utilized for such a problem. A new formulation of stabilization theory of network interconnection was formulated and developed by the authors in \cite{msm1}. The variations of parameters of the network are used to define a worst case neighborhood of the network in terms of its coprime fractions at the nominal values of parameters. The solution of the optimization problem is then carried out by the standard procedure of converting such a problem to the Nehari optimization problem \cite{fran}. This methodology of solving the robust compensation of multiport networks using feedback control theory is believed to be novel.      
### 26.Forwarding design for stabilization of a coupled transport equation-ODE with a cone-bounded input nonlinearity  [ :arrow_down: ](https://arxiv.org/pdf/2010.13417.pdf)
>  We propose a new design technique for the stabilization of coupled ODE-PDE systems in feedforward form. In particular, we address the stabilization problem of a one-dimensional transport equation driven by a scalar ODE which is controlled via a cone-bounded nonlinearity. The unforced transport equation is conservative but not asymptotically stable. The proposed technique is inspired by the forwarding approach early introduced in the 90's. Well-posedness and asymptotic stability of the closed-loop system are discussed.      
### 27.Identifiability and interpretability of hybrid, gray-box models  [ :arrow_down: ](https://arxiv.org/pdf/2010.13416.pdf)
>  Model identifiability concerns the uniqueness of uncertain model parameters to be estimated from available process data and is often thought of as a prerequisite for physical interpretability of the parameters in a mechanistic model. Nevertheless, model identifiability is rarely accomplished in practice due to both stochastic and deterministic uncertainties: lack of sufficient variability in the data, noisy measurements, erroneous model structure, stochasticity and locality of the optimization algorithm, initial values, and even the parameter of the true underlying process. For gray-box, hybrid models, model identifiability is even less obtainable due to a higher number of parameters compared to mechanistic models. We illustrate by means of an industrial case study, modeling of a production choke valve in a petroleum well, how physical interpretation of the physical model parameters may be preserved for non-identifiable models by utilizing parameter regularization in the estimation problem. Further, we design and train interpretable gray-box choke models on historical process data from six wells on the petroleum asset Edvard Grieg. Comparing the prediction performance of the gray-box model to a mechanistic model for the median well shows a significant increase in prediction accuracy.      
### 28.Node-Adaptive Regularization for Graph Signal Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2010.13413.pdf)
>  A critical task in graph signal processing is to estimate the true signal from noisy observations over a subset of nodes, also known as the reconstruction problem. In this paper, we propose a node-adaptive regularization for graph signal reconstruction, which surmounts the conventional Tikhonov regularization, giving rise to more degrees of freedom; hence, an improved performance. We formulate the node-adaptive graph signal denoising problem, study its bias-variance trade-off, and identify conditions under which a lower mean squared error and variance can be obtained with respect to Tikhonov regularization. Compared with existing approaches, the node-adaptive regularization enjoys more general priors on the local signal variation, which can be obtained by optimally designing the regularization weights based on Prony's method or semidefinite programming. As these approaches require additional prior knowledge, we also propose a minimax (worst-case) strategy to address instances where this extra information is unavailable. Numerical experiments with synthetic and real data corroborate the proposed regularization strategy for graph signal denoising and interpolation, and show its improved performance compared with competing alternatives.      
### 29.Closed-Form Solutions for a Low-Order System Fast Frequency Response Model  [ :arrow_down: ](https://arxiv.org/pdf/2010.13401.pdf)
>  This paper presents a novel closed-form solution for a low-order system frequency response (SFR) model that is accurate for all time periods and an accompanying approximation for representing primary frequency responses at two different speeds while still maintaining mathematical tractability. This allows for the inclusion of both fast frequency responses (e.g. from battery energy storage systems) and more conventional responses (e.g. from thermal generation) in a single SFR formulation. The closed-form expressions can be efficiently used in applications that use the SFR model such as frequency stability studies and security-constrained unit commitment (SCUC) studies.      
### 30.What is the best data augmentation approach for brain tumor segmentation using 3D U-Net?  [ :arrow_down: ](https://arxiv.org/pdf/2010.13372.pdf)
>  Training segmentation networks requires large annotated datasets, which in medical imaging can be hard to obtain. Despite this fact, data augmentation has in our opinion not been fully explored for brain tumor segmentation (a possible explanation is that the number of training subjects (369) is rather large in the BraTS 2020 dataset). Here we apply different types of data augmentation (flipping, rotation, scaling, brightness adjustment, elastic deformation) when training a standard 3D U-Net, and demonstrate that augmentation significantly improves performance on the validation set (125 subjects) in many cases. Our conclusion is that brightness augmentation and elastic deformation works best, and that combinations of different augmentation techniques do not provide further improvement compared to only using one augmentation technique.      
### 31.Integrating end-to-end neural and clustering-based diarization: Getting the best of both worlds  [ :arrow_down: ](https://arxiv.org/pdf/2010.13366.pdf)
>  Recent diarization technologies can be categorized into two approaches, i.e., clustering and end-to-end neural approaches, which have different pros and cons. The clustering-based approaches assign speaker labels to speech regions by clustering speaker embeddings such as x-vectors. While it can be seen as a current state-of-the-art approach that works for various challenging data with reasonable robustness and accuracy, it has a critical disadvantage that it cannot handle overlapped speech that is inevitable in natural conversational data. In contrast, the end-to-end neural diarization (EEND), which directly predicts diarization labels using a neural network, was devised to handle the overlapped speech. While the EEND, which can easily incorporate emerging deep-learning technologies, has started outperforming the x-vector clustering approach in some realistic database, it is difficult to make it work for `long' recordings (e.g., recordings longer than 10 minutes) because of, e.g., its huge memory consumption. Block-wise independent processing is also difficult because it poses an inter-block label permutation problem, i.e., an ambiguity of the speaker label assignments between blocks. In this paper, we propose a simple but effective hybrid diarization framework that works with overlapped speech and for long recordings containing an arbitrary number of speakers. It modifies the conventional EEND framework to simultaneously output global speaker embeddings so that speaker clustering can be performed across blocks to solve the permutation problem. With experiments based on simulated noisy reverberant 2-speaker meeting-like data, we show that the proposed framework works significantly better than the original EEND especially when the input data is long.      
### 32.Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.13350.pdf)
>  Neural text-to-speech (TTS) approaches generally require a huge number of high quality speech data, which makes it difficult to obtain such a dataset with extra emotion labels. In this paper, we propose a novel approach for emotional TTS synthesis on a TTS dataset without emotion labels. Specifically, our proposed method consists of a cross-domain speech emotion recognition (SER) model and an emotional TTS model. Firstly, we train the cross-domain SER model on both SER and TTS datasets. Then, we use emotion labels on the TTS dataset predicted by the trained SER model to build an auxiliary SER task and jointly train it with the TTS model. Experimental results show that our proposed method can generate speech with the specified emotional expressiveness and nearly no hindering on the speech quality.      
### 33.Improving pronunciation assessment via ordinal regression with anchored reference samples  [ :arrow_down: ](https://arxiv.org/pdf/2010.13339.pdf)
>  Sentence level pronunciation assessment is important for Computer Assisted Language Learning (CALL). Traditional speech pronunciation assessment, based on the Goodness of Pronunciation (GOP) algorithm, has some weakness in assessing a speech utterance: 1) Phoneme GOP scores cannot be easily translated into a sentence score with a simple average for effective assessment; 2) The rank ordering information has not been well exploited in GOP scoring for delivering a robust assessment and correlate well with a human rater's evaluations. In this paper, we propose two new statistical features, average GOP (aGOP) and confusion GOP (cGOP) and use them to train a binary classifier in Ordinal Regression with Anchored Reference Samples (ORARS). When the proposed approach is tested on Microsoft mTutor ESL Dataset, a relative improvement of Pearson correlation coefficient of 26.9% is obtained over the conventional GOP-based one. The performance is at a human-parity level or better than human raters.      
### 34.Deep Sequential Learning for Cervical Spine Fracture Detection on Computed Tomography Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.13336.pdf)
>  Fractures of the cervical spine are a medical emergency and may lead to permanent paralysis and even death. Accurate diagnosis in patients with suspected fractures by computed tomography (CT) is critical to patient management. In this paper, we propose a deep convolutional neural network (DCNN) with a bidirectional long-short term memory (BLSTM) layer for the automated detection of cervical spine fractures in CT axial images. We used an annotated dataset of 3,666 CT scans (729 positive and 2,937 negative cases) to train and validate the model. The validation results show a classification accuracy of 70,92% and 79.18% on the balanced (104 positive and 104 negative cases) and imbalanced (104 positive and 419 negative cases) test datasets.      
### 35.Distributed Node-Specific Block-Diagonal LCMV Beamforming in Wireless Acoustic Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.13334.pdf)
>  This paper derives the analytical solution of a novel distributed node-specific block-diagonal linearly constrained minimum variance beamformer from the centralized linearly constrained minimum variance (LCMV) beamformer when considering that the noise covariance matrix is block-diagonal. To further reduce the computational complexity of the proposed beamformer, the ShermanMorrison-Woodbury formula is introduced to compute the inversion of noise sample covariance matrix. By doing so, the exchanged signals can be computed with lower dimensions between nodes, where the optimal LCMV beamformer is still available at each node as if each node is to transmit its all raw sensor signal observations. The proposed beamformer is fully distributable without imposing restrictions on the underlying network topology or scaling computational complexity, i.e., there is no increase in the per-node complexity when new nodes are added to the networks. Compared with state-of-the-art distributed node-specific algorithms that are often time-recursive, the proposed beamformer exactly solves the LCMV beamformer optimally frame by frame, which has much lower computational complexity and is more robust to acoustic transfer function estimation error and voice activity detector error. Numerous experimental results are presented to validate the effectiveness of the proposed beamformer.      
### 36.Solar PV Frequency Control in the U.S. EI and ERCOT Interconnections -- Case Studies and Recommendations  [ :arrow_down: ](https://arxiv.org/pdf/2010.13316.pdf)
>  This paper studied the solar PV frequency control in the U.S. Eastern Interconnection (EI) and Texas Interconnection (ERCOT) systems. The studied frequency control approaches include droop frequency control, inertia control, and droop-inertia-combined frequency control. The control effects of different frequency controls of PV in the EI and ERCOT are studied using actual high PV penetration interconnection grid models to provide suggestions to the future revision of future PV frequency control standards.      
### 37.A Dark and Bright Channel Prior Guided Deep Network for Retinal Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2010.13313.pdf)
>  Retinal image quality assessment is an essential task in the diagnosis of retinal diseases. Recently, there are emerging deep models to grade quality of retinal images. Current state-of-the-arts either directly transfer classification networks originally designed for natural images to quality classification of retinal image or introduce extra image quality priors via multiple CNN branches or independent CNNs. This paper proposes a dark and bright prior guided deep network for retinal image quality assessment called GuidedNet. Specifically, the dark and bright channel priors are embedded into the start layer of network to improve the discriminate ability of deep features. Experimental results on retinal image quality dataset Eye-Quality demonstrate the effectiveness of the proposed GuidedNet.      
### 38.Geometrically Matched Multi-source Microscopic Image Synthesis Using Bidirectional Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.13308.pdf)
>  Microscopic images from different modality can provide more complete experimental information. In practice, biological and physical limitations may prohibit the acquisition of enough microscopic images at a given observation period. Image synthesis is one promising solution. However, most existing data synthesis methods only translate the image from a source domain to a target domain without strong geometric correlations. To address this issue, we propose a novel model to synthesize diversified microscopic images from multi-sources with different geometric features. The application of our model to a 3D live time-lapse embryonic images of C. elegans presents favorable results. To the best of our knowledge, it is the first effort to synthesize microscopic images with strong underlie geometric correlations from multi-source domains that of entirely separated spatial features.      
### 39.Online State Estimation for a Physics-Based Lithium-Sulfur Battery Model  [ :arrow_down: ](https://arxiv.org/pdf/2010.13277.pdf)
>  This article examines the problem of Lithium-Sulfur (Li-S) battery state estimation. Such estimation is important for the online management of this energy-dense chemistry. The literature uses equivalent circuit models (ECMs) for Li-S state estimation. This article's main goal is to perform estimation using a physics-based model instead. This approach is attractive because it furnishes online estimates of the masses of individual species in a given Li-S cell. The estimation is performed using an experimentally-validated, computationally tractable zero-dimensional model. Reformulation converts this model from differential algebraic equations (DAEs) to ordinary differential equations (ODEs), simplifying the estimation problem. The article's first contribution is to show that this model has poor observability, especially in the low plateau region, where the low sensitivity of cell voltage to precipitated sulfur mass complicates the estimation of this mass. The second contribution is to exploit mass conservation to derive a reduced-order model with attractive observability properties in both high and low plateau regions. The final contribution is to use an unscented Kalman filter (UKF) for estimating internal Li-S battery states, while taking constraints on species masses into account. Consistent with the article's observability analysis, the UKF achieves better low-plateau estimation accuracy when the reduced-order model is used.      
### 40.Interpreting Uncertainty in Model Predictions For COVID-19 Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2010.13271.pdf)
>  COVID-19, due to its accelerated spread has brought in the need to use assistive tools for faster diagnosis in addition to typical lab swab testing. Chest X-Rays for COVID cases tend to show changes in the lungs such as ground glass opacities and peripheral consolidations which can be detected by deep neural networks. However, traditional convolutional networks use point estimate for predictions, lacking in capture of uncertainty, which makes them less reliable for adoption. There have been several works so far in predicting COVID positive cases with chest X-Rays. However, not much has been explored on quantifying the uncertainty of these predictions, interpreting uncertainty, and decomposing this to model or data uncertainty. To address these needs, we develop a visualization framework to address interpretability of uncertainty and its components, with uncertainty in predictions computed with a Bayesian Convolutional Neural Network. This framework aims to understand the contribution of individual features in the Chest-X-Ray images to predictive uncertainty. Providing this as an assistive tool can help the radiologist understand why the model came up with a prediction and whether the regions of interest captured by the model for the specific prediction are of significance in diagnosis. We demonstrate the usefulness of the tool in chest x-ray interpretation through several test cases from a benchmark dataset.      
### 41.Improved Mask-CTC for Non-Autoregressive End-to-End ASR  [ :arrow_down: ](https://arxiv.org/pdf/2010.13270.pdf)
>  For real-world deployment of automatic speech recognition (ASR), the system is desired to be capable of fast inference while relieving the requirement of computational resources. The recently proposed end-to-end ASR system based on mask-predict with connectionist temporal classification (CTC), Mask-CTC, fulfills this demand by generating tokens in a non-autoregressive fashion. While Mask-CTC achieves remarkably fast inference speed, its recognition performance falls behind that of conventional autoregressive (AR) systems. To boost the performance of Mask-CTC, we first propose to enhance the encoder network architecture by employing a recently proposed architecture called Conformer. Next, we propose new training and decoding methods by introducing auxiliary objective to predict the length of a partial target sequence, which allows the model to delete or insert tokens during inference. Experimental results on different ASR tasks show that the proposed approaches improve Mask-CTC significantly, outperforming a standard CTC model (15.5% $\rightarrow$ 9.1% WER on WSJ). Moreover, Mask-CTC now achieves competitive results to AR models with no degradation of inference speed ($&lt;$ 0.1 RTF using CPU). We also show a potential application of Mask-CTC to end-to-end speech translation.      
### 42.Cooperative Energy Management of HVAC via Transactive Energy  [ :arrow_down: ](https://arxiv.org/pdf/2010.13265.pdf)
>  Heating, Ventilation, and Air Conditioning (HVAC) energy consumption accounts for a significant part of the total energy consumption of buildings and households. The ubiquitous adoption of distributed renewable energy and smart meters helps to decarbonize the HVAC energy consumption and improve energy efficiency. However, how to scale up HVAC energy management for a group of users while persevering users' privacy remains a big challenge. In this work, we utilize the concept of transactive energy to build a cooperative energy management system for independent HVAC units in a distributed manner. Specifically, we develop a distributed energy trading algorithm that consists of two layers based on the alternating direction method of multipliers method. The distributed energy trading algorithm achieves optimal trading performance and also preserves users' privacy. Furthermore, we evaluate the performance of the distributed trading algorithm by extensive simulations with real-world data. Simulation results show that the energy trading algorithm converges fast and the cooperative energy platform reduces the user's individual cost by up to 50% and lowers the overall cost of all users by 23%.      
### 43.Blockchain-Empowered Socially Optimal Transactive Energy System: Framework and Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13264.pdf)
>  Transactive energy plays a key role in the operation and energy management of future power systems. However, the conventional operational mechanism, which follows a centralized design, is often less secure, vulnerable to malicious behaviors, and suffers from privacy leakage. In this work, we introduce blockchain technology in transactive energy to address these challenges. Specifically, we develop a novel blockchain-based transactive energy framework for prosumers and design a decentralized energy trading algorithm that matches the operation of the underlying blockchain system. We prove that the trading algorithm improves the individual benefit and guarantees the socially optimal performance, and thus incentivizes prosumers to join the transactive energy platform. Moreover, we evaluate the feasibility of the transactive energy platform throughout the implementation of a small-scale network of Internet of Things (IoT) devices and extensive simulations using real-world data. Our results show that this blockchain-based transactive energy platform is feasible in practice, and the decentralized trading algorithm reduces the user's individual cost by up to 77% and lowers the overall cost by 24%.      
### 44.Self-Supervised Training For Low Dose CT Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2010.13232.pdf)
>  Ionizing radiation has been the biggest concern in CT imaging. To reduce the dose level without compromising the image quality, low dose CT reconstruction has been offered with the availability of compressed sensing based reconstruction methods. Recently, data-driven methods got attention with the rise of deep learning, the availability of high computational power, and big datasets. Deep learning based methods have also been used in low dose CT reconstruction problem in different manners. Usually, the success of these methods depends on clean labeled data. However, recent studies showed that training can be achieved successfully without clean datasets. In this study, we defined a training scheme to use low dose sinograms as their own training targets. We applied the self-supervision principle in the projection domain where the noise is element-wise independent as required in these methods. Using the self-supervised training, the filtering part of the FBP method and the parameters of a denoiser neural network are optimized. We demonstrate that our method outperforms both conventional and compressed sensing based iterative reconstruction methods qualitatively and quantitatively in the reconstruction of analytic CT phantoms and real-world CT images in low dose CT reconstruction task.      
### 45.Wideband Index Modulation with Circularly-Shifted Chirps  [ :arrow_down: ](https://arxiv.org/pdf/2010.13220.pdf)
>  In this study, we propose a wideband index modulation (IM) based on circularly-shifted chirps. To derive the proposed method, we first prove that a Golay complementary pair (GCP) can be constructed by linearly combining the Fourier series of chirps. We show that Fresnel integrals and/or Bessel functions, arising from sinusoidal and linear chirps, respectively, can lead to GCPs. We then exploit discrete Fourier transform-spread orthogonal frequency division multiplexing (DFT-s-OFDM) to obtain a low-complexity transmitter and receiver. We also discuss its generalization for achieving a trade-off between peak-to-mean envelope power ratio (PMEPR) and spectral efficiency (SE). Through comprehensive simulations, we compare the proposed scheme with DFT-s-OFDM with IM, orthogonal frequency division multiplexing (OFDM) with IM and complementary sequences (CSs) from Reed-Muller (RM) code. Our numerical results show that the proposed method limits the PMEPR while exploiting the frequency.      
### 46.SUREMap: Predicting Uncertainty in CNN-based Image Reconstruction Using Stein's Unbiased Risk Estimate  [ :arrow_down: ](https://arxiv.org/pdf/2010.13214.pdf)
>  Convolutional neural networks (CNN) have emerged as a powerful tool for solving computational imaging reconstruction problems. However, CNNs are generally difficult-to-understand black-boxes. Accordingly, it is challenging to know when they will work and, more importantly, when they will fail. This limitation is a major barrier to their use in safety-critical applications like medical imaging: Is that blob in the reconstruction an artifact or a tumor? <br>In this work we use Stein's unbiased risk estimate (SURE) to develop per-pixel confidence intervals, in the form of heatmaps, for compressive sensing reconstruction using the approximate message passing (AMP) framework with CNN-based denoisers. These heatmaps tell end-users how much to trust an image formed by a CNN, which could greatly improve the utility of CNNs in various computational imaging applications.      
### 47.D-VDAMP: Denoising-based Approximate Message Passing for Compressive MRI  [ :arrow_down: ](https://arxiv.org/pdf/2010.13211.pdf)
>  Plug and play (P&amp;P) algorithms iteratively apply highly optimized image denoisers to impose priors and solve computational image reconstruction problems, to great effect. However, in general the "effective noise", that is the difference between the true signal and the intermediate solution, within the iterations of P&amp;P algorithms is neither Gaussian nor white. This fact makes existing denoising algorithms suboptimal. <br>In this work, we propose a CNN architecture for removing colored Gaussian noise and combine it with the recently proposed VDAMP algorithm, whose effective noise follows a predictable colored Gaussian distribution. We apply the resulting denoising-based VDAMP (D-VDAMP) algorithm to variable density sampled compressive MRI where it substantially outperforms existing techniques.      
### 48.Cognitive Preadaptation for Resilient Adaptive Control  [ :arrow_down: ](https://arxiv.org/pdf/2010.13205.pdf)
>  In this paper, we investigate a novel control architecture and algorithm for incorporating preadaption functions. We propose a preadaptation mechanism that can augment any adaptive control scheme and improve its resilience. Through simulations of a flight control system we illustrate the effectiveness of the preadaptation mechanism in improving the adaptation. We show that the preadaptation mechanism we propose can reduce the peak of the response by as much as $50\%$. The scenarios we present also show that the preadaptation mechanism is effective across a wide range of scenarios suggesting that the mechanism is reliable.      
### 49.A Crowdsourcing Extension of the ITU-T Recommendation P.835 with Validation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13200.pdf)
>  The quality of the speech communication systems, which include noise suppression algorithms, are typically evaluated in laboratory experiments according to the ITU-T Rec. P.835. In this paper, we introduce an open-source implementation of the ITU-T Rec. P.835 for the crowdsourcing approach following the ITU-T Rec. P.808 on crowdsourcing recommendations. The implementation is an extension of the P.808 Toolkit and is highly automated to avoid operational errors. To assess our evaluation method's validity, we compared the Mean Opinion Scores (MOS), calculate using ratings collected with our implementation, and the MOS values from a standard laboratory experiment conducted according to the ITU-T Rec, P.835. Results show a high validity in all three scales (average PCC = 0.961). Results of a round-robin test showed that our implementation is a highly reproducible evaluation method (PCC=1.00). Finally, we investigated the performance of five models deep noise suppression models using our P.835 implementation and show what insights can be learned.      
### 50.Learning Sparse Graph Laplacian with $K$ Eigenvector Prior via Iterative GLASSO and Projection  [ :arrow_down: ](https://arxiv.org/pdf/2010.13179.pdf)
>  Learning a suitable graph is an important precursor to many graph signal processing (GSP) pipelines, such as graph spectral signal compression and denoising. Previous graph learning algorithms either i) make some assumptions on connectivity (e.g., graph sparsity), or ii) make simple graph edge assumptions such as positive edges only. In this paper, given an empirical covariance matrix $\bar{C}$ computed from data as input, we consider a structural assumption on the graph Laplacian matrix $L$: the first $K$ eigenvectors of $L$ are pre-selected, e.g., based on domain-specific criteria, such as computation requirement, and the remaining eigenvectors are then learned from data. One example use case is image coding, where the first eigenvector is pre-chosen to be constant, regardless of available observed data. We first prove that the subspace of symmetric positive semi-definite (PSD) matrices $H_{u}^+$ with the first $K$ eigenvectors being $\{u_k\}$ in a defined Hilbert space is a convex cone. We then construct an operator to project a given positive definite (PD) matrix $L$ to $H_{u}^+$, inspired by the Gram-Schmidt procedure. Finally, we design an efficient hybrid graphical lasso/projection algorithm to compute the most suitable graph Laplacian matrix $L^* \in H_{u}^+$ given $\bar{C}$. Experimental results show that given the first $K$ eigenvectors as a prior, our algorithm outperforms competing graph learning schemes using a variety of graph comparison metrics.      
### 51.Weighted-CEL0 sparse regularisation for molecule localisation in super-resolution microscopy with Poisson data  [ :arrow_down: ](https://arxiv.org/pdf/2010.13173.pdf)
>  We propose a continuous non-convex variational model for Single Molecule Localisation Microscopy (SMLM) super-resolution in order to overcome light diffraction barriers. Namely, we consider a variation of the Continuous Exact $\ell_0$ (CEL0) penalty recently introduced to relax the $\ell_2-\ell_0$ problem where a weighted-$\ell_2$ data fidelity is considered to model signal-dependent Poisson noise. For the numerical solution of the associated minimisation problem, we consider an iterative reweighted $\ell_1$ (IRL1) strategy for which we detail efficient parameter computation strategies. We report qualitative and quantitative molecule localisation results showing that the proposed weighted-CEL0 (wCEL0) model improves the results obtained by CEL0 and state-of-the art deep-learning approaches for the high-density SMLM ISBI 2013 dataset.      
### 52.Unsupervised Super-Resolution: Creating High-Resolution Medical Images from Low-Resolution Anisotropic Examples  [ :arrow_down: ](https://arxiv.org/pdf/2010.13172.pdf)
>  Although high resolution isotropic 3D medical images are desired in clinical practice, their acquisition is not always feasible. Instead, lower resolution images are upsampled to higher resolution using conventional interpolation methods. Sophisticated learning-based super-resolution approaches are frequently unavailable in clinical setting, because such methods require training with high-resolution isotropic examples. To address this issue, we propose a learning-based super-resolution approach that can be trained using solely anisotropic images, i.e. without high-resolution ground truth data. The method exploits the latent space, generated by autoencoders trained on anisotropic images, to increase spatial resolution in low-resolution images. The method was trained and evaluated using 100 publicly available cardiac cine MR scans from the Automated Cardiac Diagnosis Challenge (ACDC). The quantitative results show that the proposed method performs better than conventional interpolation methods. Furthermore, the qualitative results indicate that especially finer cardiac structures are synthesized with high quality. The method has the potential to be applied to other anatomies and modalities and can be easily applied to any 3D anisotropic medical image dataset.      
### 53.Compressed Fourier-Domain Convolutional Beamforming for Wireless Ultrasound imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.13171.pdf)
>  Wireless ultrasound (US) systems that produce high-quality images can improve current clinical diagnosis capabilities by making the imaging process much more efficient, affordable, and accessible to users. The most common technique for generating B-mode US images is delay and sum (DAS) beamforming, where an appropriate delay is introduced to signals sampled and processed at each transducer element. However, sampling rates that are much higher than the Nyquist rate of the signal are required for high resolution DAS beamforming, leading to large amounts of data, making transmission of channel data over WIFI impractical. Moreover, the production of US images that exhibit high resolution and good image contrast requires a large set of transducers which further increases the data size. Previous works suggest methods for reduction in sampling rate and in array size. In this work, we introduce compressed Fourier domain convolutional beamforming, combining Fourier domain beamforming, sparse convolutional beamforming, and compressed sensing methods. This allows reducing both the number of array elements and the sampling rate in each element, while achieving high resolution images. Using in vivo data we demonstrate that the proposed method can generate B-mode images using 142 times less data than DAS. Our results pave the way towards wireless US and demonstrate that high resolution US images can be produced using sub-Nyquist sampling and a small number of receiving channels.      
### 54.A Hierarchical Graph Signal Processing Approach to Inference from Spatiotemporal Signals  [ :arrow_down: ](https://arxiv.org/pdf/2010.13164.pdf)
>  Motivated by the emerging area of graph signal processing (GSP), we introduce a novel method to draw inference from spatiotemporal signals. Data acquisition in different locations over time is common in sensor networks, for diverse applications ranging from object tracking in wireless networks to medical uses such as electroencephalography (EEG) signal processing. In this paper we leverage novel techniques of GSP to develop a hierarchical feature extraction approach by mapping the data onto a series of spatiotemporal graphs. Such a model maps signals onto vertices of a graph and the time-space dependencies among signals are modeled by the edge weights. Signal components acquired from different locations and time often have complicated functional dependencies. Accordingly, their corresponding graph weights are learned from data and used in two ways. First, they are used as a part of the embedding related to the topology of graph, such as density. Second, they provide the connectivities of the base graph for extracting higher level GSP-based features. The latter include the energies of the signal's graph Fourier transform in different frequency bands. We test our approach on the intracranial EEG (iEEG) data set of the Kaggle epileptic seizure detection contest. In comparison to the winning code, the results show a slight net improvement and up to 6 percent improvement in per subject analysis, while the number of features are decreased by 75 percent on average.      
### 55.Attention is All You Need in Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13154.pdf)
>  Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism. In this paper, we propose the `SepFormer', a novel RNN-free Transformer-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model matches or overtakes the state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It indeed achieves an SI-SNRi of 20.2 dB on WSJ0-2mix matching the SOTA, and an SI-SNRi of 17.6 dB on WSJ0-3mix, a SOTA result. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest RNN-based systems.      
### 56.Spiking Neural Network Based Low-Power Radioisotope Identification using FPGA  [ :arrow_down: ](https://arxiv.org/pdf/2010.13125.pdf)
>  this paper presents a detailed methodology of a Spiking Neural Network (SNN) based low-power design for radioisotope identification. A low power cost of 72 mW has been achieved on FPGA with the inference accuracy of 100% at 10 cm test distance and 97% at 25 cm. The design verification and chip validation methods are presented. It also discusses SNN simulation on SpiNNaker for rapid prototyping and various considerations specific to the application such as test distance, integration time, and SNN hyperparameter selections.      
### 57.Context Aware 3D UNet for Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13082.pdf)
>  Deep convolutional neural network (CNN) achieves remarkable performance for medical image analysis. UNet is the primary source in the performance of 3D CNN architectures for medical imaging tasks, including brain tumor segmentation. The skip connection in the UNet architecture concatenates features from both encoder and decoder paths to extract multi-contexual information from image data. The multi-scaled features play an essential role in brain tumor segmentation. However, the limited use of features can degrade the performance of the UNet approach for segmentation. In this paper, we propose a modified UNet architecture for brain tumor segmentation. In the proposed architecture, we used densely connected blocks in both encoder and decoder paths to extract multi-contexual information from the concept of feature reusability. The proposed residual inception blocks (RIB) are used to extract local and global information by merging features of different kernel sizes. We validate the proposed architecture on the multimodal brain tumor segmentation challenges (BRATS) 2020 testing dataset. The dice (DSC) scores of the whole tumor (WT), tumor core (TC), and enhancement tumor (ET) are 89.12%, 84.74%, and 79.12%, respectively. Our proposed work is in the top ten methods based on the dice scores of the testing dataset.      
### 58.Crowdsourcing approach for subjective evaluation of echo impairment  [ :arrow_down: ](https://arxiv.org/pdf/2010.13063.pdf)
>  The quality of acoustic echo cancellers (AECs) in real-time communication systems is typically evaluated using objective metrics like ERLE and PESQ, and less commonly with lab-based subjective tests like ITU-T Rec. P.831. We will show that these objective measures are not well correlated to subjective measures. We then introduce an open-source crowdsourcing approach for subjective evaluation of echo impairment which can be used to evaluate the performance of AECs. We provide a study that shows this tool is highly reproducible. This new tool has been recently used in the ICASSP 2021 AEC Challenge which made the challenge possible to do quickly and cost effectively.      
### 59.Semi-Blind Source Separation for Nonlinear Acoustic Echo Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13060.pdf)
>  The mismatch between the numerical and actual nonlinear models is a challenge to nonlinear acoustic echo cancellation (NAEC) when nonlinear adaptive filter is utilized. To alleviate this problem, we propose an effective method based on semi-blind source separation (SBSS), which uses a basis-generic expansion of the memoryless nonlinearity and then merges the unknown nonlinear expansion coefficients into the echo path. By regarding all the basis functions of the far-end input signal as the known equivalent reference signals, an SBSS updating algorithm is derived based on the constrained scaled natural gradient strategy. Unlike the commonly utilized adaptive algorithm, the proposed SBSS is based on the independence between the near-end signal and the reference signals, and is less sensitive to the mismatch of nonlinearity between the numerical and actual models. The experimental results with both simulated and real captured data validate the efficacy of the proposed method in NAEC.      
### 60.A QP-adaptive Mechanism for CNN-based Filter in Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2010.13059.pdf)
>  Convolutional neural network (CNN)-based filters have achieved great success in video coding. However, in most previous works, individual models are needed for each quantization parameter (QP) band. This paper presents a generic method to help an arbitrary CNN-filter handle different quantization noise. We model the quantization noise problem and implement a feasible solution on CNN, which introduces the quantization step (Qstep) into the convolution. When the quantization noise increases, the ability of the CNN-filter to suppress noise improves accordingly. This method can be used directly to replace the (vanilla) convolution layer in any existing CNN-filters. By using only 25% of the parameters, the proposed method achieves better performance than using multiple models with VTM-6.3 anchor. Besides, an additional BD-rate reduction of 0.2% is achieved by our proposed method for chroma components.      
### 61.Probing Acoustic Representations for Phonetic Properties  [ :arrow_down: ](https://arxiv.org/pdf/2010.13007.pdf)
>  Pre-trained acoustic representations such as wav2vec and DeCoAR have attained impressive word error rates (WER) for speech recognition benchmarks, particularly when labeled data is limited. But little is known about what phonetic properties these various representations acquire, and how well they encode transferable features of speech. We compare features from two conventional and four pre-trained systems in some simple frame-level phonetic classification tasks, with classifiers trained on features from one version of the TIMIT dataset and tested on features from another. All contextualized representations offered some level of transferability across domains, and models pre-trained on more audio data give better results; but overall, DeCoAR, the system with the simplest architecture, performs best. This type of benchmarking analysis can thus uncover relative strengths of various proposed acoustic representations.      
### 62.Sensor-Based Spreader Automation for Reducing Salt Use and Improving Safety  [ :arrow_down: ](https://arxiv.org/pdf/2010.12983.pdf)
>  Over 30 million tons of deicing salt is applied on U.S. roads annually at a cost of roughly $1.2 billion and with significant negative environmental impact. Therefore, it is desirable to reduce salt use while maintaining winter road safety. Automatic adjustment of application rate in response to road, weather, traffic, and other conditions has the potential to achieve this goal. <br>In the US, salt application rates are typically pre-set manually based on roadway classification, cycle time, desired level of service (LOS), and expected traffic, road, and weather conditions. The operators can temporarily change the application rate manually based on their experience and observations. Current spreader automation mostly involves adjusting discharge rate in response to spreader speed, although pavement temperature sensors are likely to be adopted in the future. <br>This paper explores extending spreader automation for adjusting salt discharge rate on curves, inclines, and other areas with historically high accident concentrations. First, we propose the use of gyroscopic sensors and inclinometers on board the spreader to adjust discharge rate in real time in response to curves and inclines. The second part of the proposed solution includes the installation of roadside Radio-Frequency Identification (RFID) tags that can communicate the length and the severity of the hazard zone to an RFID reader on board the spreader to automatically adjust discharge rate accordingly. <br>Our proposed methods have the potential to reduce salt use and operator fatigue, while increasing winter road safety without adding excessive cost or complexity to the existing spreader systems in the US.      
### 63.Blind Deinterleaving of Signals in Time Series with Self-attention Based Soft Min-cost Flow Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.12972.pdf)
>  We propose an end-to-end learning approach to address deinterleaving of patterns in time series, in particular, radar signals. We link signal clustering problem to min-cost flow as an equivalent problem once the proper costs exist. We formulate a bi-level optimization problem involving min-cost flow as a sub-problem to learn such costs from the supervised training data. We then approximate the lower level optimization problem by self-attention based neural networks and provide a trainable framework that clusters the patterns in the input as the distinct flows. We evaluate our method with extensive experiments on a large dataset with several challenging scenarios to show the efficiency.      
### 64.Automated triage of COVID-19 from various lung abnormalities using chest CT features  [ :arrow_down: ](https://arxiv.org/pdf/2010.12967.pdf)
>  The outbreak of COVID-19 has lead to a global effort to decelerate the pandemic spread. For this purpose chest computed-tomography (CT) based screening and diagnosis of COVID-19 suspected patients is utilized, either as a support or replacement to reverse transcription-polymerase chain reaction (RT-PCR) test. In this paper, we propose a fully automated AI based system that takes as input chest CT scans and triages COVID-19 cases. More specifically, we produce multiple descriptive features, including lung and infections statistics, texture, shape and location, to train a machine learning based classifier that distinguishes between COVID-19 and other lung abnormalities (including community acquired pneumonia). We evaluated our system on a dataset of 2191 CT cases and demonstrated a robust solution with 90.8% sensitivity at 85.4% specificity with 94.0% ROC-AUC. In addition, we present an elaborated feature analysis and ablation study to explore the importance of each feature.      
### 65.Raw-x-vector: Multi-scale Time Domain Speaker Embedding Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.12951.pdf)
>  State-of-the-art text-independent speaker verification systems typically use cepstral features or filter bank energies of speech utterances as input features. With the ability of deep neural networks to learn representations from raw data, recent studies attempted to extract speaker embeddings directly from raw waveforms and showed competitive results. In this paper, we propose a new speaker embedding called raw-x-vector for speaker verification in the time domain, combining a multi-scale waveform encoder and an x-vector network architecture. We show that the proposed approach outperforms existing raw-waveform-based speaker verification systems by a large margin. We also show that the proposed multi-scale encoder improves over single-scale encoders for both the proposed system and another state-of-the-art raw-waveform-based speaker verification systems. A further analysis of the learned filters shows that the multi-scale encoder focuses on different frequency bands at its different scales while resulting in a more flat overall frequency response than any of the single-scale counterparts.      
### 66.Non-local Meets Global: An Iterative Paradigm for Hyperspectral Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2010.12921.pdf)
>  Non-local low-rank tensor approximation has been developed as a state-of-the-art method for hyperspectral image (HSI) restoration, which includes the tasks of denoising, compressed HSI reconstruction and inpainting. Unfortunately, while its restoration performance benefits from more spectral bands, its runtime also substantially increases. In this paper, we claim that the HSI lies in a global spectral low-rank subspace, and the spectral subspaces of each full band patch group should lie in this global low-rank subspace. This motivates us to propose a unified paradigm combining the spatial and spectral properties for HSI restoration. The proposed paradigm enjoys performance superiority from the non-local spatial denoising and light computation complexity from the low-rank orthogonal basis exploration. An efficient alternating minimization algorithm with rank adaptation is developed. It is done by first solving a fidelity term-related problem for the update of a latent input image, and then learning a low-dimensional orthogonal basis and the related reduced image from the latent input image. Subsequently, non-local low-rank denoising is developed to refine the reduced image and orthogonal basis iteratively. Finally, the experiments on HSI denoising, compressed reconstruction, and inpainting tasks, with both simulated and real datasets, demonstrate its superiority with respect to state-of-the-art HSI restoration methods.      
### 67.Electric Power Processing Using Logic Operation and Error Correction  [ :arrow_down: ](https://arxiv.org/pdf/2010.12900.pdf)
>  In this study, electric power is processed using the logic operation method and the error correction algorithms to meet load demand. Electric power was treated as physically flow through the distribution network, which was governed by circuit configuration and efficiency. The hardware required to digitize or packetize electric power, which is called power packet router, was developed in this research work. It provides the opportunity for functional electric power dispatching disregarding the power flow in the circuit. This study proposes a new design for the network, which makes the logic operation of electric power possible and provides an algorithm to correct the inaccuracies caused by dissipation and noise. Phase shift of the power supply network is resulted by implementing the introduced design.      
### 68.EEGsig machine learning-based toolbox for End-to-End EEG signal processing  [ :arrow_down: ](https://arxiv.org/pdf/2010.12877.pdf)
>  In the quest to realize comprehensive EEG signal processing toolbox, in this paper, we demonstrate the first toolbox contain three states of EEG signal processing (preprocessing, feature extraction, classification) together. Our goal is to provide a comprehensive toolbox for EEG signal processing. Using MATLAB software, we have developed an open-source toolbox for end-to-end processing of the EEG signal. As we know, in many research work in the field of neuroscience and EEG signal processing, we first clear the signal and remove noise, artifact, etc. Which we know as preprocessing, and then extract the feature from the relevant signal, and finally Machine learning classifiers used to classification of signal. We have tried to provide all the above steps in the form of EEGsig as a graphical user interface(GUI) so that there is no need for programming for all the above steps and reduce the time to complete these projects to a desirable level.      
### 69.Electromagnetic Source Imaging via a Data-Synthesis-Based Denoising Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2010.12876.pdf)
>  Electromagnetic source imaging (ESI) is a highly ill-posed inverse problem. To find a unique solution, traditional ESI methods impose a variety of priors that may not reflect the actual source properties. Such limitations of traditional ESI methods hinder their further applications. Inspired by deep learning approaches, a novel data-synthesized spatio-temporal denoising autoencoder method (DST-DAE) method was proposed to solve the ESI inverse problem. Unlike the traditional methods, we utilize a neural network to directly seek generalized mapping from the measured E/MEG signals to the cortical sources. A novel data synthesis strategy is employed by introducing the prior information of sources to the generated large-scale samples using the forward model of ESI. All the generated data are used to drive the neural network to automatically learn inverse mapping. To achieve better estimation performance, a denoising autoencoder (DAE) architecture with spatio-temporal feature extraction blocks is designed. Compared with the traditional methods, we show (1) that the novel deep learning approach provides an effective and easy-to-apply way to solve the ESI problem, that (2) compared to traditional methods, DST-DAE with the data synthesis strategy can better consider the characteristics of real sources than the mathematical formulation of prior assumptions, and that (3) the specifically designed architecture of DAE can not only provide a better estimation of source signals but also be robust to noise pollution. Extensive numerical experiments show that the proposed method is superior to the traditional knowledge-driven ESI methods.      
### 70.Output Regulation for Load Frequency Control  [ :arrow_down: ](https://arxiv.org/pdf/2010.12840.pdf)
>  Motivated by the inadequacy of the existing control strategies for power systems affected by time-varying uncontrolled power injections such as loads and the increasingly widespread renewable energy sources, this paper proposes two control schemes based on the well-known output regulation control methodology. The first one is designed based on the classical output regulation theory and addresses the so-called Load Frequency Control (LFC) problem in presence of time-varying uncontrolled power injections. Then, in order to also minimize the generation costs, we use an approximate output regulation method that solves numerically only the partial differential equation of the regulator equation and propose a controller based on this solution, minimizing an appropriate penalty function. An extensive case study shows excellent performance of the proposed control schemes in different and critical scenarios.      
### 71.New compound control algorithm in sliding mode control to reduce the chattering phenomenon: experimental validation  [ :arrow_down: ](https://arxiv.org/pdf/2010.12778.pdf)
>  In this work, a new SMS is proposed to achieve high tracking and suitable robustness. However, the chattering phenomenon should be regarded as the main drawback of the SMC. Therefore, a new compound control algorithm is used for reducing the chattering phenomenon. The applied compound control law constantly evaluates the error and send the correct value to the system. This significantly will reduce the chattering phenomenon. The performance of the control methods validated by applying on a robot arm experimentally.      
### 72.New compound fractional sliding mode control and super-twisting control of a MEMS gyroscope  [ :arrow_down: ](https://arxiv.org/pdf/2010.12774.pdf)
>  In this research we propose a new compound Fractional Order Sliding Mode Controller (FOSMC) and SuperTwisting Controller (FOSMC+STC) to control of a MEMS gyroscope. A new sliding mode surface has been defined to design the proposed new sliding mode controller. The main advantages of a FOSMC is its high tracking performance and robustness against external perturbation, but it is susceptible to chattering. By augmenting a STC with a FOSMC, the chattering phenomenon is eliminated, singularity problem is solved and systems robustness has significatnetly improved. Simulation results validate the effectiveness of the proposed control approach.      
### 73.New hybrid control of a 2 DoF Robot Arm  [ :arrow_down: ](https://arxiv.org/pdf/2010.12772.pdf)
>  Robot arms have been using in different systems, which the control of designed in desired trajectory is the main task. Also, it is anticipated that while in operation the developed 2DoF robot arm will be constantly encountered with noises such as friction forces. A new integral sliding mode control (NISMC) is therefore being introduced to suppress noise due to its robustness. Then, New hybrid control system (NHISMC) is proposed, which constantly calculates an error value and applies a correction value to the system. This will enhance trajectory and minimize tracking error. In comparison with two other controllers, such as traditional sliding mode control (SMC) and NISMC, experimental results confirmed the efficacy of the proposed control method.      
### 74.Diverse R-PPG: Camera-Based Heart Rate Estimation for Diverse Subject Skin-Tones and Scenes  [ :arrow_down: ](https://arxiv.org/pdf/2010.12769.pdf)
>  Remote vital sign monitoring has risen in prominence over recent years, with an acceleration in clinical development and deployment due to the COVID-19 pandemic. Previous work has demonstrated the feasibility of estimating subject heart rate from facial videos. However, all previous methods exhibit biased performance towards darker skin tone subjects. In this paper, we present a first attempt to mitigate biases in photoplethysmography performance on darker skin tones by relying on the statistics of imaging physics. In addition to mitigating skin tone bias, we demonstrate that the proposed method mitigates errors due to lighting changes, shadows, and specular highlights. The proposed method not only improves performance for darker skin tones but is the overall top performer on the entire dataset. We report a performance gain of 0.69 beats per minute over the benchmark for dark skin tones and an overall improvement of 0.47 bpm across all skin tones. Assessment of the proposed method is accomplished through the creation of the first telemedicine-focused smartphone camera-based remote vital signs dataset, named the VITAL dataset. A total of 344 videos (~688 minutes) consisting of 43 subjects with diverse skin tones recorded under three lighting conditions, two activity conditions, and two camera angles, is compiled with corresponding vital sign data including non-invasive continuous blood pressure, heart rate, respiratory rate, and oxygen saturation.      
### 75.X-TaSNet: Robust and Accurate Time-Domain Speaker Extraction Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.12766.pdf)
>  Extracting the speech of a target speaker from mixed audios, based on a reference speech from the target speaker, is a challenging yet powerful technology in speech processing. Recent studies of speaker-independent speech separation, such as TasNet, have shown promising results by applying deep neural networks over the time-domain waveform. Such separation neural network does not directly generate reliable and accurate output when target speakers are specified, because of the necessary prior on the number of speakers and the lack of robustness when dealing with audios with absent speakers. In this paper, we break these limitations by introducing a new speaker-aware speech masking method, called X-TaSNet. Our proposal adopts new strategies, including a distortion-based loss and corresponding alternating training scheme, to better address the robustness issue. X-TaSNet significantly enhances the extracted speech quality, doubling SDRi and SI-SNRi of the output speech audio over state-of-the-art voice filtering approach. X-TaSNet also improves the reliability of the results by improving the accuracy of speaker identity in the output audio to 95.4%, such that it returns silent audios in most cases when the target speaker is absent. These results demonstrate X-TaSNet moves one solid step towards more practical applications of speaker extraction technology.      
### 76.The DKU-DukeECE Systems for VoxCeleb Speaker Recognition Challenge 2020  [ :arrow_down: ](https://arxiv.org/pdf/2010.12731.pdf)
>  In this paper, we present the system submission for the VoxCeleb Speaker Recognition Challenge 2020 (VoxSRC-20) by the DKU-DukeECE team. For track 1, we explore various kinds of state-of-the-art front-end extractors with different pooling layers and objective loss functions. For track 3, we employ an iterative framework for self-supervised speaker representation learning based on a deep neural network (DNN). For track 4, we investigate the whole system pipeline for speaker diarization, including voice activity detection (VAD), uniform segmentation, speaker embedding extraction, and clustering.      
### 77.Deep Learning for Radio-based Human Sensing: Recent Advances and Future Directions  [ :arrow_down: ](https://arxiv.org/pdf/2010.12717.pdf)
>  While decade-long research has clearly demonstrated the vast potential of radio frequency (RF) for many human sensing tasks, scaling this technology to large scenarios remained problematic with conventional approaches. Recently, researchers have successfully applied deep learning to take radio-based sensing to a new level. Many different types of deep learning models have been proposed to achieve high sensing accuracy over a large population and activity set, as well as in unseen environments. Deep learning has also enabled detection of novel human sensing phenomena that were previously not possible. In this survey, we provide a comprehensive review and taxonomy of recent research efforts on deep learning based RF sensing. We also identify and compare several publicly released labeled RF sensing datasets that can facilitate such deep learning research. Finally, we summarize the lessons learned and discuss the current limitations and future directions of deep learning based RF sensing.      
### 78.Improving Noise Robustness of an End-to-End Neural Model for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.12715.pdf)
>  We present our experiments in training robust to noise an end-to-end automatic speech recognition (ASR) model using intensive data augmentation. We explore the efficacy of fine-tuning a pre-trained model to improve noise robustness, and we find it to be a very efficient way to train for various noisy conditions, especially when the conditions in which the model will be used, are unknown. Starting with a model trained on clean data helps establish baseline performance on clean speech. We carefully fine-tune this model to both maintain the performance on clean speech, and improve the model accuracy in noisy conditions. With this schema, we trained robust to noise English and Mandarin ASR models on large public corpora. All described models and training recipes are open sourced in NeMo, a toolkit for conversational AI.      
### 79.Exploring Multi-Channel Features for Speaker Verification with Joint VAD and Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2010.12692.pdf)
>  To improve multi-channel Speaker Verification, we propose to bring together advancements made in multi-channel speech features. For robust far-field scenario, we combine so-called spectral, spatial, and directional features. Using a simple feature concatenation scheme, we incrementally introduce new features on top of single-channel features. Experimented features include Inter-channel Phase Difference, multi-channel sinc convolutions, directional power ratio features and angle features. To maximally leverage supervised learning, our framework is also equipped with multi-channel speech enhancement and Voice Activity Detection. On a challenging language-mismatched test set, we obtain consistent improvements on various degradation levels. On noisy condition, we achieve a 32% relative reduction in Equal Error Rate (EER) w.r.t. single-channel baseline. We find the improvements from speaker-dependent directional features more consistent in noisy condition than clean. This is established via an analysis on several background disturbance levels. Lastly, we investigate if the learned multi-channel speaker embedding space can be made more discriminative through a constrastive loss based fine-tuning. With a simple choice of Triplet loss, we observe a further 8.3% relative reduction in EER.      
### 80.Super-Resolution Reconstruction of Interval Energy Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.12678.pdf)
>  High-resolution data are desired in many data-driven applications; however, in many cases only data whose resolution is lower than expected are available due to various reasons. It is then a challenge how to obtain as much useful information as possible from the low-resolution data. In this paper, we target interval energy data collected by Advanced Metering Infrastructure (AMI), and propose a Super-Resolution Reconstruction (SRR) approach to upsample low-resolution (hourly) interval data into higher-resolution (15-minute) data using deep learning. Our preliminary results show that the proposed SRR approaches can achieve much improved performance compared to the baseline model.      
### 81.SpeakerNet: 1D Depth-wise Separable Convolutional Network for Text-Independent Speaker Recognition and Verification  [ :arrow_down: ](https://arxiv.org/pdf/2010.12653.pdf)
>  We propose SpeakerNet - a new neural architecture for speaker recognition and speaker verification tasks. It is composed of residual blocks with 1D depth-wise separable convolutions, batch-normalization, and ReLU layers. This architecture uses x-vector based statistics pooling layer to map variable-length utterances to a fixed-length embedding (q-vector). SpeakerNet-M is a simple lightweight model with just 5M parameters. It doesn't use voice activity detection (VAD) and achieves close to state-of-the-art performance scoring an Equal Error Rate (EER) of 2.10% on the VoxCeleb1 cleaned and 2.29% on the VoxCeleb1 trial files.      
### 82.Bio-NICA: A biologically inspired single-layer network for Nonnegative Independent Component Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2010.12632.pdf)
>  Blind source separation, the problem of separating mixtures of unknown signals into their distinct sources, is an important problem for both biological and engineered signal processing systems. Nonnegative Independent Component Analysis (NICA) is a special case of blind source separation that assumes the mixture is a linear combination of independent, nonnegative sources. In this work, we derive a single-layer neural network implementation of NICA satisfying the following 3 constraints, which are relevant for biological systems and the design of neuromorphic hardware: (i) the network operates in the online setting, (ii) the synaptic learning rules are local, and (iii) the neural outputs are nonnegative.      
### 83.Federated Deep Unfolding for Sparse Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2010.12616.pdf)
>  This paper proposes a federated learning technique for deep algorithm unfolding with applications to sparse signal recovery and compressed sensing. We refer to this architecture as Fed-CS. Specifically, we unfold and learn the iterative shrinkage thresholding algorithm for sparse signal recovery without transporting to a central location, the training data distributed across many clients. We propose a layer-wise federated learning technique, in which each client uses local data to train a common model. Then we transmit only the model parameters of that layer from all the clients to the server, which aggregates these local models to arrive at a consensus model. The proposed layer-wise federated learning for sparse recovery is communication efficient and preserves data privacy. Through numerical experiments on synthetic and real datasets, we demonstrate Fed-CS's efficacy and present various trade-offs in terms of the number of participating clients and communications involved compared to a centralized approach of deep unfolding.      
### 84.Spectral folding and two-channel filter-banks on arbitrary graphs  [ :arrow_down: ](https://arxiv.org/pdf/2010.12604.pdf)
>  In the past decade, several multi-resolution representation theories for graph signals have been proposed. Bipartite filter-banks stand out as the most natural extension of time domain filter-banks, in part because perfect reconstruction, orthogonality and bi-orthogonality conditions in the graph spectral domain resemble those for traditional filter-banks. Therefore, many of the well known orthogonal and bi-orthogonal designs can be easily adapted for graph signals. A major limitation is that this framework can only be applied to the normalized Laplacian of bipartite graphs. In this paper we extend this theory to arbitrary graphs and positive semi-definite variation operators. Our approach is based on a different definition of the graph Fourier transform (GFT), where orthogonality is defined with the respect to the Q inner product. We construct GFTs satisfying a spectral folding property, which allows us to easily construct orthogonal and bi-orthogonal perfect reconstruction filter-banks. We illustrate signal representation and computational efficiency of our filter-banks on 3D point clouds with hundreds of thousands of points.      
### 85.ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2010.13715.pdf)
>  We consider the problem of conducting frame rate dependent video quality assessment (VQA) on videos of diverse frame rates, including high frame rate (HFR) videos. More generally, we study how perceptual quality is affected by frame rate, and how frame rate and compression combine to affect perceived quality. We devise an objective VQA model called Space-Time GeneRalized Entropic Difference (GREED) which analyzes the statistics of spatial and temporal band-pass video coefficients. A generalized Gaussian distribution (GGD) is used to model band-pass responses, while entropy variations between reference and distorted videos under the GGD model are used to capture video quality variations arising from frame rate changes. The entropic differences are calculated across multiple temporal and spatial subbands, and merged using a learned regressor. We show through extensive experiments that GREED achieves state-of-the-art performance on the LIVE-YT-HFR Database when compared with existing VQA models. The features used in GREED are highly generalizable and obtain competitive performance even on standard, non-HFR VQA databases. The implementation of GREED has been made available online: <a class="link-external link-https" href="https://github.com/pavancm/GREED" rel="external noopener nofollow">this https URL</a>      
### 86.The Frequency Spectrum and Geometry of the Hal Saflieni Hypogeum Appear Tuned  [ :arrow_down: ](https://arxiv.org/pdf/2010.13697.pdf)
>  The Hal Saflieni Hypogeum is a unique subterranean Maltese Neolithic sanctuary with a well-documented history of interest in its acoustics. Previous studies have noted its unusual strongly-defined frequency spectrum, but it is unknown if this was coincidental. In this paper, we present evidence that the Hypogeum's creators shaped the site's geometry to create or amplify its frequency spectrum, or another property closely correlated with the spectrum. Specifically, we show that the observed spectrum required jointly fine-tuning the dimensions of multiple non-contiguous cave walls across multiple independent chambers, to a degree that seems unlikely to be coincidental. We also note that the peak frequencies are evenly spaced and resemble a whole-tone scale in music, which is also unlikely to be coincidental and suggests the spectrum itself might have held some cultural significance. Taken together, it suggests acoustic or spectral properties may have played a motivational or cultural role for the site's Neolithic creators. This work identifies one of the earliest known examples of a manmade structure with a significant musical element to its interior architecture.      
### 87.Cooperative Beam Routing for Multi-IRS Aided Communication  [ :arrow_down: ](https://arxiv.org/pdf/2010.13589.pdf)
>  Intelligent reflecting surface (IRS) has been deemed as a transformative technology to achieve smart and reconfigurable environment for wireless communication. This letter studies a new IRS-aided communication system, where multiple IRSs assist in the communication between a multi-antenna base station (BS) and a remote single-antenna user by multi-hop signal reflection. Specifically, by exploiting the line-of-sight (LoS) link between nearby IRSs, a multi-hop cascaded LoS link between the BS and user is established where a set of IRSs are selected to successively reflect the BS's signal, so that the received signal power at the user is maximized. To tackle this new problem, we first present the closed-form solutions for the optimal active and cooperative passive beamforming at the BS and selected IRSs, respectively, for a given beam route. Then, we derive the end-to-end channel power, which unveils a fundamental trade-off in the optimal beam routing design between maximizing the multiplicative passive beamforming gain and minimizing the multi-reflection path loss. To reconcile this trade-off, we recast the IRS selection and beam routing problem as an equivalent shortest simple-path problem in graph theory and solve it optimally. Numerical results show significant performance gains of the proposed algorithm over benchmark schemes and also draw useful insights into the optimal beam routing design.      
### 88.Contrastive Unsupervised Learning for Audio Fingerprinting  [ :arrow_down: ](https://arxiv.org/pdf/2010.13540.pdf)
>  The rise of video-sharing platforms has attracted more and more people to shoot videos and upload them to the Internet. These videos mostly contain a carefully-edited background audio track, where serious speech change, pitch shifting and various types of audio effects may involve, and existing audio identification systems may fail to recognize the audio. To solve this problem, in this paper, we introduce the idea of contrastive learning to the task of audio fingerprinting (AFP). Contrastive learning is an unsupervised approach to learn representations that can effectively group similar samples and discriminate dissimilar ones. In our work, we consider an audio track and its differently distorted versions as similar while considering different audio tracks as dissimilar. Based on the momentum contrast (MoCo) framework, we devise a contrastive learning method for AFP, which can generate fingerprints that are both discriminative and robust. A set of experiments showed that our AFP method is effective for audio identification, with robustness to serious audio distortions, including the challenging speed change and pitch shifting.      
### 89.Lyapunov-Based Reinforcement Learning State Estimator  [ :arrow_down: ](https://arxiv.org/pdf/2010.13529.pdf)
>  In this paper, we consider the state estimation problem for nonlinear stochastic discrete-time systems. We combine Lyapunov's method in control theory and deep reinforcement learning to design the state estimator. We theoretically prove the convergence of the bounded estimate error solely using the data simulated from the model. An actor-critic reinforcement learning algorithm is proposed to learn the state estimator approximated by a deep neural network. The convergence of the algorithm is analysed. The proposed Lyapunov-based reinforcement learning state estimator is compared with a number of existing nonlinear filtering methods through Monte Carlo simulations, showing its advantage in terms of estimate convergence even under some system uncertainties such as covariance shift in system noise and randomly missing measurements. To the best of our knowledge, this is the first reinforcement learning based nonlinear state estimator with bounded estimate error performance guarantee.      
### 90.COL0RME: COvariance-based $\ell_0$ super-Resolution Microscopy with intensity Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13477.pdf)
>  Super-resolution light microscopy overcomes the physical barriers due to light diffraction, allowing for the observation of otherwise indistinguishable subcellular entities. However, the specific acquisition conditions required by state-of-the-art super-resolution methods to achieve adequate spatio-temporal resolution are often very challenging. Exploiting molecules fluctuations allows good spatio-temporal resolution live-cell imaging by means of common microscopes and conventional fluorescent dyes. In this work, we present the method COL0RME for COvariance-based $\ell_0$ super-Resolution Microscopy with intensity Estimation. It codifies the assumption of sparse distribution of the fluorescent molecules as well as the temporal and spatial independence between emitters via a non-convex optimization problem formulated in the covariance domain. In order to deal with real data, the proposed approach also estimates background and noise statistics. It also includes a final estimation step where intensity information is retrieved, which is valuable for biological interpretation and future applications to super-resolution imaging.      
### 91.Melody Harmonization Using Orderless NADE, Chord Balancing, and Blocked Gibbs Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2010.13468.pdf)
>  Coherence and interestingness are two criteria for evaluating the performance of melody harmonization, which aims to generate a chord progression from a symbolic melody. In this study, we apply the concept of orderless NADE, which takes the melody and its partially masked chord sequence as the input of the BiLSTM-based networks to learn the masked ground truth, to the training process. In addition, class weighting is used to compensate for some reasonable chord labels that are rarely seen in the training set. Consistent with the stochasticity in training, blocked Gibbs sampling with proper numbers of masking/generating loops is used in the inference phase to progressively trade the coherence of the generated chord sequence off against its interestingness. The experiments were conducted on a dataset of 18,005 melody/chord pairs. Our proposed model outperforms the state-of-the-art system MTHarmonizer in five of six different objective metrics based on chord/melody harmonicity and chord progression. The subjective test results with more than 100 participants also show the superiority of our model.      
### 92.Speaker Anonymization with Distribution-Preserving X-Vector Generation for the VoicePrivacy Challenge 2020  [ :arrow_down: ](https://arxiv.org/pdf/2010.13457.pdf)
>  In this paper, we present a Distribution-Preserving Voice Anonymization technique, as our submission to the VoicePrivacy Challenge 2020. We notice that the challenge baseline system generates fake X-vectors which are very similar to each other, significantly more so than those extracted from organic speakers. This difference arises from averaging many X-vectors from a pool of speakers in the anonymization processs, causing a loss of information. We propose a new method to generate fake X-vectors which overcomes these limitations by preserving the distributional properties of X-vectors and their intra-similarity. We use population data to learn the properties of the X-vector space, before fitting a generative model which we use to sample fake X-vectors. We show how this approach generates X-vectors that more closely follow the expected intra-similarity distribution of organic speaker X-vectors. Our method can be easily integrated with others as the anonymization component of the system and removes the need to distribute a pool of speakers to use during the anonymization. Our approach leads to an increase in EER of up to 16.8\% in males and 8.4\% in females in scenarios where enrollment and trial utterances are anonymized versus the baseline solution, demonstrating the diversity of our generated voices.      
### 93.A Signal Separation Method Based on Adaptive Continuous Wavelet Transform and its Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2010.13448.pdf)
>  Recently the synchrosqueezing transform (SST) was developed as an empirical mode decomposition (EMD)-like tool to enhance the time-frequency resolution and energy concentration of a multicomponent non-stationary signal and provides more accurate component recovery (mode retrieval). To recover individual components, the SST method consists of two steps. First the instantaneous frequency (IF) of a component is estimated from the SST plane. Secondly, after IF is recovered, the associated component is computed by a definite integral along the estimated IF curve on the SST plane. More recently, a direct method of the time-frequency approach, called signal separation operation (SSO), was introduced for multicomponent signal separation. SSO avoids the second step of the two-step SST method in component recovery. <br>The SSO method is based the short-time Fourier transform. In this paper we propose a direct method of signal separation based on the adaptive continuous wavelet transform (CWT). We introduce two models of the adaptive CWT-based approach for signal separation: the sinusoidal signal-based model and the linear chirp-based model, which are derived respectively from sinusoidal signal approximation and the linear chirp approximation at any local time. A more accurate component recovery formula is derived from linear chirp local approximation. We present the theoretical analysis of our approach. For each model, we establish the error bounds for IF estimation and component recovery.      
### 94.Pooling for First and Last Mile  [ :arrow_down: ](https://arxiv.org/pdf/2010.13438.pdf)
>  Carpooling is a system in which drivers accept to add some limited detours to their habitual journeys to pick-up and drop-off other riders. Most research and operating platforms present carpooling as an alternative to fixed schedule transit and only very little work has attempted to integrate it with fixed-schedule mass transit. The aim of this paper is to showcase the benefits of such integration, under the philosophy of Mobility as a Service (MaaS), in a daily commuting scenario. We present an integrated mass transit plus carpooling system that, by design, constructs multimodal trips, including transit and carpooling legs. To this aim, the system generates vehicle detours in order to serve transit stations. We evaluate the performance of this system via simulation. We compare the ``Current'' System, where carpooling is an alternative to transit, to our ``Integrated'' System, where carpooling and transit are integrated in a single system. We show that, by doing this, the transportation accessibility greatly increases: about 40\% less users remain without feasible travel options and the overall travel time decreases by about 10\%. We achieve this by requiring relatively small driver detours, thanks to a better utilization vehicle routes, with drivers' vehicles driving on average with more riders on board. The simulation code is available open source.      
### 95.Space-Constrained Arrays for Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2010.13371.pdf)
>  We analyse the behaviour of a massive multi-user MIMO (MU-MIMO) system comprising a base station (BS) equipped with one of five different antenna topologies for which the spatial aperture is either unconstrained, or space-constrained. We derive the normalized mean interference (NMI) with a ray-based channel model, as a metric for topology comparison in each of the two cases. Based on the derivation for a horizontal uniform rectangular array (HURA) in [1], we provide closed-form NMI equations for the uniform linear array (ULA) and uniform circular array (UCirA). We then derive the same for a vertical URA (VURA) and uniform cylindrical array (UCylA). Results for the commonly-considered unconstrained case confirm the prior understanding that topologies with wider azimuth footprints aid performance. However, in the space-constrained case performance is dictated by the angular resolution afforded by the topology, particularly in elevation. We confirm the behavioural patterns predicted by the NMI by observing the same patterns in the system SINR with minimum mean-squared error (MMSE) processing.      
### 96.Convergence Acceleration via Chebyshev Step: Plausible Interpretation of Deep-Unfolded Gradient Descent  [ :arrow_down: ](https://arxiv.org/pdf/2010.13335.pdf)
>  Deep unfolding is a promising deep-learning technique, whose network architecture is based on expanding the recursive structure of existing iterative algorithms. Although convergence acceleration is a remarkable advantage of deep unfolding, its theoretical aspects have not been revealed yet. The first half of this study details the theoretical analysis of the convergence acceleration in deep-unfolded gradient descent (DUGD) whose trainable parameters are step sizes. We propose a plausible interpretation of the learned step-size parameters in DUGD by introducing the principle of Chebyshev steps derived from Chebyshev polynomials. The use of Chebyshev steps in gradient descent (GD) enables us to bound the spectral radius of a matrix governing the convergence speed of GD, leading to a tight upper bound on the convergence rate. The convergence rate of GD using Chebyshev steps is shown to be asymptotically optimal, although it has no momentum terms. We also show that Chebyshev steps numerically explain the learned step-size parameters in DUGD well. In the second half of the study, %we apply the theory of Chebyshev steps and Chebyshev-periodical successive over-relaxation (Chebyshev-PSOR) is proposed for accelerating linear/nonlinear fixed-point iterations. Theoretical analysis and numerical experiments indicate that Chebyshev-PSOR exhibits significantly faster convergence for various examples such as Jacobi method and proximal gradient methods.      
### 97.Federated Learning in Multi-RIS Aided Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.13333.pdf)
>  This paper investigates the problem of model aggregation in the federated learning system aided by multiple reconfigurable intelligent surfaces. The effective combination of computation and communication is achieved by over-the-air computation in federated learning. Since all local parameters are transmitted over shared wireless channels, the undesirable propagation error will inevitably deteriorate the learning performance. The objective of this work is to reduce the signal distortion and improve the convergence rate of federated learning. Thus, the mean-square-error and weighted cardinality are minimized by optimizing the transmit power, controlling the receive scalar, designing the phase shifts, and selecting devices in the model uploading process. To address this challenging issue, the original mixed-integer bi-criterion problem (P0) is decomposed into a non-convex problem (P1) with continuous variables and a combinatorial problem (P2) with integer variables. In an effort to tackle P1, the closed-form expressions for transceivers are first derived, and the multi-antenna cases are addressed by the semidefinite relaxation, then the problem of phase shifts design is tackled by invoking the penalty method and successive convex approximation. In terms of P2, the difference-of-convex programming is adopted to select devices judiciously for convergence accelerating, while satisfying the aggregation error demand. After that, an alternating optimization algorithm is proposed to find a suboptimal solution for P0, where the corresponding convergence and complexity are analyzed. Finally, simulation results demonstrate that the designed algorithm can converge faster and aggregate model more accurately.      
### 98.Decentralizing Feature Extraction with Quantum Convolutional Neural Network for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.13309.pdf)
>  We propose a novel decentralized feature extraction approach in federated learning to address privacy-preservation issues for speech recognition. It is built upon a quantum convolutional neural network (QCNN) composed of a quantum circuit encoder for feature extraction, and a recurrent neural network (RNN) based end-to-end acoustic model (AM). To enhance model parameter protection in a decentralized architecture, an input speech is first up-streamed to a quantum computing server to extract Mel-spectrogram, and the corresponding convolutional features are encoded using a quantum circuit algorithm with random parameters. The encoded features are then down-streamed to the local RNN model for the final recognition. The proposed decentralized framework takes advantage of the quantum learning progress to secure models and to avoid privacy leakage attacks. Testing on the Google Speech Commands Dataset, the proposed QCNN encoder attains a competitive accuracy of 95.12\% in a decentralized model, which is better than the previous architectures using centralized RNN models with convolutional features. We also conduct an in-depth study of different quantum circuit encoder architectures to provide insights into designing QCNN-based feature extractors. Finally, neural saliency analyses demonstrate a high correlation between the proposed QCNN features, class activation maps, and the input Mel-spectrogram.      
### 99.Asymptotic Behavior of Adversarial Training in Binary Classification  [ :arrow_down: ](https://arxiv.org/pdf/2010.13275.pdf)
>  It is widely known that several machine learning models are susceptible to adversarial attacks i.e., small adversarial perturbations applied to data points causing the model to misclassify the data. Adversarial training using empirical risk minimization methods, is the state-of-the-art method for defense against adversarial attacks. Despite being successful, several problems in understanding generalization performance of adversarial training remain open. In this paper, we derive precise theoretical predictions for the performance of adversarial training in binary linear classification. We consider the modern high-dimensional regime where the dimension of data grows with the size of the training dataset at a constant ratio. Our results provide exact asymptotics for the performance of estimators obtained by adversarial training with $\ell_q$-norm bounded perturbations ($q \ge 1$) and for binary labels and Gaussian features. These sharp predictions enable us to explore the role of various factors including over-parametrization ratio, data model and attack budget on the performance of adversarial training.      
### 100.A Joint Convolutional and Spatial Quad-Directional LSTM Network for Phase Unwrapping  [ :arrow_down: ](https://arxiv.org/pdf/2010.13268.pdf)
>  Phase unwrapping is a classical ill-posed problem which aims to recover the true phase from wrapped phase. In this paper, we introduce a novel Convolutional Neural Network (CNN) that incorporates a Spatial Quad-Directional Long Short Term Memory (SQD-LSTM) for phase unwrapping, by formulating it as a regression problem. Incorporating SQD-LSTM can circumvent the typical CNNs' inherent difficulty of learning global spatial dependencies which are vital when recovering the true phase. Furthermore, we employ a problem specific composite loss function to train this network. The proposed network is found to be performing better than the existing methods under severe noise conditions (Normalized Root Mean Square Error of 1.3 % at SNR = 0 dB) while spending a significantly less computational time (0.054 s). The network also does not require a large scale dataset during training, thus making it ideal for applications with limited data that require fast and accurate phase unwrapping.      
### 101.Effect of Language Proficiency on Subjective Evaluation of Noise Suppression Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2010.13260.pdf)
>  Speech communication systems based on Voice-over-IP technology are frequently used by native as well as non-native speakers of a target language, e.g. in international phone calls or telemeetings. Frequently, such calls also occur in a noisy environment, making noise suppression modules necessary to increase perceived quality of experience. Whereas standard tests for assessing perceived quality make use of native listeners, we assume that noise-reduced speech and residual noise may affect native and non-native listeners of a target language in different ways. To test this assumption, we report results of two subjective tests conducted with English and German native listeners who judge the quality of speech samples recorded by native English, German, and Mandarin speakers, which are degraded with different background noise levels and noise suppression effects. The experiments were conducted following the standardized ITU-T Rec. P.835 approach, however implemented in a crowdsourcing setting according to ITU-T Rec. P.808. Our results show a significant influence of language on speech signal ratings and, consequently, on the overall perceived quality in specific conditions.      
### 102.Dual-energy Computed Tomography Imaging from Contrast-enhanced Single-energy Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2010.13253.pdf)
>  In a standard computed tomography (CT) image, pixels having the same Hounsfield Units (HU) can correspond to different materials and it is therefore challenging to differentiate and quantify materials. Dual-energy CT (DECT) is desirable to differentiate multiple materials, but DECT scanners are not widely available as single-energy CT (SECT) scanners. Here we purpose a deep learning approach to perform DECT imaging by using standard SECT data. We designed a predenoising and difference learning mechanism to generate DECT images from SECT data. The performance of the deep learning-based DECT approach was studied using images from patients who received contrast-enhanced abdomen DECT scan with a popular DE application: virtual non-contrast (VNC) imaging and contrast quantification. Clinically relevant metrics were used for quantitative assessment. The absolute HU difference between the predicted and original high-energy CT images are 1.3 HU, 1.6 HU, 1.8 HU and 1.3 HU for the ROIs on aorta, liver, spine and stomach, respectively. The aorta iodine quantification difference between iodine maps obtained from the original and deep learning DECT images is smaller than 1.0\%, and the noise levels in the material images have been reduced by more than 7-folds for the latter. This study demonstrates that highly accurate DECT imaging with single low-energy data is achievable by using a deep learning approach. The proposed method allows us to obtain high-quality DECT images without paying the overhead of conventional hardware-based DECT solutions and thus leads to a new paradigm of spectral CT imaging.      
### 103.Unified Gradient Reweighting for Model Biasing with Applications to Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13228.pdf)
>  Recent deep learning approaches have shown great improvement in audio source separation tasks. However, the vast majority of such work is focused on improving average separation performance, often neglecting to examine or control the distribution of the results. In this paper, we propose a simple, unified gradient reweighting scheme, with a lightweight modification to bias the learning process of a model and steer it towards a certain distribution of results. More specifically, we reweight the gradient updates of each batch, using a user-specified probability distribution. We apply this method to various source separation tasks, in order to shift the operating point of the models towards different objectives. We demonstrate different parameterizations of our unified reweighting scheme can be used towards addressing several real-world problems, such as unreliable separation estimates. Our framework enables the user to control a robustness trade-off between worst and average performance. Moreover, we experimentally show that our unified reweighting scheme can also be used in order to shift the focus of the model towards being more accurate for user-specified sound classes or even towards easier examples in order to enable faster convergence.      
### 104.IR-GAN: Room Impulse Response Generator for Speech Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13219.pdf)
>  We present a Generative Adversarial Network (GAN) based room impulse response generator for generating realistic synthetic room impulse responses. Our proposed generator can create synthetic room impulse responses by parametrically controlling the acoustic features captured in real-world room impulse responses. Our GAN-based room impulse response generator (IR-GAN) is capable of improving far-field automatic speech recognition in environments not known during training. We create far-field speech training set by augmenting our synthesized room impulse responses with clean LibriSpeech dataset. We evaluate the quality of our room impulse responses on the real-world LibriSpeech test set created using real impulse responses from BUT ReverbDB and AIR datasets. Furthermore, we combine our synthetic data with synthetic impulse responses generated using acoustic simulators, and this combination can reduce the word error rate by up to 14.3% in far-field speech recognition benchmarks.      
### 105.Cascaded all-pass filters with randomized center frequencies and phase polarity for acoustic and speech measurement and data augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.13185.pdf)
>  We introduce a new member of TSP (Time Stretched Pulse) for acoustic and speech measurement infrastructure, based on a simple all-pass filter and systematic randomization. This new infrastructure fundamentally upgrades our previous measurement procedure, which enables simultaneous measurement of multiple attributes, including non-linear ones without requiring extra filtering nor post-processing. Our new proposal establishes a theoretically solid, flexible, and extensible foundation in acoustic measurement. Moreover, it is general enough to provide versatile research tools for other fields, such as biological signal analysis. We illustrate using acoustic measurements and data augmentation as representative examples among various prospective applications. We open-sourced MATLAB implementation. It consists of an interactive and real-time acoustic tool, MATLAB functions, and supporting materials.      
### 106.A "DIY" data acquisition system for acoustic field measurements under harsh conditions  [ :arrow_down: ](https://arxiv.org/pdf/2010.13158.pdf)
>  Monitoring active volcanos is an ongoing and important task helping to understand and predict volcanic eruptions. In recent years, analysing the acoustic properties of eruptions became more relevant. We present an inexpensive, lightweight, portable, easy to use and modular acoustic data acquisition system for field measurements that can record data with up to 100~kHz. The system is based on a Raspberry Pi 3 B running a custom build bare metal operating system. It connects to an external analog - digital converter with the microphone sensor. A GPS receiver allows the logging of the position and in addition the recording of a very accurate time signal synchronously to the acoustic data. With that, it is possible for multiple modules to effectively work as a single microphone array. The whole system can be build with low cost and demands only minimal technical infrastructure. We demonstrate a possible use of such a microphone array by deploying 20 modules on the active volcano \textit{Stromboli} in the Aeolian Islands by Sicily, Italy. We use the collected acoustic data to indentify the sound source position for all recorded eruptions.      
### 107.Two-stage Textual Knowledge Distillation to Speech Encoder for Spoken Language Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2010.13105.pdf)
>  End-to-end approaches open a new way for more accurate and efficient spoken language understanding (SLU) systems by alleviating the drawbacks of traditional pipeline systems. Previous works exploit textual information for an SLU model via pre-training with automatic speech recognition or fine-tuning with knowledge distillation. To utilize textual information more effectively, this work proposes a two-stage textual knowledge distillation method that matches utterance-level representations and predicted logits of two modalities during pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a speech encoder because it captures general and rich features. Furthermore, we improve the performance, especially in a low-resource scenario, with data augmentation methods by randomly masking spans of discrete audio tokens and contextualized hidden representations. Consequently, we push the state-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy in the full dataset setting and 99.5% in the 10% subset setting. Throughout the ablation studies, we empirically verify that all used methods are crucial to the final performance, providing the best practice for spoken language understanding. Code to reproduce our results will be available upon publication.      
### 108.Gramian-Based Adaptive Combination Policies for Diffusion Learning over Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.13104.pdf)
>  This paper presents an adaptive combination strategy for distributed learning over diffusion networks. Since learning relies on the collaborative processing of the stochastic information at the dispersed agents, the overall performance can be improved by designing combination policies that adjust the weights according to the quality of the data. Such policies are important because they would add a new degree of freedom and endow multi-agent systems with the ability to control the flow of information over their edges for enhanced performance. Most adaptive and static policies available in the literature optimize certain performance metrics related to steady-state behavior, to the detriment of transient behavior. In contrast, we develop an adaptive combination rule that aims at optimizing the transient learning performance, while maintaining the enhanced steady-state performance obtained using policies previously developed in the literature.      
### 109.An Improved Event-Independent Network for Polyphonic Sound Event Localization and Detection  [ :arrow_down: ](https://arxiv.org/pdf/2010.13092.pdf)
>  Polyphonic sound event localization and detection (SELD), which jointly performs sound event detection (SED) and direction-of-arrival (DoA) estimation, has better real-world applicability than separate SED or DoA estimation. It detects the type and occurrence time of sound events as well as their corresponding DoA angles simultaneously. We study the SELD task from a multi-task learning perspective. Two open problems are addressed in the paper. Firstly, to detect overlapping sound events of the same type but with different DoAs, we propose to use a trackwise output format and solve the accompanying track permutation problem with permutation-invariant training. Multi-head self-attention is further used to separate tracks. Secondly, a previous finding is that, by using hard parameter-sharing, SELD suffers from a performance loss compared with learning the subtasks separately. This is solved by a soft parameter-sharing scheme. We term the proposed method as Event Independent Network V2 (EINV2), which is an improved version of our previously-proposed method and an end-to-end network for SELD. We show that our proposed EINV2 for joint SED and DoA estimation outperforms previous methods by a large margin. In addition, a single EINV2 model with a VGG-style architecture has comparable performance to state-of-the-art ensemble models. Source code is available.      
### 110.Fast and Accurate Light Field Saliency Detection through Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2010.13073.pdf)
>  Light field saliency detection---important due to utility in many vision tasks---still lack speed and can improve in accuracy. Due to the formulation of the saliency detection problem in light fields as a segmentation task or a "memorizing" tasks, existing approaches consume unnecessarily large amounts of computational resources for (training and) testing leading to execution times is several seconds. We solve this by aggressively reducing the large light-field images to a much smaller three-channel feature map appropriate for saliency detection using an RGB image saliency detector. We achieve this by introducing a novel convolutional neural network based features extraction and encoding module. Our saliency detector takes $0.4$ s to process a light field of size $9\times9\times512\times375$ in a CPU and is significantly faster than existing systems, with better or comparable accuracy. Our work shows that extracting features from light fields through aggressive size reduction and the attention results in a faster and accurate light-field saliency detector.      
### 111.LIRO: Tightly Coupled Lidar-Inertia-Ranging Odometry  [ :arrow_down: ](https://arxiv.org/pdf/2010.13072.pdf)
>  In recent years, thanks to the continuously reduced cost and weight of 3D Lidar, the applications of this type of sensor in robotics community have become increasingly popular. Despite many progresses, estimation drift and tracking loss are still prevalent concerns associated with these systems. However, in theory these issues can be resolved with the use of some observations to fixed landmarks in the environments. This motivates us to investigate a tightly coupled sensor fusion scheme of Ultra-Wideband (UWB) range measurements with Lidar and inertia measurements. First, data from IMU, Lidar and UWB are associated with the robot's states on a sliding windows based on their timestamps. Then, we construct a cost function comprising of factors from UWB, Lidar and IMU preintegration measurements. Finally an optimization process is carried out to estimate the robot's position and orientation. Via some real world experiments, we show that the method can effectively resolve the drift issue, while only requiring two or three anchors deployed in the environment.      
### 112.Speakerfilter-Pro: an improved target speaker extractor combines the time domain and frequency domain  [ :arrow_down: ](https://arxiv.org/pdf/2010.13053.pdf)
>  This paper introduces an improved target speaker extractor, referred to as Speakerfilter-Pro, based on our previous Speakerfilter model. The Speakerfilter uses a bi-direction gated recurrent unit (BGRU) module to characterize the target speaker from anchor speech and use a convolutional recurrent network (CRN) module to separate the target speech from a noisy signal.Different from the Speakerfilter, the Speakerfilter-Pro sticks a WaveUNet module in the beginning and the ending, respectively. The WaveUNet has been proven to have a better ability to perform speech separation in the time domain. In order to extract the target speaker information better, the complex spectrum instead of the magnitude spectrum is utilized as the input feature for the CRN module. Experiments are conducted on the two-speaker dataset (WSJ0-mix2) which is widely used for speaker extraction. The systematic evaluation shows that the Speakerfilter-Pro outperforms the Speakerfilter and other baselines, and achieves a signal-to-distortion ratio (SDR) of 14.95 dB.      
### 113.Orthros: Non-autoregressive End-to-end Speech Translation with Dual-decoder  [ :arrow_down: ](https://arxiv.org/pdf/2010.13047.pdf)
>  Fast inference speed is an important goal towards real-world deployment of speech translation (ST) systems. End-to-end (E2E) models based on the encoder-decoder architecture are more suitable for this goal than traditional cascaded systems, but their effectiveness regarding decoding speed has not been explored so far. Inspired by recent progress in non-autoregressive (NAR) methods in text-based translation, which generates target tokens in parallel by eliminating conditional dependencies, we study the problem of NAR decoding for E2E-ST. We propose a novel NAR E2E-ST framework, Orthoros, in which both NAR and autoregressive (AR) decoders are jointly trained on the shared speech encoder. The latter is used for selecting better translation among various length candidates generated from the former, which dramatically improves the effectiveness of a large length beam with negligible overhead. We further investigate effective length prediction methods from speech inputs and the impact of vocabulary sizes. Experiments on four benchmarks show the effectiveness of the proposed method in improving inference speed while maintaining competitive translation quality compared to state-of-the-art AR E2E-ST systems.      
### 114.Enactive Mandala: Audio-visualizing Brain Waves  [ :arrow_down: ](https://arxiv.org/pdf/2010.13035.pdf)
>  We are exploring the design and implementation of artificial expressions, kinetic audio-visual representations of real-time physiological data that reflect emotional and cognitive state. In this work, we demonstrate a prototype, the Enactive Mandala, which maps real-time EEG signals to modulate ambient music and animated visual music. Transparent real-time audio-visual feedback of brainwave qualities supports intuitive insight into the connection between thoughts and physiological states.      
### 115.Multi-task Supervised Learning via Cross-learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.12993.pdf)
>  In this paper we consider a problem known as multi-task learning, consisting of fitting a set of classifier or regression functions intended for solving different tasks. In our novel formulation, we couple the parameters of these functions, so that they learn in their task specific domains while staying close to each other. This facilitates cross-fertilization in which data collected across different domains help improving the learning performance at each other task. First, we present a simplified case in which the goal is to estimate the means of two Gaussian variables, for the purpose of gaining some insights on the advantage of the proposed cross-learning strategy. Then we provide a stochastic projected gradient algorithm to perform cross-learning over a generic loss function. If the number of parameters is large, then the projection step becomes computationally expensive. To avoid this situation, we derive a primal-dual algorithm that exploits the structure of the dual problem, achieving a formulation whose complexity only depends on the number of tasks. Preliminary numerical experiments for image classification by neural networks trained on a dataset divided in different domains corroborate that the cross-learned function outperforms both the task-specific and the consensus approaches.      
### 116.Unsupervised Learning of Disentangled Speech Content and Style Representation  [ :arrow_down: ](https://arxiv.org/pdf/2010.12973.pdf)
>  We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of: (1) a local encoder that captures per-frame information; (2) a global encoder that captures per-utterance information; and (3) a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that (1) the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates (WER), even with a different global encoding; (2) the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker.      
### 117.Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2010.12970.pdf)
>  Denoising is a fundamental challenge in scientific imaging. Deep convolutional neural networks (CNNs) provide the current state of the art in denoising natural images, where they produce impressive results. However, their potential has barely been explored in the context of scientific imaging. Denoising CNNs are typically trained on real natural images artificially corrupted with simulated noise. In contrast, in scientific applications, noiseless ground-truth images are usually not available. To address this issue, we propose a simulation-based denoising (SBD) framework, in which CNNs are trained on simulated images. We test the framework on data obtained from transmission electron microscopy (TEM), an imaging technique with widespread applications in material science, biology, and medicine. SBD outperforms existing techniques by a wide margin on a simulated benchmark dataset, as well as on real data. Apart from the denoised images, SBD generates likelihood maps to visualize the agreement between the structure of the denoised image and the observed data. Our results reveal shortcomings of state-of-the-art denoising architectures, such as their small field-of-view: substantially increasing the field-of-view of the CNNs allows them to exploit non-local periodic patterns in the data, which is crucial at high noise levels. In addition, we analyze the generalization capability of SBD, demonstrating that the trained networks are robust to variations of imaging parameters and of the underlying signal structure. Finally, we release the first publicly available benchmark dataset of TEM images, containing 18,000 examples.      
### 118.Power Allocation for Relayed OFDM with Index Modulation Assisted by Artificial Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.12959.pdf)
>  In this letter, we propose a power allocation scheme for relayed orthogonal frequency division multiplexing with index modulation (OFDM-IM) systems. The proposed power allocation scheme replies on artificial neural network (ANN) and deep learning to allocate transmit power among various subcarriers at the source and relay nodes. The objective of the power allocation scheme is to minimize the overall transmit power under a set of constraints. Without loss of generality, we assume all subcarriers at source and relay nodes are independently distributed with different statistical distribution parameters. The relay node adopts the fixed-gain amplify-and-forward (FG AF) relaying protocol. We employ the adaptive moment estimation method (Adam) to implement back-propagation learning and simulate the proposed power allocation scheme. The analytical and simulation results show that the proposed power allocation scheme is able to provide comparable performance as the optimal solution but with lower complexity.      
### 119.DeepAtrophy: Teaching a Neural Network to Differentiate Progressive Changes from Noise on Longitudinal MRI in Alzheimer's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2010.12948.pdf)
>  Volume change measures derived from longitudinal MRI (e.g. hippocampal atrophy) are a well-studied biomarker of disease progression in Alzheimer's Disease (AD) and are used in clinical trials to track the therapeutic efficacy of disease-modifying treatments. However, longitudinal MRI change measures can be confounded by non-biological factors, such as different degrees of head motion and susceptibility artifact between pairs of MRI scans. We hypothesize that deep learning methods applied directly to pairs of longitudinal MRI scans can be trained to differentiate between biological changes and non-biological factors better than conventional approaches based on deformable image registration. To achieve this, we make a simplifying assumption that biological factors are associated with time (i.e. the hippocampus shrinks overtime in the aging population) whereas non-biological factors are independent of time. We then formulate deep learning networks to infer the temporal order of same-subject MRI scans input to the network in arbitrary order; as well as to infer ratios between interscan intervals for two pairs of same-subject MRI scans. In the test dataset, these networks perform better in tasks of temporal ordering (89.3%) and interscan interval inference (86.1%) than a state-of-the-art deformation-based morphometry method ALOHA (76.6% and 76.1% respectively) (Das et al., 2012). Furthermore, we derive a disease progression score from the network that is able to detect a group difference between 58 preclinical AD and 75 beta-amyloid-negative cognitively normal individuals within one year, compared to two years for ALOHA. This suggests that deep learning can be trained to differentiate MRI changes due to biological factors (tissue loss) from changes due to non-biological factors, leading to novel biomarkers that are more sensitive to longitudinal changes at the earliest stages of AD.      
### 120.Recurrent Neural Based Electricity Load Forecasting of G-20 Members  [ :arrow_down: ](https://arxiv.org/pdf/2010.12934.pdf)
>  Forecasting the actual amount of electricity with respect to the need/demand of the load is always been a challenging task for each power plants based generating stations. Due to uncertain demand of electricity at receiving end of station causes several challenges such as: reduction in performance parameters of generating and receiving end stations, minimization in revenue, increases the jeopardize for the utility to predict the future energy need for a company etc. With this issues, the precise forecasting of load at the receiving end station is very consequential parameter to establish the impeccable balance between supply and demand chain. In this paper, the load forecasting of G-20 members have been performed utilizing the Recurrent Neural Network coupled with sliding window approach for data generation. During the experimentation we have achieved Mean Absolute Test Error of 16.2193 TWh using LSTM.      
### 121.Optimizing Multi-UAV Deployment in 3D Space to Minimize Task Completion Time in UAV-Enabled Mobile Edge Computing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.12894.pdf)
>  In Unmanned Aerial Vehicle (UAV)-enabled mobile edge computing (MEC) systems, UAVs can carry edge servers to help ground user equipment (UEs) offloading their computing tasks to the UAVs for execution. This paper aims to minimize the total time required for the UAVs to complete the offloaded tasks, while optimizing the three-dimensional (3D) deployment of UAVs, including their flying height and horizontal positions. Although the formulated optimization is a mixed integer nonlinear programmming, we convert it to a convex problem and develop a successive convex approximation (SCA) based algorithm to effectively solve it. The simulation results show that the joint optimization of the horizontal and the vertical position of a group of UAVs can achieve better performance than the traditional algorithms.      
### 122.Force and state-feedback control for robots with non-collocated environmental and actuator forces  [ :arrow_down: ](https://arxiv.org/pdf/2010.12889.pdf)
>  In this paper, we present an impedance control design for multi-variable linear and nonlinear robotic systems. The control design considers force and state feedback to improve the performance of the closed loop. Simultaneous feedback of forces and states allows the controller for an extra degree of freedom to approximate the desired impedance port behaviour. A numerical analysis is used to demonstrate the desired impedance closed-loop behaviour.      
### 123.Stochastic Analysis of Cooperative Satellite-UAV Communications  [ :arrow_down: ](https://arxiv.org/pdf/2010.12875.pdf)
>  In this paper, a dual-hop cooperative satellite-unmanned aerial vehicle (UAV) communication system including a satellite (S), a group of cluster headers (CHs), which are respectively with a group of uniformly distributed UAVs, is considered. Specifically, these CHs serve as aerial decode-and-forward relays to forward the information transmitted by S to UAVs. Moreover, free-space optical (FSO) and radio frequency (RF) technologies are respectively adopted over S-CH and CH-UAV links to exploit FSO's high directivity over long-distance transmission and RF's omnidirectional coverage ability. The positions of the CHs in the 3-dimensional space follow the MatÃ©rn hard-core point processes type-II in which each CH can not be closer to any other ones than a predefined distance. Three different cases over CH-UAV links are considered during the performance modeling: interference-free, interference-dominated, and interference-and-noise cases. Then, the coverage performance of S-CH link and the CH-UAV links under three cases is studied and the closed-form analytical expressions of the coverage probability (CP) over both links are derived. Also, the asymptotic expressions for the CP over S-CH link and CH-UAV link in interference-free case are derived. Finally, numerical results are provided to validate our proposed analytical models and thus some meaningful conclusions are achieved.      
### 124.Stop Bugging Me! Evading Modern-Day Wiretapping Using Adversarial Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2010.12809.pdf)
>  Mass surveillance systems for voice over IP (VoIP) conversations pose a huge risk to privacy. These automated systems use learning models to analyze conversations, and upon detecting calls that involve specific topics, route them to a human agent. In this study, we present an adversarial learning-based framework for privacy protection for VoIP conversations. We present a novel algorithm that finds a universal adversarial perturbation (UAP), which, when added to the audio stream, prevents an eavesdropper from automatically detecting the conversation's topic. As shown in our experiments, the UAP is agnostic to the speaker or audio length, and its volume can be changed in real-time, as needed. In a real-world demonstration, we use a Teensy microcontroller that acts as an external microphone and adds the UAP to the audio in real-time. We examine different speakers, VoIP applications (Skype, Zoom), audio lengths, and speech-to-text models (Deep Speech, Kaldi). Our results in the real world suggest that our approach is a feasible solution for privacy protection.      
### 125.Passivity properties for regulation of DC networks with stochastic load demand  [ :arrow_down: ](https://arxiv.org/pdf/2010.12791.pdf)
>  In this paper we present new (stochastic) passivity properties for Direct Current (DC) power networks, where the unknown and unpredictable load demand is modelled by a stochastic process. More precisely, the considered power network consists of distributed generation units supplying ZIP loads, i.e., nonlinear loads comprised of impedance (Z), current (I) and power (P) components. Differently from the majority of the results in the literature, where each of these components is assumed to be constant, we consider time-varying loads whose dynamics are described by a class of stochastic differential equations. Finally, we prove that an existing distributed control scheme achieving current sharing and (average) voltage regulation ensures the asymptotic stochastic stability of the controlled network.      
### 126.GAZEV: GAN-Based Zero-Shot Voice Conversion over Non-parallel Speech Corpus  [ :arrow_down: ](https://arxiv.org/pdf/2010.12788.pdf)
>  Non-parallel many-to-many voice conversion is recently attract-ing huge research efforts in the speech processing community. A voice conversion system transforms an utterance of a source speaker to another utterance of a target speaker by keeping the content in the original utterance and replacing by the vocal features from the target speaker. Existing solutions, e.g., StarGAN-VC2, present promising results, only when speech corpus of the engaged speakers is available during model training. AUTOVCis able to perform voice conversion on unseen speakers, but it needs an external pretrained speaker verification model. In this paper, we present our new GAN-based zero-shot voice conversion solution, called GAZEV, which targets to support unseen speakers on both source and target utterances. Our key technical contribution is the adoption of speaker embedding loss on top of the GAN framework, as well as adaptive instance normalization strategy, in order to address the limitations of speaker identity transfer in existing solutions. Our empirical evaluations demonstrate significant performance improvement on output speech quality and comparable speaker similarity to AUTOVC.      
### 127.Learning Fine-Grained Multimodal Alignment for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.12733.pdf)
>  Speech emotion recognition is a challenging task because the emotion expression is complex, multimodal and fine-grained. In this paper, we propose a novel multimodal deep learning approach to perform fine-grained emotion recognition from real-life speeches. We design a temporal alignment pooling mechanism to capture the subtle and fine-grained emotions implied in every utterance. In addition, we propose a cross modality excitation module to conduct sample-specific activations on acoustic embedding dimensions and adaptively recalibrate the corresponding values by latent semantic features. The proposed model is evaluated on two well-known real-world speech emotion recognition datasets. The results demonstrate that our approach is superior on the prediction tasks for multimodal speech utterances, and it outperforms a wide range of baselines in terms of prediction accuracy. In order to encourage the research reproducibility, we make the code publicly available at <a class="link-external link-https" href="https://github.com/hzlihang99/icassp2021_CME.git" rel="external noopener nofollow">this https URL</a>.      
### 128.Octave-Tunable Magnetostatic Wave YIG Resonators on a Chip  [ :arrow_down: ](https://arxiv.org/pdf/2010.12732.pdf)
>  We have designed, fabricated, and characterized magnetostatic wave (MSW) resonators on a chip. The resonators are fabricated by patterning single-crystal yttrium iron garnet (YIG) film on a gadolinium gallium garnet (GGG) substrate and excited by loop-inductor transducers. We achieved this technology breakthrough by developing a YIG film etching process and fabricating thick aluminum coplanar waveguide (CPW) inductor loop around each resonator to individually address and excite MSWs. At 4.77 GHz, the 0.68 square mm resonator achieves a quality factor Q &gt; 5000 with a bias field of 987 Oe. We also demonstrate YIG resonator tuning by more than one octave from 3.63 to 7.63 GHz by applying an in-plane external magnetic field. The measured quality factor of the resonator is consistently over 3000 above 4 GHz. The micromachining technology enables the fabrication of multiple single- and two-port YIG resonators on the same chip with all resonators demonstrating octave tunability and high Q .      
### 129.Dual-path Self-Attention RNN for Real-Time Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2010.12713.pdf)
>  We propose a dual-path self-attention recurrent neural network (DP-SARNN) for time-domain speech enhancement. We improve dual-path RNN (DP-RNN) by augmenting inter-chunk and intra-chunk RNN with a recently proposed efficient attention mechanism. The combination of inter-chunk and intra-chunk attention improves the attention mechanism for long sequences of speech frames. DP-SARNN outperforms a baseline DP-RNN by using a frame shift four times larger than in DP-RNN, which leads to a substantially reduced computation time per utterance. As a result, we develop a real-time DP-SARNN by using long short-term memory (LSTM) RNN and causal attention in inter-chunk SARNN. DP-SARNN significantly outperforms existing approaches to speech enhancement, and on average takes 7.9 ms CPU time to process a signal chunk of 32 ms.      
### 130.Loss-analysis via Attention-scale for Physiologic Time Series  [ :arrow_down: ](https://arxiv.org/pdf/2010.12690.pdf)
>  Physiologic signals have properties across multiple spatial and temporal scales, which can be shown by the complexity-analysis of the coarse-grained physiologic signals by scaling techniques such as the multiscale. Unfortunately, the results obtained from the coarse-grained signals by the multiscale may not fully reflect the properties of the original signals because there is a loss caused by scaling techniques and the same scaling technique may bring different losses to different signals. Another problem is that multiscale does not consider the key observations inherent in the signal. Here, we show a new analysis method for time series called the loss-analysis via attention-scale. We show that multiscale is a special case of attention-scale. The loss-analysis can complement to the complexity-analysis to capture aspects of the signals that are not captured using previously developed measures. This can be used to study ageing, diseases, and other physiologic phenomenon.      
### 131.On Minimum Word Error Rate Training of the Hybrid Autoregressive Transducer  [ :arrow_down: ](https://arxiv.org/pdf/2010.12673.pdf)
>  Hybrid Autoregressive Transducer (HAT) is a recently proposed end-to-end acoustic model that extends the standard Recurrent Neural Network Transducer (RNN-T) for the purpose of the external language model (LM) fusion. In HAT, the blank probability and the label probability are estimated using two separate probability distributions, which provides a more accurate solution for internal LM score estimation, and thus works better when combining with an external LM. Previous work mainly focuses on HAT model training with the negative log-likelihood loss, while in this paper, we study the minimum word error rate (MWER) training of HAT -- a criterion that is closer to the evaluation metric for speech recognition, and has been successfully applied to other types of end-to-end models such as sequence-to-sequence (S2S) and RNN-T models. From experiments with around 30,000 hours of training data, we show that MWER training can improve the accuracy of HAT models, while at the same time, improving the robustness of the model against the decoding hyper-parameters such as length normalization and decoding beam during inference.      
### 132.Low-rank on Graphs plus Temporally Smooth Sparse Decomposition for Anomaly Detection in Spatiotemporal Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.12633.pdf)
>  Anomaly detection in spatiotemporal data is a challenging problem encountered in a variety of applications including hyperspectral imaging, video surveillance, and urban traffic monitoring. Existing anomaly detection methods are most suited for point anomalies in sequence data and cannot deal with temporal and spatial dependencies that arise in spatiotemporal data. In recent years, tensor-based methods have been proposed for anomaly detection to address this problem. These methods rely on conventional tensor decomposition models, not taking the structure of the anomalies into account, and are supervised or semi-supervised. We introduce an unsupervised tensor-based anomaly detection method that takes the sparse and temporally continuous nature of anomalies into account. In particular, the anomaly detection problem is formulated as a robust lowrank + sparse tensor decomposition with a regularization term that minimizes the temporal variation of the sparse part, so that the extracted anomalies are temporally persistent. We also approximate rank minimization with graph total variation minimization to reduce the complexity of the optimization algorithm. The resulting optimization problem is convex, scalable, and is shown to be robust against missing data and noise. The proposed framework is evaluated on both synthetic and real spatiotemporal urban traffic data and compared with baseline methods.      
### 133.Retinal Ganglion Cell Stimulation with an Optically Powered Retinal Prosthesis  [ :arrow_down: ](https://arxiv.org/pdf/2010.12600.pdf)
>  Objective. Clinical trials previously demonstrated the spectacular capacity to elicit visual percepts in blind patients affected with retinal diseases by electrically stimulating the remaining neurons on the retina. However, these implants restored very limited visual acuity and required transcutaneous cables traversing the eyeball, leading to reduced reliability and complex surgery with high postoperative infection risks. Approach. To overcome the limitations imposed by cables, a retinal implant architecture in which near-infrared illumination carries both power and data through the pupil is presented. A high efficiency multi-junction photovoltaic cell transduces the optical power to a CMOS stimulator capable of delivering flexible interleaved sequential stimulation through a diamond microelectrode array. To demonstrate the capacity to elicit a neural response with this approach while complying with the optical irradiance safety limit at the pupil, fluorescence imaging with a calcium indicator is used on a degenerate rat retina. Main results. The power delivered by the laser at safe irradiance of 4 mW/mm2 is shown to be sufficient to both power the stimulator ASIC and elicit a response in retinal ganglion cells (RGCs), with the ability to generate of up to 35 000 pulses per second at the average stimulation threshold. Significance. This confirms the feasibility of wirelessly generating a response in RGCs with a digital stimulation controller that can deliver complex multipolar stimulation patterns at high repetition rates.      
