# ArXiv eess --Mon, 19 Oct 2020
### 1.Classification of Manifest Huntington Disease using Vowel Distortion Measures  [ :arrow_down: ](https://arxiv.org/pdf/2010.08503.pdf)
>  Huntington disease (HD) is a fatal autosomal dominant neurocognitive disorder that causes cognitive disturbances, neuropsychiatric symptoms, and impaired motor abilities (e.g., gait, speech, voice). Due to its progressive nature, HD treatment requires ongoing clinical monitoring of symptoms. Individuals with the Huntingtin gene mutation, which causes HD, may exhibit a range of speech symptoms as they progress from premanifest to manifest HD. Speech-based passive monitoring has the potential to augment clinical information by more continuously tracking manifestation symptoms. Differentiating between premanifest and manifest HD is an important yet understudied problem, as this distinction marks the need for increased treatment. In this work we present the first demonstration of how changes in speech can be measured to differentiate between premanifest and manifest HD. To do so, we focus on one speech symptom of HD: distorted vowels. We introduce a set of Filtered Vowel Distortion Measures (FVDM) which we extract from read speech. We show that FVDM, coupled with features from existing literature, can differentiate between premanifest and manifest HD with 80% accuracy.      
### 2.Automatic Myocardial Disease Prediction From Delayed-Enhancement Cardiac MRI and Clinical Information  [ :arrow_down: ](https://arxiv.org/pdf/2010.08469.pdf)
>  Delayed-enhancement cardiac magnetic resonance (DE-CMR)provides important diagnostic and prognostic information on myocardial viability. The presence and extent of late gadolinium enhancement (LGE)in DE-CMR is negatively associated with the probability of improvement in left ventricular function after revascularization. Moreover, LGE findings can support the diagnosis of several other cardiomyopathies, but their absence does not rule them out, making disease classification by visual assessment difficult. In this work, we propose deep learning neural networks that can automatically predict myocardial disease from patient clinical information and DE-CMR. All the proposed networks achieve very good classification accuracy (&gt;85%). Including information from DE-CMR (directly as images or as metadata following DE-CMR segmentation) is valuable in this classification task, improving the accuracy to 95-100%.      
### 3.VolumeNet: A Lightweight Parallel Network for Super-Resolution of Medical Volumetric Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.08357.pdf)
>  Deep learning-based super-resolution (SR) techniques have generally achieved excellent performance in the computer vision field. Recently, it has been proven that three-dimensional (3D) SR for medical volumetric data delivers better visual results than conventional two-dimensional (2D) processing. However, deepening and widening 3D networks increases training difficulty significantly due to the large number of parameters and small number of training samples. Thus, we propose a 3D convolutional neural network (CNN) for SR of medical volumetric data called ParallelNet using parallel connections. We construct a parallel connection structure based on the group convolution and feature aggregation to build a 3D CNN that is as wide as possible with few parameters. As a result, the model thoroughly learns more feature maps with larger receptive fields. In addition, to further improve accuracy, we present an efficient version of ParallelNet (called VolumeNet), which reduces the number of parameters and deepens ParallelNet using a proposed lightweight building block module called the Queue module. Unlike most lightweight CNNs based on depthwise convolutions, the Queue module is primarily constructed using separable 2D cross-channel convolutions. As a result, the number of network parameters and computational complexity can be reduced significantly while maintaining accuracy due to full channel fusion. Experimental results demonstrate that the proposed VolumeNet significantly reduces the number of model parameters and achieves high precision results compared to state-of-the-art methods.      
### 4.Vibration Influence Evaluation of a Resonant MEMS Scanning System for Automotive Lidars  [ :arrow_down: ](https://arxiv.org/pdf/2010.08327.pdf)
>  This paper demonstrates a vibration test for an operating resonant MEMS scanning system to evaluate the vibration immunity for automotive lidar applications. The MEMS mirror has a reinforcement structure on the backside of the mirror, causing vibration coupling by a mismatch between the center of mass and the rotation axis. An analysis of energy variation is proposed, showing direction dependency of vibration coupling. Vibration influences are evaluated by transient vibration response and vibration frequency sweep using a single tone vibration for translational y- and z- axis. The measurement results demonstrate standard deviation (STD) amplitude and frequency errors are up to 1.64 % and 0.26 %, respectively, for 2 grms single tone vibrations on y axis. The simulation results also show a good agreement with both measurements, proving the proposed vibration coupling mechanism of the MEMS mirror. The phased locked loop (PLL) improves the STD amplitude and frequency errors to 0.91 % and 0.15 % for y axis vibration, corresponding to 44.4 % and 43.0 % reduction, respectively, showing the benefit of a controlled MEMS mirror for reliable automotive MEMS lidars.      
### 5.Learning Accurate Entropy Model with Global Reference for Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2010.08321.pdf)
>  In recent deep image compression neural networks, the entropy model plays a critical role in estimating the prior distribution of deep image encodings. Existing methods combine hyperprior with local context in the entropy estimation function. This greatly limits their performance due to the absence of a global vision. In this work, we propose a novel Global Reference Model for image compression to effectively leverage both the local and the global context information, leading to an enhanced compression rate. The proposed method scans decoded latents and then finds the most relevant latent to assist the distribution estimating of the current latent. A by-product of this work is the innovation of a mean-shifting GDN module that further improves the performance. Experimental results demonstrate that the proposed model outperforms the rate-distortion performance of most of the state-of-the-art methods in the industry.      
### 6.Partial Discharge Direction of Arrival Estimation in Air-insulated Substation by UHF Wireless Array and RSSI Maximum Likelihood Estimator  [ :arrow_down: ](https://arxiv.org/pdf/2010.08309.pdf)
>  The quick detection and localization of partial discharge (PD) in an air-insulated substation (AIS) based on ultrahigh-frequency (UHF) sensor arrays are efficient for power equipment monitoring. The adopted UHF PD time difference of arrival (TDOA) methods mainly use the time difference of electromagnetic wave signals. Thus, the system requires both a high sampling rate and time synchronization accuracy, leading to a high cost and large size. In this study, the feasibility and accuracy of PD DOA in an AIS were investigated using a UHF wireless sensor array and the received signal strength indicator. First, the power pattern of the designed UHF wireless sensor array was obtained via an offline experiment. Then, a statistical approach to the PD DOA method based on the maximum likelihood estimator was employed to obtain the preliminary DOA result. Finally, interpolation and clustering algorithms were used to improve the accuracy of the DOA. A laboratory test was conducted. The average error of the PD DOA was less than 6Â°, and the cost-effectiveness and portability were clearly improved.      
### 7.Quantitative Digital Microscopy with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.08260.pdf)
>  Video microscopy has a long history of providing insights and breakthroughs for a broad range of disciplines, from physics to biology. Image analysis to extract quantitative information from video microscopy data has traditionally relied on algorithmic approaches, which are often difficult to implement, time consuming, and computationally expensive. Recently, alternative data-driven approaches using deep learning have greatly improved quantitative digital microscopy, potentially offering automatized, accurate, and fast image analysis. However, the combination of deep learning and video microscopy remains underutilized primarily due to the steep learning curve involved in developing custom deep-learning solutions. To overcome this issue, we introduce a software, DeepTrack 2.0, to design, train and validate deep-learning solutions for digital microscopy. We use it to exemplify how deep learning can be employed for a broad range of applications, from particle localization, tracking and characterization to cell counting and classification. Thanks to its user-friendly graphical interface, DeepTrack 2.0 can be easily customized for user-specific applications, and, thanks to its open-source object-oriented programming, it can be easily expanded to add features and functionalities, potentially introducing deep-learning-enhanced video microscopy to a far wider audience.      
### 8.Tongji University Team for the VoxCeleb Speaker Recognition Challenge 2020  [ :arrow_down: ](https://arxiv.org/pdf/2010.08179.pdf)
>  In this report, we describe the submission of Tongji University team to the CLOSE track of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020 at Interspeech 2020. We investigate different speaker recognition systems based on the popular ResNet-34 architecture, and train multiple variants via various loss functions. Both Offline and online data augmentation are introduced to improve the diversity of the training set, and score normalization with the exhaustive grid search is applied in the post-processing. Our best fusion of five selected systems for the CLOSE track achieves 0.2800 DCF and 4.7770% EER on the challenge.      
### 9.Statistical Graph Signal Recovery Using Variational Bayes  [ :arrow_down: ](https://arxiv.org/pdf/2010.08137.pdf)
>  This paper investigates the problem of graph signal recovery (GSR) when the topology of the graph is not known in advance. In this paper, the elements of the weighted adjacency matrix is statistically related to normal distribution and the graph signal is assumed to be Gaussian Markov Random Field (GMRF). Then, the problem of GSR is solved by a Variational Bayes (VB) algorithm in a Bayesian manner by computing the posteriors in a closed form. The posteriors of the elements of weighted adjacency matrix are proved to have a new distribution which we call it generalized compound confluent hypergeometric (GCCH) distribution. Moreover, the variance of the noise is estimated by calculating its posterior via VB. The simulation results on synthetic and real-world data shows the superiority of the proposed Bayesian algorithm over some state-of-the-art algorithms in recovering the graph signal.      
### 10.Model-based Decentralized Bayesian Algorithm for Distributed Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2010.08135.pdf)
>  In this paper, a novel model-based distributed compressive sensing (DCS) algorithm is proposed. DCS exploits the inter-signal correlations and has the capability to jointly recover multiple sparse signals. Proposed approach is a Bayesian decentralized algorithm which uses the type 1 joint sparsity model (JSM-1) and exploits the intra-signal correlations, as well as the inter-signal correlations. Compared to the conventional DCS algorithm, which only exploit the joint sparsity of the signals, the proposed approach takes the intra- and inter-scale dependencies among the wavelet coefficients into account to enable the utilization of the individual signal structure. Furthermore, the Bessel K-form (BKF) is used as the prior distribution which has a sharper peak at zero and heavier tails than the Gaussian distribution. The variational Bayesian (VB) inference is employed to perform the posterior distributions and acquire a closed-form solution for model parameters. Simulation results demonstrate that the proposed algorithm have good recovery performance in comparison with state-of the-art techniques.      
### 11.Joint Optimization for Coordinated Charging Control of Commercial Electric Vehicles Under Distributed Hydrogen Energy Supply  [ :arrow_down: ](https://arxiv.org/pdf/2010.08121.pdf)
>  The transition to the zero-carbon city is underway accelerating recently. Hydrogen energy and electric vehicles (EVs) are promising solutions on the supply and demand sides of zero-carbon power system. This paper presents a novel architecture that includes the hydrogen production stations (HPSs), fast charging stations (FCSs) and commercial EV fleet for passenger transportation. The proposed architecture jointly optimizes the distributed hydrogen energy dispatch and the EV charging location selection to minimize the operating cost, and is formulated by a time-varying bi-level bipartite graph (T-BBG) model for real-time operation. Based on the receding-horizon control framework, we develop a bi-level iteration optimization method combining mixed-integer linear programming (MILP) and Kuhn-Munkres (KM) algorithm to solve the joint optimization problem whose optimality is proved theoretically. The effectiveness of the proposed architecture on reducing the operating cost is verified via case studies in Shanghai. The proposed method outperforms other strategies and improves the performance by at least 13% which shows the potential economic benefits of the joint architecture. The convergence and impact of the pile number, battery capacity, EV speed and penalty factor are assessed.      
### 12.Early-stage COVID-19 diagnosis in presence of limited posteroanterior chest X-ray images via novel Pinball-OCSVM  [ :arrow_down: ](https://arxiv.org/pdf/2010.08115.pdf)
>  It is evident that the infection with this severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) starts with the upper respiratory tract and as the virus grows, the infection can progress to lungs and develop pneumonia. According to the statistics, approximately 14\% of the infected people with COVID-19 have severe cough and shortness of breath due to pneumonia, because as the viral infection increases, it damages the alveoli (small air sacs) and surrounding tissues. The conventional way of COVID-19 diagnosis is reverse transcription polymerase chain reaction (RT-PCR), which is less sensitive during early stages specially, if the patient is asymptomatic that may further lead to more severe pneumonia. To overcome this problem an early diagnosis method is proposed in this paper via one-class classification approach using a novel pinball loss function based one-class support vector machine (PB-OCSVM) considering posteroanterior chest X-ray images. Recently, several automated COVID-19 diagnosis models have been proposed based on various deep learning architectures to identify pulmonary infections using publicly available chest X-ray (CXR) where the presence of less number of COVID-19 positive samples compared to other classes (normal, pneumonia and Tuberculosis) raises the challenge for unbiased learning in deep learning models that has been solved using class balancing techniques which however should be avoided in any medical diagnosis process. Inspired by this phenomenon, this article proposes a novel PB-OCSVM model to work in presence of limited COVID-19 positive CXR samples with objectives to maximize the learning efficiency while minimize the false-positive and false-negative predictions. The proposed model outperformed over recently published deep learning approaches where accuracy, precision, specificity and sensitivity are used as performance measure parameters.      
### 13.Stability and Robustness of the Disturbance Observer-based Motion Control Systems in Discrete-Time Domain  [ :arrow_down: ](https://arxiv.org/pdf/2010.08075.pdf)
>  This paper analyses the robust stability and performance of the Disturbance Observer- (DOb-) based digital motion control systems in discrete-time domain. It is shown that the phase margin and the robustness of the digital motion controller can be directly adjusted by tuning the nominal plant model and the bandwidth of the observer. However, they have upper and lower bounds due to robust stability and performance constraints as well as noise-sensitivity. The constraints on the design parameters of the DOb change when the digital motion controller is synthesised by measuring different states of a servo system. For example, the bandwidth of the DOb is limited by noise-sensitivity and waterbed effect when velocity and position measurements are employed in the digital robust motion controller synthesis. The robustness constraint due to the waterbed effect is removed when the DOb is implemented by acceleration measurement. The design constraints on the nominal plant model and the bandwidth of the observer are analytically derived by employing the generalised Bode Integral Theorem in discrete-time. The proposed design constraints allow one to systematically synthesise a high-performance DOb-based digital robust motion controller. Experimental results are given to verify the proposed analysis and synthesis methods.      
### 14.Risk-Informed Participation in T&amp;D Markets  [ :arrow_down: ](https://arxiv.org/pdf/2010.08008.pdf)
>  Power producers can exhibit strategic behavior in electricity markets to maximize their profits. This behavior is more pronounced with the deregulation of distribution markets, which offers an opportunity for profit arbitrage between transmission and distribution (T&amp;D) markets. However, the temporally distinct nature of these two markets introduces a significant risk in profit for such producers. This paper derives its motivation from the perspective of a strategic producer and develops a Single Leader Multi-Follower (SLMF) game for deriving its participation strategies in T&amp;D markets, while accounting for different T&amp;D coordination schemes based on the individual market Gate Closure Times (GCT). We compare and contrast joint and sequential market clearing models with regulated and deregulated distribution environments and evaluate the risk of producer by leveraging consistent and coherent risk measures. SLMF game is reformulated as a Mathematical Program with Equilibrium Constraints (MPEC) and is solved using the seminal Scholtes's relaxation scheme. We validate the efficacy of our model and solution approach via the case study carried out on the 11-zone New York ISO, and 7-bus Manhattan power networks, used as transmission and distribution markets, respectively.      
### 15.Categorical Semantics of Cyber-Physical Systems Theory  [ :arrow_down: ](https://arxiv.org/pdf/2010.08003.pdf)
>  Cyber-physical systems require the construction and management of various models to assure their correct, safe, and secure operation. These various models are necessary because of the coupled physical and computational dynamics present in cyber-physical systems. However, to date the different model views of cyber-physical systems are largely related informally, which raises issues with the degree of formal consistency between those various models of requirements, system behavior, and system architecture. We present a category-theoretic framework to make different types of composition explicit in the modeling and analysis of cyber-physical systems, which could assist in verifying the system as a whole. This compositional framework for cyber-physical systems gives rise to unified system models, where system behavior is hierarchically decomposed and related to a system architecture using the systems-as-algebras <a class="link-external link-http" href="http://paradigm.As" rel="external noopener nofollow">this http URL</a> part of this paradigm, we show that an algebra of (safety) contracts generalizes over the state of the art, providing more uniform mathematical tools for constraining the behavior over a richer set of composite cyber-physical system models, which has the potential of minimizing or eliminating hazardous behavior.      
### 16.The Shift to 6G Communications: Vision and Requirements  [ :arrow_down: ](https://arxiv.org/pdf/2010.07993.pdf)
>  The sixth-generation (6G) wireless communication network is expected to integrate the terrestrial, aerial, and maritime communications into a robust network which would be more reliable, fast, and can support a massive number of devices with ultra-low latency requirements. The researchers around the globe are proposing cutting edge technologies such as artificial intelligence (AI)/machine learning (ML), quantum communication/quantum machine learning (QML), blockchain, tera-Hertz and millimeter waves communication, tactile Internet, non-orthogonal multiple access (NOMA), small cells communication, fog/edge computing, etc., as the key technologies in the realization of beyond 5G (B5G) and 6G communications. In this article, we provide a detailed overview of the 6G network dimensions with air interface and associated potential technologies. More specifically, we highlight the use cases and applications of the proposed 6G networks in various dimensions. Furthermore, we also discuss the key performance indicators (KPI) for the B5G/6G network, challenges, and future research opportunities in this domain.      
### 17.Adaptive Feature Selection for End-to-End Speech Translation  [ :arrow_down: ](https://arxiv.org/pdf/2010.08518.pdf)
>  Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR encoder and apply AFS to dynamically estimate the importance of each encoded speech feature to SR. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take L0DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ~84% temporal features, yielding an average translation gain of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation)      
### 18.Design of periodic scheduling and control for networked systems under random data loss  [ :arrow_down: ](https://arxiv.org/pdf/2010.08447.pdf)
>  This paper deals with Networked Control Systems (NCSs) whose shared networks have limited communication capacity and are prone to data losses. We assume that among (N) plants, only (M &lt; N) plants can communicate with their controllers at any time instant. In addition, a control input, at any time instant, is lost in a channel with a probability (p). Our contributions are threefold. First, we identify necessary and sufficient conditions on the open-loop and closed-loop dynamics of the plants that ensure existence of purely time-dependent periodic scheduling sequences under which stability of each plant is preserved for all admissible data loss signals. Second, given the open-loop and closed-loop dynamics of the plants, relevant parameters of the shared network and a period for the scheduling sequence, we present an algorithm that verifies our stability conditions and if satisfied, designs stabilizing scheduling sequences. Otherwise, the algorithm reports non-existence of a stabilizing periodic scheduling sequence with the given period and stability margins. Third, given the plant matrices, the parameters of the network and a period for the scheduling sequence, we present an algorithm that designs static state-feedback controllers such that our stability conditions are satisfied. The main apparatus for our analysis is a switched systems representation of the individual plants in an NCS whose switching signals are time-inhomogeneous Markov chains. Our stability conditions rely on the existence of sets of symmetric and positive definite matrices that satisfy certain (in)equalities.      
### 19.Policy Gradient for Continuing Tasks in Non-stationary Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2010.08443.pdf)
>  Reinforcement learning considers the problem of finding policies that maximize an expected cumulative reward in a Markov decision process with unknown transition probabilities. In this paper we consider the problem of finding optimal policies assuming that they belong to a reproducing kernel Hilbert space (RKHS). To that end we compute unbiased stochastic gradients of the value function which we use as ascent directions to update the policy. A major drawback of policy gradient-type algorithms is that they are limited to episodic tasks unless stationarity assumptions are imposed. Hence preventing these algorithms to be fully implemented online, which is a desirable property for systems that need to adapt to new tasks and/or environments in deployment. The main requirement for a policy gradient algorithm to work is that the estimate of the gradient at any point in time is an ascent direction for the initial value function. In this work we establish that indeed this is the case which enables to show the convergence of the online algorithm to the critical points of the initial value function. A numerical example shows the ability of our online algorithm to learn to solve a navigation and surveillance problem, in which an agent must loop between to goal locations. This example corroborates our theoretical findings about the ascent directions of subsequent stochastic gradients. It also shows how the agent running our online algorithm succeeds in learning to navigate, following a continuing cyclic trajectory that does not comply with the standard stationarity assumptions in the literature for non episodic training.      
### 20.Are Multiple Cross-Correlation Identities better than just Two? Improving the Estimate of Time Differences-of-Arrivals from Blind Audio Signals  [ :arrow_down: ](https://arxiv.org/pdf/2010.08428.pdf)
>  Given an unknown audio source, the estimation of time differences-of-arrivals (TDOAs) can be efficiently and robustly solved using blind channel identification and exploiting the cross-correlation identity (CCI). Prior "blind" works have improved the estimate of TDOAs by means of different algorithmic solutions and optimization strategies, while always sticking to the case N = 2 microphones. But what if we can obtain a direct improvement in performance by just increasing N? In this paper we try to investigate this direction, showing that, despite the arguable simplicity, this is capable of (sharply) improving upon state-of-the-art blind channel identification methods based on CCI, without modifying the computational pipeline. Inspired by our results, we seek to warm up the community and the practitioners by paving the way (with two concrete, yet preliminary, examples) towards joint approaches in which advances in the optimization are combined with an increased number of microphones, in order to achieve further improvements.      
### 21.Volumetric Calculation of Quantization Error in 3-D Vision Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.08390.pdf)
>  This paper investigates how the inherent quantization of camera sensors introduces uncertainty in the calculated position of an observed feature during 3-D mapping. It is typically assumed that pixels and scene features are points, however, a pixel is a two-dimensional area that maps onto multiple points in the scene. This uncertainty region is a bound for quantization error in the calculated point positions. Earlier studies calculated the volume of two intersecting pixel views, approximated as a cuboid, by projecting pyramids and cones from the pixels into the scene. In this paper, we reverse this approach by generating an array of scene points and calculating which scene points are detected by which pixel in each camera. This enables us to map the uncertainty regions for every pixel correspondence for a given camera system in one calculation, without approximating the complex shapes. The dependence of the volumes of the uncertainty regions on camera baseline length, focal length, pixel size, and distance to object, shows that earlier studies overestimated the quantization error by at least a factor of two. For static camera systems the method can also be used to determine volumetric scene geometry without the need to calculate disparity maps.      
### 22.A Signal Peak Separation Index for axisymmetric B-tensor encoding  [ :arrow_down: ](https://arxiv.org/pdf/2010.08389.pdf)
>  Diffusion-weighted MRI (DW-MRI) has recently seen a rising interest in planar, spherical and general B-tensor encodings. Some of these sequences have aided traditional linear encoding in the estimation of white matter microstructural features, generally by making DW-MRI less sensitive to the orientation of axon fascicles in a voxel. However, less is known about their potential to make the signal more sensitive to fascicle orientation, especially in crossing-fascicle voxels. <br>Although planar encoding has been commended for the resemblance of its signal with the voxel's orientation distribution function (ODF), linear encoding remains the near undisputed method of choice for orientation estimation. This paper presents a theoretical framework to gauge the sensitivity of axisymmetric B-tensors to fascicle orientations. A signal peak separation index (SPSI) is proposed, motivated by theoretical considerations on a simple multi-tensor model of fascicle crossing. Theory and simulations confirm the intuition that linear encoding, because it maximizes B-tensor anisotropy, possesses an intrinsic advantage over all other axisymmetric B-tensors. At identical SPSI however, oblate B-tensors yield higher signal and may be more robust to acquisition noise than their prolate counterparts. The proposed index relates the properties of the B-tensor to those of the tissue microstructure in a straightforward way and can thus guide the design of diffusion sequences for improved orientation estimation and tractography.      
### 23.Second Harmonic Imaging Enhanced by Deep Learning Decipher  [ :arrow_down: ](https://arxiv.org/pdf/2010.08211.pdf)
>  Wavefront sensing and reconstruction are widely used for adaptive optics, aberration correction, and high-resolution optical phase imaging. Traditionally, interference and/or microlens arrays are used to convert the optical phase into intensity variation. Direct imaging of distorted wavefront usually results in complicated phase retrieval with low contrast and low sensitivity. Here, a novel approach has been developed and experimentally demonstrated based on the phase-sensitive information encoded into second harmonic signals, which are intrinsically sensitive to wavefront modulations. By designing and implementing a deep neural network, we demonstrate the second harmonic imaging enhanced by deep learning decipher (SHIELD) for efficient and resilient phase retrieval. Inheriting the advantages of two-photon microscopy, SHIELD demonstrates single-shot, reference-free, and video-rate phase imaging with sensitivity better than {\lambda}/100 and high robustness against noises, facilitating numerous applications from biological imaging to wavefront sensing.      
### 24.Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2010.08136.pdf)
>  Recent state-of-the-art neural text-to-speech (TTS) synthesis models have dramatically improved intelligibility and naturalness of generated speech from text. However, building a good bilingual or code-switched TTS for a particular voice is still a challenge. The main reason is that it is not easy to obtain a bilingual corpus from a speaker who achieves native-level fluency in both languages. In this paper, we explore the use of Mandarin speech recordings from a Mandarin speaker, and English speech recordings from another English speaker to build high-quality bilingual and code-switched TTS for both speakers. A Tacotron2-based cross-lingual voice conversion system is employed to generate the Mandarin speaker's English speech and the English speaker's Mandarin speech, which show good naturalness and speaker similarity. The obtained bilingual data are then augmented with code-switched utterances synthesized using a Transformer model. With these data, three neural TTS models -- Tacotron2, Transformer and FastSpeech are applied for building bilingual and code-switched TTS. Subjective evaluation results show that all the three systems can produce (near-)native-level speech in both languages for each of the speaker.      
### 25.Melody Classifier with Stacked-LSTM  [ :arrow_down: ](https://arxiv.org/pdf/2010.08123.pdf)
>  Attempts to use neural network models for music generation have been common in recent years, and some of them have achieved good results. However, the research on the evaluation system of machine-generated music is still at a relatively early stage. This paper proposes a stacked-LSTM binary classifier based on a language model, which can distinguish the human composer's work from the machine-generated melody by learning the MIDI file's pitch, position, and duration.      
### 26.Joint Inference of Multiple Graphs from Matrix Polynomials  [ :arrow_down: ](https://arxiv.org/pdf/2010.08120.pdf)
>  Inferring graph structure from observations on the nodes is an important and popular network science task. Departing from the more common inference of a single graph and motivated by social and biological networks, we study the problem of jointly inferring multiple graphs from the observation of signals at their nodes (graph signals), which are assumed to be stationary in the sought graphs. From a mathematical point of view, graph stationarity implies that the mapping between the covariance of the signals and the sparse matrix representing the underlying graph is given by a matrix polynomial. A prominent example is that of Markov random fields, where the inverse of the covariance yields the sparse matrix of interest. From a modeling perspective, stationary graph signals can be used to model linear network processes evolving on a set of (not necessarily known) networks. Leveraging that matrix polynomials commute, a convex optimization method along with sufficient conditions that guarantee the recovery of the true graphs are provided when perfect covariance information is available. Particularly important from an empirical viewpoint, we provide high-probability bounds on the recovery error as a function of the number of signals observed and other key problem parameters. Numerical experiments using synthetic and real-world data demonstrate the effectiveness of the proposed method with perfect covariance information as well as its robustness in the noisy regime.      
### 27.Physics-informed GANs for Coastal Flood Visualization  [ :arrow_down: ](https://arxiv.org/pdf/2010.08103.pdf)
>  As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, but during hurricanes the area is largely covered by clouds and emergency managers must rely on nonintuitive flood visualizations for mission planning. To assist these emergency managers, we have created a deep learning pipeline that generates visual satellite images of current and future coastal flooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. While this work focused on the visualization of coastal floods, we envision the creation of a global visualization of how climate change will shape our earth.      
### 28.PiRhDy: Learning Pitch-, Rhythm-, and Dynamics-aware Embeddings for Symbolic Music  [ :arrow_down: ](https://arxiv.org/pdf/2010.08091.pdf)
>  Definitive embeddings remain a fundamental challenge of computational musicology for symbolic music in deep learning today. Analogous to natural language, music can be modeled as a sequence of tokens. This motivates the majority of existing solutions to explore the utilization of word embedding models to build music embeddings. However, music differs from natural languages in two key aspects: (1) musical token is multi-faceted -- it comprises of pitch, rhythm and dynamics information; and (2) musical context is two-dimensional -- each musical token is dependent on both melodic and harmonic contexts. In this work, we provide a comprehensive solution by proposing a novel framework named PiRhDy that integrates pitch, rhythm, and dynamics information seamlessly. PiRhDy adopts a hierarchical strategy which can be decomposed into two steps: (1) token (i.e., note event) modeling, which separately represents pitch, rhythm, and dynamics and integrates them into a single token embedding; and (2) context modeling, which utilizes melodic and harmonic knowledge to train the token embedding. A thorough study was made on each component and sub-strategy of PiRhDy. We further validate our embeddings in three downstream tasks -- melody completion, accompaniment suggestion, and genre classification. Results indicate a significant advancement of the neural approach towards symbolic music as well as PiRhDy's potential as a pretrained tool for a broad range of symbolic music applications.      
### 29.APF-PF: Probabilistic Depth Perception for 3D Reactive Obstacle Avoidance  [ :arrow_down: ](https://arxiv.org/pdf/2010.08063.pdf)
>  This paper proposes a framework for 3D obstacle avoidance in the presence of partial observability of environment obstacles. The method focuses on the utility of the Artificial Potential Field (APF) controller in a practical setting where noisy and incomplete information about the proximity is inevitable. We propose a Particle Filter (PF) approach to estimate potential obstacle locations in an input depth image stream. The probable candidates are then used to generate an action that maneuvers the robot towards the negative gradient of potential at each time instant. Rigorous experimental validation on a quadrotor UAV demonstrates the robustness and reliability of the method when robot's sensitivity to incorrect perception information can be concerning. The proposed method is highly compute efficient for real-time applications and agile robots.      
### 30.IoT Platform for COVID-19 Prevention and Control: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2010.08056.pdf)
>  As a result of the worldwide transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), coronavirus disease 2019 (COVID-19) has evolved into an unprecedented pandemic. Currently, with unavailable pharmaceutical treatments and vaccines, this novel coronavirus results in a great impact on public health, human society, and global economy, which is likely to last for many years. One of the lessons learned from the COVID-19 pandemic is that a long-term system with non-pharmaceutical interventions for preventing and controlling new infectious diseases is desirable to be implemented. Internet of things (IoT) platform is preferred to be utilized to achieve this goal, due to its ubiquitous sensing ability and seamless connectivity. IoT technology is changing our lives through smart healthcare, smart home, and smart city, which aims to build a more convenient and intelligent community. This paper presents how the IoT could be incorporated into the epidemic prevention and control system. Specifically, we demonstrate a potential fog-cloud combined IoT platform that can be used in the systematic and intelligent COVID-19 prevention and control, which involves five interventions including COVID-19 Symptom Diagnosis, Quarantine Monitoring, Contact Tracing \&amp; Social Distancing, COVID-19 Outbreak Forecasting, and SARS-CoV-2 Mutation Tracking. We investigate and review the state-of-the-art literatures of these five interventions to present the capabilities of IoT in countering against the current COVID-19 pandemic or future infectious disease epidemics.      
### 31.Data-Driven Stochastic Reachability Using Hilbert Space Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2010.08036.pdf)
>  We compute finite sample bounds for approximations of the solution to stochastic reachability problems computed using kernel distribution embeddings, a non-parametric machine learning technique. Our approach enables assurances of safety from observed data, through construction of probabilistic violation bounds on the computed stochastic reachability probability. By embedding the stochastic kernel of a Markov control process in a reproducing kernel Hilbert space, we can compute the safety probabilities for systems with arbitrary disturbances as simple matrix operations and inner products. We present finite sample bounds for the approximation using elements from statistical learning theory. We numerically evaluate the approach, and demonstrate its efficacy on neural net-controlled pendulum system.      
### 32.QReLU and m-QReLU: Two novel quantum activation functions to aid medical diagnostics  [ :arrow_down: ](https://arxiv.org/pdf/2010.08031.pdf)
>  The ReLU activation function (AF) has been extensively applied in deep neural networks, in particular Convolutional Neural Networks (CNN), for image classification despite its unresolved dying ReLU problem, which poses challenges to reliable applications. This issue has obvious important implications for critical applications, such as those in healthcare. Recent approaches are just proposing variations of the activation function within the same unresolved dying ReLU challenge. This contribution reports a different research direction by investigating the development of an innovative quantum approach to the ReLU AF that avoids the dying ReLU problem by disruptive design. The Leaky ReLU was leveraged as a baseline on which the two quantum principles of entanglement and superposition were applied to derive the proposed Quantum ReLU (QReLU) and the modified-QReLU (m-QReLU) activation functions. Both QReLU and m-QReLU are implemented and made freely available in TensorFlow and Keras. This original approach is effective and validated extensively in case studies that facilitate the detection of COVID-19 and Parkinson Disease (PD) from medical images. The two novel AFs were evaluated in a two-layered CNN against nine ReLU-based AFs on seven benchmark datasets, including images of spiral drawings taken via graphic tablets from patients with Parkinson Disease and healthy subjects, and point-of-care ultrasound images on the lungs of patients with COVID-19, those with pneumonia and healthy controls. Despite a higher computational cost, results indicated an overall higher classification accuracy, precision, recall and F1-score brought about by either quantum AFs on five of the seven bench-mark datasets, thus demonstrating its potential to be the new benchmark or gold standard AF in CNNs and aid image classification tasks involved in critical applications, such as medical diagnoses of COVID-19 and PD.      
### 33.Data Valuation for Medical Imaging Using Shapley Value: Application on A Large-scale Chest X-ray Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2010.08006.pdf)
>  The reliability of machine learning models can be compromised when trained on low quality data. Many large-scale medical imaging datasets contain low quality labels extracted from sources such as medical reports. Moreover, images within a dataset may have heterogeneous quality due to artifacts and biases arising from equipment or measurement errors. Therefore, algorithms that can automatically identify low quality data are highly desired. In this study, we used data Shapley, a data valuation metric, to quantify the value of training data to the performance of a pneumonia detection algorithm in a large chest X-ray dataset. We characterized the effectiveness of data Shapley in identifying low quality versus valuable data for pneumonia detection. We found that removing training data with high Shapley values decreased the pneumonia detection performance, whereas removing data with low Shapley values improved the model performance. Furthermore, there were more mislabeled examples in low Shapley value data and more true pneumonia cases in high Shapley value data. Our results suggest that low Shapley value indicates mislabeled or poor quality images, whereas high Shapley value indicates data that are valuable for pneumonia detection. Our method can serve as a framework for using data Shapley to denoise large-scale medical imaging datasets.      
### 34.Fundamental Challenges of Cyber-Physical Systems Security Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2005.00043.pdf)
>  Systems modeling practice lacks security analysis tools that can interface with modeling languages to facilitate security by design. Security by design is a necessity in the age of safety critical cyber-physical systems, where security violations can cause hazards. Currently, the overlap between security and safety is narrow. But deploying cyber-physical systems means that today's adversaries can intentionally trigger accidents. By implementing security assessment tools for modeling languages we are better able to address threats earlier in the system's lifecycle and, therefore, assure their safe and secure behavior in their eventual deployment. We posit that cyber-physical systems security modeling is practiced insufficiently because it is still addressed similarly to information technology systems.      
### 35.Multi-agent estimation and filtering for minimizing team mean-squared error  [ :arrow_down: ](https://arxiv.org/pdf/1903.12018.pdf)
>  Motivated by estimation problems arising in autonomous vehicles and decentralized control of unmanned aerial vehicles, we consider multi-agent estimation and filtering problems in which multiple agents generate state estimates based on decentralized information and the objective is to minimize a coupled mean-squared error which we call \emph{team mean-square error}. We call the resulting estimates as minimum team mean-squared error (MTMSE) estimates. We show that MTMSE estimates are different from minimum mean-squared error (MMSE) estimates. We derive closed-form expressions for MTMSE estimates, which are linear function of the observations where the corresponding gain depends on the weight matrix that couples the estimation error. We then consider a filtering problem where a linear stochastic process is monitored by multiple agents which can share their observations (with delay) over a communication graph. We derive expressions to recursively compute the MTMSE estimates. To illustrate the effectiveness of the proposed scheme we consider an example of estimating the distances between vehicles in a platoon and show that MTMSE estimates significantly outperform MMSE estimates and consensus Kalman filtering estimates.      
