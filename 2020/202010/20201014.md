# ArXiv eess --Wed, 14 Oct 2020
### 1.Learning Tensor Train Representation with Automatic Rank Determination from Incomplete Noisy Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.06564.pdf)
>  Tensor train (TT) decomposition, a powerful tool for analysing multidimensional data, exhibits superior performance in many signal processing and machine learning tasks. However, existing methods for TT decomposition either require the knowledge of the true TT ranks, or extensive fine-tuning the balance between model complexity and representation accuracy. In this paper, a fully Bayesian treatment of TT decomposition is employed to enable automatic rank determination. In particular, theoretical evidence is established for adopting a Gaussian-product-Gamma prior to induce sparsity on the slices of the TT cores, so that the model complexity is automatically determined even under incomplete and noisy observed data. Based on the proposed probabilistic model, an efficient learning algorithm is derived under the variational inference framework. Simulation results on synthetic data show the success of the proposed model and algorithm on recovering the ground-truth TT structure from incomplete noisy data. Further experiments on real world data demonstrate the proposed algorithm performs better in image completion and image classification, compared to other existing TT decomposition algorithms.      
### 2.Piece-wise Matching Layer in Representation Learning for ECG Classification  [ :arrow_down: ](https://arxiv.org/pdf/2010.06510.pdf)
>  This paper proposes piece-wise matching layer as a novel layer in representation learning methods for electrocardiogram (ECG) classification. Despite the remarkable performance of representation learning methods in the analysis of time series, there are still several challenges associated with these methods ranging from the complex structures of methods, the lack of generality of solutions, the need for expert knowledge, and large-scale training datasets. We introduce the piece-wise matching layer that works based on two levels to address some of the aforementioned challenges. At the first level, a set of morphological, statistical, and frequency features and comparative forms of them are computed based on each periodic part and its neighbors. At the second level, these features are modified by predefined transformation functions based on a receptive field scenario. Several scenarios of offline processing, incremental processing, fixed sliding receptive field, and event-based triggering receptive field can be implemented based on the choice of length and mechanism of indicating the receptive field. We propose dynamic time wrapping as a mechanism that indicates a receptive field based on event triggering tactics. To evaluate the performance of this method in time series analysis, we applied the proposed layer in two publicly available datasets of PhysioNet competitions in 2015 and 2017 where the input data is ECG signal. We compared the performance of our method against a variety of known tuned methods from expert knowledge, machine learning, deep learning methods, and the combination of them. The proposed approach improves the state of the art in two known completions 2015 and 2017 around 4% and 7% correspondingly while it does not rely on in advance knowledge of the classes or the possible places of arrhythmia.      
### 3.Machine Learning of Partial Differential Equations from Noise Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.06507.pdf)
>  Machine learning of partial differential equations from data is a potential breakthrough to solve the lack of physical equations in complex dynamic systems, but because numerical differentiation is ill-posed to noise data, noise has become the biggest obstacle in the application of partial differential equation identification method. To overcome this problem, we propose Frequency Domain Identification method based on Fourier transforms, which effectively eliminates the influence of noise by using the low frequency component of frequency domain data to identify partial differential equations in frequency domain. We also propose a new sparse identification criterion, which can accurately identify the terms in the equation from low signal-to-noise ratio data. Through identifying a variety of canonical equations spanning a number of scientific domains, the proposed method is proved to have high accuracy and robustness for equation structure and parameters identification for low signal-to-noise ratio data. The method provides a promising technique to discover potential partial differential equations from noisy experimental data.      
### 4.Single-Sideband Time-Modulated Phased Array With 2-bit Phased Shifters  [ :arrow_down: ](https://arxiv.org/pdf/2010.06504.pdf)
>  A novel single-sideband (SSB) time-modulated technique with 2-bit phase shifters is proposed. The timemodulated module is implemented by adding periodic phase modulation to 2-bit phase shifters, which is simpler without performance loss compared to existing SSB time-modulated method. During one modulation period, four phase states (0, {\pi}/2, {\pi}, 3{\pi}/2) of 2-bit phase shifters are switched in sequence. After the modulation, the SSB time modulation is realized and the main power is distributed to the first harmonic component. The feasibility of the proposed method is verified by experiments. The undesired harmonics are efficiently suppressed. Meanwhile, 80Â° beam scanning range are realized through the proposed module.      
### 5.Transfer Learning and SpecAugment applied to SSVEP Based BCI Classification  [ :arrow_down: ](https://arxiv.org/pdf/2010.06503.pdf)
>  In this work, we used a deep convolutional neural network (DCNN) to classify electroencephalography (EEG) signals in a steady-state visually evoked potentials (SSVEP) based brain-computer interface (BCI). The raw EEG signals were converted to spectrograms and served as input to train a DCNN using the transfer learning technique. We applied a second technique, data augmentation, mostly SpecAugment, generally employed to speech recognition. The results, when excluding the evaluated user's data from the fine-tuning process, reached 99.3% mean test accuracy and 0.992 mean F1 score on 35 subjects from an open dataset.      
### 6.Experimental Demonstration of Optoelectronic Equalization for Short-reach Transmission with Reservoir Computing  [ :arrow_down: ](https://arxiv.org/pdf/2010.06502.pdf)
>  A receiver with shared complexity between optical and digital domains is experimentally demonstrated. Reservoir computing is used to equalize up to 4 directly-detected optically filtered spectral slices of a 32 GBd OOK signal over up to 80 km of SMF.      
### 7.Multiple Node Immunisation for Preventing Epidemics on Networks by Exact Multiobjective Optimisation of Cost and Shield-Value  [ :arrow_down: ](https://arxiv.org/pdf/2010.06488.pdf)
>  The general problem in this paper is vertex (node) subset selection with the goal to contain an infection that spreads in a network. Instead of selecting the single most important node, this paper deals with the problem of selecting multiple nodes for removal. As compared to previous work on multiple-node selection, the trade-off between cost and benefit is considered. The benefit is measured in terms of increasing the epidemic threshold which is a measure of how difficult it is for an infection to spread in a network. The cost is measured in terms of the number and size of nodes to be removed or controlled. Already in its single-objective instance with a fixed number of $k$ nodes to be removed, the multiple vertex immunisation problems have been proven to be NP-hard. Several heuristics have been developed to approximate the problem. In this work, we compare meta-heuristic techniques with exact methods on the Shield-value, which is a sub-modular proxy for the maximal eigenvalue and used in the current state-of-the-art greedy node-removal strategies. We generalise it to the multi-objective case and replace the greedy algorithm by a quadratic program (QP), which then can be solved with exact QP solvers. The main contribution of this paper is the insight that, if time permits, exact and problem-specific methods approximation should be used, which are often far better than Pareto front approximations obtained by general meta-heuristics. Based on these, it will be more effective to develop strategies for controlling real-world networks when the goal is to prevent or contain epidemic outbreaks. This paper is supported by ready to use Python implementation of the optimization methods and datasets.      
### 8.Stochastic Geometry-based Modelling of Mobile UAV Relay Networks under Realistic Fading  [ :arrow_down: ](https://arxiv.org/pdf/2010.06443.pdf)
>  We consider a relay network based on Unmanned Aerial Vehicles (UAV). Terrestrial Base Stations (TBS) and UAV Relay Nodes (RN) are modelled using two Homogeneous Poisson Point Processes (HPPP). UAVs can hover at a fixed position or move following specific displacement schemes. The Coverage Probability (CP) of a typical user equipment (UE) is derived, either when it communicates via a direct link (from the TBS to the UE) or via a relay link (from the TBS to the UE through a UAV RN). Every link can be in Line-of-Sight (LoS) or Non Line-of-Sight (NLoS), and suffers from Rician fading with distance-dependent parameters. This coverage is calculated by means of both stochastic geometry (SG) and Monte-Carlo (MC) simulations. The benefits of the use of UAV as RNs are analysed depending on their altitude, density, and mobility scheme.      
### 9.Sound event localization and detection based on crnn using rectangular filters and channel rotation data augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.06422.pdf)
>  Sound Event Localization and Detection refers to the problem of identifying the presence of independent or temporally-overlapped sound sources, correctly identifying to which sound class it belongs, estimating their spatial directions while they are active. In the last years, neural networks have become the prevailing method for sound Event Localization and Detection task, with convolutional recurrent neural networks being among the most used systems. This paper presents a system submitted to the Detection and Classification of Acoustic Scenes and Events 2020 Challenge Task 3. The algorithm consists of a convolutional recurrent neural network using rectangular filters, specialized in recognizing significant spectral features related to the task. In order to further improve the score and to generalize the system performance to unseen data, the training dataset size has been increased using data augmentation. The technique used for that is based on channel rotations and reflection on the xy plane in the First Order Ambisonic domain, which allows improving Direction of Arrival labels keeping the physical relationships between channels. Evaluation results on the development dataset show that the proposed system outperforms the baseline results, considerably improving Error Rate and F-score for location-aware detection.      
### 10.RANDGAN: Randomized Generative Adversarial Network for Detection of COVID-19 in Chest X-ray  [ :arrow_down: ](https://arxiv.org/pdf/2010.06418.pdf)
>  COVID-19 spread across the globe at an immense rate has left healthcare systems incapacitated to diagnose and test patients at the needed rate. Studies have shown promising results for detection of COVID-19 from viral bacterial pneumonia in chest X-rays. Automation of COVID-19 testing using medical images can speed up the testing process of patients where health care systems lack sufficient numbers of the reverse-transcription polymerase chain reaction (RT-PCR) tests. Supervised deep learning models such as convolutional neural networks (CNN) need enough labeled data for all classes to correctly learn the task of detection. Gathering labeled data is a cumbersome task and requires time and resources which could further strain health care systems and radiologists at the early stages of a pandemic such as COVID-19. In this study, we propose a randomized generative adversarial network (RANDGAN) that detects images of an unknown class (COVID-19) from known and labelled classes (Normal and Viral Pneumonia) without the need for labels and training data from the unknown class of images (COVID-19). We used the largest publicly available COVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia, and COVID-19 images from multiple public databases. In this work, we use transfer learning to segment the lungs in the COVIDx dataset. Next, we show why segmentation of the region of interest (lungs) is vital to correctly learn the task of classification, specifically in datasets that contain images from different resources as it is the case for the COVIDx dataset. Finally, we show improved results in detection of COVID-19 cases using our generative model (RANDGAN) compared to conventional generative adversarial networks (GANs) for anomaly detection in medical images, improving the area under the ROC curve from 0.71 to 0.77.      
### 11.Procedural 3D Terrain Generation using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.06411.pdf)
>  Procedural 3D Terrain generation has become a necessity in open world games, as it can provide unlimited content, through a functionally infinite number of different areas, for players to explore. In our approach, we use Generative Adversarial Networks (GAN) to yield realistic 3D environments based on the distribution of remotely sensed images of landscapes, captured by satellites or drones. Our task consists of synthesizing a random but plausible RGB satellite image and generating a corresponding Height Map in the form of a 3D point cloud that will serve as an appropriate mesh of the landscape. For the first step, we utilize a GAN trained with satellite images that manages to learn the distribution of the dataset, creating novel satellite images. For the second part, we need a one-to-one mapping from RGB images to Digital Elevation Models (DEM). We deploy a Conditional Generative Adversarial network (CGAN), which is the state-of-the-art approach to image-to-image translation, to generate a plausible height map for every randomly generated image of the first model. Combining the generated DEM and RGB image, we are able to construct 3D scenery consisting of a plausible height distribution and colorization, in relation to the remotely sensed landscapes provided during training.      
### 12.A Simplified Formulation for the Backward/Forward Sweep Power Flow Method  [ :arrow_down: ](https://arxiv.org/pdf/2010.06389.pdf)
>  This paper describes a simplified formulation of the Backward/Forward (BW/FW) Sweep Power Flow applied to radial distribution systems with distributed generation under positive sequence modelling. Proposed formulation was applied in an illustrative test system.      
### 13.Technical Barriers for Harnessing the Green Hydrogen: A Power System Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2010.06384.pdf)
>  Extracting green hydrogen from renewable energy sources is a new concept in the energy industry. As an energy carrier, hydrogen is well capable of facilitating a strong coupling between various energy sectors, as well as integration of renewable energy sources. This paper investigates the system-wide technical factors that might limit the amount of producible hydrogen in a given power system. A non-linear programming formulation is proposed to quantify the impact of voltage security constraints, the location and size of power to hydrogen facilities, and finally the wind penetration levels on the harvest-able green hydrogen. The applicability of the proposed framework is demonstrated on the IEEE 39 bus system.      
### 14.Optimal Allocation of Quantized Human Eye Depth Perception for Light Field Display Design  [ :arrow_down: ](https://arxiv.org/pdf/2010.06382.pdf)
>  Creating immersive 3D stereoscopic, autostereoscopic, and lightfield experiences are becoming the center point of optical design of future head mounted displays and lightfield displays. However, despite the advancement in 3D and light field displays; there is no consensus on what are the necessary quantized depth levels for such emerging displays at stereoscopic or monocular modalities. Here we start from psychophysical theories and work toward defining and prioritizing quantized levels of depth that would saturate the human depth perception. We propose a general optimization framework, which locates the depth levels in a \emph{globally optimal} way for band limited displays. While the original problem is computationally intractable, we manage to find a tractable reformulation as maximally covering a region of interest with a selection of hypographs corresponding to the monocular depth of field profiles. The results show that on average 1731 stereoscopic and 8 monocular depth levels (distributed from 25 cm to infinity) would saturate the visual depth perception. Also the first 3 depth levels should be allocated at (148), then (83, 170), then (53, 90, 170) distances respectively from the face plane to minimize the monocular error in the entire population. The study further discusses the 3D spatial profile of the quantized stereoscopic and monocular depth levels. The study provides fundamental guidelines for designing optimal near eye displays, light-field monitors, and 3D screens.      
### 15.When is Enough Enough? "Just Enough" Decision Making with Recurrent Neural Networks for Radio Frequency Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.06352.pdf)
>  Prior work has demonstrated that recurrent neural network architectures show promising improvements over other machine learning architectures when processing temporally correlated inputs, such as wireless communication signals. Additionally, recurrent neural networks typically process data on a sequential basis, enabling the potential for near real-time results. In this work, we investigate the novel usage of "just enough" decision making metrics for making decisions during inference based on a variable number of input symbols. Since some signals are more complex than others, due to channel conditions, transmitter/receiver effects, etc., being able to dynamically utilize just enough of the received symbols to make a reliable decision allows for more efficient decision making in applications such as electronic warfare and dynamic spectrum sharing. To demonstrate the validity of this concept, four approaches to making "just enough" decisions are considered in this work and each are analyzed for their applicability to wireless communication machine learning applications.      
### 16.Time efficiency analysis for undersampled quantitative MRI acquisitions  [ :arrow_down: ](https://arxiv.org/pdf/2010.06330.pdf)
>  Quantitative MRI (QMRI) at present is clinically impractical due to the long scan time. Undersampled acquisitions supported by conventional acceleration techniques such as parallel imaging are inadequate to reduce the scan time to clinical requirements. Further undersampling is possible by exploiting the redundancy due to multi-contrast measurements required in QMRI. Jointly performing the reconstruction and estimation of parameters is one way of exploiting this redundancy. In this work, we propose a framework: Time Efficiency analysis for UnderSampled QMRI Acquisition (TEUSQA). TEUSQA enables the selection of an efficient undersampling pattern taking the multi-contrast properties of the acquisition sequence into account. To verify the time efficiency predicted by TEUSQA, we performed Monte Carlo simulations and a comparison with the test retest variability of a prospectively undersampled, 32 fold accelerated joint T1 &amp; T2 mapping experiment. Using TEUSQA, we assessed several ways to generate undersampling patterns in silico, providing insight into their time efficiency in relation to the sample distribution and acceleration factor, and suggesting a class of optimal undersampling patterns obtained by low-discrepancy sampling. We believe that TEUSQA offers a valuable instrument for developers of novel QMRI sequences pushing the boundaries of acceleration in order to achieve clinically feasible protocols.      
### 17.Linear Matrix Inequality Design of Observer-Based Controllers for port-Hamiltonian Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.06314.pdf)
>  The design of an observer-based state feedback controller for port-Hamiltonian (pH) systems is addressed using linear matrix inequalities (LMIs). The controller is composed of the observer and the state feedback. By passivity, the asymptotic stability of the closed-loop system is guaranteed even if the controller is implemented on complex physical systems such as the ones defined by infinite-dimensional or nonlinear models. An infinite-dimensional Timoshenko beam model and a microelectromechanical system are used to illustrate the achievable performances using such an approach under simulations.      
### 18.Novel Architectures for Unsupervised Information Bottleneck based Speaker Diarization of Meetings  [ :arrow_down: ](https://arxiv.org/pdf/2010.06304.pdf)
>  Speaker diarization is an important problem that is topical, and is especially useful as a preprocessor for conversational speech related applications. The objective of this paper is two-fold: (i) segment initialization by uniformly distributing speaker information across the initial segments, and (ii) incorporating speaker discriminative features within the unsupervised diarization framework. In the first part of the work, a varying length segment initialization technique for Information Bottleneck (IB) based speaker diarization system using phoneme rate as the side information is proposed. This initialization distributes speaker information uniformly across the segments and provides a better starting point for IB based clustering. In the second part of the work, we present a Two-Pass Information Bottleneck (TPIB) based speaker diarization system that incorporates speaker discriminative features during the process of diarization. The TPIB based speaker diarization system has shown improvement over the baseline IB based system. During the first pass of the TPIB system, a coarse segmentation is performed using IB based clustering. The alignments obtained are used to generate speaker discriminative features using a shallow feed-forward neural network and linear discriminant analysis. The discriminative features obtained are used in the second pass to obtain the final speaker boundaries. In the final part of the paper, variable segment initialization is combined with the TPIB framework. This leverages the advantages of better segment initialization and speaker discriminative features that results in an additional improvement in performance. An evaluation on standard meeting datasets shows that a significant absolute improvement of 3.9% and 4.7% is obtained on the NIST and AMI datasets, respectively.      
### 19.Tire Force Estimation in Intelligent Tires Using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.06299.pdf)
>  The concept of intelligent tires has drawn attention of researchers in the areas of autonomous driving, advanced vehicle control, and artificial intelligence. The focus of this paper is on intelligent tires and the application of machine learning techniques to tire force estimation. We present an intelligent tire system with a tri-axial acceleration sensor, which is installed onto the inner liner of the tire, and Neural Network techniques for real-time processing of the sensor data. The accelerometer is capable of measuring the acceleration in x,y, and z directions. When the accelerometer enters the tire contact patch, it starts generating signals until it fully leaves it. Simultaneously, by using MTS Flat-Trac test platform, tire actual forces are measured. Signals generated by the accelerometer and MTS Flat-Trac testing system are used for training three different machine learning techniques with the purpose of online prediction of tire forces. It is shown that the developed intelligent tire in conjunction with machine learning is effective in accurate prediction of tire forces under different driving conditions. The results presented in this work will open a new avenue of research in the area of intelligent tires, vehicle systems, and tire force estimation.      
### 20.Deep Multi-Agent Reinforcement Learning for Cost Efficient Distributed Load Frequency Control  [ :arrow_down: ](https://arxiv.org/pdf/2010.06293.pdf)
>  The rise of microgrid-based architectures is heavily modifying the energy control landscape in distribution systems making distributed control mechanisms necessary to ensure reliable power system operations. In this paper, we propose the use of Reinforcement Learning techniques to implement load frequency control without requiring a central authority. To this end, we approximate the optimal solution of the primary, secondary, and tertiary control with the use of the Multi- Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. Generation units are characterised as agents that learn how to maximise their long-term performance by acting and interacting with the environment to balance generation and load in a cost efficient way. Network effects are also modelled in our framework for the restoration of frequency to the nominal value. We validate our Reinforcement Learning methodology through numerical results and show that it can be used to implement the load frequency control in a distributed and cost efficient way.      
### 21.Towards predictive crowd based transport infrastructure monitoring system  [ :arrow_down: ](https://arxiv.org/pdf/2010.06292.pdf)
>  To be able to measure relevant data for transport infrastructure monitoring and to obtain maintenance indicators in a crowd sensing-based fashion, a set of requirements (both from hardware and software points of view) needs to be satisfied. Heterogeneity of smartphones and various uncertainties associated with the mainstream \textit{off-the-shelf} hardware combined with the fact that inexperienced participants will take an active part in the data collection process necessitates that the system accounts for dynamicity, uncertainty, unreliability, and heterogeneity of the environment, participants, and technology. Modern Software Development Kits (SDK) and Application Programming Interfaces (API) provided by the mobile Operating Systems allow us to build applications for the smartphones ecosystem and to interact with components, services, and applications. They also provide specifications for hardware components and modules (e.g. sensors, location, and networking modules). Being a system implemented on a heterogeneous and dynamic smartphone platform, our crowd sensing-based transport infrastructure monitoring system satisfies a number of hardware and software requirements. Hence, we propose a detailed and rigorous methodology to research the requirements and describe the overall system architecture including its functionalities and services.      
### 22.Exploring Universal Speech Attributes for Speaker Verification with an Improved Cross-stitch Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.06248.pdf)
>  The universal speech attributes for x-vector based speaker verification (SV) are addressed in this paper. The manner and place of articulation form the fundamental speech attribute unit (SAU), and then new speech attribute (NSA) units for acoustic modeling are generated by tied tri-SAU states. An improved cross-stitch network is adopted as a multitask learning (MTL) framework for integrating these universal speech attributes into the x-vector network training process. Experiments are conducted on common condition 5 (CC5) of the core-core and the 10 s-10 s tests of the NIST SRE10 evaluation set, and the proposed algorithm can achieve consistent improvements over the baseline x-vector on both these tasks.      
### 23.Average Cost Optimal Control of Stochastic Systems Using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.06236.pdf)
>  This paper addresses the average cost minimization problem for discrete-time systems with multiplicative and additive noises via reinforcement learning. By using Q-function, we propose an online learning scheme to estimate the kernel matrix of Q-function and to update the control gain using the data along the system trajectories. The obtained control gain and kernel matrix are proved to converge to the optimal ones. To implement the proposed learning scheme, an online model-free reinforcement learning algorithm is given, where recursive least squares method is used to estimate the kernel matrix of Q-function. A numerical example is presented to illustrate the proposed approach.      
### 24.Multi-layer Residual Sparsifying Transform (MARS) Model for Low-dose CT Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2010.06144.pdf)
>  Signal models based on sparse representations have received considerable attention in recent years. On the other hand, deep models consisting of a cascade of functional layers, commonly known as deep neural networks, have been highly successful for the task of object classification and have been recently introduced to image reconstruction. In this work, we develop a new image reconstruction approach based on a novel multi-layer model learned in an unsupervised manner by combining both sparse representations and deep models. The proposed framework extends the classical sparsifying transform model for images to a Multi-lAyer Residual Sparsifying transform (MARS) model, wherein the transform domain data are jointly sparsified over layers. We investigate the application of MARS models learned from limited regular-dose images for low-dose CT reconstruction using Penalized Weighted Least Squares (PWLS) optimization. We propose new formulations for multi-layer transform learning and image reconstruction. We derive an efficient block coordinate descent algorithm to learn the transforms across layers, in an unsupervised manner from limited regular-dose images. The learned model is then incorporated into the low-dose image reconstruction phase. Low-dose CT experimental results with both the XCAT phantom and Mayo Clinic data show that the MARS model outperforms conventional methods such as FBP and PWLS methods based on the edge-preserving (EP) regularizer and the single-layer learned transform (ST) model, especially in terms of reducing noise and maintaining some subtle details.      
### 25.Single-Antenna-Based GPS Anti-Jamming Method Exploiting Polarization Diversity  [ :arrow_down: ](https://arxiv.org/pdf/2010.06130.pdf)
>  The vulnerability of Global Positioning System (GPS) receivers to jammers is a major concern owing to the extremely weak received signal power of GPS. Researches have been conducted on a variety of antenna array techniques to be used as countermeasures to GPS jammers, and their anti-jamming performance is known to be greater than that of single antenna methods. However, the application of antenna arrays remains limited because of their size, cost, and computational complexity. This study proposes and experimentally validates a novel space-time-polarization domain adaptive processing for a single-element dual-polarized antenna (STPAPS) by focusing on the polarization diversity of a dual-polarized antenna. The mathematical models of arbitrarily polarized signals received by dual-polarized antenna are derived, and an appropriate constraint matrix for dual-polarized-antenna-based GPS anti-jam is suggested. To reduce the computational complexity of the constraint matrix approach, the eigenvector constraint design scheme is adopted. The performance of STPAPS is quantitively and qualitatively evaluated through experiments as follows. 1) The carrier-to-noise-density ratio (C/N0) of STPAPS under synthetic jamming is demonstrated to be higher than that of the previous minimum mean squared error (MMSE) or minimum variance distortionless response (MVDR) based dual-polarized antenna methods. 2) The strengths and weaknesses of STPAPS are qualitatively compared with those of the previous single-element dual-polarized antenna methods that are not based on the MMSE or MVDR algorithms. 3) The characteristics of STPAPS (in terms of the directions and polarizations of the GPS and jamming signals) are compared with those of the conventional two-element single-polarized antenna array method, which has the same degree of freedom as that of STPAPS.      
### 26.Assessing Lesion Segmentation Bias of Neural Networks on Motion Corrupted Brain MRI  [ :arrow_down: ](https://arxiv.org/pdf/2010.06027.pdf)
>  Patient motion during the magnetic resonance imaging (MRI) acquisition process results in motion artifacts, which limits the ability of radiologists to provide a quantitative assessment of a condition visualized. Often times, radiologists either "see through" the artifacts with reduced diagnostic confidence, or the MR scans are rejected and patients are asked to be recalled and re-scanned. Presently, there are many published approaches that focus on MRI artifact detection and correction. However, the key question of the bias exhibited by these algorithms on motion corrupted MRI images is still unanswered. In this paper, we seek to quantify the bias in terms of the impact that different levels of motion artifacts have on the performance of neural networks engaged in a lesion segmentation task. Additionally, we explore the effect of a different learning strategy, curriculum learning, on the segmentation performance. Our results suggest that a network trained using curriculum learning is effective at compensating for different levels of motion artifacts, and improved the segmentation performance by ~9%-15% (p &lt; 0.05) when compared against a conventional shuffled learning strategy on the same motion data. Within each motion category, it either improved or maintained the dice score. To the best of our knowledge, we are the first to quantitatively assess the segmentation bias on various levels of motion artifacts present in a brain MRI image.      
### 27.Principles for Designing Computer Music Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2010.06524.pdf)
>  This paper will present observations on the design, artistic, and human factors of creating digital music controllers. Specific projects will be presented, and a set of design principles will be supported from those examples.      
### 28.LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2010.06499.pdf)
>  The collection of high-resolution training data is crucial in building robust plant disease diagnosis systems, since such data have a significant impact on diagnostic performance. However, they are very difficult to obtain and are not always available in practice. Deep learning-based techniques, and particularly generative adversarial networks (GANs), can be applied to generate high-quality super-resolution images, but these methods often produce unexpected artifacts that can lower the diagnostic performance. In this paper, we propose a novel artifact-suppression super-resolution method that is specifically designed for diagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution (LASSR). Thanks to its own artifact removal module that detects and suppresses artifacts to a considerable extent, LASSR can generate much more pleasing, high-quality images compared to the state-of-the-art ESRGAN model. Experiments based on a five-class cucumber disease (including healthy) discrimination model show that training with data generated by LASSR significantly boosts the performance on an unseen test dataset by nearly 22% compared with the baseline, and that our approach is more than 2% better than a model trained with images generated by ESRGAN.      
### 29.Improving Road Signs Detection performance by Combining the Features of Hough Transform and Texture  [ :arrow_down: ](https://arxiv.org/pdf/2010.06453.pdf)
>  With the large uses of the intelligent systems in different domains, and in order to increase the drivers and pedestrians safety, the road and traffic sign recognition system has been a challenging issue and an important task for many years. But studies, done in this field of detection and recognition of traffic signs in an image, which are interested in the Arab context, are still insufficient. Detection of the road signs present in the scene is the one of the main stages of the traffic sign detection and recognition. In this paper, an efficient solution to enhance road signs detection, including Arabic context, performance based on color segmentation, Randomized Hough Transform and the combination of Zernike moments and Haralick features has been made. Segmentation stage is useful to determine the Region of Interest (ROI) in the image. The Randomized Hough Transform (RHT) is used to detect the circular and octagonal shapes. This stage is improved by the extraction of the Haralick features and Zernike moments. Furthermore, we use it as input of a classifier based on SVM. Experimental results show that the proposed approach allows us to perform the measurements precision.      
### 30.Revisiting SIR in the age of COVID-19: Explicit Solutions and Control Problems  [ :arrow_down: ](https://arxiv.org/pdf/2010.06445.pdf)
>  The non-population conserving SIR (SIR-NC) model to describe the spread of infections in a community is proposed and studied. Unlike the standard SIR model, SIR-NC does not assume population conservation. Although similar in form to the standard SIR, SIR-NC admits a closed form solution while allowing us to model mortality, and also provides different, and arguably a more realistic, interpretation of the model parameters. Numerical comparisons of this SIR-NC model with the standard, population conserving, SIR model are provided. Extensions to include imported infections, interacting communities, and models that include births and deaths are presented and analyzed. Several numerical examples are also presented to illustrate these models. Two control problems for the SIR-NC epidemic model are presented. First we consider the continuous time model predictive control in which the cost function variables correspond to the levels of lockdown, the level of testing and quarantine, and the number of infections. We also include a switching cost for moving between lockdown levels. A discrete time version that is more amenable to computation is then presented along with numerical illustrations. We then consider a multi-objective and multi-community control where we can define multiple cost functions on the different communities and obtain the minimum cost control to keep the value function corresponding to these control objectives below a prescribed threshold.      
### 31.A comprehensive protocol for manual segmentation of the human claustrum and its sub-regions using high-resolution MRI  [ :arrow_down: ](https://arxiv.org/pdf/2010.06423.pdf)
>  The claustrum (Cl) is a thin grey matter structure located in the center of each brain hemisphere. Cl has been hypothesized as a central hub of the brain for multisensory/sensorimotor integration, consciousness, and attention. Accumulating evidence has suggested that Cl might be important in the development of severe neurological and psychiatric symptoms including epileptic seizures and psychosis. However, the specifics of the roles of Cl in human epilepsy and psychosis are largely unknown, primarily due to methodological limitations related to the thin morphology of Cl that is challenging to delineate accurately using conventional methods. The goal of this work is to develop noninvasive multimodal neuroimaging methods to delineate Cl anatomy by utilizing a large healthy adult high resolution (0.7mm3) T1-weighted MRI collected as part of the Washington University-Minnesota Consortium Human Connectome Project (WU-Minn HCP). We developed a comprehensive manual segmentation protocol to delineate Cl based on a cellular level brain atlas. The protocol involves detailed guidelines to delineate the three subregions of Cl, including the dorsal, ventral, and temporal Cl that can be parcellated based on a geometric method. As demonstrated in a representative result, Cl is large in its anterior-posterior, and the dorsal-ventral extent. Also, the volume is comparable to that of the amygdala. It is required to assess the reliability of the protocol so that it can be used for future anatomical studies of neuropsychiatric disorders, including epilepsy and schizophrenia.      
### 32.Structure-Preserving Model Reduction for Dissipative Mechanical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.06331.pdf)
>  Suppressing vibrations in mechanical models, usually described by second-order dynamical systems, is a challenging task in mechanical engineering in terms of computational resources even nowadays. One remedy is structure-preserving model order reduction to construct easy-to-evaluate surrogates for the original dynamical system having the same structure. In our work, we present an overview of our recently developed structure-preserving model reduction methods for second-order systems. These methods are based on modal and balanced truncation in different variants, as well as on rational interpolation. Numerical examples are used to illustrate the effectiveness of all described methods.      
### 33.Whole-Body MPC and Online Gait Sequence Generation for Wheeled-Legged Robots  [ :arrow_down: ](https://arxiv.org/pdf/2010.06322.pdf)
>  The additional degrees of freedom and missing counterparts in nature make designing locomotion capabilities for wheeled-legged robots more challenging. We propose a whole-body model predictive controller as a single task formulation that simultaneously optimizes wheel and torso motions. Due to the real-time joint velocity and ground reaction force optimization based on a kinodynamic model, our approach accurately captures the real robot's dynamics and automatically discovers complex and dynamic motions cumbersome to hand-craft through heuristics. Thanks to the single set of parameters for all behaviors, whole-body optimization makes online gait sequence adaptation possible. Aperiodic gait sequences are automatically found through kinematic leg utilities without the need for predefined contact and lift-off timings. Also, this enables us to reduce the cost of transport of wheeled-legged robots significantly. Our experiments demonstrate highly dynamic motions on a quadrupedal robot with non-steerable wheels in challenging indoor and outdoor environments. Herewith, we verify that a single task formulation is key to reveal the full potential of wheeled-legged robots.      
### 34.Approximate Simultaneous Diagonalization of Matrices via Structured Low-Rank Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2010.06305.pdf)
>  Approximate Simultaneous Diagonalization (ASD) is a problem to find a common similarity transformation which approximately diagonalizes a given square-matrix tuple. Many data science problems have been reduced into ASD through ingenious modelling. For ASD, the so-called Jacobi-like methods have been extensively used. However, the methods have no guarantee to suppress the magnitude of off-diagonal entries of the transformed tuple even if the given tuple has a common exact diagonalizer, i.e., the given tuple is simultaneously diagonalizable. In this paper, to establish an alternative powerful strategy for ASD, we present a novel two-step strategy, called Approximate-Then-Diagonalize-Simultaneously (ATDS) algorithm. The ATDS algorithm decomposes ASD into (Step 1) finding a simultaneously diagonalizable tuple near the given one; and (Step 2) finding a common similarity transformation which diagonalizes exactly the tuple obtained in Step 1. The proposed approach to Step 1 is realized by solving a Structured Low-Rank Approximation (SLRA) with Cadzow's algorithm. In Step 2, by exploiting the idea in the constructive proof regarding the conditions for the exact simultaneous diagonalizability, we obtain a common exact diagonalizer of the obtained tuple in Step 1 as a solution for the original ASD. Unlike the Jacobi-like methods, the ATDS algorithm has a guarantee to find a common exact diagonalizer if the given tuple happens to be simultaneously diagonalizable. Numerical experiments show that the ATDS algorithm achieves better performance than the Jacobi-like methods.      
### 35.Correlation Filter for UAV-Based Aerial Tracking: A Review and Experimental Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2010.06255.pdf)
>  Aerial tracking, which has exhibited its omnipresent dedication and splendid performance, is one of the most active applications in the remote sensing field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system, equipped with a visual tracking approach, has been widely used in aviation, navigation, agriculture, transportation, and public security, etc. As is mentioned above, the UAV-based aerial tracking platform has been gradually developed from research to practical application stage, reaching one of the main aerial remote sensing technologies in the future. However, due to real-world challenging situations, the vibration of the UAV's mechanical structure (especially under strong wind conditions), and limited computation resources, accuracy, robustness, and high efficiency are all crucial for the onboard tracking methods. Recently, the discriminative correlation filter (DCF)-based trackers have stood out for their high computational efficiency and appealing robustness on a single CPU, and have flourished in the UAV visual tracking community. In this work, the basic framework of the DCF-based trackers is firstly generalized, based on which, 20 state-of-the-art DCF-based trackers are orderly summarized according to their innovations for soloving various issues. Besides, exhaustive and quantitative experiments have been extended on various prevailing UAV tracking benchmarks, i.e., UAV123, UAV123_10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,625 frames in total. The experiments show the performance, verify the feasibility, and demonstrate the current challenges of DCF-based trackers onboard UAV tracking. Finally, comprehensive conclusions on open challenges and directions for future research is presented.      
### 36.A variational autoencoder for music generation controlled by tonal tension  [ :arrow_down: ](https://arxiv.org/pdf/2010.06230.pdf)
>  Many of the music generation systems based on neural networks are fully autonomous and do not offer control over the generation process. In this research, we present a controllable music generation system in terms of tonal tension. We incorporate two tonal tension measures based on the Spiral Array Tension theory into a variational autoencoder model. This allows us to control the direction of the tonal tension throughout the generated piece, as well as the overall level of tonal tension. Given a seed musical fragment, stemming from either the user input or from directly sampling from the latent space, the model can generate variations of this original seed fragment with altered tonal tension. This altered music still resembles the seed music rhythmically, but the pitch of the notes are changed to match the desired tonal tension as conditioned by the user.      
### 37.End-to-end Triplet Loss based Emotion Embedding System for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.06200.pdf)
>  In this paper, an end-to-end neural embedding system based on triplet loss and residual learning has been proposed for speech emotion recognition. The proposed system learns the embeddings from the emotional information of the speech utterances. The learned embeddings are used to recognize the emotions portrayed by given speech samples of various lengths. The proposed system implements Residual Neural Network architecture. It is trained using softmax pre-training and triplet loss function. The weights between the fully connected and embedding layers of the trained network are used to calculate the embedding values. The embedding representations of various emotions are mapped onto a hyperplane, and the angles among them are computed using the cosine similarity. These angles are utilized to classify a new speech sample into its appropriate emotion class. The proposed system has demonstrated 91.67% and 64.44% accuracy while recognizing emotions for RAVDESS and IEMOCAP dataset, respectively.      
### 38.Correlation-wise Smoothing: Lightweight Knowledge Extraction for HPC Monitoring Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.06186.pdf)
>  Modern High-Performance Computing (HPC) and data center operators rely more and more on data analytics techniques to improve the efficiency and reliability of their operations. They employ models that ingest time-series monitoring sensor data and transform it into actionable knowledge for system tuning: a process known as Operational Data Analytics (ODA). However, monitoring data has a high dimensionality, is hardware-dependent and difficult to interpret. This, coupled with the strict requirements of ODA, makes most traditional data mining methods impractical and in turn renders this type of data cumbersome to process. Most current ODA solutions use ad-hoc processing methods that are not generic, are sensible to the sensors' features and are not fit for visualization. <br>In this paper we propose a novel method, called Correlation-wise Smoothing (CS), to extract descriptive signatures from time-series monitoring data in a generic and lightweight way. Our CS method exploits correlations between data dimensions to form groups and produces image-like signatures that can be easily manipulated, visualized and compared. We evaluate the CS method on HPC-ODA, a collection of datasets that we release with this work, and show that it leads to the same performance as most state-of-the-art methods while producing signatures that are up to ten times smaller and up to ten times faster, while gaining visualizability, portability across systems and clear scaling properties.      
### 39.Mass Estimation in Manipulation Tasks of Domestic Service Robots using Fault Reconstruction Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2010.06116.pdf)
>  Manipulation is a key capability in domestic service robots, as can be seen in the rulebooks of last Robocup@Home editions. Currently, object recognition is performed based mostly on visual information. Some robots use also 3D information such as point clouds or laser scans but, to the knowledge of authors, robots don't use physical properties to improve object recognition. Estimation of an object's weight during a manipulation task is something new in the @Home league and such ability can improve performance of domestic service robots. In this work we propose to estimate the weight of the grasped object using Sliding Mode Observers. If we consider the manipulator without load as the nominal system and object's weight as a fault signal, we can estimate such weight by an appropriate filtering of the output error injection term of the sliding mode observer. To implement our proposal we used MATLAB and Simulink Robotics System Toolbox, ROS Toolbox and Simscape. To improve computation time we exported all algorithms to standalone ROS nodes from Simulink models. Tests were performed using two platforms: Justina's left manipulator (a robot developed at Biorobotics Laboratory, UNAM) and Neuronics Katana manipulators. We present results in simulation and discuss the performance of the proposed system and the possible sources of error. Finally we present our conclusions and state the future work.      
### 40.CADET: A Systematic Method For Debugging Misconfigurations using Counterfactual Reasoning  [ :arrow_down: ](https://arxiv.org/pdf/2010.06061.pdf)
>  Modern computing platforms are highly-configurable with thousands of interacting configurations. However, configuring these systems is challenging. Erroneous configurations can cause unexpected non-functional faults. This paper proposes CADET (short for Causal Debugging Toolkit) that enables users to identify, explain, and fix the root cause of non-functional faults early and in a principled fashion. CADET builds a causal model by observing the performance of the system under different configurations. Then, it uses casual path extraction followed by counterfactual reasoning over the causal model to: (a) identify the root causes of non-functional faults, (b) estimate the effects of various configurable parameters on the performance objective(s), and (c) prescribe candidate repairs to the relevant configuration options to fix the non-functional fault. We evaluated CADET on 5 highly-configurable systems deployed on 3 NVIDIA Jetson systems-on-chip. We compare CADET with state-of-the-art configuration optimization and ML-based debugging approaches. The experimental results indicate that CADET can find effective repairs for faults in multiple non-functional properties with (at most) 17% more accuracy, 28% higher gain, and $40\times$ speed-up than other ML-based performance debugging methods. Compared to multi-objective optimization approaches, CADET can find fixes (at most) $9\times$ faster with comparable or better performance gain. Our case study of non-functional faults reported in NVIDIA's forum show that CADET can find $14%$ better repairs than the experts' advice in less than 30 minutes.      
### 41.On Optimizing the Secrecy Performance of RIS-Assisted Cooperative Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.06052.pdf)
>  Employing reconfigurable intelligent surfaces (RIS) is emerging as a game-changer candidate, thanks to their unique capabilities in improving the power efficiency and supporting the ubiquity of future wireless communication systems. Conventionally, a wireless network design has been limited to the communicating end points, i.e., the transmitter and the receiver. In general, we take advantage of the imposed channel state knowledge to manipulate the transmitted signal and to improve the detection quality at the receiver. With the aid of RISs, and to some extent, the propagation channel has become a part of the design problem. In this paper, we consider a single-input single-output cooperative network and investigate the effect of using RISs in enhancing the physical layer security of the system. Specifically, we formulate an optimization problem to study the effectiveness of the RIS in improving the system secrecy by introducing a weighted variant of the secrecy capacity definition. Numerical simulations are provided to show the design trade-offs and to present the superiority of RIS-assisted networks over the conventional ones in terms of the system's secrecy performance.      
### 42.Artificial Intelligence, speech and language processing approaches to monitoring Alzheimer's Disease: a systematic review  [ :arrow_down: ](https://arxiv.org/pdf/2010.06047.pdf)
>  Language is a valuable source of clinical information in Alzheimer's Disease, as it declines concurrently with neurodegeneration. Consequently, speech and language data have been extensively studied in connection with its diagnosis. This paper summarises current findings on the use of artificial intelligence, speech and language processing to predict cognitive decline in the context of Alzheimer's Disease, detailing current research procedures, highlighting their limitations and suggesting strategies to address them. We conducted a systematic review of original research between 2000 and 2019, registered in PROSPERO (reference CRD42018116606). An interdisciplinary search covered six databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine (PubMed and Embase) and Web of Science. Bibliographies of relevant papers were screened until December 2019. From 3,654 search results 51 articles were selected against the eligibility criteria. Four tables summarise their findings: study details (aim, population, interventions, comparisons, methods and outcomes), data details (size, type, modalities, annotation, balance, availability and language of study), methodology (pre-processing, feature generation, machine learning, evaluation and results) and clinical applicability (research implications, clinical potential, risk of bias and strengths/limitations). While promising results are reported across nearly all 51 studies, very few have been implemented in clinical research or practice. We concluded that the main limitations of the field are poor standardisation, limited comparability of results, and a degree of disconnect between study aims and clinical applications. Attempts to close these gaps should support translation of future research into clinical practice.      
### 43.Spectral Synthesis for Satellite-to-Satellite Translation  [ :arrow_down: ](https://arxiv.org/pdf/2010.06045.pdf)
>  Earth observing satellites carrying multi-spectral sensors are widely used to monitor the physical and biological states of the atmosphere, land, and oceans. These satellites have different vantage points above the earth and different spectral imaging bands resulting in inconsistent imagery from one to another. This presents challenges in building downstream applications. What if we could generate synthetic bands for existing satellites from the union of all domains? We tackle the problem of generating synthetic spectral imagery for multispectral sensors as an unsupervised image-to-image translation problem with partial labels and introduce a novel shared spectral reconstruction loss. Simulated experiments performed by dropping one or more spectral bands show that cross-domain reconstruction outperforms measurements obtained from a second vantage point. On a downstream cloud detection task, we show that generating synthetic bands with our model improves segmentation performance beyond our baseline. Our proposed approach enables synchronization of multispectral data and provides a basis for more homogeneous remote sensing datasets.      
### 44.Universal ASR: Unify and Improve Streaming ASR with Full-context Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2010.06030.pdf)
>  Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible, while full-context ASR waits for the completion of a full speech utterance before emitting completed hypotheses. In this work, we propose a unified framework, Universal ASR, to train a single end-to-end ASR model with shared weights for both streaming and full-context speech recognition. We show that the latency and accuracy of streaming ASR significantly benefit from weight sharing and joint training of full-context ASR, especially with inplace knowledge distillation. The Universal ASR framework can be applied to recent state-of-the-art convolution-based and transformer-based ASR networks. We present extensive experiments with two state-of-the-art ASR networks, ContextNet and Conformer, on two datasets, a widely used public dataset LibriSpeech and an internal large-scale dataset MultiDomain. Experiments and ablation studies demonstrate that Universal ASR not only simplifies the workflow of training and deploying streaming and full-context ASR models, but also significantly improves both emission latency and recognition accuracy of streaming ASR. With Universal ASR, we achieve new state-of-the-art streaming ASR results on both LibriSpeech and MultiDomain in terms of accuracy and latency.      
### 45.Learning Pugachev's Cobra Maneuver for Tail-sitter UAVs Using Acceleration Model  [ :arrow_down: ](https://arxiv.org/pdf/1906.02596.pdf)
>  The Pugachev's cobra maneuver is a dramatic and demanding maneuver requiring the aircraft to fly at extremely high Angle of Attacks (AOA) where stalling occurs. This paper considers this maneuver on tail-sitter UAVs. We present a simple yet very effective feedback-iterative learning position control structure to regulate the altitude error and lateral displacement during the maneuver. Both the feedback controller and the iterative learning controller are based on the aircraft acceleration model, which is directly measurable by the onboard accelerometer. Moreover, the acceleration model leads to an extremely simple dynamic model that does not require any model identification in designing the position controller, greatly simplifying the implementation of the iterative learning control. Real-world outdoor flight experiments on the "Hong Hu" UAV, an aerobatic yet efficient quadrotor tail-sitter UAV of small-size, are provided to show the effectiveness of the proposed controller.      
