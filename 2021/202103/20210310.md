# ArXiv eess --Wed, 10 Mar 2021
### 1.Distributed Frequency Restoration and SoC Balancing Control for AC Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2103.05576.pdf)
>  This paper develops an improved distributed finite-time control algorithm for multiagent-based ac microgrids with battery energy storage systems (BESSs) utilizing a low-width communication network. The proposed control algorithm can simultaneously coordinate BESSs to eliminate any deviation from the nominal frequency as well as solving the state of charge (SoC) balancing problem. The stability of the proposed control algorithm is established using the Lyapunov method and homogeneous approximation theory, which guarantees an accelerated convergence within a settling time that does not dependent on initial conditions. Based on this, to significantly reduce the communication burdens, an event-triggered communication mechanism is designed which can also avoid Zeno behavior. Then sufficient conditions on the event-triggered boundary are derived to guarantee the stability and reliability of the whole system. Practical local constraints are imposed to implement the control protocol, and the theoretical results are applied to a test system consisting of five DGs and five BESSs, which verifies the effectiveness of the proposed strategy.      
### 2.Quadruple Augmented Pyramid Network for Multi-class COVID-19 Segmentation via CT  [ :arrow_down: ](https://arxiv.org/pdf/2103.05546.pdf)
>  COVID-19, a new strain of coronavirus disease, has been one of the most serious and infectious disease in the world. Chest CT is essential in prognostication, diagnosing this disease, and assessing the complication. In this paper, a multi-class COVID-19 CT segmentation is proposed aiming at helping radiologists estimate the extent of effected lung volume. We utilized four augmented pyramid networks on an encoder-decoder segmentation framework. Quadruple Augmented Pyramid Network (QAP-Net) not only enable CNN capture features from variation size of CT images, but also act as spatial interconnections and down-sampling to transfer sufficient feature information for semantic segmentation. Experimental results achieve competitive performance in segmentation with the Dice of 0.8163, which outperforms other state-of-the-art methods, demonstrating the proposed framework can segments of consolidation as well as glass, ground area via COVID-19 chest CT efficiently and accurately.      
### 3.Shift Equivariance for Pixel-wise Self-supervised SAR-optical Feature Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2103.05543.pdf)
>  An important topic in the community of remote sensing is how to combine the complementary information provided by the huge amount of unlabeled multi-sensor data, such as Synthetic Aperture Radar (SAR) and optical images. Recently, contrastive learning methods have reached remarkable success in obtaining meaningful feature representations from multi-view data. However, these methods only focus on the image-level features, which may not satisfy the requirement for dense prediction tasks such as land-cover mapping. In this work, we propose a new self-supervised approach for SAR-optical data-fusion that can learn disentangled pixel-wise feature representations directly by taking advantage of both multi-view contrastive learning and the BYOL. The two key contributions were proposed for this approach: multi-view contrastive loss to encode the multi-modal images and the shift operation to reconstruct learned representations for each pixel by building the local consistency between different augmented views. With the aim to validate the effectiveness of the proposed approach, we conduct experiments on the land cover mapping task, where we trained the proposed approach using unlabeled SAR-optical image pairs while labeled data pairs were used for the linear classification and finetuning evaluations. We empirically show that the presented approach outperforms the state-of-the-art methods. In particular, it achieves an improvement on both linear classification and finetuning evaluations and reduces the dimension of representations with respect to the image-level contrastive learning method. Moreover, the proposed method is also validated to bring a sharp improvement on SAR-optical feature fusion than the early fusion fashion for the land-cover mapping task.      
### 4.A Bayesian approach for $T_2^*$ Mapping and Quantitative Susceptibility Mapping  [ :arrow_down: ](https://arxiv.org/pdf/2103.05535.pdf)
>  Magnetic resonance $T_2^*$ mapping and quantitative susceptibility mapping (QSM) provide direct and precise mappings of tissue contrasts. They are widely used to study iron deposition, hemorrhage and calcification in various clinical applications. In practice, the measurements can be undersampled in the $k$-space to reduce the scan time needed for high-resolution 3D maps, and sparse prior on the wavelet coefficients of images can be used to fill in the missing information via compressive sensing. To avoid the extensive parameter tuning process of conventional regularization methods, we adopt a Bayesian approach to perform $T_2^*$ mapping and QSM using approximate message passing (AMP): the sparse prior is enforced through probability distributions, and the distribution parameters can be automatically and adaptively estimated. In this paper we propose a new nonlinear AMP framework that incorporates the mono-exponential decay model, and use it to recover the proton density, the $T_2^*$ map and complex multi-echo images. The QSM can be computed from the multi-echo images subsequently. Experimental results show that the proposed approach successfully recovers $T_2^*$ map and QSM across various sampling rates, and performs much better than the state-of-the-art $l_1$-norm regularization approach.      
### 5.Potential Advantages of Peak Picking Multi-Voltage Threshold Digitizer in Energy Determination in Radiation Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2103.05532.pdf)
>  The Multi-voltage Threshold (MVT) method, which samples the signal by certain reference voltages, has been well developed as being adopted in pre-clinical and clinical digital positron emission tomography(PET) system. To improve its energy measurement performance, we propose a Peak Picking MVT(PP-MVT) Digitizer in this paper. Firstly, a sampled Peak Point(the highest point in pulse signal), which carries the values of amplitude feature voltage and amplitude arriving time, is added to traditional MVT with a simple peak sampling circuit. Secondly, an amplitude deviation statistical analysis, which compares the energy deviation of various reconstruction models, is used to select adaptive reconstruction models for signal pulses with different amplitudes. After processing 30,000 randomly-chosen pulses sampled by the oscilloscope with a 22Na point source, our method achieves an energy resolution of 17.50% within a 450-650 KeV energy window, which is 2.44% better than the result of traditional MVT with same thresholds; and we get a count number at 15225 in the same energy window while the result of MVT is at 14678. When the PP-MVT involves less thresholds than traditional MVT, the advantages of better energy resolution and larger count number can still be maintained, which shows the robustness and the flexibility of PP-MVT Digitizer. This improved method indicates that adding feature peak information could improve the performance on signal sampling and reconstruction, which canbe proved by the better performance in energy determination in radiation measurement.      
### 6.Deep and Statistical Learning in Biomedical Imaging: State of the Art in 3D MRI Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.05529.pdf)
>  Clinical diagnostic and treatment decisions rely upon the integration of patient-specific data with clinical reasoning. Cancer presents a unique context that influence treatment decisions, given its diverse forms of disease evolution. Biomedical imaging allows noninvasive assessment of disease based on visual evaluations leading to better clinical outcome prediction and therapeutic planning. Early methods of brain cancer characterization predominantly relied upon statistical modeling of neuroimaging data. Driven by the breakthroughs in computer vision, deep learning became the de facto standard in the domain of medical imaging. Integrated statistical and deep learning methods have recently emerged as a new direction in the automation of the medical practice unifying multi-disciplinary knowledge in medicine, statistics, and artificial intelligence. In this study, we critically review major statistical and deep learning models and their applications in brain imaging research with a focus on MRI-based brain tumor segmentation. The results do highlight that model-driven classical statistics and data-driven deep learning is a potent combination for developing automated systems in clinical oncology.      
### 7.Using AoI Forecasts in Communicating and Robust Distributed Model-Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2103.05526.pdf)
>  In order to enhance the performance of cyber-physical systems, this paper proposes the integrated de-sign of distributed controllers for distributed plants andthe control of the communication network. Conventionaldesign methods use static interfaces between both enti-ties and therefore rely on worst-case estimations of com-munication delay, often leading to conservative behaviorof the overall system. By contrast, the present approachestablishes a robust distributed model-predictive controlscheme, in which the local subsystem controllers oper-ate under the assumption of a variable communicationschedule that is predicted by a network controller. Us-ing appropriate models for the communication network,the network controller applies a predictive network policyfor scheduling the communication among the subsystemcontrollers across the network. Given the resulting time-varying predictions of the age of information, the papershows under which conditions the subsystem controllerscan robustly stabilize the distributed system. To illustratethe approach, the paper also reports on the application to avehicle platooning scenario.      
### 8.Multi-phase Deformable Registration for Time-dependent Abdominal Organ Variations  [ :arrow_down: ](https://arxiv.org/pdf/2103.05525.pdf)
>  Human body is a complex dynamic system composed of various sub-dynamic parts. Especially, thoracic and abdominal organs have complex internal shape variations with different frequencies by various reasons such as respiration with fast motion and peristalsis with slower motion. CT protocols for abdominal lesions are multi-phase scans for various tumor detection to use different vascular contrast, however, they are not aligned well enough to visually check the same area. In this paper, we propose a time-efficient and accurate deformable registration algorithm for multi-phase CT scans considering abdominal organ motions, which can be applied for differentiable or non-differentiable motions of abdominal organs. Experimental results shows the registration accuracy as 0.85 +/- 0.45mm (mean +/- STD) for pancreas within 1 minute for the whole abdominal region.      
### 9.Approximate Optimal Filter for Linear Gaussian Time-invariant Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.05505.pdf)
>  State estimation is critical to control systems, especially when the states cannot be directly measured. This paper presents an approximate optimal filter, which enables to use policy iteration technique to obtain the steady-state gain in linear Gaussian time-invariant systems. This design transforms the optimal filtering problem with minimum mean square error into an optimal control problem, called Approximate Optimal Filtering (AOF) problem. The equivalence holds given certain conditions about initial state distributions and policy formats, in which the system state is the estimation error, control input is the filter gain, and control objective function is the accumulated estimation error. We present a policy iteration algorithm to solve the AOF problem in steady-state. A classic vehicle state estimation problem finally evaluates the approximate filter. The results show that the policy converges to the steady-state Kalman gain, and its accuracy is within 2 %.      
### 10.CNN-based Spoken Term Detection and Localization without Dynamic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2103.05468.pdf)
>  In this paper, we propose a spoken term detection algorithm for simultaneous prediction and localization of in-vocabulary and out-of-vocabulary terms within an audio segment. The proposed algorithm infers whether a term was uttered within a given speech signal or not by predicting the word embeddings of various parts of the speech signal and comparing them to the word embedding of the desired term. The algorithm utilizes an existing embedding space for this task and does not need to train a task-specific embedding space. At inference the algorithm simultaneously predicts all possible locations of the target term and does not need dynamic programming for optimal search. We evaluate our system on several spoken term detection tasks on read speech corpora.      
### 11.Convolutional Neural Network Denoising in Fluorescence Lifetime Imaging Microscopy (FLIM)  [ :arrow_down: ](https://arxiv.org/pdf/2103.05448.pdf)
>  Fluorescence lifetime imaging microscopy (FLIM) systems are limited by their slow processing speed, low signal-to-noise ratio (SNR), and expensive and challenging hardware setups. In this work, we demonstrate applying a denoising convolutional network to improve FLIM SNR. The network will be integrated with an instant FLIM system with fast data acquisition based on analog signal processing, high SNR using high-efficiency pulse-modulation, and cost-effective implementation utilizing off-the-shelf radio-frequency components. Our instant FLIM system simultaneously provides the intensity, lifetime, and phasor plots \textit{in vivo} and \textit{ex vivo}. By integrating image denoising using the trained deep learning model on the FLIM data, provide accurate FLIM phasor measurements are obtained. The enhanced phasor is then passed through the K-means clustering segmentation method, an unbiased and unsupervised machine learning technique to separate different fluorophores accurately. Our experimental \textit{in vivo} mouse kidney results indicate that introducing the deep learning image denoising model before the segmentation effectively removes the noise in the phasor compared to existing methods and provides clearer segments. Hence, the proposed deep learning-based workflow provides fast and accurate automatic segmentation of fluorescence images using instant FLIM. The denoising operation is effective for the segmentation if the FLIM measurements are noisy. The clustering can effectively enhance the detection of biological structures of interest in biomedical imaging applications.      
### 12.Efficient Multi-Stage Video Denoising with Recurrent Spatio-Temporal Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2103.05407.pdf)
>  In recent years, methods based on deep learning have achieved unparalleled performance at the cost of large computational complexity. In this work, we propose an Efficient Multi-stage Video Denoising algorithm, called EMVD, to drastically reduce the complexity while maintaining or even improving the performance. First, a fusion stage reduces the noise through a recursive combination of all past frames in the video. Then, a denoising stage removes the noise in the fused frame. Finally, a refinement stage restores the missing high frequency in the denoised frame. All stages operate on a transform-domain representation obtained by learnable and invertible linear operators which simultaneously increase accuracy and decrease complexity of the model. A single loss on the final output is sufficient for successful convergence, hence making EMVD easy to train. Experiments on real raw data demonstrate that EMVD outperforms the state of the art when complexity is constrained, and even remains competitive against methods whose complexities are several orders of magnitude higher. The low complexity and memory requirements of EMVD enable real-time video denoising on low-powered commercial SoC.      
### 13.Semantic Communications in Networked Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.05391.pdf)
>  We present our vision for a departure from the established way of architecting and assessing communication networks, by incorporating the semantics of information for communications and control in networked systems. We define semantics of information, not as the meaning of the messages, but as their significance, possibly within a real time constraint, relative to the purpose of the data exchange. We argue that research efforts must focus on laying the theoretical foundations of a redesign of the entire process of information generation, transmission and usage in unison by developing: advanced semantic metrics for communications and control systems; an optimal sampling theory combining signal sparsity and semantics, for real-time prediction, reconstruction and control under communication constraints and delays; semantic compressed sensing techniques for decision making and inference directly in the compressed domain; semantic-aware data generation, channel coding, feedback, multiple and random access schemes that reduce the volume of data and the energy consumption, increasing the number of supportable devices.      
### 14.Machine Learning based Efficient QT-MTT Partitioning Scheme for VVC Intra Encoders  [ :arrow_down: ](https://arxiv.org/pdf/2103.05319.pdf)
>  The next-generation Versatile Video Coding (VVC) standard introduces a new Multi-Type Tree (MTT) block partitioning structure that supports Binary-Tree (BT) and Ternary-Tree (TT) splits in both vertical and horizontal directions. This new approach leads to five possible splits at each block depth and thereby improves the coding efficiency of VVC over that of the preceding High Efficiency Video Coding (HEVC) standard, which only supports Quad-Tree (QT) partitioning with a single split per block depth. However, MTT also has brought a considerable impact on encoder computational complexity. In this paper, a two-stage learning-based technique is proposed to tackle the complexity overhead of MTT in VVC intra encoders. In our scheme, the input block is first processed by a Convolutional Neural Network (CNN) to predict its spatial features through a vector of probabilities describing the partition at each 4x4 edge. Subsequently, a Decision Tree (DT) model leverages this vector of spatial features to predict the most likely splits at each block. Finally, based on this prediction, only the N most likely splits are processed by the Rate-Distortion (RD) process of the encoder. In order to train our CNN and DT models on a wide range of image contents, we also propose a public VVC frame partitioning dataset based on existing image dataset encoded with the VVC reference software encoder. Our proposal relying on the top-3 configuration reaches 46.6% complexity reduction for a negligible bitrate increase of 0.86%. A top-2 configuration enables a higher complexity reduction of 69.8% for 2.57% bitrate loss. These results emphasis a better trade-off between VTM intra coding efficiency and complexity reduction compared to the state-of-the-art solutions.      
### 15.3D-QCNet -- A Pipeline for Automated Artifact Detection in Diffusion MRI images  [ :arrow_down: ](https://arxiv.org/pdf/2103.05285.pdf)
>  Artifacts are a common occurrence in Diffusion MRI (dMRI) scans. Identifying and removing them is essential to ensure the accuracy and viability of any post processing carried out on these scans. This makes QC (quality control) a crucial first step prior to any analysis of dMRI data. Several QC methods for artifact detection exist, however they suffer from problems like requiring manual intervention and the inability to generalize across different artifacts and datasets. In this paper, we propose an automated deep learning (DL) pipeline that utilizes a 3D-Densenet architecture to train a model on diffusion volumes for automatic artifact detection. Our method is applied on a vast dataset consisting of 9000 volumes sourced from 7 large clinical datasets. These datasets comprise scans from multiple scanners with different gradient directions, high and low b values, single shell and multi shell acquisitions. Additionally, they represent diverse subject demographics like the presence or absence of pathologies. Our QC method is found to accurately generalize across this heterogenous data by correctly detecting 92% artifacts on average across our test set. This consistent performance over diverse datasets underlines the generalizability of our method, which currently is a significant barrier hindering the widespread adoption of automated QC techniques. For these reasons, we believe that 3D-QCNet can be integrated in diffusion pipelines to effectively automate the arduous and time-intensive process of artifact detection.      
### 16.2D histology meets 3D topology: Cytoarchitectonic brain mapping with Graph Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05259.pdf)
>  Cytoarchitecture describes the spatial organization of neuronal cells in the brain, including their arrangement into layers and columns with respect to cell density, orientation, or presence of certain cell types. It allows to segregate the brain into cortical areas and subcortical nuclei, links structure with connectivity and function, and provides a microstructural reference for human brain atlases. Mapping boundaries between areas requires to scan histological sections at microscopic resolution. While recent high-throughput scanners allow to scan a complete human brain in the order of a year, it is practically impossible to delineate regions at the same pace using the established gold standard method. Researchers have recently addressed cytoarchitectonic mapping of cortical regions with deep neural networks, relying on image patches from individual 2D sections for classification. However, the 3D context, which is needed to disambiguate complex or obliquely cut brain regions, is not taken into account. In this work, we combine 2D histology with 3D topology by reformulating the mapping task as a node classification problem on an approximate 3D midsurface mesh through the isocortex. We extract deep features from cortical patches in 2D histological sections which are descriptive of cytoarchitecture, and assign them to the corresponding nodes on the 3D mesh to construct a large attributed graph. By solving the brain mapping problem on this graph using graph neural networks, we obtain significantly improved classification results. The proposed framework lends itself nicely to integration of additional neuroanatomical priors for mapping.      
### 17.Generalizable Limited-Angle CT Reconstruction via Sinogram Extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2103.05255.pdf)
>  Computed tomography (CT) reconstruction from X-ray projections acquired within a limited angle range is challenging, especially when the angle range is extremely small. Both analytical and iterative models need more projections for effective modeling. Deep learning methods have gained prevalence due to their excellent reconstruction performances, but such success is mainly limited within the same dataset and does not generalize across datasets with different distributions. Hereby we propose ExtraPolationNetwork for limited-angle CT reconstruction via the introduction of a sinogram extrapolation module, which is theoretically justified. The module complements extra sinogram information and boots model generalizability. Extensive experimental results show that our reconstruction model achieves state-of-the-art performance on NIH-AAPM dataset, similar to existing approaches. More importantly, we show that using such a sinogram extrapolation module significantly improves the generalization capability of the model on unseen datasets (e.g., COVID-19 and LIDC datasets) when compared to existing approaches.      
### 18.Negative Imaginary State Feedback Equivalence for Systems of Relative Degree One and Relative Degree Two  [ :arrow_down: ](https://arxiv.org/pdf/2103.05249.pdf)
>  This paper presents necessary and sufficient conditions under which a linear system of relative degree either one or two is state feedback equivalent to a negative imaginary (NI) system. More precisely, we show for a class of linear time-invariant strictly proper systems, that such a system can be rendered minimal and negative imaginary using full state feedback if and only if it is controllable and weakly minimum phase. A strongly strict negative imaginary (SSNI) state feedback equivalence result is also provided. The NI state feedback equivalence result is then applied in a robust stabilization problem for an uncertain system with a strictly negative imaginary (SNI) uncertainty.      
### 19.A review of flywheel energy storage systems: state of the art and opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2103.05224.pdf)
>  Thanks to the unique advantages such as long life cycles, high power density and quality, and minimal environmental impact, the flywheel/kinetic energy storage system (FESS) is gaining steam recently. There is noticeable progress made in FESS, especially in utility, large-scale deployment for the electrical grid, and renewable energy applications. This paper gives a review of the recent developments in FESS technologies. Due to the highly interdisciplinary nature of FESSs, we survey different design approaches, choices of subsystems, and the effects on performance, cost, and applications. This review focuses on the state of the art of FESS technologies, especially for those who have been commissioned or prototyped. We also highlighted the opportunities and potential directions for the future development of FESS technologies.      
### 20.Prediction of 5-year Progression-Free Survival in Advanced Nasopharyngeal Carcinoma with Pretreatment PET/CT using Multi-Modality Deep Learning-based Radiomics  [ :arrow_down: ](https://arxiv.org/pdf/2103.05220.pdf)
>  Deep Learning-based Radiomics (DLR) has achieved great success on medical image analysis. In this study, we aim to explore the capability of DLR for survival prediction in NPC. We developed an end-to-end multi-modality DLR model using pretreatment PET/CT images to predict 5-year Progression-Free Survival (PFS) in advanced NPC. A total of 170 patients with pathological confirmed advanced NPC (TNM stage III or IVa) were enrolled in this study. A 3D Convolutional Neural Network (CNN), with two branches to process PET and CT separately, was optimized to extract deep features from pretreatment multi-modality PET/CT images and use the derived features to predict the probability of 5-year PFS. Optionally, TNM stage, as a high-level clinical feature, can be integrated into our DLR model to further improve prognostic performance. For a comparison between CR and DLR, 1456 handcrafted features were extracted, and three top CR methods were selected as benchmarks from 54 combinations of 6 feature selection methods and 9 classification methods. Compared to the three CR methods, our multi-modality DLR models using both PET and CT, with or without TNM stage (named PCT or PC model), resulted in the highest prognostic performance. Furthermore, the multi-modality PCT model outperformed single-modality DLR models using only PET and TNM stage (PT model) or only CT and TNM stage (CT model). Our study identified potential radiomics-based prognostic model for survival prediction in advanced NPC, and suggests that DLR could serve as a tool for aiding in cancer management.      
### 21.A Simplified Multifractal Model for Self-Similar Traffic Flows in High-Speed Computer Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05183.pdf)
>  This paper proposes a multifractal model, with the aim of providing a possible explanation for the locality phenomenon that appears in the estimation of the Hurst exponent in stationary second order temporal series representing self-similar traffic flows in current high-speed computer networks. It is shown analytically that this phenomenon occurs if the network flow consists of several components with different Hurst exponents.      
### 22.Fast Quantized Average Consensus over Static and Dynamic Directed Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2103.05172.pdf)
>  In this paper we study the distributed average consensus problem in multi-agent systems with directed communication links that are subject to quantized information flow. Specifically, we present and analyze a distributed averaging algorithm which operates exclusively with quantized values (i.e., the information stored, processed and exchanged between neighboring agents is subject to deterministic uniform quantization) and relies on event-driven updates (e.g., to reduce energy consumption, communication bandwidth, network congestion, and/or processor usage). The main idea of the proposed algorithm is that each node (i) models its initial state as two quantized fractions which have numerators equal to the node's initial state and denominators equal to one, and (ii) transmits one fraction randomly while it keeps the other stored. Then, every time it receives one or more fractions, it averages their numerators with the numerator of the fraction it stored, and then transmits them to randomly selected out-neighbors. We characterize the properties of the proposed distributed algorithm and show that its execution, on any static and strongly connected digraph, allows each agent to reach in finite time a fixed state that is equal (within one quantisation level) to the average of the initial states. We extend the operation of the algorithm to achieve finite-time convergence in the presence of a dynamic directed communication topology subject to some connectivity conditions. Finally, we provide examples to illustrate the operation, performance, and potential advantages of the proposed algorithm. We compare against state-of-the-art quantized average consensus algorithms and show that our algorithm's convergence speed significantly outperforms most existing protocols.      
### 23.Stochastic Entry Guidance  [ :arrow_down: ](https://arxiv.org/pdf/2103.05168.pdf)
>  In this paper, closed-loop entry guidance in a randomly perturbed atmosphere, using bank angle control, is posed as a stochastic optimal control problem. The entry trajectory, as well as the closed-loop controls, are both modeled as random processes with statistics determined by the entry dynamics, the entry guidance, and the probabilistic structure of altitude-dependent atmospheric density variations. The entry guidance, which is parameterized as a sequence of linear feedback gains, is designed to steer the probability distribution of the entry trajectories while satisfying bounds on the allowable control inputs and on the maximum allowable state errors. Numerical simulations of a Mars entry scenario demonstrate improved range targeting performance when using the developed stochastic guidance scheme as compared to the existing Apollo final phase algorithm.      
### 24.Formal Verification of Stochastic Systems with ReLU Neural Network Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2103.05142.pdf)
>  In this work, we address the problem of formal safety verification for stochastic cyber-physical systems (CPS) equipped with ReLU neural network (NN) controllers. Our goal is to find the set of initial states from where, with a predetermined confidence, the system will not reach an unsafe configuration within a specified time horizon. Specifically, we consider discrete-time LTI systems with Gaussian noise, which we abstract by a suitable graph. Then, we formulate a Satisfiability Modulo Convex (SMC) problem to estimate upper bounds on the transition probabilities between nodes in the graph. Using this abstraction, we propose a method to compute tight bounds on the safety probabilities of nodes in this graph, despite possible over-approximations of the transition probabilities between these nodes. Additionally, using the proposed SMC formula, we devise a heuristic method to refine the abstraction of the system in order to further improve the estimated safety bounds. Finally, we corroborate the efficacy of the proposed method with simulation results considering a robot navigation example and comparison against a state-of-the-art verification scheme.      
### 25.ASL to PET Translation by a Semi-supervised Residual-based Attention-guided Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.05116.pdf)
>  Positron Emission Tomography (PET) is an imaging method that can assess physiological function rather than structural disturbances by measuring cerebral perfusion or glucose consumption. However, this imaging technique relies on injection of radioactive tracers and is expensive. On the contrary, Arterial Spin Labeling (ASL) MRI is a non-invasive, non-radioactive, and relatively cheap imaging technique for brain hemodynamic measurements, which allows quantification to some extent. In this paper we propose a convolutional neural network (CNN) based model for translating ASL to PET images, which could benefit patients as well as the healthcare system in terms of expenses and adverse side effects. However, acquiring a sufficient number of paired ASL-PET scans for training a CNN is prohibitive for many reasons. To tackle this problem, we present a new semi-supervised multitask CNN which is trained on both paired data, i.e. ASL and PET scans, and unpaired data, i.e. only ASL scans, which alleviates the problem of training a network on limited paired data. Moreover, we present a new residual-based-attention guided mechanism to improve the contextual features during the training process. Also, we show that incorporating T1-weighted scans as an input, due to its high resolution and availability of anatomical information, improves the results. We performed a two-stage evaluation based on quantitative image metrics by conducting a 7-fold cross validation followed by a double-blind observer study. The proposed network achieved structural similarity index measure (SSIM), mean squared error (MSE) and peak signal-to-noise ratio (PSNR) values of $0.85\pm0.08$, $0.01\pm0.01$, and $21.8\pm4.5$ respectively, for translating from 2D ASL and T1-weighted images to PET data. The proposed model is publicly available via <a class="link-external link-https" href="https://github.com/yousefis/ASL2PET" rel="external noopener nofollow">this https URL</a>.      
### 26.Deep reinforcement learning in medical imaging: A literature review  [ :arrow_down: ](https://arxiv.org/pdf/2103.05115.pdf)
>  Deep reinforcement learning (DRL) augments the reinforcement learning framework, which learns a sequence of actions that maximizes the expected reward, with the representative power of deep neural networks. Recent works have demonstrated the great potential of DRL in medicine and healthcare. This paper presents a literature review of DRL in medical imaging. We start with a comprehensive tutorial of DRL, including the latest model-free and model-based algorithms. We then cover existing DRL applications for medical imaging, which are roughly divided into three main categories: (I) parametric medical image analysis tasks including landmark detection, object/lesion detection, registration, and view plane localization; (ii) solving optimization tasks including hyperparameter tuning, selecting augmentation strategies, and neural architecture search; and (iii) miscellaneous applications including surgical gesture segmentation, personalized mobile health intervention, and computational model personalization. The paper concludes with discussions of future perspectives.      
### 27.Learning Invariant Representations across Domains and Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05114.pdf)
>  Being expensive and time-consuming to collect massive COVID-19 image samples to train deep classification models, transfer learning is a promising approach by transferring knowledge from the abundant typical pneumonia datasets for COVID-19 image classification. However, negative transfer may deteriorate the performance due to the feature distribution divergence between two datasets and task semantic difference in diagnosing pneumonia and COVID-19 that rely on different characteristics. It is even more challenging when the target dataset has no labels available, i.e., unsupervised task transfer learning. In this paper, we propose a novel Task Adaptation Network (TAN) to solve this unsupervised task transfer problem. In addition to learning transferable features via domain-adversarial training, we propose a novel task semantic adaptor that uses the learning-to-learn strategy to adapt the task semantics. Experiments on three public COVID-19 datasets demonstrate that our proposed method achieves superior performance. Especially on COVID-DA dataset, TAN significantly increases the recall and F1 score by 5.0% and 7.8% compared to recently strong baselines. Moreover, we show that TAN also achieves superior performance on several public domain adaptation benchmarks.      
### 28.CovidGAN: Data Augmentation Using Auxiliary Classifier GAN for Improved Covid-19 Detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.05094.pdf)
>  Coronavirus (COVID-19) is a viral disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The spread of COVID-19 seems to have a detrimental effect on the global economy and health. A positive chest X-ray of infected patients is a crucial step in the battle against COVID-19. Early results suggest that abnormalities exist in chest X-rays of patients suggestive of COVID-19. This has led to the introduction of a variety of deep learning systems and studies have shown that the accuracy of COVID-19 patient detection through the use of chest X-rays is strongly optimistic. Deep learning networks like convolutional neural networks (CNNs) need a substantial amount of training data. Because the outbreak is recent, it is difficult to gather a significant number of radiographic images in such a short time. Therefore, in this research, we present a method to generate synthetic chest X-ray (CXR) images by developing an Auxiliary Classifier Generative Adversarial Network (ACGAN) based model called CovidGAN. In addition, we demonstrate that the synthetic images produced from CovidGAN can be utilized to enhance the performance of CNN for COVID-19 detection. Classification using CNN alone yielded 85% accuracy. By adding synthetic images produced by CovidGAN, the accuracy increased to 95%. We hope this method will speed up COVID-19 detection and lead to more robust systems of radiology.      
### 29.Bayesian Dynamical System Identification With Unified Sparsity Priors And Model Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2103.05090.pdf)
>  This work is concerned with uncertainty quantification in reduced-order dynamical system identification. Reduced-order models for system dynamics are ubiquitous in design and control applications and recent efforts focus on their data-driven construction. Our starting point is the sparse-identification of nonlinear dynamics (SINDy) framework, which reformulates system identification as a regression problem, where unknown functions are approximated from a sparse subset of an underlying library. In this manuscript, we formulate this system identification method in a Bayesian framework to handle parameter and structural model uncertainties. We present a general approach to enforce sparsity, which builds on the recently introduced class of neuronized priors. We perform comparisons between different variants such as Lasso, horseshoe, and spike and slab priors, which are all obtained by modifying a single activation function. We also outline how state observation noise can be incorporated with a probabilistic state-space model. The resulting Bayesian regression framework is robust and simple to implement. We apply the method to two generic numerical applications, the pendulum and the Lorenz system, and one aerodynamic application using experimental measurements.      
### 30.A Parallelizable Lattice Rescoring Strategy with Neural Language Models  [ :arrow_down: ](https://arxiv.org/pdf/2103.05081.pdf)
>  This paper proposes a parallel computation strategy and a posterior-based lattice expansion algorithm for efficient lattice rescoring with neural language models (LMs) for automatic speech recognition. First, lattices from first-pass decoding are expanded by the proposed posterior-based lattice expansion algorithm. Second, each expanded lattice is converted into a minimal list of hypotheses that covers every arc. Each hypothesis is constrained to be the best path for at least one arc it includes. For each lattice, the neural LM scores of the minimal list are computed in parallel and are then integrated back to the lattice in the rescoring stage. Experiments on the Switchboard dataset show that the proposed rescoring strategy obtains comparable recognition performance and generates more compact lattices than a competitive baseline method. Furthermore, the parallel rescoring method offers more flexibility by simplifying the integration of PyTorch-trained neural LMs for lattice rescoring with Kaldi.      
### 31.Cocktail: Learn a Better Neural Network Controller from Multiple Experts via Adaptive Mixing and Robust Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2103.05046.pdf)
>  Neural networks are being increasingly applied to control and decision-making for learning-enabled cyber-physical systems (LE-CPSs). They have shown promising performance without requiring the development of complex physical models; however, their adoption is significantly hindered by the concerns on their safety, robustness, and efficiency. In this work, we propose COCKTAIL, a novel design framework that automatically learns a neural network-based controller from multiple existing control methods (experts) that could be either model-based or neural network-based. In particular, COCKTAIL first performs reinforcement learning to learn an optimal system-level adaptive mixing strategy that incorporates the underlying experts with dynamically-assigned weights and then conducts a teacher-student distillation with probabilistic adversarial training and regularization to synthesize a student neural network controller with improved control robustness (measured by a safe control rate metric with respect to adversarial attacks or measurement noises), control energy efficiency, and verifiability (measured by the computation time for verification). Experiments on three non-linear systems demonstrate significant advantages of our approach on these properties over various baseline methods.      
### 32.On the Hurst Exponent, Markov Processes, and Fractional Brownian Motion  [ :arrow_down: ](https://arxiv.org/pdf/2103.05019.pdf)
>  There is much confusion in the literature over Hurst exponent (H). The purpose of this paper is to illustrate the difference between fractional Brownian motion (fBm) on the one hand and Gaussian Markov processes where H is different to 1/2 on the other. The difference lies in the increments, which are stationary and correlated in one case and nonstationary and uncorrelated in the other. The two- and one-point densities of fBm are constructed explicitly. The two-point density does not scale. The one-point density for a semi-infinite time interval is identical to that for a scaling Gaussian Markov process with H different to 1/2 over a finite time interval. We conclude that both Hurst exponents and one-point densities are inadequate for deducing the underlying dynamics from empirical data. We apply these conclusions in the end to make a focused statement about nonlinear diffusion.      
### 33.Deep learning-based super-resolution fluorescence microscopy on small datasets  [ :arrow_down: ](https://arxiv.org/pdf/2103.04989.pdf)
>  Fluorescence microscopy has enabled a dramatic development in modern biology by visualizing biological organisms with micrometer scale resolution. However, due to the diffraction limit, sub-micron/nanometer features are difficult to resolve. While various super-resolution techniques are developed to achieve nanometer-scale resolution, they often either require expensive optical setup or specialized fluorophores. In recent years, deep learning has shown the potentials to reduce the technical barrier and obtain super-resolution from diffraction-limited images. For accurate results, conventional deep learning techniques require thousands of images as a training dataset. Obtaining large datasets from biological samples is not often feasible due to the photobleaching of fluorophores, phototoxicity, and dynamic processes occurring within the organism. Therefore, achieving deep learning-based super-resolution using small datasets is challenging. We address this limitation with a new convolutional neural network-based approach that is successfully trained with small datasets and achieves super-resolution images. We captured 750 images in total from 15 different field-of-views as the training dataset to demonstrate the technique. In each FOV, a single target image is generated using the super-resolution radial fluctuation method. As expected, this small dataset failed to produce a usable model using traditional super-resolution architecture. However, using the new approach, a network can be trained to achieve super-resolution images from this small dataset. This deep learning model can be applied to other biomedical imaging modalities such as MRI and X-ray imaging, where obtaining large training datasets is challenging.      
### 34.Discrete Function Bases and Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05609.pdf)
>  We discuss the notion of "discrete function bases" with a particular focus on the discrete basis derived from the Legendre Delay Network (LDN). We characterize the performance of these bases in a delay computation task, and as fixed temporal convolutions in neural networks. Networks using fixed temporal convolutions are conceptually simple and yield state-of-the-art results in tasks such as psMNIST. <br>Main Results <br>(1) We present a numerically stable algorithm for constructing a matrix of DLOPs L in O(qN) <br>(2) The Legendre Delay Network (LDN) can be used to form a discrete function basis with a basis transformation matrix H in O(qN). <br>(3) If q &lt; 300, convolving with the LDN basis online has a lower run-time complexity than convolving with arbitrary FIR filters. <br>(4) Sliding window transformations exist for some bases (Haar, cosine, Fourier) and require O(q) operations per sample and O(N) memory. <br>(5) LTI systems similar to the LDN can be constructed for many discrete function bases; the LDN system is superior in terms of having a finite impulse response. <br>(6) We compare discrete function bases by linearly decoding delays from signals represented with respect to these bases. Results are depicted in Figure 20. Overall, decoding errors are similar. The LDN basis has the highest and the Fourier and cosine bases have the smallest errors. <br>(7) The Fourier and cosine bases feature a uniform decoding error for all delays. These bases should be used if the signal can be represented well in the Fourier domain. <br>(8) Neural network experiments suggest that fixed temporal convolutions can outperform learned convolutions. The basis choice is not critical; we roughly observe the same performance trends as in the delay task. <br>(9) The LDN is the right choice for small q, if the O(q) Euler update is feasible, and if the low O(q) memory requirement is of importance.      
### 35.The Laser-Diode Heated Floating Zone Method for Automated Optimal Synthesis of Refractory Oxides and Alloys  [ :arrow_down: ](https://arxiv.org/pdf/2103.05587.pdf)
>  We perform an experimental investigation of the synthesis of complex materials using the Laser-diode heated floating zone method (L-FZ). We will briefly introduce the Infrared-heated floating zone method of bulk crystal growth and then delve into recent advances in using a Laser-diode heated floating zone method. We demonstrate L-FZ for the growth of large high-quality single crystalline samples of of the n=2 Ruddlesden-Popper homologous series $RE_2SrAl_2O_7$, RE = Nd, Sm, Eu, Gd, Tb, Dy with physically interesting properties (e.g spin-ice) as well as the incongruently melting stuffed-tridymite type oxide $BaCoSiO_4$. We conclude with a summary of the results and future research directions in automating crystal growth, which will open access into the synthesis and characterization of previously unstudied class of materials such as refractory complex oxides and alloys.      
### 36.Constrained Contextual Bandit Learning for Adaptive Radar Waveform Selection  [ :arrow_down: ](https://arxiv.org/pdf/2103.05541.pdf)
>  A sequential decision process in which an adaptive radar system repeatedly interacts with a finite-state target channel is studied. The radar is capable of passively sensing the spectrum at regular intervals, which provides side information for the waveform selection process. The radar transmitter uses the sequence of spectrum observations as well as feedback from a collocated receiver to select waveforms which accurately estimate target parameters. It is shown that the waveform selection problem can be effectively addressed using a linear contextual bandit formulation in a manner that is both computationally feasible and sample efficient. Stochastic and adversarial linear contextual bandit models are introduced, allowing the radar to achieve effective performance in broad classes of physical environments. Simulations in a radar-communication coexistence scenario, as well as in an adversarial radar-jammer scenario, demonstrate that the proposed formulation provides a substantial improvement in target detection performance when Thompson Sampling and EXP3 algorithms are used to drive the waveform selection process. Further, it is shown that the harmful impacts of pulse-agile behavior on coherently processed radar data can be mitigated by adopting a time-varying constraint on the radar's waveform catalog.      
### 37.Characterizing Trust and Resilience in Distributed Consensus for Cyberphysical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.05464.pdf)
>  This work considers the problem of resilient consensus where stochastic values of trust between agents are available. Specifically, we derive a unified mathematical framework to characterize convergence, deviation of the consensus from the true consensus value, and expected convergence rate, when there exists additional information of trust between agents. We show that under certain conditions on the stochastic trust values and consensus protocol: 1) almost sure convergence to a common limit value is possible even when malicious agents constitute more than half of the network connectivity, 2) the deviation of the converged limit, from the case where there is no attack, i.e., the true consensus value, can be bounded with probability that approaches 1 exponentially, and 3) correct classification of malicious and legitimate agents can be attained in finite time almost surely. Further, the expected convergence rate decays exponentially with the quality of the trust observations between agents.      
### 38.Multimodal fusion using sparse CCA for breast cancer survival prediction  [ :arrow_down: ](https://arxiv.org/pdf/2103.05432.pdf)
>  Effective understanding of a disease such as cancer requires fusing multiple sources of information captured across physical scales by multimodal data. In this work, we propose a novel feature embedding module that derives from canonical correlation analyses to account for intra-modality and inter-modality correlations. Experiments on simulated and real data demonstrate how our proposed module can learn well-correlated multi-dimensional embeddings. These embeddings perform competitively on one-year survival classification of TCGA-BRCA breast cancer patients, yielding average F1 scores up to 58.69% under 5-fold cross-validation.      
### 39.Automatic Borescope Damage Assessments for Gas Turbine Blades via Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.05430.pdf)
>  To maximise fuel economy, bladed components in aero-engines operate close to material limits. The severe operating environment leads to in-service damage on compressor and turbine blades, having a profound and immediate impact on the performance of the engine. Current methods of blade visual inspection are mainly based on borescope imaging. During these inspections, the sentencing of components under inspection requires significant manual effort, with a lack of systematic approaches to avoid human biases. To perform fast and accurate sentencing, we propose an automatic workflow based on deep learning for detecting damage present on rotor blades using borescope videos. Building upon state-of-the-art methods from computer vision, we show that damage statistics can be presented for each blade in a blade row separately, and demonstrate the workflow on two borescope videos.      
### 40.Decentralized Non-Convex Learning with Linearly Coupled Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2103.05378.pdf)
>  Motivated by the need for decentralized learning, this paper aims at designing a distributed algorithm for solving nonconvex problems with general linear constraints over a multi-agent network. In the considered problem, each agent owns some local information and a local variable for jointly minimizing a cost function, but local variables are coupled by linear constraints. Most of the existing methods for such problems are only applicable for convex problems or problems with specific linear constraints. There still lacks a distributed algorithm for such problems with general linear constraints and under nonconvex setting. In this paper, to tackle this problem, we propose a new algorithm, called "proximal dual consensus" (PDC) algorithm, which combines a proximal technique and a dual consensus method. We build the theoretical convergence conditions and show that the proposed PDC algorithm can converge to an $\epsilon$-Karush-Kuhn-Tucker solution within $\mathcal{O}(1/\epsilon)$ iterations. For computation reduction, the PDC algorithm can choose to perform cheap gradient descent per iteration while preserving the same order of $\mathcal{O}(1/\epsilon)$ iteration complexity. Numerical results are presented to demonstrate the good performance of the proposed algorithms for solving a regression problem and a classification problem over a network where agents have only partial observations of data features.      
### 41.Ultrafast Parallel LiDAR with Time-encoding and Spectral Scanning: Breaking the Time-of-flight Limit  [ :arrow_down: ](https://arxiv.org/pdf/2103.05360.pdf)
>  Light detection and ranging (LiDAR) has been widely used in autonomous driving and large-scale manufacturing. Although state-of-the-art scanning LiDAR can perform long-range three-dimensional imaging, the frame rate is limited by both round-trip delay and the beam steering speed, hindering the development of high-speed autonomous vehicles. For hundred-meter level ranging applications, a several-time speedup is highly desirable. Here, we uniquely combine fiber-based encoders with wavelength-division multiplexing devices to implement all-optical time-encoding on the illumination light. Using this method, parallel detection and fast inertia-free spectral scanning can be achieved simultaneously with single-pixel detection. As a result, the frame rate of a scanning LiDAR can be multiplied with scalability. We demonstrate a 4.4-fold speedup for a maximum 75-m detection range, compared with a time-of-flight-limited laser ranging system. This approach has the potential to improve the velocity of LiDAR-based autonomous vehicles to the regime of hundred kilometers per hour and open up a new paradigm for ultrafast-frame-rate LiDAR imaging.      
### 42.Control Design with Guaranteed Transient Performance: an Approach with Polyhedral Target Tubes  [ :arrow_down: ](https://arxiv.org/pdf/2103.05333.pdf)
>  In this paper a novel approach is presented for control design with guaranteed transient performance for multiple-input multiple-output discrete-time linear polytopic difference inclusions. We establish a theorem that gives necessary and sufficient conditions for the state to evolve from one polyhedral subset of the state-space to another. Then we present an algorithm which constructs a time-varying output feedback law which guarantees that the state evolves within a time-varying polyhedral target-tube specifying the system's desired transient performance. We present generalisations involving constraints on the control, and a bounded additive disturbance term. Our formulation is very general and includes reference tracking with any desired transient behaviour in the face of disturbances, as specified, for example, by the most popular step response specifications. The approach is demonstrated by an example involving the control of water levels in two coupled tanks.      
### 43.Memory-Efficient, Limb Position-Aware Hand Gesture Recognition using Hyperdimensional Computing  [ :arrow_down: ](https://arxiv.org/pdf/2103.05267.pdf)
>  Electromyogram (EMG) pattern recognition can be used to classify hand gestures and movements for human-machine interface and prosthetics applications, but it often faces reliability issues resulting from limb position change. One method to address this is dual-stage classification, in which the limb position is first determined using additional sensors to select between multiple position-specific gesture classifiers. While improving performance, this also increases model complexity and memory footprint, making a dual-stage classifier difficult to implement in a wearable device with limited resources. In this paper, we present sensor fusion of accelerometer and EMG signals using a hyperdimensional computing model to emulate dual-stage classification in a memory-efficient way. We demonstrate two methods of encoding accelerometer features to act as keys for retrieval of position-specific parameters from multiple models stored in superposition. Through validation on a dataset of 13 gestures in 8 limb positions, we obtain a classification accuracy of up to 93.34%, an improvement of 17.79% over using a model trained solely on EMG. We achieve this while only marginally increasing memory footprint over a single limb position model, requiring $8\times$ less memory than a traditional dual-stage classification architecture.      
### 44.GAN Vocoder: Multi-Resolution Discriminator Is All You Need  [ :arrow_down: ](https://arxiv.org/pdf/2103.05236.pdf)
>  Several of the latest GAN-based vocoders show remarkable achievements, outperforming autoregressive and flow-based competitors in both qualitative and quantitative measures while synthesizing orders of magnitude faster. In this work, we hypothesize that the common factor underlying their success is the multi-resolution discriminating framework, not the minute details in architecture, loss function, or training strategy. We experimentally test the hypothesis by evaluating six different generators paired with one shared multi-resolution discriminating framework. For all evaluative measures with respect to text-to-speech syntheses and for all perceptual metrics, their performances are not distinguishable from one another, which supports our hypothesis.      
### 45.Stabilized Medical Image Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05232.pdf)
>  Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.      
### 46.Universal Undersampled MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2103.05214.pdf)
>  Deep neural networks have been extensively studied for undersampled MRI reconstruction. While achieving state-of-the-art performance, they are trained and deployed specifically for one anatomy with limited generalization ability to another anatomy. Rather than building multiple models, a universal model that reconstructs images across different anatomies is highly desirable for efficient deployment and better generalization. Simply mixing images from multiple anatomies for training a single network does not lead to an ideal universal model due to the statistical shift among datasets of various anatomies, the need to retrain from scratch on all datasets with the addition of a new dataset, and the difficulty in dealing with imbalanced sampling when the new dataset is further of a smaller size. In this paper, for the first time, we propose a framework to learn a universal deep neural network for undersampled MRI reconstruction. Specifically, anatomy-specific instance normalization is proposed to compensate for statistical shift and allow easy generalization to new datasets. Moreover, the universal model is trained by distilling knowledge from available independent models to further exploit representations across anatomies. Experimental results show the proposed universal model can reconstruct both brain and knee images with high image quality. Also, it is easy to adapt the trained model to new datasets of smaller size, i.e., abdomen, cardiac and prostate, with little effort and superior performance.      
### 47.Enhancing Medical Image Registration via Appearance Adjustment Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05213.pdf)
>  Deformable image registration is fundamental for many medical image analyses. A key obstacle for accurate image registration is the variations in image appearance. Recently, deep learning-based registration methods (DLRs), using deep neural networks, have computational efficiency that is several orders of magnitude greater than traditional optimization-based registration methods (ORs). A major drawback, however, of DLRs is a disregard for the target-pair-specific optimization that is inherent in ORs and instead they rely on a globally optimized network that is trained with a set of training samples to achieve faster registration. Thus, DLRs inherently have degraded ability to adapt to appearance variations and perform poorly, compared to ORs, when image pairs (fixed/moving images) have large differences in appearance. Hence, we propose an Appearance Adjustment Network (AAN) where we leverage anatomy edges, through an anatomy-constrained loss function, to generate an anatomy-preserving appearance transformation. We designed the AAN so that it can be readily inserted into a wide range of DLRs, to reduce the appearance differences between the fixed and moving images. Our AAN and DLR's network can be trained cooperatively in an unsupervised and end-to-end manner. We evaluated our AAN with two widely used DLRs - Voxelmorph (VM) and FAst IMage registration (FAIM) - on three public 3D brain magnetic resonance (MR) image datasets - IBSR18, Mindboggle101, and LPBA40. The results show that DLRs, using the AAN, improved performance and achieved higher results than state-of-the-art ORs.      
### 48.Continuous body 3-D reconstruction of limbless animals  [ :arrow_down: ](https://arxiv.org/pdf/2103.05198.pdf)
>  Limbless animals such as snakes, limbless lizards, worms, eels, and lampreys move their slender, long bodies in three dimensions to traverse diverse environments. Accurately quantifying their continuous body's 3-D shape and motion is important for understanding body-environment interactions in complex terrain, but this is difficult to achieve (especially for local orientation and rotation). Here, we describe an interpolation method to quantify continuous body 3-D position and orientation. We simplify the body as an elastic rod and apply a backbone optimization method to interpolate continuous body shape between end constraints imposed by tracked markers. Despite over-simplifying the biomechanics, our method achieves a higher interpolation accuracy (~50% error) in both 3-D position and orientation compared with the widely-used cubic B-spline interpolation method. Beyond snakes traversing large obstacles as demonstrated, our method applies to other long, slender, limbless animals and continuum robots. We provide codes and demo files for easy application of our method.      
### 49.Near-zero Downtime Recovery from Transient-error-induced Crashes  [ :arrow_down: ](https://arxiv.org/pdf/2103.05185.pdf)
>  Due to the system scaling, transient errors caused by external noises, e.g., heat fluxes and particle strikes, have become a growing concern for the current and upcoming extreme-scale high-performance-computing (HPC) systems. However, since such errors are still quite rare as compared to no-fault cases, desirable solutions call for low/no-overhead systems that do not compromise the performance under no-fault conditions and also allow very fast fault recovery to minimize downtime. In this paper, we present IterPro, a light-weight compiler-assisted resilience technique to quickly and accurately recover processes from transient-error-induced crashes. IterPro repairs the corrupted process states on-the-fly upon occurrences of errors, enabling applications to continue their executions instead of being terminated. IterPro also exploits side effects introduced by induction variable based code optimization techniques to improve its recovery capability. To this end, two new code transformation passes are introduced to expose the side effects for resilience purposes. We evaluated IterPro with 4 scientific workloads as well as the NPB benchmarks suite. During their normal execution, IterPro incurs almost zero runtime overhead and a small, fixed 27MB memory overhead. Meanwhile, IterPro can recover on an average 83.55% of crash-causing errors within dozens of milliseconds with negligible downtime. With such an effective recovery mechanism, IterPro could tremendously mitigate the overheads and resource requirements of the resilience subsystem in future extreme-scale systems.      
### 50.Deep Learning-based High-precision Depth Map Estimation from Missing Viewpoints for 360 Degree Digital Holography  [ :arrow_down: ](https://arxiv.org/pdf/2103.05158.pdf)
>  In this paper, we propose a novel, convolutional neural network model to extract highly precise depth maps from missing viewpoints, especially well applicable to generate holographic 3D contents. The depth map is an essential element for phase extraction which is required for synthesis of computer-generated hologram (CGH). The proposed model called the HDD Net uses MSE for the better performance of depth map estimation as loss function, and utilizes the bilinear interpolation in up sampling layer with the Relu as activation function. We design and prepare a total of 8,192 multi-view images, each resolution of 640 by 360 for the deep learning study. The proposed model estimates depth maps through extracting features, up sampling. For quantitative assessment, we compare the estimated depth maps with the ground truths by using the PSNR, ACC, and RMSE. We also compare the CGH patterns made from estimated depth maps with ones made from ground truths. Furthermore, we demonstrate the experimental results to test the quality of estimated depth maps through directly reconstructing holographic 3D image scenes from the CGHs.      
### 51.Deep Transfer Learning for WiFi Localization  [ :arrow_down: ](https://arxiv.org/pdf/2103.05123.pdf)
>  This paper studies a WiFi indoor localisation technique based on using a deep learning model and its transfer strategies. We take CSI packets collected via the WiFi standard channel sounding as the training dataset and verify the CNN model on the subsets collected in three experimental environments. We achieve a localisation accuracy of 46.55 cm in an ideal $(6.5m \times 2.5m)$ office with no obstacles, 58.30 cm in an office with obstacles, and 102.8 cm in a sports hall $(40 \times 35m)$. Then, we evaluate the transfer ability of the proposed model to different environments. The experimental results show that, for a trained localisation model, feature extraction layers can be directly transferred to other models and only the fully connected layers need to be retrained to achieve the same baseline accuracy with non-transferred base models. This can save 60% of the training parameters and reduce the training time by more than half. Finally, an ablation study of the training dataset shows that, in both office and sport hall scenarios, after reusing the feature extraction layers of the base model, only 55% of the training data is required to obtain the models' accuracy similar to the base models.      
### 52.Highly Efficient Representation and Active Learning Framework for Imbalanced Data and its Application to COVID-19 X-Ray Classification  [ :arrow_down: ](https://arxiv.org/pdf/2103.05109.pdf)
>  We propose a highly data-efficient classification and active learning framework for classifying chest X-rays. It is based on (1) unsupervised representation learning of a CNN (Convolutional Neural Network) and (2) the GP (Gaussian Process) method. The unsupervised representation learning employs self-supervision that does not require class labels, and the learned features are proven to achieve label-efficient classification. GP is a kernel-based Bayesian approach that also leads to data-efficient predictions with the added benefit of estimating each decision's uncertainty. Our novel framework combines these two elements in sequence to achieve highly data and label efficient classifications. Moreover, both elements are less sensitive to the prevalent and challenging class imbalance issue, thanks to the (1) feature learned without labels and (2) the Bayesian nature of GP. The GP-provided uncertainty estimates enable active learning by ranking samples based on the uncertainty and selectively labeling samples showing higher uncertainty. We apply this novel combination to the data-deficient and severely imbalanced case of COVID-19 chest X-ray classification. We demonstrate that only $\sim 10\%$ of the labeled data is needed to reach the accuracy from training all available labels. Its application to the COVID-19 data in a fully supervised classification scenario shows that our model, with a generic ResNet backbone, outperforms (COVID-19 case by 4\%) the state-of-the-art model with a highly tuned architecture. Our model architecture and proposed framework are general and straightforward to apply to a broader class of datasets, with expected success.      
### 53.Self-supervised Multisensor Change Detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.05102.pdf)
>  Multimodal and multisensor data analysis is a long-standing goal in machine learning research. In this paper we revisit multisensor analysis in context of self-supervised change detection in bi-temporal satellite images. Most change detection methods assume that pre-change and post-change images are acquired by the same sensor. However, in many real-life scenarios, e.g., natural disaster, it is more practical to use the latest available images before and after the occurrence of incidence, which may be acquired using different sensors. In particular, we are interested in the combination of the images acquired by optical and Synthetic Aperture Radar (SAR) sensors. While optical images are like the natural images dealt in computer vision, SAR images appear vastly different even when capturing the same scene. Adding to this, change detection methods are often constrained to use only target image-pair, no labeled data, and no additional unlabeled data. Such constraints limit the scope of traditional supervised machine learning and unsupervised generative approaches for multi-sensor change detection. Recent rapid development of self-supervised learning methods has shown that some of them can even work with only few images. Motivated by this, in this work we propose a method for multi-sensor change detection using only the unlabeled target bi-temporal images that are used for training a network in self-supervised fashion by using deep clustering and contrastive learning. The trained network is evaluated on multi-modal satellite data showing change and the benefits of our self-supervised approach are demonstrated.      
### 54.Subjective and Objective Quality Assessment of Mobile Gaming Video  [ :arrow_down: ](https://arxiv.org/pdf/2103.05099.pdf)
>  Nowadays, with the vigorous expansion and development of gaming video streaming techniques and services, the expectation of users, especially the mobile phone users, for higher quality of experience is also growing swiftly. As most of the existing research focuses on traditional video streaming, there is a clear lack of both subjective study and objective quality models that are tailored for quality assessment of mobile gaming content. To this end, in this study, we first present a brand new Tencent Gaming Video dataset containing 1293 mobile gaming sequences encoded with three different codecs. Second, we propose an objective quality framework, namely Efficient hard-RAnk Quality Estimator (ERAQUE), that is equipped with (1) a novel hard pairwise ranking loss, which forces the model to put more emphasis on differentiating similar pairs; (2) an adapted model distillation strategy, which could be utilized to compress the proposed model efficiently without causing significant performance drop. Extensive experiments demonstrate the efficiency and robustness of our model.      
### 55.Uncorrelated binary sequences of lengths 2a3b4c5d7e11f13g based on nested Barker codes and complementary sequences  [ :arrow_down: ](https://arxiv.org/pdf/2103.05042.pdf)
>  Certain applications require the use of signals that combine both the capability to operate with low signal-to-noise ratios and the ability to support multiple users without interference. In the case where many users have very different signal-to-noise ratios, it is necessary to consider coding schemes that can be used in a multi-user environment but with different noise immunity levels. Traditional detection systems based on the correlation function and coding sequences have significant limitations in satisfying both objectives, since the cross-correlation between coded signals corresponding with different users is linked to the use of the same coded sequences length. The research topic of binary sequences that have null cross-correlation and different length has not been studied in depth, but it has potential applications in multi-user environments. In this work an algorithm to generate binary sequences completely uncorrelated with certain sets of complementary sequences is presented. The proposed algorithm is based on nested Barker sequences, and it is compared with a previous proposal based on an iterative algorithm. This approach allows to generate more diversity of sequences of different length than the iterative approach, which it makes useful for applications based on binary sequences detection and expand the horizon of many applications.      
### 56.An Efficient Quadratic Programming Relaxation Based Algorithm for Large-Scale MIMO Detection  [ :arrow_down: ](https://arxiv.org/pdf/2006.12123.pdf)
>  Multiple-input multiple-output (MIMO) detection is a fundamental problem in wireless communications and it is strongly NP-hard in general. Massive MIMO has been recognized as a key technology in the fifth generation (5G) and beyond communication networks, which on one hand can significantly improve the communication performance, and on the other hand poses new challenges of solving the corresponding optimization problems due to the large problem size. While various efficient algorithms such as semidefinite relaxation (SDR) based approaches have been proposed for solving the small-scale MIMO detection problem, they are not suitable to solve the large-scale MIMO detection problem due to their high computational complexities. In this paper, we propose an efficient sparse quadratic programming (SQP) relaxation based algorithm for solving the large-scale MIMO detection problem. In particular, we first reformulate the MIMO detection problem as an SQP problem. By dropping the sparse constraint, the resulting relaxation problem shares the same global minimizer with the SQP problem. In sharp contrast to the SDRs for the MIMO detection problem, our relaxation does not contain any (positive semidefinite) matrix variable and the numbers of variables and constraints in our relaxation are significantly less than those in the SDRs, which makes it particularly suitable for the large-scale problem. Then we propose a projected Newton based quadratic penalty method to solve the relaxation problem, which is guaranteed to converge to the vector of transmitted signals under reasonable conditions. By extensive numerical experiments, when applied to solve large-scale problems, the proposed algorithm achieves better detection performance than a recently proposed generalized power method.      
