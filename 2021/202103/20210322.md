# ArXiv eess --Mon, 22 Mar 2021
### 1.A Unified Framework for IRS Enabled Wireless Powered Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.10903.pdf)
>  This paper unveils the importance of intelligent reflecting surface (IRS) in a wireless powered sensor network (WPSN). Specifically, a multi-antenna power station (PS) employs energy beamforming to provide wireless charging for multiple Internet of Things (IoT) devices, which utilize the harvested energy to deliver their own messages to an access point (AP). Meanwhile, an IRS is deployed to enhance the performances of wireless energy transfer (WET) and wireless information transfer (WIT) by intelligently adjusting the phase shift of each reflecting element. To evaluate the performance of this IRS assisted WPSN, we are interested in maximizing its system sum throughput to jointly optimize the energy beamforming of the PS, the transmission time allocation, as well as the phase shifts of the WET and WIT phases. The formulated problem is not jointly convex due to the multiple coupled variables. To deal with its non-convexity, we first independently find the phase shifts of the WIT phase in closed-form. We further propose an alternating optimization (AO) algorithm to iteratively solve the sum throughput maximization problem. To be specific, a semidefinite programming (SDP) relaxation approach is adopted to design the energy beamforming and the time allocation for given phase shifts of WET phase, which is then optimized for given energy beamforming and time allocation. Moreover, we propose an AO low-complexity scheme to significantly reduce the computational complexity incurred by the SDP relaxation, where the optimal closed-form energy beamforming, time allocation, and phase shifts of the WET phase are derived. Finally, numerical results are demonstrated to validate the effectiveness of the proposed algorithm, and highlight the beneficial role of the IRS in comparison to the benchmark schemes.      
### 2.Deep Label Fusion: A 3D End-to-End Hybrid Multi-Atlas Segmentation and Deep Learning Pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2103.10892.pdf)
>  Deep learning (DL) is the state-of-the-art methodology in various medical image segmentation tasks. However, it requires relatively large amounts of manually labeled training data, which may be infeasible to generate in some applications. In addition, DL methods have relatively poor generalizability to out-of-sample data. Multi-atlas segmentation (MAS), on the other hand, has promising performance using limited amounts of training data and good generalizability. A hybrid method that integrates the high accuracy of DL and good generalizability of MAS is highly desired and could play an important role in segmentation problems where manually labeled data is hard to generate. Most of the prior work focuses on improving single components of MAS using DL rather than directly optimizing the final segmentation accuracy via an end-to-end pipeline. Only one study explored this idea in binary segmentation of 2D images, but it remains unknown whether it generalizes well to multi-class 3D segmentation problems. In this study, we propose a 3D end-to-end hybrid pipeline, named deep label fusion (DLF), that takes advantage of the strengths of MAS and DL. Experimental results demonstrate that DLF yields significant improvements over conventional label fusion methods and U-Net, a direct DL approach, in the context of segmenting medial temporal lobe subregions using 3T T1-weighted and T2-weighted MRI. Further, when applied to an unseen similar dataset acquired in 7T, DLF maintains its superior performance, which demonstrates its good generalizability.      
### 3.Feedback from Pixels: Output Regulation via Learning-based Scene View Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2103.10888.pdf)
>  We propose a novel controller synthesis involving feedback from pixels, whereby the measurement is a high dimensional signal representing a pixelated image with Red-Green-Blue (RGB) values. The approach neither requires feature extraction, nor object detection, nor visual correspondence. The control policy does not involve the estimation of states or similar latent representations. Instead, tracking is achieved directly in image space, with a model of the reference signal embedded as required by the internal model principle. The reference signal is generated by a neural network with learning-based scene view synthesis capabilities. Our approach does not require an end-to-end learning of a pixel-to-action control policy. The approach is applied to a motion control problem, namely the longitudinal dynamics of a car-following problem. We show how this approach lend itself to a tractable stability analysis with associated bounds critical to establishing trustworthiness and interpretability of the closed-loop dynamics.      
### 4.Supervisory Control of Multi-Agent Discrete-Event Systems with Partial Observation  [ :arrow_down: ](https://arxiv.org/pdf/2103.10877.pdf)
>  In this paper we investigate multi-agent discrete-event systems with partial observation. The agents can be divided into several groups in each of which the agents have similar (isomorphic) state transition structures, and thus can be relabeled into the same template. Based on the template a scalable supervisor whose state size and computational cost are independent of the number of agents is designed for the case of partial observation. The scalable supervisor under partial observation does not need to be recomputed regardless of how many agents are added to or removed from the system. We generalize our earlier results to partial observation by proposing sufficient conditions for safety and maximal permissiveness of the scalable least restrictive supervisor on the template level. An example is provided to illustrate the proposed scalable supervisory synthesis.      
### 5.Direct Symbol Decoding using GA-SVM in Chaotic Baseband Wireless Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2103.10855.pdf)
>  To retrieve the information from the serious distorted received signal is the key challenge of communication signal processing. The chaotic baseband communication promises theoretically to eliminate the inter-symbol interference (ISI), however, it needs complicated calculation, if it is not impossible. In this paper, a genetic algorithm support vector machine (GA-SVM) based symbol detection method is proposed for chaotic baseband wireless communication system (CBWCS), by this way, treating the problem from a different viewpoint, the symbol decoding process is converted to be a binary classification through GA-SVM model. A trained GA-SVM model is used to decode the symbols directly at the receiver, so as to improve the bit error rate (BER) performance of the CBWCS and simplify the symbol detection process by removing the channel identification and the threshold calculation process as compared to that using the calculated threshold to decode symbol in the traditional methods. The simulation results show that the proposed method has better BER performance in both the static and time-varying wireless channels. The experimental results, based on the wireless open-access research platform, indicate that the BER of the proposed GA-SVM based symbol detection approach is superior to the other counterparts under a practical wireless multipath channel.      
### 6.Privacy-Aware Load Ensemble Control: A Linearly-Solvable MDP Approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.10828.pdf)
>  Demand response (DR) programs engage distributed demand-side resources, e.g., controllable residential and commercial loads, in providing ancillary services for electric power systems. Ensembles of these resources can help reducing system load peaks and meeting operational limits by adjusting their electric power consumption. To equip utilities or load aggregators with adequate decision-support tools for ensemble dispatch, we develop a Markov Decision Process (MDP) approach to optimally control load ensembles in a privacy-preserving manner. To this end, the concept of differential privacy is internalized into the MDP routine to protect transition probabilities and, thus, privacy of DR participants. The proposed approach also provides a trade-off between solution optimality and privacy guarantees, and is analyzed using real-world data from DR events in the New York University microgrid in New York, NY.      
### 7.Joint RFI Mitigation and Radar Echo Recovery for One-Bit UWB Radar  [ :arrow_down: ](https://arxiv.org/pdf/2103.10827.pdf)
>  Radio frequency interference (RFI) mitigation and radar echo recovery are critically important for the proper functioning of ultra-wideband (UWB) radar systems using one-bit sampling techniques. We recently introduced a technique for one-bit UWB radar, which first uses a majorization-minimization method for RFI parameter estimation followed by a sparse method for radar echo recovery. However, this technique suffers from high computational complexity due to the need to estimate the parameters of each RFI source separately and iteratively. In this paper, we present a computationally efficient joint RFI mitigation and radar echo recovery framework to greatly reduce the computational cost. Specifically, we exploit the sparsity of RFI in the fast-frequency domain and the sparsity of radar echoes in the fast-time domain to design a one-bit weighted SPICE (SParse Iterative Covariance-based Estimation) based framework for the joint RFI mitigation and radar echo recovery of one-bit UWB radar. Both simulated and experimental results are presented to show that the proposed one-bit weighted SPICE framework can not only reduce the computational cost but also outperform the existing approach for decoupled RFI mitigation and radar echo recovery of one-bit UWB radar.      
### 8.Variational Knowledge Distillation for Disease Classification in Chest X-Rays  [ :arrow_down: ](https://arxiv.org/pdf/2103.10825.pdf)
>  Disease classification relying solely on imaging data attracts great interest in medical image analysis. Current models could be further improved, however, by also employing Electronic Health Records (EHRs), which contain rich information on patients and findings from clinicians. It is challenging to incorporate this information into disease classification due to the high reliance on clinician input in EHRs, limiting the possibility for automated diagnosis. In this paper, we propose \textit{variational knowledge distillation} (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that leverages knowledge from EHRs. Specifically, we introduce a conditional latent variable model, where we infer the latent representation of the X-ray image with the variational posterior conditioning on the associated EHR text. By doing so, the model acquires the ability to extract the visual features relevant to the disease during learning and can therefore perform more accurate classification for unseen patients at inference based solely on their X-ray scans. We demonstrate the effectiveness of our method on three public benchmark datasets with paired X-ray images and EHRs. The results show that the proposed variational knowledge distillation can consistently improve the performance of medical image classification and significantly surpasses current methods.      
### 9.Non-causal regularized least-squares for continuous-time system identification with band-limited input excitations  [ :arrow_down: ](https://arxiv.org/pdf/2103.10820.pdf)
>  In continuous-time system identification, the intersample behavior of the input signal is known to play a crucial role in the performance of estimation methods. One common input behavior assumption is that the spectrum of the input is band-limited. The sinc interpolation property of these input signals yields equivalent discrete-time representations that are non-causal. This observation, often overlooked in the literature, is exploited in this work to study non-parametric frequency response estimators of linear continuous-time systems. We study the properties of non-causal least-square estimators for continuous-time system identification, and propose a kernel-based non-causal regularized least-squares approach for estimating the band-limited equivalent impulse response. The proposed methods are tested via extensive numerical simulations.      
### 10.Incremental Stability and Performance Analysis of Discrete-Time Nonlinear Systems using the LPV Framework  [ :arrow_down: ](https://arxiv.org/pdf/2103.10819.pdf)
>  The dissipativity framework is widely used to analyze stability and performance of nonlinear systems. By embedding nonlinear systems in an LPV representation, the convex tools of LPV framework can be applied to nonlinear systems for convex dissipativity based analysis and controller synthesis. However, as has been shown recently in literature, naive application of these tools to nonlinear systems for analysis and controller synthesis can fail to provide the desired guarantees. Namely, only performance and stability with respect to the origin is guaranteed. In this paper, inspired by the results for continuous time nonlinear systems, the notion of incremental dissipativity for discrete time systems is proposed, whereby stability and performance analysis is done between trajectories. Furthermore, it is shown how, through the use of the LPV framework, convex conditions can be obtained for incremental dissipativity analysis of discrete time nonlinear systems. The developed concepts and tools are demonstrated by analyzing incremental dissipativity of a controlled unbalanced disk system.      
### 11.Motion Estimation for Optical Coherence Elastography Using Signal Phase and Intensity  [ :arrow_down: ](https://arxiv.org/pdf/2103.10784.pdf)
>  Displacement estimation in optical coherence tomography (OCT) imaging is relevant for several potential applications, e.g. for optical coherence elastography (OCE) for corneal biomechanical characterization. Larger displacements may be resolved using correlation-based block matching techniques, which however are prone to signal de-correlation and imprecise at commonly desired sub-pixel resolutions. Phase-based tracking methods can estimate tiny sub-wavelength motion, but are not suitable for motion magnitudes larger than half the wavelength due to phase wrapping and the difficulty of any unwrapping due to noise. In this paper a robust OCT displacement estimation method is introduced by formulating tracking as an optimization problem that jointly penalizes intensity disparity, phase difference, and motion discontinuity. This is then solved using yynamic programming, utilizing both sub-wavelength-scale phase and pixel-scale intensity information from OCT imaging, while inherently seeking for the number of phase wraps. This allows for effectively tracking axial and lateral displacements, respectively, with sub-wavelength and pixel scale resolution. Results with tissue mimicking phantoms show that our proposed approach substantially outperforms conventional methods in terms of axial tracking precision, in particular for displacements exceeding half the imaging wavelength.      
### 12.PMBM filter with partially grid-based birth model with applications in sensor management  [ :arrow_down: ](https://arxiv.org/pdf/2103.10775.pdf)
>  This paper introduces a Poisson multi-Bernoulli mixture (PMBM) filter in which the intensities of target birth and undetected targets are grid-based. A simplified version of the Rao-Blackwellized point mass filter is used to predict the intensity of undetected targets, and the density of targets detected for the first time are approximated as Gaussian. Whereas conventional PMBM filter implementations typically use Gaussian mixtures to model the intensity of undetected targets, the proposed representation allows the intensity to vary over the region of interest with sharp edges around the sensor's field of view, without using a large number of Gaussian mixture components. This reduces the computational complexity compared to the conventional approach. The proposed method is illustrated in a sensor management setting where trajectories of sensors with limited fields of view are controlled to search for and track the targets in a region of interest.      
### 13.New Computational Techniques for a Faster Variation of BM3D Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2103.10765.pdf)
>  BM3D has been considered the standard for comparison in the image denoising literature for the last decade. Though it has been shown to be surpassed numerous times by alternative algorithms in terms of PSNR, the margins are very thin, and denoising is approaching a limiting point. The reason for the continued use of BM3D within the literature is due to its off-the-shelf ease-of-use in any application, which alternative improved denoising algorithms sometimes fail to match. This article proposes a new variation of BM3D, which maintains its ease of use but is notably faster. This development brings us closer to real-time ease-of-use application of new state-of-the-art image reconstruction algorithms such as plug-and-play priors. <br>We refer to our variation of BM3D as G-BM3D. In terms of image quality, our algorithm attains very similar denoising performance to the original algorithm. Though our algorithm is written completely in MATLAB software, it is already between 5-20 times faster than the original algorithm, and the modifications to the algorithm are such that it is expected to be significantly faster when ported to CUDA language and with more powerful GPUs. The improved processing time is achieved by two main components. The first component is a new computational strategy that achieves faster block matching, and the second is a new global approach to the 3D wavelet filtering step that allows for significantly improved processing times on GPUs. The fast block matching strategy could also be applied to any of the vast number of nonlocal self-similarity (NSS) denoisers to improve processing times.      
### 14.Spatial response identification enables robust experimental ultrasound computed tomography  [ :arrow_down: ](https://arxiv.org/pdf/2103.10722.pdf)
>  Ultrasound computed tomography techniques have the potential to provide clinicians with 3D, quantitative and high-resolution information of both soft and hard tissues such as the breast or the adult human brain. Their experimental application requires accurate modelling of the acquisition setup: the spatial location, orientation and impulse response of each ultrasound transducer. However, existing calibration methods fail to successfully characterise these transducers unless their size can be considered negligible when compared to the relevant wavelength, reducing signal-to-noise ratios below usable levels in the presence high-contrast tissues such as the skull. Consequently, we introduce here a methodology that can simultaneously estimate the location, orientation and impulse response of the ultrasound transducers in a single calibration. We do this by extending spatial response identification, an algorithm that we have recently proposed to estimate transducer impulse responses. Our proposed methodology replaces the transducers in the acquisition device with a surrogate model whose effective response matches the experimental data by fitting a numerical model of wave propagation. This results in a flexible and robust calibration that can accurately predict the behaviour of the ultrasound acquisition device without ever having to know where the real transducers are or their individual impulse response. Experimental results using a ring acquisition system show that spatial response identification produces higher calibration performance than standard methodologies across all transducers, both in transmission and in reception. Furthermore, experimental full-waveform inversion reconstructions of a tissue-mimicking phantom demonstrate that spatial response identification generates more accurate reconstructions than those produced with standard calibration techniques.      
### 15.Robust State Estimation and Integrity Monitoring within Multi-Sensor Navigation System  [ :arrow_down: ](https://arxiv.org/pdf/2103.10696.pdf)
>  In autonomous applications, GNSS aided INS utilizing an EKF is the most widely investigated solution for high-rate and high-accurate vehicle states estimation. However, such navigation system suffers from poor parameterization, environment disturbances, human error, or even software and hardware failures under worst-case scenarios. In this paper, a novel scheme of multi-sensor navigation system is proposed, contributing to following research questions: 1) How to provide a reliable state estimation under minor system aberrations, i.e. improve the robustness of navigation system against e.g. inappropriate parameterization or environment disturbances; 2) How to provide system integrity against worst-case scenarios, i.e. significant system aberrations or even failures. The proposed scheme involves EHF for robustness enhancement, zonotope for protection level generation of the navigation solution and vehicle dynamic model aided fault detection of the inertial sensor. The designed approach is validated using the recorded data from an experimental platform called 'IRT-Buggy', which is an electrical land vehicle. The results show that the proposed scheme provides reliable integrity monitoring and accurate state estimation, under both real-world and artificial abnormalities and shows significant advantages against conventional 'GNSS+INS+EKF' approach.      
### 16.Model-based Reconstruction for Single Particle Cryo-Electron Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2103.10630.pdf)
>  Single particle cryo-electron microscopy is a vital tool for 3D characterization of protein structures. A typical workflow involves acquiring projection images of a collection of randomly oriented particles, picking and classifying individual particle projections by orientation, and finally using the individual particle projections to reconstruct a 3D map of the electron density profile. The reconstruction is challenging because of the low signal-to-noise ratio of the data, the unknown orientation of the particles, and the sparsity of data especially when dealing with flexible proteins where there may not be sufficient data corresponding to each class to obtain an accurate reconstruction using standard algorithms. In this paper we present a model-based image reconstruction technique that uses a regularized cost function to reconstruct the 3D density map by assuming known orientations for the particles. Our method casts the reconstruction as minimizing a cost function involving a novel forward model term that accounts for the contrast transfer function of the microscope, the orientation of the particles and the center of rotation offsets. We combine the forward model term with a regularizer that enforces desirable properties in the volume to be reconstructed. Using simulated data, we demonstrate how our method can significantly improve upon the typically used approach.      
### 17.Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2103.10626.pdf)
>  In recent years, the availability of digitized Whole Slide Images (WSIs) has enabled the use of deep learning-based computer vision techniques for automated disease diagnosis. However, WSIs present unique computational and algorithmic challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them infeasible to be used directly for training deep neural networks. Also, often only slide-level labels are available for training as detailed annotations are tedious and can be time-consuming for experts. Approaches using multiple-instance learning (MIL) frameworks have been shown to overcome these challenges. Current state-of-the-art approaches divide the learning framework into two decoupled parts: a convolutional neural network (CNN) for encoding the patches followed by an independent aggregation approach for slide-level prediction. In this approach, the aggregation step has no bearing on the representations learned by the CNN encoder. We have proposed an end-to-end framework that clusters the patches from a WSI into ${k}$-groups, samples ${k}'$ patches from each group for training, and uses an adaptive attention mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have demonstrated that dividing a WSI into clusters can improve the model training by exposing it to diverse discriminative features extracted from the patches. We regularized the clustering mechanism by introducing a KL-divergence loss between the attention weights of patches in a cluster and the uniform distribution. The framework is optimized end-to-end on slide-level cross-entropy, patch-level cross-entropy, and KL-divergence loss (Implementation: <a class="link-external link-https" href="https://github.com/YashSharma/C2C" rel="external noopener nofollow">this https URL</a>).      
### 18.On the Value of Preview Information For Safety Control  [ :arrow_down: ](https://arxiv.org/pdf/2103.10625.pdf)
>  Incorporating predictions of external inputs, which can otherwise be treated as disturbances, has been widely studied in control and computer science communities. These predictions are commonly referred to as preview in optimal control and lookahead in temporal logic synthesis. However, little work has been done for analyzing the value of preview information for safety control for systems with continuous state spaces. In this work, we start from showing general properties for discrete-time nonlinear systems with preview and strategies on how to determine a good preview time, and then we study a special class of linear systems, called systems in Brunovsky canonical form, and show special properties for this class of systems. In the end, we provide two numerical examples to further illustrate the value of preview in safety control.      
### 19.Model-based Reconstruction for Enhanced X-ray CT of Tri-structural Isotropic (TRISO) Particles  [ :arrow_down: ](https://arxiv.org/pdf/2103.10624.pdf)
>  Tri-Structural Isotropic (TRISO) fuel particles are a key component of next generation nuclear fuels. Using X-ray computed tomography (CT) to characterize TRISO particles is challenging because of the strong attenuation of the X-ray beam by the uranium core leading to severe photon starvation in a substantial fraction of the measurements. Furthermore, the overall acquisition time for a high-resolution CT scan can be very long when using conventional lab-based X-ray systems and reconstruction algorithms. Specifically, when analytic methods like the Feldkamp-Davis-Kress (FDK) algorithm is used for reconstruction, it results in severe streaks artifacts and noise in the corresponding 3D volume which make subsequent analysis of the particles challenging. In this article, we develop and apply model-based image reconstruction (MBIR) algorithms for improving the quality of CT reconstructions for TRISO particles in order to facilitate better characterization. We demonstrate that the proposed MBIR algorithms can significantly suppress artifacts with minimal pre-processing compared to the conventional approaches. Furthermore, we demonstrate the proposed MBIR approach can obtain high-quality reconstruction compared to the FDK approach even when using a fraction of the typically acquired measurements, thereby enabling dramatically faster measurement times for TRISO particles.      
### 20.EMS and DMS Integration of the Coordinative Real-time Sub-Transmission Volt-Var Control Tool under High DER Penetration  [ :arrow_down: ](https://arxiv.org/pdf/2103.10511.pdf)
>  This paper proposes an applicable approach to deploy the Coordinative Real-time Sub-Transmission Volt-Var Control Tool (CReST-VCT), and a holistic system integration framework considering both the energy management system (EMS) and distribution system management system (DMS). This provides an architectural basis and can serve as the implementation guideline of CReST-VCT and other advanced grid support tools, to co-optimize the operation benefits of distributed energy resources (DERs) and assets in both transmission and distribution networks. Potential communication protocols for different physical domains of a real application is included. Performance and security issues are also discussed, along with specific considerations for field deployment. Finally, the paper presents a viable pathway for CReST-VCT and other advanced grid support tools to be integrated in an open-source standardized-based platform that supports distribution utilities.      
### 21.UNETR: Transformers for 3D Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.10504.pdf)
>  Fully Convolutional Neural Networks (FCNNs) with contracting and expansive paths (e.g. encoder and decoder) have shown prominence in various medical image segmentation applications during the recent years. In these architectures, the encoder plays an integral role by learning global contextual representations which will be further utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers , as the main building block of FCNNs limits the capability of learning long-range spatial dependencies in such networks. Inspired by the recent success of transformers in Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. In particular, we introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a pure transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have extensively validated the performance of our proposed model across different imaging modalities(i.e. MR and CT) on volumetric brain tumour and spleen segmentation tasks using the Medical Segmentation Decathlon (MSD) dataset, and our results consistently demonstrate favorable benchmarks.      
### 22.Quantisation Scale-Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2103.10491.pdf)
>  Recently, sparsification scale-spaces have been obtained as a sequence of inpainted images by gradually removing known image data. Thus, these scale-spaces rely on spatial sparsity. In the present paper, we show that sparsification of the co-domain, the set of admissible grey values, also constitutes scale-spaces with induced hierarchical quantisation techniques. These quantisation scale-spaces are closely tied to information theoretical measures for coding cost, and therefore particularly interesting for inpainting-based compression. Based on this observation, we propose a sparsification algorithm for the grey-value domain that outperforms uniform quantisation as well as classical clustering approaches.      
### 23.IA Planner: Motion Planning Using Instantaneous Analysis for Autonomous Vehicle in the Dense Dynamic Scenarios on Highways  [ :arrow_down: ](https://arxiv.org/pdf/2103.10909.pdf)
>  In dense and dynamic scenarios, planning a safe and comfortable trajectory is full of challenges when traffic participants are driving at high speed. The classic graph search and sampling methods first perform path planning and then configure the corresponding speed, which lacks a strategy to deal with the high-speed obstacles. Decoupling optimization methods perform motion planning in the S-L and S-T domains respectively. These methods require a large free configuration space to plan the lane change trajectory. In dense dynamic scenes, it is easy to cause the failure of trajectory planning and be cut in by others, causing slow driving speed and bring safety hazards. We analyze the collision relationship in the spatio-temporal domain, and propose an instantaneous analysis model which only analyzes the collision relationship at the same time. In the model, the collision-free constraints in 3D spatio-temporal domain is projected to the 2D space domain to remove redundant constraints and reduce computational complexity. Experimental results show that our method can plan a safe and comfortable lane-changing trajectory in dense dynamic scenarios. At the same time, it improves traffic efficiency and increases ride comfort.      
### 24.Prospective Identification of Ictal Electroencephalogram  [ :arrow_down: ](https://arxiv.org/pdf/2103.10900.pdf)
>  A vast majority of epileptic seizure (ictal) detection on electroencephalogram (EEG) data has been retrospective. Therefore, even though some may include many patients and extensive evaluation benchmarking, they all share a heavy reliance on labelled data. This is perhaps the most significant obstacle against the utility of seizure detection systems in clinical settings. In this paper, we present a prospective automatic ictal detection and labelling performed at the level of a human expert (arbiter) and reduces labelling time by more than an order of magnitude. Accurate seizure detection and labelling are still a time-consuming and cumbersome task in epilepsy monitoring units (EMUs) and epilepsy centres, particularly in countries with limited facilities and insufficiently trained human resources. This work implements a convolutional long short-term memory (ConvLSTM) network that is pre-trained and tested on Temple University Hospital (TUH) EEG corpus. It is then deployed prospectively at the Comprehensive Epilepsy Service at the Royal Prince Alfred Hospital (RPAH) in Sydney, Australia, testing nearly 14,590 hours of EEG data across nine years. Our system prospectively labelled RPAH epilepsy ward data and subsequently reviewed by two neurologists and three certified EEG specialists. Our clinical result shows the proposed method achieves a 92.19% detection rate for an average time of 7.62 mins per 24 hrs of recorded 18-channel EEG. A human expert usually requires about 2 hrs of reviewing and labelling per any 24 hrs of recorded EEG and is often assisted by a wide range of auxiliary data such as patient, carer, or nurse inputs. In this prospective analysis, we consider humans' role as an expert arbiter who confirms to reject each alarm raised by our system. We achieved an average of 56 false alarms per 24 hrs.      
### 25.Fully Onboard AI-powered Human-Drone Pose Estimation on Ultra-low Power Autonomous Flying Nano-UAVs  [ :arrow_down: ](https://arxiv.org/pdf/2103.10873.pdf)
>  Artificial intelligence-powered pocket-sized air robots have the potential to revolutionize the Internet-of-Things ecosystem, acting as autonomous, unobtrusive, and ubiquitous smart sensors. With a few cm$^{2}$ form-factor, nano-sized unmanned aerial vehicles (UAVs) are the natural befit for indoor human-drone interaction missions, as the pose estimation task we address in this work. However, this scenario is challenged by the nano-UAVs' limited payload and computational power that severely relegates the onboard brain to the sub-100 mW microcontroller unit-class. Our work stands at the intersection of the novel parallel ultra-low-power (PULP) architectural paradigm and our general development methodology for deep neural network (DNN) visual pipelines, i.e., covering from perception to control. Addressing the DNN model design, from training and dataset augmentation to 8-bit quantization and deployment, we demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for the real-time execution (up to 135 frame/s) of our novel DNN, called PULP-Frontnet. We showcase how, scaling our model's memory and computational requirement, we can significantly improve the onboard inference (top energy efficiency of 0.43 mJ/frame) with no compromise in the quality-of-result vs. a resource-unconstrained baseline (i.e., full-precision DNN). Field experiments demonstrate a closed-loop top-notch autonomous navigation capability, with a heavily resource-constrained 27-gram Crazyflie 2.1 nano-quadrotor. Compared against the control performance achieved using an ideal sensing setup, onboard relative pose inference yields excellent drone behavior in terms of median absolute errors, such as positional (onboard: 41 cm, ideal: 26 cm) and angular (onboard: 3.7$^{\circ}$, ideal: 4.1$^{\circ}$).      
### 26.Towards Better Adaptive Systems by Combining MAPE, Control Theory, and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.10847.pdf)
>  Two established approaches to engineer adaptive systems are architecture-based adaptation that uses a Monitor-Analysis-Planning-Executing (MAPE) loop that reasons over architectural models (aka Knowledge) to make adaptation decisions, and control-based adaptation that relies on principles of control theory (CT) to realize adaptation. Recently, we also observe a rapidly growing interest in applying machine learning (ML) to support different adaptation mechanisms. While MAPE and CT have particular characteristics and strengths to be applied independently, in this paper, we are concerned with the question of how these approaches are related with one another and whether combining them and supporting them with ML can produce better adaptive systems. We motivate the combined use of different adaptation approaches using a scenario of a cloud-based enterprise system and illustrate the analysis when combining the different approaches. To conclude, we offer a set of open questions for further research in this interesting area.      
### 27.Prediction of progressive lens performance from neural network simulations  [ :arrow_down: ](https://arxiv.org/pdf/2103.10842.pdf)
>  Purpose: The purpose of this study is to present a framework to predict visual acuity (VA) based on a convolutional neural network (CNN) and to further to compare PAL designs. <br>Method: A simple two hidden layer CNN was trained to classify the gap orientations of Landolt Cs by combining the feature extraction abilities of a CNN with psychophysical staircase methods. The simulation was validated regarding its predictability of clinical VA from induced spherical defocus (between +/-1.5 D, step size: 0.5 D) from 39 subjectively measured eyes. Afterwards, a simulation for a presbyopic eye corrected by either a generic hard or a soft PAL design (addition power: 2.5 D) was performed including lower and higher order aberrations. <br>Result: The validation revealed consistent offset of +0.20 logMAR +/-0.035 logMAR from simulated VA. Bland-Altman analysis from offset-corrected results showed limits of agreement (+/-1.96 SD) of -0.08 logMAR and +0.07 logMAR, which is comparable to clinical repeatability of VA assessment. The application of the simulation for PALs confirmed a bigger far zone for generic hard design but did not reveal zone width differences for the intermediate or near zone. Furthermore, a horizontal area of better VA at the mid of the PAL was found, which confirms the importance for realistic performance simulations using object-based aberration and physiological performance measures as VA. <br>Conclusion: The proposed holistic simulation tool was shown to act as an accurate model for subjective visual performance. Further, the simulations application for PALs indicated its potential as an effective method to compare visual performance of different optical designs. Moreover, the simulation provides the basis to incorporate neural aspects of visual perception and thus simulate the VA including neural processing in future.      
### 28.Gaussian Channels with Feedback: A Dynamic Programming Approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.10807.pdf)
>  In this paper, we consider a communication system where a sender sends messages over a memoryless Gaussian point-to-point channel to a receiver and receives the output feedback over another Gaussian channel with known variance and unit delay. The sender sequentially transmits the message over multiple times till a certain error performance is achieved. The aim of our work is to design a transmission strategy to process every transmission with the information that was received in the previous feedback and send a signal so that the estimation error drops as quickly as possible. The optimal code is unknown for channels with noisy output feedback when the block length is finite. Even within the family of linear codes, optimal codes are unknown in general. Bridging this gap, we propose a family of linear sequential codes and provide a dynamic programming algorithm to solve for a closed form expression for the optimal code within a class of sequential linear codes. The optimal code discovered via dynamic programming is a generalized version of which the Schalkwijk-Kailath (SK) scheme is one special case with noiseless feedback; our proposed code coincides with the celebrated SK scheme for noiseless feedback settings.      
### 29.Acoustic word embeddings for zero-resource languages using self-supervised contrastive learning and multilingual adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2103.10731.pdf)
>  Acoustic word embeddings (AWEs) are fixed-dimensional representations of variable-length speech segments. For zero-resource languages where labelled data is not available, one AWE approach is to use unsupervised autoencoder-based recurrent models. Another recent approach is to use multilingual transfer: a supervised AWE model is trained on several well-resourced languages and then applied to an unseen zero-resource language. We consider how a recent contrastive learning loss can be used in both the purely unsupervised and multilingual transfer settings. Firstly, we show that terms from an unsupervised term discovery system can be used for contrastive self-supervision, resulting in improvements over previous unsupervised monolingual AWE models. Secondly, we consider how multilingual AWE models can be adapted to a specific zero-resource language using discovered terms. We find that self-supervised contrastive adaptation outperforms adapted multilingual correspondence autoencoder and Siamese AWE models, giving the best overall results in a word discrimination task on six zero-resource languages.      
### 30.USTC-NELSLIP System Description for DIHARD-III Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2103.10661.pdf)
>  This system description describes our submission system to the Third DIHARD Speech Diarization Challenge. Besides the traditional clustering based system, the innovation of our system lies in the combination of various front-end techniques to solve the diarization problem, including speech separation and target-speaker based voice activity detection (TS-VAD), combined with iterative data purification. We also adopted audio domain classification to design domain-dependent processing. Finally, we performed post processing to do system fusion and selection. Our best system achieved DERs of 11.30% in track 1 and 16.78% in track 2 on evaluation set, respectively.      
### 31.Beyond Linear Subspace Clustering: A Comparative Study of Nonlinear Manifold Clustering Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2103.10656.pdf)
>  Subspace clustering is an important unsupervised clustering approach. It is based on the assumption that the high-dimensional data points are approximately distributed around several low-dimensional linear subspaces. The majority of the prominent subspace clustering algorithms rely on the representation of the data points as linear combinations of other data points, which is known as a self-expressive representation. To overcome the restrictive linearity assumption, numerous nonlinear approaches were proposed to extend successful subspace clustering approaches to data on a union of nonlinear manifolds. In this comparative study, we provide a comprehensive overview of nonlinear subspace clustering approaches proposed in the last decade. We introduce a new taxonomy to classify the state-of-the-art approaches into three categories, namely locality preserving, kernel based, and neural network based. The major representative algorithms within each category are extensively compared on carefully designed synthetic and real-world data sets. The detailed analysis of these approaches unfolds potential research directions and unsolved challenges in this field.      
### 32.SoK: A Modularized Approach to Study the Security of Automatic Speech Recognition Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.10651.pdf)
>  With the wide use of Automatic Speech Recognition (ASR) in applications such as human machine interaction, simultaneous interpretation, audio transcription, etc., its security protection becomes increasingly important. Although recent studies have brought to light the weaknesses of popular ASR systems that enable out-of-band signal attack, adversarial attack, etc., and further proposed various remedies (signal smoothing, adversarial training, etc.), a systematic understanding of ASR security (both attacks and defenses) is still missing, especially on how realistic such threats are and how general existing protection could be. In this paper, we present our systematization of knowledge for ASR security and provide a comprehensive taxonomy for existing work based on a modularized workflow. More importantly, we align the research in this domain with that on security in Image Recognition System (IRS), which has been extensively studied, using the domain knowledge in the latter to help understand where we stand in the former. Generally, both IRS and ASR are perceptual systems. Their similarities allow us to systematically study existing literature in ASR security based on the spectrum of attacks and defense solutions proposed for IRS, and pinpoint the directions of more advanced attacks and the directions potentially leading to more effective protection in ASR. In contrast, their differences, especially the complexity of ASR compared with IRS, help us learn unique challenges and opportunities in ASR security. Particularly, our experimental study shows that transfer learning across ASR models is feasible, even in the absence of knowledge about models (even their types) and training data.      
### 33.Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings  [ :arrow_down: ](https://arxiv.org/pdf/2103.10614.pdf)
>  Hyperspectral images (HSIs) with narrow spectral bands can capture rich spectral information, making them suitable for many computer vision tasks. One of the fundamental limitations of HSI is its low spatial resolution, and several recent works on super-resolution(SR) have been proposed to tackle this challenge. However, due to HSI cameras' diversity, different cameras capture images with different spectral response functions and the number of total channels. The existing HSI datasets are usually small and consequently insufficient for modeling. We propose a Meta-Learning-Based Super-Resolution(MLSR) model, which can take in HSI images at an arbitrary number of input bands' peak wavelengths and generate super-resolved HSIs with an arbitrary number of output bands' peak wavelengths. We artificially create sub-datasets by sampling the bands from NTIRE2020 and ICVL datasets to simulate the cross-dataset settings and perform HSI SR with spectral interpolation and extrapolation on them. We train a single MLSR model for all sub-datasets and train dedicated baseline models for each sub-dataset. The results show the proposed model has the same level or better performance compared to the-state-of-the-art HSI SR methods.      
### 34.Comprehensive Survey and Taxonomies of False Injection Attacks in Smart Grid: Attack Models, Targets, and Impacts  [ :arrow_down: ](https://arxiv.org/pdf/2103.10594.pdf)
>  Smart Grid has rapidly transformed the centrally controlled power system into a massively interconnected cyber-physical system that benefits from the revolutions happening in the communications (e.g. 5G) and the growing proliferation of the Internet of Things devices (such as smart metres and intelligent electronic devices). While the convergence of a significant number of cyber-physical elements has enabled the Smart Grid to be far more efficient and competitive in addressing the growing global energy challenges, it has also introduced a large number of vulnerabilities culminating in violations of data availability, integrity, and confidentiality. Recently, false data injection (FDI) has become one of the most critical cyberattacks, and appears to be a focal point of interest for both research and industry. To this end, this paper presents a comprehensive review in the recent advances of the FDI attacks, with particular emphasis on 1) adversarial models, 2) attack targets, and 3) impacts in the Smart Grid infrastructure. This review paper aims to provide a thorough understanding of the incumbent threats affecting the entire spectrum of the Smart Grid. Related literature are analysed and compared in terms of their theoretical and practical implications to the Smart Grid cybersecurity. In conclusion, a range of technical limitations of existing false data attack research is identified, and a number of future research directions is recommended.      
### 35.Finite-Horizon, Energy-Optimal Trajectories in Unsteady Flows  [ :arrow_down: ](https://arxiv.org/pdf/2103.10556.pdf)
>  Intelligent mobile sensors, such as uninhabited aerial or underwater vehicles, are becoming prevalent in environmental sensing and monitoring applications. These active sensing platforms operate in unsteady fluid flows, including windy urban environments, hurricanes, and ocean currents. Often constrained in their actuation capabilities, the dynamics of these mobile sensors depend strongly on the background flow, making their deployment and control particularly challenging. Therefore, efficient trajectory planning with partial knowledge about the background flow is essential for teams of mobile sensors to adaptively sense and monitor their environments. In this work, we investigate the use of finite-horizon model predictive control (MPC) for the energy-efficient trajectory planning of an active mobile sensor in an unsteady fluid flow field. We uncover connections between the finite-time optimal trajectories and finite-time Lyapunov exponents (FTLE) of the background flow, confirming that energy-efficient trajectories exploit invariant coherent structures in the flow. We demonstrate our findings on the unsteady double gyre vector field, which is a canonical model for chaotic mixing in the ocean. We present an exhaustive search through critical MPC parameters including the prediction horizon, maximum sensor actuation, and relative penalty on the accumulated state error and actuation effort. We find that even relatively short prediction horizons can often yield nearly energy-optimal trajectories. These results are promising for the adaptive planning of energy-efficient trajectories for swarms of mobile sensors in distributed sensing and monitoring.      
### 36.Resilient Cooperative Adaptive Cruise Control for Autonomous Vehicles Using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.10533.pdf)
>  Cooperative Adaptive Cruise Control (CACC) is a fundamental connected vehicle application that extends Adaptive Cruise Control by exploiting vehicle-to-vehicle (V2V) communication. CACC is a crucial ingredient for numerous autonomous vehicle functionalities including platooning, distributed route management, etc. Unfortunately, malicious V2V communications can subvert CACC, leading to string instability and road accidents. In this paper, we develop a novel resiliency infrastructure, RACCON, for detecting and mitigating V2V attacks on CACC. RACCON uses machine learning to develop an on-board prediction model that captures anomalous vehicular responses and performs mitigation in real time. RACCON-enabled vehicles can exploit the high efficiency of CACC without compromising safety, even under potentially adversarial scenarios. We present extensive experimental evaluation to demonstrate the efficacy of RACCON.      
### 37.Maximum Likelihood Recursive State Estimation using the Expectation Maximization Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2103.10475.pdf)
>  A Maximum Likelihood recursive state estimator is derived for non-linear and non-Gaussian state-space models. The estimator combines a particle filter to generate the conditional density and the Expectation Maximization algorithm to compute the maximum likelihood state estimate iteratively. Algorithms for maximum likelihood state filtering, prediction and smoothing are presented. The convergence properties of these algorithms, which are inherited from the Expectation Maximization algorithm, are proven and examined in two examples. It is shown that, with randomized reinitialization, which is feasible because of the algorithm simplicity, these methods are able to converge to the Maximum Likelihood Estimate (MLE) of multimodal, truncated and skewed densities, as well as those of disjoint support.      
### 38.Robot Manipulator Control with Inverse Kinematics PD-Pseudoinverse Jacobian and Forward Kinematics Denavit Hartenberg  [ :arrow_down: ](https://arxiv.org/pdf/2103.10461.pdf)
>  This paper presents the development of vision-based robotic arm manipulator control by applying Proportional Derivative-Pseudoinverse Jacobian (PD-PIJ) kinematics and Denavit Hartenberg forward kinematics. The task of sorting objects based on color is carried out to observe error propagation in the implementation of manipulator on real system. The objects image captured by the digital camera were processed based on HSV-color model and the centroid coordinate of each object detected were calculated. These coordinates are end effector position target to pick each object and were placed to the right position based on its color. Based on the end effector position target, PD-PIJ inverse kinematics method was used to determine the right angle of each joint of manipulator links. The angles found by PD-PIJ is the input of DH forward kinematics. The process was repeated until the square end effector reached the target. The experiment of model and implementation to actual manipulator were analyzed using Probability Density Function (PDF) and Weibull Probability Distribution. The result shows that the manipulator navigation system had a good performance. The real implementation of color sorting task on manipulator shows the probability of success rate cm is 94.46% for euclidian distance error less than 1.2 cm.      
### 39.Localization of Cochlear Implant Electrodes from Cone Beam Computed Tomography using Particle Belief Propagation  [ :arrow_down: ](https://arxiv.org/pdf/2103.10434.pdf)
>  Cochlear implants (CIs) are implantable medical devices that can restore the hearing sense of people suffering from profound hearing loss. The CI uses a set of electrode contacts placed inside the cochlea to stimulate the auditory nerve with current pulses. The exact location of these electrodes may be an important parameter to improve and predict the performance with these devices. Currently the methods used in clinics to characterize the geometry of the cochlea as well as to estimate the electrode positions are manual, error-prone and time consuming. We propose a Markov random field (MRF) model for CI electrode localization for cone beam computed tomography (CBCT) data-sets. Intensity and shape of electrodes are included as prior knowledge as well as distance and angles between contacts. MRF inference is based on slice sampling particle belief propagation and guided by several heuristics. A stochastic search finds the best maximum a posteriori estimation among sampled MRF realizations. We evaluate our algorithm on synthetic and real CBCT data-sets and compare its performance with two state of the art algorithms. An increase of localization precision up to 31.5% (mean), or 48.6% (median) respectively, on real CBCT data-sets is shown.      
