# ArXiv eess --Thu, 4 Mar 2021
### 1.Advanced control based on Recurrent Neural Networks learned using Virtual Reference Feedback Tuning and application to an Electronic Throttle Body (with supplementary material)  [ :arrow_down: ](https://arxiv.org/pdf/2103.02567.pdf)
>  In this paper the application of Virtual Reference Feedback Tuning (VRFT) for control of nonlinear systems with regulators defined by Echo State Networks (ESN) and Long Short Term Memory (LSTM) networks is investigated. The capability of this class of regulators of constraining the control variable is pointed out and an advanced control scheme that allows to achieve zero steady-state error is presented. The developed algorithms are validated on a benchmark example that consists of an electronic throttle body (ETB).      
### 2.Wind Flow Estimation in Thermal Sky Images for Sun Occlusion Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2103.02556.pdf)
>  Moving clouds affect the global solar irradiance that reaches the surface of the Earth. As a consequence, the amount of resources available to meet the energy demand in a smart grid powered using Photovoltaic (PV) systems depends on the shadows projected by passing clouds. This research introduces an algorithm for tracking clouds to predict Sun occlusion. Using thermal images of clouds, the algorithm is capable of estimating multiple wind velocity fields with different altitudes, velocity magnitudes and directions.      
### 3.Multi-Channel and Multi-Microphone Acoustic Echo Cancellation Using A Deep Learning Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.02552.pdf)
>  Building on the deep learning based acoustic echo cancellation (AEC) in the single-loudspeaker (single-channel) and single-microphone setup, this paper investigates multi-channel AEC (MCAEC) and multi-microphone AEC (MMAEC). We train a deep neural network (DNN) to predict the near-end speech from microphone signals with far-end signals used as additional information. We find that the deep learning approach avoids the non-uniqueness problem in traditional MCAEC algorithms. For the AEC setup with multiple microphones, rather than employing AEC for each microphone, a single DNN is trained to achieve echo removal for all microphones. Also, combining deep learning based AEC with deep learning based beamforming further improves the system performance. Experimental results show the effectiveness of both bidirectional long short-term memory (BLSTM) and convolutional recurrent network (CRN) based methods for MCAEC and MMAEC. Furthermore, deep learning based methods are capable of removing echo and noise simultaneously and work well in the presence of nonlinear distortions.      
### 4.Constraint Control of a Boom Crane System  [ :arrow_down: ](https://arxiv.org/pdf/2103.02528.pdf)
>  Boom cranes are among the most used cranes to lift heavy loads. Although fairly simple mechanically, from the control viewpoint this kind of crane is a nonlinear underactuated system which presents several challenges, especially when con-trolled in the presence of constraints. To solve this problem, we propose an approach based on the Explicit Reference Governor (ERG), which does not require any online optimization, thus making it computationally inexpensive. The proposed control scheme is able to steer the crane to a desired position ensuring the respect of limited joint ranges, maximum oscillation angle, and the avoidance of static obstacles.      
### 5.Oscillation Reduction for Knuckle Cranes  [ :arrow_down: ](https://arxiv.org/pdf/2103.02509.pdf)
>  Boom cranes are among the most common material handling systems due to their simple design. Some boom cranes also have an auxiliary jib connected to the boom with a flexible joint to enhance the maneuverability and increase the workspace of the crane. Such boom cranes are commonly called knuckle boom cranes. Due to their underactuated properties, it is fairly challenging to control knuckle boom cranes. To the best of our knowledge, only a few techniques are present in the literature to control this type of cranes using approximate models of the crane. In this paper we present for the first time a complete mathematical model for this crane where it is possible to control the three rotations of the crane (known as luff, slew, and jib movement), and the cable length. One of the main challenges to control this system is how to reduce the oscillations in an effective way. In this paper we propose a nonlinear control based on energy considerations capable of guiding the crane to desired sets points while effectively reducing load oscillations. The corresponding stability and convergence analysis is proved using the LaSalle's invariance principle. Simulation results are provided to demonstrate the effectiveness and feasibility of the proposed method.      
### 6.Non-Blockingness Verification of Bounded Petri Nets Using Basis Reachability Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2103.02475.pdf)
>  In this paper, we study the problem of non-blockingness verification by tapping into the basis reachability graph (BRG). Non-blockingness is a property that ensures that all pre-specified tasks can be completed, which is a mandatory requirement during the system design stage. In this paper we develop a condition of transition partition of a given net such that the corresponding conflict-increase BRG contains sufficient information on verifying non-blockingness of its corresponding Petri net. Thanks to the compactness of the BRG, our approach possesses practical efficiency since the exhaustive enumeration of the state space can be avoided. In particular, our method does not require that the net is deadlock-free.      
### 7.Arthroscopic Multi-Spectral Scene Segmentation Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.02465.pdf)
>  Knee arthroscopy is a minimally invasive surgical (MIS) procedure which is performed to treat knee-joint ailment. Lack of visual information of the surgical site obtained from miniaturized cameras make this surgical procedure more complex. Knee cavity is a very confined space; therefore, surgical scenes are captured at close proximity. Insignificant context of knee atlas often makes them unrecognizable as a consequence unintentional tissue damage often occurred and shows a long learning curve to train new surgeons. Automatic context awareness through labeling of the surgical site can be an alternative to mitigate these drawbacks. However, from the previous studies, it is confirmed that the surgical site exhibits several limitations, among others, lack of discriminative contextual information such as texture and features which drastically limits this vision task. Additionally, poor imaging conditions and lack of accurate ground-truth labels are also limiting the accuracy. To mitigate these limitations of knee arthroscopy, in this work we proposed a scene segmentation method that successfully segments multi structures.      
### 8.A System Level Approach to Robust Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2103.02460.pdf)
>  Robust tube-based model predictive control (MPC) methods address constraint satisfaction by leveraging an a-priori determined tube controller in the prediction to tighten the constraints. This paper presents a system level tube-MPC (SLTMPC) method derived from the system level parameterization (SLP), which allows optimization over the tube controller online when solving the MPC problem, which can significantly reduce conservativeness. We derive the SLTMPC method by establishing an equivalence relation between a class of robust MPC methods and the SLP. Finally, we show that the SLTMPC formulation naturally arises from an extended SLP formulation and show its merits in a numerical example.      
### 9.Modeling and control of 5-DoF boom crane  [ :arrow_down: ](https://arxiv.org/pdf/2103.02454.pdf)
>  Automation of cranes can have a direct impact on the productivity of construction projects. In this paper, we focus on the control of one of the most used cranes, the boom crane. Tower cranes and overhead cranes have been widely studied in the literature, whereas the control of boom cranes has been investigated only by a few works. Typically, these works make use of simple models making use of a large number of simplifying assumptions (e.g. fixed length cable, assuming certain dynamics are uncoupled, etc.) A first result of this paper is to present a fairly complete nonlinear dynamic model of a boom crane taking into account all coupling dynamics and where the only simplifying assumption is that the cable is considered as rigid. The boom crane involves pitching and rotational movements, which generate complicated centrifugal forces, and consequently, equations of motion highly nonlinear. On the basis of this model, a control law has been developed able to perform position control of the crane while actively damping the oscillations of the load. The effectiveness of the approach has been tested in simulation with realistic physical parameters and tested in the presence of wind disturbances.      
### 10.The effect of speech and noise levels on the quality perceived by cochlear implant and normal hearing listeners  [ :arrow_down: ](https://arxiv.org/pdf/2103.02421.pdf)
>  Electrical hearing by cochlear implants (CIs) may be fundamentally different from acoustic hearing by normal-hearing (NH) listeners, presumably showing unequal speech quality perception in various noise environments. Noise reduction (NR) algorithms used in CI reduce the noise in favor of signal-to-noise ratio (SNR), regardless of plausible accompanying distortions that may degrade the speech quality perception. To gain better understanding of CI speech quality perception, the present work aimed investigating speech quality perception in a diverse noise conditions, including factors of speech/noise levels, type of noise, and distortions caused by NR models. Fifteen NH and seven CI subjects participated in this study. Speech sentences were set to two different levels (65 and 75 dB SPL). Two types of noise (Cafeteria and Babble) at three levels (55, 65, and 75 dB SPL) were used. Sentences were processed using two NR algorithms to investigate the perceptual sensitivity of CI and NH listeners to the distortion. All sentences processed with the combinations of these sets were presented to CI and NH listeners, and they were asked to rate the sound quality of speech as they perceived. The effect of each factor on the perceived speech quality was investigated based on the group averaged quality rated by CI and NH listeners. Consistent with previous studies, CI listeners were not as sensitive as NH to the distortion made by NR algorithms. Statistical analysis showed that the speech level has significant effect on quality perception. At the same SNR, the quality of 65 dB speech was rated higher than that of 75 dB for CI users, but vice versa for NH listeners. Therefore, the present study showed that the perceived speech quality patterns were different between CI and NH listeners in terms of their sensitivity to distortion and speech level in complex listening environment.      
### 11.Nonlinear MPC for Offset-Free Tracking of systems learned by GRU Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.02383.pdf)
>  The use of Recurrent Neural Networks (RNNs) for system identification has recently gathered increasing attention, thanks to their black-box modeling capabilities.Albeit RNNs have been fruitfully adopted in many applications, only few works are devoted to provide rigorous theoretical foundations that justify their use for control purposes. The aim of this paper is to describe how stable Gated Recurrent Units (GRUs), a particular RNN architecture, can be trained and employed in a Nonlinear MPC framework to perform offset-free tracking of constant references with guaranteed closed-loop stability. The proposed approach is tested on a pH neutralization process benchmark, showing remarkable performances.      
### 12.Real-World Single Image Super-Resolution: A Brief Review  [ :arrow_down: ](https://arxiv.org/pdf/2103.02368.pdf)
>  Single image super-resolution (SISR), which aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) observation, has been an active research topic in the area of image processing in recent decades. Particularly, deep learning-based super-resolution (SR) approaches have drawn much attention and have greatly improved the reconstruction performance on synthetic data. Recent studies show that simulation results on synthetic data usually overestimate the capacity to super-resolve real-world images. In this context, more and more researchers devote themselves to develop SR approaches for realistic images. This article aims to make a comprehensive review on real-world single image super-resolution (RSISR). More specifically, this review covers the critical publically available datasets and assessment metrics for RSISR, and four major categories of RSISR methods, namely the degradation modeling-based RSISR, image pairs-based RSISR, domain translation-based RSISR, and self-learning-based RSISR. Comparisons are also made among representative RSISR methods on benchmark datasets, in terms of both reconstruction quality and computational efficiency. Besides, we discuss challenges and promising research topics on RSISR.      
### 13.Open community platform for hearing aid algorithm research: open Master Hearing Aid (openMHA)  [ :arrow_down: ](https://arxiv.org/pdf/2103.02313.pdf)
>  open Master Hearing Aid (openMHA) was developed and provided to the hearing aid research community as an open-source software platform with the aim to support sustainable and reproducible research towards improvement and new types of assistive hearing systems not limited by proprietary software. The software offers a flexible framework that allows the users to conduct hearing aid research using tools and a number of signal processing plugins provided with the software as well as the implementation of own methods. The openMHA software is independent of a specific hardware and supports Linux, MacOS and Windows operating systems as well as 32- bit and 64-bit ARM-based architectures such as used in small portable integrated systems. <a class="link-external link-http" href="http://www.openmha.org" rel="external noopener nofollow">this http URL</a>      
### 14.Rate Analysis and Deep Neural Network Detectors for SEFDM FTN Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.02306.pdf)
>  In this work we compare the capacity and achievable rate of uncoded faster than Nyquist (FTN) signalling in the frequency domain, also referred to as spectrally efficient FDM (SEFDM). We propose a deep residual convolutional neural network detector for SEFDM signals in additive white Gaussian noise channels, that allows to approach the Mazo limit in systems with up to 60 subcarriers. Notably, the deep detectors achieve a loss less than 0.4-0.7 dB for uncoded QPSK SEFDM systems of 12 to 60 subcarriers at a 15% spectral compression.      
### 15.Fair heat distribution under deficits in district heating networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.02300.pdf)
>  In order to improve the energy efficiency in district heating networks and the comfort of their customers, these networks need to overcome the problem of unfair heat distribution under heat deficits. This paper introduces a new strategy to achieve this thermal fairness objective: it is low-cost in terms of communication and computation. The proposed approach is illustrated on a simulation example.      
### 16.Scaling Laws for Unamplified Coherent Transmission in Next-generation Short-Reach and Access Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.02299.pdf)
>  International standardization bodies (IEEE and ITU-T) working on the evolution of transmission technologies are still considering traditional direct detection solutions for the most relevant short reach optical link applications, that are Passive Optical Networks (PON) and intra-data center interconnects. Anyway, future jumps towards even higher bit rates per wavelength will require a complete paradigm shift, moving towards coherent technologies. In this paper, we thus study both analytically and experimentally the scaling laws of unamplified coherent transmission in the short-reach communications ecosystems. We believe that, given the extremely tight techno-economic constraints, such a revolutionary transition towards coherent in short-reach first requires a very detailed study of its intrinsic capabilities in largely extending the limitation currently imposed by direct detection systems. To this end, this paper focuses on the ultimate physical layer limitations of unamplified coherent systems in terms of bit rate and power budget. The main parameters of our performance estimation model are extracted through fitting with a set of experimental characterizations and later used as the starting point of a scaling laws study regarding local oscillator power, modulator-induced attenuation, bit rate, and maximum achievable power budget. The analytically predicted performance is then verified through transmission experiments, including a demonstration on a 37-km installed metropolitan dark fiber in the city of Turin.      
### 17.Computation Resource Allocation Solution in Recommender Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.02259.pdf)
>  Recommender systems rely heavily on increasing computation resources to improve their business goal. By deploying computation-intensive models and algorithms, these systems are able to inference user interests and exhibit certain ads or commodities from the candidate set to maximize their business goals. However, such systems are facing two challenges in achieving their goals. On the one hand, facing massive online requests, computation-intensive models and algorithms are pushing their computation resources to the limit. On the other hand, the response time of these systems is strictly limited to a short period, e.g. 300 milliseconds in our real system, which is also being exhausted by the increasingly complex models and algorithms. <br>In this paper, we propose the computation resource allocation solution (CRAS) that maximizes the business goal with limited computation resources and response time. We comprehensively illustrate the problem and formulate such a problem as an optimization problem with multiple constraints, which could be broken down into independent sub-problems. To solve the sub-problems, we propose the revenue function to facilitate the theoretical analysis, and obtain the optimal computation resource allocation strategy. To address the applicability issues, we devise the feedback control system to help our strategy constantly adapt to the changing online environment. The effectiveness of our method is verified by extensive experiments based on the real dataset from <a class="link-external link-http" href="http://Taobao.com" rel="external noopener nofollow">this http URL</a>. We also deploy our method in the display advertising system of Alibaba. The online results show that our computation resource allocation solution achieves significant business goal improvement without any increment of computation cost, which demonstrates the efficacy of our method in real industrial practice.      
### 18.Compositional Synthesis of Control Barrier Certificates for Networks of Stochastic Systems against $ω$-Regular Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2103.02226.pdf)
>  This paper is concerned with a compositional scheme for the construction of control barrier certificates for interconnected discrete-time stochastic systems. The main objective is to synthesize switching control policies against $\omega$-regular properties that can be described by accepting languages of deterministic Streett automata (DSA) along with providing probabilistic guarantees for the satisfaction of such specifications. The proposed framework leverages the interconnection topology and a notion of so-called control sub-barrier certificates of subsystems, which are used to compositionally construct control barrier certificates of interconnected systems by imposing some dissipativity-type compositionality conditions. We propose a systematic approach to decompose high-level $\omega$-regular specifications into simpler tasks by utilizing the automata corresponding to the complement of specifications. In addition, we formulate an alternating direction method of multipliers (ADMM) optimization problem in order to obtain suitable control sub-barrier certificates of subsystems while satisfying compositionality conditions. We also provide a sum-of-squares (SOS) optimization problem for the computation of control sub-barrier certificates and local control policies of subsystems. Finally, we demonstrate the effectiveness of our proposed approaches by applying them to a physical case study.      
### 19.The generalized method of moments for multi-reference alignment  [ :arrow_down: ](https://arxiv.org/pdf/2103.02215.pdf)
>  This paper studies the application of the generalized method of moments (GMM) to multi-reference alignment (MRA): the problem of estimating a signal from its circularly-translated and noisy copies. We begin by proving that the GMM estimator maintains its asymptotic optimality for statistical models with group symmetry, including MRA. Then, we conduct a comprehensive numerical study and show that the GMM substantially outperforms the classical method of moments, whose application to MRA has been studied thoroughly in the literature. We also formulate the GMM to estimate a three-dimensional molecular structure using cryo-electron microscopy and present numerical results on simulated data.      
### 20.Eye-gaze Estimation with HEOG and Neck EMG using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.02186.pdf)
>  Hearing-impaired listeners usually have troubles attending target talker in multi-talker scenes, even with hearing aids (HAs). The problem can be solved with eye-gaze steering HAs, which requires listeners eye-gazing on the target. In a situation where head rotates, eye-gaze is subject to both behaviors of saccade and head rotation. However, existing methods of eye-gaze estimation did not work reliably, since the listener's strategy of eye-gaze varies and measurements of the two behaviors were not properly combined. Besides, existing methods were based on hand-craft features, which could overlook some important information. In this paper, a head-fixed and a head-free experiments were conducted. We used horizontal electrooculography (HEOG) and neck electromyography (NEMG), which separately measured saccade and head rotation to commonly estimate eye-gaze. Besides traditional classifier and hand-craft features, deep neural networks (DNN) were introduced to automatically extract features from intact waveforms. Evaluation results showed that when the input was HEOG with inertial measurement unit, the best performance of our proposed DNN classifiers achieved 93.3%; and when HEOG was with NEMG together, the accuracy reached 72.6%, higher than that with HEOG (about 71.0%) or NEMG (about 35.7%) alone. These results indicated the feasibility to estimate eye-gaze with HEOG and NEMG.      
### 21.Auditory Attention Decoding from EEG using Convolutional Recurrent Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.02183.pdf)
>  The auditory attention decoding (AAD) approach was proposed to determine the identity of the attended talker in a multi-talker scenario by analyzing electroencephalography (EEG) data. Although the linear model-based method has been widely used in AAD, the linear assumption was considered oversimplified and the decoding accuracy remained lower for shorter decoding windows. Recently, nonlinear models based on deep neural networks (DNN) have been proposed to solve this problem. However, these models did not fully utilize both the spatial and temporal features of EEG, and the interpretability of DNN models was rarely investigated. In this paper, we proposed novel convolutional recurrent neural network (CRNN) based regression model and classification model, and compared them with both the linear model and the state-of-the-art DNN models. Results showed that, our proposed CRNN-based classification model outperformed others for shorter decoding windows (around 90% for 2 s and 5 s). Although worse than classification models, the decoding accuracy of the proposed CRNN-based regression model was about 5% greater than other regression models. The interpretability of DNN models was also investigated by visualizing layers' weight.      
### 22.Real Time Vigilance Detection using Frontal EEG  [ :arrow_down: ](https://arxiv.org/pdf/2103.02169.pdf)
>  Vigilance of an operator is compromised in performing many monotonous activities like workshop and manufacturing floor tasks, driving, night shift workers, flying, and in general any activity which requires keen attention of an individual over prolonged periods of time. Driver or operator fatigue in these situations leads to drowsiness and lowered vigilance which is one of the largest contributors to injuries and fatalities amongst road accidents or workshop floor accidents. Having a vigilance monitoring system to detect drop in vigilance in these situations becomes very important. <br>This paper presents a system which uses non-invasively recorded Frontal EEG from an easy-to-use commercially available Brain Computer Interface wearable device to determine the vigilance state of an individual. The change in the power spectrum in the Frontal Theta Band (4-8Hz) of an individual's brain wave predicts the changes in the attention level of an individual - providing an early detection and warning system. This method provides an accurate, yet cheap and practical system for vigilance monitoring across different environments.      
### 23.Multi-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.02148.pdf)
>  Fast and accurate reconstruction of magnetic resonance (MR) images from under-sampled data is important in many clinical applications. In recent years, deep learning-based methods have been shown to produce superior performance on MR image reconstruction. However, these methods require large amounts of data which is difficult to collect and share due to the high cost of acquisition and medical data privacy regulations. In order to overcome this challenge, we propose a federated learning (FL) based solution in which we take advantage of the MR data available at different institutions while preserving patients' privacy. However, the generalizability of models trained with the FL setting can still be suboptimal due to domain shift, which results from the data collected at multiple institutions with different sensors, disease types, and acquisition protocols, etc. With the motivation of circumventing this challenge, we propose a cross-site modeling for MR image reconstruction in which the learned intermediate latent features among different source sites are aligned with the distribution of the latent features at the target site. Extensive experiments are conducted to provide various insights about FL for MR image reconstruction. Experimental results demonstrate that the proposed framework is a promising direction to utilize multi-institutional data without compromising patients' privacy for achieving improved MR image reconstruction. Our code will be available at <a class="link-external link-https" href="https://github.com/guopengf/FLMRCM" rel="external noopener nofollow">this https URL</a>.      
### 24.Reverb Conversion of Mixed Vocal Tracks Using an End-to-end Convolutional Deep Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.02147.pdf)
>  Reverb plays a critical role in music production, where it provides listeners with spatial realization, timbre, and texture of the music. Yet, it is challenging to reproduce the musical reverb of a reference music track even by skilled engineers. In response, we propose an end-to-end system capable of switching the musical reverb factor of two different mixed vocal tracks. This method enables us to apply the reverb of the reference track to the source track to which the effect is desired. Further, our model can perform de-reverberation when the reference track is used as a dry vocal source. The proposed model is trained in combination with an adversarial objective, which makes it possible to handle high-resolution audio samples. The perceptual evaluation confirmed that the proposed model can convert the reverb factor with the preferred rate of 64.8%. To the best of our knowledge, this is the first attempt to apply deep neural networks to converting music reverb of vocal tracks.      
### 25.Fast Security Evaluation for Operation of Water Distribution Systems Against Extreme Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2103.02146.pdf)
>  This paper defines a security injection region (SIR) to guarantee reliable operation of water distribution systems (WDS) under extreme conditions. The model of WDSs is highly nonlinear and nonconvex. Understanding the accurate SIRs of WDSs involves the analysis of nonlinear constraints, which is computationally expensive. To reduce the computational burden, this paper first investigates the convexity of the SIR of WDSs under certain conditions. Then, an algorithm based on a monotone inner polytope sequence is proposed to effectively and accurately determine these SIRs. The proposed algorithm estimates a sequence of inner polytopes that converge to the whole convex region. Each polytope adds a new area to the SIR. The algorithm is validated on two different WDSs, and the conclusion is drawn. The computational study shows this method is applicable and fast for both systems.      
### 26.Toward a Scalable Upper Bound for a CVaR-LQ Problem  [ :arrow_down: ](https://arxiv.org/pdf/2103.02136.pdf)
>  We consider a linear-quadratic optimal control problem in discrete time with distributional ambiguity, where the random cumulative quadratic cost is assessed via the Conditional Value-at-Risk (CVaR) functional. We take steps toward deriving a scalable value iteration algorithm to upper-bound the solution to this problem. A remarkable finding is that the value functions depend on positive definite matrices, which are computed by a Riccati-like recursion.      
### 27.Preliminaries on the Accurate Estimation of the Hurst Exponent Using Time Series  [ :arrow_down: ](https://arxiv.org/pdf/2103.02091.pdf)
>  This article explores the required amount of time series points from a high-speed computer network to accurately estimate the Hurst exponent. The methodology consists in designing an experiment using estimators that are applied to time series addresses resulting from the capture of high-speed network traffic, followed by addressing the minimum amount of point required to obtain in accurate estimates of the Hurst exponent. The methodology addresses the exhaustive analysis of the Hurst exponent considering bias behaviour, standard deviation, and Mean Squared Error using fractional Gaussian noise signals with stationary increases. Our results show that the Whittle estimator successfully estimates the Hurst exponent in series with few points. Based on the results obtained, a minimum length for the time series is empirically proposed. Finally, to validate the results, the methodology is applied to real traffic captures in a high-speed computer network.      
### 28.Deep J-Sense: Accelerated MRI Reconstruction via Unrolled Alternating Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2103.02087.pdf)
>  Accelerated multi-coil magnetic resonance imaging reconstruction has seen a substantial recent improvement combining compressed sensing with deep learning. However, most of these methods rely on estimates of the coil sensitivity profiles, or on calibration data for estimating model parameters. Prior work has shown that these methods degrade in performance when the quality of these estimators are poor or when the scan parameters differ from the training conditions. Here we introduce Deep J-Sense as a deep learning approach that builds on unrolled alternating minimization and increases robustness: our algorithm refines both the magnetization (image) kernel and the coil sensitivity maps. Experimental results on a subset of the knee fastMRI dataset show that this increases reconstruction performance and provides a significant degree of robustness to varying acceleration factors and calibration region sizes.      
### 29.Ultrasound Matrix Imaging. II. The distortion matrix for aberration correction over multiple isoplanatic patches  [ :arrow_down: ](https://arxiv.org/pdf/2103.02036.pdf)
>  This is the second article in a series of two which report on a matrix approach for ultrasound imaging in heterogeneous media. This article describes the quantification and correction of aberration, i.e. the distortion of an image caused by spatial variations in the medium speed-of-sound. Adaptive focusing can compensate for aberration, but is only effective over a restricted area called the isoplanatic patch. Here, we use an experimentally-recorded matrix of reflected acoustic signals to synthesize a set of virtual transducers. We then examine wave propagation between these virtual transducers and an arbitrary correction plane. Such wave-fronts consist of two components: (i) An ideal geometric wave-front linked to diffraction and the input focusing point, and; (ii) Phase distortions induced by the speed-of-sound variations. These distortions are stored in a so-called distortion matrix, the singular value decomposition of which gives access to an optimized focusing law at any point. We show that, by decoupling the aberrations undergone by the outgoing and incoming waves and applying an iterative strategy, compensation for even high-order and spatially-distributed aberrations can be achieved. As a proof-of-concept, ultrasound matrix imaging (UMI) is applied to the in-vivo imaging of a human calf. A map of isoplanatic patches is retrieved and is shown to be strongly correlated with the arrangement of tissues constituting the medium. The corresponding focusing laws yield an ultrasound image with an optimal contrast and a transverse resolution close to the ideal value predicted by diffraction theory. UMI thus provides a flexible and powerful route towards computational ultrasound.      
### 30.Rightsizing the Railway Signal Workforce: a Zero-Based Resourcing Approach Towards Asset Management  [ :arrow_down: ](https://arxiv.org/pdf/2103.02025.pdf)
>  Classic asset management approaches begin by inventorying all infrastructure assets and then assigning maintenance tasks and resources. Our approach collects similar data, but by starting with current personnel assignment and describing their job responsibilities and work processes, staff resistance in a railroad infrastructure owner-operator environment is minimized. Resulting "manning model" quantitatively measures signal maintenance burden including Federally mandated tests, trouble tickets, non-FRA maintenance, overhead and vacation coverage, location/shift assignment, administrative process, and work curfew productivity losses. It is capable of delivering immediate results by rightsizing allocation of workforce across shifts and maintenance base locations--even before all assets are formally inventoried. Typical data from a commuter passenger railroad shows that work curfews and shift assignment constraints have significant impacts on workforce productivity. Just over half of signal maintenance employee-hours are devoted to Federally mandated tests, whilst non-FRA and repair maintenance consumes about 25% each. These indicators provide intelligence driving strategic management actions to improve signal maintenance cost-effectiveness. This model provides workload-based employee assignment by craft, location, gang, and shift for maintenance manager use, but also provides analytical basis for establishing or abolishing positions in the budgeting process. Comparing its results with current employee payroll provides a measure of how much staffing stress the maintenance organization is under, which can help measure whether the current overtime usage is appropriate. Asset and maintenance task inventories collected in this process can also feed normal asset management processes to assess replacement cycles, asset failure risk, and to inform strategic and investment decisions.      
### 31.ADCS Preliminary Design For GNB  [ :arrow_down: ](https://arxiv.org/pdf/2103.02017.pdf)
>  This work deals with an ADCS model for a satellite orbiting around Earth. The object is to achieve a preliminary design and perform some analysis on it. To do so, a GNB was selected and main properties are exploited. Previous works of [9], [13], [14], [15] and [17] were analyzed and a synthesis was obtained; then a suitable control system was designed to satisfy technical requirements. Coding was performed using Matlab and Simulink. <br>Keywords: Attitude Determination, Attitude Control, Nanosatellite, Orbital Perturbations, Quaternion, Two Body Problem, Euler's Equations, Lyapunov Function.      
### 32.ICAM-reg: Interpretable Classification and Regression with Feature Attribution for Mapping Neurological Phenotypes in Individual Scans  [ :arrow_down: ](https://arxiv.org/pdf/2103.02561.pdf)
>  An important goal of medical imaging is to be able to precisely detect patterns of disease specific to individual scans; however, this is challenged in brain imaging by the degree of heterogeneity of shape and appearance. Traditional methods, based on image registration to a global template, historically fail to detect variable features of disease, as they utilise population-based analyses, suited primarily to studying group-average effects. In this paper we therefore take advantage of recent developments in generative deep learning to develop a method for simultaneous classification, or regression, and feature attribution (FA). Specifically, we explore the use of a VAE-GAN translation network called ICAM, to explicitly disentangle class relevant features from background confounds for improved interpretability and regression of neurological phenotypes. We validate our method on the tasks of Mini-Mental State Examination (MMSE) cognitive test score prediction for the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort, as well as brain age prediction, for both neurodevelopment and neurodegeneration, using the developing Human Connectome Project (dHCP) and UK Biobank datasets. We show that the generated FA maps can be used to explain outlier predictions and demonstrate that the inclusion of a regression module improves the disentanglement of the latent space. Our code is freely available on Github <a class="link-external link-https" href="https://github.com/CherBass/ICAM" rel="external noopener nofollow">this https URL</a>.      
### 33.Finite Sample Analysis of Spectral Radius Estimation and Its Applications to Networked Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.02553.pdf)
>  The spectral radius of the system state matrix plays an important role in linear system analysis and control design. Traditional methods rely on the exact knowledge of the system matrices, from which the spectral radius can be calculated directly. Instead, we consider the setting, where one only has finitely many samples about the system input and state measurements and would like to estimate the spectral radius from these data. We provide a method for constructing an estimate of the spectral radius and derive high probability estimation error bounds. Moreover, we show how to use the estimate and the estimation error bound to assert stability criteria for networked control systems over lossy channels when only finitely many samples of system dynamics and the packet drop sequence are available.      
### 34.Vanishing Twin GAN: How training a weak Generative Adversarial Network can improve semi-supervised image classification  [ :arrow_down: ](https://arxiv.org/pdf/2103.02496.pdf)
>  Generative Adversarial Networks can learn the mapping of random noise to realistic images in a semi-supervised framework. This mapping ability can be used for semi-supervised image classification to detect images of an unknown class where there is no training data to be used for supervised classification. However, if the unknown class shares similar characteristics to the known class(es), GANs can learn to generalize and generate images that look like both classes. This generalization ability can hinder the classification performance. In this work, we propose the Vanishing Twin GAN. By training a weak GAN and using its generated output image parallel to the regular GAN, the Vanishing Twin training improves semi-supervised image classification where image similarity can hurt classification tasks.      
### 35.DeepFN: Towards Generalizable Facial Action Unit Recognition with Deep Face Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2103.02484.pdf)
>  Facial action unit recognition has many applications from market research to psychotherapy and from image captioning to entertainment. Despite its recent progress, deployment of these models has been impeded due to their limited generalization to unseen people and demographics. This work conducts an in-depth analysis of performance across several dimensions: individuals(40 subjects), genders (male and female), skin types (darker and lighter), and databases (BP4D and DISFA). To help suppress the variance in data, we use the notion of self-supervised denoising autoencoders to design a method for deep face normalization(DeepFN) that transfers facial expressions of different people onto a common facial template which is then used to train and evaluate facial action recognition models. We show that person-independent models yield significantly lower performance (55% average F1 and accuracy across 40 subjects) than person-dependent models (60.3%), leading to a generalization gap of 5.3%. However, normalizing the data with the newly introduced DeepFN significantly increased the performance of person-independent models (59.6%), effectively reducing the gap. Similarly, we observed generalization gaps when considering gender (2.4%), skin type (5.3%), and dataset (9.4%), which were significantly reduced with the use of DeepFN. These findings represent an important step towards the creation of more generalizable facial action unit recognition systems.      
### 36.Sparsity Aware Normalization for GANs  [ :arrow_down: ](https://arxiv.org/pdf/2103.02458.pdf)
>  Generative adversarial networks (GANs) are known to benefit from regularization or normalization of their critic (discriminator) network during training. In this paper, we analyze the popular spectral normalization scheme, find a significant drawback and introduce sparsity aware normalization (SAN), a new alternative approach for stabilizing GAN training. As opposed to other normalization methods, our approach explicitly accounts for the sparse nature of the feature maps in convolutional networks with ReLU activations. We illustrate the effectiveness of our method through extensive experiments with a variety of network architectures. As we show, sparsity is particularly dominant in critics used for image-to-image translation settings. In these cases our approach improves upon existing methods, in less training epochs and with smaller capacity networks, while requiring practically no computational overhead.      
### 37.Multi-view Audio and Music Classification  [ :arrow_down: ](https://arxiv.org/pdf/2103.02420.pdf)
>  We propose in this work a multi-view learning approach for audio and music classification. Considering four typical low-level representations (i.e. different views) commonly used for audio and music recognition tasks, the proposed multi-view network consists of four subnetworks, each handling one input types. The learned embedding in the subnetworks are then concatenated to form the multi-view embedding for classification similar to a simple concatenation network. However, apart from the joint classification branch, the network also maintains four classification branches on the single-view embedding of the subnetworks. A novel method is then proposed to keep track of the learning behavior on the classification branches and adapt their weights to proportionally blend their gradients for network training. The weights are adapted in such a way that learning on a branch that is generalizing well will be encouraged whereas learning on a branch that is overfitting will be slowed down. Experiments on three different audio and music classification tasks show that the proposed multi-view network not only outperforms the single-view baselines but also is superior to the multi-view baselines based on concatenation and late fusion.      
### 38.Deep Learning-based Automated Aortic Area and Distensibility Assessment: The Multi-Ethnic Study of Atherosclerosis (MESA)  [ :arrow_down: ](https://arxiv.org/pdf/2103.02417.pdf)
>  This study applies convolutional neural network (CNN)-based automatic segmentation and distensibility measurement of the ascending and descending aorta from 2D phase-contrast cine magnetic resonance imaging (PC-cine MRI) within the large MESA cohort with subsequent assessment on an external cohort of thoracic aortic aneurysm (TAA) patients. 2D PC-cine MRI images of the ascending and descending aorta at the pulmonary artery bifurcation from the MESA study were included. Train, validation, and internal test sets consisted of 1123 studies (24282 images), 374 studies (8067 images), and 375 studies (8069 images), respectively. An external test set of TAAs consisted of 37 studies (3224 images). A U-Net based CNN was constructed, and performance was evaluated utilizing dice coefficient (for segmentation) and concordance correlation coefficients (CCC) of aortic geometric parameters by comparing to manual segmentation and parameter estimation. Dice coefficients for aorta segmentation were 97.6% (CI: 97.5%-97.6%) and 93.6% (84.6%-96.7%) on the internal and external test of TAAs, respectively. CCC for comparison of manual and CNN maximum and minimum ascending aortic areas were 0.97 and 0.95, respectively, on the internal test set and 0.997 and 0.995, respectively, for the external test. CCCs for maximum and minimum descending aortic areas were 0.96 and 0. 98, respectively, on the internal test set and 0.93 and 0.93, respectively, on the external test set. We successfully developed and validated a U-Net based ascending and descending aortic segmentation and distensibility quantification model in a large multi-ethnic database and in an external cohort of TAA patients.      
### 39.Filter-Based Abstractions with Correctness Guarantees for Planning under Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2103.02398.pdf)
>  We study planning problems for continuous control systems with uncertainty caused by measurement and process noise. The goal is to find an optimal plan that guarantees that the system reaches a desired goal state within finite time. Measurement noise causes limited observability of system states, and process noise causes uncertainty in the outcome of a given plan. These factors render the problem undecidable in general. Our key contribution is a novel abstraction scheme that employs Kalman filtering as a state estimator to obtain a finite-state model, which we formalize as a Markov decision process (MDP). For this MDP, we employ state-of-the-art model checking techniques to efficiently compute plans that maximize the probability of reaching goal states. Moreover, we account for numerical imprecision in computing the abstraction by extending the MDP with intervals of probabilities as a more robust model. We show the correctness of the abstraction and provide several optimizations that aim to balance the quality of the plan and the scalability of the approach. We demonstrate that our method can handle systems that result in MDPs with thousands of states and millions of transitions.      
### 40.Continuous Speech Separation with Ad Hoc Microphone Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2103.02378.pdf)
>  Speech separation has been shown effective for multi-talker speech recognition. Under the ad hoc microphone array setup where the array consists of spatially distributed asynchronous microphones, additional challenges must be overcome as the geometry and number of microphones are unknown beforehand. Prior studies show, with a spatial-temporalinterleaving structure, neural networks can efficiently utilize the multi-channel signals of the ad hoc array. In this paper, we further extend this approach to continuous speech separation. Several techniques are introduced to enable speech separation for real continuous recordings. First, we apply a transformer-based network for spatio-temporal modeling of the ad hoc array signals. In addition, two methods are proposed to mitigate a speech duplication problem during single talker segments, which seems more severe in the ad hoc array scenarios. One method is device distortion simulation for reducing the acoustic mismatch between simulated training data and real recordings. The other is speaker counting to detect the single speaker segments and merge the output signal channels. Experimental results for AdHoc-LibiCSS, a new dataset consisting of continuous recordings of concatenated LibriSpeech utterances obtained by multiple different devices, show the proposed separation method can significantly improve the ASR accuracy for overlapped speech with little performance degradation for single talker segments.      
### 41.Terahertz-Band MIMO-NOMA: Adaptive Superposition Coding and Subspace Detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.02348.pdf)
>  We consider the problem of efficient ultra-massive multiple-input multiple-output (UM-MIMO) data detection in terahertz (THz)-band non-orthogonal multiple access (NOMA) systems. We argue that the most common THz NOMA configuration is power-domain superposition coding over quasi-optical doubly-massive MIMO channels. We propose spatial tuning techniques that modify antenna subarray arrangements to enhance channel conditions. Towards recovering the superposed data at the receiver side, we propose a family of data detectors based on low-complexity channel matrix puncturing, in which higher-order detectors are dynamically formed from lower-order component detectors. We first detail the proposed solutions for the case of superposition coding of multiple streams in point-to-point THz MIMO links. We then extend the study to multi-user NOMA, in which randomly distributed users get grouped into narrow cell sectors and are allocated different power levels depending on their proximity to the base station. We show that successive interference cancellation is carried with minimal performance and complexity costs under spatial tuning. We derive approximate bit error rate (BER) equations, and we propose an architectural design to illustrate complexity reductions. Under typical THz conditions, channel puncturing introduces more than an order of magnitude reduction in BER at high signal-to-noise ratios while reducing complexity by approximately 90%.      
### 42.Application of NOMA in 6G Networks: Future Vision and Research Opportunities for Next Generation Multiple Access  [ :arrow_down: ](https://arxiv.org/pdf/2103.02334.pdf)
>  As a prominent member of the next generation multiple access (NGMA) family, non-orthogonal multiple access (NOMA) has been recognized as a promising multiple access candidate for the sixth-generation (6G) networks. This article focuses on applying NOMA in 6G networks, with an emphasis on proposing the so-called "One Basic Principle plus Four New" concept. Starting with the basic NOMA principle, the importance of successive interference cancellation (SIC) becomes evident. In particular, the advantages and drawbacks of both the channel state information based SIC and quality-of-service based SIC are discussed. Then, the application of NOMA to meet the new 6G performance requirements, especially for massive connectivity, is explored. Furthermore, the integration of NOMA with new physical layer techniques is considered, followed by introducing new application scenarios for NOMA towards 6G. Finally, the application of machine learning in NOMA networks is investigated, ushering in the machine learning empowered NGMA era.      
### 43.K-means Segmentation Based-on Lab Color Space for Embryo Egg Detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.02288.pdf)
>  The hatching process also influences the success of hatching eggs beside the initial egg factor. So that the results have a large percentage of hatching, it is necessary to check the development of the embryo at the beginning of the hatching. This process aims to sort eggs that have embryos to remain hatched until the end. Maximum checking is done the first week in the hatching period. This study aims to detect the presence of embryos in eggs. Detection of the existence of embryos is processed using segmentation. Egg images are segmented using the K-means algorithm based on Lab color images. The results of the images acquisition are converted into Lab color space images. The results of Lab color space images are processed using K-means for each color. The K-means process uses cluster k=3, where this cluster divided the image into three parts, namely background, eggs, and yolk eggs. Yolk eggs are part of eggs that have embryonic characteristics. This study applies the concept of color in the initial segmentation and grayscale in the final stages. The results of the initial phase show that the image segmentation results using k-means clustering based on Lab color space provide a grouping of three parts. At the grayscale image processing stage, the results of color image segmentation are processed with grayscaling, image enhancement, and morphology. Thus, it seems clear that the yolk segmented shows the presence of egg embryos. Based on this process and results, K-means segmentation based on Lab color space can be used for the initial stages of the embryo detection process. The evaluation uses MSE and MSSIM, with values of 0.0486 and 0.9979; this can be used as a reference that the results obtained can indicate the detection of embryos in egg yolk.      
### 44.The Ultimate Weapon for Ultra-Broadband 6G: Digital Beamforming and Doubly Massive mmWave MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2103.02286.pdf)
>  The use of millimeter waves for wireless communications is one of the main technological innovations of 5G systems with respect to previous generations of cellular systems. Their consideration, however, has been up to now mainly restricted to the case in which analog or, at most, hybrid analog-digital beamforming structures were used, thus posing a limitation on the multiplexing capabilities and peak data rates that could be theoretically achieved at these frequencies. Recent progress in the field of electronics, however, has made the energy consumption of digital beamforming structures at least on par with that of analog beamforming, thus redeeming them from the ghetto they had been placed in over the last year. Digital beamforming, coupled with the use of large antenna arrays at both sides of the communication link, promises thus to be one of the secret weapons of future 6G networks, capable of unleashing unprecedented values of spectral and energy efficiency for ultra-broadband connectivity.      
### 45.Predicting Driver Fatigue in Automated Driving with Explainability  [ :arrow_down: ](https://arxiv.org/pdf/2103.02162.pdf)
>  Research indicates that monotonous automated driving increases the incidence of fatigued driving. Although many prediction models based on advanced machine learning techniques were proposed to monitor driver fatigue, especially in manual driving, little is known about how these black-box machine learning models work. In this paper, we proposed a combination of eXtreme Gradient Boosting (XGBoost) and SHAP (SHapley Additive exPlanations) to predict driver fatigue with explanations due to their efficiency and accuracy. First, in order to obtain the ground truth of driver fatigue, PERCLOS (percentage of eyelid closure over the pupil over time) between 0 and 100 was used as the response variable. Second, we built a driver fatigue regression model using both physiological and behavioral measures with XGBoost and it outperformed other selected machine learning models with 3.847 root-mean-squared error (RMSE), 1.768 mean absolute error (MAE) and 0.996 adjusted $R^2$. Third, we employed SHAP to identify the most important predictor variables and uncovered the black-box XGBoost model by showing the main effects of most important predictor variables globally and explaining individual predictions locally. Such an explainable driver fatigue prediction model offered insights into how to intervene in automated driving when necessary, such as during the takeover transition period from automated driving to manual driving.      
### 46.Sensing population distribution from satellite imagery via deep learning: model selection, neighboring effect, and systematic biases  [ :arrow_down: ](https://arxiv.org/pdf/2103.02155.pdf)
>  The rapid development of remote sensing techniques provides rich, large-coverage, and high-temporal information of the ground, which can be coupled with the emerging deep learning approaches that enable latent features and hidden geographical patterns to be extracted. This study marks the first attempt to cross-compare performances of popular state-of-the-art deep learning models in estimating population distribution from remote sensing images, investigate the contribution of neighboring effect, and explore the potential systematic population estimation biases. We conduct an end-to-end training of four popular deep learning architectures, i.e., VGG, ResNet, Xception, and DenseNet, by establishing a mapping between Sentinel-2 image patches and their corresponding population count from the LandScan population grid. The results reveal that DenseNet outperforms the other three models, while VGG has the worst performances in all evaluating metrics under all selected neighboring scenarios. As for the neighboring effect, contradicting existing studies, our results suggest that the increase of neighboring sizes leads to reduced population estimation performance, which is found universal for all four selected models in all evaluating metrics. In addition, there exists a notable, universal bias that all selected deep learning models tend to overestimate sparsely populated image patches and underestimate densely populated image patches, regardless of neighboring sizes. The methodological, experimental, and contextual knowledge this study provides is expected to benefit a wide range of future studies that estimate population distribution via remote sensing imagery.      
### 47.Energy and Cost Efficient Resource Allocation for Blockchain-Enabled NFV  [ :arrow_down: ](https://arxiv.org/pdf/2103.02139.pdf)
>  Network function virtualization (NFV) is a promising technology to make 5G networks flexible and agile. NFV decreases operators' OPEX and CAPEX by decoupling the physical hardware from the functions they perform. In NFV, users' service request can be viewed as a service function chain (SFC) consisting of several virtual network functions (VNFs) which are connected through virtual links. Resource allocation in NFV is done through a centralized authority called NFV Orchestrator (NFVO). This centralized authority suffers from some drawbacks such as single point of failure and security. Blockchain (BC) technology is able to address these problems by decentralizing resource allocation. The drawbacks of NFVO in NFV architecture and the exceptional BC characteristics to address these problems motivate us to focus on NFV resource allocation to users' SFCs without the need for an NFVO based on BC technology. To this end, we assume there are two types of users: users who send SFC requests (SFC requesting users) and users who perform mining process (miner users). For SFC requesting users, we formulate NFV resource allocation (NFV-RA) problem as a multi-objective problem to minimize the energy consumption and utilized resource cost, simultaneously.      
### 48.QoS-Driven Resource Optimization for Intelligent Fog Radio Access Network: A Dynamic Power Allocation Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2103.02134.pdf)
>  The fog radio access network (Fog-RAN) has been considered a promising wireless access architecture to help shorten the communication delay and relieve the large data delivery burden over the backhaul links. However, limited by conventional inflexible communication design, Fog-RAN cannot be used in some complex communication scenarios. In this study, we focus on investigating a more intelligent Fog-RAN to assist the communication in a high-speed railway environment. Due to the train's continuously moving, the communication should be designed intelligently to adapt to channel variation. Specifically, we dynamically optimize the power allocation in the remote radio heads (RRHs) to minimize the total network power cost considering multiple quality-of-service (QoS) requirements and channel variation. The impact of caching on the power allocation is considered. The dynamic power optimization is analyzed to obtain a closed-form solution in certain cases. The inherent tradeoff among the total network cost, delay and delivery content size is further discussed. To evaluate the performance of the proposed dynamic power allocation, we present an invariant power allocation counterpart as a performance comparison benchmark. The result of our simulation reveals that dynamic power allocation can significantly outperform the invariant power allocation scheme, especially with a random caching strategy or limited caching resources at the RRHs.      
### 49.A Computational Design and Evaluation Tool for 3D Structures with Planar Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2103.02114.pdf)
>  Three dimensional (3D) structures composed of planar surfaces can be build out of accessible materials using easier fabrication technique with shorter fabrication time. To better design 3D structures with planar surfaces, realistic models are required to understand and evaluate mechanical behaviors. Existing design tools are either effort-consuming (e.g. finite element analysis) or bounded by assumptions (e.g. numerical solutions). In this project, We have built a computational design tool that is (1) capable of rapidly and inexpensively evaluating planar surfaces in 3D structures, with sufficient computational efficiency and accuracy; (2) applicable to complex boundary conditions and loading conditions, both isotropic materials and orthotropic materials; and (3) suitable for rapid accommodation when design parameters need to be adjusted. We demonstrate the efficiency and necessity of this design tool by evaluating a glass table as well as a wood bookcase, and iteratively designing an origami gripper to satisfy performance requirements. This design tool gives non-expert users as well as engineers a simple and effective modus operandi in structural design.      
### 50.Geospatial Transformations for Ground-Based Sky Imaging Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.02066.pdf)
>  Sky imaging systems use lenses to acquire images concentrating light beams in an imager. The light beams received by the imager have an elevation angle with respect to the normal of the device. This produces that the pixels in an image contain information from different areas of the sky within imaging system Field Of View (FOV). The area of the field of view contained in the pixels increases as the elevation angle of the incident light beams decreases. When the sky imaging system are mounted on a solar tracker the angle of incidence of the light beams varies along time. This investigation introduces a transformation that projects the original euclidean frame of the plane of the imager to the geospatial frame of the sky imaging system field of view.      
### 51.A 3D Printing Hexacopter: Design and Demonstration  [ :arrow_down: ](https://arxiv.org/pdf/2103.02063.pdf)
>  3D printing using robots has garnered significant interest in manufacturing and construction in recent years. A robot's versatility paired with the design freedom of 3D printing offers promising opportunities for how parts and structures are built in the future. However, 3D printed objects are still limited in size and location due to a lack of vertical mobility of ground robots. These limitations severely restrict the potential of the 3D printing process. To overcome these limitations, we develop a hexacopter testbed that can print via fused deposition modeling during flight. We discuss the design of this testbed and develop a simple control strategy for initial print tests. By successfully performing these initial print tests, we demonstrate the feasibility of this approach and lay the groundwork for printing 3D parts and structures with drones.      
### 52.XNAT-PIC: Extending XNAT to Preclinical Imaging Centers  [ :arrow_down: ](https://arxiv.org/pdf/2103.02044.pdf)
>  Molecular imaging generates large volumes of heterogeneous biomedical imagery with an impelling need of guidelines for handling image data. Although several successful solutions have been implemented for human epidemiologic studies, few and limited approaches have been proposed for animal population studies. Preclinical imaging research deals with a variety of machinery yielding tons of raw data but the current practices to store and distribute image data are inadequate. Therefore, standard tools for the analysis of large image datasets need to be established. In this paper, we present an extension of XNAT for Preclinical Imaging Centers (XNAT-PIC). XNAT is a worldwide used, open-source platform for securely hosting, sharing, and processing of clinical imaging studies. Despite its success, neither tools for importing large, multimodal preclinical image datasets nor pipelines for processing whole imaging studies are yet available in XNAT. In order to overcome these limitations, we have developed several tools to expand the XNAT core functionalities for supporting preclinical imaging facilities. Our aim is to streamline the management and exchange of image data within the preclinical imaging community, thereby enhancing the reproducibility of the results of image processing and promoting open science practices.      
### 53.Ultrasound Matrix Imaging. I. The focused reflection matrix and the F-factor  [ :arrow_down: ](https://arxiv.org/pdf/2103.02029.pdf)
>  This is the first article in a series of two dealing with a matrix approach \alex{for} aberration quantification and correction in ultrasound imaging. Advanced synthetic beamforming relies on a double focusing operation at transmission and reception on each point of the medium. Ultrasound matrix imaging (UMI) consists in decoupling the location of these transmitted and received focal spots. The response between those virtual transducers form the so-called focused reflection matrix that actually contains much more information than a raw ultrasound image. In this paper, a time-frequency analysis of this matrix is performed, which highlights the single and multiple scattering contributions as well as the impact of aberrations in the monochromatic and broadband regimes. Interestingly, this analysis enables the measurement of the incoherent input-output point spread function at any pixel of this image. A focusing criterion can then be built, and its evolution used to quantify the amount of aberration throughout the ultrasound image. In contrast to the standard coherence factor used in the literature, this new indicator is robust to multiple scattering and electronic noise, thereby providing a highly contrasted map of the focusing quality. As a proof-of-concept, UMI is applied here to the in-vivo study of a human calf, but it can be extended to any kind of ultrasound diagnosis or non-destructive evaluation.      
### 54.PECNet: A Deep Multi-Label Segmentation Network for Eosinophilic Esophagitis Biopsy Diagnostics  [ :arrow_down: ](https://arxiv.org/pdf/2103.02015.pdf)
>  Background. Eosinophilic esophagitis (EoE) is an allergic inflammatory condition of the esophagus associated with elevated numbers of eosinophils. Disease diagnosis and monitoring requires determining the concentration of eosinophils in esophageal biopsies, a time-consuming, tedious and somewhat subjective task currently performed by pathologists. Methods. Herein, we aimed to use machine learning to identify, quantitate and diagnose EoE. We labeled more than 100M pixels of 4345 images obtained by scanning whole slides of H&amp;E-stained sections of esophageal biopsies derived from 23 EoE patients. We used this dataset to train a multi-label segmentation deep network. To validate the network, we examined a replication cohort of 1089 whole slide images from 419 patients derived from multiple institutions. Findings. PECNet segmented both intact and not-intact eosinophils with a mean intersection over union (mIoU) of 0.93. This segmentation was able to quantitate intact eosinophils with a mean absolute error of 0.611 eosinophils and classify EoE disease activity with an accuracy of 98.5%. Using whole slide images from the validation cohort, PECNet achieved an accuracy of 94.8%, sensitivity of 94.3%, and specificity of 95.14% in reporting EoE disease activity. Interpretation. We have developed a deep learning multi-label semantic segmentation network that successfully addresses two of the main challenges in EoE diagnostics and digital pathology, the need to detect several types of small features simultaneously and the ability to analyze whole slides efficiently. Our results pave the way for an automated diagnosis of EoE and can be utilized for other conditions with similar challenges.      
