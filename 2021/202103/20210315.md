# ArXiv eess --Mon, 15 Mar 2021
### 1.Hyperspectral Image Denoising and Anomaly Detection Based on Low-rank and Sparse Representations  [ :arrow_down: ](https://arxiv.org/pdf/2103.07437.pdf)
>  Hyperspectral imaging measures the amount of electromagnetic energy across the instantaneous field of view at a very high resolution in hundreds or thousands of spectral channels. This enables objects to be detected and the identification of materials that have subtle differences between them. However, the increase in spectral resolution often means that there is a decrease in the number of photons received in each channel, which means that the noise linked to the image formation process is greater. This degradation limits the quality of the extracted information and its potential applications. Thus, denoising is a fundamental problem in hyperspectral image (HSI) processing. As images of natural scenes with highly correlated spectral channels, HSIs are characterized by a high level of self-similarity and can be well approximated by low-rank representations. These characteristics underlie the state-of-the-art methods used in HSI denoising. However, where there are rarely occurring pixel types, the denoising performance of these methods is not optimal, and the subsequent detection of these pixels may be compromised. To address these hurdles, in this article, we introduce RhyDe (Robust hyperspectral Denoising), a powerful HSI denoiser, which implements explicit low-rank representation, promotes self-similarity, and, by using a form of collaborative sparsity, preserves rare pixels. The denoising and detection effectiveness of the proposed robust HSI denoiser is illustrated using semireal and real data.      
### 2.Radiomic Deformation and Textural Heterogeneity (R-DepTH) Descriptor to characterize Tumor Field Effect: Application to Survival Prediction in Glioblastoma  [ :arrow_down: ](https://arxiv.org/pdf/2103.07423.pdf)
>  The concept of tumor field effect implies that cancer is a systemic disease with its impact way beyond the visible tumor confines. For instance, in Glioblastoma (GBM), an aggressive brain tumor, the increase in intracranial pressure due to tumor burden often leads to brain herniation and poor outcomes. Our work is based on the rationale that highly aggressive tumors tend to grow uncontrollably, leading to pronounced biomechanical tissue deformations in the normal parenchyma, which when combined with local morphological differences in the tumor confines on MRI scans, will comprehensively capture tumor field effect. Specifically, we present an integrated MRI-based descriptor, radiomic-Deformation and Textural Heterogeneity (r-DepTH). This descriptor comprises measurements of the subtle perturbations in tissue deformations throughout the surrounding normal parenchyma due to mass effect. This involves non-rigidly aligning the patients MRI scans to a healthy atlas via diffeomorphic registration. The resulting inverse mapping is used to obtain the deformation field magnitudes in the normal parenchyma. These measurements are then combined with a 3D texture descriptor, Co-occurrence of Local Anisotropic Gradient Orientations (COLLAGE), which captures the morphological heterogeneity within the tumor confines, on MRI scans. R-DepTH, on N = 207 GBM cases (training set (St) = 128, testing set (Sv) = 79), demonstrated improved prognosis of overall survival by categorizing patients into low- (prolonged survival) and high-risk (poor survival) groups (on St, p-value = 0.0000035, and on Sv, p-value = 0.0024). R-DepTH descriptor may serve as a comprehensive MRI-based prognostic marker of disease aggressiveness and survival in solid tumors.      
### 3.Despeckling Polarimetric SAR Data Using a Multi-Stream Complex-Valued Fully Convolutional Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.07394.pdf)
>  A Polarimetric Synthetic Aperture Radar (PolSAR) sensor is able to collect images in different polarization states, making it a rich source of information for target characterization. PolSAR images are inherently affected by speckle. Therefore, before deriving ad hoc products from the data, the polarimetric covariance matrix needs to be estimated by reducing speckle. In recent years, deep learning based despeckling methods have started to evolve from single channel SAR images to PolSAR images. To this aim, deep learning based approaches separate the real and imaginary components of the complex-valued covariance matrix and use them as independent channels in a standard convolutional neural networks. However, this approach neglects the mathematical relationship that exists between the real and imaginary components, resulting in sub-optimal output. Here, we propose a multi-stream complex-valued fully convolutional network to reduce speckle and effectively estimate the PolSAR covariance matrix. To evaluate the performance of CV-deSpeckNet, we used Sentinel-1 dual polarimetric SAR images to compare against its real-valued counterpart, that separates the real and imaginary parts of the complex covariance matrix. CV-deSpeckNet was also compared against the state of the art PolSAR despeckling methods. The results show CV-deSpeckNet was able to be trained with a fewer number of samples, has a higher generalization capability and resulted in a higher accuracy than its real-valued counterpart and state-of-the-art PolSAR despeckling methods. These results showcase the potential of complex-valued deep learning for PolSAR despeckling.      
### 4.Signal Representations for Synthesizing Audio Textures with Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.07390.pdf)
>  Generative Adversarial Networks (GANs) currently achieve the state-of-the-art sound synthesis quality for pitched musical instruments using a 2-channel spectrogram representation consisting of log magnitude and instantaneous frequency (the "IFSpectrogram"). Many other synthesis systems use representations derived from the magnitude spectra, and then depend on a backend component to invert the output magnitude spectrograms that generally result in audible artefacts associated with the inversion process. However, for signals that have closely-spaced frequency components such as non-pitched and other noisy sounds, training the GAN on the 2-channel IFSpectrogram representation offers no advantage over the magnitude spectra based representations. In this paper, we propose that training GANs on single-channel magnitude spectra, and using the Phase Gradient Heap Integration (PGHI) inversion algorithm is a better comprehensive approach for audio synthesis modeling of diverse signals that include pitched, non-pitched, and dynamically complex sounds. We show that this method produces higher-quality output for wideband and noisy sounds, such as pops and chirps, compared to using the IFSpectrogram. Furthermore, the sound quality for pitched sounds is comparable to using the IFSpectrogram, even while using a simpler representation with half the memory requirements.      
### 5.Smooth nonnegative tensor factorization for multi-sites electrical load monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2103.07308.pdf)
>  The analysis of load curves collected from smart meters is a key step for many energy management tasks ranging from consumption forecasting to customers characterization and load monitoring. In this contribution, we propose a model based on a functional formulation of nonnegative tensor factorization and derive updates for the corresponding optimization problem. We show on the concrete example of multi-sites load curves disaggregation how this formulation is helpful for 1) exhibiting smooth intraday consumption patterns and 2) taking into account external variables such as the outside temperature. The benefits are demonstrated on simulated and real data by exhibiting a meaningful clustering of the observed sites based on the obtained decomposition.      
### 6.The Enhanced Parameter Estimation (EPE) -- A New Calibration Methodology for Building Energy Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2103.07283.pdf)
>  Buildings rarely perform as designed/simulated and and there are numerous tangible benefits if this gap is reconciled. A new scientific yet pragmatic methodology - called Enhanced Parameter Estimation (EPE) - is proposed that allows physically relevant parameter estimation rather than a blind force-fit to energy use data. It calibrates a rapidly and inexpensively created simulation model of the building in two stages: (a) building shell calibration with the HVAC system is replaced by an by an ideal system that meets the loads. EPE identifies a small number of high-level heat flows in the energy balance, calculates them with specifically tailored individual driving functions, introduces physically significant parameters to best accomplish energy balance, and, estimates the parameters and their uncertainty bounds. Calibration is thus done with corrective heat flows without any arbitrary tuning of input parameters, (b) HVAC system calibration with the building shell replaced by a box with only process loads; as many parameters as the data allows are estimated. Calibration accuracy is enhanced by machine learning of the residual errors. The EPE methodology is demonstrated through: a synthetic building and one an actual 75,000 Sq.Ft. building in Pennsylvania. A subsequent paper will provide further details and applications.      
### 7.Proof that the Kalman gain minimizes the generalized variance  [ :arrow_down: ](https://arxiv.org/pdf/2103.07275.pdf)
>  The optimal gain matrix of the Kalman filter is often derived by minimizing the trace of the posterior covariance matrix. Here, I show that the Kalman gain also minimizes the determinant of the covariance matrix, a quantity known as the generalized variance. When the error distributions are Gaussian, the differential entropy is also minimized.      
### 8.Fish Growth Trajectory Tracking via Reinforcement Learning in Precision Aquaculture  [ :arrow_down: ](https://arxiv.org/pdf/2103.07251.pdf)
>  This paper studies the fish growth trajectory tracking via reinforcement learning under a representative bioenergetic growth model. Due to the complex aquaculture condition and uncertain environmental factors such as temperature, dissolved oxygen, un-ionized ammonia, and strong nonlinear couplings, including multi-inputs of the fish growth model, the growth trajectory tracking problem can not be efficiently solved by the model-based control approaches in precision aquaculture. To this purpose, we formulate the growth trajectory tracking problem as sampled-data optimal control using discrete state-action pairs Markov decision process. We propose two Q-learning algorithms that learn the optimal control policy from the sampled data of the fish growth trajectories at every stage of the fish life cycle from juveniles to the desired market weight in the aquaculture environment. The Q-learning scheme learns the optimal feeding control policy to fish growth rate cultured in cages and the optimal feeding rate control policy with an optimal temperature profile for the aquaculture fish growth rate in tanks. The simulation results demonstrate that both Q-learning strategies achieve high trajectory tracking performance with less amount feeding rates.      
### 9.Longitudinal Quantitative Assessment of COVID-19 Infection Progression from Chest CTs  [ :arrow_down: ](https://arxiv.org/pdf/2103.07240.pdf)
>  Chest computed tomography (CT) has played an essential diagnostic role in assessing patients with COVID-19 by showing disease-specific image features such as ground-glass opacity and consolidation. Image segmentation methods have proven to help quantify the disease burden and even help predict the outcome. The availability of longitudinal CT series may also result in an efficient and effective method to reliably assess the progression of COVID-19, monitor the healing process and the response to different therapeutic strategies. In this paper, we propose a new framework to identify infection at a voxel level (identification of healthy lung, consolidation, and ground-glass opacity) and visualize the progression of COVID-19 using sequential low-dose non-contrast CT scans. In particular, we devise a longitudinal segmentation network that utilizes the reference scan information to improve the performance of disease identification. Experimental results on a clinical longitudinal dataset collected in our institution show the effectiveness of the proposed method compared to the static deep neural networks for disease quantification.      
### 10.Low-complexity graph-based traveling wave models for HVDC grids with hybrid transmission lines: Application to fault identification  [ :arrow_down: ](https://arxiv.org/pdf/2103.07194.pdf)
>  The fast protection of meshed HVDC grids requires the modeling of the transient phenomena affecting the grid after a fault. In the case of hybrid lines comprising both overhead and underground parts, the numerous generated traveling waves may be difficult to describe and evaluate. This paper proposes a representation of the grid as a graph, allowing to take into account any waves traveling through the grid. A relatively compact description of the waves is then derived, based on a combined physical and behavioral modeling approach. The obtained model depends explicitly on the characteristics of the grid as well as on the fault parameters. An application of the model to the identification of the faulty portion of an hybrid line is proposed. The knowledge of the faulty portion is profitable as faults in overhead lines, generally temporary, can lead to the reclosing of the line.      
### 11.Dynamic Acoustic Unit Augmentation With BPE-Dropout for Low-Resource End-to-End Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2103.07186.pdf)
>  With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. Researchers and industry prefer to use end-to-end ASR systems for on-device speech recognition tasks. This is because end-to-end systems can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Another challenging task associated with speech assistants is personalization, which mainly lies in handling out-of-vocabulary (OOV) words. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. To address the aforementioned problems, we propose a method of dynamic acoustic unit augmentation based on the BPE-dropout technique. It non-deterministically tokenizes utterances to extend the token's contexts and to regularize their distribution for the model's recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative WER and 25% relative F-score) at no additional computational cost. Owing to the use of BPE-dropout, our monolingual Turkish Conformer established a competitive result with 22.2% character error rate (CER) and 38.9% word error rate (WER), which is close to the best published multilingual system.      
### 12.Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2103.07152.pdf)
>  In coded aperture snapshot spectral imaging (CASSI) system, the real-world hyperspectral image (HSI) can be reconstructed from the captured compressive image in a snapshot. Model-based HSI reconstruction methods employed hand-crafted priors to solve the reconstruction problem, but most of which achieved limited success due to the poor representation capability of these hand-crafted priors. Deep learning based methods learning the mappings between the compressive images and the HSIs directly achieved much better results. Yet, it is nontrivial to design a powerful deep network heuristically for achieving satisfied results. In this paper, we propose a novel HSI reconstruction method based on the Maximum a Posterior (MAP) estimation framework using learned Gaussian Scale Mixture (GSM) prior. Different from existing GSM models using hand-crafted scale priors (e.g., the Jeffrey's prior), we propose to learn the scale prior through a deep convolutional neural network (DCNN). Furthermore, we also propose to estimate the local means of the GSM models by the DCNN. All the parameters of the MAP estimation algorithm and the DCNN parameters are jointly optimized through end-to-end training. Extensive experimental results on both synthetic and real datasets demonstrate that the proposed method outperforms existing state-of-the-art methods. The code is available at <a class="link-external link-https" href="https://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm" rel="external noopener nofollow">this https URL</a>.      
### 13.Securing Fresh Data in Wireless Monitoring Networks: Age-of-Information Sensitive Coverage Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2103.07149.pdf)
>  With the development of IoT, the sensor usage has been elevated to a new level, and it becomes more crucial to maintain reliable sensor networks. In this paper, we provide how to efficiently and reliably manage the sensor monitoring system for securing fresh data at the data center (DC). A sensor transmits its sensing information regularly to the DC, and the freshness of the information at the DC is characterized by the age of information (AoI) that quantifies the timeliness of information. By considering the effect of the AoI and the spatial distance from the sensor on the information error at the DC, we newly define an error-tolerable sensing (ETS) coverage as the area that the estimated information is with smaller error than the target value. We then derive the average AoI and the AoI violation probability of the sensor monitoring system, and finally present the {\eta}-coverage probability, which is the probability that the ETS coverage is greater than {\eta} ratio of the maximum sensor coverage. We also provide the optimal transmission power of the sensor, which minimizes the average energy consumption while guaranteeing certain level of the {\eta}-coverage probability. Numerical results validate the theoretical analysis and show the tendency of the optimal transmission power according to the maximum number of retransmissions. This paper can pave the way to efficient design of the AoI-sensitive sensor networks for IoT.      
### 14.Synthesis of Covert Sensor Attacks in Networked Discrete-Event Systems with Non-FIFO Channels  [ :arrow_down: ](https://arxiv.org/pdf/2103.07132.pdf)
>  In this paper, we investigate the covert sensor attack synthesis problem in the framework of supervisory control of networked discrete-event systems (DES), where the observation channel and the control channel are assumed to be non-FIFO and have bounded network delays. We focus on the class of sensor attacks satisfying the following properties: 1) the attacker might not have the same observation capability as the networked supervisor; 2) the attacker aims to remain covert, i.e., hide its presence against the networked monitor; 3) the attacker could insert, delete, or replace compromised observable events; 4) it performs bounded sensor attacks, i.e., the length of each string output of the sensor attacker is upper bounded by a given constant. The solution methodology proposed in this work is to solve the covert sensor attack synthesis problem for networked DES by modeling it as the well studied Ramadge-Wonham supervisor synthesis problem, and the constructions work for both the damage-reachable attacks and the damage-nonblocking attacks. In particular, we show the supremal covert sensor attack exists in the networked setup and can be effectively computed by using the normality property based synthesis approach.      
### 15.Severity Quantification and Lesion Localization of COVID-19 on CXR using Vision Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2103.07062.pdf)
>  Under the global pandemic of COVID-19, building an automated framework that quantifies the severity of COVID-19 and localizes the relevant lesion on chest X-ray images has become increasingly important. Although pixel-level lesion severity labels, e.g. lesion segmentation, can be the most excellent target to build a robust model, collecting enough data with such labels is difficult due to time and labor-intensive annotation tasks. Instead, array-based severity labeling that assigns integer scores on six subdivisions of lungs can be an alternative choice enabling the quick labeling. Several groups proposed deep learning algorithms that quantify the severity of COVID-19 using the array-based COVID-19 labels and localize the lesions with explainability maps. To further improve the accuracy and interpretability, here we propose a novel Vision Transformer tailored for both quantification of the severity and clinically applicable localization of the COVID-19 related lesions. Our model is trained in a weakly-supervised manner to generate the full probability maps from weak array-based labels. Furthermore, a novel progressive self-training method enables us to build a model with a small labeled dataset. The quantitative and qualitative analysis on the external testset demonstrates that our method shows comparable performance with radiologists for both tasks with stability in a real-world application.      
### 16.Two Mirroring And Interpolating Methods To Estimate Peak Position For Symmetric Signals With Single Peak  [ :arrow_down: ](https://arxiv.org/pdf/2103.07059.pdf)
>  Signals with single peak and symmetry property are very common in various fields, such as probability density function of normal distribution. Among the information contained in such signals, peak position is the most important, sometimes even the only parameter concerned. Current methods for peak position estimation are always with low precision and bad noise resistance, perform badly for sparse spectrum. This manuscript proposes two new algorithms that take advantage of symmetry property, conduct mirroring and interpolating operations to condense signal spectrum. From tests done in this paper, these two algorithms indicate outstanding advantages compared with current methods.      
### 17.Vision Transformer for COVID-19 CXR Diagnosis using Chest X-ray Feature Corpus  [ :arrow_down: ](https://arxiv.org/pdf/2103.07055.pdf)
>  Under the global COVID-19 crisis, developing robust diagnosis algorithm for COVID-19 using CXR is hampered by the lack of the well-curated COVID-19 data set, although CXR data with other disease are abundant. This situation is suitable for vision transformer architecture that can exploit the abundant unlabeled data using pre-training. However, the direct use of existing vision transformer that uses the corpus generated by the ResNet is not optimal for correct feature embedding. To mitigate this problem, we propose a novel vision Transformer by using the low-level CXR feature corpus that are obtained to extract the abnormal CXR features. Specifically, the backbone network is trained using large public datasets to obtain the abnormal features in routine diagnosis such as consolidation, glass-grass opacity (GGO), etc. Then, the embedded features from the backbone network are used as corpus for vision transformer training. We examine our model on various external test datasets acquired from totally different institutions to assess the generalization ability. Our experiments demonstrate that our method achieved the state-of-art performance and has better generalization capability, which are crucial for a widespread deployment.      
### 18.An Adaptive Receiver for Underwater Acoustic Full-Duplex Communication with Joint Tracking of the Remote and Self-Interference Channels  [ :arrow_down: ](https://arxiv.org/pdf/2103.07002.pdf)
>  Full-duplex (FD) communication is a promising candidate to address the data rate limitations in underwater acoustic (UWA) channels. Because of transmission at the same time and on the same frequency band, the signal from the local transmitter creates self-interference (SI) that contaminates the signal from the remote transmitter. At the local receiver, channel state information for both the SI and remote channels is required to remove the SI and equalize the SI-free signal, respectively. However, because of the rapid time-variations of the UWA environment, real-time tracking of the channels is necessary. In this paper, we propose a receiver for UWA-FD communication in which the variations of the SI and remote channels are jointly tracked by using a recursive least squares (RLS) algorithm fed by feedback from the previously detected data symbols. Because of the joint channel estimation, SI cancellation is more successful compared to UWA-FD receivers with separate channel estimators. In addition, due to providing a real-time channel tracking without the need for frequent training sequences, the bandwidth efficiency is preserved in the proposed receiver.      
### 19.Low Frequency AC Transmission Upgrades with Optimal Frequency Selection  [ :arrow_down: ](https://arxiv.org/pdf/2103.06996.pdf)
>  The advantages of operating parts of the electric grid at frequencies other than the standard 50 or 60 Hz are numerous, encompassing increased power transfer capacity and better utilization of existing infrastructure. While high voltage DC (HVDC) is by far the most well-established example, there has been an emerging interest low frequency AC (LFAC) transmission in applications ranging from offshore wind to railway systems and mining. In this paper, we investigate the use of LFAC as a transmission upgrade and propose models and analysis methods to determine the optimal choice of frequency. The paper first presents an optimal power flow model with frequency as a variable, assuming modular multilevel converters for frequency conversion. Using this model, we analyze LFAC as an embedded upgrade in a transmission system using existing lines. We quantify the system-wide advantages from improved power flow control and frequency reduction and find that an LFAC upgrade achieves similar and sometimes better results compared with HVDC upgrades. Finally, we analyze the factors which determine the optimal transmission frequency, and we demonstrate the benefits of changing the frequency in response to different system topologies and operating conditions.      
### 20.Adversarial attacks in consensus-based multi-agent reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.06967.pdf)
>  Recently, many cooperative distributed multi-agent reinforcement learning (MARL) algorithms have been proposed in the literature. In this work, we study the effect of adversarial attacks on a network that employs a consensus-based MARL algorithm. We show that an adversarial agent can persuade all the other agents in the network to implement policies that optimize an objective that it desires. In this sense, the standard consensus-based MARL algorithms are fragile to attacks.      
### 21.Heuristics for optimizing 3D mapping missions over swarm-powered ad hoc clouds  [ :arrow_down: ](https://arxiv.org/pdf/2103.06953.pdf)
>  Drones have been getting more and more popular in many economy sectors. Both scientific and industrial communities aim at making the impact of drones even more disruptive by empowering collaborative autonomous behaviors -- also known as swarming behaviors -- within fleets of multiple drones. In swarming-powered 3D mapping missions, unmanned aerial vehicles typically collect the aerial pictures of the target area whereas the 3D reconstruction process is performed in a centralized manner. However, such approaches do not leverage computational and storage resources from the swarm members.We address the optimization of a swarm-powered distributed 3D mapping mission for a real-life humanitarian emergency response application through the exploitation of a swarm-powered ad hoc cloud. Producing the relevant 3D maps in a timely manner, even when the cloud connectivity is not available, is crucial to increase the chances of success of the operation. In this work, we present a mathematical programming heuristic based on decomposition and a variable neighborhood search heuristic to minimize the completion time of the 3D reconstruction process necessary in such missions. Our computational results reveal that the proposed heuristics either quickly reach optimality or improve the best known solutions for almost all tested realistic instances comprising up to 1000 images and fifteen drones.      
### 22.Development of Multifractal Models for Self-Similar Traffic Flows  [ :arrow_down: ](https://arxiv.org/pdf/2103.06946.pdf)
>  This paper presents a simple technique of multifractal traffic modeling. It proposes a method of fitting model to a given traffic trace. A comparison of simulation results obtained for an exemplary trace, multifractal model and Markov Modulated Poisson Process models has been performed.      
### 23.Multiview Sensing With Unknown Permutations: An Optimal Transport Approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.07458.pdf)
>  In several applications, including imaging of deformable objects while in motion, simultaneous localization and mapping, and unlabeled sensing, we encounter the problem of recovering a signal that is measured subject to unknown permutations. In this paper we take a fresh look at this problem through the lens of optimal transport (OT). In particular, we recognize that in most practical applications the unknown permutations are not arbitrary but some are more likely to occur than others. We exploit this by introducing a regularization function that promotes the more likely permutations in the solution. We show that, even though the general problem is not convex, an appropriate relaxation of the resulting regularized problem allows us to exploit the well-developed machinery of OT and develop a tractable algorithm.      
### 24.EventGraD: Event-Triggered Communication in Parallel Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.07454.pdf)
>  Communication in parallel systems imposes significant overhead which often turns out to be a bottleneck in parallel machine learning. To relieve some of this overhead, in this paper, we present EventGraD - an algorithm with event-triggered communication for stochastic gradient descent in parallel machine learning. The main idea of this algorithm is to modify the requirement of communication at every iteration in standard implementations of stochastic gradient descent in parallel machine learning to communicating only when necessary at certain iterations. We provide theoretical analysis of convergence of our proposed algorithm. We also implement the proposed algorithm for data-parallel training of a popular residual neural network used for training the CIFAR-10 dataset and show that EventGraD can reduce the communication load by up to 60% while retaining the same level of accuracy.      
### 25.Generating and Characterizing Scenarios for Safety Testing of Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2103.07403.pdf)
>  Extracting interesting scenarios from real-world data as well as generating failure cases is important for the development and testing of autonomous systems. We propose efficient mechanisms to both characterize and generate testing scenarios using a state-of-the-art driving simulator. For any scenario, our method generates a set of possible driving paths and identifies all the possible safe driving trajectories that can be taken starting at different times, to compute metrics that quantify the complexity of the scenario. We use our method to characterize real driving data from the Next Generation Simulation (NGSIM) project, as well as adversarial scenarios generated in simulation. We rank the scenarios by defining metrics based on the complexity of avoiding accidents and provide insights into how the AV could have minimized the probability of incurring an accident. We demonstrate a strong correlation between the proposed metrics and human intuition.      
### 26.Value of information from vibration-based structural health monitoring extracted via Bayesian model updating  [ :arrow_down: ](https://arxiv.org/pdf/2103.07382.pdf)
>  Quantifying the value of the information extracted from a structural health monitoring (SHM) system is an important step towards convincing decision makers to implement these systems. We quantify this value by adaptation of the Bayesian decision analysis framework. In contrast to previous works, we model in detail the entire process of data generation to processing, model updating and reliability calculation, and investigate it on a deteriorating bridge system. The framework assumes that dynamic response data are obtained in a sequential fashion from deployed accelerometers, subsequently processed by an output-only operational modal analysis scheme for identifying the system's modal characteristics. We employ a classical Bayesian model updating methodology to sequentially learn the deterioration and estimate the structural damage evolution over time. This leads to sequential updating of the structural reliability, which constitutes the basis for a preposterior Bayesian decision analysis. Alternative actions are defined and a heuristic-based approach is employed for the life-cycle optimization. By solving the preposterior Bayesian decision analysis, one is able to quantify the benefit of the availability of long-term SHM vibrational data. Numerical investigations show that this framework can provide quantitative measures on the optimality of an SHM system in a specific decision context.      
### 27.Offset-free Model Predictive Control: A Ball Catching Application with a Spherical Soft Robotic Arm  [ :arrow_down: ](https://arxiv.org/pdf/2103.07379.pdf)
>  This paper presents an offset-free model predictive controller for fast and accurate control of a spherical soft robotic arm. In this control scheme, a linear model is combined with an online disturbance estimation technique to systematically compensate model deviations. Dynamic effects such as material relaxation resulting from the use of soft materials can be addressed to achieve offset-free tracking. The tracking error can be reduced by 35% when compared to a standard model predictive controller without a disturbance compensation scheme. The improved tracking performance enables the realization of a ball catching application, where the spherical soft robotic arm can catch a ball thrown by a human.      
### 28.Patient-specific virtual spine straightening and vertebra inpainting: An automatic framework for osteoplasty planning  [ :arrow_down: ](https://arxiv.org/pdf/2103.07279.pdf)
>  Symptomatic spinal vertebral compression fractures (VCFs) often require osteoplasty treatment. A cement-like material is injected into the bone to stabilize the fracture, restore the vertebral body height and alleviate pain. Leakage is a common complication and may occur due to too much cement being injected. In this work, we propose an automated patient-specific framework that can allow physicians to calculate an upper bound of cement for the injection and estimate the optimal outcome of osteoplasty. The framework uses the patient CT scan and the fractured vertebra label to build a virtual healthy spine using a high-level approach. Firstly, the fractured spine is segmented with a three-step Convolution Neural Network (CNN) architecture. Next, a per-vertebra rigid registration to a healthy spine atlas restores its curvature. Finally, a GAN-based inpainting approach replaces the fractured vertebra with an estimation of its original shape. Based on this outcome, we then estimate the maximum amount of bone cement for injection. We evaluate our framework by comparing the virtual vertebrae volumes of ten patients to their healthy equivalent and report an average error of 3.88$\pm$7.63\%. The presented pipeline offers a first approach to a personalized automatic high-level framework for planning osteoplasty procedures.      
### 29.Modelling Animal Biodiversity Using Acoustic Monitoring and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.07276.pdf)
>  For centuries researchers have used sound to monitor and study wildlife. Traditionally, conservationists have identified species by ear; however, it is now common to deploy audio recording technology to monitor animal and ecosystem sounds. Animals use sound for communication, mating, navigation and territorial defence. Animal sounds provide valuable information and help conservationists to quantify biodiversity. Acoustic monitoring has grown in popularity due to the availability of diverse sensor types which include camera traps, portable acoustic sensors, passive acoustic sensors, and even smartphones. Passive acoustic sensors are easy to deploy and can be left running for long durations to provide insights on habitat and the sounds made by animals and illegal activity. While this technology brings enormous benefits, the amount of data that is generated makes processing a time-consuming process for conservationists. Consequently, there is interest among conservationists to automatically process acoustic data to help speed up biodiversity assessments. Processing these large data sources and extracting relevant sounds from background noise introduces significant challenges. In this paper we outline an approach for achieving this using state of the art in machine learning to automatically extract features from time-series audio signals and modelling deep learning models to classify different bird species based on the sounds they make. The acquired bird songs are processed using mel-frequency cepstrum (MFC) to extract features which are later classified using a multilayer perceptron (MLP). Our proposed method achieved promising results with 0.74 sensitivity, 0.92 specificity and an accuracy of 0.74.      
### 30.Multimodal EEG and Keystroke Dynamics Based Biometric System Using Machine Learning Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2103.07274.pdf)
>  With the rapid advancement of technology, different biometric user authentication, and identification systems are emerging. Traditional biometric systems like face, fingerprint, and iris recognition, keystroke dynamics, etc. are prone to cyber-attacks and suffer from different disadvantages. Electroencephalography (EEG) based authentication has shown promise in overcoming these limitations. However, EEG-based authentication is less accurate due to signal variability at different psychological and physiological conditions. On the other hand, keystroke dynamics-based identification offers high accuracy but suffers from different spoofing attacks. To overcome these challenges, we propose a novel multimodal biometric system combining EEG and keystroke dynamics. Firstly, a dataset was created by acquiring both keystroke dynamics and EEG signals from 10 users with 500 trials per user at 10 different sessions. Different statistical, time, and frequency domain features were extracted and ranked from the EEG signals and key features were extracted from the keystroke dynamics. Different classifiers were trained, validated, and tested for both individual and combined modalities for two different classification strategies - personalized and generalized. Results show that very high accuracy can be achieved both in generalized and personalized cases for the combination of EEG and keystroke dynamics. The identification and authentication accuracies were found to be 99.80% and 99.68% for Extreme Gradient Boosting (XGBoost) and Random Forest classifiers, respectively which outperform the individual modalities with a significant margin (around 5 percent). We also developed a binary template matching-based algorithm, which gives 93.64% accuracy 6X faster. The proposed method is secured and reliable for any kind of biometric authentication.      
### 31.Projection-based QLP Algorithm for Efficiently Computing Low-Rank Approximation of Matrices  [ :arrow_down: ](https://arxiv.org/pdf/2103.07245.pdf)
>  Matrices with low numerical rank are omnipresent in many signal processing and data analysis applications. The pivoted QLP (p-QLP) algorithm constructs a highly accurate approximation to an input low-rank matrix. However, it is computationally prohibitive for large matrices. In this paper, we introduce a new algorithm termed Projection-based Partial QLP (PbP-QLP) that efficiently approximates the p-QLP with high accuracy. Fundamental in our work is the exploitation of randomization and in contrast to the p-QLP, PbP-QLP does not use the pivoting strategy. As such, PbP-QLP can harness modern computer architectures, even better than competing randomized algorithms. The efficiency and effectiveness of our proposed PbP-QLP algorithm are investigated through various classes of synthetic and real-world data matrices.      
### 32.Real-time Timbre Transfer and Sound Synthesis using DDSP  [ :arrow_down: ](https://arxiv.org/pdf/2103.07220.pdf)
>  Neural audio synthesis is an actively researched topic, having yielded a wide range of techniques that leverages machine learning architectures. Google Magenta elaborated a novel approach called Differential Digital Signal Processing (DDSP) that incorporates deep neural networks with preconditioned digital signal processing techniques, reaching state-of-the-art results especially in timbre transfer applications. However, most of these techniques, including the DDSP, are generally not applicable in real-time constraints, making them ineligible in a musical workflow. In this paper, we present a real-time implementation of the DDSP library embedded in a virtual synthesizer as a plug-in that can be used in a Digital Audio Workstation. We focused on timbre transfer from learned representations of real instruments to arbitrary sound inputs as well as controlling these models by MIDI. Furthermore, we developed a GUI for intuitive high-level controls which can be used for post-processing and manipulating the parameters estimated by the neural network. We have conducted a user experience test with seven participants online. The results indicated that our users found the interface appealing, easy to understand, and worth exploring further. At the same time, we have identified issues in the timbre transfer quality, in some components we did not implement, and in installation and distribution of our plugin. The next iteration of our design will address these issues. Our real-time MATLAB and JUCE implementations are available at <a class="link-external link-https" href="https://github.com/SMC704/juce-ddsp" rel="external noopener nofollow">this https URL</a> and <a class="link-external link-https" href="https://github.com/SMC704/matlab-ddsp" rel="external noopener nofollow">this https URL</a> , respectively.      
### 33.Latent Space Explorations of Singing Voice Synthesis using DDSP  [ :arrow_down: ](https://arxiv.org/pdf/2103.07197.pdf)
>  Machine learning based singing voice models require large datasets and lengthy training times. In this work we present a lightweight architecture, based on the Differentiable Digital Signal Processing (DDSP) library, that is able to output song-like utterances conditioned only on pitch and amplitude, after twelve hours of training using small datasets of unprocessed audio. The results are promising, as both the melody and the singer's voice are recognizable. In addition, we present two zero-configuration tools to train new models and experiment with them. Currently we are exploring the latent space representation, which is included in the DDSP library, but not in the original DDSP examples. Our results indicate that the latent space improves both the identification of the singer as well as the comprehension of the lyrics. Our code is available at <a class="link-external link-https" href="https://github.com/juanalonso/DDSP-singing-experiments" rel="external noopener nofollow">this https URL</a> with links to the zero-configuration notebooks, and our sound examples are at <a class="link-external link-https" href="https://juanalonso.github.io/DDSP-singing-experiments/" rel="external noopener nofollow">this https URL</a> .      
### 34.Secure Outage Analysis of FSO Communications Over Arbitrarily Correlated Málaga Turbulence Channels  [ :arrow_down: ](https://arxiv.org/pdf/2103.07163.pdf)
>  In this paper, we analyze the secrecy outage performance for more realistic eavesdropping scenario of free-space optical (FSO) communications, where the main and wiretap links are correlated. The FSO fading channels are modeled by the well-known Málaga distribution. Exact expressions for the secrecy performance metrics such as secrecy outage probability (SOP) and probability of the non zero secrecy capacity (PNZSC) are derived, and asymptotic analysis on the SOP is also conducted. The obtained results reveal useful insights on the effect of channel correlation on FSO communications. Counterintuitively, it is found that the secrecy outage performance demonstrates a non-monotonic behavior with the increase of correlation. More specifically, there is an SNR penalty for achieving a target SOP as the correlation increases within some range. However, when the correlation is further increased beyond some threshold, the SOP performance improves significantly.      
### 35.Enabling Smart Reflection in Integrated Air-Ground Wireless Network: IRS Meets UAV  [ :arrow_down: ](https://arxiv.org/pdf/2103.07151.pdf)
>  Intelligent reflecting surface (IRS) and unmanned aerial vehicle (UAV) have emerged as two promising technologies to boost the performance of wireless communication networks, by proactively altering the wireless communication channels via smart signal reflection and maneuver control, respectively. However, they face different limitations in practice, which restrain their future applications. In this article, we propose new methods to jointly apply IRS and UAV in integrated air-ground wireless networks by exploiting their complementary advantages. Specifically, terrestrial IRS is used to enhance the UAV-ground communication performance, while UAV-mounted IRS is employed to assist in the terrestrial communication. We present their promising application scenarios, new communication design issues as well as potential solutions. In particular, we show that it is practically beneficial to deploy both the terrestrial and aerial IRSs in future wireless networks to reap the benefits of smart reflections in three-dimensional (3D) space.      
### 36.A Risk-taking Broker Model to Optimise User Requests placement on On-demand and Contract VMs  [ :arrow_down: ](https://arxiv.org/pdf/2103.07133.pdf)
>  Cloud providers offer end-users various pricing schemes to allow them to tailor VMs to their needs, e.g., a pay-as-you-go billing scheme, called \textit{on-demand}, and a discounted contract scheme, called \textit{reserved instances}. This paper presents a cloud broker which offers users both the flexibility of on-demand instances and some level of discounts found in reserved instances. The broker employs a buy-low-and-sell-high strategy that places user requests into a resource pool of pre-purchased discounted cloud resources. By analysing user request time-series data, the broker takes a risk-oriented approach to dynamically adjust the resource pool. <br>This approach does not require a training process which is useful at processing the large data stream. The broker is evaluated with high-frequency real cloud datasets from Alibaba. The results show that the overall profit of the broker is close to the theoretical optimal scenario where user requests can be perfectly predicted.      
### 37.Thousand to One: Semantic Prior Modeling for Conceptual Coding  [ :arrow_down: ](https://arxiv.org/pdf/2103.07131.pdf)
>  Conceptual coding has been an emerging research topic recently, which encodes natural images into disentangled conceptual representations for compression. However, the compression performance of the existing methods is still sub-optimal due to the lack of comprehensive consideration of rate constraint and reconstruction quality. To this end, we propose a novel end-to-end semantic prior modeling-based conceptual coding scheme towards extremely low bitrate image compression, which leverages semantic-wise deep representations as a unified prior for entropy estimation and texture synthesis. Specifically, we employ semantic segmentation maps as structural guidance for extracting deep semantic prior, which provides fine-grained texture distribution modeling for better detail construction and higher flexibility in subsequent high-level vision tasks. Moreover, a cross-channel entropy model is proposed to further exploit the inter-channel correlation of the spatially independent semantic prior, leading to more accurate entropy estimation for rate-constrained training. The proposed scheme achieves an ultra-high 1000x compression ratio, while still enjoying high visual reconstruction quality and versatility towards visual processing and analysis tasks.      
### 38.Learning spectro-temporal representations of complex sounds with parameterized neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.07125.pdf)
>  Deep Learning models have become potential candidates for auditory neuroscience research, thanks to their recent successes on a variety of auditory tasks. Yet, these models often lack interpretability to fully understand the exact computations that have been performed. Here, we proposed a parametrized neural network layer, that computes specific spectro-temporal modulations based on Gabor kernels (Learnable STRFs) and that is fully interpretable. We evaluated predictive capabilities of this layer on Speech Activity Detection, Speaker Verification, Urban Sound Classification and Zebra Finch Call Type Classification. We found out that models based on Learnable STRFs are on par for all tasks with different toplines, and obtain the best performance for Speech Activity Detection. As this layer is fully interpretable, we used quantitative measures to describe the distribution of the learned spectro-temporal modulations. The filters adapted to each task and focused mostly on low temporal and spectral modulations. The analyses show that the filters learned on human speech have similar spectro-temporal parameters as the ones measured directly in the human auditory cortex. Finally, we observed that the tasks organized in a meaningful way: the human vocalizations tasks closer to each other and bird vocalizations far away from human vocalizations and urban sounds tasks.      
### 39.Evaluation Framework for Performance Limitation of Autonomous Systems under Sensor Attack  [ :arrow_down: ](https://arxiv.org/pdf/2103.07118.pdf)
>  Autonomous systems such as self-driving cars rely on sensors to perceive the surrounding world. Measures must be taken against attacks on sensors, which have been a hot topic in the last few years. For that goal one must first evaluate how sensor attacks affect the system, i.e. which part or whole of the system will fail if some of the built-in sensors are compromised, or will keep safe, etc. Among the relevant safety standards, ISO/PAS 21448 addresses the safety of road vehicles taking into account the performance limitations of sensors, but leaves security aspects out of scope. On the other hand, ISO/SAE 21434 addresses the security perspective during the development process of vehicular systems, but not specific threats such as sensor attacks. As a result the safety of autonomous systems under sensor attack is yet to be addressed. In this paper we propose a framework that combines safety analysis for scenario identification, and scenario-based simulation with sensor attack models embedded. Given an autonomous system model, we identify hazard scenarios caused by sensor attacks, and evaluate the performance limitations in the scenarios. We report on a prototype simulator for autonomous vehicles with radar, cameras and LiDAR along with attack models against the sensors. Our experiments show that our framework can evaluate how the system safety changes as parameters of the attacks and the sensors vary.      
### 40.iToF2dToF: A Robust and Flexible Representation for Data-Driven Time-of-Flight Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2103.07087.pdf)
>  Indirect Time-of-Flight (iToF) cameras are a promising depth sensing technology. However, they are prone to errors caused by multi-path interference (MPI) and low signal-to-noise ratio (SNR). Traditional methods, after denoising, mitigate MPI by estimating a transient image that encodes depths. Recently, data-driven methods that jointly denoise and mitigate MPI have become state-of-the-art without using the intermediate transient representation. In this paper, we propose to revisit the transient representation. Using data-driven priors, we interpolate/extrapolate iToF frequencies and use them to estimate the transient image. Given direct ToF (dToF) sensors capture transient images, we name our method iToF2dToF. The transient representation is flexible. It can be integrated with different rule-based depth sensing algorithms that are robust to low SNR and can deal with ambiguous scenarios that arise in practice (e.g., specular MPI, optical cross-talk). We demonstrate the benefits of iToF2dToF over previous methods in real depth sensing scenarios.      
### 41.IRS-Assisted Ambient Backscatter Communications Utilizing Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.07083.pdf)
>  We consider an ambient backscatter communication (AmBC) system aided by an intelligent reflecting surface (IRS). The optimization of the IRS to assist AmBC is extremely difficult when there is no prior channel knowledge, for which no design solutions are currently available. We utilize a deep reinforcement learning-based framework to jointly optimize the IRS and reader beamforming, with no knowledge of the channels or ambient signal. We show that the proposed framework can facilitate efficient AmBC communication with a detection performance comparable to several benchmarks under full channel knowledge.      
### 42.A Novel Probability Weighting Method To Fit Gaussian Functions  [ :arrow_down: ](https://arxiv.org/pdf/2103.07060.pdf)
>  Gaussian functions are commonly used in different fields, many real signals can be modeled into such form. Research aiming to obtain a precise fitting result for these functions is very meaningful. This manuscript intends to introduce a new algorithm used to estimate the full parameters of the Gaussian-shaped function. It is basically a weighting method, starting from Caruana's method, while the selection of weighting factors is from the statistics view and based on the estimation of the confidence level for the samples. Tests designed for comparison with current similar methods have been conducted. The simulation results indicate a good performance for this new method, mainly in precision and robustness.      
### 43.Smart and Reconfigurable Wireless Communications: From IRS Modeling to Algorithm Design  [ :arrow_down: ](https://arxiv.org/pdf/2103.07046.pdf)
>  Intelligent reflecting surfaces (IRSs) have been introduced into wireless communications systems due to their great potential to smartly customize and reconfigure radio propagation environments in a cost-effective manner. Despite the promising advantages of IRSs, academic research on IRSs is still in its infancy. In particular, the design and analysis of IRS-assisted wireless communication systems critically depend on an accurate and tractable modeling of the IRS. In this article, we first present and compare three IRS models, namely the conventional independent diffusive scatterer-based model, physics-based model, and impedance network-based model, in terms of their accuracy, tractability, and hardware complexity. Besides, a new framework based on partitioning the IRS into tiles and employing codebooks of transmission modes is introduced to enable scalable IRS optimization. Then, we investigate the impact of the three considered IRS models on system design, where several crucial technical challenges for the efficient design of IRS-assisted wireless systems are identified and the corresponding solutions are unraveled. Furthermore, to illustrate the properties of the considered models and the efficiency of the proposed solution concepts, IRS-assisted secure wireless systems and simultaneous wireless information and power transfer (SWIPT) systems are studied in more detail. Finally, several promising future research directions for IRS-assisted wireless systems are highlighted.      
### 44.Impulsive fishery resource transporting strategies based on an open-ended stochastic growth model having a latent variable  [ :arrow_down: ](https://arxiv.org/pdf/2103.07032.pdf)
>  In inland fisheries, transporting fishery resource individuals from a habitat to spatially apart habitat(s) has recently been considered for fisheries stock management in the natural environment. However, its mathematical optimization, especially finding when and how much of the population should be transported, is still a fundamental unresolved issue. We propose a new impulse control framework to tackle this issue based on a simple but new stochastic growth model of individual fishes. The novel growth model governing individuals' body weights uses a Wright-Fisher model as a latent driver to reproduce plausible growth dynamics. The optimization problem is formulated as an impulse control problem of a cost-benefit functional constrained by a degenerate parabolic Fokker-Planck equation of the stochastic growth dynamics. Because the growth dynamics have an observable variable and an unobservable variable (a variable difficult or impossible to observe), we consider both full-information and partial-information cases. The latter is more involved but more realistic because of not explicitly using the unobservable variable in designing the controls. In both cases, resolving an optimization problem reduces to solving the associated Fokker-Planck and its adjoint equations, the latter being non-trivial. We present a derivation procedure of the adjoint equation and its internal boundary conditions in time to efficiently derive the optimal transporting strategy. We finally provide a demonstrative computational example of a transporting problem of Ayu sweetfish (Plecoglossus altivelis altivelis) based on the latest real data set.      
### 45.An Efficient Hypergraph Approach to Robust Point Cloud Resampling  [ :arrow_down: ](https://arxiv.org/pdf/2103.06999.pdf)
>  Efficient processing and feature extraction of largescale point clouds are important in related computer vision and cyber-physical systems. This work investigates point cloud resampling based on hypergraph signal processing (HGSP) to better explore the underlying relationship among different cloud points and to extract contour-enhanced features. Specifically, we design hypergraph spectral filters to capture multi-lateral interactions among the signal nodes of point clouds and to better preserve their surface outlines. Without the need and the computation to first construct the underlying hypergraph, our low complexity approach directly estimates hypergraph spectrum of point clouds by leveraging hypergraph stationary processes from the observed 3D coordinates. Evaluating the proposed resampling methods with several metrics, our test results validate the high efficacy of hypergraph characterization of point clouds and demonstrate the robustness of hypergraph-based resampling under noisy observations.      
### 46.The Location of Optimal Object Colors with More Than Two Transitions  [ :arrow_down: ](https://arxiv.org/pdf/2103.06997.pdf)
>  The chromaticity diagram associated with the CIE 1931 color matching functions is shown to be slightly non-convex. While having no impact on practical colorimetric computations, the non-convexity does have a significant impact on the shape of some optimal object color reflectance distributions associated with the outer surface of the object color solid. Instead of the usual two-transition Schrödinger form, many optimal colors exhibit higher transition counts. A linear programming formulation is developed and is used to locate where these higher-transition optimal object colors reside on the object color solid surface.      
### 47.RIS-Assisted Code-Domain MIMO-NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2103.06985.pdf)
>  We consider the combination of uplink code-domain non-orthogonal multiple access (NOMA) with massive multiple-input multiple-output (MIMO) and reconfigurable intelligent surfaces (RISs). We assume a setup in which the base station (BS) is capable of forming beams towards the RISs under line-of-sight conditions, and where each RIS is covering a cluster of users. In order to support multi-user transmissions within a cluster, code-domain NOMA via spreading is utilized. We investigate the optimization of the RIS weights such that a large number of users is supported. As it turns out, it is a coupled optimization problem that depends on the detection order under interference cancellation and the applied filtering at the BS. We propose to decouple those variables by using sum-rate optimized weights as the initial solution, allowing us to obtain a decoupled estimate of those variables. Then, in order to determine the final weights, the problem is relaxed into a semidefinite program that can be solved efficiently via convex optimization algorithms. Simulation results show the effectiveness of our approach in improving the detectability of the users.      
