# ArXiv eess --Mon, 8 Mar 2021
### 1.Lyapunov-Regularized Reinforcement Learning for Power System Transient Stability  [ :arrow_down: ](https://arxiv.org/pdf/2103.03869.pdf)
>  Transient stability of power systems is becoming increasingly important because of the growing integration of renewable resources. These resources lead to a reduction in mechanical inertia but also provide increased flexibility in frequency responses. Namely, their power electronic interfaces can implement almost arbitrary control laws. To design these controllers, reinforcement learning (RL) has emerged as a powerful method in searching for optimal non-linear control policy parameterized by neural networks. <br>A key challenge is to enforce that a learned controller must be stabilizing. This paper proposes a Lyapunov regularized RL approach for optimal frequency control for transient stability in lossy networks. Because the lack of an analytical Lyapunov function, we learn a Lyapunov function parameterized by a neural network. The losses are specially designed with respect to the physical power system. The learned neural Lyapunov function is then utilized as a regularization to train the neural network controller by penalizing actions that violate the Lyapunov conditions. Case study shows that introducing the Lyapunov regularization enables the controller to be stabilizing and achieve smaller losses.      
### 2.Inverse design of Raman amplifier in frequency and distance domain using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.03837.pdf)
>  We present a Convolutional Neural Network (CNN) architecture for inverse Raman amplifier design. This model aims at finding the pump powers and wavelengths required for a target signal power evolution, both in distance along the fiber and in frequency. Using the proposed framework, the prediction of the pump configuration required to achieve a target power profile is demonstrated numerically with high accuracy in C-band considering both counter-propagating and bidirectional pumping schemes. For a distributed Raman amplifier based on a 100 km single-mode fiber, a low mean set (0.51, 0.54 and 0.64 dB) and standard deviation set (0.62, 0.43 and 0.38 dB) of the maximum test error are obtained numerically employing 2 and 3 counter, and 4 bidirectional propagating pumps, respectively.      
### 3.Human Activity Recognition using Deep Learning Models on Smartphones and Smartwatches Sensor Data  [ :arrow_down: ](https://arxiv.org/pdf/2103.03836.pdf)
>  In recent years, human activity recognition has garnered considerable attention both in industrial and academic research because of the wide deployment of sensors, such as accelerometers and gyroscopes, in products such as smartphones and smartwatches. Activity recognition is currently applied in various fields where valuable information about an individual's functional ability and lifestyle is needed. In this study, we used the popular WISDM dataset for activity recognition. Using multivariate analysis of covariance (MANCOVA), we established a statistically significant difference (p&lt;0.05) between the data generated from the sensors embedded in smartphones and smartwatches. By doing this, we show that smartphones and smartwatches don't capture data in the same way due to the location where they are worn. We deployed several neural network architectures to classify 15 different hand and non-hand-oriented activities. These models include Long short-term memory (LSTM), Bi-directional Long short-term memory (BiLSTM), Convolutional Neural Network (CNN), and Convolutional LSTM (ConvLSTM). The developed models performed best with watch accelerometer data. Also, we saw that the classification precision obtained with the convolutional input classifiers (CNN and ConvLSTM) was higher than the end-to-end LSTM classifier in 12 of the 15 activities. Additionally, the CNN model for the watch accelerometer was better able to classify non-hand oriented activities when compared to hand-oriented activities.      
### 4.Digital Interference Mitigation in Space Division Multiplexing Self-Homodyne Coherent Detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.03835.pdf)
>  We propose a digital interference mitigation scheme to reduce the impact of mode coupling in space division multiplexing self-homodyne coherent detection and experimentally verify its effectiveness in 240-Gbps mode-multiplexed transmission over 3-mode multimode fiber.      
### 5.Performance and Complexity Analysis of bi-directional Recurrent Neural Network Models vs. Volterra Nonlinear Equalizers in Digital Coherent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.03832.pdf)
>  We investigate the complexity and performance of recurrent neural network (RNN) models as post-processing units for the compensation of fibre nonlinearities in digital coherent systems carrying polarization multiplexed 16-QAM and 32-QAM signals. We evaluate three bi-directional RNN models, namely the bi-LSTM, bi-GRU and bi-Vanilla-RNN and show that all of them are promising nonlinearity compensators especially in dispersion unmanaged systems. Our simulations show that during inference the three models provide similar compensation performance, therefore in real-life systems the simplest scheme based on Vanilla-RNN units should be preferred. We compare bi-Vanilla-RNN with Volterra nonlinear equalizers and exhibit its superiority both in terms of performance and complexity, thus highlighting that RNN processing is a very promising pathway for the upgrade of long-haul optical communication systems utilizing coherent detection.      
### 6.Proactive and AoI-aware Failure Recovery for Stateful NFV-enabled Zero-Touch 6G Networks: Model-Free DRL Approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.03817.pdf)
>  In this paper, we propose a model-free deep reinforcement learning (DRL)- based proactive failure recovery (PFR) framework called zero-touch PFR (ZT-PFR) for the embedded stateful virtual network functions (VNFs) in network function virtualization (NFV) enabled networks. To realize the ZT-PFR concept, sequential decision-making based on network status is necessary. To this end, we formulate an optimization problem for efficient resource usage by minimizing the defined network cost function including resource cost and wrong decision penalty. Inspired by ETSI and ITU, we propose a novel impending failure model where each VNF state transition follows a Markov process. As a solution, we propose state-of-the-art DRL-based methods such as soft actor-critic and proximal policy optimization. Moreover, to keep network state monitoring information at an acceptable level of freshness in order to make appropriate decisions, we apply the concept of the age of information (AoI) to strike a balance between the event and scheduling-based monitoring. Several simulation scenarios are considered to show the effectiveness of our algorithm and provide a fair comparison with baselines. Several key systems and DRL algorithm design insights for PFR are drawn from our analysis and simulation results. For example we use a hybrid neural network, consisting of long short time memory (LSTM) layers in the DRL agent structure, to capture impending failure time dependency.      
### 7.Model-free two-step design for improving transient learning performance in nonlinear optimal regulator problems  [ :arrow_down: ](https://arxiv.org/pdf/2103.03808.pdf)
>  Reinforcement learning (RL) provides a model-free approach to designing an optimal controller for nonlinear dynamical systems. However, the learning process requires a considerable number of trial-and-error experiments using the poorly controlled system, and accumulates wear and tear on the plant. Thus, it is desirable to maintain some degree of control performance during the learning process. In this paper, we propose a model-free two-step design approach to improve the transient learning performance of RL in an optimal regulator design problem for unknown nonlinear systems. Specifically, a linear control law pre-designed in a model-free manner is used in parallel with online RL to ensure a certain level of performance at the early stage of learning. Numerical simulations show that the proposed method improves the transient learning performance and efficiency in hyperparameter tuning of RL.      
### 8.Liver Fibrosis and NAS scoring from CT images using self-supervised learning and texture encoding  [ :arrow_down: ](https://arxiv.org/pdf/2103.03761.pdf)
>  Non-alcoholic fatty liver disease (NAFLD) is one of the most common causes of chronic liver diseases (CLD) which can progress to liver cancer. The severity and treatment of NAFLD is determined by NAFLD Activity Scores (NAS)and liver fibrosis stage, which are usually obtained from liver biopsy. However, biopsy is invasive in nature and involves risk of procedural complications. Current methods to predict the fibrosis and NAS scores from noninvasive CT images rely heavily on either a large annotated dataset or transfer learning using pretrained networks. However, the availability of a large annotated dataset cannot be always ensured andthere can be domain shifts when using transfer learning. In this work, we propose a self-supervised learning method to address both problems. As the NAFLD causes changes in the liver texture, we also propose to use texture encoded inputs to improve the performance of the model. Given a relatively small dataset with 30 patients, we employ a self-supervised network which achieves better performance than a network trained via transfer learning. The code is publicly available at <a class="link-external link-https" href="https://github.com/ananyajana/fibrosis_code" rel="external noopener nofollow">this https URL</a>.      
### 9.FedDis: Disentangled Federated Learning for Unsupervised Brain Pathology Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.03705.pdf)
>  In recent years, data-driven machine learning (ML) methods have revolutionized the computer vision community by providing novel efficient solutions to many unsolved (medical) image analysis problems. However, due to the increasing privacy concerns and data fragmentation on many different sites, existing medical data are not fully utilized, thus limiting the potential of ML. Federated learning (FL) enables multiple parties to collaboratively train a ML model without exchanging local data. However, data heterogeneity (non-IID) among the distributed clients is yet a challenge. To this end, we propose a novel federated method, denoted Federated Disentanglement (FedDis), to disentangle the parameter space into shape and appearance, and only share the shape parameter with the clients. FedDis is based on the assumption that the anatomical structure in brain MRI images is similar across multiple institutions, and sharing the shape knowledge would be beneficial in anomaly detection. In this paper, we leverage healthy brain scans of 623 subjects from multiple sites with real data (OASIS, ADNI) in a privacy-preserving fashion to learn a model of normal anatomy, that allows to segment abnormal structures. We demonstrate a superior performance of FedDis on real pathological databases containing 109 subjects; two publicly available MS Lesions (MSLUB, MSISBI), and an in-house database with MS and Glioblastoma (MSI and GBI). FedDis achieved an average dice performance of 0.38, outperforming the state-of-the-art (SOTA) auto-encoder by 42% and the SOTA federated method by 11%. Further, we illustrate that FedDis learns a shape embedding that is orthogonal to the appearance and consistent under different intensity augmentations.      
### 10.Multi-rate Control Design under Input Constraints via Fixed-Time Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2103.03695.pdf)
>  In this paper, we introduce the notion of periodic safety, which requires that the system trajectories periodically visit a subset of a forward-invariant safe set, and utilize it in a multi-rate framework where a high-level planner generates a reference trajectory that is tracked by a low-level controller under input constraints. We introduce the notion of fixed-time barrier functions which is leveraged by the proposed low-level controller in a quadratic programming framework. Then, we design a model predictive control policy for high-level planning with a bound on the rate of change for the reference trajectory to guarantee that periodic safety is achieved. We demonstrate the effectiveness of the proposed strategy on a simulation example, where the proposed fixed-time stabilizing low-level controller shows successful satisfaction of control objectives, whereas an exponentially stabilizing low-level controller fails.      
### 11.Optimal Stationary State Estimation Over Multiple Markovian Packet Drop Channels  [ :arrow_down: ](https://arxiv.org/pdf/2103.03689.pdf)
>  In this paper, we investigate the state estimation problem over multiple Markovian packet drop channels. In this problem setup, a remote estimator receives measurement data transmitted from multiple sensors over individual channels. By the method of Markovian jump linear systems, an optimal stationary estimator that minimizes the error variance in the steady state is obtained, based on the mean-square (MS) stabilizing solution to the coupled algebraic Riccati equations. An explicit necessary and sufficient condition is derived for the existence of the MS stabilizing solution, which coincides with that of the standard Kalman filter. More importantly, we provide a sufficient condition under which the MS detectability with multiple Markovian packet drop channels can be decoupled, and propose a locally optimal stationary estimator but computationally more tractable. Analytic sufficient and necessary MS detectability conditions are presented for the decoupled subsystems subsequently. Finally, numerical simulations are conducted to illustrate the results on the MS stabilizing solution, the MS detectability, and the performance of the optimal and locally optimal stationary estimators.      
### 12.Experimental Validation of a Dynamic Equivalent Model for Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2103.03641.pdf)
>  The goal of this paper is the experimental validation of a gray-box equivalent modeling approach applied to microgrids. The main objective of the equivalent modeling is to represent the dynamic response of a microgrid with a simplified model. The main contribution of this work is the experimental validation of a two-step process, composed by the definition of a nonlinear equivalent model with operational constraints, adapted to the microgrid environment, and the identification procedure used to define the model parameters. Once the parameters are identified, the simplified model is ready to reproduce the microgrid behavior to voltage and frequency variations, in terms of active and reactive power exchanges at the point of common coupling. To validate the proposed approach, a set of experimental tests have been carried out on a real LV microgrid considering different configurations, including both grid-connected and islanded operating conditions. Results show the effectiveness of the proposed technique and the applicability of the model to perform dynamic simulations.      
### 13.An Optimized H.266/VVC Software Decoder On Mobile Platform  [ :arrow_down: ](https://arxiv.org/pdf/2103.03612.pdf)
>  As the successor of H.265/HEVC, the new versatile video coding standard (H.266/VVC) can provide up to 50% bitrate saving with the same subjective quality, at the cost of increased decoding complexity. To accelerate the application of the new coding standard, a real-time H.266/VVC software decoder that can support various platforms is implemented, where SIMD technologies, parallelism optimization, and the acceleration strategies based on the characteristics of each coding tool are applied. As the mobile devices have become an essential carrier for video services nowadays, the mentioned optimization efforts are not only implemented for the x86 platform, but more importantly utilized to highly optimize the decoding performance on the ARM platform in this work. The experimental results show that when running on the Apple A14 SoC (iPhone 12pro), the average single-thread decoding speed of the present implementation can achieve 53fps (RA and LB) for full HD (1080p) bitstreams generated by VTM-11.0 reference software using 8bit Common Test Conditions (CTC). When multi-threading is enabled, an average of 32 fps (RA) can be achieved when decoding the 4K bitstreams.      
### 14.Randomized eigen-spectrograms extraction for an effective fault diagnosis of bearings  [ :arrow_down: ](https://arxiv.org/pdf/2103.03608.pdf)
>  The Intelligent Fault Diagnosis of rotating machinery proposes some captivating challenges in light of the imminent big data era. Large amounts of data are expected to populate the Internet of Things (IoT) diagnostic services. Consequently, todays deep learning strategies are evolving towards effective approaches such as transfer learning to uncover hidden paths in extensive vibration data. However, this field is characterized by several open issues. Models interpretation is still buried under the foundations of data driven science, thus requiring attention to the development of new opportunities also for machine learning theories. This study proposes a diagnosis model, based on intelligent spectrogram recognition, via image processing. The novel approach is embodied by the introduction of the eigen-spectrograms and randomized linear algebra in fault diagnosis. The eigen-spectrograms hierarchically display inherent structures underlying spectrogram images. Also, different combinations of eigen-spectrograms are expected to describe multiple machine health states. Randomized algebra and eigen-spectrograms enable the construction of a significant feature space, which nonetheless emerges as a viable device to explore models interpretations. The computational efficiency of randomized approaches further collocates this methodology in the big data perspective and provides new reading keys of well-established statistical learning theories, such as the Support Vector Machine (SVM). The conjunction of randomized algebra and Support Vector Machine for spectrogram recognition shows to be extremely accurate and efficient as compared to state of the art results and transfer learning strategies.      
### 15.SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.03604.pdf)
>  Hyperspectral imaging (HSI) unlocks the huge potential to a wide variety of applications relied on high-precision pathology image segmentation, such as computational pathology and precision medicine. Since hyperspectral pathology images benefit from the rich and detailed spectral information even beyond the visible spectrum, the key to achieve high-precision hyperspectral pathology image segmentation is to felicitously model the context along high-dimensional spectral bands. Inspired by the strong context modeling ability of transformers, we hereby, for the first time, formulate the contextual feature learning across spectral bands for hyperspectral pathology image segmentation as a sequence-to-sequence prediction procedure by transformers. To assist spectral context learning procedure, we introduce two important strategies: (1) a sparsity scheme enforces the learned contextual relationship to be sparse, so as to eliminates the distraction from the redundant bands; (2) a spectral normalization, a separate group normalization for each spectral band, mitigates the nuisance caused by heterogeneous underlying distributions of bands. We name our method Spectral Transformer (SpecTr), which enjoys two benefits: (1) it has a strong ability to model long-range dependency among spectral bands, and (2) it jointly explores the spatial-spectral features of HSI. Experiments show that SpecTr outperforms other competing methods in a hyperspectral pathology image segmentation benchmark without the need of pre-training. Code is available at <a class="link-external link-https" href="https://github.com/hfut-xc-yun/SpecTr" rel="external noopener nofollow">this https URL</a>.      
### 16.OFDM with Index Modulation in Orbital Angular Momentum Multiplexed Free Space Optical Links  [ :arrow_down: ](https://arxiv.org/pdf/2103.03573.pdf)
>  Communication using orbital angular momentum (OAM) modes has recently received a considerable interest in free space optical (FSO) communications. Propagating OAM modes through free space may be subject to atmospheric turbulence (AT) distortions that cause signal attenuation and crosstalk which degrades the system capacity and increases the error probability. In this paper, we propose to enhance the OAM FSO communications in terms of bit error rate and spectral efficiency, for different levels of AT regimes. The performance gain is achieved by introducing orthogonal frequency division multiplexing (OFDM) with index modulation technique to the OAM FSO system.      
### 17.Learning the sampling density in 2D SPARKLING MRI acquisition for optimized image reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2103.03559.pdf)
>  The SPARKLING algorithm was originally developed for accelerated 2D magnetic resonance imaging (MRI) in the compressed sensing (CS) context. It yields non-Cartesian sampling trajectories that jointly fulfill a target sampling density while each individual trajectory complies with MR hardware constraints. However, the two main limitations of SPARKLING are first that the optimal target sampling density is unknown and thus a user-defined parameter and second that this sampling pattern generation remains disconnected from MR image reconstruction thus from the optimization of image quality. Recently, datadriven learning schemes such as LOUPE have been proposed to learn a discrete sampling pattern, by jointly optimizing the whole pipeline from data acquisition to image reconstruction. In this work, we merge these methods with a state-of-the-art deep neural network for image reconstruction, called XPDNET, to learn the optimal target sampling density. Next, this density is used as input parameter to SPARKLING to obtain 20x accelerated non-Cartesian trajectories. These trajectories are tested on retrospective compressed sensing (CS) studies and show superior performance in terms of image quality with both deep learning (DL) and conventional CS reconstruction schemes.      
### 18.A Hybrid CNN-BiLSTM Voice Activity Detector  [ :arrow_down: ](https://arxiv.org/pdf/2103.03529.pdf)
>  This paper presents a new hybrid architecture for voice activity detection (VAD) incorporating both convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) layers trained in an end-to-end manner. In addition, we focus specifically on optimising the computational efficiency of our architecture in order to deliver robust performance in difficult in-the-wild noise conditions in a severely under-resourced setting. Nested k-fold cross-validation was used to explore the hyperparameter space, and the trade-off between optimal parameters and model size is discussed. The performance effect of a BiLSTM layer compared to a unidirectional LSTM layer was also considered. We compare our systems with three established baselines on the AVA-Speech dataset. We find that significantly smaller models with near optimal parameters perform on par with larger models trained with optimal parameters. BiLSTM layers were shown to improve accuracy over unidirectional layers by $\approx$2% absolute on average. With an area under the curve (AUC) of 0.951, our system outperforms all baselines, including a much larger ResNet system, particularly in difficult noise conditions.      
### 19.Testing for a Random Walk Structure in the Frequency Evolution of a Tone in Noise  [ :arrow_down: ](https://arxiv.org/pdf/2103.03459.pdf)
>  Inference and hypothesis testing is constructed on the basis that a specific model holds for the data. To determine the veracity of conclusions drawn from such data analyses, one must be able to identify the presence of the assumed structure within the data. A model verification test is developed for the presence of a random walk-like structure for variations in the frequency of complex-valued sinusoidal signals measured in additive Gaussian noise. This test evaluates the joint inference of the random walk hypothesis tests found in economics literature that seek random walk behaviours in time series data, with an additional test to account for how the random walk behaves in frequency space.      
### 20.Study Group Learning: Improving Retinal Vessel Segmentation Trained with Noisy Labels  [ :arrow_down: ](https://arxiv.org/pdf/2103.03451.pdf)
>  Retinal vessel segmentation from retinal images is an essential task for developing the computer-aided diagnosis system for retinal diseases. Efforts have been made on high-performance deep learning-based approaches to segment the retinal images in an end-to-end manner. However, the acquisition of retinal vessel images and segmentation labels requires onerous work from professional clinicians, which results in smaller training dataset with incomplete labels. As known, data-driven methods suffer from data insufficiency, and the models will easily over-fit the small-scale training data. Such a situation becomes more severe when the training vessel labels are incomplete or incorrect. In this paper, we propose a Study Group Learning (SGL) scheme to improve the robustness of the model trained on noisy labels. Besides, a learned enhancement map provides better visualization than conventional methods as an auxiliary tool for clinicians. Experiments demonstrate that the proposed method further improves the vessel segmentation performance in DRIVE and CHASE$\_$DB1 datasets, especially when the training labels are noisy.      
### 21.User-Centric Cooperative MEC Service Offloading  [ :arrow_down: ](https://arxiv.org/pdf/2103.03447.pdf)
>  Mobile edge computing provides users with a cloud environment close to the edge of the wireless network, supporting the computing intensive applications that have low latency requirements. The combination of offloading with the wireless communication brings new challenges. This paper investigates the service caching problem during the long-term service offloading in the user-centric wireless network. To meet the time-varying service demands of a typical user, a cooperative service caching strategy in the unit of the base station (BS) cluster is proposed. We formulate the caching problem as a time-averaged completion delay minimization problem and transform it into time-decoupled instantaneous problems with a virtual caching cost queue at first. Then we propose a distributed algorithm which is based on the consensus-sharing alternating direction method of multipliers to solve each instantaneous problem. The simulations validate that the proposed online distributed service caching algorithm can achieve the optimal time-averaged completion delay of offloading tasks with the smallest caching cost in the unit of a BS cluster.      
### 22.Optimization of User Selection and Bandwidth Allocation for Federated Learning in VLC/RF Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.03444.pdf)
>  Limited radio frequency (RF) resources restrict the number of users that can participate in federated learning (FL) thus affecting FL convergence speed and performance. In this paper, we first introduce visible light communication (VLC) as a supplement to RF in FL and build a hybrid VLC/RF communication system, in which each indoor user can use both VLC and RF to transmit its FL model parameters. Then, the problem of user selection and bandwidth allocation is studied for FL implemented over a hybrid VLC/RF system aiming to optimize the FL performance. The problem is first separated into two subproblems. The first subproblem is a user selection problem with a given bandwidth allocation, which is solved by a traversal algorithm. The second subproblem is a bandwidth allocation problem with a given user selection, which is solved by a numerical method. The final user selection and bandwidth allocation are obtained by iteratively solving these two subproblems. Simulation results show that the proposed FL algorithm that efficiently uses VLC and RF for FL model transmission can improve the prediction accuracy by up to 10% compared with a conventional FL system using only RF.      
### 23.Network Consensus with Privacy: A Secret Sharing Method  [ :arrow_down: ](https://arxiv.org/pdf/2103.03432.pdf)
>  In this work, inspired by secret sharing schemes, we introduce a privacy-preserving approach for network consensus, by which all nodes in a network can reach an agreement on their states without exposing the individual state to neighbors. With the privacy degree defined for the agents, the proposed method makes the network resistant to the collusion of any given number of neighbors, and protects the consensus procedure from communication eavesdropping. Unlike existing works, the proposed privacy-preserving algorithm is resilient to node failures. When a node fails, the method offers the possibility of rebuilding the lost node via the information kept in its neighbors, even though none of the neighbors knows the exact state of the failing node. Moreover, it is shown that the proposed method can achieve consensus and average consensus almost surely, when the agents have arbitrary privacy degrees and a common privacy degree, respectively. To illustrate the theory, two numerical examples are presented.      
### 24.The Effect of Behavioral Probability Weighting in a Simultaneous Multi-Target Attacker-Defender Game  [ :arrow_down: ](https://arxiv.org/pdf/2103.03392.pdf)
>  We consider a security game in a setting consisting of two players (an attacker and a defender), each with a given budget to allocate towards attack and defense, respectively, of a set of nodes. Each node has a certain value to the attacker and the defender, along with a probability of being successfully compromised, which is a function of the investments in that node by both players. For such games, we characterize the optimal investment strategies by the players at the (unique) Nash Equilibrium. We then investigate the impacts of behavioral probability weighting on the investment strategies; such probability weighting, where humans overweight low probabilities and underweight high probabilities, has been identified by behavioral economists to be a common feature of human decision-making. We show via numerical experiments that behavioral decision-making by the defender causes the Nash Equilibrium investments in each node to change (where the defender overinvests in the high-value nodes and underinvests in the low-value nodes).      
### 25.Koopman Operator Based Modeling for Quadrotor Control on $SE(3)$  [ :arrow_down: ](https://arxiv.org/pdf/2103.03363.pdf)
>  In this paper, we propose a Koopman operator based approach to describe the nonlinear dynamics of a quadrotor on $SE(3)$ in terms of an infinite-dimensional linear system which evolves in the space of observable functions (lifted space) and which is more appropriate for control design purposes. The major challenge when using the Koopman operator is the characterization of a set of observable functions that can span the lifted space. Most of the existing methods either start from a set of dictionary functions and then search for a subset that best fits the underlying nonlinear dynamics or they rely on machine learning algorithms to learn these observable functions. Instead of guessing or learning the observables, in this work we derive them in a systematic way for the quadrotor dynamics on $SE(3)$. In addition, we prove that the proposed sequence of observable functions converges pointwise to the zero function, which allows us to select only a finite set of observable functions to form (an approximation of) the lifted space. Our theoretical analysis is also confirmed by numerical simulations which demonstrate that by increasing the dimension of the lifted space, the derived linear state space model can approximate the nonlinear quadrotor dynamics more accurately.      
### 26.On Angular Speed Estimation of Rigid Bodies  [ :arrow_down: ](https://arxiv.org/pdf/2103.03310.pdf)
>  The problem of estimating the angular speed of a solid body from attitude measurements is addressed. To solve this problem, we propose an observer whose dynamics are not constrained to evolve on any specific manifold. This drastically simplifies the analysis of the proposed observer. Using Lyapunov analysis, sufficient conditions for global asymptotic stability of a set wherein the estimation error is equal to zero are established. In addition, the proposed methodology is adapted to deal with angular speed estimation for systems evolving on the unit circle. The approach is illustrated through several numerical simulations.      
### 27.A Hybrid Controller for DOS-Resilient String-Stable Vehicle Platoons  [ :arrow_down: ](https://arxiv.org/pdf/2103.03304.pdf)
>  This paper deals with the design of resilient Cooperative Adaptive Cruise Control (CACC) for homogeneous vehicle platoons in which communication is vulnerable to Denial-of-Service (DOS) attacks. We consider DOS attacks as consecutive packet dropouts. We present a controller tuning procedure based on linear matrix inequalities (LMI) that maximizes the resiliency to DOS attacks, while guaranteeing performance and string stability. The design procedure returns controller gains and gives a lower bound on the maximum allowable number of successive packet dropouts. A numerical example is employed to illustrate the effectiveness of the proposed approach.      
### 28.Super-resolution Method for Coherent DOA Estimation of Multiple Wideband Sources  [ :arrow_down: ](https://arxiv.org/pdf/2103.03271.pdf)
>  We focus on coherent direction of arrival estimation of wideband sources based on spatial sparsity. This area of research is encountered in many applications such as passive radar, sonar, mining, and communication problems, in which an increasing attention has been devoted to improving the estimation accuracy and robustness to noise. By the development of super-resolution algorithms, narrowband direction of arrival estimation based on gridless sparse algorithms and atomic norm minimization has already been addressed. In this paper, a superresolution based method is proposed for coherent direction of arrival estimation of multiple wideband sources. First, unlike the conventional coherent methods, we develop a new focusing method to map the subband with the largest center frequency to the other ones, which leads to an accurate method with no requirement for initial estimates for DOAs. Then, we introduce an atomic norm problem by defining a new set of atoms and exploiting the signal joint sparsity of different frequency subbands in a continuous spatial domain. This problem is then cast as a semidefinite program, which leads to implementing a new coherent direction of arrival estimation method with higher resolution and more robustness to noise. Our method needs only one single snapshot for each frequency subband, leading to a small number of snapshots for the received wideband signal compared to the other coherent DOA techniques. Numerical simulations show the outperformance of the proposed method compared to the conventional ones.      
### 29.A Pilot Study on Visually-Stimulated Cognitive Tasks for EEG-Based Dementia Recognition Using Frequency and Time Features  [ :arrow_down: ](https://arxiv.org/pdf/2103.03854.pdf)
>  Dementia is one of the main causes of cognitive decline. Since the majority of dementia patients cannot be cured, being able to diagnose them before the onset of the symptoms can prevent the rapid progression of the cognitive impairment. This study aims to investigate the difference in the Electroencephalograph (EEG) signals of three groups of subjects: Normal Control (NC), Mild Cognitive Impairment (MCI), and Dementia (DEM). Unlike previous works that focus on the diagnosis of Alzheimer's disease (AD) from EEG signals, we study the detection of dementia to generalize the classification models to other types of dementia. We have developed a pilot study on machine learning-based dementia diagnosis using EEG signals from four visual stimulation tasks (Fixation, Mental Imagery, Symbol Recognition, and Visually Evoked Related Potential) to identify the most suitable task and method to detect dementia using EEG signals. We extracted both frequency and time domain features from the EEG signals and applied a Support Vector Machine (SVM) for each domain to classify the patients using those extracted features. Additionally, we study the feasibility of the Filter Bank Common Spatial Pattern (FBCSP) algorithm to extract features from the frequency domain to detect dementia. The evaluation of the model shows that the tasks that test the working memory are the most appropriate to detect dementia using EEG signals in both time and frequency domain analysis. However, the best results in both domains are obtained by combining features of all four cognitive tasks.      
### 30.Neuron Signal Propagation Analysis of Cytokine-Storm induced Demyelination  [ :arrow_down: ](https://arxiv.org/pdf/2103.03790.pdf)
>  The COVID-19 pandemic has shaken the world unprecedentedly, where it has affected the vast global population both socially and economically. The pandemic has also opened our eyes to the many threats that novel virus infections can pose for humanity. While numerous unknowns are being investigated in terms of the distributed damage that the virus can do to the human body, recent studies have also shown that the infection can lead to lifelong sequelae that could affect other parts of the body, and one example is the brain. As part of this work, we investigate how viral infection can affect the brain by modelling and simulating a neuron's behaviour under demyelination that is affected by the cytokine storm. We quantify the effects of cytokine-induced demyelination on the propagation of action potential signals within a neuron. We used information and communication theory analysis on the signal propagated through the axonal pathway under different intensity levels of demyelination to analyse these effects. Our simulations demonstrate that virus-induced degeneration can play a role in the signal power and spiking rate and the probability of releasing neurotransmitters and compromising the propagation and processing of information between the neurons. We also propose a transfer function that models these attenuation effects that degenerates the action potential, where this model has the potential to be used as a framework for the analysis of virus-induced neurodegeneration that can pave the way to improved understanding of virus-induced demyelination.      
### 31.Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2103.03781.pdf)
>  Despite the successes of deep neural networks on many challenging vision tasks, they often fail to generalize to new test domains that are not distributed identically to the training data. The domain adaptation becomes more challenging for cross-modality medical data with a notable domain shift. Given that specific annotated imaging modalities may not be accessible nor complete. Our proposed solution is based on the cross-modality synthesis of medical images to reduce the costly annotation burden by radiologists and bridge the domain gap in radiological images. We present a novel approach for image-to-image translation in medical images, capable of supervised or unsupervised (unpaired image data) setups. Built upon adversarial training, we propose a learnable self-attentive spatial normalization of the deep convolutional generator network's intermediate activations. Unlike previous attention-based image-to-image translation approaches, which are either domain-specific or require distortion of the source domain's structures, we unearth the importance of the auxiliary semantic information to handle the geometric changes and preserve anatomical structures during image translation. We achieve superior results for cross-modality segmentation between unpaired MRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI (T1/T2) datasets compared to the state-of-the-art methods. We also observe encouraging results in cross-modality conversion for paired MRI and CT images on a brain dataset. Furthermore, a detailed analysis of the cross-modality image translation, thorough ablation studies confirm our proposed method's efficacy.      
### 32.Online Graph Learning under Smoothness Priors  [ :arrow_down: ](https://arxiv.org/pdf/2103.03762.pdf)
>  The growing success of graph signal processing (GSP) approaches relies heavily on prior identification of a graph over which network data admit certain regularity. However, adaptation to increasingly dynamic environments as well as demands for real-time processing of streaming data pose major challenges to this end. In this context, we develop novel algorithms for online network topology inference given streaming observations assumed to be smooth on the sought graph. Unlike existing batch algorithms, our goal is to track the (possibly) time-varying network topology while maintaining the memory and computational costs in check by processing graph signals sequentially-in-time. To recover the graph in an online fashion, we leverage proximal gradient (PG) methods to solve a judicious smoothness-regularized, time-varying optimization problem. Under mild technical conditions, we establish that the online graph learning algorithm converges to within a neighborhood of (i.e., it tracks) the optimal time-varying batch solution. Computer simulations using both synthetic and real financial market data illustrate the effectiveness of the proposed algorithm in adapting to streaming signals to track slowly-varying network connectivity.      
### 33.Data-Driven Short-Term Voltage Stability Assessment Based on Spatial-Temporal Graph Convolutional Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.03729.pdf)
>  Post-fault dynamics of short-term voltage stability (SVS) present spatial-temporal characteristics, but the existing data-driven methods for online SVS assessment fail to incorporate such characteristics into their models effectively. Confronted with this dilemma, this paper develops a novel spatial-temporal graph convolutional network (STGCN) to address this problem. The proposed STGCN utilizes graph convolution to integrate network topology information into the learning model to exploit spatial information. Then, it adopts one-dimensional convolution to exploit temporal information. In this way, it models the spatial-temporal characteristics of SVS with complete convolutional structures. After that, a node layer and a system layer are strategically designed in the STGCN for SVS assessment. The proposed STGCN incorporates the characteristics of SVS into the data-driven classification model. It can result in higher assessment accuracy, better robustness and adaptability than conventional methods. Besides, parameters in the system layer can provide valuable information about the influences of individual buses on SVS. Test results on the real-world Guangdong Power Grid in South China verify the effectiveness of the proposed network.      
### 34.Conformal Transformation Electromagnetics Based on Schwarz-Christoffel Mapping for the Synthesis of Doubly Connected Metalenses  [ :arrow_down: ](https://arxiv.org/pdf/2103.03681.pdf)
>  An innovative transformation electromagnetics (TE) paradigm, which leverages on the Schwarz-Christoffel (SC) theorem, is proposed to design effective and realistic field manipulation devices (FMDs). Thanks to the conformal property, such a TE design method allows one to considerably mitigate the anisotropy of the synthesized metalenses (i.e., devices with artificially engineered materials covering an antenna to modify its radiation features) with respect to those yielded by the competitive state-of-the-art TE techniques. Moreover, devices with doubly connected contours, thus including masts with arbitrary sections and lenses with holes/forbidden regions in which the material properties cannot be controlled, can be handled. A set of numerical experiments is presented to assess the features of the proposed method in terms of field-manipulation capabilities and complexity of the lens material in a comparative fashion.      
### 35.An Application-Driven Conceptualization of Corner Cases for Perception in Highly Automated Driving  [ :arrow_down: ](https://arxiv.org/pdf/2103.03678.pdf)
>  Systems and functions that rely on machine learning (ML) are the basis of highly automated driving. An essential task of such ML models is to reliably detect and interpret unusual, new, and potentially dangerous situations. The detection of those situations, which we refer to as corner cases, is highly relevant for successfully developing, applying, and validating automotive perception functions in future vehicles where multiple sensor modalities will be used. A complication for the development of corner case detectors is the lack of consistent definitions, terms, and corner case descriptions, especially when taking into account various automotive sensors. In this work, we provide an application-driven view of corner cases in highly automated driving. To achieve this goal, we first consider existing definitions from the general outlier, novelty, anomaly, and out-of-distribution detection to show relations and differences to corner cases. Moreover, we extend an existing camera-focused systematization of corner cases by adding RADAR (radio detection and ranging) and LiDAR (light detection and ranging) sensors. For this, we describe an exemplary toolchain for data acquisition and processing, highlighting the interfaces of the corner case detection. We also define a novel level of corner cases, the method layer corner cases, which appear due to uncertainty inherent in the methodology or the data distribution.      
### 36.Control Barrier Functions in Sampled-Data Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.03677.pdf)
>  This paper presents conditions for ensuring forward invariance of safe sets under sampled-data system dynamics with piecewise-constant controllers and fixed time-steps. First, we introduce two different metrics to compare the conservativeness of sufficient conditions on forward invariance under piecewise-constant controllers. Then, we propose three approaches for guaranteeing forward invariance, two motivated by continuous-time barrier functions, and one motivated by discrete-time barrier functions. All proposed conditions are control affine, and thus can be incorporated into quadratic programs for control synthesis. We show that the proposed conditions are less conservative than those in earlier studies. The advantages of the new approaches are demonstrated via simulations on an obstacle-avoidance problem for a unicycle agent and on a spacecraft attitude reorientation problem, in which the new approaches achieve the mission objectives for cases where the existing methods fail.      
### 37.The Design of Dual Band Stacked Metasurfaces Using Integral Equations  [ :arrow_down: ](https://arxiv.org/pdf/2103.03676.pdf)
>  An integral equation-based approach for the design of dual band stacked metasurfaces is presented. The stacked metasurface will generate collimated beams at desired angles in each band upon reflection. The conductor-backed stacked metasurface consists of two metasurfaces (a patterned metallic cladding supported by a dielectric spacer) stacked one upon the other. The stacked metasurface is designed in three phases. First the patterned metallic cladding of each metasurface is homogenized and modeled as an inhomogeneous impedance sheet. An Electric Field Integral Equation (EFIE) is written to model the mutual coupling between the homogenized elements within each metasurface, and from metasurface to metasurface. The EFIE is transformed into matrix equations by the method of moments. The nonlinear matrix equations are solved at both bands iteratively resulting in dual band complex-valued impedance sheets. In the second phase, optimization is applied to transform these complex-valued impedance sheets into purely reactive sheets suitable for printed circuit board fabrication by introducing surface waves. In the third phase, the metallic claddings of each metasurface are patterned for full-wave simulation of the dual band stacked metasurface. Using this approach, two dual band stacked metasurfaces are designed.      
### 38.ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.03664.pdf)
>  We introduce a neural network framework, utilizing adversarial learning to partition an image into two cuts, with one cut falling into a reference distribution provided by the user. This concept tackles the task of unsupervised anomaly segmentation, which has attracted increasing attention in recent years due to their broad applications in tasks with unlabelled data. This Adversarial-based Selective Cutting network (ASC-Net) bridges the two domains of cluster-based deep learning methods and adversarial-based anomaly/novelty detection algorithms. We evaluate this unsupervised learning model on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and MS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN family, our model demonstrates tremendous performance gains in unsupervised anomaly segmentation tasks. Although there is still room to further improve performance compared to supervised learning algorithms, the promising experimental results shed light on building an unsupervised learning algorithm using user-defined knowledge.      
### 39.Automatic Exploration Process Adjustment for Safe Reinforcement Learning with Joint Chance Constraint Satisfaction  [ :arrow_down: ](https://arxiv.org/pdf/2103.03656.pdf)
>  In reinforcement learning (RL) algorithms, exploratory control inputs are used during learning to acquire knowledge for decision making and control, while the true dynamics of a controlled object is unknown. However, this exploring property sometimes causes undesired situations by violating constraints regarding the state of the controlled object. In this paper, we propose an automatic exploration process adjustment method for safe RL in continuous state and action spaces utilizing a linear nominal model of the controlled object. Specifically, our proposed method automatically selects whether the exploratory input is used or not at each time depending on the state and its predicted value as well as adjusts the variance-covariance matrix used in the Gaussian policy for exploration. We also show that our exploration process adjustment method theoretically guarantees the satisfaction of the constraints with the pre-specified probability, that is, the satisfaction of a joint chance constraint at every time. Finally, we illustrate the validity and the effectiveness of our method through numerical simulation.      
### 40.Fail-Aware LIDAR-Based Odometry for Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2103.03626.pdf)
>  Autonomous driving systems are set to become a reality in transport systems and, so, maximum acceptance is being sought among users. Currently, the most advanced architectures require driver intervention when functional system failures or critical sensor operations take place, presenting problems related to driver state, distractions, fatigue, and other factors that prevent safe control. Therefore, this work presents a redundant, accurate, robust, and scalable LiDAR odometry system with fail-aware system features that can allow other systems to perform a safe stop manoeuvre without driver mediation. All odometry systems have drift error, making it difficult to use them for localisation tasks over extended periods. For this reason, the paper presents an accurate LiDAR odometry system with a fail-aware indicator. This indicator estimates a time window in which the system manages the localisation tasks appropriately. The odometry error is minimised by applying a dynamic 6-DoF model and fusing measures based on the Iterative Closest Points (ICP), environment feature extraction, and Singular Value Decomposition (SVD) methods. The obtained results are promising for two reasons: First, in the KITTI odometry data set, the ranking achieved by the proposed method is twelfth, considering only LiDAR-based methods, where its translation and rotation errors are 1.00% and 0.0041 deg/m, respectively. Second, the encouraging results of the fail-aware indicator demonstrate the safety of the proposed LiDAR odometry system. The results depict that, in order to achieve an accurate odometry system, complex models and measurement fusion techniques must be used to improve its behaviour. Furthermore, if an odometry system is to be used for redundant localisation features, it must integrate a fail-aware indicator for use in a safe manner.      
### 41.Low-latency auditory spatial attention detection based on spectro-spatial features from EEG  [ :arrow_down: ](https://arxiv.org/pdf/2103.03621.pdf)
>  Detecting auditory attention based on brain signals enables many everyday applications, and serves as part of the solution to the cocktail party effect in speech processing. Several studies leverage the correlation between brain signals and auditory stimuli to detect the auditory attention of listeners. Recently, studies show that the alpha band (8-13 Hz) EEG signals enable the localization of auditory stimuli. We believe that it is possible to detect auditory spatial attention without the need of auditory stimuli as references. In this work, we use alpha power signals for automatic auditory spatial attention detection. To the best of our knowledge, this is the first attempt to detect spatial attention based on alpha power neural signals. We propose a spectro-spatial feature extraction technique to detect the auditory spatial attention (left/right) based on the topographic specificity of alpha power. Experiments show that the proposed neural approach achieves 81.7% and 94.6% accuracy for 1-second and 10-second decision windows, respectively. Our comparative results show that this neural approach outperforms other competitive models by a large margin in all test cases.      
### 42.Polyhedral Lyapunov Functions with Fixed Complexity  [ :arrow_down: ](https://arxiv.org/pdf/2103.03613.pdf)
>  Polyhedral Lyapunov functions can approximate any norm arbitrarily well. Because of this, they are used to study the stability of linear time varying and linear parameter varying systems without being conservative. However, the computational cost associated with using them grows unbounded as the size of their representation increases. Finding them is also a hard computational problem. <br>Here we present an algorithm that attempts to find polyhedral functions while keeping the size of the representation fixed, to limit computational costs. We do this by measuring the gap from contraction for a given polyhedral set. The solution is then used to find perturbations on the polyhedral set that reduce the contraction gap. The process is repeated until a valid polyhedral Lyapunov function is obtained. <br>The approach is rooted in linear programming. This leads to a flexible method capable of handling additional linear constraints and objectives, and enables the use of the algorithm for control synthesis.      
### 43.Combining Forensics and Privacy Requirements for Digital Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.03569.pdf)
>  This paper proposes to study the impact of image selective encryption on both forensics and privacy preserving mechanisms. The proposed selective encryption scheme works independently on each bitplane by encrypting the s most significant bits of each pixel. We show that this mechanism can be used to increase privacy by mitigating image recognition tasks. In order to guarantee a trade-off between forensics analysis and privacy, the signal of interest used for forensics purposes is extracted from the 8--s least significant bits of the protected image. We show on the CASIA2 database that good tampering detection capabilities can be achieved for s $\in$ {3,. .. , 5} with an accuracy above 80% using SRMQ1 features, while preventing class recognition tasks using CNN with an accuracy smaller than 50%.      
### 44.Multilingual Byte2Speech Text-To-Speech Models Are Few-shot Spoken Language Learners  [ :arrow_down: ](https://arxiv.org/pdf/2103.03541.pdf)
>  We present a multilingual end-to-end Text-To-Speech framework that maps byte inputs to spectrograms, thus allowing arbitrary input scripts. Besides strong results on 40+ languages, the framework demonstrates capabilities to adapt to various new languages under extreme low-resource and even few-shot scenarios of merely 40s transcribed recording without the need of lexicon, extra corpus, auxiliary models, or particular linguistic expertise, while retains satisfactory intelligibility and naturalness matching rich-resource models. Exhaustive comparative studies are performed to reveal the potential of the framework for low-resource application and the impact of various factors contributory to adaptation. Furthermore, we propose a novel method to extract language-specific sub-networks for a better understanding of the mechanism of multilingual models.      
### 45.Iterative DNA Coding Scheme With GC Balance and Run-Length Constraints Using a Greedy Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2103.03540.pdf)
>  In this paper, we propose a novel iterative encoding algorithm for DNA storage to satisfy both the GC balance and run-length constraints using a greedy algorithm. DNA strands with run-length more than three and the GC balance ratio far from 50\% are known to be prone to errors. The proposed encoding algorithm stores data at high information density with high flexibility of run-length at most $m$ and GC balance between $0.5\pm\alpha$ for arbitrary $m$ and $\alpha$. More importantly, we propose a novel mapping method to reduce the average bit error compared to the randomly generated mapping method, using a greedy algorithm. The proposed algorithm is implemented through iterative encoding, consisting of three main steps: randomization, M-ary mapping, and verification. It has an information density of 1.8616 bits/nt in the case of $m=3$, which approaches the theoretical upper bound of 1.98 bits/nt, while satisfying two constraints. Also, the average bit error caused by the one nt error is 2.3455 bits, which is reduced by $20.5\%$, compared to the randomized mapping.      
### 46.Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.03518.pdf)
>  In this manuscript, a pipeline to develop an inspection system for defect detection of solar cells is proposed. The pipeline is divided into two phases: In the first phase, a Generative Adversarial Network (GAN) employed in the medical domain for anomaly detection is adapted for inspection improving the detection rate and reducing the processing rates. This initial approach allows obtaining a model that does not require defective samples for training and can start detecting and location anomaly cells from the very beginning of a new production line. Then, in a second stage, as defective samples arise, they will be automatically labeled at pixel-level with the trained model and employed for supervised training of a second model. The experimental results show that the use of such automatically generated labels can improve the detection rates with respect to the anomaly detection model and the model trained on manual labels made by experts.      
### 47.Slow-Fast Auditory Streams For Audio Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2103.03516.pdf)
>  We propose a two-stream convolutional network for audio recognition, that operates on time-frequency spectrogram inputs. Following similar success in visual recognition, we learn Slow-Fast auditory streams with separable convolutions and multi-level lateral connections. The Slow pathway has high channel capacity while the Fast pathway operates at a fine-grained temporal resolution. We showcase the importance of our two-stream proposal on two diverse datasets: VGG-Sound and EPIC-KITCHENS-100, and achieve state-of-the-art results on both.      
### 48.Solving Linear Equations with Separable Problem Data over Directed Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.03507.pdf)
>  This paper deals with linear algebraic equations where the global coefficient matrix and constant vector are given respectively, by the summation of the coefficient matrices and constant vectors of the individual agents. Our approach is based on reformulating the original problem as an unconstrained optimization. Based on this exact reformulation, we first provide a gradient-based, centralized algorithm which serves as a reference for the ensuing design of distributed algorithms. We propose two sets of exponentially stable continuous-time distributed algorithms that do not require the individual agent matrices to be invertible, and are based on estimating non-distributed terms in the centralized algorithm using dynamic average consensus. The first algorithm works for time-varying weight-balanced directed networks, and the second algorithm works for general directed networks for which the communication graphs might not be balanced. Numerical simulations illustrate our results.      
### 49.Extremum seeking control of a class of constrained nonlinear systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.03504.pdf)
>  This paper studies the extremum seeking control (ESC) problem for a class of constrained nonlinear systems. Specifically, we focus on a family of constraints allowing to reformulate the original nonlinear system in the so-called input-output normal form. To steer the system to optimize a performance function without knowing its explicit form, we propose a novel numerical optimization-based extremum seeking control (NOESC) design consisting of a constrained numerical optimization method and an inversion based feedforward controller. In particular, a projected gradient descent algorithm is exploited to produce the state sequence to optimize the performance function, whereas a suitable boundary value problem accommodates the finite-time state transition between each two consecutive points of the state sequence. Compared to available NOESC methods, the proposed approach i) can explicitly deal with output constraints; ii) the performance function can consider a direct dependence on the states of the internal dynamics; iii) the internal dynamics do not have to be necessarily stable. The effectiveness of the proposed ESC scheme is shown through extensive numerical simulations.      
### 50.Environmental Sound Classification on the Edge: Deep Acoustic Networks for Extremely Resource-Constrained Devices  [ :arrow_down: ](https://arxiv.org/pdf/2103.03483.pdf)
>  Significant efforts are being invested to bring the classification and recognition powers of desktop and cloud systemsdirectly to edge devices. The main challenge for deep learning on the edge is to handle extreme resource constraints(memory, CPU speed and lack of GPU support). We present an edge solution for audio classification that achieves close to state-of-the-art performance on ESC-50, the same benchmark used to assess large, non resource-constrained networks. Importantly, we do not specifically engineer thenetwork for edge devices. Rather, we present a universalpipeline that converts a large deep convolutional neuralnetwork (CNN) automatically via compression and quantization into a network suitable for resource-impoverishededge devices. We first introduce a new sound classification architecture, ACDNet, that produces above state-of-the-art accuracy on both ESC-10 and ESC-50 which are 96.75% and 87.05% respectively. We then compress ACDNet using a novel network-independent approach to obtain an extremely small model. Despite 97.22% size reduction and 97.28% reduction in FLOPs, the compressed network still achieves 82.90% accuracy on ESC-50, staying close to the state-of-the-art. Using 8-bit quantization, we deploy ACD-Net on standard microcontroller units (MCUs). To the best of our knowledge, this is the first time that a deep network for sound classification of 50 classes has successfully been deployed on an edge device. While this should be of interestin its own right, we believe it to be of particular impor-tance that this has been achieved with a universal conver-sion pipeline rather than hand-crafting a network for mini-mal size.      
### 51.Joint Network Topology Inference via Structured Fusion Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2103.03471.pdf)
>  Joint network topology inference represents a canonical problem of jointly learning multiple graph Laplacian matrices from heterogeneous graph signals. In such a problem, a widely employed assumption is that of a simple common component shared among multiple networks. However, in practice, a more intricate topological pattern, comprising simultaneously of sparse, homogeneity and heterogeneity components, would exhibit in multiple networks. In this paper, we propose a general graph estimator based on a novel structured fusion regularization that enables us to jointly learn multiple graph Laplacian matrices with such complex topological patterns, and enjoys both high computational efficiency and rigorous theoretical guarantee. Moreover, in the proposed regularization term, the topological pattern among networks is characterized by a Gram matrix, endowing our graph estimator with the ability of flexible modelling different types of topological patterns by different choices of the Gram matrix. Computationally, the regularization term, coupling the parameters together, makes the formulated optimization problem intractable and thus, we develop a computationally-scalable algorithm based on the alternating direction method of multipliers (ADMM) to solve it efficiently. Theoretically, we provide a theoretical analysis of the proposed graph estimator, which establishes a non-asymptotic bound of the estimation error under the high-dimensional setting and reflects the effect of several key factors on the convergence rate of our algorithm. Finally, the superior performance of the proposed method is illustrated through simulated and real data examples.      
### 52.Haptic Feedback Improves Human-Robot Agreement and User Satisfaction in Shared-Autonomy Teleoperation  [ :arrow_down: ](https://arxiv.org/pdf/2103.03453.pdf)
>  Shared autonomy teleoperation can guarantee safety, but does so by reducing the human operator's control authority, which can lead to reduced levels of human-robot agreement and user satisfaction. This paper presents a novel haptic shared autonomy teleoperation paradigm that uses haptic feedback to inform the user about the inner state of a shared autonomy paradigm, while still guaranteeing safety. This differs from haptic shared control, which uses haptic feedback to inform the user's actions, but gives the human operator full control over the robot's actions. We conducted a user study in which twelve users flew a simulated UAV in a search-and-rescue task with no assistance or assistance provided by haptic shared control, shared autonomy, or haptic shared autonomy. All assistive teleoperation methods use control barrier functions to find a control command that is both safe and as close as possible to the human-generated control command. For assistive teleoperation conditions with haptic feedback, we apply a force to the user that is proportional to the difference between the human-generated control and the safe control. We find that haptic shared autonomy improves the user's task performance and satisfaction. We also find that haptic feedback in assistive teleoperation can improve the user's situational awareness. Finally, results show that adding haptic feedback to shared-autonomy teleoperation can improve human-robot agreement.      
### 53.Sector coupling via hydrogen to lower the cost of energy system decarbonization  [ :arrow_down: ](https://arxiv.org/pdf/2103.03442.pdf)
>  There is growing interest in hydrogen (H$_2$) use for long-duration energy storage in a future electric grid dominated by variable renewable energy (VRE) resources. Modelling the role of H$_2$ as grid-scale energy storage, often referred as "power-to-gas-to-power (P2G2P)" overlooks the cost-sharing and emission benefits from using the deployed H$_2$ production and storage assets to also supply H$_2$ for decarbonizing other end-use sectors where direct electrification may be challenged. Here, we develop a generalized modelling framework for co-optimizing energy infrastructure investment and operation across power and transportation sectors and the supply chains of electricity and H$_2$, while accounting for spatio-temporal variations in energy demand and supply. Applying this sector-coupling framework to the U.S. Northeast under a range of technology cost and carbon price scenarios, we find a greater value of power-to-H$_2$ (P2G) versus P2G2P routes. P2G provides flexible demand response, while the extra cost and efficiency penalties of P2G2P routes make the solution less attractive for grid balancing. The effects of sector-coupling are significant, boosting VRE generation by 12-55% with both increased capacities and reduced curtailments and reducing the total system cost (or levelized costs of energy) by 6-14% under 96% decarbonization scenarios. Both the cost savings and emission reductions from sector coupling increase with H$_2$ demand for other end-uses, more than doubling for a 96% decarbonization scenario as H$_2$ demand quadraples. Moreover, we found that the deployment of carbon capture and storage is more cost-effective in the H$_2$ sector because of the lower cost and higher utilization rate. These findings highlight the importance of using an integrated multi-sector energy system framework with multiple energy vectors in planning energy system decarbonization pathways.      
### 54.Limits of Probabilistic Safety Guarantees when Considering Human Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2103.03388.pdf)
>  When autonomous robots interact with humans, such as during autonomous driving, explicit safety guarantees are crucial in order to avoid potentially life-threatening accidents. Many data-driven methods have explored learning probabilistic bounds over human agents' trajectories (i.e. confidence tubes that contain trajectories with probability $\delta$), which can then be used to guarantee safety with probability $1-\delta$. However, almost all existing works consider $\delta \geq 0.001$. The purpose of this paper is to argue that (1) in safety-critical applications, it is necessary to provide safety guarantees with $\delta &lt; 10^{-8}$, and (2) current learning-based methods are ill-equipped to compute accurate confidence bounds at such low $\delta$. Using human driving data (from the highD dataset), as well as synthetically generated data, we show that current uncertainty models use inaccurate distributional assumptions to describe human behavior and/or require infeasible amounts of data to accurately learn confidence bounds for $\delta \leq 10^{-8}$. These two issues result in unreliable confidence bounds, which can have dangerous implications if deployed on safety-critical systems.      
### 55.Information Encoding/Decoding using the Memory Effect in Fractional-order Capacitive Devices  [ :arrow_down: ](https://arxiv.org/pdf/2103.03362.pdf)
>  In this study, we show that the discharge voltage pattern of a fractional-order supercapacitor from the same initial steady-state voltage into a constant resistor is dependent on the past charging voltage profile. The charging voltage was designed to follow a power-law function, i.e. $v_c(t)=V_{cc} \left( {t}/{t_{ss}}\right)^p \;(0&lt;t \leqslant t_{ss})$, in which $t_{ss}$ (charging time duration between zero voltage to the terminal voltage $V_{cc}$) and $p$ ($0&lt;p&lt;1$) act as two variable parameters. We used this history-dependence of the dynamic behavior of the device to uniquely retrieve information pre-coded in the charging waveform pattern. Furthermore, we provide an analytical model based on fractional calculus that explains phenomenologically the information storage mechanism. The use of this intrinsic material memory effect may lead to new types of methods for information storage and retrieval.      
### 56.Optical data transmission field trial @ 44Tb/s with a 49GHz Kerr soliton crystal microcomb  [ :arrow_down: ](https://arxiv.org/pdf/2103.03354.pdf)
>  We report world record high data transmission over standard optical fiber from a single optical source. We achieve a line rate of 44.2 Terabits per second (Tb/s) employing only the C-band at 1550nm, resulting in a spectral efficiency of 10.4 bits/s/Hz. We use a new and powerful class of micro-comb called soliton crystals that exhibit robust operation and stable generation as well as a high intrinsic efficiency that, together with an extremely low spacing of 48.9 GHz enables a very high coherent data modulation format of 64 QAM. We achieve error free transmission across 75 km of standard optical fiber in the lab and over a field trial with a metropolitan optical fiber network. This work demonstrates the ability of optical micro-combs to exceed other approaches in performance for the most demanding practical optical communications applications.      
### 57.WaveGuard: Understanding and Mitigating Audio Adversarial Examples  [ :arrow_down: ](https://arxiv.org/pdf/2103.03344.pdf)
>  There has been a recent surge in adversarial attacks on deep learning based automatic speech recognition (ASR) systems. These attacks pose new challenges to deep learning security and have raised significant concerns in deploying ASR systems in safety-critical applications. In this work, we introduce WaveGuard: a framework for detecting adversarial inputs that are crafted to attack ASR systems. Our framework incorporates audio transformation functions and analyses the ASR transcriptions of the original and transformed audio to detect adversarial inputs. We demonstrate that our defense framework is able to reliably detect adversarial examples constructed by four recent audio adversarial attacks, with a variety of audio transformation functions. With careful regard for best practices in defense evaluations, we analyze our proposed defense and its strength to withstand adaptive and robust attacks in the audio domain. We empirically demonstrate that audio transformations that recover audio from perceptually informed representations can lead to a strong defense that is robust against an adaptive adversary even in a complete white-box setting. Furthermore, WaveGuard can be used out-of-the box and integrated directly with any ASR model to efficiently detect audio adversarial examples, without the need for model retraining.      
### 58.Performance Analysis and Optimization of Uplink Cellular Networks with Flexible Frame Structure  [ :arrow_down: ](https://arxiv.org/pdf/2103.03342.pdf)
>  Future wireless cellular networks must support both enhanced mobile broadband (eMBB) and ultra reliable low latency communication (URLLC) to manage heterogeneous data traffic for emerging wireless services. To achieve this goal, a promising technique is to enable flexible frame structure by dynamically changing the data frame's numerology according to the channel information as well as traffic quality of service requirements. However, due to nonorthogonal subcarriers, this technique can result in an interference, known as inter numerology interference (INI), thus, degrading the network performance. In this work, a novel framework is proposed to analyze the INI in the uplink cellular communications. In particular, a closed form expression is derived for the INI power in the uplink with a flexible frame structure, and a new resource allocation problem is formulated to maximize the network spectral efficiency (SE) by jointly optimizing the power allocation and numerology selection in a multi user uplink scenario. The simulation results validate the derived theoretical INI analyses and provide guidelines for power allocation and numerology selection.      
### 59.Differential Flatness as a Sufficient Condition to Generate Optimal Trajectories in Real Time  [ :arrow_down: ](https://arxiv.org/pdf/2103.03339.pdf)
>  As robotic systems increase in autonomy, there is a strong need to plan efficient trajectories in real-time. In this paper, we propose an approach to significantly reduce the complexity of solving optimal control problems both numerically and analytically. We exploit the property of differential flatness to show that it is always possible to decouple the forward dynamics of the system's state from the backward dynamics that emerge from the Euler-Lagrange equations. This coupling generally leads to instabilities in numerical approaches; thus, we expect our method to make traditional "shooting" methods a viable choice for optimal trajectory planning in differentially flat systems. To provide intuition for our approach, we also present an illustrative example of generating minimum-thrust trajectories for a quadrotor. Furthermore, we employ quaternions to track the quadrotor's orientation, which, unlike the Euler-angle representation, do not introduce additional singularities into the model.      
### 60.Epistemic Signaling Games for Cyber Deception with Asymmetric Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2103.03287.pdf)
>  This study provides a model of cyber deception with asymmetric recognition represented by private beliefs. Signaling games, which are often used in existing works, are built on the implicit premise that the player's belief is public information. However, this assumption, which leads to symmetric recognition, is unrealistic in adversarial decision making. For precise evaluation of risks arising from cognitive gaps, this paper proposes epistemic signaling games based on the Mertens-Zamir model, which explicitly quantifies players' asymmetric recognition. Equilibria of the games are analytically characterized, and it is found that distinct equilibria are obtained depending on the quantity of cognitive gap. Numerical examples demonstrate analysis using the proposed model.      
### 61.A Deep Learning Approach to Mapping Irrigation: IrrMapper-U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2103.03278.pdf)
>  Accurate maps of irrigation are essential for understanding and managing water resources. We present a new method of mapping irrigation and demonstrate its accuracy for the state of Montana from years 2000-2019. The method is based off of an ensemble of convolutional neural networks that use reflectance information from Landsat imagery to classify irrigated pixels, that we call IrrMapper-U-Net. The methodology does not rely on extensive feature engineering and does not condition the classification with land use information from existing geospatial datasets. The ensemble does not need exhaustive hyperparameter tuning and the analysis pipeline is lightweight enough to be implemented on a personal computer. Furthermore, the proposed methodology provides an estimate of the uncertainty associated with classification. We evaluated our methodology and the resulting irrigation maps using a highly accurate novel spatially-explicit ground truth data set, using county-scale USDA surveys of irrigation extent, and using cadastral surveys. We found that that our method outperforms other methods of mapping irrigation in Montana in terms of overall accuracy and precision. We found that our method agrees better statewide with the USDA National Agricultural Statistics Survey estimates of irrigated area compared to other methods, and has far fewer errors of commission in rainfed agriculture areas. The method learns to mask clouds and ignore Landsat 7 scan-line failures without supervision, reducing the need for preprocessing data. This methodology has the potential to be applied across the entire United States and for the complete Landsat record.      
### 62.Time granularity impact on propagation of disruptions in a system-of-systems simulation of infrastructure and business networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.03247.pdf)
>  System-of-systems (SoS) approach is often used for simulating disruptions to business and infrastructure system networks allowing for integration of several models into one simulation. However, the integration is frequently challenging as each system is designed individually with different characteristics, such as time granularity. Understanding the impact of time granularity on propagation of disruptions between businesses and infrastructure systems and finding the appropriate granularity for the SoS simulation remain as major challenges. To tackle these, we explore how time granularity, recovery time, and disruption size affect the propagation of disruptions between constituent systems of an SoS simulation. To address this issue, we developed a High Level Architecture (HLA) simulation of 3 networks and performed a series of simulation experiments. Our results revealed that time granularity and especially recovery time have huge impact on propagation of disruptions. Consequently, we developed a model for selecting an appropriate time granularity for an SoS simulation based on expected recovery time. Our simulation experiments show that time granularity should be less than 1.13 of expected recovery time. We identified some areas for future research centered around extending the experimental factors space.      
