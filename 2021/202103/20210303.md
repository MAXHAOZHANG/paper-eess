# ArXiv eess --Wed, 3 Mar 2021
### 1.Medical Imaging and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.01938.pdf)
>  Advances in computing power, deep learning architectures, and expert labelled datasets have spurred the development of medical imaging artificial intelligence systems that rival clinical experts in a variety of scenarios. The National Institutes of Health in 2018 identified key focus areas for the future of artificial intelligence in medical imaging, creating a foundational roadmap for research in image acquisition, algorithms, data standardization, and translatable clinical decision support systems. Among the key issues raised in the report: data availability, need for novel computing architectures and explainable AI algorithms, are still relevant despite the tremendous progress made over the past few years alone. Furthermore, translational goals of data sharing, validation of performance for regulatory approval, generalizability and mitigation of unintended bias must be accounted for early in the development process. In this perspective paper we explore challenges unique to high dimensional clinical imaging data, in addition to highlighting some of the technical and ethical considerations in developing high-dimensional, multi-modality, machine learning systems for clinical decision support.      
### 2.Scheduling Optimization of Heterogeneous Services by Resolving Conflicts  [ :arrow_down: ](https://arxiv.org/pdf/2103.01897.pdf)
>  Fifth generation (5G) new radio introduced flexible numerology to provide the necessary flexibility for accommodating heterogeneous services. However, optimizing the scheduling of heterogeneous services with differing delay and throughput requirements over 5G new radio is a challenging task. In this paper, we investigate near optimal, low complexity scheduling of radio resources for ultra-reliable low-latency communications (URLLC) when coexisting with enhanced mobile broadband (eMBB) services. We demonstrate that maximizing the sum throughput of eMBB services while servicing URLLC users, is, in the long-term, equivalent to minimizing the number of URLLC placements in the time-frequency grid; this result stems from reducing the number of infeasible placements for eMBB, to which we refer to as "conflicts". To meet this new objective, we propose and investigate new conflict-aware heuristics; a family of "greedy" and a lightweight heuristic inspired by bin packing optimization, all of near optimal performance. Moreover, having shed light on the impact of conflict in layer-2 scheduling, non-orthogonal multiple access (NOMA) emerges as a competitive approach for conflict resolution, in addition to the well established increased spectral efficiency with respect to OMA. The superior performance of NOMA, thanks to alleviating conflicts,is showcased by extensive numerical results.      
### 3.Spectral nature of soiling and its impact on multi-junction based concentrator systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.01873.pdf)
>  Soiling, which consists of dust, dirt and particles accumulated on the surface of conventional or concentrator photovoltaic modules, absorbs, scatters, and reflects part of the incoming sunlight. Therefore, it reduces the amount of energy converted by the semiconductor solar cells. This work focuses on the effect of soiling on the spectral performance of multi-junction (MJ) cells, widely used in concentrator photovoltaic (CPV) applications. Novel indexes, useful to quantify the spectral impact of soiling are introduced, and their meanings are discussed. The results of a one-year experimental investigation conducted in Spain are presented and are used to discuss how soiling impacts each of the subcells of a MJ cell, as well as the cell current-matching. Results show that soiling affects the current balance among the junctions, i.e. the transmittance losses have found to be around 4% higher in the top than in the middle subcell. The spectral nature of soiling has demonstrated to increase the annual spectral losses of around 2%. Ideal conditions for the mitigation of soiling are also discussed and found to be in blue-rich environments, where the higher light intensity at the shorter wavelengths can limit the impact of soiling on the overall production of the CPV system.      
### 4.Research on the Power Transport Theorem Based Decoupling Mode Theory for Transceiving Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.01853.pdf)
>  The physical pictures of eigen-mode theory (EMT) and the conventional characteristic mode theory (CMT) reveal a fact that: the EMT and CMT are the modal theories for electromagnetic wave-guiding and scattering (for details, please see the Appendices E, F, G and H) systems respectively, rather than for electromagnetic transceiving systems. This Postdoctoral Research Report is devoted to establishing a novel modal theory - decoupling mode theory (DMT) - for transceiving systems, and constructing the energy-decoupled modes (DMs) of objective transceiving system. This Postdoctoral Research Report is a companion volume of the author's Doctoral Dissertation "Research on the Work-Energy Principle Based Characteristic Mode Theory for Scattering Systems" (<a class="link-https" data-arxiv-id="1907.11787" href="https://arxiv.org/abs/1907.11787">arXiv:1907.11787</a>).      
### 5.Deep Reinforcement Learning for URLLC data management on top of scheduled eMBB traffic  [ :arrow_down: ](https://arxiv.org/pdf/2103.01801.pdf)
>  With the advent of 5G and the research into beyond 5G (B5G) networks, a novel and very relevant research issue is how to manage the coexistence of different types of traffic, each with very stringent but completely different requirements. In this paper we propose a deep reinforcement learning (DRL) algorithm to slice the available physical layer resources between ultra-reliable low-latency communications (URLLC) and enhanced Mobile BroadBand (eMBB) traffic. Specifically, in our setting the time-frequency resource grid is fully occupied by eMBB traffic and we train the DRL agent to employ proximal policy optimization (PPO), a state-of-the-art DRL algorithm, to dynamically allocate the incoming URLLC traffic by puncturing eMBB codewords. Assuming that each eMBB codeword can tolerate a certain limited amount of puncturing beyond which is in outage, we show that the policy devised by the DRL agent never violates the latency requirement of URLLC traffic and, at the same time, manages to keep the number of eMBB codewords in outage at minimum levels, when compared to other state-of-the-art schemes.      
### 6.Set-Membership Estimation in Shared Situational Awareness for Automated Vehicles in Occluded Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2103.01791.pdf)
>  One of the main challenges in developing autonomous transport systems based on connected and automated vehicles is the comprehension and understanding of the environment around each vehicle. In many situations, the understanding is limited to the information gathered by the sensors mounted on the ego-vehicle, and it might be severely affected by occlusion caused by other vehicles or fixed obstacles along the road. Situational awareness is the ability to perceive and comprehend a traffic situation and to predict the intent of vehicles and road users in the surrounding of the ego-vehicle. The main objective of this paper is to propose a framework for how to automatically increase the situational awareness for an automatic bus in a realistic scenario when a pedestrian behind a parked truck might decide to walk across the road. Depending on the ego-vehicle's ability to fuse information from sensors in other vehicles or in the infrastructure, shared situational awareness is developed using a set-based estimation technique that provides robust guarantees for the location of the pedestrian. A two-level information fusion architecture is adopted, where sensor measurements are fused locally, and then the corresponding estimates are shared between vehicles and units in the infrastructure. Thanks to the provided safety guarantees, it is possible to appropriately adjust the ego-vehicle speed to maintain a proper safety margin. It is also argued that the framework is suitable for handling sensor failures and false detections in a systematic way. Three scenarios of growing information complexity are considered throughout the study. Simulations show how the increased situational awareness allows the ego-vehicle to maintain a reasonable speed without sacrificing safety.      
### 7.MetaSCI: Scalable and Adaptive Reconstruction for Video Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2103.01786.pdf)
>  To capture high-speed videos using a two-dimensional detector, video snapshot compressive imaging (SCI) is a promising system, where the video frames are coded by different masks and then compressed to a snapshot measurement. Following this, efficient algorithms are desired to reconstruct the high-speed frames, where the state-of-the-art results are achieved by deep learning networks. However, these networks are usually trained for specific small-scale masks and often have high demands of training time and GPU memory, which are hence {\bf \em not flexible} to $i$) a new mask with the same size and $ii$) a larger-scale mask. We address these challenges by developing a Meta Modulated Convolutional Network for SCI reconstruction, dubbed MetaSCI. MetaSCI is composed of a shared backbone for different masks, and light-weight meta-modulation parameters to evolve to different modulation parameters for each mask, thus having the properties of {\bf \em fast adaptation} to new masks (or systems) and ready to {\bf \em scale to large data}. Extensive simulation and real data results demonstrate the superior performance of our proposed approach. Our code is available at {\small\url{<a class="link-external link-https" href="https://github.com/xyvirtualgroup/MetaSCI-CVPR2021" rel="external noopener nofollow">this https URL</a>}}.      
### 8.A matrix-free Levenberg-Marquardt algorithm for efficient ptychographic phase retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2103.01767.pdf)
>  The phase retrieval problem, where one aims to recover a complex-valued image from far-field intensity measurements, is a classic problem encountered in a range of imaging applications. Modern phase retrieval approaches usually rely on gradient descent methods in a nonlinear minimization framework. Calculating closed-form gradients for use in these methods is tedious work, and formulating second order derivatives is even more laborious. Additionally, second order techniques often require the storage and inversion of large matrices of partial derivatives, with memory requirements that can be prohibitive for data-rich imaging modalities. We use a reverse-mode automatic differentiation (AD) framework to implement an efficient matrix-free version of the Levenberg-Marquardt (LM) algorithm, a longstanding method that finds popular use in nonlinear least-square minimization problems but which has seen little use in phase retrieval. Furthermore, we extend the basic LM algorithm so that it can be applied for general constrained optimization problems beyond just the least-square applications. Since we use AD, we only need to specify the physics-based forward model for a specific imaging application; the derivative terms are calculated automatically through matrix-vector products, without explicitly forming any large Jacobian or Gauss-Newton matrices. We demonstrate that this algorithm can be used to solve both the unconstrained ptychographic object retrieval problem and the constrained "blind" ptychographic object and probe retrieval problems, under both the Gaussian and Poisson noise models, and that this method outperforms best-in-class first-order ptychographic reconstruction methods: it provides excellent convergence guarantees with (in many cases) a superlinear rate of convergence, all with a computational cost comparable to, or lower than, the tested first-order algorithms.      
### 9.Transform Network Architectures for Deep Learning based End-to-End Image/Video Coding in Subsampled Color Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2103.01760.pdf)
>  Most of the existing deep learning based end-to-end image/video coding (DLEC) architectures are designed for non-subsampled RGB color format. However, in order to achieve a superior coding performance, many state-of-the-art block-based compression standards such as High Efficiency Video Coding (HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for YUV 4:2:0 format, where U and V components are subsampled by considering the human visual system. This paper investigates various DLEC designs to support YUV 4:2:0 format by comparing their performance against the main profiles of HEVC and VVC standards under a common evaluation framework. Moreover, a new transform network architecture is proposed to improve the efficiency of coding YUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the proposed architecture significantly outperforms naive extensions of existing architectures designed for RGB format and achieves about 10% average BD-rate improvement over the intra-frame coding in HEVC.      
### 10.Simulation of Variable Speed Wind Turbines based on Open-Source Solutions: application to Bachelor and Master Degrees  [ :arrow_down: ](https://arxiv.org/pdf/2103.01759.pdf)
>  This paper describes variable speed wind turbine (Types 3 and 4, IEC 61400-27-1) simulations based on an open-source solution to be applied to Bachelor and Master Degrees. It is an attempt to improve the education quality of such sustainable energy by giving an open-source experimental environment for both undergraduate and graduate students. Indeed, among the renewable sources, wind energy is currently becoming essential in most power systems. The simulations include both one-mass and two-mass mechanical models, as well as pitch angle control. A general overview of the structure, control, and operation of the variable speed wind turbine is provided by these easy-to-use interactive virtual experiments. In addition, a comparison between commercial and open-source software packages is described and discussed in detail. Examples and extensive results are also included in the paper. The models are available in Scilab-Xcos file exchange for power system education and researcher communities.      
### 11.Super-resolving Compressed Images via Parallel and Series Integration of Artifact Reduction and Resolution Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2103.01698.pdf)
>  In real-world applications, images may be not only sub-sampled but also heavily compressed thus often containing various artifacts. Simple methods for enhancing the resolution of such images will exacerbate the artifacts, rendering them visually objectionable. In spite of its high practical values, super-resolving compressed images is not well studied in the literature. In this paper, we propose a novel compressed image super resolution (CISR) framework based on parallel and series integration of artifact removal and resolution enhancement. Based on maximum a posterior inference for estimating a clean low-resolution (LR) input image and a clean high resolution (HR) output image from down-sampled and compressed observations, we have designed a CISR architecture consisting of two deep neural network modules: the artifact reduction module (ARM) and resolution enhancement module (REM). ARM and REM work in parallel with both taking the compressed LR image as their inputs, while they also work in series with REM taking the output of ARM as one of its inputs and ARM taking the output of REM as its other input. A unique property of our CSIR system is that a single trained model is able to super-resolve LR images compressed by different methods to various qualities. This is achieved by exploiting deep neural net-works capacity for handling image degradations, and the parallel and series connections between ARM and REM to reduce the dependency on specific degradations. ARM and REM are trained simultaneously by the deep unfolding technique. Experiments are conducted on a mixture of JPEG and WebP compressed images without a priori knowledge of the compression type and com-pression factor. Visual and quantitative comparisons demonstrate the superiority of our method over state-of-the-art super resolu-tion methods.      
### 12.SimHumalator: An Open Source WiFi Based Passive Radar Human Simulator For Activity Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2103.01677.pdf)
>  This work presents a simulation framework to generate human micro-Dopplers in WiFi based passive radar scenarios, wherein we simulate IEEE 802.11g complaint WiFi transmissions using MATLAB's WLAN toolbox and human animation models derived from a marker-based motion capture system. We integrate WiFi transmission signals with the human animation data to generate the micro-Doppler features that incorporate the diversity of human motion characteristics, and the sensor parameters. In this paper, we consider five human activities. We uniformly benchmark the classification performance of multiple machine learning and deep learning models against a common dataset. Further, we validate the classification performance using the real radar data captured simultaneously with the motion capture system. We present experimental results using simulations and measurements demonstrating good classification accuracy of $\geq$ 95\% and $\approx$ 90\%, respectively.      
### 13.Long-Running Speech Recognizer:An End-to-End Multi-Task Learning Framework for Online ASR and VAD  [ :arrow_down: ](https://arxiv.org/pdf/2103.01661.pdf)
>  When we use End-to-end automatic speech recognition (E2E-ASR) system for real-world applications, a voice activity detection (VAD) system is usually needed to improve the performance and to reduce the computational cost by discarding non-speech parts in the audio. This paper presents a novel end-to-end (E2E), multi-task learning (MTL) framework that integrates ASR and VAD into one model. The proposed system, which we refer to as Long-Running Speech Recognizer (LR-SR), learns ASR and VAD jointly from two seperate task-specific datasets in the training stage. With the assistance of VAD, the ASR performance improves as its connectionist temporal classification (CTC) loss function can leverage the VAD alignment information. In the inference stage, the LR-SR system removes non-speech parts at low computational cost and recognizes speech parts with high robustness. Experimental results on segmented speech data show that the proposed MTL framework outperforms the baseline single-task learning (STL) framework in ASR task. On unsegmented speech data, we find that the LR-SR system outperforms the baseline ASR systems that build an extra GMM-based or DNN-based voice activity detector.      
### 14.Minimizing Information Leakage of Abrupt Changes in Stochastic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.01658.pdf)
>  This work investigates the problem of analyzing privacy for general Markov processes. These processes may be affected by changes, or exogenous signals, that need to remain private. Privacy refers to the disclosure of information of these changes through observations of the underlying Markov chain. In contrast to previous work on privacy, we study the problem for an online sequence of data. We use theoretical tools from optimal detection theory to motivate a definition of online privacy based on the average amount of information per observation of the stochastic system in consideration. Two cases are considered: the full-information case, where the eavesdropper measures all but the signals that indicate a change, and the limited-information case, where the eavesdropper only measures the state of the Markov process. For both cases, we provide ways to derive privacy upper-bounds and compute policies that attain a higher privacy level. It turns out that the problem of computing privacy-aware policies is concave, and we conclude with some examples and numerical simulations for both cases.      
### 15.Efficient Deep Image Denoising via Class Specific Convolution  [ :arrow_down: ](https://arxiv.org/pdf/2103.01624.pdf)
>  Deep neural networks have been widely used in image denoising during the past few years. Even though they achieve great success on this problem, they are computationally inefficient which makes them inappropriate to be implemented in mobile devices. In this paper, we propose an efficient deep neural network for image denoising based on pixel-wise classification. Despite using a computationally efficient network cannot effectively remove the noises from any content, it is still capable to denoise from a specific type of pattern or texture. The proposed method follows such a divide and conquer scheme. We first use an efficient U-net to pixel-wisely classify pixels in the noisy image based on the local gradient statistics. Then we replace part of the convolution layers in existing denoising networks by the proposed Class Specific Convolution layers (CSConv) which use different weights for different classes of pixels. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method can reduce the computational costs without sacrificing the performance compared to state-of-the-art algorithms.      
### 16.A Practical Framework for ROI Detection in Medical Images -- a case study for hip detection in anteroposterior pelvic radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2103.01584.pdf)
>  Purpose Automated detection of region of interest (ROI) is a critical step for many medical image applications such as heart ROIs detection in perfusion MRI images, lung boundary detection in chest X-rays, and femoral head detection in pelvic radiographs. Thus, we proposed a practical framework of ROIs detection in medical images, with a case study for hip detection in anteroposterior (AP) pelvic radiographs. <br>Materials and Methods: We conducted a retrospective study which analyzed hip joints seen on 7,399 AP pelvic radiographs from three diverse sources, including 4,290 high resolution radiographs from Chang Gung Memorial Hospital Osteoarthritis, 3,008 low to medium resolution radiographs from Osteoarthritis Initiative, and 101 heterogeneous radiographs from Google image search engine. We presented a deep learning-based ROI detection framework utilizing single-shot multi-box detector (SSD) with ResNet-101 backbone and customized head structure based on the characteristics of the obtained datasets, whose ground truths were labeled by non-medical annotators in a simple graphical interface. <br>Results: Our method achieved average intersection over union (IoU)=0.8115, average confidence=0.9812, and average precision with threshold IoU=0.5 (AP50)=0.9901 in the independent test set, suggesting that the detected hip regions have appropriately covered main features of the hip joints. <br>Conclusion: The proposed approach featured on low-cost labeling, data-driven model design, and heterogeneous data testing. We have demonstrated the feasibility of training a robust hip region detector for AP pelvic radiographs. This practical framework has a promising potential for a wide range of medical image applications.      
### 17.Intelligent Spectrum Learning for Wireless Networks with Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2103.01531.pdf)
>  Reconfigurable intelligent surface (RIS) has become a promising technology for enhancing the reliability of wireless communications, which is capable of reflecting the desired signals through appropriate phase shifts. However, the intended signals that impinge upon an RIS are often mixed with interfering signals, which are usually dynamic and unknown. In particular, the received signal-to-interference-plus-noise ratio (SINR) may be degraded by the signals reflected from the RISs that originate from non-intended users. To tackle this issue, we introduce the concept of intelligent spectrum learning (ISL), which uses an appropriately trained convolutional neural network (CNN) at the RIS controller to help the RISs infer the interfering signals directly from the incident signals. By capitalizing on the ISL, a distributed control algorithm is proposed to maximize the received SINR by dynamically configuring the active/inactive binary status of the RIS elements. Simulation results validate the performance improvement offered by deep learning and demonstrate the superiority of the proposed ISL-aided approach.      
### 18.Feature-Align Network and Knowledge Distillation for Efficient Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2103.01524.pdf)
>  Deep learning-based RAW image denoising is a quintessential problem in image restoration. Recent works have pushed the state-of-the-art in denoising image quality. However, many of these networks are computationally too expensive for efficient use in mobile devices. Here, we propose a novel network for efficient RAW denoising on mobile devices. Our contributions are: (1) An efficient encoder-decoder network augmented with a new Feature-Align layer to attend to spatially varying noise. (2) A new perceptual Feature Loss calculated in the RAW domain to preserve high frequency image content. (3) An analysis of the use of multiple models tuned to different subranges of noise levels. (4) An open-source RAW noisy-clean paired dataset with noise modeling, to facilitate research in RAW denoising. We evaluate the effectiveness of our proposed network and training techniques and show results that compete with the state-of-the-art network, while using significantly fewer parameters and MACs. On the Darmstadt Noise Dataset benchmark, we achieve a PSNR of 48.28dB, with 263 times fewer MACs, and 17.6 times fewer parameters than the state-of-the-art network, which achieves 49.12 dB.      
### 19.Heimdall: an AI-based infrastructure for traffic monitoring and anomalies detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.01506.pdf)
>  Since their appearance, Smart Cities have aimed at improving the daily life of people, helping to make public services smarter and more efficient. Several of these services are often intended to provide better security conditions for citizens and drivers. In this vein, we present Heimdall, an AI-based video surveillance system for traffic monitoring and anomalies detection. The proposed system features three main tiers: a ground level, consisting of a set of smart lampposts equipped with cameras and sensors, and an advanced AI unit for detecting accidents and traffic anomalies in real time; a territorial level, which integrates and combines the information collected from the different lampposts, and cross-correlates it with external data sources, in order to coordinate and handle warnings and alerts; a training level, in charge of continuously improving the accuracy of the modules that have to sense the environment. Finally, we propose and discuss an early experimental approach for the detection of anomalies, based on a Faster R-CNN, and adopted in the proposed infrastructure.      
### 20.Time-Optimal Navigation in Uncertain Environments with High-Level Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2103.01476.pdf)
>  Mixed observable Markov decision processes (MOMDPs) are a modeling framework for autonomous systems described by both fully and partially observable states. In this work, we study the problem of synthesizing a control policy for MOMDPs that minimizes the expected time to complete the control task while satisfying syntactically co-safe Linear Temporal Logic (scLTL) specifications. First, we present an exact dynamic programming update to compute the value function. Afterwards, we propose a point-based approximation, which allows us to compute a lower bound of the closed-loop probability of satisfying the specifications. The effectiveness of the proposed approach and comparisons with standard strategies are shown on high-fidelity navigation tasks with partially observable static obstacles.      
### 21.Tune-In: Training Under Negative Environments with Interference for Attention Networks Simulating Cocktail Party Effect  [ :arrow_down: ](https://arxiv.org/pdf/2103.01461.pdf)
>  We study the cocktail party problem and propose a novel attention network called Tune-In, abbreviated for training under negative environments with interference. It firstly learns two separate spaces of speaker-knowledge and speech-stimuli based on a shared feature space, where a new block structure is designed as the building block for all spaces, and then cooperatively solves different tasks. Between the two spaces, information is cast towards each other via a novel cross- and dual-attention mechanism, mimicking the bottom-up and top-down processes of a human's cocktail party effect. It turns out that substantially discriminative and generalizable speaker representations can be learnt in severely interfered conditions via our self-supervised training. The experimental results verify this seeming paradox. The learnt speaker embedding has superior discriminative power than a standard speaker verification method; meanwhile, Tune-In achieves remarkably better speech separation performances in terms of SI-SNRi and SDRi consistently in all test modes, and especially at lower memory and computational consumption, than state-of-the-art benchmark systems.      
### 22.Social Profit Optimization with Demand Response Management in Electricity Market: A Multi-timescale Leader-following Approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.01452.pdf)
>  In the electricity market, it is quite common that the market participants make "selfish" strategies to harvest the maximum profits for themselves, which may cause the social benefit loss and impair the sustainability of the society in the long term. Regarding this issue, in this work, we will discuss how the social profit can be improved through strategic demand response (DR) management. Specifically, we explore two interaction mechanisms in the market: Nash equilibrium (NE) and Stackelberg equilibrium (SE) among utility companies (UCs) and user-UC interactions, respectively. At the user side, each user determines the optimal energy-purchasing strategy to maximize its own profit. At the UC side, a governmental UC (g-UC) is considered, who aims to optimize the social profit of the market. Meanwhile, normal UCs play games to maximize their own profits. As a result, a basic leader-following problem among the UCs is formulated under the coordination of the independent system operator (ISO). Moreover, by using our proposed demand function amelioration (DFA) strategy, a multi-timescale leader-following problem is formulated. In this case, the maximal market efficiency can be achieved without changing the "selfish instinct" of normal UCs. In addition, by considering the local constraints for the UCs, two projection-based pricing algorithms are proposed for UCs, which can provide approximate optimal solutions for the resulting non-convex social profit optimization problems. The feasibility of the proposed algorithms is verified by using the concept of price of anarchy (PoA) in a multi-UC multi-user market model in the simulation.      
### 23.Local Change Point Detection and Signal Cleaning using EEMD with applications to Acoustic Shockwaves and Cardiac Signals  [ :arrow_down: ](https://arxiv.org/pdf/2103.01352.pdf)
>  With the ability to create time varying basis functions, the Ensemble Empirical Mode Decomposition (EEMD) has quickly become the preferred way to decompose nonlinear and nonstationary signals. However, we find current EEMD signal cleaning techniques lacking, unable to deal with the nonlinearities that are common for the complex signals that the EEMD is used for. By combining change point detection and a new sparse basis function optimization problem, we are able to show that it is possible to create unique filters for each change point which emphasize the basis functions that are observing a change. This not only allows one to understand which frequency bands are observing a change, but cleaning the signal to emphasize changes can lead to improved signal classification accuracy. We show that this technique has implications for a variety of applications including acoustics and medicine. The technique is implemented in R via the \textbf{LCDSC} package.      
### 24.Robust 3D U-Net Segmentation of Macular Holes  [ :arrow_down: ](https://arxiv.org/pdf/2103.01299.pdf)
>  Macular holes are a common eye condition which result in visual impairment. We look at the application of deep convolutional neural networks to the problem of macular hole segmentation. We use the 3D U-Net architecture as a basis and experiment with a number of design variants. Manually annotating and measuring macular holes is time consuming and error prone. Previous automated approaches to macular hole segmentation take minutes to segment a single 3D scan. Our proposed model generates significantly more accurate segmentations in less than a second. We found that an approach of architectural simplification, by greatly simplifying the network capacity and depth, exceeds both expert performance and state-of-the-art models such as residual 3D U-Nets.      
### 25.Statistics for Building Synthetic Power System Cyber Models  [ :arrow_down: ](https://arxiv.org/pdf/2103.01275.pdf)
>  A realistic communication system model is critical in power system studies emphasizing the cyber and physical intercoupling. In this paper, we provide characteristics that could be used in modeling the underlying cyber network for power grid models. A real utility communication network and a simplified inter-substation connectivity model are studied, and their statistics could be used to fulfill the requirements for different modeling resolutions.      
### 26.Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image  [ :arrow_down: ](https://arxiv.org/pdf/2103.01263.pdf)
>  The most common technique for generating B-mode ultrasound (US) images is delay and sum (DAS) beamforming, where the signals received at the transducer array are sampled before an appropriate delay is applied. This necessitates sampling rates exceeding the Nyquist rate and the use of a large number of antenna elements to ensure sufficient image quality. Recently we proposed methods to reduce the sampling rate and the array size relying on image recovery using iterative algorithms, based on compressed sensing (CS) and the finite rate of innovation (FRI) frameworks. Iterative algorithms typically require a large number of iterations, making them difficult to use in real-time. Here, we propose a reconstruction method from sub-Nyquist samples in the time and spatial domain, that is based on unfolding the ISTA algorithm, resulting in an efficient and interpretable deep network. The inputs to our network are the subsampled beamformed signals after summation and delay in the frequency domain, requiring only a subset of the US signal to be stored for recovery. Our method allows reducing the number of array elements, sampling rate, and computational time while ensuring high quality imaging performance. Using \emph{in vivo} data we demonstrate that the proposed method yields high-quality images while reducing the data volume traditionally used up to 36 times. In terms of image resolution and contrast, our technique outperforms previously suggested methods as well as DAS and minimum-variance (MV) beamforming, paving the way to real-time applicable recovery methods.      
### 27.Meta-Learning-Based Robust Adaptive Flight Control Under Uncertain Wind Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2103.01932.pdf)
>  Realtime model learning proves challenging for complex dynamical systems, such as drones flying in variable wind conditions. Machine learning technique such as deep neural networks have high representation power but is often too slow to update onboard. On the other hand, adaptive control relies on simple linear parameter models can update as fast as the feedback control loop. We propose an online composite adaptation method that treats outputs from a deep neural network as a set of basis functions capable of representing different wind conditions. To help with training, meta-learning techniques are used to optimize the network output useful for adaptation. We validate our approach by flying a drone in an open air wind tunnel under varying wind conditions and along challenging trajectories. We compare the result with other adaptive controller with different basis function sets and show improvement over tracking and prediction errors.      
### 28.SoundCLR: Contrastive Learning of Representations For Improved Environmental Sound Classification  [ :arrow_down: ](https://arxiv.org/pdf/2103.01929.pdf)
>  Environmental Sound Classification (ESC) is a challenging field of research in non-speech audio processing. Most of current research in ESC focuses on designing deep models with special architectures tailored for specific audio datasets, which usually cannot exploit the intrinsic patterns in the data. However recent studies have surprisingly shown that transfer learning from models trained on ImageNet is a very effective technique in ESC. Herein, we propose SoundCLR, a supervised contrastive learning method for effective environment sound classification with state-of-the-art performance, which works by learning representations that disentangle the samples of each class from those of other classes. Our deep network models are trained by combining a contrastive loss that contributes to a better probability output by the classification layer with a cross-entropy loss on the output of the classifier layer to map the samples to their respective 1-hot encoded labels. Due to the comparatively small sizes of the available environmental sound datasets, we propose and exploit a transfer learning and strong data augmentation pipeline and apply the augmentations on both the sound signals and their log-mel spectrograms before inputting them to the model. Our experiments show that our masking based augmentation technique on the log-mel spectrograms can significantly improve the recognition performance. Our extensive benchmark experiments show that our hybrid deep network models trained with combined contrastive and cross-entropy loss achieved the state-of-the-art performance on three benchmark datasets ESC-10, ESC-50, and US8K with validation accuracies of 99.75\%, 93.4\%, and 86.49\% respectively. The ensemble version of our models also outperforms other top ensemble methods. The code is available at <a class="link-external link-https" href="https://github.com/alireza-nasiri/SoundCLR" rel="external noopener nofollow">this https URL</a>.      
### 29.Investigations on Audiovisual Emotion Recognition in Noisy Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2103.01894.pdf)
>  In this paper we explore audiovisual emotion recognition under noisy acoustic conditions with a focus on speech features. We attempt to answer the following research questions: (i) How does speech emotion recognition perform on noisy data? and (ii) To what extend does a multimodal approach improve the accuracy and compensate for potential performance degradation at different noise levels? We present an analytical investigation on two emotion datasets with superimposed noise at different signal-to-noise ratios, comparing three types of acoustic features. Visual features are incorporated with a hybrid fusion approach: The first neural network layers are separate modality-specific ones, followed by at least one shared layer before the final prediction. The results show a significant performance decrease when a model trained on clean audio is applied to noisy data and that the addition of visual features alleviates this effect.      
### 30.Listen, Read, and Identify: Multimodal Singing Language Identification  [ :arrow_down: ](https://arxiv.org/pdf/2103.01893.pdf)
>  We propose a multimodal singing language classification model that uses both audio content and textual metadata. LRID-Net, the proposed model, takes an audio signal and a language probability vector estimated from the metadata and outputs the probabilities of the ten target languages. Optionally, LRID-Net is facilitated with modality dropouts to handle a missing modality. In the experiment, we trained several LRID-Nets with varying modality dropout configuration and test them with various combinations of input modalities. The experiment results demonstrate that using multimodal input improves the performance. The results also suggest that adopting modality dropout does not degrade performance of the model when there are full modality inputs while enabling the model to handle missing modality cases to some extent.      
### 31.Data-driven MIMO control of room temperature and bidirectional EV charging using deep reinforcement learning: simulation and experiments  [ :arrow_down: ](https://arxiv.org/pdf/2103.01886.pdf)
>  The control of modern buildings is, on one hand, a complex multi-variable control problem due to the integration of renewable energy generation devices, storage devices, and connection of electrical vehicles (EVs), and, on the other hand, a complex multi-criteria problem due to requirements for overall energy minimization and comfort satisfaction. Both conventional rule-based (RB) and advanced model-based controllers, such as model predictive control (MPC), cannot fulfil the current building automation industry requirements of achieving system-wide optimal performance of a modern building at low commissioning and maintenance costs. In this work, we present a fully black-box, data-driven method to obtain a control policy for a multi-input-multi-output (MIMO) problem in buildings -- the joint control of a room temperature and a bidirectional EV charging -- with the aim to maximize occupants comfort and energy savings while leaving enough energy in the EV battery for the next trip. We modelled the room temperature and EV charging using recurrent neural networks and a piece-wise linear function, respectively, and used these models as a simulation environment for the Deep Deterministic Policy Gradient (DDPG) reinforcement learning algorithm to find an optimal control policy. In the simulation, the DDPG control agent achieved on average 17% energy savings and 19% better comfort during the heating season compared to a standard RB controller. Similarly, for the joint room heating and bidirectional EV charging control, the DDPG MIMO controller achieved on average 12% better comfort satisfaction, 11% energy savings, and 42% energy costs savings compared to two standard RB controllers. We also validated the method on the DFAB HOUSE at Empa, Duebendorf, in Switzerland where we obtained 27% energy savings at better comfort over three weeks during the heating season.      
### 32.HED-UNet: Combined Segmentation and Edge Detection for Monitoring the Antarctic Coastline  [ :arrow_down: ](https://arxiv.org/pdf/2103.01849.pdf)
>  Deep learning-based coastline detection algorithms have begun to outshine traditional statistical methods in recent years. However, they are usually trained only as single-purpose models to either segment land and water or delineate the coastline. In contrast to this, a human annotator will usually keep a mental map of both segmentation and delineation when performing manual coastline detection. To take into account this task duality, we therefore devise a new model to unite these two approaches in a deep learning model. By taking inspiration from the main building blocks of a semantic segmentation framework (UNet) and an edge detection framework (HED), both tasks are combined in a natural way. Training is made efficient by employing deep supervision on side predictions at multiple resolutions. Finally, a hierarchical attention mechanism is introduced to adaptively merge these multiscale predictions into the final model output. The advantages of this approach over other traditional and deep learning-based methods for coastline detection are demonstrated on a dataset of Sentinel-1 imagery covering parts of the Antarctic coast, where coastline detection is notoriously difficult. An implementation of our method is available at \url{<a class="link-external link-https" href="https://github.com/khdlr/HED-UNet" rel="external noopener nofollow">this https URL</a>}.      
### 33.Expectation-Maximization-Aided Hybrid Generalized Expectation Consistent for Sparse Signal Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2103.01833.pdf)
>  The reconstruction of sparse signal is an active area of research. Different from a typical i.i.d. assumption, this paper considers a non-independent prior of group structure. For this more practical setup, we propose EM-aided HyGEC, a new algorithm to address the stability issue and the hyper-parameter issue facing the other algorithms. The instability problem results from the ill condition of the transform matrix, while the unavailability of the hyper-parameters is a ground truth that their values are not known beforehand. The proposed algorithm is built on the paradigm of HyGAMP (proposed by Rangan et al.) but we replace its inner engine, the GAMP, by a matrix-insensitive alternative, the GEC, so that the first issue is solved. For the second issue, we take expectation-maximization as an outer loop, and together with the inner engine HyGEC, we learn the value of the hyper-parameters. Effectiveness of the proposed algorithm is also verified by means of numerical simulations.      
### 34.Audio scene monitoring using redundant un-localized microphone arrays  [ :arrow_down: ](https://arxiv.org/pdf/2103.01830.pdf)
>  We present a system for localizing sound sources in a room with several microphone arrays. Unlike most existing approaches, the positions of the arrays in space are assumed to be unknown. Each circular array performs direction of arrival (DOA) estimation independently. The DOAs are then fed to a fusion center where they are concatenated and used to perform the localization based on two proposed methods, which require only few labeled source locations for calibration. The first proposed method is based on principal component analysis (PCA) of the observed DOA and does not require any calibration. The array cluster can then perform localization on a manifold defined by the PCA of concatenated DOAs over time. The second proposed method performs localization using an affine transformation between the DOA vectors and the room manifold. The PCA approach has fewer requirements on the training sequence, but is less robust to missing DOAs from one of the arrays. The approach is demonstrated with a set of five 8-microphone circular arrays, placed at unknown fixed locations in an office. Both the PCA approach and the direct approach can easily map out a rectangle based on a few calibration points with similar accuracy as calibration points. The methods demonstrated here provide a step towards monitoring activities in a smart home and require little installation effort as the array locations are not needed.      
### 35.Terahertz Ultra-Massive MIMO-Based Aeronautical Communications in Space-Air-Ground Integrated Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.01829.pdf)
>  The emerging space-air-ground integrated network has attracted intensive research and necessitates reliable and efficient aeronautical communications. This paper investigates terahertz Ultra-Massive (UM)-MIMO-based aeronautical communications and proposes an effective channel estimation and tracking scheme, which can solve the performance degradation problem caused by the unique {\emph{triple delay-beam-Doppler squint effects}} of aeronautical terahertz UM-MIMO channels. Specifically, based on the rough angle estimates acquired from navigation information, an initial aeronautical link is established, where the delay-beam squint at transceiver can be significantly mitigated by employing a Grouping True-Time Delay Unit (GTTDU) module (e.g., the designed {\emph{Rotman lens}}-based GTTDU module). According to the proposed prior-aided iterative angle estimation algorithm, azimuth/elevation angles can be estimated, and these angles are adopted to achieve precise beam-alignment and refine GTTDU module for further eliminating delay-beam squint. Doppler shifts can be subsequently estimated using the proposed prior-aided iterative Doppler shift estimation algorithm. On this basis, path delays and channel gains can be estimated accurately, where the Doppler squint can be effectively attenuated via compensation process. For data transmission, a data-aided decision-directed based channel tracking algorithm is developed to track the beam-aligned effective channels. When the data-aided channel tracking is invalid, angles will be re-estimated at the pilot-aided channel tracking stage with an equivalent sparse digital array, where angle ambiguity can be resolved based on the previously estimated angles. The simulation results and the derived Cramér-Rao lower bounds verify the effectiveness of our solution.      
### 36.A Structurally Regularized Convolutional Neural Network for Image Classification using Wavelet-based SubBand Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2103.01823.pdf)
>  We propose a convolutional neural network (CNN) architecture for image classification based on subband decomposition of the image using wavelets. The proposed architecture decomposes the input image spectra into multiple critically sampled subbands, extracts features using a single CNN per subband, and finally, performs classification by combining the extracted features using a fully connected layer. Processing each of the subbands by an individual CNN, thereby limiting the learning scope of each CNN to a single subband, imposes a form of structural regularization. This provides better generalization capability as seen by the presented results. The proposed architecture achieves best-in-class performance in terms of total multiply-add-accumulator operations and nearly best-in-class performance in terms of total parameters required, yet it maintains competitive classification performance. We also show the proposed architecture is more robust than the regular full-band CNN to noise caused by weight-and-bias quantization and input quantization.      
### 37.Virufy: A Multi-Branch Deep Learning Network for Automated Detection of COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2103.01806.pdf)
>  Fast and affordable solutions for COVID-19 testing are necessary to contain the spread of the global pandemic and help relieve the burden on medical facilities. Currently, limited testing locations and expensive equipment pose difficulties for individuals trying to be tested, especially in low-resource settings. Researchers have successfully presented models for detecting COVID-19 infection status using audio samples recorded in clinical settings [5, 15], suggesting that audio-based Artificial Intelligence models can be used to identify COVID-19. Such models have the potential to be deployed on smartphones for fast, widespread, and low-resource testing. However, while previous studies have trained models on cleaned audio samples collected mainly from clinical settings, audio samples collected from average smartphones may yield suboptimal quality data that is different from the clean data that models were trained on. This discrepancy may add a bias that affects COVID-19 status predictions. To tackle this issue, we propose a multi-branch deep learning network that is trained and tested on crowdsourced data where most of the data has not been manually processed and cleaned. Furthermore, the model achieves state-of-art results for the COUGHVID dataset [16]. After breaking down results for each category, we have shown an AUC of 0.99 for audio samples with COVID-19 positive labels.      
### 38.HW/SW Framework for Improving the Safety of Implantable and Wearable Medical Devices  [ :arrow_down: ](https://arxiv.org/pdf/2103.01781.pdf)
>  Implantable and wearable medical devices (IWMDs) are widely used for the monitoring and therapy of an increasing range of medical conditions. Improvements in medical devices, enabled by advances in low-power processors, more complex firmware, and wireless connectivity, have greatly improved therapeutic outcomes and patients' quality-of-life. However, security attacks, malfunctions and sometimes user errors have raised great concerns regarding the safety of IWMDs. In this work, we present a HW/SW (Hardware/Software) framework for improving the safety of IWMDs, wherein a set of safety rules and a rule check mechanism are used to monitor both the extrinsic state (the patient's physiological parameters sensed by the IWMD) and the internal state of the IWMD (I/O activities of the microcontroller) to infer unsafe operations that may be triggered by user errors, software bugs, or security attacks. We discuss how this approach can be realized in the context of a artificial pancreas with wireless connectivity and implement a prototype to demonstrate its effectiveness in improving safety at modest overheads.      
### 39.A region-based descriptor network for uniformly sampled keypoints  [ :arrow_down: ](https://arxiv.org/pdf/2103.01780.pdf)
>  Matching keypoint pairs of different images is a basic task of computer vision. Most methods require customized extremum point schemes to obtain the coordinates of feature points with high confidence, which often need complex algorithmic design or a network with higher training difficulty and also ignore the possibility that flat regions can be used as candidate regions of matching points. In this paper, we design a region-based descriptor by combining the context features of a deep network. The new descriptor can give a robust representation of a point even in flat regions. By the new descriptor, we can obtain more high confidence matching points without extremum operation. The experimental results show that our proposed method achieves a performance comparable to state-of-the-art.      
### 40.Graph-Time Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.01730.pdf)
>  Spatiotemporal data can be represented as a process over a graph, which captures their spatial relationships either explicitly or implicitly. How to leverage such a structure for learning representations is one of the key challenges when working with graphs. In this paper, we represent the spatiotemporal relationships through product graphs and develop a first principle graph-time convolutional neural network (GTCNN). The GTCNN is a compositional architecture with each layer comprising a graph-time convolutional module, a graph-time pooling module, and a nonlinearity. We develop a graph-time convolutional filter by following the shift-and-sum principles of the convolutional operator to learn higher-level features over the product graph. The product graph itself is parametric so that we can learn also the spatiotemporal coupling from data. We develop a zero-pad pooling that preserves the spatial graph (the prior about the data) while reducing the number of active nodes and the parameters. Experimental results with synthetic and real data corroborate the different components and compare with baseline and state-of-the-art solutions.      
### 41.Transportation Density Reduction Caused by City Lockdowns Across the World during the COVID-19 Epidemic: From the View of High-resolution Remote Sensing Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2103.01717.pdf)
>  As the COVID-19 epidemic began to worsen in the first months of 2020, stringent lockdown policies were implemented in numerous cities throughout the world to control human transmission and mitigate its spread. Although transportation density reduction inside the city was felt subjectively, there has thus far been no objective and quantitative study of its variation to reflect the intracity population flows and their corresponding relationship with lockdown policy stringency from the view of remote sensing images with the high resolution under 1m. Accordingly, we here provide a quantitative investigation of the transportation density reduction before and after lockdown was implemented in six epicenter cities (Wuhan, Milan, Madrid, Paris, New York, and London) around the world during the COVID-19 epidemic, which is accomplished by extracting vehicles from the multi-temporal high-resolution remote sensing images. A novel vehicle detection model combining unsupervised vehicle candidate extraction and deep learning identification was specifically proposed for the images with the resolution of 0.5m. Our results indicate that transportation densities were reduced by an average of approximately 50% (and as much as 75.96%) in these six cities following lockdown. The influences on transportation density reduction rates are also highly correlated with policy stringency, with an R^2 value exceeding 0.83. Even within a specific city, the transportation density changes differed and tended to be distributed in accordance with the city's land-use patterns. Considering that public transportation was mostly reduced or even forbidden, our results indicate that city lockdown policies are effective at limiting human transmission within cities.      
### 42.Probabilistic Inference for Structural Health Monitoring: New Modes of Learning from Data  [ :arrow_down: ](https://arxiv.org/pdf/2103.01676.pdf)
>  In data-driven SHM, the signals recorded from systems in operation can be noisy and incomplete. Data corresponding to each of the operational, environmental, and damage states are rarely available a priori; furthermore, labelling to describe the measurements is often unavailable. In consequence, the algorithms used to implement SHM should be robust and adaptive, while accommodating for missing information in the training-data -- such that new information can be included if it becomes available. By reviewing novel techniques for statistical learning (introduced in previous work), it is argued that probabilistic algorithms offer a natural solution to the modelling of SHM data in practice. In three case-studies, probabilistic methods are adapted for applications to SHM signals -- including semi-supervised learning, active learning, and multi-task learning.      
### 43.Solving Inverse Problems by Joint Posterior Maximization with Autoencoding Prior  [ :arrow_down: ](https://arxiv.org/pdf/2103.01648.pdf)
>  In this work we address the problem of solving ill-posed inverse problems in imaging where the prior is a variational autoencoder (VAE). Specifically we consider the decoupled case where the prior is trained once and can be reused for many different log-concave degradation models without retraining. Whereas previous MAP-based approaches to this problem lead to highly non-convex optimization algorithms, our approach computes the joint (space-latent) MAP that naturally leads to alternate optimization algorithms and to the use of a stochastic encoder to accelerate computations. The resulting technique (JPMAP) performs Joint Posterior Maximization using an Autoencoding Prior. We show theoretical and experimental evidence that the proposed objective function is quite close to bi-convex. Indeed it satisfies a weak bi-convexity property which is sufficient to guarantee that our optimization scheme converges to a stationary point. We also highlight the importance of correctly training the VAE using a denoising criterion, in order to ensure that the encoder generalizes well to out-of-distribution images, without affecting the quality of the generative model. This simple modification is key to providing robustness to the whole procedure. Finally we show how our joint MAP methodology relates to more common MAP approaches, and we propose a continuation scheme that makes use of our JPMAP algorithm to provide more robust MAP estimates. Experimental results also show the higher quality of the solutions obtained by our JPMAP approach with respect to other non-convex MAP approaches which more often get stuck in spurious local optima.      
### 44.Brain-inspired algorithms for processing of visual data  [ :arrow_down: ](https://arxiv.org/pdf/2103.01634.pdf)
>  The study of the visual system of the brain has attracted the attention and interest of many neuro-scientists, that derived computational models of some types of neuron that compose it. These findings inspired researchers in image processing and computer vision to deploy such models to solve problems of visual data processing. In this paper, we review approaches for image processing and computer vision, the design of which is based on neuro-scientific findings about the functions of some neurons in the visual cortex. Furthermore, we analyze the connection between the hierarchical organization of the visual system of the brain and the structure of Convolutional Networks (ConvNets). We pay particular attention to the mechanisms of inhibition of the responses of some neurons, which provide the visual system with improved stability to changing input stimuli, and discuss their implementation in image processing operators and in ConvNets.      
### 45.Reachability-based Identification, Analysis, and Control Synthesis of Robot Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.01626.pdf)
>  We introduce reachability analysis for the formal examination of robots. We propose a novel identification method, which preserves reachset conformance of linear systems. We additionally propose a simultaneous identification and control synthesis scheme to obtain optimal controllers with formal guarantees. In a case study, we examine the effectiveness of using reachability analysis to synthesize a state-feedback controller, a velocity observer, and an output feedback controller.      
### 46.Deep learning-based synthetic CT generation from MR images: comparison of generative adversarial and residual neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.01609.pdf)
>  Currently, MRI-only radiotherapy (RT) eliminates some of the concerns about using CT images in RT chains such as the registration of MR images to a separate CT, extra dose delivery, and the additional cost of repeated imaging. However, one remaining challenge is that the signal intensities of MRI are not related to the attenuation coefficient of the biological tissue. This work compares the performance of two state-of-the-art deep learning models; a generative adversarial network (GAN) and a residual network (ResNet) for synthetic CTs (sCT) generation from MR images. The brain MR and CT images of 86 participants were analyzed. GAN and ResNet models were implemented for the generation of synthetic CTs from the 3D T1-weighted MR images using a six-fold cross-validation scheme. The resulting sCTs were compared, considering the CT images as a reference using standard metrics such as the mean absolute error (MAE), peak signal-to-noise ratio (PSNR) and the structural similarity index (SSIM). Overall, the ResNet model exhibited higher accuracy in relation to the delineation of brain tissues. The ResNet model estimated the CT values for the entire head region with an MAE of 114.1 HU compared to MAE=-10.9 HU obtained from the GAN model. Moreover, both models offered comparable SSIM and PSNR values, although the ResNet method exhibited a slightly superior performance over the GAN method. We compared two state-of-the-art deep learning models for the task of MR-based sCT generation. The ResNet model exhibited superior results, thus demonstrating its potential to be used for the challenge of synthetic CT generation in PET/MR AC and MR-only RT planning.      
### 47.Learning Robust Beamforming for MISO Downlink Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.01602.pdf)
>  This paper investigates a learning solution for robust beamforming optimization in downlink multi-user systems. A base station (BS) identifies efficient multi-antenna transmission strategies only with imperfect channel state information (CSI) and its stochastic features. To this end, we propose a robust training algorithm where a deep neural network (DNN), which only accepts estimates and statistical knowledge of the perfect CSI, is optimized to fit to real-world propagation environment. Consequently, the trained DNN can provide efficient robust beamforming solutions based only on imperfect observations of the actual CSI. Numerical results validate the advantages of the proposed learning approach compared to conventional schemes.      
### 48.Open Range Pitch Tracking for Carrier Frequency Difference Estimation from HF Transmitted Speech  [ :arrow_down: ](https://arxiv.org/pdf/2103.01599.pdf)
>  In this paper we investigate the task of detecting carrier frequency differences from demodulated single sideband signals by examining the pitch contours of the received baseband speech signal in the short-time spectral domain. From the detected pitch frequency trajectory and its harmonics a carrier frequency difference, which is caused by demodulating the radio signal with the wrong carrier frequency, can be deduced. A computationally efficient realization in the power cepstral domain is presented. The core component, i.e., the pitch tracking algorithm, is shown to perform comparably to a state of the art algorithm. The full carrier frequency difference estimation system is tested on recordings of real transmissions over HF links. A comparison with an existing approach shows improved estimation accuracy, both on short and longer speech utterances      
### 49.Performance Analysis of OTFS Modulation with Receive Antenna Selection  [ :arrow_down: ](https://arxiv.org/pdf/2103.01563.pdf)
>  In this paper, we analyze the performance of orthogonal time frequency space (OTFS) modulation with antenna selection at the receiver, where $n_s$ out of $n_r$ receive antennas with maximum channel Frobenius norms in the delay-Doppler (DD) domain are selected. Single-input multiple-output OTFS (SIMO-OTFS), multiple-input multiple-output OTFS (MIMO-OTFS), and space-time coded OTFS (STC-OTFS) systems with receive antenna selection (RAS) are considered. We consider these systems without and with phase rotation. Our diversity analysis results show that, with no phase rotation, SIMO-OTFS and MIMO-OTFS systems with RAS are rank deficient, and therefore they do not extract the full receive diversity as well as the diversity present in the DD domain. Also, Alamouti coded STC-OTFS system with RAS and no phase rotation extracts the full transmit diversity, but it fails to extract the DD diversity. On the other hand, SIMO-OTFS and STC-OTFS systems with RAS become full-ranked when phase rotation is used, because of which they extract the full spatial as well as the DD diversity present in the system. Also, when phase rotation is used, MIMO-OTFS systems with RAS extract the full DD diversity, but they do not extract the full receive diversity because of rank deficiency. Simulation results are shown to validate the analytically predicted diversity performance.      
### 50.Signal recovery from a few linear measurements of its high-order spectra  [ :arrow_down: ](https://arxiv.org/pdf/2103.01551.pdf)
>  The $q$-th order spectrum is a polynomial of degree $q$ in the entries of a signal $x\in\mathbb{C}^N$, which is invariant under circular shifts of the signal. For $q\geq 3$, this polynomial determines the signal uniquely, up to a circular shift, and is called a high-order spectrum. The high-order spectra, and in particular the bispectrum ($q=3$) and the trispectrum ($q=4$), play a prominent role in various statistical signal processing and imaging applications, such as phase retrieval and single-particle reconstruction. However, the dimension of the $q$-th order spectrum is $N^{q-1}$, far exceeding the dimension of $x$, leading to increased computational load and storage requirements. In this work, we show that it is unnecessary to store and process the full high-order spectra: a signal can be characterized uniquely, up to a circular shift, from only $N+1$ linear measurements of its high-order spectra. The proof relies on tools from algebraic geometry and is corroborated by numerical experiments.      
### 51.Nested Vehicle Routing Problem: Optimizing Drone-Truck Surveillance Operations  [ :arrow_down: ](https://arxiv.org/pdf/2103.01528.pdf)
>  Unmanned aerial vehicles or drones are becoming increasingly popular due to their low cost and high mobility. In this paper we address the routing and coordination of a drone-truck pairing where the drone travels to multiple locations to perform specified observation tasks and rendezvous periodically with the truck to swap its batteries. We refer to this as the Nested-Vehicle Routing Problem (Nested-VRP) and develop a Mixed Integer Programming (MIP) formulation with critical operational constraints, including drone battery capacity and synchronization of both vehicles during scheduled rendezvous. Given the NP-hard nature of the Nested-VRP, we propose an efficient neighborhood search (NS) heuristic where we generate and improve on a good initial solution (i.e., where the optimality gap is on average less than 6% in large instances) by iteratively solving the Nested-VRP on a local scale. We provide comparisons of both the MIP and NS heuristic methods with a relaxation lower bound in the cases of small and large problem sizes, and present the results of a computational study to show the effectiveness of the MIP model and the efficiency of the NS heuristic, including for a real-life instance with 631 locations. We envision that this framework will facilitate the planning and operations of combined drone-truck missions.      
### 52.On the Memory Mechanism of Tensor-Power Recurrent Models  [ :arrow_down: ](https://arxiv.org/pdf/2103.01521.pdf)
>  Tensor-power (TP) recurrent model is a family of non-linear dynamical systems, of which the recurrence relation consists of a p-fold (a.k.a., degree-p) tensor product. Despite such the model frequently appears in the advanced recurrent neural networks (RNNs), to this date there is limited study on its memory property, a critical characteristic in sequence tasks. In this work, we conduct a thorough investigation of the memory mechanism of TP recurrent models. Theoretically, we prove that a large degree p is an essential condition to achieve the long memory effect, yet it would lead to unstable dynamical behaviors. Empirically, we tackle this issue by extending the degree p from discrete to a differentiable domain, such that it is efficiently learnable from a variety of datasets. Taken together, the new model is expected to benefit from the long memory effect in a stable manner. We experimentally show that the proposed model achieves competitive performance compared to various advanced RNNs in both the single-cell and seq2seq architectures.      
### 53.Online Orthogonal Dictionary Learning Based on Frank-Wolfe Method  [ :arrow_down: ](https://arxiv.org/pdf/2103.01484.pdf)
>  Dictionary learning is a widely used unsupervised learning method in signal processing and machine learning. Most existing works of dictionary learning are in an offline manner. There are mainly two offline ways for dictionary learning. One is to do an alternative optimization of both the dictionary and the sparse code; the other way is to optimize the dictionary by restricting it over the orthogonal group. The latter one is called orthogonal dictionary learning which has a lower complexity implementation, hence, it is more favorable for lowcost devices. However, existing schemes on orthogonal dictionary learning only work with batch data and can not be implemented online, which is not applicable for real-time applications. This paper proposes a novel online orthogonal dictionary scheme to dynamically learn the dictionary from streaming data without storing the historical data. The proposed scheme includes a novel problem formulation and an efficient online algorithm design with convergence analysis. In the problem formulation, we relax the orthogonal constraint to enable an efficient online algorithm. In the algorithm design, we propose a new Frank-Wolfe-based online algorithm with a convergence rate of O(ln t/t^(1/4)). The convergence rate in terms of key system parameters is also derived. Experiments with synthetic data and real-world sensor readings demonstrate the effectiveness and efficiency of the proposed online orthogonal dictionary learning scheme.      
### 54.Audio-Visual Speech Separation Using Cross-Modal Correspondence Loss  [ :arrow_down: ](https://arxiv.org/pdf/2103.01463.pdf)
>  We present an audio-visual speech separation learning method that considers the correspondence between the separated signals and the visual signals to reflect the speech characteristics during training. Audio-visual speech separation is a technique to estimate the individual speech signals from a mixture using the visual signals of the speakers. Conventional studies on audio-visual speech separation mainly train the separation model on the audio-only loss, which reflects the distance between the source signals and the separated signals. However, conventional losses do not reflect the characteristics of the speech signals, including the speaker's characteristics and phonetic information, which leads to distortion or remaining noise. To address this problem, we propose the cross-modal correspondence (CMC) loss, which is based on the cooccurrence of the speech signal and the visual signal. Since the visual signal is not affected by background noise and contains speaker and phonetic information, using the CMC loss enables the audio-visual speech separation model to remove noise while preserving the speech characteristics. Experimental results demonstrate that the proposed method learns the cooccurrence on the basis of CMC loss, which improves separation performance.      
### 55.Interpretable Hyperspectral AI: When Non-Convex Modeling meets Hyperspectral Remote Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2103.01449.pdf)
>  Hyperspectral imaging, also known as image spectrometry, is a landmark technique in geoscience and remote sensing (RS). In the past decade, enormous efforts have been made to process and analyze these hyperspectral (HS) products mainly by means of seasoned experts. However, with the ever-growing volume of data, the bulk of costs in manpower and material resources poses new challenges on reducing the burden of manual labor and improving efficiency. For this reason, it is, therefore, urgent to develop more intelligent and automatic approaches for various HS RS applications. Machine learning (ML) tools with convex optimization have successfully undertaken the tasks of numerous artificial intelligence (AI)-related applications. However, their ability in handling complex practical problems remains limited, particularly for HS data, due to the effects of various spectral variabilities in the process of HS imaging and the complexity and redundancy of higher dimensional HS signals. Compared to the convex models, non-convex modeling, which is capable of characterizing more complex real scenes and providing the model interpretability technically and theoretically, has been proven to be a feasible solution to reduce the gap between challenging HS vision tasks and currently advanced intelligent data processing models.      
### 56.Safe Learning of Uncertain Environments for Nonlinear Control-Affine Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.01413.pdf)
>  In many learning based control methodologies, learning the unknown dynamic model precedes the control phase, while the aim is to control the system such that it remains in some safe region of the state space. In this work our aim is to guarantee safety while learning and control proceed simultaneously. Specifically, we consider the problem of safe learning in nonlinear control-affine systems subject to unknown additive uncertainty. We model uncertainty as a Gaussian signal and use state measurements to learn its mean and covariance. We provide rigorous time-varying bounds on the mean and covariance of the uncertainty and employ them to modify the control input via an optimisation program with safety constraints encoded as a barrier function on the state space. We show that with an arbitrarily large probability we can guarantee that the state will remain in the safe set, while learning and control are carried out simultaneously, provided that a feasible solution exists for the optimisation problem. We provide a secondary formulation of this optimisation that is computationally more efficient. This is based on tightening the safety constraints to counter the uncertainty about the learned mean and covariance. The magnitude of the tightening can be decreased as our confidence in the learned mean and covariance increases (i.e., as we gather more measurements about the environment). Extensions of the method are provided for Gaussian uncertainties with piecewise constant mean and covariance to accommodate more general environments.      
### 57.Channel-Driven Monte Carlo Sampling for Bayesian Distributed Learning in Wireless Data Centers  [ :arrow_down: ](https://arxiv.org/pdf/2103.01351.pdf)
>  Conventional frequentist learning, as assumed by existing federated learning protocols, is limited in its ability to quantify uncertainty, incorporate prior knowledge, guide active learning, and enable continual learning. Bayesian learning provides a principled approach to address all these limitations, at the cost of an increase in computational complexity. This paper studies distributed Bayesian learning in a wireless data center setting encompassing a central server and multiple distributed workers. Prior work on wireless distributed learning has focused exclusively on frequentist learning, and has introduced the idea of leveraging uncoded transmission to enable "over-the-air" computing. Unlike frequentist learning, Bayesian learning aims at evaluating approximations or samples from a global posterior distribution in the model parameter space. This work investigates for the first time the design of distributed one-shot, or "embarrassingly parallel", Bayesian learning protocols in wireless data centers via consensus Monte Carlo (CMC). Uncoded transmission is introduced not only as a way to implement "over-the-air" computing, but also as a mechanism to deploy channel-driven MC sampling: Rather than treating channel noise as a nuisance to be mitigated, channel-driven sampling utilizes channel noise as an integral part of the MC sampling process. A simple wireless CMC scheme is first proposed that is asymptotically optimal under Gaussian local posteriors. Then, for arbitrary local posteriors, a variational optimization strategy is introduced. Simulation results demonstrate that, if properly accounted for, channel noise can indeed contribute to MC sampling and does not necessarily decrease the accuracy level.      
### 58.LTO: Lazy Trajectory Optimization with Graph-Search Planning for High DOF Robots in Cluttered Environments  [ :arrow_down: ](https://arxiv.org/pdf/2103.01333.pdf)
>  Although Trajectory Optimization (TO) is one of the most powerful motion planning tools, it suffers from expensive computational complexity as a time horizon increases in cluttered environments. It can also fail to converge to a globally optimal solution. In this paper, we present Lazy Trajectory Optimization (LTO) that unifies local short-horizon TO and global Graph-Search Planning (GSP) to generate a long-horizon global optimal trajectory. LTO solves TO with the same constraints as the original long-horizon TO with improved time complexity. We also propose a TO-aware cost function that can balance both solution cost and planning time. Since LTO solves many nearly identical TO in a roadmap, it can provide an informed warm-start for TO to accelerate the planning process. We also present proofs of the computational complexity and optimality of LTO. Finally, we demonstrate LTO's performance on motion planning problems for a 2 DOF free-flying robot and a 21 DOF legged robot, showing that LTO outperforms existing algorithms in terms of its runtime and reliability.      
### 59.Implementation of binary stochastic STDP learning using chalcogenide-based memristive devices  [ :arrow_down: ](https://arxiv.org/pdf/2103.01271.pdf)
>  The emergence of nano-scale memristive devices encouraged many different research areas to exploit their use in multiple applications. One of the proposed applications was to implement synaptic connections in bio-inspired neuromorphic systems. Large-scale neuromorphic hardware platforms are being developed with increasing number of neurons and synapses, having a critical bottleneck in the online learning capabilities. Spike-timing-dependent plasticity (STDP) is a widely used learning mechanism inspired by biology which updates the synaptic weight as a function of the temporal correlation between pre- and post-synaptic spikes. In this work, we demonstrate experimentally that binary stochastic STDP learning can be obtained from a memristor when the appropriate pulses are applied at both sides of the device.      
### 60.Centralized and Distributed Intrusion Detection for Resource Constrained Wireless SDN Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.01262.pdf)
>  Software-defined networking (SDN) was devised to simplify network management and automate infrastructure sharing in wired networks. These benefits motivated the application of SDN in wireless sensor networks to leverage solutions for complex applications. However, some of the core SDN traits turn the networks prone to denial of service attacks (DoS). There are proposals in the literature to detect DoS in wireless SDN networks, however, not without shortcomings: there is little focus on resource constraints, high detection rates have been reported only for small networks, and the detection is disengaged from the identification of the type of the attack or the attacker. Our work targets these shortcomings by introducing a lightweight, online change point detector to monitor performance metrics that are impacted when the network is under attack. A key novelty is that the proposed detector is able to operate in either centralized or distributed mode. The centralized detector has very high detection rates and can further distinguish the type of the attack (from a list of known attacks). On the other hand, the distributed detector provides information that allows to identify the nodes launching the attack. Our proposal is tested over IEEE 802.15.4 networks. The results show detection rates exceeding $96\%$ in networks of 36 and 100 nodes and identification of the type of the attack with a probability exceeding $0.89$ when using the centralized approach. Additionally, for some types of attack it was possible to pinpoint the attackers with an identification probability over $0.93$ when using distributed detectors.      
### 61.Uncertainty Quantification by Ensemble Learning for Computational Optical Form Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2103.01259.pdf)
>  Uncertainty quantification by ensemble learning is explored in terms of an application from computational optical form measurements. The application requires to solve a large-scale, nonlinear inverse problem. Ensemble learning is used to extend a recently developed deep learning approach for this application in order to provide an uncertainty quantification of its predicted solution to the inverse problem. By systematically inserting out-of-distribution errors as well as noisy data the reliability of the developed uncertainty quantification is explored. Results are encouraging and the proposed application exemplifies the ability of ensemble methods to make trustworthy predictions on high dimensional data in a real-world application.      
### 62.Three-Dimensional Virtual Histology in Unprocessed Resected Tissues with Photoacoustic Remote Sensing (PARS) Microscopy and Optical Coherence Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2103.01224.pdf)
>  Histological visualizations are critical in the diagnosis and treatment of cancers and other malignancies. Unfortunately, the current method for capturing these visualizations requires resource intensive tissue preparation which can delay diagnostics for weeks. To streamline this process, clinicians are limited to assessing small macroscopically representative subsets of tissues. Here, we present a conjoined photoacoustic remote sensing (PARS) microscope and swept source optical coherence tomography (SS-OCT) system aiming to circumvent these diagnostic limitations. The proposed multimodal microscope provides label-free three-dimensional depth resolved virtual histology visualizations, capturing nuclear and extranuclear tissue morphology directly on thick unprocessed specimens. The capabilities of the proposed method are demonstrated directly in unprocessed formalin fixed resected tissues. Here, we present the first images of nuclear contrast in resected human tissues, and the first 3-dimensional visualization of subsurface nuclear morphology in resected Rattus tissues, captured with a non-contact photoacoustic system. Moreover, we present the first co-registered OCT and PARS images enabling direct histological assessment of unprocessed tissues. This work represents a vital step towards the development of a real-time histological imaging modality to circumvent the limitations of current histopathology techniques.      
