# ArXiv eess --Mon, 1 Mar 2021
### 1.Convolution-Free Medical Image Segmentation using Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2102.13645.pdf)
>  Like other applications in computer vision, medical image segmentation has been most successfully addressed using deep learning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties such as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks (CNNs) a strong and useful inductive bias for vision tasks. In this work we show that a different method, based entirely on self-attention between neighboring image patches and without any convolution operations, can achieve competitive or better results. Given a 3D image block, our network divides it into $n^3$ 3D patches, where $n=3 \text{ or } 5$ and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the block based on the self-attention between these patch embeddings. We show that the proposed model can achieve segmentation accuracies that are better than the state of the art CNNs on three datasets. We also propose methods for pre-training this model on large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over CNNs can be significant when labeled training data is small.      
### 2.ECO: Enabling Energy-Neutral IoT Devices through Runtime Allocation of Harvested Energy  [ :arrow_down: ](https://arxiv.org/pdf/2102.13605.pdf)
>  Energy harvesting offers an attractive and promising mechanism to power low-energy devices. However, it alone is insufficient to enable an energy-neutral operation, which can eliminate tedious battery charging and replacement requirements. Achieving an energy-neutral operation is challenging since the uncertainties in harvested energy undermine the quality of service requirements. To address this challenge, we present a rollout-based runtime energy-allocation framework that optimizes the utility of the target device under energy constraints. The proposed framework uses an efficient iterative algorithm to compute initial energy allocations at the beginning of a day. The initial allocations are then corrected at every interval to compensate for the deviations from the expected energy harvesting pattern. We evaluate this framework using solar and motion energy harvesting modalities and American Time Use Survey data from 4772 different users. Compared to state-of-the-art techniques, the proposed framework achieves 34.6% higher utility even under energy-limited scenarios. Moreover, measurements on a wearable device prototype show that the proposed framework has less than 0.1% energy overhead compared to iterative approaches with a negligible loss in utility.      
### 3.3D Vessel Reconstruction in OCT-Angiography via Depth Map Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2102.13588.pdf)
>  Optical Coherence Tomography Angiography (OCTA) has been increasingly used in the management of eye and systemic diseases in recent years. Manual or automatic analysis of blood vessel in 2D OCTA images (en face angiograms) is commonly used in clinical practice, however it may lose rich 3D spatial distribution information of blood vessels or capillaries that are useful for clinical decision-making. In this paper, we introduce a novel 3D vessel reconstruction framework based on the estimation of vessel depth maps from OCTA images. First, we design a network with structural constraints to predict the depth of blood vessels in OCTA images. In order to promote the accuracy of the predicted depth map at both the overall structure- and pixel- level, we combine MSE and SSIM loss as the training loss function. Finally, the 3D vessel reconstruction is achieved by utilizing the estimated depth map and 2D vessel segmentation results. Experimental results demonstrate that our method is effective in the depth prediction and 3D vessel reconstruction for OCTA images.% results may be used to guide subsequent vascular analysis      
### 4.Robust Adaptive Model Predictive Control of Quadrotors  [ :arrow_down: ](https://arxiv.org/pdf/2102.13544.pdf)
>  Robust adaptive model predictive control (RAMPC) is a novel control method that combines robustness guarantees with respect to unknown parameters and bounded disturbances into a model predictive control scheme. However, RAMPC has so far only been developed in theory. The goal of this paper is to apply RAMPC to a physical quadrotor experiment. To the best of our knowledge this is the first time that RAMPC has been applied in practice using a state space formulation. In doing so, we highlight important practical challenges such as computation of $\lambda$-contractive polytopes and dealing with measurement noise, and propose modifications to RAMPC so that it can be applied on a quadrotor. We first simulate quadrotor flight with a direct and a decoupled control architecture in different scenarios. The scenarios include: (i) an unknown mass of the quadrotor as a package delivery scenario with wind as a bounded disturbance; and (ii) all rotor efficiencies drop as a power delivery problem. We then implement these scenarios on a physical quadrotor and present the experimental results.      
### 5.Texture-aware Video Frame Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2102.13520.pdf)
>  Temporal interpolation has the potential to be a powerful tool for video compression. Existing methods for frame interpolation do not discriminate between video textures and generally invoke a single general model capable of interpolating a wide range of video content. However, past work on video texture analysis and synthesis has shown that different textures exhibit vastly different motion characteristics and they can be divided into three classes (static, dynamic continuous and dynamic discrete). In this work, we study the impact of video textures on video frame interpolation, and propose a novel framework where, given an interpolation algorithm, separate models are trained on different textures. Our study shows that video texture has significant impact on the performance of frame interpolation models and it is beneficial to have separate models specifically adapted to these texture classes, instead of training a single model that tries to learn generic motion. Our results demonstrate that models fine-tuned using our framework achieve, on average, a 0.3dB gain in PSNR on the test set used.      
### 6.Microsimulation of Space Time Trellis Code  [ :arrow_down: ](https://arxiv.org/pdf/2102.13491.pdf)
>  This letter explores the possibility of using microsimulation in space time trellis code. Performing a pairwise comparison between generator matrices is essential in the validation of optimality. This is often done with simulation, which can be a time consuming process altogether. Microsimulation considerably cuts down the computational cost of simulation by employing smaller data and iteration. The effort is feasible with the assistance of a machine learning model known as multilayer perceptron. When properly conducted, it can offer 93.86% accuracy and 98.25% reduction in temporal cost.      
### 7.PySensors: A Python Package for Sparse Sensor Placement  [ :arrow_down: ](https://arxiv.org/pdf/2102.13476.pdf)
>  PySensors is a Python package for selecting and placing a sparse set of sensors for classification and reconstruction tasks. Specifically, PySensors implements algorithms for data-driven sparse sensor placement optimization for reconstruction (SSPOR) and sparse sensor placement optimization for classification (SSPOC). In this work we provide a brief description of the mathematical algorithms and theory for sparse sensor optimization, along with an overview and demonstration of the features implemented in PySensors (with code examples). We also include practical advice for user and a list of potential extensions to PySensors. Software is available at <a class="link-external link-https" href="https://github.com/dynamicslab/pysensors" rel="external noopener nofollow">this https URL</a>.      
### 8.DNN-assisted optical geometric constellation shaped PSK modulation for PAM4-to-QPSK format conversion gateway node  [ :arrow_down: ](https://arxiv.org/pdf/2102.13474.pdf)
>  An optical gateway to convert four-level pulse amplitude modulation to quadrature phase shift keying modulation format having shaping gain was proposed for flexible intensity to phase mapping which exploits non-uniform phase noise. The power consumption of the optical modulation format conversion can save by making a DNN-based decision on the receiver side for the generated QPSK signal with non-uniform phase noise. A proof-of-principle experiment has shown that an optically geometric constellation shaped QPSK modulated signals generated from regular PAM4 signals with Gaussian-distributed noise. The shaped QPSK signal shows BER and generalized mutual information improvement by 1dB gain through the use of digital neural network signal recovery.      
### 9.Sleep Apnea and Respiratory Anomaly Detection from a Wearable Band and Oxygen Saturation  [ :arrow_down: ](https://arxiv.org/pdf/2102.13473.pdf)
>  Objective: Sleep related respiratory abnormalities are typically detected using polysomnography. There is a need in general medicine and critical care for a more convenient method to automatically detect sleep apnea from a simple, easy-to-wear device. The objective is to automatically detect abnormal respiration and estimate the Apnea-Hypopnea-Index (AHI) with a wearable respiratory device, compared to an SpO2 signal or polysomnography using a large (n = 412) dataset serving as ground truth. Methods: Simultaneously recorded polysomnographic (PSG) and wearable respiratory effort data were used to train and evaluate models in a cross-validation fashion. Time domain and complexity features were extracted, important features were identified, and a random forest model employed to detect events and predict AHI. Four models were trained: one each using the respiratory features only, a feature from the SpO2 (%)-signal only, and two additional models that use the respiratory features and the SpO2 (%)-feature, one allowing a time lag of 30 seconds between the two signals. Results: Event-based classification resulted in areas under the receiver operating characteristic curves of 0.94, 0.86, 0.82, and areas under the precision-recall curves of 0.48, 0.32, 0.51 for the models using respiration and SpO2, respiration-only, and SpO2-only respectively. Correlation between expert-labelled and predicted AHI was 0.96, 0.78, and 0.93, respectively. Conclusions: A wearable respiratory effort signal with or without SpO2 predicted AHI accurately. Given the large dataset and rigorous testing design, we expect our models are generalizable to evaluating respiration in a variety of environments, such as at home and in critical care.      
### 10.The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation &amp; Primates  [ :arrow_down: ](https://arxiv.org/pdf/2102.13468.pdf)
>  The INTERSPEECH 2021 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the COVID-19 Cough and COVID-19 Speech Sub-Challenges, a binary classification on COVID-19 infection has to be made based on coughing sounds and speech; in the Escalation SubChallenge, a three-way assessment of the level of escalation in a dialogue is featured; and in the Primates Sub-Challenge, four species vs background need to be classified. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the 'usual' COMPARE and BoAW features as well as deep unsupervised representation learning using the AuDeep toolkit, and deep feature extraction from pre-trained CNNs using the Deep Spectrum toolkit; in addition, we add deep end-to-end sequential modelling, and partially linguistic analysis.      
### 11.Morning commute in congested urban rail transit system: A macroscopic model for equilibrium distribution of passenger arrivals  [ :arrow_down: ](https://arxiv.org/pdf/2102.13454.pdf)
>  This paper proposes a macroscopic model to describe the equilibrium distribution of passenger arrivals for the morning commute problem in a congested urban rail transit system. We employ a macroscopic train operation sub-model developed by Seo et al. (2017a,b) to express the interaction between dynamics of passengers and trains in a simplified manner while maintaining their essential physical relations. We derive the equilibrium conditions of the proposed model and discuss the existence of equilibrium. The characteristics of the equilibrium are then examined through numerical examples under different passenger demand settings. As an application of the proposed model, we finally analyze a simple time-dependent timetable optimization problem with equilibrium constraints and show that there exists a "capacity increasing paradox" in which a higher dispatch frequency can increase the equilibrium cost. Further insights into the design of the timetable and its influence on passengers' equilibrium travel costs are also obtained.      
### 12.Robust Rational Polynomial Camera Modelling for SAR and Pushbroom Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2102.13423.pdf)
>  The Rational Polynomial Camera (RPC) model can be used to describe a variety of image acquisition systems in remote sensing, notably optical and Synthetic Aperture Radar (SAR) sensors. RPC functions relate 3D to 2D coordinates and vice versa, regardless of physical sensor specificities, which has made them an essential tool to harness satellite images in a generic way. This article describes a terrain-independent algorithm to accurately derive a RPC model from a set of 3D-2D point correspondences based on a regularized least squares fit. The performance of the method is assessed by varying the point correspondences and the size of the area that they cover. We test the algorithm on SAR and optical data, to derive RPCs from physical sensor models or from other RPC models after composition with corrective functions.      
### 13.Near-Field Millimeter-Wave Imaging via Circular-Arc MIMO Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2102.13418.pdf)
>  Millimeter-wave (MMW) imaging has a wide prospect in application of concealed weapons detection. We propose a circular-arc multiple-input multiple-output (MIMO) array scheme with uniformly spaced transmit and receive antennas along the horizontal-arc direction, while scanning along the vertical direction. The antenna beams of the circular-arc MIMO array can provide more uniform coverage of the imaging scene than those of the linear or planar MIMO arrays. Further, a near-field three-dimensional (3-D) imaging algorithm, based on the spatial frequency domain processing, is presented with analysis of sampling criteria and resolutions. Numerical simulations, as well as comparisons with the back-projection (BP) algorithm, are provided to show the efficacy of the proposed approach.      
### 14.Underwater Acoustic Communication Receiver Using Deep Belief Network  [ :arrow_down: ](https://arxiv.org/pdf/2102.13397.pdf)
>  Underwater environments create a challenging channel for communications. In this paper, we design a novel receiver system by exploring the machine learning technique--Deep Belief Network (DBN)-- to combat the signal distortion caused by the Doppler effect and multi-path propagation. We evaluate the performance of the proposed receiver system in both simulation experiments and sea trials. Our proposed receiver system comprises of DBN based de-noising and classification of the received signal. First, the received signal is segmented into frames before the each of these frames is individually pre-processed using a novel pixelization algorithm. Then, using the DBN based de-noising algorithm, features are extracted from these frames and used to reconstruct the received signal. Finally, DBN based classification of the reconstructed signal occurs. Our proposed DBN based receiver system does show better performance in channels influenced by the Doppler effect and multi-path propagation with a performance improvement of 13.2dB at $10^{-3}$ Bit Error Rate (BER).      
### 15.Depth extraction from a single compressive hologram  [ :arrow_down: ](https://arxiv.org/pdf/2102.13371.pdf)
>  We propose a novel method that records a single compressive hologram in a short time and extracts the depth of a scene from that hologram using a stereo disparity technique. The method is verified with numerical simulations, but there is no restriction on adapting this into an optical experiment. In the simulations, a computer-generated hologram is first sampled with random binary patterns, and measurements are utilized in a recovery algorithm to form a compressive hologram. The compressive hologram is then divided into two parts (two apertures), and these parts are separately reconstructed to form a stereo image pair. The pair is eventually utilized in stereo disparity method for depth map extraction. The depth maps of the compressive holograms with the sampling rates of 2, 25, and 50 percent are compared with the depth map extracted from the original hologram, on which compressed sensing is not applied. It is demonstrated that the depth profiles obtained from the compressive holograms are in very good agreement with the depth profile obtained from the original hologram despite the data reduction.      
### 16.Robust Survivability-Oriented Scheduling of Separable Mobile Energy Storage and Demand Response for Isolated Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2102.13346.pdf)
>  Extreme circumstances in which a local distribution system is electrically isolated from the main power supply may not always be avoidable. Efforts must be made to keep the lights on for such an isolated distribution system (IDS) until reconnection to the main power source. In this paper, we propose a strategy to enhance IDS survivability utilizing the coordination of two flexible approaches, namely, separable energy storage systems (SMESSs), which construct non-wires links for energy transmission between the IDS and the external live power sources, and demand response (DR), which adjusts the internal electrical demand of the IDS to provide effective operating stress alleviation. Considering the uncertainty of renewable energy generation and loads, a two-stage robust optimization (RO) model involving the joint scheduling of these two approaches is constructed. The objective is to minimize the fuel consumption rate and the decreased and nonserved demand under the worst-case scenario to endow the IDS with extended survivability. Finally, the test is conducted and the results demonstrate the effectiveness of the proposed method in enhancing the survivability of IDS.      
### 17.A Behavioral Input-Output Parametrization of Control Policies with Suboptimality Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2102.13338.pdf)
>  Recent work in data-driven control has revived behavioral theory to perform a variety of complex control tasks, by directly plugging libraries of past input-output trajectories into optimal control problems. Despite recent advances, a key aspect remains unclear: how and to what extent do noise-corrupted data impact the achieved control performance? In this work, we provide a quantitative answer to these questions. We formulate a Behavioral version of the Input-Output Parametrization (BIOP) for the predictive control of unknown systems using output-feedback dynamic control policies. The main advantages of the proposed framework are that 1) the state-space parameters and the initial state need not be specified for controller synthesis, 2) it can be used in combination with state-of-the-art impulse response estimators, and 3) it allows to recover recent suboptimality results for the Linear Quadratic Gaussian (LQG) control problem, therefore revealing, in a quantitative way, how the level of noise in the data affects the performance of behavioral methods. Specifically, it is shown that the performance degrades linearly with the prediction error of a behavioral model. We conclude the paper with numerical experiments to validate our results.      
### 18.Integration of deep learning with expectation maximization for spatial cue based speech separation in reverberant conditions  [ :arrow_down: ](https://arxiv.org/pdf/2102.13334.pdf)
>  In this paper, we formulate a blind source separation (BSS) framework, which allows integrating U-Net based deep learning source separation network with probabilistic spatial machine learning expectation maximization (EM) algorithm for separating speech in reverberant conditions. Our proposed model uses a pre-trained deep learning convolutional neural network, U-Net, for clustering the interaural level difference (ILD) cues and machine learning expectation maximization (EM) algorithm for clustering the interaural phase difference (IPD) cues. The integrated model exploits the complementary strengths of the two approaches to BSS: the strong modeling power of supervised neural networks and the ease of unsupervised machine learning algorithms, whose few parameters can be estimated on as little as a single segment of an audio mixture. The results show an average improvement of 4.3 dB in signal to distortion ratio (SDR) and 4.3% in short time speech intelligibility (STOI) over the EM based source separation algorithm MESSL-GS (model-based expectation-maximization source separation and localization with garbage source) and 4.5 dB in SDR and 8% in STOI over deep learning convolutional neural network (U-Net) based speech separation algorithm SONET under the reverberant conditions ranging from anechoic to those mostly encountered in the real world.      
### 19.Feasibility Enhancement of Constrained Receding Horizon Control Using Generalized Control Barrier Function  [ :arrow_down: ](https://arxiv.org/pdf/2102.13304.pdf)
>  Receding horizon control (RHC) is a popular procedure to deal with optimal control problems. Due to the existence of state constraints, optimization-based RHC often suffers the notorious issue of infeasibility, which strongly shrinks the region of controllable state. This paper proposes a generalized control barrier function (CBF) to enlarge the feasible region of constrained RHC with only a one-step constraint on the prediction horizon. This design can reduce the constrained steps by penalizing the tendency to move towards the constraint boundary. Additionally, generalized CBF is able to handle high-order equality or inequality constraints through extending the constrained step to nonadjacent nodes. We apply this technique on an automated vehicle control task. The results show that compared to multi-step pointwise constraints, generalized CBF can effectively avoid the infeasibility issue in a larger partition of the state space, and the computing efficiency is also improved by 14%-23%.      
### 20.Power Systems Transient Stability Indices: Hierarchical Clustering Based Detection of Coherent Groups Of Generators  [ :arrow_down: ](https://arxiv.org/pdf/2102.13286.pdf)
>  Coherent groups of generators, i.e., machines with perfectly correlated rotor angles, play an important role in power system stability analysis. This paper introduces a real-time methodology based on hierarchical clustering techniques for discovering the degree of coherency among generators using the synchronization coefficient and the correlation coefficient of the generators' rotor angle as the coherency index. Furthermore, the Power Transient Stability Indices (PTSI) were employed to examine the versatile response of the power system. The method uses power systems transients Stability indices, i.e., power Connectivity Factor (CF) index which presents coherently strong generators within the groups, the power Separation Factor (SF) index which unveils to the extent that the generators in different groups tend to swing against the other groups in the event of a disturbance, and the overall system separation index which demonstrates the overall system separation status (CF/SF). The approach is assessed on an IEEE-39 test system with a fully dynamic model. The simulation results presented in this paper demonstrate the efficiency of the proposed approach.      
### 21.NCH Sleep DataBank: A Large Collection of Real-world Pediatric Sleep Studies  [ :arrow_down: ](https://arxiv.org/pdf/2102.13284.pdf)
>  Despite being crucial to health and quality of life, sleep -- especially pediatric sleep -- is not yet well understood. This is exacerbated by lack of access to sufficient pediatric sleep data with clinical annotation. In order to accelerate research on pediatric sleep and its connection to health, we create the Nationwide Children's Hospital (NCH) Sleep DataBank and publish it at the National Sleep Research Resource (NSRR), which is a large sleep data common with physiological data, clinical data, and tools for analyses. The NCH Sleep DataBank consists of 3,984 polysomnography studies and over 5.6 million clinical observations on 3,673 unique patients between 2017 and 2019 at NCH. The novelties of this dataset include: 1) large-scale sleep dataset suitable for discovering new insights via data mining, 2) explicit focus on pediatric patients, 3) gathered in a real-world clinical setting, and 4) the accompanying rich set of clinical data. The NCH Sleep DataBank is a valuable resource for advancing automatic sleep scoring and real-time sleep disorder prediction, among many other potential scientific discoveries.      
### 22.Robust Kalman filter-based dynamic state estimation of natural gas pipeline networks  [ :arrow_down: ](https://arxiv.org/pdf/2102.13251.pdf)
>  To obtain the accurate transient states of the big scale natural gas pipeline networks under the bad data and non-zero mean noises conditions, a robust Kalman filter-based dynamic state estimation method is proposed using the linearized gas pipeline transient flow equations in this paper. Firstly, the dynamic state estimation model is built. Since the gas pipeline transient flow equations are less than the states, the boundary conditions are used as supplementary constraints to predict the transient states. To increase the measurement redundancy, the zero mass flow rate constraints at the sink nodes are taken as virtual measurements. Secondly, to ensure the stability under bad data condition, the robust Kalman filter algorithm is proposed by introducing a time-varying scalar matrix to regulate the measurement error variances correctly according to the innovation vector at every time step. At last, the proposed method is applied to a 30-node gas pipeline networks in several kinds of measurement conditions. The simulation shows that the proposed robust dynamic state estimation can decrease the effects of bad data and achieve better estimating results.      
### 23.A High-Throughput Multi-Mode LDPC Decoder for 5G NR  [ :arrow_down: ](https://arxiv.org/pdf/2102.13228.pdf)
>  This paper presents a partially parallel low-density parity-check (LDPC) decoder designed for the 5G New Radio (NR) standard. The design is using a multi-block parallel architecture with a flooding schedule. The decoder can support any code rates and code lengths up to the lifting size Zmax= 96. To compensate for the dropped throughput associated with the smaller Z values, the design can double and quadruple its parallelism when lifting sizes Z&lt;= 48 and Z&lt;= 24 are selected respectively. Therefore, the decoder can process up to eight frames and restore the throughput to the maximum. To simplify the design's architecture, a new variable node for decoding the extended parity bits present in the lower code rates is proposed. The FPGA implementation of the decoder results in a throughput of 2.1 Gbps decoding the 11/12 code rate. Additionally, the synthesized decoder using the 28 nm TSMC technology, achieves a maximum clock frequency of 526 MHz and a throughput of 13.46 Gbps. The core decoder occupies 1.03 mm2, and the power consumption is 229 mW.      
### 24.Joint Resource Block and Beamforming Optimization for Cellular-Connected UAV Networks: A Hybrid D3QN-DDPG Approach  [ :arrow_down: ](https://arxiv.org/pdf/2102.13222.pdf)
>  Integrating unmanned aerial vehicle (UAV) into the existing cellular networks that are delicately designed for terrestrial transmissions faces lots of challenges, in which one of the most striking concerns is how to adopt UAV into the cellular networks with less (or even without) adverse effects to ground users. In this paper, a cellular-connected UAV network is considered, in which multiple UAVs receive messages from terrestrial base stations (BSs) in the down-link, while BSs are serving ground users in their cells. Besides, the line-of-sight (LoS) wireless links are more likely to be established in ground-to-air (G2A) transmission scenarios. On one hand, UAVs may potentially get access to more BSs. On the other hand, more co-channel interferences could be involved. To enhance wireless transmission quality between UAVs and BSs while protecting the ground users from being interfered by the G2A communications, a joint time-frequency resource block (RB) and beamforming optimization problem is proposed and investigated in this paper. Specifically, with given flying trajectory, the ergodic outage duration (EOD) of UAV is minimized with the aid of RB resource allocation and beamforming design. Unfortunately, the proposed optimization problem is hard to be solved via standard optimization techniques, if not impossible. To crack this nut, a deep reinforcement learning (DRL) solution is proposed, where deep double duelling Q network (D3QN) and deep deterministic policy gradient (DDPG) are invoked to deal with RB allocation in discrete action domain and beamforming design in continuous action regime, respectively. The hybrid D3QN-DDPG solution is applied to solve the outer Markov decision process (MDP) and the inner MDP interactively so that it can achieve the sub-optimal result for the considered optimization problem.      
### 25.Data-Driven Methods for Present and Future Pandemics: Monitoring, Modelling and Managing  [ :arrow_down: ](https://arxiv.org/pdf/2102.13130.pdf)
>  This survey analyses the role of data-driven methodologies for pandemic modelling and control. We provide a roadmap from the access to epidemiological data sources to the control of epidemic phenomena. We review the available methodologies and discuss the challenges in the development of data-driven strategies to combat the spreading of infectious diseases. Our aim is to bring together several different disciplines required to provide a holistic approach to the epidemic, such as data science, epidemiology, or systems-and-control theory. A 3M-analysis is presented, whose three pillars are: Monitoring, Modelling and Managing. The focus is on the potential of data-driven schemes to address different challenges raised by a pandemic: (i) monitoring the epidemic evolution and assessing the effectiveness of the adopted countermeasures; (ii) modelling and forecasting the spread of the epidemic; (iii) making timely decisions to manage, mitigate and suppress the contagion. For each step of this roadmap, we review consolidated theoretical approaches (including data-driven methodologies that have been shown to be successful in other contexts) and discuss their application to past or present epidemics, as well as their potential application to future epidemics.      
### 26.On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2102.13651.pdf)
>  Model-based Reinforcement Learning (MBRL) is a promising framework for learning control in a data-efficient manner. MBRL algorithms can be fairly complex due to the separate dynamics modeling and the subsequent planning algorithm, and as a result, they often possess tens of hyperparameters and architectural choices. For this reason, MBRL typically requires significant human expertise before it can be applied to new problems and domains. To alleviate this problem, we propose to use automatic hyperparameter optimization (HPO). We demonstrate that this problem can be tackled effectively with automated HPO, which we demonstrate to yield significantly improved performance compared to human experts. In addition, we show that tuning of several MBRL hyperparameters dynamically, i.e. during the training itself, further improves the performance compared to using static hyperparameters which are kept fixed for the whole training. Finally, our experiments provide valuable insights into the effects of several hyperparameters, such as plan horizon or learning rate and their influence on the stability of training and resulting rewards.      
### 27.Using Deep Learning to Automate the Detection of Flaws in Nuclear Fuel Channel UT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2102.13635.pdf)
>  Nuclear reactor inspections are critical to ensure the safety and reliability of plants operation. Inspections occur during planned outages and include the inspection of the reactor's fuel channels. In Canada, Ultrasonic Testing (UT) is used to inspect the health of fuel channels in Canada's Deuterium Uranium (CANDU) reactors. Currently, analysis of the UT scans is performed by manual visualization and measurement to locate, characterize, and disposition flaws. Therefore, there is a motivation to develop an automated method that is fast and accurate. In this paper, a proof of concept (PoC) that automates the detection of flaws in nuclear fuel channel UT scans using a convolutional neural network (CNN) is presented. This industry research was conducted at Alithya Digital Technology Corporation in Pickering, Ontario, Canada. The CNN model was trained after constructing a dataset using historical UT scans and the corresponding inspection results. This data was obtained from a large nuclear power generation company in Ontario. The requirement for this prototype was to identify the location of at least a portion of each flaw in fuel channel scans while minimizing false positives (FPs). This allows for automatic detection of the location of each flaw where further manual analysis is performed to identify the extent and the type of the flaw. Based on the defined requirement, the proposed model was able to achieve 100% accuracy for UT scans with minor chatter and a 100% sensitivity with minimal FPs for complicated UT scans with severe chatter using 18 UT full test scans.      
### 28.Federated Edge Learning with Misaligned Over-The-Air Computation  [ :arrow_down: ](https://arxiv.org/pdf/2102.13604.pdf)
>  Over-the-air computation (OAC) is a promising technique to realize fast model aggregation in the uplink of federated edge learning. OAC, however, hinges on accurate channel-gain precoding and strict synchronization among the edge devices, which are challenging in practice. As such, how to design the maximum likelihood (ML) estimator in the presence of residual channel-gain mismatch and asynchronies is an open problem. To fill this gap, this paper formulates the problem of misaligned OAC for federated edge learning and puts forth a whitened matched filtering and sampling scheme to obtain oversampled, but independent, samples from the misaligned and overlapped signals. Given the whitened samples, a sum-product ML estimator and an aligned-sample estimator are devised to estimate the arithmetic sum of the transmitted symbols. In particular, the computational complexity of our sum-product ML estimator is linear in the packet length and hence is significantly lower than the conventional ML estimator. Extensive simulations on the test accuracy versus the average received energy per symbol to noise power spectral density ratio (EsN0) yield two main results: 1) In the low EsN0 regime, the aligned-sample estimator can achieve superior test accuracy provided that the phase misalignment is non-severe. In contrast, the ML estimator does not work well due to the error propagation and noise enhancement in the estimation process. 2) In the high EsN0 regime, the ML estimator attains the optimal learning performance regardless of the severity of phase misalignment. On the other hand, the aligned-sample estimator suffers from a test-accuracy loss caused by phase misalignment.      
### 29.Adaptive Transmission Parameters Selection Algorithm for URLLC Traffic in Uplink  [ :arrow_down: ](https://arxiv.org/pdf/2102.13554.pdf)
>  Ultra-Reliable Low-Latency Communications (URLLC) is a novel feature of 5G cellular systems. To satisfy strict URLLC requirements for uplink data transmission, the specifications of 5G systems introduce the grant-free channel access method. According to this method, a User Equipment (UE) performs packet transmission without requesting channel resources from a base station (gNB). With the grant-free channel access, the gNB configures the uplink transmission parameters in a long-term time scale. Since the channel quality can significantly change in time and frequency domains, the gNB should select robust transmission parameters to satisfy the URLLC requirements. Many existing studies consider fixed robust uplink transmission parameter selection that allows satisfying the requirements even for UEs with poor channel conditions. However, the more robust transmission parameters are selected, the lower is the network capacity. In this paper, we propose an adaptive algorithm that selects the transmission parameters depending on the channel quality based on the signal-to-noise ratio statistics analysis at the gNB. Simulation results obtained with NS-3 show that the algorithm allows meeting the URLLC latency and reliability requirements while reducing the channel resource consumption more than twice in comparison with the fixed transmission parameters selection.      
### 30.The NPU System for the 2020 Personalized Voice Trigger Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2102.13552.pdf)
>  This paper describes the system developed by the NPU team for the 2020 personalized voice trigger challenge. Our submitted system consists of two independently trained subsystems: a small footprint keyword spotting (KWS) system and a speaker verification (SV) system. For the KWS system, a multi-scale dilated temporal convolutional (MDTC) network is proposed to detect wake-up word (WuW). For SV system, Write something here. The KWS predicts posterior probabilities of whether an audio utterance contains WuW and estimates the location of WuW at the same time. When the posterior probability ofWuW reaches a predefined threshold, the identity information of triggered segment is determined by the SV system. On evaluation dataset, our submitted system obtains detection costs of 0.081and 0.091 in close talking and far-field tasks, respectively.      
### 31.Data-driven modeling of linear dynamical systems with quadratic output in the AAA framework  [ :arrow_down: ](https://arxiv.org/pdf/2102.13487.pdf)
>  We extend the AAA (Adaptive-Antoulas-Anderson) algorithm to develop a data-driven modeling framework for linear systems with quadratic output (LQO). Such systems are characterized by two transfer functions: one corresponding to the linear part of the output and another one to the quadratic part. We first establish the joint barycentric representations and the interpolation theory for the two transfer functions of LQO systems. This analysis leads to the proposed AAA-LQO algorithm. We show that by interpolating the transfer function values on a subset of samples together with imposing a least-squares minimization on the rest, we construct reliable data-driven LQO models. Two numerical test cases illustrate the efficiency of the proposed method.      
### 32.Towards Explaining Expressive Qualities in Piano Recordings: Transfer of Explanatory Features via Acoustic Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2102.13479.pdf)
>  Emotion and expressivity in music have been topics of considerable interest in the field of music information retrieval. In recent years, mid-level perceptual features have been suggested as means to explain computational predictions of musical emotion. We find that the diversity of musical styles and genres in the available dataset for learning these features is not sufficient for models to generalise well to specialised acoustic domains such as solo piano music. In this work, we show that by utilising unsupervised domain adaptation together with receptive-field regularised deep neural networks, it is possible to significantly improve generalisation to this domain. Additionally, we demonstrate that our domain-adapted models can better predict and explain expressive qualities in classical piano performances, as perceived and described by human listeners.      
### 33.B-ETS: A Trusted Blockchain-based Emissions Trading System for Vehicle-to-Vehicle Networks  [ :arrow_down: ](https://arxiv.org/pdf/2102.13477.pdf)
>  Urban areas are negatively impacted by Carbon Dioxide (CO2 ) and Nitrogen Oxide (NOx) emissions. In order to achieve a cost-effective reduction of greenhouse gas emissions and to combat climate change, the European Union (EU) introduced an Emissions Trading System (ETS) where organizations can buy or receive emission allowances as needed. The current ETS is a centralized one, consisting of a set of complex rules. It is currently administered at the organizational level and is used for fixed-point sources of pollution such as factories, power plants, and refineries. However, the current ETS cannot efficiently cope with vehicle mobility, even though vehicles are one of the primary sources of CO2 and NOx emissions. In this study, we propose a new distributed Blockchain-based emissions allowance trading system called B-ETS. This system enables transparent and trustworthy data exchange as well as trading of allowances among vehicles, relying on vehicle-to-vehicle communication. In addition, we introduce an economic incentive-based mechanism that appeals to individual drivers and leads them to modify their driving behavior in order to reduce emissions. The efficiency of the proposed system is studied through extensive simulations, showing how increased vehicle connectivity can lead to a reduction of the emissions generated from those vehicles. We demonstrate that our method can be used for full life-cycle monitoring and fuel economy reporting. This leads us to conjecture that the proposed system could lead to important behavioral changes among the drivers      
### 34.Robust Implementable Regulator Design of General Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2102.13413.pdf)
>  Robust implementable output regulator design approaches are studied for general linear continuous-time \mbox{systems} with periodically sampled measurements, consisting of both the regulation errors and extra measurements that are generally non-vanishing in steady state. A digital regulator is first developed via the conventional emulation-based approach, rendering the regulation errors asymptotically bounded with a small sampling period. We then develop a hybrid design framework by incorporating a generalized hold device, which transforms the original problem into the problem of designing an output feedback controller fulfilling two conditions for a discrete-time system. We show that such a controller can always be obtained by designing a discrete-time internal model, a discrete-time washout filter, and a discrete-time output feedback stabilizer. As a result, the regulation errors are shown to be globally exponentially convergent to zero, while the sampling period is fixed but can be arbitrarily large. This design framework is further developed for a multi-rate digital regulator with a large sampling period of the measurements and a small control execution period.      
### 35.Robust I&amp;I Adaptive Tracking Control of Systems with Nonlinear Parameterization: An ISS Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2102.13411.pdf)
>  This paper studies the immersion and invariance (I&amp;I) adaptive tracking problem for a class of nonlinear systems with nonlinear parameterization in the ISS framework. Under some mild assumptions, a novel I&amp;I adaptive control algorithm is proposed,leading to an interconnection of an ISS estimation error subsystem and an ISS tracking error subsystem. Using an ISS small-gain condition, the desired uniform global asymptotic stability of the resulting interconnected "error" system can be achieved and a sum-type strict Lyapunov function can be explicitly constructed. Taking advantage of this ISS-based design framework,it is shown that the corresponding robustness with respect to the input perturbation can be rendered to be ISS. To remove the need to solve the immersion manifold shaping PDE, a new filter-based approach is proposed, which preserves the ISS-based design framework. Finally, we demonstrate the validness of the proposed framework on a tracking problem for series elastic actuators.      
### 36.Panoramic annular SLAM with loop closure and global optimization  [ :arrow_down: ](https://arxiv.org/pdf/2102.13400.pdf)
>  In this paper, we propose PA-SLAM, a monocular panoramic annular visual SLAM system with loop closure and global optimization. A hybrid point selection strategy is put forward in the tracking front-end, which ensures repeatability of keypoints and enables loop closure detection based on the bag-of-words approach. Every detected loop candidate is verified geometrically and the $Sim(3)$ relative pose constraint is estimated to perform pose graph optimization and global bundle adjustment in the back-end. A comprehensive set of experiments on real-world datasets demonstrates that the hybrid point selection strategy allows reliable loop closure detection, and the accumulated error and scale drift have been significantly reduced via global optimization, enabling PA-SLAM to reach state-of-the-art accuracy while maintaining high robustness and efficiency.      
### 37.Energy Efficiency Maximization in the Uplink Delta-OMA Networks  [ :arrow_down: ](https://arxiv.org/pdf/2102.13359.pdf)
>  Delta-orthogonal multiple access (D-OMA) has been recently investigated as a potential technique to enhance the spectral efficiency in 6G networks. D-OMA enables partial overlapping of the adjacent sub-channels that are assigned to different clusters of users served by non-orthogonal multiple access (NOMA), at the expense of additional interference. In this paper, we analyze the performance of D-OMA in the uplink and develop a multi-objective optimization framework to maximize the uplink energy efficiency in a multi-cell network enabled by D-OMA. Specifically, we optimize the subchannel and transmit power allocations of the users as well as the overlapping percentage of the spectrum between the adjacent sub-channels. The formulated problem is a mixed binary non-linear programming problem; therefore, we first transform the problem into a single-objective problem using Tchebyshev method. Then, we apply the monotonic optimization (MO) to explore the hidden monotonicity of the objective function and constraints, and reformulate the problem into a standard MO in canonical form. The re-formulated problem is then solved by applying the outer polyblock approximation method. Our numerical results show that D-OMA outperforms the conventional non-orthogonal multiple access (NOMA) and orthogonal frequency division multiple access (OFDMA) when the adjacent sub-channel overlap and scheduling is optimized jointly.      
### 38.Efficient Client Contribution Evaluation for Horizontal Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2102.13314.pdf)
>  In federated learning (FL), fair and accurate measurement of the contribution of each federated participant is of great significance. The level of contribution not only provides a rational metric for distributing financial benefits among federated participants, but also helps to discover malicious participants that try to poison the FL framework. Previous methods for contribution measurement were based on enumeration over possible combination of federated participants. Their computation costs increase drastically with the number of participants or feature dimensions, making them inapplicable in practical situations. In this paper an efficient method is proposed to evaluate the contributions of federated participants. This paper focuses on the horizontal FL framework, where client servers calculate parameter gradients over their local data, and upload the gradients to the central server. Before aggregating the client gradients, the central server train a data value estimator of the gradients using reinforcement learning techniques. As shown by experimental results, the proposed method consistently outperforms the conventional leave-one-out method in terms of valuation authenticity as well as time complexity.      
### 39.Deep learning can differentiate IDH-mutant from IDH-wild type GBM  [ :arrow_down: ](https://arxiv.org/pdf/2102.13205.pdf)
>  Background: Distinction of IDH mutant and wildtype GBMs is challenging on MRI, since conventional imaging shows considerable overlap. While few studies employed deep-learning in a mixed low/high grade glioma population, a GBM-specific model is still lacking in the literature. Our objective was to develop a deep-learning model for IDH prediction in GBM by using Convoluted Neural Networks (CNN) on multiparametric MRI. Methods: We included 100 adult patients with pathologically proven GBM and IDH testing. MRI data included: morphologic sequences, rCBV and ADC maps. Tumor area was obtained by a bounding box function on the axial slice with widest tumor extension on T2 images and was projected on every sequence. Data was split into training and test (80:20) sets. A 4 block 2D - CNN architecture was implemented for IDH prediction on every MRI sequence. IDH mutation probability was calculated with softmax activation function from the last dense layer. Highest performance was calculated accounting for model accuracy and categorical cross-entropy loss (CCEL) in the test cohort. Results: Our model achieved the following performance: T1 (accuracy 77%, CCEL 1.4), T2 (accuracy 67%, CCEL 2.41), FLAIR (accuracy 77%, CCEL 1.98), MPRAGE (accuracy 66%, CCEL 2.55), rCBV (accuracy 83%, CCEL 0.64). ADC achieved lower performance. Conclusion: We built a GBM-tailored deep-learning model for IDH mutation prediction, achieving accuracy of 83% with rCBV maps. High predictivity of perfusion images may reflect the known correlation between IDH, hypoxia inducible factor (HIF) and neoangiogenesis. This model may set a path for non-invasive evaluation of IDH mutation in GBM.      
### 40.Clustering for epidemics on networks: a geometric approach  [ :arrow_down: ](https://arxiv.org/pdf/2102.13151.pdf)
>  Infectious diseases typically spread over a contact network with millions of individuals, whose sheer size is a tremendous challenge to analysing and controlling an epidemic outbreak. For some contact networks, it is possible to group individuals into clusters. A high-level description of the epidemic between a few clusters is considerably simpler than on an individual level. However, to cluster individuals, most studies rely on equitable partitions, a rather restrictive structural property of the contact network. In this work, we focus on Susceptible-Infected-Susceptible (SIS) epidemics, and our contribution is threefold. First, we propose a geometric approach to specify all networks for which an epidemic outbreak simplifies to the interaction of only a few clusters. Second, for the complete graph and any initial viral state vectors, we derive the closed-form solution of the nonlinear differential equations of the N-Intertwined Mean-Field Approximation (NIMFA) of the SIS process. Third, by relaxing the notion of equitable partitions, we derive low-complexity approximations and bounds for epidemics on arbitrary contact networks. Our results are an important step towards understanding and controlling epidemics on large networks.      
### 41.Multi-Domain Learning by Meta-Learning: Taking Optimal Steps in Multi-Domain Loss Landscapes by Inner-Loop Learning  [ :arrow_down: ](https://arxiv.org/pdf/2102.13147.pdf)
>  We consider a model-agnostic solution to the problem of Multi-Domain Learning (MDL) for multi-modal applications. Many existing MDL techniques are model-dependent solutions which explicitly require nontrivial architectural changes to construct domain-specific modules. Thus, properly applying these MDL techniques for new problems with well-established models, e.g. U-Net for semantic segmentation, may demand various low-level implementation efforts. In this paper, given emerging multi-modal data (e.g., various structural neuroimaging modalities), we aim to enable MDL purely algorithmically so that widely used neural networks can trivially achieve MDL in a model-independent manner. To this end, we consider a weighted loss function and extend it to an effective procedure by employing techniques from the recently active area of learning-to-learn (meta-learning). Specifically, we take inner-loop gradient steps to dynamically estimate posterior distributions over the hyperparameters of our loss function. Thus, our method is model-agnostic, requiring no additional model parameters and no network architecture changes; instead, only a few efficient algorithmic modifications are needed to improve performance in MDL. We demonstrate our solution to a fitting problem in medical imaging, specifically, in the automatic segmentation of white matter hyperintensity (WMH). We look at two neuroimaging modalities (T1-MR and FLAIR) with complementary information fitting for our problem.      
### 42.Robust Pollen Imagery Classification with Generative Modeling and Mixup Training  [ :arrow_down: ](https://arxiv.org/pdf/2102.13143.pdf)
>  Deep learning approaches have shown great success in image classification tasks and can aid greatly towards the fast and reliable classification of pollen grain aerial imagery. However, often-times deep learning methods in the setting of natural images can suffer generalization problems and yield poor performance on unseen test distribution. In this work, we present and a robust deep learning framework that can generalize well for pollen grain aerobiological imagery classification. We develop a convolutional neural network-based pollen grain classification approach and combine some of the best practices in deep learning for better generalization. In addition to commonplace approaches like data-augmentation and weight regularization, we utilize implicit regularization methods like manifold mixup to allow learning of smoother decision boundaries. We also make use of proven state-of-the-art architectural choices like EfficientNet convolutional neural networks. Inspired by the success of generative modeling with variational autoencoders, we train models with a richer learning objective which can allow the model to focus on the relevant parts of the image. Finally, we create an ensemble of neural networks, for the robustness of the test set predictions. Based on our experiments, we show improved generalization performance as measured with a weighted F1-score with the aforementioned approaches. The proposed approach earned a fourth-place in the final rankings in the ICPR-2020 Pollen Grain Classification Challenge; with a 0.972578 weighted F1 score,0.950828 macro average F1 scores, and 0.972877 recognition accuracy.      
### 43.Over-The-Air Computation in Correlated Channels  [ :arrow_down: ](https://arxiv.org/pdf/2101.04690.pdf)
>  This paper addresses the problem of Over-The-Air (OTA) computation in wireless networks which has the potential to realize huge efficiency gains for instance in training of distributed ML models. We provide non-asymptotic, theoretical guarantees for OTA computation in fast-fading wireless channels where the fading and noise may be correlated. The distributions of fading and noise are not restricted to Gaussian distributions, but instead are assumed to follow a distribution in the more general sub-gaussian class. Furthermore, our result does not make any assumptions on the distribution of the sources and therefore, it can, e.g., be applied to arbitrarily correlated sources. We illustrate our analysis with numerical evaluations for OTA computation of two example functions in large wireless networks: the arithmetic mean and the Euclidean norm.      
### 44.Over-The-Air Computation in Correlated Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.02648.pdf)
>  This paper presents and analyzes a one-shot coding scheme for the \gls{ota} computation over a fast-fading multiple-access wireless channel. The assumed channel model incorporates correlations both in fading and noise over time as well as among users. The model also allows for non-Gaussian components in fading and noise, provided that the distributions are sub-Gaussian (as is the case for a sum of Gaussian and bounded random variables), rendering the proposed scheme robust to a large class of non-Gaussian interference and noise known to occur in many practical scenarios. OTA computation has a huge potential for reducing communication cost in applications such as Machine Learning (ML)-based distributed anomaly detection in large wireless sensor networks. We illustrate this potential through extensive numerical simulations.      
### 45.Distributed Approximation of Functions over Fast Fading Channels with Applications to Distributed Learning and the Max-Consensus Problem  [ :arrow_down: ](https://arxiv.org/pdf/1907.03777.pdf)
>  In this work, we consider the problem of distributed approximation of functions over multiple-access channels with additive noise. In contrast to previous works, we take fast fading into account and give explicit probability bounds for the approximation error allowing us to derive bounds on the number of channel uses that are needed to approximate a function up to a given approximation accuracy. Neither the fading nor the noise process is limited to Gaussian distributions. Instead, we consider sub-gaussian random variables which include Gaussian as well as many other distributions of practical relevance. The results are motivated by and have immediate applications to a) computing predictors in models for distributed machine learning and b) the max-consensus problem in ultra-dense networks.      
### 46.A Scalable Max-Consensus Protocol For Noisy Ultra-Dense Networks  [ :arrow_down: ](https://arxiv.org/pdf/1903.02885.pdf)
>  We introduce \emph{ScalableMax}, a novel communication scheme for achieving max-consensus in a network of multiple agents which harnesses the interference in the wireless channel as well as its multicast capabilities. In a sufficiently dense network, the amount of communication resources required grows logarithmically with the number of nodes, while in state-of-the-art approaches, this growth is at least linear. ScalableMax can handle additive noise and works well in a high SNR regime. For medium and low SNR, we propose the \emph{ScalableMax-EC} scheme, which extends the ideas of ScalableMax by introducing a novel error correction scheme. It achieves lower error rates at the cost of using more channel resources. However, it preserves the logarithmic growth with the number of agents in the system.      
