# ArXiv eess --Fri, 5 Mar 2021
### 1.Super-Resolution DOA Estimation for Wideband Signals using Arbitrary Linear Arrays without Focusing Matrices  [ :arrow_down: ](https://arxiv.org/pdf/2103.03244.pdf)
>  We focus on developing an effective Direction Of Arrival (DOA) estimation method for wideband sources based on the gridless sparse concept. Previous coherent methods have been designed by dividing wideband frequencies into a few subbands which are transferred to a reference subband using focusing matrices. In this work, as opposed to the previous techniques, we propose a convex optimization problem that leads to an accurate wideband DOA estimation method with no need for any focusing matrix. Moreover, in this method, no initial DOA estimates are required and it can be used for any arbitrary linear arrays. Numerical simulations show that in comparison to some well-known techniques, the proposed method generates outstanding accuracy and better robustness to noise. The effectiveness of the method is also verified in presence of close adjacent sources.      
### 2.Front-end Diarization for Percussion Separation in Taniavartanam of Carnatic Music Concerts  [ :arrow_down: ](https://arxiv.org/pdf/2103.03215.pdf)
>  Instrument separation in an ensemble is a challenging task. In this work, we address the problem of separating the percussive voices in the taniavartanam segments of Carnatic music. In taniavartanam, a number of percussive instruments play together or in tandem. Separation of instruments in regions where only one percussion is present leads to interference and artifacts at the output, as source separation algorithms assume the presence of multiple percussive voices throughout the audio segment. We prevent this by first subjecting the taniavartanam to diarization. This process results in homogeneous clusters consisting of segments of either a single voice or multiple voices. A cluster of segments with multiple voices is identified using the Gaussian mixture model (GMM), which is then subjected to source separation. A deep recurrent neural network (DRNN) based approach is used to separate the multiple instrument segments. The effectiveness of the proposed system is evaluated on a standard Carnatic music dataset. The proposed approach provides close-to-oracle performance for non-overlapping segments and a significant improvement over traditional separation schemes.      
### 3.State and parameter estimation for model-based retinal laser treatment  [ :arrow_down: ](https://arxiv.org/pdf/2103.03189.pdf)
>  We present an approach for state and parameter estimation in retinal laser treatment by a novel setup where both measurement and heating is performed by a single laser. In this medical application, the temperature that is induced by the laser in the patient's eye is critical for a successful and safe treatment. To this end, we pursue a model-based approach using a model given by a heat diffusion equation on a cylindrical domain, where the source term is given by the absorbed laser power. The model is parametric in the sense that it involves an absorption coefficient, which depends on the treatment spot and plays a central role in the input-output behavior of the system. After discretization, we apply a particularly suited parametric model order reduction to ensure real-time tractability while retaining parameter dependence. We augment known state estimation techniques, i.e., extended Kalman filtering and moving horizon estimation, with parameter estimation to estimate the absorption coefficient and the current state of the system. Eventually, we show first results for simulated and experimental data from porcine eyes. We find that, regarding convergence speed, the moving horizon estimation slightly outperforms the extended Kalman filter on measurement data in terms of parameter and state estimation, however, on simulated data the results are very similar.      
### 4.The Effect of Prosumer Duality on Power Market: Evidence from the Cournot Model  [ :arrow_down: ](https://arxiv.org/pdf/2103.03155.pdf)
>  Distributed energy resources behind the meter and automation systems enable traditional electricity consumers to become prosumers (producers/consumers) that can participate in peer-to-peer exchange of electricity and in retail electricity markets. Emerging prosumers can provide benefits to the system by exchanging energy and energy-related services. More importantly, they can do so in a more honest and more competitive way than the traditional producer/consumer systems. We extend the traditional Cournot model to show that the dual nature of prosumers can lead to more competitive behavior under a game theoretic scenario. We show that best response supply quantities of a prosumer are usually closer to the competitive level compared to those of a producer.      
### 5.Serial Interference Cancellation for Improving uplink in LoRa-like Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.03146.pdf)
>  In this paper, we present a new receiver design, which significantly improves performance in the Internet of Things networks such as LoRa, i.e., having a chirp spread spectrum modulation. The proposed receiver is able to demodulate multiple users simultaneously transmitted over the same frequency channel with the same spreading factor. From a non-orthogonal multiple access point of view, it is based on the power domain and uses serial interference cancellation. Simulation results show that the receiver allows a significant increase in the number of connected devices in the network.      
### 6.COIN: COmpression with Implicit Neural representations  [ :arrow_down: ](https://arxiv.org/pdf/2103.03123.pdf)
>  We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.      
### 7.Memory-Efficient Network for Large-scale Video Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2103.03089.pdf)
>  Video snapshot compressive imaging (SCI) captures a sequence of video frames in a single shot using a 2D detector. The underlying principle is that during one exposure time, different masks are imposed on the high-speed scene to form a compressed measurement. With the knowledge of masks, optimization algorithms or deep learning methods are employed to reconstruct the desired high-speed video frames from this snapshot measurement. Unfortunately, though these methods can achieve decent results, the long running time of optimization algorithms or huge training memory occupation of deep networks still preclude them in practical applications. In this paper, we develop a memory-efficient network for large-scale video SCI based on multi-group reversible 3D convolutional neural networks. In addition to the basic model for the grayscale SCI system, we take one step further to combine demosaicing and SCI reconstruction to directly recover color video from Bayer measurements. Extensive results on both simulation and real data captured by SCI cameras demonstrate that our proposed model outperforms previous state-of-the-art with less memory and thus can be used in large-scale problems. The code is at <a class="link-external link-https" href="https://github.com/BoChenGroup/RevSCI-net" rel="external noopener nofollow">this https URL</a>.      
### 8.A Multi-Modal Respiratory Disease Exacerbation Prediction Technique Based on a Spatio-Temporal Machine Learning Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2103.03086.pdf)
>  Chronic respiratory diseases, such as chronic obstructive pulmonary disease and asthma, are a serious health crisis, affecting a large number of people globally and inflicting major costs on the economy. Current methods for assessing the progression of respiratory symptoms are either subjective and inaccurate, or complex and cumbersome, and do not incorporate environmental factors. Lacking predictive assessments and early intervention, unexpected exacerbations can lead to hospitalizations and high medical costs. This work presents a multi-modal solution for predicting the exacerbation risks of respiratory diseases, such as COPD, based on a novel spatio-temporal machine learning architecture for real-time and accurate respiratory events detection, and tracking of local environmental and meteorological data and trends. The proposed new machine learning architecture blends key attributes of both convolutional and recurrent neural networks, allowing extraction of both spatial and temporal features encoded in respiratory sounds, thereby leading to accurate classification and tracking of symptoms. Combined with the data from environmental and meteorological sensors, and a predictive model based on retrospective medical studies, this solution can assess and provide early warnings of respiratory disease exacerbations. This research will improve the quality of patients' lives through early medical intervention, thereby reducing hospitalization rates and medical costs.      
### 9.Model-based image adjustment for a successful pansharpening  [ :arrow_down: ](https://arxiv.org/pdf/2103.03062.pdf)
>  A new model-based image adjustment for the enhancement of multi-resolution image fusion or pansharpening is proposed. Such image adjustment is needed for most pansharpening methods using panchromatic band and/or intensity image (calculated as a weighted sum of multispectral bands) as an input. Due various reasons, e.g. calibration inaccuracies, usage of different sensors, input images for pansharpening: low resolution multispectral image or more precisely the calculated intensity image and high resolution panchromatic image may differ in values of their physical properties, e.g. radiances or reflectances depending on the processing level. But the same objects/classes in both images should exhibit similar values or more generally similar statistics. Similarity definition will depend on a particular application. For a successful fusion of data from two sensors the energy balance between radiances/reflectances of both sensors should hold. A virtual band is introduced to compensate for total energy disbalance in different sensors. Its estimation consists of several steps: first, weights for individual spectral bands are estimated in a low resolution scale, where both multispectral and panchromatic images (low pass filtered version) are available, then, the estimated virtual band is up-sampled to a high scale and, finally, high resolution panchromatic band is corrected by subtracting virtual band. This corrected panchromatic band is used instead of original panchromatic image in the following pansharpening. It is shown, for example, that the performance quality of component substitution based methods can be increased significantly.      
### 10.Self-supervised deep convolutional neural network for chest X-ray classification  [ :arrow_down: ](https://arxiv.org/pdf/2103.03055.pdf)
>  Chest radiography is a relatively cheap, widely available medical procedure that conveys key information for making diagnostic decisions. Chest X-rays are almost always used in the diagnosis of respiratory diseases such as pneumonia or the recent COVID-19. In this paper, we propose a self-supervised deep neural network that is pretrained on an unlabeled chest X-ray dataset. The learned representations are transferred to downstream task - the classification of respiratory diseases. The results obtained on four public datasets show that our approach yields competitive results without requiring large amounts of labeled training data.      
### 11.A Neural Text-to-Speech Model Utilizing Broadcast Data Mixed with Background Music  [ :arrow_down: ](https://arxiv.org/pdf/2103.03049.pdf)
>  Recently, it has become easier to obtain speech data from various media such as the internet or YouTube, but directly utilizing them to train a neural text-to-speech (TTS) model is difficult. The proportion of clean speech is insufficient and the remainder includes background music. Even with the global style token (GST). Therefore, we propose the following method to successfully train an end-to-end TTS model with limited broadcast data. First, the background music is removed from the speech by introducing a music filter. Second, the GST-TTS model with an auxiliary quality classifier is trained with the filtered speech and a small amount of clean speech. In particular, the quality classifier makes the embedding vector of the GST layer focus on representing the speech quality (filtered or clean) of the input speech. The experimental results verified that the proposed method synthesized much more high-quality speech than conventional methods.      
### 12.Detecting Spurious Correlations with Sanity Tests for Artificial Intelligence Guided Radiology Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.03048.pdf)
>  Artificial intelligence (AI) has been successful at solving numerous problems in machine perception. In radiology, AI systems are rapidly evolving and show progress in guiding treatment decisions, diagnosing, localizing disease on medical images, and improving radiologists' efficiency. A critical component to deploying AI in radiology is to gain confidence in a developed system's efficacy and safety. The current gold standard approach is to conduct an analytical validation of performance on a generalization dataset from one or more institutions, followed by a clinical validation study of the system's efficacy during deployment. Clinical validation studies are time-consuming, and best practices dictate limited re-use of analytical validation data, so it is ideal to know ahead of time if a system is likely to fail analytical or clinical validation. In this paper, we describe a series of sanity tests to identify when a system performs well on development data for the wrong reasons. We illustrate the sanity tests' value by designing a deep learning system to classify pancreatic cancer seen in computed tomography scans.      
### 13.Cognitive-Driven Optimization of Sparse Array Transceiver for MIMO Radar Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2103.03031.pdf)
>  Cognitive multiple-input multiple-output (MIMO) radar is capable of adjusting system parameters adaptively by sensing and learning in complex dynamic environment. Beamforming performance of MIMO radar is guided by both beamforming weight coefficients and the transceiver configuration. We propose a cognitive-driven MIMO array design where both the beamforming weights and the transceiver configuration are adaptively and concurrently optimized under different environmental conditions. The perception-action cycle involves data collection of full virtual array, covariance reconstruction and joint design of the transmit and receive arrays by antenna selection.The optimal transceiver array design is realized by promoting two-dimensional group sparsity via iteratively minimizing reweighted mixed L21-norm, with constraints imposed on transceiver antenna spacing for proper transmit/receive isolation. Simulations are provided to demonstrate the "perception-action" capability of the proposed cognitive sparse MIMO array in achieving enhanced beamforming and anti-jamming in dynamic target and interference environment.      
### 14.Adaptive Sparse Array Beamformer Design by Regularized Complementary Antenna Switching  [ :arrow_down: ](https://arxiv.org/pdf/2103.03026.pdf)
>  In this work, we propose a novel strategy of adaptive sparse array beamformer design, referred to as regularized complementary antenna switching (RCAS), to swiftly adapt both array configuration and excitation weights in accordance to the dynamic environment for enhancing interference suppression. In order to achieve an implementable design of array reconfiguration, the RCAS is conducted in the framework of regularized antenna switching, whereby the full array aperture is collectively divided into separate groups and only one antenna in each group is switched on to connect with the processing channel. A set of deterministic complementary sparse arrays with good quiescent beampatterns is first designed by RCAS and full array data is collected by switching among them while maintaining resilient interference suppression. Subsequently, adaptive sparse array tailored for the specific environment is calculated and reconfigured based on the information extracted from the full array data. The RCAS is devised as an exclusive cardinality-constrained optimization, which is reformulated by introducing an auxiliary variable combined with a piece-wise linear function to approximate the $l_0$-norm function. A regularization formulation is proposed to solve the problem iteratively and eliminate the requirement of feasible initial search point. A rigorous theoretical analysis is conducted, which proves that the proposed algorithm is essentially an equivalent transformation of the original cardinality-constrained optimization. Simulation results validate the effectiveness of the proposed RCAS strategy.      
### 15.End-to-End Mispronunciation Detection and Diagnosis From Raw Waveforms  [ :arrow_down: ](https://arxiv.org/pdf/2103.03023.pdf)
>  Mispronunciation detection and diagnosis (MDD) is designed to identify pronunciation errors and provide instructive feedback to guide non-native language learners, which is a core component in computer-assisted pronunciation training (CAPT) systems. However, MDD often suffers from the data-sparsity problem due to that collecting non-native data and the associated annotations is time-consuming and labor-intensive. To address this issue, we explore a fully end-to-end (E2E) neural model for MDD, which processes learners' speech directly based on raw waveforms. Compared to conventional hand-crafted acoustic features, raw waveforms retain more acoustic phenomena and potentially can help neural networks discover better and more customized representations. To this end, our MDD model adopts a co-called SincNet module to take input a raw waveform and covert it to a suitable vector representation sequence. SincNet employs the cardinal sine (sinc) function to implement learnable bandpass filters, drawing inspiration from the convolutional neural network (CNN). By comparison to CNN, SincNet has fewer parameters and is more amenable to human interpretation. Extensive experiments are conducted on the L2-ARCTIC dataset, which is a publicly-available non-native English speech corpus compiled for research on CAPT. We find that the sinc filters of SincNet can be adapted quickly for non-native language learners of different nationalities. Furthermore, our model can achieve comparable mispronunciation detection performance in relation to state-of-the-art E2E MDD models that take input the standard handcrafted acoustic features. Besides that, our model also provides considerable improvements on phone error rate (PER) and diagnosis accuracy.      
### 16.Perceptual Image Restoration with High-Quality Priori and Degradation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.03010.pdf)
>  Perceptual image restoration seeks for high-fidelity images that most likely degrade to given images. For better visual quality, previous work proposed to search for solutions within the natural image manifold, by exploiting the latent space of a generative model. However, the quality of generated images are only guaranteed when latent embedding lies close to the prior distribution. In this work, we propose to restrict the feasible region within the prior manifold. This is accomplished with a non-parametric metric for two distributions: the Maximum Mean Discrepancy (MMD). Moreover, we model the degradation process directly as a conditional distribution. We show that our model performs well in measuring the similarity between restored and degraded images. Instead of optimizing the long criticized pixel-wise distance over degraded images, we rely on such model to find visual pleasing images with high probability. Our simultaneous restoration and enhancement framework generalizes well to real-world complicated degradation types. The experimental results on perceptual quality and no-reference image quality assessment (NR-IQA) demonstrate the superior performance of our method.      
### 17.Data-driven distributionally robust MPC for constrained stochastic systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.03006.pdf)
>  In this paper we introduce a novel approach to distributionally robust optimal control that supports online learning of the ambiguity set, while guaranteeing recursive feasibility. We introduce conic representable risk, which is useful to derive tractable reformulations of distributionally robust optimization problems. Specifically, to illustrate the techniques introduced, we utilize risk measures constructed based on data-driven ambiguity sets, constraining the second moment of the random disturbance. In the optimal control setting, such moment-based risk measures lead to tractable optimal controllers when combined with affine disturbance feedback. Assumptions on the constraints are given that guarantee recursive feasibility. The resulting control scheme acts as a robust controller when little data is available and converges to the certainty equivalent controller when a large sample count implies high confidence in the estimated second moment. This is illustrated in a numerical experiment.      
### 18.Automated Detection of Coronary Artery Stenosis in X-ray Angiography using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.02969.pdf)
>  Coronary artery disease leading up to stenosis, the partial or total blocking of coronary arteries, is a severe condition that affects millions of patients each year. Automated identification and classification of stenosis severity from minimally invasive procedures would be of great clinical value, but existing methods do not match the accuracy of experienced cardiologists, due to the complexity of the task. Although a number of computational approaches for quantitative assessment of stenosis have been proposed to date, the performance of these methods is still far from the required levels for clinical applications. In this paper, we propose a two-step deep-learning framework to partially automate the detection of stenosis from X-ray coronary angiography images. In the two steps, we used two distinct convolutional neural network architectures, one to automatically identify and classify the angle of view, and another to determine the bounding boxes of the regions of interest in frames where stenosis is visible. Transfer learning and data augmentation techniques were used to boost the performance of the system in both tasks. We achieved a 0.97 accuracy on the task of classifying the Left/Right Coronary Artery (LCA/RCA) angle view and 0.68/0.73 recall on the determination of the regions of interest, for LCA and RCA, respectively. These results compare favorably with previous results obtained using related approaches, and open the way to a fully automated method for the identification of stenosis severity from X-ray angiographies.      
### 19.Probabilistic combination of eigenlungs-based classifiers for COVID-19 diagnosis in chest CT images  [ :arrow_down: ](https://arxiv.org/pdf/2103.02961.pdf)
>  The outbreak of the COVID-19 (Coronavirus disease 2019) pandemic has changed the world. According to the World Health Organization (WHO), there have been more than 100 million confirmed cases of COVID-19, including more than 2.4 million deaths. It is extremely important the early detection of the disease, and the use of medical imaging such as chest X-ray (CXR) and chest Computed Tomography (CCT) have proved to be an excellent solution. However, this process requires clinicians to do it within a manual and time-consuming task, which is not ideal when trying to speed up the diagnosis. In this work, we propose an ensemble classifier based on probabilistic Support Vector Machine (SVM) in order to identify pneumonia patterns while providing information about the reliability of the classification. Specifically, each CCT scan is divided into cubic patches and features contained in each one of them are extracted by applying kernel PCA. The use of base classifiers within an ensemble allows our system to identify the pneumonia patterns regardless of their size or location. Decisions of each individual patch are then combined into a global one according to the reliability of each individual classification: the lower the uncertainty, the higher the contribution. Performance is evaluated in a real scenario, yielding an accuracy of 97.86%. The large performance obtained and the simplicity of the system (use of deep learning in CCT images would result in a huge computational cost) evidence the applicability of our proposal in a real-world environment.      
### 20.Model Reference Adaptive Control of Piecewise Affine Systems with State Tracking Performance Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2103.02910.pdf)
>  In this paper, we investigate the model reference adaptive control approach for uncertain piecewise affine systems with performance guarantees. The proposed approach ensures the error metric, defined as the weighted Euclidean norm of the state tracking error, to be confined within a user-defined time-varying performance bound. We introduce an auxiliary performance function to construct a barrier Lyapunov function. This auxiliary performance signal is reset at each switching instant, which prevents the transgression of the barriers caused by the jumps of the error metric at switching instants. The dwell time constraints are derived based on the parameters of the user-defined performance bound and the auxiliary performance function. We also prove that the Lyapunov function is non-increasing even at the switching instants and thus does not impose extra dwell time constraints. Furthermore, we propose the robust modification of the adaptive controller for the uncertain piecewise affine systems subject to unmatched disturbances. A Numerical example validates the correctness of the proposed approach.      
### 21.End-to-end acoustic modelling for phone recognition of young readers  [ :arrow_down: ](https://arxiv.org/pdf/2103.02899.pdf)
>  Automatic recognition systems for child speech are lagging behind those dedicated to adult speech in the race of performance. This phenomenon is due to the high acoustic and linguistic variability present in child speech caused by their body development, as well as the lack of available child speech data. Young readers speech additionally displays peculiarities, such as slow reading rate and presence of reading mistakes, that hardens the task. This work attempts to tackle the main challenges in phone acoustic modelling for young child speech with limited data, and improve understanding of strengths and weaknesses of a wide selection of model architectures in this domain. We find that transfer learning techniques are highly efficient on end-to-end architectures for adult-to-child adaptation with a small amount of child speech data. Through transfer learning, a Transformer model complemented with a Connectionist Temporal Classification (CTC) objective function, reaches a phone error rate of 28.1%, outperforming a state-of-the-art DNN-HMM model by 6.6% relative, as well as other end-to-end architectures by more than 8.5% relative. An analysis of the models' performance on two specific reading tasks (isolated words and sentences) is provided, showing the influence of the utterance length on attention-based and CTC-based models. The Transformer+CTC model displays an ability to better detect reading mistakes made by children, that can be attributed to the CTC objective function effectively constraining the attention mechanisms to be monotonic.      
### 22.Stability and $\mathcal{H}_{\infty}$ Performance Analysis of Stochastic Linear Networked and Quantized Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.02894.pdf)
>  This paper studies the stability and $\mathcal{H}_{\infty}$ performance analysis problem for linear networked and quantized control systems with both communication delays random packet losses. To deal with the network-induced uncertainties and random packet dropouts, a novel discrete-time stochastic system model is developed for continuous-time networked control systems, and further overapproximated via a polytopic system with norm-bounded uncertainty. Based on the overapproximated system model, sufficient conditions are established for linear networked and quantized control systems in different cases to guarantee input-to-state stability and $\mathcal{H}_{\infty}$ performance with respect to the network-induced errors. Finally, a numerical example is presented to illustrated the developed results.      
### 23.Data-Based System Analysis and Control of Flat Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.02892.pdf)
>  Willems et al. showed that all input-output trajectories of a discrete-time linear time-invariant system can be obtained using linear combinations of time shifts of a single, persistently exciting, input-output trajectory of that system. In this paper, we extend this result to the class of discrete-time single-input single-output flat nonlinear systems. We propose a data-based parametrization of all trajectories using only input-output data. Further, we use this parametrization to solve the data-based simulation and output-matching control problems for the unknown system without explicitly identifying a model. Finally, we illustrate the main results with numerical examples.      
### 24.A Cross Channel Context Model for Latents in Deep Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2103.02884.pdf)
>  This paper presents a cross channel context model for latents in deep image compression. Generally, deep image compression is based on an autoencoder framework, which transforms the original image to latents at the encoder and recovers the reconstructed image from the quantized latents at the decoder. The transform is usually combined with an entropy model, which estimates the probability distribution of the quantized latents for arithmetic coding. Currently, joint autoregressive and hierarchical prior entropy models are widely adopted to capture both the global contexts from the hyper latents and the local contexts from the quantized latent elements. For the local contexts, the widely adopted 2D mask convolution can only capture the spatial context. However, we observe that there are strong correlations between different channels in the latents. To utilize the cross channel correlations, we propose to divide the latents into several groups according to channel index and code the groups one by one, where previously coded groups are utilized to provide cross channel context for the current group. The proposed cross channel context model is combined with the joint autoregressive and hierarchical prior entropy model. Experimental results show that, using PSNR as the distortion metric, the combined model achieves BD-rate reductions of 6.30% and 6.31% over the baseline entropy model, and 2.50% and 2.20% over the latest video coding standard Versatile Video Coding (VVC) for the Kodak and CVPR CLIC2020 professional dataset, respectively. In addition, when optimized for the MS-SSIM metric, our approach generates visually more pleasant reconstructed images.      
### 25.crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2103.02858.pdf)
>  In this paper, we present an open-source software for developing a nonparallel voice conversion (VC) system named crank. Although we have released an open-source VC software based on the Gaussian mixture model named sprocket in the last VC Challenge, it is not straightforward to apply any speech corpus because it is necessary to prepare parallel utterances of source and target speakers to model a statistical conversion function. To address this issue, in this study, we developed a new open-source VC software that enables users to model the conversion function by using only a nonparallel speech corpus. For implementing the VC software, we used a vector-quantized variational autoencoder (VQVAE). To rapidly examine the effectiveness of recent technologies developed in this research field, crank also supports several representative works for autoencoder-based VC methods such as the use of hierarchical architectures, cyclic architectures, generative adversarial networks, speaker adversarial training, and neural vocoders. Moreover, it is possible to automatically estimate objective measures such as mel-cepstrum distortion and pseudo mean opinion score based on MOSNet. In this paper, we describe representative functions developed in crank and make brief comparisons by objective evaluations.      
### 26.Distributed Optimal Load Frequency Control with Stochastic Wind Power Generation  [ :arrow_down: ](https://arxiv.org/pdf/2103.02857.pdf)
>  Motivated by the inadequacy of conventional control methods for power networks with a large share of renewable generation, in this paper we study the (stochastic) passivity property of wind turbines based on the Doubly Fed Induction Generator (DFIG). Differently from the majority of the results in the literature, where renewable generation is ignored or assumed to be constant, we model wind power generation as a stochastic process, where wind speed is described by a class of stochastic differential equations. Then, we design a distributed control scheme that achieves load frequency control and economic dispatch, ensuring the stochastic stability of the controlled network.      
### 27.Learning With Context Feedback Loop for Robust Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.02844.pdf)
>  Deep learning has successfully been leveraged for medical image segmentation. It employs convolutional neural networks (CNN) to learn distinctive image features from a defined pixel-wise objective function. However, this approach can lead to less output pixel interdependence producing incomplete and unrealistic segmentation results. In this paper, we present a fully automatic deep learning method for robust medical image segmentation by formulating the segmentation problem as a recurrent framework using two systems. The first one is a forward system of an encoder-decoder CNN that predicts the segmentation result from the input image. The predicted probabilistic output of the forward system is then encoded by a fully convolutional network (FCN)-based context feedback system. The encoded feature space of the FCN is then integrated back into the forward system's feed-forward learning process. Using the FCN-based context feedback loop allows the forward system to learn and extract more high-level image features and fix previous mistakes, thereby improving prediction accuracy over time. Experimental results, performed on four different clinical datasets, demonstrate our method's potential application for single and multi-structure medical image segmentation by outperforming the state of the art methods. With the feedback loop, deep learning methods can now produce results that are both anatomically plausible and robust to low contrast images. Therefore, formulating image segmentation as a recurrent framework of two interconnected networks via context feedback loop can be a potential method for robust and efficient medical image analysis.      
### 28.PET Image Reconstruction with Multiple Kernels and Multiple Kernel Space Regularizers  [ :arrow_down: ](https://arxiv.org/pdf/2103.02813.pdf)
>  Kernelized maximum-likelihood (ML) expectation maximization (EM) methods have recently gained prominence in PET image reconstruction, outperforming many previous state-of-the-art methods. But they are not immune to the problems of non-kernelized MLEM methods in potentially large reconstruction error and high sensitivity to iteration number. This paper demonstrates these problems by theoretical reasoning and experiment results, and provides a novel solution to solve these problems. The solution is a regularized kernelized MLEM with multiple kernel matrices and multiple kernel space regularizers that can be tailored for different applications. To reduce the reconstruction error and the sensitivity to iteration number, we present a general class of multi-kernel matrices and two regularizers consisting of kernel image dictionary and kernel image Laplacian quatradic, and use them to derive the single-kernel regularized EM and multi-kernel regularized EM algorithms for PET image reconstruction. These new algorithms are derived using the technical tools of multi-kernel combination in machine learning, image dictionary learning in sparse coding, and graph Laplcian quadratic in graph signal processing. Extensive tests and comparisons on the simulated and in vivo data are presented to validate and evaluate the new algorithms, and demonstrate their superior performance and advantages over the kernelized MLEM and other conventional methods.      
### 29.Hybrid Interference Mitigation Using Analog Prewhitening  [ :arrow_down: ](https://arxiv.org/pdf/2103.02791.pdf)
>  This paper proposes a novel scheme for mitigating strong interferences, which is applicable to various wireless scenarios, including full-duplex wireless communications and uncoordinated heterogenous networks. As strong interferences can saturate the receiver's analog-to-digital converters (ADC), they need to be mitigated both before and after the ADCs, i.e., via hybrid processing. The key idea of the proposed scheme, namely the Hybrid Interference Mitigation using Analog Prewhitening (HIMAP), is to insert an M-input M-output analog phase shifter network (PSN) between the receive antennas and the ADCs to spatially prewhiten the interferences, which requires no signal information but only an estimate of the covariance matrix. After interference mitigation by the PSN prewhitener, the preamble can be synchronized, the signal channel response can be estimated, and thus a minimum mean squared error (MMSE) beamformer can be applied in the digital domain to further mitigate the residual interferences. The simulation results verify that the HIMAP scheme can suppress interferences 80dB stronger than the signal by using off-the-shelf phase shifters (PS) of 6-bit resolution.      
### 30.Contrast Adaptive Tissue Classification by Alternating Segmentation and Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2103.02767.pdf)
>  Deep learning approaches to the segmentation of magnetic resonance images have shown significant promise in automating the quantitative analysis of brain images. However, a continuing challenge has been its sensitivity to the variability of acquisition protocols. Attempting to segment images that have different contrast properties from those within the training data generally leads to significantly reduced performance. Furthermore, heterogeneous data sets cannot be easily evaluated because the quantitative variation due to acquisition differences often dwarfs the variation due to the biological differences that one seeks to measure. In this work, we describe an approach using alternating segmentation and synthesis steps that adapts the contrast properties of the training data to the input image. This allows input images that do not resemble the training data to be more consistently segmented. A notable advantage of this approach is that only a single example of the acquisition protocol is required to adapt to its contrast properties. We demonstrate the efficacy of our approaching using brain images from a set of human subjects scanned with two different T1-weighted volumetric protocols.      
### 31.Sub-Terahertz and mmWave Penetration Loss Measurements for Indoor Environments  [ :arrow_down: ](https://arxiv.org/pdf/2103.02745.pdf)
>  Millimeter-wave (mmWave) and terahertz (THz) spectrum can support significantly higher data rates compared to lower frequency bands and hence are being actively considered for 5G wireless networks and beyond. These bands have high free-space path loss (FSPL) in line-of-sight (LOS) propagation due to their shorter wavelength. Moreover, in non-line-of-sight (NLOS) scenario, these two bands suffer higher penetration loss than lower frequency bands which could seriously affect the network coverage. It is therefore critical to study the NLOS penetration loss introduced by different building materials at mmWave and THz bands, to help establish link budgets for an accurate performance analysis in indoor environments. In this work, we measured the penetration loss and the attenuation of several common constructional materials at mmWave (28 and 39 GHz) and sub-THz (120 and 144 GHz) bands. Measurements were conducted using a channel sounder based on NI PXI platforms. Results show that the penetration loss changes extensively based on the frequency and the material properties, ranging from 0.401 dB for ceiling tile at 28 GHz, to 16.608 dB for plywood at 144 GHz. Ceiling tile has the lowest measured attenuation at 28 GHz, while clear glass has the highest attenuation of 27.633 dB/cm at 144 GHz. As expected, the penetration loss and attenuation increased with frequency for all the tested materials.      
### 32.Adaptive Transmission for Distributed Detection in Energy Harvesting Wireless Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.02742.pdf)
>  We consider a wireless sensor network, consisting of N heterogeneous sensors and a fusion center (FC), tasked with detecting a known signal in uncorrelated Gaussian noises. Each sensor can harvest randomly arriving energy and store it in a finite-size battery. Sensors communicate directly with the FC over orthogonal fading channels. Each sensor adapts its transmit symbol power, such that the larger its stored energy and its quantized channel gain are, the higher its transmit symbol power is. To strike a balance between energy harvesting and energy consumption, we formulate two constrained optimization problems (P1) and (P2), where in both problems we seek the jointly optimal local decision thresholds and channel gain quantization thresholds. While in (P1) we maximize the Jdivergence of the received signal densities at the FC, in (P2) we minimize the average total transmit power, subject to certain constraints. We solve (P1) and (P2), assuming that the batteries reach their steady-state. Our simulation results demonstrate the effectiveness of our optimization on enhancing the detection performance in (P1), and lowering the average total transmit power in (P2). They also reveal how the energy harvesting rate, the battery size, the sensor observation and communication channel parameters impact obtained solutions.      
### 33.The Spatial Selective Auditory Attention of Cochlear Implant Users in Different Conversational Sound Levels  [ :arrow_down: ](https://arxiv.org/pdf/2103.02703.pdf)
>  In multi speakers environments, cochlear implant (CI) users may attend to a target sound source in a different manner from the normal hearing (NH) individuals during a conversation. This study attempted to investigate the effect of conversational sound levels on the mechanisms adopted by CI and NH listeners in selective auditory attention and how it affects their daily conversation. Nine CI users (five bilateral, three unilateral, and one bimodal) and eight NH listeners participated in this study. The behavioral speech recognition scores were collected using a matrix sentences test and neural tracking to speech envelope was recorded using electroencephalography (EEG). Speech stimuli were presented at three different levels (75, 65, and 55 dB SPL) in the presence of two maskers from three spatially separated speakers. Different combinations of assisted/impaired hearing modes were evaluated for CI users and the outcomes were analyzed in three categories: electric hearing only, acoustic hearing only, and electric+acoustic hearing. Our results showed that increasing the conversational sound level degraded the selective auditory attention in electrical hearing. On the other hand, increasing the sound level improved the selective auditory attention for the acoustic hearing group. In NH listeners, however, increasing the sound level did not cause a significant change in the auditory attention. Our result implies that the effect of the sound level on the selective auditory attention varies depending on hearing modes and the loudness control is necessary for the ease of attending to the conversation by CI users.      
### 34.Stability of Neural Networks on Riemannian Manifolds  [ :arrow_down: ](https://arxiv.org/pdf/2103.02663.pdf)
>  Convolutional Neural Networks (CNNs) have been applied to data with underlying non-Euclidean structures and have achieved impressive successes. This brings the stability analysis of CNNs on non-Euclidean domains into notice because CNNs have been proved stable on Euclidean domains. This paper focuses on the stability of CNNs on Riemannian manifolds. By taking the Laplace-Beltrami operators into consideration, we construct an $\alpha$-frequency difference threshold filter to help separate the spectrum of the operator with an infinite dimensionality. We further construct a manifold neural network architecture with these filters. We prove that both the manifold filters and neural networks are stable under absolute perturbations to the operators. The results also implicate a trade-off between the stability and discriminability of manifold neural networks. Finally we verify our conclusions with numerical experiments in a wireless adhoc network scenario.      
### 35.Semidefinite Programming Two-way TOA Localization for User Devices with Motion and Clock Drift  [ :arrow_down: ](https://arxiv.org/pdf/2103.02635.pdf)
>  In two-way time-of-arrival (TOA) systems, a user device (UD) obtains its position by round-trip communications to a number of anchor nodes (ANs) at known locations. The objective function of the maximum likelihood (ML) method for two-way TOA localization is nonconvex. Thus, the widely-adopted Gauss-Newton iterative method to solve the ML estimator usually suffers from the local minima problem. In this paper, we convert the original estimator into a convex problem by relaxation, and develop a new semidefinite programming (SDP) based localization method for moving UDs, namely SDP-M. Numerical result demonstrates that compared with the iterative method, which often fall into local minima, the SDP-M always converge to the global optimal solution and significantly reduces the localization error by more than 40%. It also has stable localization accuracy regardless of the UD movement, and outperforms the conventional method for stationary UDs, which has larger error with growing UD velocity.      
### 36.Perceiver: General Perception with Iterative Attention  [ :arrow_down: ](https://arxiv.org/pdf/2103.03206.pdf)
>  Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.      
### 37.A Structural Causal Model for MR Images of Multiple Sclerosis  [ :arrow_down: ](https://arxiv.org/pdf/2103.03158.pdf)
>  Precision medicine involves answering counterfactual questions such as "Would this patient respond better to treatment A or treatment B?" These types of questions are causal in nature and require the tools of causal inference to be answered, e.g., with a structural causal model (SCM). In this work, we develop an SCM that models the interaction between demographic information, disease covariates, and magnetic resonance (MR) images of the brain for people with multiple sclerosis (MS). Inference in the SCM generates counterfactual images that show what an MR image of the brain would look like when demographic or disease covariates are changed. These images can be used for modeling disease progression or used for downstream image processing tasks where controlling for confounders is necessary.      
### 38.Error-driven Fixed-Budget ASR Personalization for Accented Speakers  [ :arrow_down: ](https://arxiv.org/pdf/2103.03142.pdf)
>  We consider the task of personalizing ASR models while being constrained by a fixed budget on recording speaker-specific utterances. Given a speaker and an ASR model, we propose a method of identifying sentences for which the speaker's utterances are likely to be harder for the given ASR model to recognize. We assume a tiny amount of speaker-specific data to learn phoneme-level error models which help us select such sentences. We show that speaker's utterances on the sentences selected using our error model indeed have larger error rates when compared to speaker's utterances on randomly selected sentences. We find that fine-tuning the ASR model on the sentence utterances selected with the help of error models yield higher WER improvements in comparison to fine-tuning on an equal number of randomly selected sentence utterances. Thus, our method provides an efficient way of collecting speaker utterances under budget constraints for personalizing ASR models.      
### 39.Optimization-based parametric model order reduction via $\mathcal{H}_2\otimes\mathcal{L}_2$ first-order necessary conditions  [ :arrow_down: ](https://arxiv.org/pdf/2103.03136.pdf)
>  In this paper, we generalize existing frameworks for $\mathcal{H}_2\otimes\mathcal{L}_2$-optimal model order reduction to a broad class of parametric linear time-invariant systems. To this end, we derive first-order necessary ptimality conditions for a class of structured reduced-order models, and then building on those, propose a stability-preserving optimization-based method for computing locally $\mathcal{H}_2\otimes\mathcal{L}_2$-optimal reduced-order models. We also make a theoretical comparison to existing approaches in the literature, and in numerical experiments, show how our new method, with reasonable computational effort, produces stable optimized reduced-order models with significantly lower approximation errors.      
### 40.Learning to run a Power Network Challenge: a Retrospective Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2103.03104.pdf)
>  Power networks, responsible for transporting electricity across large geographical regions, are complex infrastructures on which modern life critically depend. Variations in demand and production profiles, with increasing renewable energy integration, as well as the high voltage network technology, constitute a real challenge for human operators when optimizing electricity transportation while avoiding blackouts. Motivated to investigate the potential of Artificial Intelligence methods in enabling adaptability in power network operation, we have designed a L2RPN challenge to encourage the development of reinforcement learning solutions to key problems present in the next-generation power networks. The NeurIPS 2020 competition was well received by the international community attracting over 300 participants worldwide. The main contribution of this challenge is our proposed comprehensive Grid2Op framework, and associated benchmark, which plays realistic sequential network operations scenarios. The framework is open-sourced and easily re-usable to define new environments with its companion GridAlive ecosystem. It relies on existing non-linear physical simulators and let us create a series of perturbations and challenges that are representative of two important problems: a) the uncertainty resulting from the increased use of unpredictable renewable energy sources, and b) the robustness required with contingent line disconnections. In this paper, we provide details about the competition highlights. We present the benchmark suite and analyse the winning solutions of the challenge, observing one super-human performance demonstration by the best agent. We propose our organizational insights for a successful competition and conclude on open research avenues. We expect our work will foster research to create more sustainable solutions for power network operations.      
### 41.An Overview on Artificial Intelligence Techniques for Diagnosis of Schizophrenia Based on Magnetic Resonance Imaging Modalities: Methods, Challenges, and Future Works  [ :arrow_down: ](https://arxiv.org/pdf/2103.03081.pdf)
>  Schizophrenia (SZ) is a mental disorder that typically emerges in late adolescence or early adulthood. It reduces the life expectancy of patients by 15 years. Abnormal behavior, perception of emotions, social relationships, and reality perception are among its most significant symptoms. Past studies have revealed the temporal and anterior lobes of hippocampus regions of brain get affected by SZ. Also, increased volume of cerebrospinal fluid (CSF) and decreased volume of white and gray matter can be observed due to this disease. The magnetic resonance imaging (MRI) is the popular neuroimaging technique used to explore structural/functional brain abnormalities in SZ disorder owing to its high spatial resolution. Various artificial intelligence (AI) techniques have been employed with advanced image/signal processing methods to obtain accurate diagnosis of SZ. This paper presents a comprehensive overview of studies conducted on automated diagnosis of SZ using MRI modalities. Main findings, various challenges, and future works in developing the automated SZ detection are described in this paper.      
### 42.Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods  [ :arrow_down: ](https://arxiv.org/pdf/2103.03078.pdf)
>  Machine learning (ML) models used in medical imaging diagnostics can be vulnerable to a variety of privacy attacks, including membership inference attacks, that lead to violations of regulations governing the use of medical data and threaten to compromise their effective deployment in the clinic. In contrast to most recent work in privacy-aware ML that has been focused on model alteration and post-processing steps, we propose here a novel and complementary scheme that enhances the security of medical data by controlling the data sharing process. We develop and evaluate a privacy defense protocol based on using a generative adversarial network (GAN) that allows a medical data sourcer (e.g. a hospital) to provide an external agent (a modeler) a proxy dataset synthesized from the original images, so that the resulting diagnostic systems made available to model consumers is rendered resilient to privacy attackers. We validate the proposed method on retinal diagnostics AI used for diabetic retinopathy that bears the risk of possibly leaking private information. To incorporate concerns of both privacy advocates and modelers, we introduce a metric to evaluate privacy and utility performance in combination, and demonstrate, using these novel and classical metrics, that our approach, by itself or in conjunction with other defenses, provides state of the art (SOTA) performance for defending against privacy attacks.      
### 43.Real-Time Navigation System for a Low-Cost Mobile Robot with an RGB-D Camera  [ :arrow_down: ](https://arxiv.org/pdf/2103.03054.pdf)
>  Currently, mobile robots are developing rapidly and are finding numerous applications in industry. However, there remain a number of problems related to their practical use, such as the need for expensive hardware and their high power consumption levels. In this study, we propose a navigation system that is operable on a low-end computer with an RGB-D camera and a mobile robot platform for the operation of an integrated autonomous driving system. The proposed system does not require LiDARs or a GPU. Our raw depth image ground segmentation approach extracts a traversability map for the safe driving of low-body mobile robots. It is designed to guarantee real-time performance on a low-cost commercial single board computer with integrated SLAM, global path planning, and motion planning. Running sensor data processing and other autonomous driving functions simultaneously, our navigation method performs rapidly at a refresh rate of 18Hz for control command, whereas other systems have slower refresh rates. Our method outperforms current state-of-the-art navigation approaches as shown in 3D simulation tests. In addition, we demonstrate the applicability of our mobile robot system through successful autonomous driving in a residential lobby.      
### 44.Reinforcement Learning Trajectory Generation and Control for Aggressive Perching on Vertical Walls with Quadrotors  [ :arrow_down: ](https://arxiv.org/pdf/2103.03011.pdf)
>  Micro aerial vehicles are widely being researched and employed due to their relative low operation costs and high flexibility in various applications. We study the under-actuated quadrotor perching problem, designing a trajectory planner and controller which generates feasible trajectories and drives quadrotors to desired state in state space. This paper proposes a trajectory generating and tracking method for quadrotor perching that takes the advantages of reinforcement learning controller and traditional controller. The trained low-level reinforcement learning controller would manipulate quadrotor toward the perching point in simulation environment. Once the simulated quadrotor has successfully perched, the relative trajectory information in simulation will be sent to tracking controller on real quadrotor and start the actual perching task. Generating feasible trajectories via the trained reinforcement learning controller requires less time, and the traditional trajectory tracking controller could easily be modified to control the quadrotor and mathematically analysis its stability and robustness. We show that this approach permits the control structure of trajectories and controllers enabling such aggressive maneuvers perching on vertical surfaces with high precision.      
### 45.Speech Emotion Recognition using Semantic Information  [ :arrow_down: ](https://arxiv.org/pdf/2103.02993.pdf)
>  Speech emotion recognition is a crucial problem manifesting in a multitude of applications such as human computer interaction and education. Although several advancements have been made in the recent years, especially with the advent of Deep Neural Networks (DNN), most of the studies in the literature fail to consider the semantic information in the speech signal. In this paper, we propose a novel framework that can capture both the semantic and the paralinguistic information in the signal. In particular, our framework is comprised of a semantic feature extractor, that captures the semantic information, and a paralinguistic feature extractor, that captures the paralinguistic information. Both semantic and paraliguistic features are then combined to a unified representation using a novel attention mechanism. The unified feature vector is passed through a LSTM to capture the temporal dynamics in the signal, before the final prediction. To validate the effectiveness of our framework, we use the popular SEWA dataset of the AVEC challenge series and compare with the three winning papers. Our model provides state-of-the-art results in the valence and liking dimensions.      
### 46.Learning-based Adaptive Control via Contraction Theory  [ :arrow_down: ](https://arxiv.org/pdf/2103.02987.pdf)
>  We present a new deep learning-based adaptive control framework for nonlinear systems with multiplicatively-separable parametric uncertainty, called an adaptive Neural Contraction Metric (aNCM). The aNCM uses a neural network model of an optimal adaptive contraction metric, the existence of which guarantees asymptotic stability and exponential boundedness of system trajectories under the parametric uncertainty. In particular, we exploit the concept of a Neural Contraction Metric (NCM) to obtain a nominal provably stable robust control policy for nonlinear systems with bounded disturbances, and combine this policy with a novel adaptation law to achieve stability guarantees. We also show that the framework is applicable to adaptive control of dynamical systems modeled via basis function approximation. Furthermore, the use of neural networks in the aNCM permits its real-time implementation, resulting in broad applicability to a variety of systems. Its superiority to the state-of-the-art is illustrated with a simple cart-pole balancing task.      
### 47.Towards Ultrafast MRI via Extreme k-Space Undersampling and Superresolution  [ :arrow_down: ](https://arxiv.org/pdf/2103.02940.pdf)
>  We went below the MRI acceleration factors (a.k.a., k-space undersampling) reported by all published papers that reference the original fastMRI challenge, and then considered powerful deep learning based image enhancement methods to compensate for the underresolved images. We thoroughly study the influence of the sampling patterns, the undersampling and the downscaling factors, as well as the recovery models on the final image quality for both the brain and the knee fastMRI benchmarks. The quality of the reconstructed images surpasses that of the other methods, yielding an MSE of 0.00114, a PSNR of 29.6 dB, and an SSIM of 0.956 at x16 acceleration factor. More extreme undersampling factors of x32 and x64 are also investigated, holding promise for certain clinical applications such as computer-assisted surgery or radiation planning. We survey 5 expert radiologists to assess 100 pairs of images and show that the recovered undersampled images statistically preserve their diagnostic value.      
### 48.FootApp: an AI-Powered System for Football Match Annotation  [ :arrow_down: ](https://arxiv.org/pdf/2103.02938.pdf)
>  In the last years, scientific and industrial research has experienced a growing interest in acquiring large annotated data sets to train artificial intelligence algorithms for tackling problems in different domains. In this context, we have observed that even the market for football data has substantially grown. The analysis of football matches relies on the annotation of both individual players' and team actions, as well as the athletic performance of players. Consequently, annotating football events at a fine-grained level is a very expensive and error-prone task. Most existing semi-automatic tools for football match annotation rely on cameras and computer vision. However, those tools fall short in capturing team dynamics, and in extracting data of players who are not visible in the camera frame. To address these issues, in this manuscript we present FootApp, an AI-based system for football match annotation. First, our system relies on an advanced and mixed user interface that exploits both vocal and touch interaction. Second, the motor performance of players is captured and processed by applying machine learning algorithms to data collected from inertial sensors worn by players. Artificial intelligence techniques are then used to check the consistency of generated labels, including those regarding the physical activity of players, to automatically recognize annotation errors. Notably, we implemented a full prototype of the proposed system, performing experiments to show its effectiveness in a real-world adoption scenario.      
### 49.The Bounded Acceleration Shortest Path problem: complexity and solution algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2103.02914.pdf)
>  The purpose of this work is to introduce and characterize the Bounded Acceleration Shortest Path (BASP) problem, a generalization of the Shortest Path (SP) problem. This problem is associated to a graph: the nodes represent positions of a mobile vehicle and the arcs are associated to pre-assigned geometric paths that connect these positions. BASP consists in finding the minimum-time path between two nodes. Differently from SP, we require that the vehicle satisfy bounds on maximum and minimum acceleration and speed, that depend on the vehicle position on the currently traveled arc. We prove that BASP is NP-hard and define solution algorithm that achieves polynomial time-complexity under some additional hypotheses on problem data.      
### 50.Probabilistic stabilizability certificates for a class of black-box linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.02905.pdf)
>  We provide out-of-sample certificates on the controlled invariance property of a given set with respect to a class of black-box linear systems. Specifically, we consider linear time-invariant models whose state space matrices are known only to belong to a certain family due to a possibly inexact quantification of some parameters. By exploiting a set of realizations of those undetermined parameters, verifying the controlled invariance property of the given set amounts to a linear program, whose feasibility allows us to establish an a-posteriori probabilistic certificate on the controlled invariance property of such a set with respect to the nominal linear time-invariant dynamics. The proposed framework is applied to the control of a networked multi-agent system with unknown weighted graph.      
### 51.Mask DnGAN: Multi-Stage Raw Video Denoising with Adversarial Loss and Gradient Mask  [ :arrow_down: ](https://arxiv.org/pdf/2103.02861.pdf)
>  In this paper, we propose a learning-based approach for denoising raw videos captured under low lighting conditions. We propose to do this by first explicitly aligning the neighboring frames to the current frame using a convolutional neural network (CNN). We then fuse the registered frames using another CNN to obtain the final denoised frame. To avoid directly aligning the temporally distant frames, we perform the two processes of alignment and fusion in multiple stages. Specifically, at each stage, we perform the denoising process on three consecutive input frames to generate the intermediate denoised frames which are then passed as the input to the next stage. By performing the process in multiple stages, we can effectively utilize the information of neighboring frames without directly aligning the temporally distant frames. We train our multi-stage system using an adversarial loss with a conditional discriminator. Specifically, we condition the discriminator on a soft gradient mask to prevent introducing high-frequency artifacts in smooth regions. We show that our system is able to produce temporally coherent videos with realistic details. Furthermore, we demonstrate through extensive experiments that our approach outperforms state-of-the-art image and video denoising methods both numerically and visually.      
### 52.Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception  [ :arrow_down: ](https://arxiv.org/pdf/2103.02850.pdf)
>  Distortion quantification of point clouds plays a stealth, yet vital role in a wide range of human and machine perception tasks. For human perception tasks, a distortion quantification can substitute subjective experiments to guide 3D visualization; while for machine perception tasks, a distortion quantification can work as a loss function to guide the training of deep neural networks for unsupervised learning tasks. To handle a variety of demands in many applications, a distortion quantification needs to be distortion discriminable, differentiable, and have a low computational complexity. Currently, however, there is a lack of a general distortion quantification that can satisfy all three conditions. To fill this gap, this work proposes multiscale potential energy discrepancy (MPED), a distortion quantification to measure point cloud geometry and color difference. By evaluating at various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. Extensive experimental studies validate MPED's superiority for both human and machine perception tasks.      
### 53.Arraymetrics: Authentication Through Chaotic Antenna Array Geometries  [ :arrow_down: ](https://arxiv.org/pdf/2103.02841.pdf)
>  Advances in computing have resulted in an emerging need for multi-factor authentication using an amalgamation of cryptographic and physical keys. This letter presents a novel authentication approach using a combination of signal and antenna activation sequences, and most importantly, perturbed antenna array geometries. Possible degrees of freedom in perturbing antenna array geometries affected physical properties and their detection are presented. Channel estimation for the plurality of validly authorized arrays is discussed. Accuracy is investigated as a function of signal-to-noise ratio (SNR) and number of authorized arrays. It is observed that the proposed authentication scheme can provide 1% false authentication rate at 10 dB SNR, while it is achieving less than 1% missed authentication rates.      
### 54.STEP: Stochastic Traversability Evaluation and Planning for Safe Off-road Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2103.02828.pdf)
>  Although ground robotic autonomy has gained widespread usage in structured and controlled environments, autonomy in unknown and off-road terrain remains a difficult problem. Extreme, off-road, and unstructured environments such as undeveloped wilderness, caves, and rubble pose unique and challenging problems for autonomous navigation. To tackle these problems we propose an approach for assessing traversability and planning a safe, feasible, and fast trajectory in real-time. Our approach, which we name STEP (Stochastic Traversability Evaluation and Planning), relies on: 1) rapid uncertainty-aware mapping and traversability evaluation, 2) tail risk assessment using the Conditional Value-at-Risk (CVaR), and 3) efficient risk and constraint-aware kinodynamic motion planning using sequential quadratic programming-based (SQP) model predictive control (MPC). We analyze our method in simulation and validate its efficacy on wheeled and legged robotic platforms exploring extreme terrains including an underground lava tube.      
### 55.Convergence Analysis of Dual Decomposition Algorithm in Distributed Optimization: Asynchrony and Inexactness  [ :arrow_down: ](https://arxiv.org/pdf/2103.02784.pdf)
>  Dual decomposition is widely utilized in distributed optimization of multi-agent systems. In practice, the dual decomposition algorithm is desired to admit an asynchronous implementation due to imperfect communication, such as time delay and packet drop. In addition, computational errors also exist when individual agents solve their own subproblems. In this paper, we analyze the convergence of the dual decomposition algorithm in distributed optimization when both the asynchrony in communication and the inexactness in solving subproblems exist. We find that the interaction between asynchrony and inexactness slows down the convergence rate from $\mathcal{O} ( 1 / k )$ to $\mathcal{O} ( 1 / \sqrt{k} )$. Specifically, with a constant step size, the value of objective function converges to a neighborhood of the optimal value, and the solution converges to a neighborhood of the exact optimal solution. Moreover, the violation of the constraints diminishes in $\mathcal{O} ( 1 / \sqrt{k} )$. Our result generalizes and unifies the existing ones that only consider either asynchrony or inexactness. Finally, numerical simulations validate the theoretical results.      
### 56.Skyrmion Logic Clocked via Voltage Controlled Magnetic Anisotropy  [ :arrow_down: ](https://arxiv.org/pdf/2103.02724.pdf)
>  Magnetic skyrmions are exciting candidates for energy-efficient computing due to their non-volatility, detectability,and mobility. A recent proposal within the paradigm of reversible computing enables large-scale circuits composed ofdirectly-cascaded skyrmion logic gates, but it is limited by the manufacturing difficulty and energy costs associated withthe use of notches for skyrmion synchronization. To overcome these challenges, we therefore propose a skyrmion logicsynchronized via modulation of voltage-controlled magnetic anisotropy (VCMA). In addition to demonstrating theprinciple of VCMA synchronization through micromagnetic simulations, we also quantify the impacts of current den-sity, skyrmion velocity, and anisotropy barrier height on skyrmion motion. Further micromagnetic results demonstratethe feasibility of cascaded logic circuits in which VCMA synchronizers enable clocking and pipelining, illustrating afeasible pathway toward energy-efficient large-scale computing systems based on magnetic skyrmions.      
### 57.A Robust Adversarial Network-Based End-to-End Communications System With Strong Generalization Ability Against Adversarial Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2103.02654.pdf)
>  We propose a novel defensive mechanism based on a generative adversarial network (GAN) framework to defend against adversarial attacks in end-to-end communications systems. Specifically, we utilize a generative network to model a powerful adversary and enable the end-to-end communications system to combat the generative attack network via a minimax game. We show that the proposed system not only works well against white-box and black-box adversarial attacks but also possesses excellent generalization capabilities to maintain good performance under no attacks. We also show that our GAN-based end-to-end system outperforms the conventional communications system and the end-to-end communications system with/without adversarial training.      
### 58.Experimental Body-input Three-stage DC offset Calibration Scheme for Memristive Crossbar  [ :arrow_down: ](https://arxiv.org/pdf/2103.02651.pdf)
>  Reading several ReRAMs simultaneously in a neuromorphic circuit increases power consumption and limits scalability. Applying small inference read pulses is a vain attempt when offset voltages of the read-out circuit are decisively more. This paper presents an experimental validation of a three-stage calibration scheme to calibrate the DC offset voltage across the rows of the memristive crossbar. The proposed method is based on biasing the body terminal of one of the differential pair MOSFETs of the buffer through a series of cascaded resistor banks arranged in three stages: coarse, fine and finer stages. The circuit is designed in a 130 nm CMOS technology, where the OxRAM-based binary memristors are built on top of it. A dedicated PCB and other auxiliary boards have been designed for testing the chip. Experimental results validate the presented approach, which is only limited by mismatch and electrical noise.      
### 59.Compute and memory efficient universal sound source separation  [ :arrow_down: ](https://arxiv.org/pdf/2103.02644.pdf)
>  Recent progress in audio source separation lead by deep learning has enabled many neural network models to provide robust solutions to this fundamental estimation problem. In this study, we provide a family of efficient neural network architectures for general purpose audio source separation while focusing on multiple computational aspects that hinder the application of neural networks in real-world scenarios. The backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. This mechanism enables our models to obtain high fidelity signal separation in a wide variety of settings where variable number of sources are present and with limited computational resources (e.g. floating point operations, memory footprint, number of parameters and latency). Our experiments show that SuDoRM-RF models perform comparably and even surpass several state-of-the-art benchmarks with significantly higher computational resource requirements. The causal variation of SuDoRM-RF is able to obtain competitive performance in real-time speech separation of around 10dB scale-invariant signal-to-distortion ratio improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a laptop device.      
### 60.Multiple Video Frame Interpolation via Enhanced Deformable Separable Convolution  [ :arrow_down: ](https://arxiv.org/pdf/2006.08070.pdf)
>  Generating non-existing frames from a consecutive video sequence has been an interesting and challenging problem in the video processing field. Typical kernel-based interpolation methods predict pixels with a single convolution process that convolves source frames with spatially adaptive local kernels, which circumvents the time-consuming, explicit motion estimation in the form of optical flow. However, when scene motion is larger than the pre-defined kernel size, these methods are prone to yield less plausible results. In addition, they cannot directly generate a frame at an arbitrary temporal position because the learned kernels are tied to the midpoint in time between the input frames. In this paper, we try to solve these problems and propose a novel non-flow kernel-based approach that we refer to as enhanced deformable separable convolution (EDSC) to estimate not only adaptive kernels, but also offsets, masks and biases to make the network obtain information from non-local neighborhood. During the learning process, different intermediate time step can be involved as a control variable by means of an extension of coord-conv trick, allowing the estimated components to vary with different input temporal information. This makes our method capable to produce multiple in-between frames. Furthermore, we investigate the relationships between our method and other typical kernel- and flow-based methods. Experimental results show that our method performs favorably against the state-of-the-art methods across a broad range of datasets. Code will be publicly available on URL: \url{<a class="link-external link-https" href="https://github.com/Xianhang/EDSC-pytorch" rel="external noopener nofollow">this https URL</a>}.      
### 61.Towards Mesh Saliency Detection in 6 Degrees of Freedom  [ :arrow_down: ](https://arxiv.org/pdf/2005.13127.pdf)
>  Traditional 3D mesh saliency detection algorithms and corresponding databases were proposed under several constraints such as providing limited viewing directions and not taking the subject's movement into consideration. In this work, a novel 6DoF mesh saliency database is developed which provides both the subject's 6DoF data and eye-movement data. Different from traditional databases, subjects in the experiment are allowed to move freely to observe 3D meshes in a virtual reality environment. Based on the database, we first analyze the inter-observer variation and the influence of viewing direction towards subject's visual attention, then we provide further investigations about the subject's visual attention bias during observation. Furthermore, we propose a 6DoF mesh saliency detection algorithm based on the uniqueness measure and the bias preference. To evaluate the proposed approach, we also design an evaluation metric accordingly which takes the 6DoF information into consideration, and extend some state-of-the-art 3D saliency detection methods to make comparisons. The experimental results demonstrate the superior performance of our approach for 6DoF mesh saliency detection, in addition to providing benchmarks for the presented 6DoF mesh saliency database. The database and the corresponding algorithms will be made publicly available for research purposes.      
