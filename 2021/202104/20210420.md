# ArXiv eess --Tue, 20 Apr 2021
### 1.NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2104.09494.pdf)
>  In this paper, we present an update to the NISQA speech quality prediction model that is focused on distortions that occur in communication networks. In contrast to the previous version, the model is trained end-to-end and the time-dependency modelling and time-pooling is achieved through a Self-Attention mechanism. Besides overall speech quality, the model also predicts the four speech quality dimensions Noisiness, Coloration, Discontinuity, and Loudness, and in this way gives more insight into the cause of a quality degradation. Furthermore, new datasets with over 13,000 speech files were created for training and validation of the model. The model was finally tested on a new, live-talking test dataset that contains recordings of real telephone calls. Overall, NISQA was trained and evaluated on 81 datasets from different sources and showed to provide reliable predictions also for unknown speech samples. The code, model weights, and datasets are open-sourced.      
### 2.Oracle-based economic predictive control  [ :arrow_down: ](https://arxiv.org/pdf/2104.09490.pdf)
>  This paper presents an economic model predictive controller, under the assumption that the only measurable signal of the plant is the economic cost to be minimized. In order to forecast the evolution of this economic cost for a given input trajectory, a prediction model with a NARX structure, the so-called oracle, is proposed. Sufficient conditions to ensure the existence of such oracle are studied, proving that it can be derived for a general nonlinear system if the economic cost function is a Morse function. Based on this oracle, economic model predictive controllers are proposed, and their stability is demonstrated in nominal conditions under a standard dissipativity assumption. The viability of these controllers in practical settings (where the oracle may provide imperfect predictions for generic inputs) is proven by means of input-to-state stability. These properties have been illustrated in a case study based on a continuously stirred tank reactor.      
### 3.Fusing information streams in end-to-end audio-visual speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2104.09482.pdf)
>  End-to-end acoustic speech recognition has quickly gained widespread popularity and shows promising results in many studies. Specifically the joint transformer/CTC model provides very good performance in many tasks. However, under noisy and distorted conditions, the performance still degrades notably. While audio-visual speech recognition can significantly improve the recognition rate of end-to-end models in such poor conditions, it is not obvious how to best utilize any available information on acoustic and visual signal quality and reliability in these models. We thus consider the question of how to optimally inform the transformer/CTC model of any time-variant reliability of the acoustic and visual information streams. We propose a new fusion strategy, incorporating reliability information in a decision fusion net that considers the temporal effects of the attention mechanism. This approach yields significant improvements compared to a state-of-the-art baseline model on the Lip Reading Sentences 2 and 3 (LRS2 and LRS3) corpus. On average, the new system achieves a relative word error rate reduction of 43% compared to the audio-only setup and 31% compared to the audiovisual end-to-end baseline.      
### 4.FPGA Implementations of Layered MinSum LDPC Decoders Using RCQ Message Passing  [ :arrow_down: ](https://arxiv.org/pdf/2104.09480.pdf)
>  Non-uniform message quantization techniques such as reconstruction-computation-quantization (RCQ) improve error-correction performance and decrease hardware complexity of low-density parity-check (LDPC) decoders that use a flooding schedule. Layered MinSum RCQ (L-msRCQ) enables message quantization to be utilized for layered decoders and irregular LDPC codes. We investigate field-programmable gate array (FPGA) implementations of L-msRCQ decoders. Three design methods for message quantization are presented, which we name the Lookup, Broadcast, and Dribble methods. The decoding performance and hardware complexity of these schemes are compared to a layered offset MinSum (OMS) decoder. Simulation results on a (16384, 8192) protograph-based raptor-like (PBRL) LDPC code show that a 4-bit L-msRCQ decoder using the Broadcast method can achieve a 0.03 dB improvement in error-correction performance while using 12% fewer registers than the OMS decoder. A Broadcast-based 3-bit L-msRCQ decoder uses 15% fewer lookup tables, 18% fewer registers, and 13% fewer routed nets than the OMS decoder, but results in a 0.09 dB loss in performance.      
### 5.Self-supervised Representation Learning With Path Integral Clustering For Speaker Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2104.09456.pdf)
>  Automatic speaker diarization techniques typically involve a two-stage processing approach where audio segments of fixed duration are converted to vector representations in the first stage. This is followed by an unsupervised clustering of the representations in the second stage. In most of the prior approaches, these two stages are performed in an isolated manner with independent optimization steps. In this paper, we propose a representation learning and clustering algorithm that can be iteratively performed for improved speaker diarization. The representation learning is based on principles of self-supervised learning while the clustering algorithm is a graph structural method based on path integral clustering (PIC). The representation learning step uses the cluster targets from PIC and the clustering step is performed on embeddings learned from the self-supervised deep model. This iterative approach is referred to as self-supervised clustering (SSC). The diarization experiments are performed on CALLHOME and AMI meeting datasets. In these experiments, we show that the SSC algorithm improves significantly over the baseline system (relative improvements of 13% and 59% on CALLHOME and AMI datasets respectively in terms of diarization error rate (DER)). In addition, the DER results reported in this work improve over several other recent approaches for speaker diarization.      
### 6.Continual Learning in Sensor-based Human Activity Recognition: an Empirical Benchmark Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2104.09396.pdf)
>  Sensor-based human activity recognition (HAR), i.e., the ability to discover human daily activity patterns from wearable or embedded sensors, is a key enabler for many real-world applications in smart homes, personal healthcare, and urban planning. However, with an increasing number of applications being deployed, an important question arises: how can a HAR system autonomously learn new activities over a long period of time without being re-engineered from scratch? This problem is known as continual learning and has been particularly popular in the domain of computer vision, where several techniques to attack it have been developed. This paper aims to assess to what extent such continual learning techniques can be applied to the HAR domain. To this end, we propose a general framework to evaluate the performance of such techniques on various types of commonly used HAR datasets. We then present a comprehensive empirical analysis of their computational cost and effectiveness of tackling HAR-specific challenges (i.e., sensor noise and labels' scarcity). The presented results uncover useful insights on their applicability and suggest future research directions for HAR systems. Our code, models and data are available at <a class="link-external link-https" href="https://github.com/srvCodes/continual-learning-benchmark" rel="external noopener nofollow">this https URL</a>.      
### 7.Voltage Collapse Stabilization in Star DC Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.09381.pdf)
>  Voltage collapse is a type of blackout-inducing dynamic instability that occurs when power demand exceeds the maximum power that can be transferred through a network. The traditional (preventive) approach to avoid voltage collapse is based on ensuring that the network never reaches its maximum capacity. However, such an approach leads to inefficient use of network resources and does not account for unforeseen events. To overcome this limitation, this paper seeks to initiate the study of voltage collapse stabilization, i.e., the design of load controllers aimed at stabilizing the point of voltage collapse. We formulate the problem of voltage stability for a star direct current network as a dynamic problem where each load seeks to achieve a constant power consumption by updating its conductance as the voltage changes. We introduce a voltage collapse stabilization controller and show that the high-voltage equilibrium is stabilized. More importantly, we are able to achieve proportional load shedding under extreme loading conditions. We further highlight the key features of our controller using numerical illustrations.      
### 8.My Experience in Physical Layer Communications  [ :arrow_down: ](https://arxiv.org/pdf/2104.09373.pdf)
>  I feel that I have been very lucky since I have experienced the most dynamic 30 years on electronics in the past. I think that the most visible change in our daily life over the past 30 years is communications. From computer modems, to internet, and to smart phones, people now feel much less lonely or bored since they are always connected. In this article, I would like to share with you on my own experience working on communications in the past decades.      
### 9.Detecting cognitive decline using speech only: The ADReSSo Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2104.09356.pdf)
>  Building on the success of the ADReSS Challenge at Interspeech 2020, which attracted the participation of 34 teams from across the world, the ADReSSo Challenge targets three difficult automatic prediction problems of societal and medical relevance, namely: detection of Alzheimer's Dementia, inference of cognitive testing scores, and prediction of cognitive decline. This paper presents these prediction tasks in detail, describes the datasets used, and reports the results of the baseline classification and regression models we developed for each task. A combination of acoustic and linguistic features extracted directly from audio recordings, without human intervention, yielded a baseline accuracy of 78.87% for the AD classification task, an MMSE prediction root mean squared (RMSE) error of 5.28, and 68.75% accuracy for the cognitive decline prediction task.      
### 10.Machine-Learning Classification of Closed and Open Radiating Wires from Near Magnetic or Electric Field Scan Images  [ :arrow_down: ](https://arxiv.org/pdf/2104.09277.pdf)
>  Sets of intelligent classifiers are applied to the near-field scan-data in order to automatically classify the shape of radiating wirings. The support vector machine, k-nearest neighbors algorithm, and Gaussian process classifications are trained using the near-field radiation pattern of diverse radiating wire configurations. Leave-one-out cross-validation is used for estimating the performance of the predictive models. The output of this research is a software package well-suited to be retrained based on any measured near-field databank to automate the identification of magnetic-type or electric-type of the radiating coupling sources.      
### 11.Fitbeat: COVID-19 Estimation based on Wristband Heart Rate  [ :arrow_down: ](https://arxiv.org/pdf/2104.09263.pdf)
>  This study investigates the potential of deep learning methods to identify individuals with suspected COVID-19 infection using remotely collected heart-rate data. The study utilises data from the ongoing EU IMI RADAR-CNS research project that is investigating the feasibility of wearable devices and smart phones to monitor individuals with multiple sclerosis (MS), depression or epilepsy. Aspart of the project protocol, heart-rate data was collected from participants using a Fitbit wristband. The presence of COVID-19 in the cohort in this work was either confirmed through a positive swab test, or inferred through the self-reporting of a combination of symptoms including fever, respiratory symptoms, loss of smell or taste, tiredness and gastrointestinal symptoms. Experimental results indicate that our proposed contrastive convolutional auto-encoder (contrastive CAE), i. e., a combined architecture of an auto-encoder and contrastive loss, outperforms a conventional convolutional neural network (CNN), as well as a convolutional auto-encoder (CAE) without using contrastive loss. Our final contrastive CAE achieves 95.3% unweighted average recall, 86.4% precision, anF1 measure of 88.2%, a sensitivity of 100% and a specificity of 90.6% on a testset of 19 participants with MS who reported symptoms of COVID-19. Each of these participants was paired with a participant with MS with no COVID-19 symptoms.      
### 12.Resilient Distributed Self-Triggered Control of Networked Systems under Hybrid DoS Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2104.09250.pdf)
>  This paper addresses a consensus control problem for networked systems subject to hybrid denial of service (DoS) attacks, which could simultaneously act on measurement, communication and control actuation channels. A novel notation, Persistency-of-Data-Flow (PoDF), is proposed to characterise all these potential vulnerabilities. Then, a distributed resilient control scheme is proposed in line with an edge-based self-triggered framework. Under such self-triggered framework and PoDF, the global consensus of the networked control systems in the presence of hybrid DoS attacks is proved based on the worst effects of the attack, and the bounded convergence time is derived analytically. To mitigate the conservativeness introduced by the global worst case analysis, a self-adaptive scheme is designed from a local perspective. Finally, the effectiveness of the proposed distributed self-triggered hybrid-DoS resilient control is verified by numerical simulations, and a case study with regard to the power network is carried out for further validation.      
### 13.An Unsupervised Learning-Based Approach for Symbol-Level-Precoding  [ :arrow_down: ](https://arxiv.org/pdf/2104.09214.pdf)
>  This paper proposes an unsupervised learning-based precoding framework that trains deep neural networks (DNNs) with no target labels by unfolding an interior point method (IPM) proximal `log' barrier function. The proximal `log' barrier function is derived from the strict power minimization formulation subject to signal-to-interference-plus-noise ratio (SINR) constraint. The proposed scheme exploits the known interference via symbol-level precoding (SLP) to minimize the transmit power and is named strict Symbol-Level-Precoding deep network (SLP-SDNet). The results show that SLP-SDNet outperforms the conventional block-level-precoding (Conventional BLP) scheme while achieving near-optimal performance faster than the SLP optimization-based approach      
### 14.A Survey on Millimeter-Wave Beamforming Enabled UAV Communications and Networking  [ :arrow_down: ](https://arxiv.org/pdf/2104.09204.pdf)
>  Unmanned aerial vehicles (UAVs) have found widespread commercial, civilian, and military applications. Wireless communication has always been one of the core technologies for UAV. However, the communication capacity is becoming a bottleneck for UAV to support more challenging application scenarios. The heavily-occupied sub-6 GHz frequency band is not sufficient to meet the ultra high-data-traffic requirements. The utilization of the millimeter-wave (mmWave) frequency bands is a promising direction for UAV communications, where large antenna arrays can be packed in a small area on the UAV to perform three-dimensional (3D) beamforming. On the other hand, UAVs serving as aerial access points or relays can significantly enhance the coverage and quality of service of the terrestrial mmWave cellular networks. In this paper, we provide a comprehensive survey on mmWave beamforming enabled UAV communications and networking. The technical potential of and challenges for mmWave-UAV communications are presented first. Then, we provide an overview on relevant mmWave antenna structures and channel modeling. Subsequently, the technologies and solutions for UAV-connected mmWave cellular networks and mmWave-UAV ad hoc networks are reviewed, respectively. Finally, we present open issues and promising directions for future research in mmWave beamforming enabled UAV communications and networking.      
### 15.SPSA-Based Successive Beamforming for Mobile Satellite Receivers with Phased Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2104.09199.pdf)
>  Efficient and low-complexity beamforming design is an important element of satellite communication systems with mobile receivers equipped with phased arrays. In this work, we apply the simultaneous perturbation stochastic approximation (SPSA) method with successive sub-array selection for finding the optimal antenna weights that maximize the received signal power at a uniform plane array (UPA). The proposed algorithms are based on iterative gradient approximation by injecting some carefully designed perturbations on the parameters to be estimated. Additionally, the successive sub-array selection technique enhances the performance of SPSA-based algorithms and makes them less sensitive to the initial beam direction. Simulation results show that our proposed algorithms can achieve efficient and reliable performance even when the initial beam direction is not well aligned with the satellite direction.      
### 16.Research on Resource Allocation for Efficient Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.09177.pdf)
>  As a promising solution to achieve efficient learning among isolated data owners and solve data privacy issues, federated learning is receiving wide attention. Using the edge server as an intermediary can effectively collect sensor data, perform local model training, and upload model parameters for global aggregation. So this paper proposes a new framework for resource allocation in a hierarchical network supported by edge computing. In this framework, we minimize the weighted sum of system cost and learning cost by optimizing bandwidth, computing frequency, power allocation and subcarrier assignment. To solve this challenging mixed-integer non-linear problem, we first decouple the bandwidth optimization problem(P1) from the whole problem and obtain a closed-form solution. The remaining computational frequency, power, and subcarrier joint optimization problem(P2) can be further decomposed into two sub-problems: latency and computational frequency optimization problem(P3) and transmission power and subcarrier optimization problem(P4). P3 is a convex optimization problem that is easy to solve. In the joint optimization problem(P4), the optimal power under each subcarrier selection can be obtained first through the successive convex approximation(SCA) algorithm. Substituting the optimal power value obtained back to P4, the subproblem can be regarded as an assignment problem, so the Hungarian algorithm can be effectively used to solve it. The solution of problem P2 is accomplished by solving P3 and P4 iteratively. To verify the performance of the algorithm, we compare the proposed algorithm with five algorithms; namely Equal bandwidth allocation, Learning cost guaranteed, Greedy subcarrier allocation, System cost guaranteed and Time-biased algorithm. Numerical results show the significant performance gain and the robustness of the proposed algorithm in the face of parameter changes.      
### 17.How Does the Vulnerability of an Evolving Power Grid Change?  [ :arrow_down: ](https://arxiv.org/pdf/2104.09080.pdf)
>  In this paper the seven-decade historical dataset of the Hungarian power system is used to perform vulnerability assessment applying a complex network approach.      
### 18.Energy Efficiency Optimization in Radar-Communication Spectrum Sharing  [ :arrow_down: ](https://arxiv.org/pdf/2104.09074.pdf)
>  Energy efficiency, possibly coupled with cognition-based and spectrum-sharing architectures, is a key enabling technology for green communications in 5G-and-beyond standards. In this context, the present paper considers a multiple-input multiple-output communication system cooperatively coexisting with a surveillance radar: the objective function is the communication system energy efficiency, while radar operation is safeguarded by constraining the minimum received signal-to-disturbance ratio for a set of range-azimuth cells of the controlled scene, and no time synchronization between them is assumed. The degrees of freedom are the transmit powers of both systems, the space-time communication codebook and the linear filters at the radar receiver. The resulting optimization problem is non-convex, due to both the objective function and the presence of signal-dependent interference (clutter): we develop a block-coordinate-ascent approximate solution, and offer a thorough performance assessment, so as to elicit the merits of the proposed approach, along with the interplay among the achievable energy efficiency, the density of scatterers in the environment, and the size of the set of protected radar cells.      
### 19.Automatic Stroke Classification of Tabla Accompaniment in Hindustani Vocal Concert Audio  [ :arrow_down: ](https://arxiv.org/pdf/2104.09064.pdf)
>  The tabla is a unique percussion instrument due to the combined harmonic and percussive nature of its timbre, and the contrasting harmonic frequency ranges of its two drums. This allows a tabla player to uniquely emphasize parts of the rhythmic cycle (theka) in order to mark the salient positions. An analysis of the loudness dynamics and timing deviations at various cycle positions is an important part of musicological studies on the expressivity in tabla accompaniment. To achieve this at a corpus-level, and not restrict it to the few recordings that manual annotation can afford, it is helpful to have access to an automatic tabla transcription system. Although a few systems have been built by training models on labeled tabla strokes, the achieved accuracy does not necessarily carry over to unseen instruments. In this article, we report our work towards building an instrument-independent stroke classification system for accompaniment tabla based on the more easily available tabla solo audio tracks. We present acoustic features that capture the distinctive characteristics of tabla strokes and build an automatic system to predict the label as one of a reduced, but musicologically motivated, target set of four stroke categories. To address the lack of sufficient labeled training data, we turn to common data augmentation methods and find the use of pitch-shifting based augmentation to be most promising. We then analyse the important features and highlight the problem of their instrument-dependence while motivating the use of more task-specific data augmentation strategies to improve the diversity of training data.      
### 20.Additive Networks of Chen-Fliess Series: Local Convergence and Relative Degree  [ :arrow_down: ](https://arxiv.org/pdf/2104.08950.pdf)
>  Given an additive network of input-output systems where each node of the network is modeled by a locally convergent Chen-Fliess series, two basic properties of the network are established. First, it is shown that every input-output map between a given pair of nodes has a locally convergent Chen-Fliess series representation. Second, sufficient conditions are given under which the input-output map between a pair of nodes has a well defined relative degree as defined by its generating series. This analysis leads to the conclusion that this relative degree property is generic in a certain sense.      
### 21.DW-GAN: A Discrete Wavelet Transform GAN for NonHomogeneous Dehazing  [ :arrow_down: ](https://arxiv.org/pdf/2104.08911.pdf)
>  Hazy images are often subject to color distortion, blurring, and other visible quality degradation. Some existing CNN-based methods have great performance on removing homogeneous haze, but they are not robust in non-homogeneous case. The reasons are mainly in two folds. Firstly, due to the complicated haze distribution, texture details are easy to be lost during the dehazing process. Secondly, since the training pairs are hard to be collected, training on limited data can easily lead to over-fitting problem. To tackle these two issues, we introduce a novel dehazing network using 2D discrete wavelet transform, namely DW-GAN. Specifically, we propose a two-branch network to deal with the aforementioned problems. By utilizing wavelet transform in DWT branch, our proposed method can retain more high-frequency knowledge in feature maps. In order to prevent over-fitting, ImageNet pre-trained Res2Net is adopted in the knowledge adaptation branch. Owing to the robust feature representations of ImageNet pre-training, the generalization ability of our network is improved dramatically. Finally, a patch-based discriminator is used to reduce artifacts of the restored images. Extensive experimental results demonstrate that the proposed method outperforms the state-of-the-arts quantitatively and qualitatively.      
### 22.A Two-branch Neural Network for Non-homogeneous Dehazing via Ensemble Learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.08902.pdf)
>  Recently, there has been rapid and significant progress on image dehazing. Many deep learning based methods have shown their superb performance in handling homogeneous dehazing problems. However, we observe that even if a carefully designed convolutional neural network (CNN) can perform well on large-scaled dehazing benchmarks, the network usually fails on the non-homogeneous dehazing datasets introduced by NTIRE challenges. The reasons are mainly in two folds. Firstly, due to its non-homogeneous nature, the non-uniformly distributed haze is harder to be removed than the homogeneous haze. Secondly, the research challenge only provides limited data (there are only 25 training pairs in NH-Haze 2021 dataset). Thus, learning the mapping from the domain of hazy images to that of clear ones based on very limited data is extremely hard. To this end, we propose a simple but effective approach for non-homogeneous dehazing via ensemble learning. To be specific, we introduce a two-branch neural network to separately deal with the aforementioned problems and then map their distinct features by a learnable fusion tail. We show extensive experimental results to illustrate the effectiveness of our proposed method.      
### 23.Internet of Fly Things For Post-Disaster Recovery Based on Multi-environment  [ :arrow_down: ](https://arxiv.org/pdf/2104.08892.pdf)
>  Natural disasters such as floods and earthquakes immensely impact the telecommunication network infrastructure, leading to the malfunctioning and interruption of wireless services. Consequently, the user devices under the disaster zone are unable to access the cellular base stations. Wireless coverage on an unmanned aerial vehicle (UAV) is considered for providing coverage service to ground user devices in disaster events. This work evaluated the efficient performance of wireless coverage services of UAVs to provide the internet to fly things to help recover the communications link in a natural disaster in multi environments. The results demonstrate the line of sight, nonline of sight, path loss, and coverage probability for the radio propagation environment scenario. Therefore, the path loss and coverage probability are affected by the user devices' elevation angle and distance in the multi-environment system. The user position's optimum user device distance and elevation angle are also investigated to improve the coverage probability, which could be especially useful for the UAV deployment design.      
### 24.Grid Monitoring for Efficient Flexibility Provision in Distribution Grids  [ :arrow_down: ](https://arxiv.org/pdf/2104.08880.pdf)
>  The increased flexibility requirement needs a flexibility market at the distribution grid level operated by the distribution system operators (DSOs) to resolve challenges to ensure secure operation and, to integrate new renewable productions or loads in the grid. Therefore, the network visibility and monitoring are paramount to distribution network operation. This paper presents a methodology demonstrating the value distribution grid monitoring can bring for the realisation of a local flexibility market. This paper further illustrates the reduction of costs of operation of a DSO with the help of grid monitoring and local flexibility market while maintaining a secure and reliable operation. The methodology has been applied on a real 35 node MV network of a Swiss DSO with several GridEye measurement devices. The performance in terms of losses and voltage and flow violation costs is compared with sub-optimal operation without monitoring.      
### 25.Invariant Subspace Approach to Boolean (Control) Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.08837.pdf)
>  A logical function can be used to characterizing a property of a state of Boolean network (BN), which is considered as an aggregation of states. To illustrate the dynamics of a set of logical functions, which characterize our concerned properties of a BN, the invariant subspace containing the set of logical functions is proposed, and its properties are investigated. Then the invariant subspace of Boolean control network (BCN) is also proposed. The dynamics of invariant subspace of BCN is also invariant. Finally, using outputs as the set of logical functions, the minimum realization of BCN is proposed, which provides a possible solution to overcome the computational complexity of large scale BNs/BCNs.      
### 26.XCloud-pFISTA: A Medical Intelligence Cloud for Accelerated MRI  [ :arrow_down: ](https://arxiv.org/pdf/2104.08824.pdf)
>  Machine learning and artificial intelligence have shown remarkable performance in accelerated magnetic resonance imaging (MRI). Cloud computing technologies have great advantages in building an easily accessible platform to deploy advanced algorithms. In this work, we develop an open-access, easy-to-use and high-performance medical intelligence cloud computing platform (XCloud-pFISTA) to reconstruct MRI images from undersampled k-space data. Two state-of-the-art approaches of the Projected Fast Iterative Soft-Thresholding Algorithm (pFISTA) family have been successfully implemented on the cloud. This work can be considered as a good example of cloud-based medical image reconstruction and may benefit the future development of integrated reconstruction and online diagnosis system.      
### 27.Ecological Adaptive Cruise Control for City Buses based on Hybrid Model Predictive Control using PnG and Traffic Light Information  [ :arrow_down: ](https://arxiv.org/pdf/2104.08796.pdf)
>  This paper proposes an ecological adaptive cruise control (EACC) concept with the primary goal to minimize the fuel consumption in a city bus with an internal combustion engine (ICE). A hybrid model predictive control (HMPC) is implemented in this work to control both continuous and discrete-time variables. Moreover, a multi-objective optimization problem for EACC is formulated in time-domain as a mixed-integer quadratically constrained quadratic programming (MIQCQP) problem. The proposed HMPC-EACC performs robust vehicle-following while tracking a leading vehicle and plans fuel-efficient acceleration and deceleration maneuvers for the host vehicle. Additionally, it uses the signal phase and timing (SPaT) information to compute a green wave reference speed for the host vehicle to cross the signalized intersections at a green phase. Moreover, the proposed controller performs pulse and glide (PnG) to optimally control the engine ON and OFF states and save additional fuel. Furthermore, the performance of the proposed strategy is evaluated on a real-world driving profile and compared against a baseline controller from the literature. Finally, the influence of different prediction horizons on the fuel savings and computation times are studied. The results reveal significant reduction in fuel consumption with HMPC-EACC and demonstrate that the proposed controller is real-time capable.      
### 28.External Dynamic InTerference Estimation and Removal (EDITER) for low field MRI  [ :arrow_down: ](https://arxiv.org/pdf/2104.08680.pdf)
>  Purpose: Point-of-care MRI requires operation outside of a faraday shielded room normally used to block image-degrading electromagnetic Interference (EMI). To address this, we introduce the EDITER method, an external sensor based dynamic EMI estimation and removal method to retrospectively remove time-varying external interference sources. <br>Theory and Methods: The method acquires data from multiple EMI detectors (tuned receive coils and electrodes placed on the body) simultaneous with the primary MR coil during image data acquisition. We dynamically calculate impulse response functions that map the data from the detectors to the artifacts in the kspace data, then remove the transformed detected EMI from the MR data. Performance of the EDITER algorithm was assessed in phantom and in vivo imaging experiments in an 80mT portable brain MRI in a controlled EMI environment and with an open 47.5mT MRI scanner in an uncontrolled EMI setting. <br>Results: In the controlled setting, the effectiveness of the EDITER technique was demonstrated for specific types of introduced EMI sources with up to a 97% reduction of structured EMI and up to 76% reduction of broadband EMI. In the uncontrolled EMI experiments, we demonstrate EMI reductions of 37% with a single pickup coil and 89% with a single electrode and up to 99% with both. <br>Conclusion: The EDITER technique is a flexible and robust method to improve image quality in portable MRI systems with minimal passive shielding. This could reduce the reliance of MRI on shielded rooms and allow for truly portable MRI with specialized compact POC scanners      
### 29.Optimizing Furnace efficiency for Factory of Future using Cooperative Games  [ :arrow_down: ](https://arxiv.org/pdf/2104.08586.pdf)
>  Approximately 75% of energy used in petrochemical and refining industries is consumed by furnaces. Operating furnaces at optimal conditions results in huge amounts of savings. In this paper, we model the furnace efficiency optimization as a multi-objective problem involving multiple interactions among the controlled variables and propose a cooperative game based formulation for the factory of future. The controlled variables are Absorbed Duty and Coil Outlet Temperature. We propose a comprehensive solution to select the best combination of manipulated variables (fired duty, throughput and coil inlet temperature) satisfying multiple criteria using a cooperative game theory approach. We compare this approach with the standard multi-objective optimization using NSGA-II and RNSGA-II algorithms.      
### 30.Exploring Cybersecurity Issues in 5G Enabled Electric Vehicle Charging Station with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.08553.pdf)
>  The surging usage of electric vehicles (EVs) demand the robust deployment of trustworthy electric vehicle charging station (EVCS) with millisecond range latency and massive machine to machine communications where 5G could act. However, 5G suffers from inherent protocols, hardware, and software vulnerabilities that seriously threaten the communicating entities' cyber-physical security. To overcome these limitations in the EVCS system, this paper analyses the impact of False Data Injection (FDI) and Distributed Denial of Services (DDoS) attacks on the operation of EVCS. This work is an extension of our previously published conference paper about the EVCS. As new features, this paper simulates the FDI attack and the syn flood DDoS attacks on 5G enabled remote Supervisory Control and Data Acquisition (SCADA) system that controls the solar photovoltaics (PV) controller, Battery Energy Storage (BES) controller, and EV controller of the EVCS. The attacks make the EVCS system oscillate or shift the DC operating point. The frequency of oscillation, its damping, and the system's resiliency are found to be related to the attacks' intensity and target controller. Finally, we propose the novel stacked Long Short-Term Memory (LSTM) based intrusion detection systems (IDS) solely based on the electrical fingerprint. This model can detect the stealthy cyberattacks that bypass the cyber layer and go unnoticed in the monitoring system with nearly 100% detection accuracy.      
### 31.Cycle-free CycleGAN using Invertible Generator for Unsupervised Low-Dose CT Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2104.08538.pdf)
>  Recently, CycleGAN was shown to provide high-performance, ultra-fast denoising for low-dose X-ray computed tomography (CT) without the need for a paired training dataset. Although this was possible thanks to cycle consistency, CycleGAN requires two generators and two discriminators to enforce cycle consistency, demanding significant GPU resources and technical skills for training. A recent proposal of tunable CycleGAN with Adaptive Instance Normalization (AdaIN) alleviates the problem in part by using a single generator. However, two discriminators and an additional AdaIN code generator are still required for training. To solve this problem, here we present a novel cycle-free Cycle-GAN architecture, which consists of a single generator and a discriminator but still guarantees cycle consistency. The main innovation comes from the observation that the use of an invertible generator automatically fulfills the cycle consistency condition and eliminates the additional discriminator in the CycleGAN formulation. To make the invertible generator more effective, our network is implemented in the wavelet residual domain. Extensive experiments using various levels of low-dose CT images confirm that our method can significantly improve denoising performance using only 10% of learnable parameters and faster training time compared to the conventional CycleGAN.      
### 32.Noise Attention based Spectrum Anomaly Detection Method for Unauthorized Bands  [ :arrow_down: ](https://arxiv.org/pdf/2104.08517.pdf)
>  Spectrum anomaly detection is of great importance in wireless communication to secure safety and improve spectrum efficiency. However, spectrum anomaly detection faces many difficulties, especially in unauthorized frequency bands. For example, the composition of unauthorized frequency bands is very complex and the abnormal usage patterns are unknown in prior. In this paper, a noise attention method is proposed for unsupervised spectrum anomaly detection in unauthorized bands. First of all, we theoretically prove that the anomalies in unauthorized bands will raise the noise floor of spectrogram after VAE reconstruction. Then, we introduce a novel anomaly metric named as noise attention score to more effectively capture spectrum anomaly. The effectiveness of the proposed method is experimentally verified in 2.4 GHz ISM band. Leveraging the noise attention score, the AUC metric of anomaly detection is increased by 0.193. The proposed method is beneficial to reliably detecting abnormal spectrum while keeping low false alarm rate.      
### 33.Spatial Correlation Aware Compressed Sensing for User Activity Detection and Channel Estimation in Massive MTC  [ :arrow_down: ](https://arxiv.org/pdf/2104.08508.pdf)
>  Grant-free access is considered as a key enabler to massive machine-type communications (mMTC) as it promotes energy-efficiency and small signalling overhead. Due to the sporadic user activity in mMTC, joint user identification and channel estimation (JUICE) is a main challenge. This paper addresses the JUICE in single-cell mMTC with single-antenna users and a multi-antenna base station (BS) under spatially correlated fading channels. In particular, by leveraging the sporadic user activity, we solve the JUICE in a multi measurement vector compressed sensing (CS) framework under two different cases, with and without the knowledge of prior channel distribution information (CDI) at the BS. First, for the case without prior information, we formulate the JUICE as an iterative reweighted $\ell_{2,1}$-norm minimization problem. Second, when the CDI is known to the BS, we exploit the available information and formulate the JUICE from a Bayesian estimation perspective as a maximum \emph{a posteriori} probability (MAP) estimation problem. For both JUICE formulations, we derive efficient iterative solutions based on the alternating direction method of multipliers (ADMM). The numerical experiments show that the proposed solutions achieve higher channel estimation quality and activity detection accuracy with shorter pilot sequences compared to existing algorithms.      
### 34.Multi-Metric Optimization using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2104.08499.pdf)
>  The intelligibility of speech severely degrades in the presence of environmental noise and reverberation. In this paper, we propose a novel deep learning based system for modifying the speech signal to increase its intelligibility under the equal-power constraint, i.e., signal power before and after modification must be the same. To achieve this, we use generative adversarial networks (GANs) to obtain time-frequency dependent amplification factors, which are then applied to the input raw speech to reallocate the speech energy. Instead of optimizing only a single, simple metric, we train a deep neural network (DNN) model to simultaneously optimize multiple advanced speech metrics, including both intelligibility- and quality-related ones, which results in notable improvements in performance and robustness. Our system can not only work in non-realtime mode for offline audio playback but also support practical real-time speech applications. Experimental results using both objective measurements and subjective listening tests indicate that the proposed system significantly outperforms state-ofthe-art baseline systems under various noisy and reverberant listening conditions.      
### 35.KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2104.08459.pdf)
>  This paper introduces a high-quality open-source speech synthesis dataset for Kazakh, a low-resource language spoken by over 13 million people worldwide. The dataset consists of about 91 hours of transcribed audio recordings spoken by two professional speakers (female and male). It is the first publicly available large-scale dataset developed to promote Kazakh text-to-speech (TTS) applications in both academia and industry. In this paper, we share our experience by describing the dataset development procedures and faced challenges, and discuss important future directions. To demonstrate the reliability of our dataset, we built baseline end-to-end TTS models and evaluated them using the subjective mean opinion score (MOS) measure. Evaluation results show that the best TTS models trained on our dataset achieve MOS above 4 for both speakers, which makes them applicable for practical use. The dataset, training recipe, and pretrained TTS models are freely available.      
### 36.Deep Chaos Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2104.08436.pdf)
>  In this study, we address the problem of chaotic synchronization over a noisy channel by introducing a novel Deep Chaos Synchronization (DCS) system using a Convolutional Neural Network (CNN). Conventional Deep Learning (DL) based communication strategies are extremely powerful but training on large data sets is usually a difficult and time-consuming procedure. To tackle this challenge, DCS does not require prior information or large data sets. In addition, we provide a novel Recurrent Neural Network (RNN)-based chaotic synchronization system for comparative analysis. The results show that the proposed DCS architecture is competitive with RNN-based synchronization in terms of robustness against noise, convergence, and training. Hence, with these features, the DCS scheme will open the door for a new class of modulator schemes and meet the robustness against noise, convergence, and training requirements of the Ultra Reliable Low Latency Communications (URLLC) and Industrial Internet of Things (IIoT).      
### 37.Models and Predictive Control for Nonplanar Vehicle Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2104.08427.pdf)
>  We present a simplified model of a vehicle driving on a nonplanar road. A parametric surface is used to describe the nonplanar road which can describe any combination of curvature, bank and slope. We show that the proposed modeling approach generalizes planar vehicle models that reference a centerline, such as the Frenet model. <br>We use the proposed approach for vehicle path planning and following using model predictive control. We also model and control vehicle contact with the road surface. We demonstrate that the proposed controller improves speed and lane following on complex roads compared to planar vehicle controllers, and mitigates loss of control on complex road surfaces including off-camber turns.      
### 38.Model-Based Deep Autoencoder Networks for Nonlinear Hyperspectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2104.08409.pdf)
>  Autoencoder (AEC) networks have recently emerged as a promising approach to perform unsupervised hyperspectral unmixing (HU) by associating the latent representations with the abundances, the decoder with the mixing model and the encoder with its inverse. AECs are especially appealing for nonlinear HU since they lead to unsupervised and model-free algorithms. However, existing approaches fail to explore the fact that the encoder should invert the mixing process, which might reduce their robustness. In this paper, we propose a model-based AEC for nonlinear HU by considering the mixing model a nonlinear fluctuation over a linear mixture. Differently from previous works, we show that this restriction naturally imposes a particular structure to both the encoder and to the decoder networks. This introduces prior information in the AEC without reducing the flexibility of the mixing model. Simulations with synthetic and real data indicate that the proposed strategy improves nonlinear HU.      
### 39.Manifold Model for High-Resolution fMRI Joint Reconstruction and Dynamic Quantification  [ :arrow_down: ](https://arxiv.org/pdf/2104.08395.pdf)
>  Oscillating Steady-State Imaging (OSSI) is a recent fMRI acquisition method that exploits a large and oscillating signal, and can provide high SNR fMRI. However, the oscillatory nature of the signal leads to an increased number of acquisitions. To improve temporal resolution and accurately model the nonlinearity of OSSI signals, we build the MR physics for OSSI signal generation as a regularizer for the undersampled reconstruction rather than using subspace models that are not well suited for the data. Our proposed physics-based manifold model turns the disadvantages of OSSI acquisition into advantages and enables joint reconstruction and quantification. OSSI manifold model (OSSIMM) outperforms subspace models and reconstructs high-resolution fMRI images with a factor of 12 acceleration and without spatial or temporal resolution smoothing. Furthermore, OSSIMM can dynamically quantify important physics parameters, including $R_2^*$ maps, with a temporal resolution of 150 ms.      
### 40.H_\infty Almost Output and Regulated Output Synchronization of Heterogeneous Multi-agent Systems: A Scale-free Protocol Design  [ :arrow_down: ](https://arxiv.org/pdf/2104.08347.pdf)
>  This paper studies scale-free protocol design for H_\infty almost output and regulated output synchronization of heterogeneous multi-agent systems with linear, right-invertible, and introspective agents in presence of external disturbances. The collaborative linear protocol designs are based on localized information exchange over the same communication network, which do not require any knowledge of the directed network topology and spectrum of the associated Laplacian matrix. Moreover, the proposed scale-free protocols achieve H_\infty almost synchronization with a given arbitrary degree of accuracy for any size of the network.      
### 41.Unsupervised Hyperspectral Stimulated Raman Microscopy Image Enhancement: De-Noising and Segmentation via One-Shot Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.08338.pdf)
>  Hyperspectral stimulated Raman scattering (SRS) microscopy is a powerful label-free, chemical-specific technique for biomedical and mineralogical imaging which can suffer from low signal-to-noise ratios due to requirements of low input laser power or fast imaging, or from optical scattering and low target concentration. Here, we demonstrate a deep learning neural net model and unsupervised machine-learning algorithm for rapid and automatic de-noising and segmentation of SRS images based on a ten layer convolutional autoencoder: UHRED (Unsupervised Hyperspectral Resolution Enhancement and De-noising). UHRED is trained in an unsupervised manner using only a single (one-shot) hyperspectral image, with no requirements for training on high quality (ground truth) labelled data sets or images. Importantly, although we illustrate this method using SRS, the hyperspectral index (signal as a function of a laser parameter) may be any imaging modality such as harmonic generation, linear and/or nonlinear fluorescence, CARS, Pump-Probe, Thermal Lensing and Cross-Phase microscopy. UHRED significantly enhances SRS image contrast by de-noising the extracted Raman spectra at every image pixel. Applying a k-means clustering algorithm provides automatic, unsupervised image segmentation based on Raman vibrational spectra of the sample constituents, yielding intuitive chemical species maps, as we demonstrate for the case of a complex lithium ore sample.      
### 42.Automated Seizure Detection and Seizure Type Classification From Electroencephalography With a Graph Neural Network and Self-Supervised Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2104.08336.pdf)
>  Automated seizure detection and classification from electroencephalography (EEG) can greatly improve the diagnosis and treatment of seizures. While prior studies mainly used convolutional neural networks (CNNs) that assume image-like structure in EEG signals or spectrograms, this modeling choice does not reflect the natural geometry of or connectivity between EEG electrodes. In this study, we propose modeling EEGs as graphs and present a graph neural network for automated seizure detection and classification. In addition, we leverage unlabeled EEG data using a self-supervised pre-training strategy. Our graph model with self-supervised pre-training significantly outperforms previous state-of-the-art CNN and Long Short-Term Memory (LSTM) models by 6.3 points (7.8%) in Area Under the Receiver Operating Characteristic curve (AUROC) for seizure detection and 6.3 points (9.2%) in weighted F1-score for seizure type classification. Ablation studies show that our graph-based modeling approach significantly outperforms existing CNN or LSTM models, and that self-supervision helps further improve the model performance. Moreover, we find that self-supervised pre-training substantially improves model performance on combined tonic seizures, a low-prevalence seizure type. Furthermore, our model interpretability analysis suggests that our model is better at identifying seizure regions compared to an existing CNN. In summary, our graph-based modeling approach integrates domain knowledge about EEG, sets a new state-of-the-art for seizure detection and classification on a large public dataset (5,499 EEG files), and provides better ability to identify seizure regions.      
### 43.Interpreting intermediate convolutional layers of CNNs trained on raw speech  [ :arrow_down: ](https://arxiv.org/pdf/2104.09489.pdf)
>  This paper presents a technique to interpret and visualize intermediate layers in CNNs trained on raw speech data in an unsupervised manner. We show that averaging over feature maps after ReLU activation in each convolutional layer yields interpretable time-series data. The proposed technique enables acoustic analysis of intermediate convolutional layers. To uncover how meaningful representation in speech gets encoded in intermediate layers of CNNs, we manipulate individual latent variables to marginal levels outside of the training range. We train and probe internal representations on two models -- a bare GAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). We also argue that the proposed technique allows acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. The models are trained on two speech processes with different degrees of complexity: a simple presence of [s] and a computationally complex presence of reduplication (copied material). Observing the causal effect between interpolation and the resulting changes in intermediate layers can reveal how individual variables get transformed into spikes in activation in intermediate layers. Using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in different convolutional layers.      
### 44.Minimum-Energy Estimation for Discrete-Time Fractional-Order Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.09409.pdf)
>  Fractional-order dynamical systems are increasingly being used to model and describe processes demonstrating long-term memory or complex interlaced dependencies amongst the spatial and temporal components of a wide variety of cyber-physical systems. Notable examples include networked control systems or neurophysiological signals such as electroencephalogram (EEG). As a result, the estimation of states of a fractional-order dynamical system becomes an important problem to be solved. To this effect, this paper addresses the problem of minimum-energy state estimation for discrete-time fractional-order dynamical systems (DT-FODS). Specifically, we exploit a finite-dimensional approximation of the infinite-dimensional DT-FODS to design a minimum-energy state observer. The state and output equations are affected by an additive noise that is considered to be deterministic but unknown. The resulting estimation error is proved to be exponentially input-to-state stable with respect to the disturbances and to a signal that is decreasing with the increase of the accuracy of the adopted approximation model. An illustrative example shows the effectiveness of the proposed method on real-world neurophysiological EEG signals.      
### 45.DANICE: Domain adaptation without forgetting in neural image compression  [ :arrow_down: ](https://arxiv.org/pdf/2104.09370.pdf)
>  Neural image compression (NIC) is a new coding paradigm where coding capabilities are captured by deep models learned from data. This data-driven nature enables new potential functionalities. In this paper, we study the adaptability of codecs to custom domains of interest. We show that NIC codecs are transferable and that they can be adapted with relatively few target domain images. However, naive adaptation interferes with the solution optimized for the original source domain, resulting in forgetting the original coding capabilities in that domain, and may even break the compatibility with previously encoded bitstreams. Addressing these problems, we propose Codec Adaptation without Forgetting (CAwF), a framework that can avoid these problems by adding a small amount of custom parameters, where the source codec remains embedded and unchanged during the adaptation process. Experiments demonstrate its effectiveness and provide useful insights on the characteristics of catastrophic interference in NIC.      
### 46.The Impact of COVID-19 on Urban Energy Consumption of the Commercial Tourism City  [ :arrow_down: ](https://arxiv.org/pdf/2104.09351.pdf)
>  In 2020, the COVID-19 pandemic spreads all over the world. In order to alleviate the spread of the epidemic, various blockade policies have been implemented in many areas. In order to formulate a better epidemic prevention policy for urban energy consumption of the commercial tourism cities, this paper first analyses the energy characteristics of Macao during the epidemic period from two aspects, based on the energy consumption data of Macao. On this basis, the power consumption characteristics of commercial tourism cities during the epidemic were analyzed. Then, this paper provides analysis of the characteristics of the energy consumption in different fields of commercial tourism cities from the aspects of hotel, transportation, tourism culture and public utilities. Finally, a detailed analysis of the energy consumption characteristics of commercial tourism cities represented by Macao during the epidemic period is provided, by comparing with some typical countries.      
### 47.A SAR speckle filter based on Residual Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.09350.pdf)
>  In recent years, Machine Learning (ML) algorithms have become widespread in all fields of Remote Sensing (RS) and Earth Observation (EO). This has allowed a rapid development of new procedures to solve problems affecting these sectors. In this context, the authors of this work aim to present a novel method for filtering the speckle noise from Sentinel-1 data by applying Deep Learning (DL) algorithms, based on Convolutional Neural Networks (CNNs). The obtained results, if compared with the state of the art, show a clear improvement in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index ({SSIM}), by proving the effectiveness of the proposed architecture. Moreover, the generated open-source code and dataset have been made available for further developments and investigation by interested researchers.      
### 48.Approximate Multi-Agent Fitted Q Iteration  [ :arrow_down: ](https://arxiv.org/pdf/2104.09343.pdf)
>  We formulate an efficient approximation for multi-agent batch reinforcement learning, the approximate multi-agent fitted Q iteration (AMAFQI). We present a detailed derivation of our approach. We propose an iterative policy search and show that it yields a greedy policy with respect to multiple approximations of the centralized, standard Q-function. In each iteration and policy evaluation, AMAFQI requires a number of computations that scales linearly with the number of agents whereas the analogous number of computations increase exponentially for the fitted Q iteration (FQI), one of the most commonly used approaches in batch reinforcement learning. This property of AMAFQI is fundamental for the design of a tractable multi-agent approach. We evaluate the performance of AMAFQI and compare it to FQI in numerical simulations. Numerical examples illustrate the significant computation time reduction when using AMAFQI instead of FQI in multi-agent problems and corroborate the similar decision-making performance of both approaches.      
### 49.How modular structure determines operational resilience of power grids  [ :arrow_down: ](https://arxiv.org/pdf/2104.09338.pdf)
>  The synchronization stability has been analyzed as one of the important dynamical characteristics of power grids. In this study, we bring the operational perspective to the synchronization stability analysis by counting not only full but also partial synchronization between nodes. To do so, we introduce two distinct measures that estimate the operational resilience of power-grid nodes: functional stability and functional resistance. We demonstrate the practical applicability of the measures in a model network motif and an IEEE test power grid. As a case study of German power grid, we reveal that the modular structure of a power grid and particular unidirectional current flow determine the distribution of the operational resilience of power-grid nodes. Reproducing our finding on clustered benchmark networks, we validate the modular effect on power grid stability and confirm that our measures can be the insightful tools to understand the power grids' synchronization dynamics.      
### 50.Multi-objective Eco-Routing Model Development and Evaluation for Battery Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2104.09336.pdf)
>  This paper develops and investigates the impacts of multi-objective Nash optimum (user equilibrium) traffic assignment on a large-scale network for battery electric vehicles (BEVs) and internal combustion engine vehicles (ICEVs) in a microscopic traffic simulation environment. Eco-routing is a technique that finds the most energy efficient route. ICEV and BEV energy consumption patterns are significantly different with regard to their sensitivity to driving cycles. Unlike ICEVs, BEVs are more energy efficient on low-speed arterial trips compared to highway trips. Different energy consumption patterns require different eco-routing strategies for ICEVs and BEVs. This study found that eco-routing could reduce energy consumption for BEVs but also significantly increases their average travel time. The simulation study found that multi-objective routing could reduce the energy consumption of BEVs by 13.5, 14.2, 12.9, and 10.7 percent, as well as the fuel consumption of ICEVs by 0.1, 4.3, 3.4, and 10.6 percent for "not congested", "slightly congested", "moderately congested", and "highly congested" conditions, respectively. The study also found that multi-objective user equilibrium routing reduced the average vehicle travel time by up to 10.1% compared to the standard user equilibrium traffic assignment for the highly congested conditions, producing a solution closer to the system optimum traffic assignment. The results indicate that the multi-objective eco-routing can effectively reduce fuel/energy consumption with minimum impacts on travel times for both BEVs and ICEVs.      
### 51.Assessment of deep learning based blood pressure prediction from PPG and rPPG signals  [ :arrow_down: ](https://arxiv.org/pdf/2104.09313.pdf)
>  Exploiting photoplethysmography signals (PPG) for non-invasive blood pressure (BP) measurement is interesting for various reasons. First, PPG can easily be measured using fingerclip sensors. Second, camera-based approaches allow to derive remote PPG (rPPG) signals similar to PPG and therefore provide the opportunity for non-invasive measurements of BP. Various methods relying on machine learning techniques have recently been published. Performances are often reported as the mean average error (MAE) on the data which is problematic. This work aims to analyze the PPG- and rPPG-based BP prediction error with respect to the underlying data distribution. First, we train established neural network (NN) architectures and derive an appropriate parameterization of input segments drawn from continuous PPG signals. Second, we apply this parameterization to a larger PPG dataset and train NNs to predict BP. The resulting prediction errors increase towards less frequent BP values. Third, we use transfer learning to train the NNs for rPPG based BP prediction. The resulting performances are similar to the PPG-only case. Finally, we apply a personalization technique and retrain our NNs with subject-specific data. This slightly reduces the prediction errors.      
### 52.Evaluation of the impact of Heat-Wave on Distribution System Resilience  [ :arrow_down: ](https://arxiv.org/pdf/2104.09308.pdf)
>  This paper presents the findings about the impact of heat waves on a real urban distribution system. A data-driven methodology is proposed to simulate the portion of faults that can be associated to normal conditions (and hence to reliability) and the portion correlated to the heat wave occurrence. Based on real data collected in the years 2012-2017, the fault rates associated to reliability and resilience have been calculated and then used to feed a Monte Carlo simulation aiming to manage the uncertainty in the fault occurrence. Finally, based on the Italian legislation, the benefits deriving by the substitution of the faulted portion of the system have been calculated.      
### 53.Tracking agitation in people living with dementia in a care environment  [ :arrow_down: ](https://arxiv.org/pdf/2104.09305.pdf)
>  Agitation is a symptom that communicates distress in people living with dementia (PwD), and that can place them and others at risk. In a long term care (LTC) environment, care staff track and document these symptoms as a way to detect when there has been a change in resident status to assess risk, and to monitor for response to interventions. However, this documentation can be time-consuming, and due to staffing constraints, episodes of agitation may go unobserved. This brings into question the reliability of these assessments, and presents an opportunity for technology to help track and monitor behavioural symptoms in dementia. In this paper, we present the outcomes of a 2 year real-world study performed in a dementia unit, where a multi-modal wearable device was worn by $20$ PwD. In line with a commonly used clinical documentation tool, this large multi-modal time-series data was analyzed to track the presence of episodes of agitation in 8-hour nursing shifts. The development of a baseline classification model (AUC=0.717) on this dataset and subsequent improvement (AUC= 0.779) lays the groundwork for automating the process of annotating agitation events in nursing charts.      
### 54.Towards realistic statistical models of the grid frequency  [ :arrow_down: ](https://arxiv.org/pdf/2104.09289.pdf)
>  Increased share of renewable sources of energy in a power grid leads to larger deviations in grid frequency from the nominal value resulting in more challenging control and its modelling. In this paper we focus on the grid frequency for the power system of Great Britain because the large share of renewables makes it a template for other power grids in the future and because it exhibits peculiar statistical properties, such as long-term correlations in fluctuations, periodicity, and bi-modal distribution of the grid frequency. By modifications of the swing equation and the underlying noise statistics, which we justify qualitatively and quantitatively, we reproduce these peculiar statistical properties. We apply our model to a realistic frequency response service and show our predictions outperform a standard swing equation model.      
### 55.Pareto-Optimal Domino-Tiling of Orthogonal Polygon Phased Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2104.09280.pdf)
>  The modular design of planar phased arrays arranged on orthogonal polygon-shaped apertures is addressed and a new method is proposed to synthesize domino-tiled arrays fitting multiple, generally conflicting, requirements. Starting from an analytic procedure to check the domino-tileability of the aperture, two multi-objective optimization techniques are derived to efficiently and effectively deal with small and medium/large arrays depending on the values of the bounds for the cardinality of the solution space of the admissible clustered solutions. A set of representative numerical examples is reported to assess the effectiveness of the proposed synthesis approach also through full-wave simulations when considering non-ideal models for the radiating elements of the array.      
### 56.Radio-Transparent Dipole Antenna Based on a Metasurface Cloak  [ :arrow_down: ](https://arxiv.org/pdf/2104.09278.pdf)
>  Antenna technology is at the basis of ubiquitous wireless communication systems and sensors. Radiation is typically sustained by conduction currents flowing around resonant metallic objects that are optimized to enhance efficiency and bandwidth. However, resonant conductors are prone to large scattering of impinging waves, leading to challenges in crowded antenna environments due to blockage and distortion. Metasurface cloaks have been explored in the quest of addressing this challenge by reducing antenna scattering, but with limited performance in terms of bandwidth, footprint and overall scattering reduction. Here we introduce a different route towards radio-transparent antennas, in which the cloak itself acts as the radiating element, drastically reducing the overall footprint while enhancing scattering suppression and bandwidth, without sacrificing other relevant radiation metrics compared to conventional antennas. This technique offers a new application of cloaking technology, with promising features for crowded wireless communication platforms and noninvasive sensing.      
### 57.Estimating Transit Vehicle Dwell Times at Bus Stops  [ :arrow_down: ](https://arxiv.org/pdf/2104.09266.pdf)
>  This work presents a quantitative approach to estimate the total time spent in the vicinity of a bus stop including the deceleration time, the boarding and alighting time (developed in an earlier study), the acceleration time, and re-entry time (time required to merge into the adjacent lane). Different statistical models were used to compute the deceleration, acceleration and merge times. Typical deceleration and acceleration levels were computed using kinematic equations that were then used to compute both the deceleration and acceleration times. The adopted method to estimate both the deceleration time and the acceleration time was validated utilizing transit data from Blacksburg Transit (BT) using the mean absolute percentage error (MAPE) and root mean square error (RMSE). The MAPE and RMSE values were calculated to be 0.3 and 13.3 percent for the deceleration time and 0.42 and 2.72 percent for the acceleration time, respectively. The re-entry time was estimated to be a function of the adjacent roadway traffic density using both a multiple linear regression and Bayesian regression approach. Both methods showed consistency in estimating the merge-time model coefficients. The proposed models can be integrated with transit applications to estimate transit vehicle travel times.      
### 58.A Novel Wireless Communication Paradigm for Intelligent Reflecting Surface Based Symbiotic Radio Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.09161.pdf)
>  This paper investigates a novel intelligent reflecting surface (IRS)-based symbiotic radio (SR) system architecture consisting of a transmitter, an IRS, and an information receiver (IR). The primary transmitter communicates with the IR and at the same time assists the IRS in forwarding information to the IR. Based on the IRS's symbol period, we distinguish two scenarios, namely, commensal SR (CSR) and parasitic SR (PSR), where two different techniques for decoding the IRS signals at the IR are employed. We formulate bit error rate (BER) minimization problems for both scenarios by jointly optimizing the active beamformer at the base station and the phase shifts at the IRS, subject to a minimum primary rate requirement. Specifically, for the CSR scenario, a penalty-based algorithm is proposed to obtain a high-quality solution, where semi-closed-form solutions for the active beamformer and the IRS phase shifts are derived based on Lagrange duality and Majorization-Minimization methods, respectively. For the PSR scenario, we apply a bisection search-based method, successive convex approximation, and difference of convex programming to develop a computationally efficient algorithm, which converges to a locally optimal solution. Simulation results demonstrate the effectiveness of the proposed algorithms and show that the proposed SR techniques are able to achieve a lower BER than benchmark schemes.      
### 59.Global Stabilization of Compressible Flow Between Two Moving Pistons  [ :arrow_down: ](https://arxiv.org/pdf/2104.09153.pdf)
>  This paper studies the global feedback stabilization problem of a system with two pistons and the area between them containing a viscous compressible fluid (gas) modeled by the Navier-Stokes equations. The control input is the force applied on the left piston (boundary input) and the overall system consists of two nonlinear Partial Differential Equations and four nonlinear Ordinary Differential Equations. Global feedback stabilizers are designed for the overall system by means of the Control Lyapunov Functional methodology. The closed-loop system exhibits global asymptotic stability with an exponential convergence rate. The proposed stabilizing boundary feedback laws do not require measurement of the density and velocity profiles inside the area between the pistons and simply require measurements of the gas density and velocity at the position of the actuated piston.      
### 60.Conditional Coding and Variable Bitrate for Practical Learned Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2104.09103.pdf)
>  This paper introduces a practical learned video codec. Conditional coding and quantization gain vectors are used to provide flexibility to a single encoder/decoder pair, which is able to compress video sequences at a variable bitrate. The flexibility is leveraged at test time by choosing the rate and GOP structure to optimize a rate-distortion cost. Using the CLIC21 video test conditions, the proposed approach shows performance on par with HEVC.      
### 61.A novel Time-frequency Transformer and its Application in Fault Diagnosis of Rolling Bearings  [ :arrow_down: ](https://arxiv.org/pdf/2104.09079.pdf)
>  The scope of data-driven fault diagnosis models is greatly improved through deep learning (DL). However, the classical convolution and recurrent structure have their defects in computational efficiency and feature representation, while the latest Transformer architecture based on attention mechanism has not been applied in this field. To solve these problems, we propose a novel time-frequency Transformer (TFT) model inspired by the massive success of standard Transformer in sequence processing. Specially, we design a fresh tokenizer and encoder module to extract effective abstractions from the time-frequency representation (TFR) of vibration signals. On this basis, a new end-to-end fault diagnosis framework based on time-frequency Transformer is presented in this paper. Through the case studies on bearing experimental datasets, we constructed the optimal Transformer structure and verified the performance of the diagnostic method. The superiority of the proposed method is demonstrated in comparison with the benchmark model and other state-of-the-art methods.      
### 62.An Interdisciplinary Review of Music Performance Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2104.09018.pdf)
>  A musical performance renders an acoustic realization of a musical score or other representation of a composition. Different performances of the same composition may vary in terms of performance parameters such as timing or dynamics, and these variations may have a major impact on how a listener perceives the music. The analysis of music performance has traditionally been a peripheral topic for the MIR research community, where often a single audio recording is used as representative of a musical work. This paper surveys the field of Music Performance Analysis (MPA) from several perspectives including the measurement of performance parameters, the relation of those parameters to the actions and intentions of a performer or perceptual effects on a listener, and finally the assessment of musical performance. This paper also discusses MPA as it relates to MIR, pointing out opportunities for collaboration and future research in both areas.      
### 63.Embedding Reservoirs in Industrial Models to Exploit their Flexibility  [ :arrow_down: ](https://arxiv.org/pdf/2104.08995.pdf)
>  In the context of energy transition, industrial plants that heavily rely on electricity face more and more price volatility. To continue operating in these conditions, the directors become continually more willing to increase their flexibility, i.e. their ability to react to price fluctuations. This work proposes an intuitive methodology to mathematically model electro-intensive processes in order to assess their flexibility potential. To this end, we introduce the notion of reservoir, a storage of either material or energy, that allows models based on this paradigm to have interpretations close to the physics of the processes. The design of the reservoir methodology has three distinct goals: (i) to be easy and quick to build by an energy-sector consultant; (ii) to be effortlessly converted into mixed-integer linear or nonlinear programs; (iii) to be straightforward to understand by nontechnical people, thanks to their graphic nature. We apply this methodology to two industrial case studies, namely an induction furnace (linear model) and an industrial cooling installation (nonlinear model), where we can achieve significant cost savings. In both cases, the models can be quickly written using our method and solved by appropriate solver technologies.      
### 64.Many-Speakers Single Channel Speech Separation with Optimal Permutation Training  [ :arrow_down: ](https://arxiv.org/pdf/2104.08955.pdf)
>  Single channel speech separation has experienced great progress in the last few years. However, training neural speech separation for a large number of speakers (e.g., more than 10 speakers) is out of reach for the current methods, which rely on the Permutation Invariant Loss (PIT). In this work, we present a permutation invariant training that employs the Hungarian algorithm in order to train with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in comparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified architecture that can handle the increased number of speakers. Our approach separates up to $20$ speakers and improves the previous results for large $C$ by a wide margin.      
### 65.CNN AE: Convolution Neural Network combined with Autoencoder approach to detect survival chance of COVID 19 patients  [ :arrow_down: ](https://arxiv.org/pdf/2104.08954.pdf)
>  In this paper, we propose a novel method named CNN-AE to predict survival chance of COVID-19 patients using a CNN trained on clinical information. To further increase the prediction accuracy, we use the CNN in combination with an autoencoder. Our method is one of the first that aims to predict survival chance of already infected patients. We rely on clinical data to carry out the prediction. The motivation is that the required resources to prepare CT images are expensive and limited compared to the resources required to collect clinical data such as blood pressure, liver disease, etc. We evaluate our method on a publicly available clinical dataset of deceased and recovered patients which we have collected. Careful analysis of the dataset properties is also presented which consists of important features extraction and correlation computation between features. Since most of COVID-19 patients are usually recovered, the number of deceased samples of our dataset is low leading to data imbalance. To remedy this issue, a data augmentation procedure based on autoencoders is proposed. To demonstrate the generality of our augmentation method, we train random forest and Naïve Bayes on our dataset with and without augmentation and compare their performance. We also evaluate our method on another dataset for further generality verification. Experimental results reveal the superiority of CNN-AE method compared to the standard CNN as well as other methods such as random forest and Naïve Bayes. COVID-19 detection average accuracy of CNN-AE is 96.05% which is higher than CNN average accuracy of 92.49%. To show that clinical data can be used as a reliable dataset for COVID-19 survival chance prediction, CNN-AE is compared with a standard CNN which is trained on CT images.      
### 66.Autonomous Situational Awareness for UAS Swarms  [ :arrow_down: ](https://arxiv.org/pdf/2104.08904.pdf)
>  This paper describes a technique for the autonomous mission planning of unmanned aerial system swarms. Given a swarm operating in a known area, a central command system generates measurements from the swarm. If those measurements indicate changes to the mission situation such as target movement, the swarm planning is updated to reflect the new situation and guidance updates are broadcast to the swarm. The primary algorithms featured in this work are A* pathfinding and the Generalized Labeled Multi-Bernoulli multi-target tracking method.      
### 67.Convolutional Neural Networks in Orthodontics: a review  [ :arrow_down: ](https://arxiv.org/pdf/2104.08886.pdf)
>  Convolutional neural networks (CNNs) are used in many areas of computer vision, such as object tracking and recognition, security, military, and biomedical image analysis. This review presents the application of convolutional neural networks in one of the fields of dentistry - orthodontics. Advances in medical imaging technologies and methods allow CNNs to be used in orthodontics to shorten the planning time of orthodontic treatment, including an automatic search of landmarks on cephalometric X-ray images, tooth segmentation on Cone-Beam Computed Tomography (CBCT) images or digital models, and classification of defects on X-Ray panoramic images. In this work, we describe the current methods, the architectures of deep convolutional neural networks used, and their implementations, together with a comparison of the results achieved by them. The promising results and visualizations of the described studies show that the use of methods based on convolutional neural networks allows for the improvement of computer-based orthodontic treatment planning, both by reducing the examination time and, in many cases, by performing the analysis much more accurately than a manual orthodontist does.      
### 68.Quick Learner Automated Vehicle Adapting its Roadmanship to Varying Traffic Cultures with Meta Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.08876.pdf)
>  It is essential for an automated vehicle in the field to perform discretionary lane changes with appropriate roadmanship - driving safely and efficiently without annoying or endangering other road users - under a wide range of traffic cultures and driving conditions. While deep reinforcement learning methods have excelled in recent years and been applied to automated vehicle driving policy, there are concerns about their capability to quickly adapt to unseen traffic with new environment dynamics. We formulate this challenge as a multi-Markov Decision Processes (MDPs) adaptation problem and developed Meta Reinforcement Learning (MRL) driving policies to showcase their quick learning capability. Two types of distribution variation in environments were designed and simulated to validate the fast adaptation capability of resulting MRL driving policies which significantly outperform a baseline RL.      
### 69.Low-Frequency Characterization of Music Sounds -- Ultra-Bass Richness from the Sound Wave Beats  [ :arrow_down: ](https://arxiv.org/pdf/2104.08872.pdf)
>  The orchestra performance is full of sublime rich sounds. In particular, the unison of violins sounds different from the solo violin. We try to clarify this difference and similarity of unison and solo numerically analyzing the beat of `violins` with timbre, vibrato, melody, and resonance. Characteristic properties appear in the very low-frequency part in the power spectrum of the wave amplitude squared. This ultra-buss richness (UBR) can be a new characteristic of sound on top of the well-known pitch, loudness, and timbre, although being inaudible directly. We find this UBR is always characterized by a power-law at low-frequency with the index around -1 and appears everywhere in music and thus being universal. Furthermore, we explore this power-law property towards much smaller frequency regions and suggest possible relation to the 1/f noise often found in music and many other fields in nature.      
### 70.Best Practices for Noise-Based Augmentation to Improve the Performance of Emotion Recognition "In the Wild"  [ :arrow_down: ](https://arxiv.org/pdf/2104.08806.pdf)
>  Emotion recognition as a key component of high-stake downstream applications has been shown to be effective, such as classroom engagement or mental health assessments. These systems are generally trained on small datasets collected in single laboratory environments, and hence falter when tested on data that has different noise characteristics. Multiple noise-based data augmentation approaches have been proposed to counteract this challenge in other speech domains. But, unlike speech recognition and speaker verification, in emotion recognition, noise-based data augmentation may change the underlying label of the original emotional sample. In this work, we generate realistic noisy samples of a well known emotion dataset (IEMOCAP) using multiple categories of environmental and synthetic noise. We evaluate how both human and machine emotion perception changes when noise is introduced. We find that some commonly used augmentation techniques for emotion recognition significantly change human perception, which may lead to unreliable evaluation metrics such as evaluating efficiency of adversarial attack. We also find that the trained state-of-the-art emotion recognition models fail to classify unseen noise-augmented samples, even when trained on noise augmented datasets. This finding demonstrates the brittleness of these systems in real-world conditions. We propose a set of recommendations for noise-based augmentation of emotion datasets and for how to deploy these emotion recognition systems "in the wild".      
### 71.Intuitive Physics Guided Exploration for Sample Efficient Sim2real Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2104.08795.pdf)
>  Physics-based reinforcement learning tasks can benefit from simplified physics simulators as they potentially allow near-optimal policies to be learned in simulation. However, such simulators require the latent factors (e.g. mass, friction coefficient etc.) of the associated objects and other environment-specific factors (e.g. wind speed, air density etc.) to be accurately specified, without which, it could take considerable additional learning effort to adapt the learned simulation policy to the real environment. As such a complete specification can be impractical, in this paper, we instead, focus on learning task-specific estimates of latent factors which allow the approximation of real world trajectories in an ideal simulation environment. Specifically, we propose two new concepts: a) action grouping - the idea that certain types of actions are closely associated with the estimation of certain latent factors, and; b) partial grounding - the idea that simulation of task-specific dynamics may not need precise estimation of all the latent factors. We first introduce intuitive action groupings based on human physics knowledge and experience, which is then used to design novel strategies for interacting with the real environment. Next, we describe how prior knowledge of a task in a given environment can be used to extract the relative importance of different latent factors, and how this can be used to inform partial grounding, which enables efficient learning of the task in any arbitrary environment. We demonstrate our approach in a range of physics based tasks, and show that it achieves superior performance relative to other baselines, using only a limited number of real-world interactions.      
### 72.Model Error Propagation via Learned Contraction Metrics for Safe Feedback Motion Planning of Unknown Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.08695.pdf)
>  We present a method for contraction-based feedback motion planning of locally incrementally exponentially stabilizable systems with unknown dynamics that provides probabilistic safety and reachability guarantees. Given a dynamics dataset, our method learns a deep control-affine approximation of the dynamics. To find a trusted domain where this model can be used for planning, we obtain an estimate of the Lipschitz constant of the model error, which is valid with a given probability, in a region around the training data, providing a local, spatially-varying model error bound. We derive a trajectory tracking error bound for a contraction-based controller that is subjected to this model error, and then learn a controller that optimizes this tracking bound. With a given probability, we verify the correctness of the controller and tracking error bound in the trusted domain. We then use the trajectory error bound together with the trusted domain to guide a sampling-based planner to return trajectories that can be robustly tracked in execution. We show results on a 4D car, a 6D quadrotor, and a 22D deformable object manipulation task, showing our method plans safely with learned models of high-dimensional underactuated systems, while baselines that plan without considering the tracking error bound or the trusted domain can fail to stabilize the system and become unsafe.      
### 73.Unveiling Anomalous Edges and Nominal Connectivity of Attributed Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.08637.pdf)
>  Uncovering anomalies in attributed networks has recently gained popularity due to its importance in unveiling outliers and flagging adversarial behavior in a gamut of data and network science applications including {the Internet of Things (IoT)}, finance, security, to list a few. The present work deals with uncovering anomalous edges in attributed graphs using two distinct formulations with complementary strengths, which can be easily distributed, and hence efficient. The first relies on decomposing the graph data matrix into low rank plus sparse components to markedly improve performance. The second broadens the scope of the first by performing robust recovery of the unperturbed graph, which enhances the anomaly identification performance. The novel methods not only capture anomalous edges linking nodes of different communities, but also spurious connections between any two nodes with different features. Experiments conducted on real and synthetic data corroborate the effectiveness of both methods in the anomaly identification task.      
### 74.DART: Accurate, Autonomous, Near Real-time 3D Reconstruction using Drones  [ :arrow_down: ](https://arxiv.org/pdf/2104.08634.pdf)
>  Drones will revolutionize 3D modeling. A 3D model represents an accurate reconstruction of an object or structure. This paper explores the design and implementation of DART, which provides near real-time, accurate reconstruction of 3D models using a drone-mounted LiDAR; such a capability can be useful to document construction or check aircraft integrity between flights. Accurate reconstruction requires high drone positioning accuracy, and, because GPS can be in accurate, DART uses SLAM. However, in doing so it must deal with several competing constraints: drone battery and compute resources, SLAM error accumulation, and LiDAR resolution. DART uses careful trajectory design to find a sweet spot in this constraint space, a fast reconnaissance flight to narrow the search area for structures, and offloads expensive computations to the cloud by streaming compressed LiDAR data over LTE. DART reconstructs large structures to within 10s of cms and incurs less than 100~ms compute latency.      
### 75.Mixed Gibbs Sampling Detector in High-Order Modulation Large-Scale MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.08626.pdf)
>  A neighborhood restricted Mixed Gibbs Sampling (MGS) based approach is proposed for low-complexity high-order modulation large-scale Multiple-Input Multiple-Output (LS-MIMO) detection. The proposed LS-MIMO detector applies a neighborhood limitation (NL) on the noisy solution from the MGS at a distance d - thus, named d-simplified MGS (d-sMGS) - in order to mitigate its impact, which can be harmful when a high order modulation is considered. Numerical simulation results considering 64-QAM demonstrated that the proposed detection method can substantially improve the MGS algorithm convergence, whereas no extra computational complexity per iteration is required. The proposed d-sMGS-based detector suitable for high-order modulation LS-MIMO further exhibits improved performance vs. complexity tradeoff when the system loading is high, i.e., when K &gt;= 0.75. N. Also, with increasing the number of dimensions, i.e., increasing the number of antennas and/or modulation order, a smaller restriction of 2-sMGS was shown to be a more interesting choice than 1-sMGS.      
### 76.Cetacean Translation Initiative: a roadmap to deciphering the communication of sperm whales  [ :arrow_down: ](https://arxiv.org/pdf/2104.08614.pdf)
>  The past decade has witnessed a groundbreaking rise of machine learning for human language analysis, with current methods capable of automatically accurately recovering various aspects of syntax and semantics - including sentence structure and grounded word meaning - from large data collections. Recent research showed the promise of such tools for analyzing acoustic communication in nonhuman species. We posit that machine learning will be the cornerstone of future collection, processing, and analysis of multimodal streams of data in animal communication studies, including bioacoustic, behavioral, biological, and environmental data. Cetaceans are unique non-human model species as they possess sophisticated acoustic communications, but utilize a very different encoding system that evolved in an aquatic rather than terrestrial medium. Sperm whales, in particular, with their highly-developed neuroanatomical features, cognitive abilities, social structures, and discrete click-based encoding make for an excellent starting point for advanced machine learning tools that can be applied to other animals in the future. This paper details a roadmap toward this goal based on currently existing technology and multidisciplinary scientific community effort. We outline the key elements required for the collection and processing of massive bioacoustic data of sperm whales, detecting their basic communication units and language-like higher-level structures, and validating these models through interactive playback experiments. The technological capabilities developed by such an undertaking are likely to yield cross-applications and advancements in broader communities investigating non-human communication and animal behavioral research.      
### 77.Uncovering audio patterns in music with Nonnegative Tucker Decomposition for structural segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2104.08580.pdf)
>  Recent work has proposed the use of tensor decomposition to model repetitions and to separate tracks in loop-based electronic music. The present work investigates further on the ability of Nonnegative Tucker Decompositon (NTD) to uncover musical patterns and structure in pop songs in their audio form. Exploiting the fact that NTD tends to express the content of bars as linear combinations of a few patterns, we illustrate the ability of the decomposition to capture and single out repeated motifs in the corresponding compressed space, which can be interpreted from a musical viewpoint. The resulting features also turn out to be efficient for structural segmentation, leading to experimental results on the RWC Pop data set which are potentially challenging state-of-the-art approaches that rely on extensive example-based learning schemes.      
### 78.Exploring Deep Learning for Joint Audio-Visual Lip Biometrics  [ :arrow_down: ](https://arxiv.org/pdf/2104.08510.pdf)
>  Audio-visual (AV) lip biometrics is a promising authentication technique that leverages the benefits of both the audio and visual modalities in speech communication. Previous works have demonstrated the usefulness of AV lip biometrics. However, the lack of a sizeable AV database hinders the exploration of deep-learning-based audio-visual lip biometrics. To address this problem, we compile a moderate-size database using existing public databases. Meanwhile, we establish the DeepLip AV lip biometrics system realized with a convolutional neural network (CNN) based video module, a time-delay neural network (TDNN) based audio module, and a multimodal fusion module. Our experiments show that DeepLip outperforms traditional speaker recognition models in context modeling and achieves over 50% relative improvements compared with our best single modality baseline, with an equal error rate of 0.75% and 1.11% on the test datasets, respectively.      
### 79.MIMO Self-attentive RNN Beamformer for Multi-speaker Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2104.08450.pdf)
>  Recently, our proposed recurrent neural network (RNN) based all deep learning minimum variance distortionless response (ADL-MVDR) beamformer method yielded superior performance over the conventional MVDR by replacing the matrix inversion and eigenvalue decomposition with two <a class="link-external link-http" href="http://RNNs.In" rel="external noopener nofollow">this http URL</a> this work, we present a self-attentive RNN beamformer to further improve our previous RNN-based beamformer by leveraging on the powerful modeling capability of self-attention. Temporal-spatial self-attention module is proposed to better learn the beamforming weights from the speech and noise spatial covariance matrices. The temporal self-attention module could help RNN to learn global statistics of covariance matrices. The spatial self-attention module is designed to attend on the cross-channel correlation in the covariance matrices. Furthermore, a multi-channel input with multi-speaker directional features and multi-speaker speech separation outputs (MIMO) model is developed to improve the inference efficiency.The evaluations demonstrate that our proposed MIMO self-attentive RNN beamformer improves both the automatic speech recognition (ASR) accuracy and the perceptual estimation of speech quality (PESQ) against prior arts.      
### 80.Identification of mental fatigue in language comprehension tasks based on EEG and deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.08337.pdf)
>  Mental fatigue increases the risk of operator error in language comprehension tasks. In order to prevent operator performance degradation, we used EEG signals to assess the mental fatigue of operators in human-computer systems. This study presents an experimental design for fatigue detection in language comprehension tasks. We obtained EEG signals from a 14-channel wireless EEG detector in 15 healthy participants. Each participant was given a cognitive test of a language comprehension task, in the form of multiple choice questions, in which pronoun references were selected between nominal and surrogate sentences. In this paper, the 2400 EEG fragments collected are divided into three data sets according to different utilization rates, namely 1200s data set with 50% utilization rate, 1500s data set with 62.5% utilization rate, and 1800s data set with 75% utilization rate. In the aspect of feature extraction, different EEG features were extracted, including time domain features, frequency domain features and entropy features, and the effects of different features and feature combinations on classification accuracy were explored. In terms of classification, we introduced the Convolutional Neural Network (CNN) method as the preferred method, It was compared with Least Squares Support Vector Machines(LSSVM),Support Vector Machines(SVM),Logistic Regression (LR), Random Forest(RF), Naive Bayes (NB), K-Nearest Neighbor (KNN) and Decision Tree(DT).According to the results, the classification accuracy of convolutional neural network (CNN) is higher than that of other classification methods. The classification results show that the classification accuracy of 1200S dataset is higher than the other two datasets. The combination of Frequency and entropy feature and CNN has the highest classification accuracy, which is 85.34%.      
### 81.Multitask Learning for VVC Quality Enhancement and Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2104.08319.pdf)
>  The latest video coding standard, called versatile video coding (VVC), includes several novel and refined coding tools at different levels of the coding chain. These tools bring significant coding gains with respect to the previous standard, high efficiency video coding (HEVC). However, the encoder may still introduce visible coding artifacts, mainly caused by coding decisions applied to adjust the bitrate to the available bandwidth. Hence, pre and post-processing techniques are generally added to the coding pipeline to improve the quality of the decoded video. These methods have recently shown outstanding results compared to traditional approaches, thanks to the recent advances in deep learning. Generally, multiple neural networks are trained independently to perform different tasks, thus omitting to benefit from the redundancy that exists between the models. In this paper, we investigate a learning-based solution as a post-processing step to enhance the decoded VVC video quality. Our method relies on multitask learning to perform both quality enhancement and super-resolution using a single shared network optimized for multiple degradation levels. The proposed solution enables a good performance in both mitigating coding artifacts and super-resolution with fewer network parameters compared to traditional specialized architectures.      
