# ArXiv eess --Wed, 14 Apr 2021
### 1.Differential chaos shift keying-based wireless power transfer  [ :arrow_down: ](https://arxiv.org/pdf/2104.06315.pdf)
>  In this work, we investigate differential chaos shift keying (DCSK), a communication-based waveform, in the context of wireless power transfer (WPT). Particularly, we present a DCSK-based WPT architecture, that employs an analog correlator at the receiver in order to boost the energy harvesting (EH) performance. By taking into account the nonlinearities of the EH process, we derive closed-form analytical expressions for the peak-to-average-power-ratio of the received signal as well as the harvested power. Nontrivial design insights are provided, where it is shown how the parameters of the transmitted waveform affects the EH performance. Furthermore, it is demonstrated that the employment of a correlator at the receiver achieves significant EH gains in DCSK-based WPT systems.      
### 2.Energy-Efficient 3D Deployment of Aerial Access Points in a UAV Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2104.06314.pdf)
>  In this letter, we propose an energy-efficient 3-dimensional placement of multiple aerial access points (AAPs), in the desired area, acting as flying base stations for uplink communication from a set of ground user equipment (UE). The globally optimal energy-efficient vertical position of AAPs is derived analytically by considering the inter-cell interference and AAP energy consumption. The horizontal position of AAPs which maximize the packing density of the AAP coverage area are determined using a novel regular polygon-based AAP placement algorithm. We also determine the maximum number of non-interfering AAPs that can be placed in the desired area. The effect of the AAP energy consumption on the optimal placement and the analytic findings are verified via numerical simulations.      
### 3.10,000 km Straight-line Transmission using a Real-time Software-defined GPU-Based Receiver  [ :arrow_down: ](https://arxiv.org/pdf/2104.06311.pdf)
>  Real-time operation of a software-defined, GPU-based optical receiver is demonstrated over a 100-span straight-line optical link. Performance of minimum-phase Kramers-Kronig 4-, 8-, 16-, 32-, and 64-QAM signals are evaluated at various distances.      
### 4.Exploration of Spanish Olive Oil Quality with a Miniaturized Low-Cost Fluorescence Sensor and Machine Learning Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2104.06310.pdf)
>  Extra virgin olive oil (EVOO) is the highest quality of olive oil and is characterized by highly beneficial nutritional properties. The large increase in both consumption and fraud, for example through adulteration, creates new challenges and an increasing demand for developing new quality assessment methodologies that are easier and cheaper to perform. As of today, the determination of olive oil quality is performed by producers through chemical analysis and organoleptic evaluation. The chemical analysis requires the advanced equipment and chemical knowledge of certified laboratories, and has therefore a limited accessibility. In this work a minimalist, portable and low-cost sensor is presented, which can perform olive oil quality assessment using fluorescence spectroscopy. The potential of the proposed technology is explored by analyzing several olive oils of different quality levels, EVOO, virgin olive oil (VOO), and lampante olive oil (LOO). The spectral data were analyzed using a large number of machine learning methods, including artificial neural networks. The analysis performed in this work demonstrates the possibility of performing classification of olive oil in the three mentioned classes with an accuracy of 100$\%$. These results confirm that this minimalist low-cost sensor has the potential of substituting expensive and complex chemical analysis.      
### 5.Signal Processing and Machine Learning Techniques for Terahertz Sensing: An Overview  [ :arrow_down: ](https://arxiv.org/pdf/2104.06309.pdf)
>  Following the recent progress in Terahertz (THz) signal generation and radiation methods, joint THz communications and sensing applications are shaping the future of wireless systems. Towards this end, THz spectroscopy is expected to be carried over user equipment devices to identify material and gaseous components of interest. THz-specific signal processing techniques should complement this re-surged interest in THz sensing for efficient utilization of the THz band. In this paper, we present an overview of these techniques, with an emphasis on signal pre-processing (standard normal variate normalization, min-max normalization, and Savitzky-Golay filtering), feature extraction (principal component analysis, partial least squares, t-distributed stochastic neighbor embedding, and nonnegative matrix factorization), and classification techniques (support vector machines, k-nearest neighbor, discriminant analysis, and naive Bayes). We also address the effectiveness of deep learning techniques by exploring their promising sensing capabilities at the THz band. Lastly, we investigate the performance and complexity trade-offs of the studied methods in the context of joint communications and sensing; we motivate the corresponding use-cases, and we present few future research directions in the field.      
### 6.SFE-Net: EEG-based Emotion Recognition with Symmetrical Spatial Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2104.06308.pdf)
>  Emotion recognition based on EEG (electroencephalography) has been widely used in human-computer interaction, distance education and health care. However, the conventional methods ignore the adjacent and symmetrical characteristics of EEG signals, which also contain salient information related to emotion. In this paper, we present a spatial folding ensemble network (SFENet) for EEG feature extraction and emotion recognition. Firstly, for the undetected area between EEG electrodes, we employ an improved Bicubic-EEG interpolation algorithm for EEG channel information completion, which allows us to extract a wider range of adjacent space features. Then, motivated by the spatial symmetry mechanism of human brain, we fold the input EEG channel data with five different symmetrical strategies: the left-right folds, the right-left folds, the top-bottom folds, the bottom-top folds, and the entire double-sided brain folding, which enable the proposed network to extract the information of space features of EEG signals more effectively. Finally, 3DCNN based spatial and temporal extraction and multi voting strategy of ensemble Learning are employed to model a new neural network. With this network, the spatial features of different symmetric folding signlas can be extracted simultaneously, which greatly improves the robustness and accuracy of feature recognition. The experimental results on DEAP and SEED data sets show that the proposed algorithm has comparable performance in term of recognition accuracy.      
### 7.Stealthy False Data Injection Attack Detection in Smart Grids with Uncertainties: A Deep Transfer Learning Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2104.06307.pdf)
>  Most traditional false data injection attack (FDIA) detection approaches rely on static system parameters or a single known snapshot of dynamic ones. However, such a setting significantly weakens the practicality of these approaches when facing the fact that the system parameters are dynamic and cannot be accurately known during operation due to the presence of uncertainties in practical smart grids. In this paper, we propose an FDIA detection mechanism from the perspective of transfer learning. Specifically, the known initial/approximate system is treated as a source domain, which provides abundant simulated normal and attack data. The real world's unknown running system is taken as a target domain where sufficient real normal data are collected for tracking the latest system states online. The designed transfer strategy that aims at making full use of data in hand is divided into two optimization stages. In the first stage, a deep neural network (DNN) is built by simultaneously optimizing several well-designed terms with both simulated data and real data, and then it is fine-tuned via real data in the second stage. Several case studies on the IEEE 14-bus power system verify the effectiveness of the proposed mechanism.      
### 8.Port Parameter Extraction Based Self Consistent Coupled EM-Circuit FEM Solvers  [ :arrow_down: ](https://arxiv.org/pdf/2104.06306.pdf)
>  Self consistent solution to electromagnetic (EM)-circuit systems is of significant interest for a number of applications. This has resulted in exhaustive research on means to couple them. In time domain, this typically involves a tight integration with field and non-linear circuit solvers. This is in stark contrast to coupled analysis of linear/weakly non-linear circuits and EM systems in frequency domain. Here, one typically extracts equivalent port parameters that are then fed into the circuit solver. Such an approach has several advantages; (a) the number of ports is typically smaller than the number of degrees of freedom, resulting in cost savings; (b) is circuit agnostic. A port representation is tantamount to an impulse response of the linear EM system. In time domain, the deconvolution required to effect this is unstable. Recently, a novel approach was developed for time domain integral equations to overcome this bottleneck. We extend this approach to time domain finite element method, and demonstrate its utility via a number of examples; significantly, we demonstrate that the coupled and port parameter solutions are identical to desired precision for non-linear circuit systems.      
### 9.Lifetime Optimization of Dense Wireless Sensor Networks Using Continuous Ring-sector Model  [ :arrow_down: ](https://arxiv.org/pdf/2104.06304.pdf)
>  Wireless sensor networks (WSNs) are becoming increasingly utilized in applications that require remote collection of data on environmental conditions. In particular dense WSNs are emerging as an important sensing platforms for the Internet of Things (IoT). WSNs are able to generate huge volumes of raw data, which require network structuring and efficient collaboration between nodes to ensure efficient transmission. In order to reduce the amount of data carried in the network, data aggregation is used in WSNs to define a policy of data fusion and compression. In this paper, we investigate a model for data aggregation in a dense {WSN} with a single sink. The model divides a circular coverage region centered at the sink into patches which are intersections of sectors of concentric rings, and data in each patch is aggregated at a single node before transmission. Nodes only communicate with other nodes in the same sector. Based on these assumptions, we formulate a linear programming problem to maximize system lifetime by minimizing the maximum proportionate energy consumption over all nodes. Under a wide variety of conditions, the optimal solution employs two transmissions mechanisms: direct transmission, in which nodes send information directly to the sink; and stepwise transmission, in which nodes transmit information to adjacent nodes. An exact formula is given for the proportionate energy consumption rate of the network. Asymptotic forms of this exact solution are also derived, and are verified to agree with the linear programming solution. We investigate three strategies for improving system lifetime: nonuniform energy and information density; iterated compression; and modifications of rings. We conclude that iterated compression has the biggest effect in increasing system lifetime.      
### 10.A Sufficient Condition to Guarantee Non-Simultaneous Charging and Discharging of Household Battery Energy Storage  [ :arrow_down: ](https://arxiv.org/pdf/2104.06267.pdf)
>  In this letter, we model the day-ahead price-based demand response of a residential household with battery energy storage and other controllable loads, as a convex optimization problem. Further using duality theory and Karush-Kuhn-Tucker optimality conditions, we derive a sufficient criterion which guarantees non-simultaneous charging and discharging of the battery energy storage, without explicitly modelling it as a constraint      
### 11.Lossless Coding of Light Fields based on 4D Minimum Rate Predictors  [ :arrow_down: ](https://arxiv.org/pdf/2104.06252.pdf)
>  Common representations of light fields use four-dimensional data structures, where a given pixel is closely related not only to its spatial neighbours within the same view, but also to its angular neighbours, co-located in adjacent views. Such structure presents increased redundancy between pixels, when compared with regular single-view images. Then, these redundancies are exploited to obtain compressed representations of the light field, using prediction algorithms specifically tailored to estimate pixel values based on both spatial and angular references. This paper proposes new encoding schemes which take advantage of the four-dimensional light field data structures to improve the coding performance of Minimum Rate Predictors. The proposed methods expand previous research on lossless coding beyond the current state-of-the-art. The experimental results, obtained using both traditional datasets and others more challenging, show bit-rate savings no smaller than 10%, when compared with existing methods for lossless light field compression.      
### 12.A State-of-the-art Survey of Artificial Neural Networks for Whole-slide Image Analysis:from Popular Convolutional Neural Networks to Potential Visual Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2104.06243.pdf)
>  In recent years, with the advancement of computer-aided diagnosis (CAD) technology and whole slide image (WSI), histopathological WSI has gradually played a crucial aspect in the diagnosis and analysis of diseases. To increase the objectivity and accuracy of pathologists' work, artificial neural network (ANN) methods have been generally needed in the segmentation, classification, and detection of histopathological WSI. In this paper, WSI analysis methods based on ANN are reviewed. Firstly, the development status of WSI and ANN methods is introduced. Secondly, we summarize the common ANN methods. Next, we discuss publicly available WSI datasets and evaluation metrics. These ANN architectures for WSI processing are divided into classical neural networks and deep neural networks (DNNs) and then analyzed. Finally, the application prospect of the analytical method in this field is discussed. The important potential method is Visual Transformers.      
### 13.Decentralized Time and Energy-Optimal Control of Connected and Automated Vehicles in a Roundabout  [ :arrow_down: ](https://arxiv.org/pdf/2104.06242.pdf)
>  The paper considers the problem of controlling Connected and Automated Vehicles (CAVs) traveling through a three-entry roundabout so as to jointly minimize both the travel time and the energy consumption while providing speed-dependent safety guarantees, as well as satisfying velocity and acceleration constraints. We first design a systematic approach to dynamically determine the safety constraints and derive the unconstrained optimal control solution. A joint optimal control and barrier function (OCBF) method is then applied to efficiently obtain a controller that optimally track the unconstrained optimal solution while guaranteeing all the constraints. Simulation experiments are performed to compare the optimal controller to a baseline of human-driven vehicles showing effectiveness under symmetric and asymmetric roundabout configurations, balanced and imbalanced traffic rates and different sequencing rules for CAVs.      
### 14.Latent Correlation Representation Learning for Brain Tumor Segmentation with Missing MRI Modalities  [ :arrow_down: ](https://arxiv.org/pdf/2104.06231.pdf)
>  Magnetic Resonance Imaging (MRI) is a widely used imaging technique to assess brain tumor. Accurately segmenting brain tumor from MR images is the key to clinical diagnostics and treatment planning. In addition, multi-modal MR images can provide complementary information for accurate brain tumor segmentation. However, it's common to miss some imaging modalities in clinical practice. In this paper, we present a novel brain tumor segmentation algorithm with missing modalities. Since it exists a strong correlation between multi-modalities, a correlation model is proposed to specially represent the latent multi-source correlation. Thanks to the obtained correlation representation, the segmentation becomes more robust in the case of missing modality. First, the individual representation produced by each encoder is used to estimate the modality independent parameter. Then, the correlation model transforms all the individual representations to the latent multi-source correlation representations. Finally, the correlation representations across modalities are fused via attention mechanism into a shared representation to emphasize the most important features for segmentation. We evaluate our model on BraTS 2018 and BraTS 2019 dataset, it outperforms the current state-of-the-art methods and produces robust results when one or more modalities are missing.      
### 15.COVID-19 detection using chest X-rays: is lung segmentation important for generalization?  [ :arrow_down: ](https://arxiv.org/pdf/2104.06176.pdf)
>  We evaluated the generalization capability of deep neural networks (DNNs), trained to classify chest X-rays as COVID-19, normal or pneumonia, using a relatively small and mixed dataset. <br>We proposed a DNN architecture to perform lung segmentation and classification. It stacks a segmentation module (U-Net), an original intermediate module and a classification module (DenseNet201). We compared it to a DenseNet201. <br>To evaluate generalization, we tested the DNNs with an external dataset (from distinct localities) and used Bayesian inference to estimate the probability distributions of performance metrics, like F1-Score. <br>Our proposed DNN achieved 0.917 AUC on the external test dataset, and the DenseNet, 0.906. Bayesian inference indicated mean accuracy of 76.1% and [0.695, 0.826] 95% HDI with segmentation and, without segmentation, 71.7% and [0.646, 0.786]. <br>We proposed a novel DNN evaluation technique, using Layer-wise Relevance Propagation (LRP) and the Brixia score. LRP heatmaps indicated that areas where radiologists found strong COVID-19 symptoms and attributed high Brixia scores are the most important for the stacked DNN classification. <br>External validation showed smaller accuracies than internal validation, indicating dataset bias, which segmentation reduces. Performance in the external dataset and LRP analysis suggest that DNNs can be trained in small and mixed datasets and detect COVID-19.      
### 16.Finite-dimensional output stabilization of linear diffusion-reaction systems -- a small-gain approach  [ :arrow_down: ](https://arxiv.org/pdf/2104.06102.pdf)
>  A small-gain approach is proposed to analyze closed-loop stability of linear diffusion-reaction systems under finite-dimensional observer-based state feedback control. For this, the decomposition of the infinite-dimensional system into a finite-dimensional slow subsystem used for design and an infinite-dimensional residual fast subsystem is considered. The effect of observer spillover in terms of a particular (dynamic) interconnection of the subsystems is thoroughly analyzed for in-domain and boundary control as well as sensing. This leads to the application of a small-gain theorem for interconnected systems based on input-to-output stability and unbounded observability properties. Moreover, an approach is presented for the computation of the required dimension of the slow subsystem used for controller design. Simulation scenarios for both scalar and coupled linear diffusion-reaction systems are used to underline the theoretical assessment and to give insight into the resulting properties of the interconnected systems.      
### 17.Reduced order modelling for spatial-temporal temperature and property estimation in a multi-stage hot sheet metal forming process  [ :arrow_down: ](https://arxiv.org/pdf/2104.06098.pdf)
>  A concise approach is proposed to determine a reduced order control design oriented dynamical model of a multi-stage hot sheet metal forming process starting from a high-dimensional coupled thermo-mechanical model. The obtained reduced order nonlinear parametric model serves as basis for the design of an Extended Kalman filter to estimate the spatial-temporal temperature distribution in the sheet metal blank during the forming process based on sparse local temperature measurements. To address modeling and approximation errors and to capture physical effects neglected during the approximation such as phase transformation from austenite to martensite a disturbance model is integrated into the Kalman filter to achieve joint state and disturbance estimation. The extension to spatial-temporal property estimation is introduced. The approach is evaluated for a hole-flanging process using a thermo-mechanical simulation model evaluated using LS-DYNA. Here, the number of states is reduced from approximately 17 000 to 30 while preserving the relevant dynamics and the computational time is 1000 times shorter. The performance of the combined temperature and disturbance estimation is validated in different simulation scenarios with three spatially fixed temperature measurements.      
### 18.Tractable robust MPC design based on nominal predictions  [ :arrow_down: ](https://arxiv.org/pdf/2104.06088.pdf)
>  One of the more popular approaches in the field of robust model predictive control are the ones based on nominal predictions. This paper presents a novel formulation of this class of controller with proven input-to-state stability and robust constraint satisfaction. Its advantages, with respect to previous formulations, are (i) the design of its main ingredients are tractable for medium to large-sized systems, and (ii) the resulting optimization problem is simpler than the ones obtained in other formulations. Furthermore, under certain conditions, the terminal constraint can be eliminated, which facilitates its application to large-scale systems. We provide procedures for the design of the main ingredients of the proposed formulation, as well as discuss some practical considerations for its implementation. Finally, we show numerical closed-loop results of its application to a multivariable chemical plant.      
### 19.Spatiotemporal Entropy Model is All You Need for Learned Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2104.06083.pdf)
>  The framework of dominant learned video compression methods is usually composed of motion prediction modules as well as motion vector and residual image compression modules, suffering from its complex structure and error propagation problem. Approaches have been proposed to reduce the complexity by replacing motion prediction modules with implicit flow networks. Error propagation aware training strategy is also proposed to alleviate incremental reconstruction errors from previously decoded frames. Although these methods have brought some improvement, little attention has been paid to the framework itself. Inspired by the success of learned image compression through simplifying the framework with a single deep neural network, it is natural to expect a better performance in video compression via a simple yet appropriate framework. Therefore, we propose a framework to directly compress raw-pixel frames (rather than residual images), where no extra motion prediction module is required. Instead, an entropy model is used to estimate the spatiotemporal redundancy in a latent space rather than pixel level, which significantly reduces the complexity of the framework. Specifically, the whole framework is a compression module, consisting of a unified auto-encoder which produces identically distributed latents for all frames, and a spatiotemporal entropy estimation model to minimize the entropy of these latents. Experiments showed that the proposed method outperforms state-of-the-art (SOTA) performance under the metric of multiscale structural similarity (MS-SSIM) and achieves competitive results under the metric of PSNR.      
### 20.Unmanned Aerial Vehicle and Optimal Relay for Extending Coverage in Post-Disaster Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2104.06037.pdf)
>  The malfunction or interruption of wireless coverage services has been shown to increase the mortality rate during natural disasters. Wireless coverage by an unmanned aerial vehicle (UAV) provides network coverage to ground user devices during and post-disaster events. The relay hops receive wireless coverage and can be forwarded to user devices that are out of coverage allowing reliable connectivity for large-scale user devices. This work evaluates the optimal relay hops performance to improve wireless coverage services and establish connectivity in post-disaster scenarios. The results demonstrate the UAV line of sights understanding to select an optimal relay for improving wireless coverage services. The path loss probability and system capacity were all affected by the user device distance and relay densities. The optimal relay hop distance and the UAV positions static are also investigated to improve coverage likelihood which could be especially useful for UAV deployment design. It is found that the dense relays node in UAV systems enhances the capacity coverage area and energy efficiency by decentralized connectivity through a multihop device to device wireless network.      
### 21.Joint Secure Design of Downlink and D2D Cooperation Strategies for Multi-User Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.06003.pdf)
>  This work studies the role of inter-user device-to-device (D2D) cooperation for improving physical-layer secret communication in multi-user downlink systems. It is assumed that there are out-of-band D2D channels, on each of which a selected legitimate user transmits an amplified version of the received downlink signal to other legitimate users. A key technical challenge for designing such systems is that eavesdroppers can overhear downlink as well as D2D cooperation signals. We tackle the problem of jointly optimizing the downlink precoding, artificial noise covariance, and amplification coefficients that maximize the minimum rate. An iterative alternating optimization algorithm is proposed based on the matrix fractional programming. Numerical results confirm the performance gains of the proposed D2D cooperation scheme compared to benchmark secret communication schemes.      
### 22.Shared-phase-dedicated-lane based intersection control with mixed traffic of human-driven vehicles and connected and automated vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2104.05956.pdf)
>  Connected and automated vehicles (CAVs) and human-driven vehicles (HVs) are expected to coexist in the near future. CAV-dedicated lanes and phases have been explored to handle the uncertainty in the driving behavior of HVs in the mixed traffic environment. However, CAV-dedicated phases could significantly sacrifice HV benefits. This study proposes a shared-phase-dedicated-lane (SPDL)-based traffic control model at isolated intersections under the mixed traffic environment. Left-turn and through CAVs share CAV-dedicated lanes and cross the intersection during the shared phases with HVs. A three-level optimization model is developed. At the upper level, a standard NEMA (National Electrical Manufacturers Association) ring barrier structure is used for the signal optimization and barrier durations are optimized by dynamic programming to minimize the total vehicle delay. At the middle level, phase sequence and phase durations are optimized by enumeration for the given barrier from the upper level and the minimum vehicle delay is fed to the upper level. At the lower level, CAV platooning in the buffer zone and trajectory planning in the passing zone are conducted based on the signal timings of the barrier from the middle level and the travel time of CAVs is fed to the middle level. A rolling-horizon scheme is further designed for the dynamical implementation of the proposed model with time-varying traffic conditions. Numerical studies validate the advantages of the SPDL-based control over the blue-phase based control in previous studies in terms of average vehicle delay and intersection capacity. Further, the SPDL-based model is extended to serve as an alternative approach without the buffer zone.      
### 23.Orthogonal Time Sequency Multiplexing Modulation: Analysis and Low-Complexity Receiver Design  [ :arrow_down: ](https://arxiv.org/pdf/2104.05939.pdf)
>  This paper proposes orthogonal time sequency multiplexing (OTSM), a novel single carrier modulation scheme that places information symbols in the delay-sequency domain followed by a cascade of time-division multiplexing (TDM) and Walsh-Hadamard sequence multiplexing. Thanks to the Walsh Hadamard transform (WHT), the modulation and demodulation do not require complex domain multiplications. For the proposed OTSM, we first derive the input-output relation in the delay-sequency domain and present a low complexity detection method taking advantage of zero-padding. We demonstrate via simulations that OTSM offers high performance gains over orthogonal frequency division multiplexing (OFDM) and similar performance to orthogonal time frequency space (OTFS), but at lower complexity owing to WHT. Then we propose a low complexity time-domain channel estimation method. Finally, we show how to include an outer error control code and a turbo decoder to improve error performance of the coded system.      
### 24.Bi-level Off-policy Reinforcement Learning for Volt/VAR Control Involving Continuous and Discrete Devices  [ :arrow_down: ](https://arxiv.org/pdf/2104.05902.pdf)
>  In Volt/Var control (VVC) of active distribution networks(ADNs), both slow timescale discrete devices (STDDs) and fast timescale continuous devices (FTCDs) are involved. The STDDs such as on-load tap changers (OLTC) and FTCDs such as distributed generators should be coordinated in time sequence. Such VCC is formulated as a two-timescale optimization problem to jointly optimize FTCDs and STDDs in ADNs. Traditional optimization methods are heavily based on accurate models of the system, but sometimes impractical because of their unaffordable effort on modelling. In this paper, a novel bi-level off-policy reinforcement learning (RL) algorithm is proposed to solve this problem in a model-free manner. A Bi-level Markov decision process (BMDP) is defined to describe the two-timescale VVC problem and separate agents are set up for the slow and fast timescale sub-problems. For the fast timescale sub-problem, we adopt an off-policy RL method soft actor-critic with high sample efficiency. For the slow one, we develop an off-policy multi-discrete soft actor-critic (MDSAC) algorithm to address the curse of dimensionality with various STDDs. To mitigate the non-stationary issue existing the two agents' learning processes, we propose a multi-timescale off-policy correction (MTOPC) method by adopting importance sampling technique. Comprehensive numerical studies not only demonstrate that the proposed method can achieve stable and satisfactory optimization of both STDDs and FTCDs without any model information, but also support that the proposed method outperforms existing two-timescale VVC methods.      
### 25.Unifying domain adaptation and self-supervised learning for CXR segmentation via AdaIN-based knowledge distillation  [ :arrow_down: ](https://arxiv.org/pdf/2104.05892.pdf)
>  As the segmentation labels are scarce, extensive researches have been conducted to train segmentation networks without labels or with only limited labels. In particular, domain adaptation, self-supervised learning, and teacher-student architecture have been intro- duced to distill knowledge from various tasks to improve the segmentation performance. However, these approaches appear different from each other, so it is not clear how these seemingly different approaches can be combined for better performance. Inspired by the recent StarGANv2 for multi-domain image translation, here we propose a novel seg- mentation framework via AdaIN-based knowledge distillation, where a single generator with AdaIN layers is trained along with the AdaIN code generator and style encoder so that the generator can perform both domain adaptation and segmentation. Specifically, our framework is designed to deal with difficult situations in chest X-ray (CXR) seg- mentation tasks where segmentation masks are only available for normal CXR data, but the trained model should be applied for both normal and abnormal CXR images. Since a single generator is used for abnormal to normal domain conversion and segmentation by simply changing the AdaIN codes, the generator can synergistically learn the com- mon features to improve segmentation performance. Experimental results using CXR data confirm that the trained network can achieve the state-of-the art segmentation per- formance for both normal and abnormal CXR images.      
### 26.Physics-oriented learning of nonlinear Schrödinger equation: optical fiber loss and dispersion profile identification  [ :arrow_down: ](https://arxiv.org/pdf/2104.05890.pdf)
>  In optical fiber communication, system identification (SI) for the nonlinear Schrödinger equation (NLSE) has long been studied mainly for fiber nonlinearity compensation (NLC). One recent line of inquiry to combine a behavioral-model approach like digital backpropagation (DBP) and a data-driven approach like neural network (NN). These works are aimed for more NLC gain; however, by directing our attention to the learned parameters in such a SI process, system status information, i.e., optical fiber parameters, will possibly be extracted. Here, we show that the model-based optimization and interpretable nature of the learned parameters in NN-based DBP enable transmission line monitoring, fully extracting the actual in-line NLSE parameter distributions. Specifically, we demonstrate that longitudinal loss and dispersion profiles along a multi-span link can be obtained at once, directly from data-carrying signals without any dedicated analog devices such as optical time-domain reflectometry. We apply the method to a long-haul (~2,080 km) link and various link conditions are tested, including excess loss inserted, different fiber input power, and non-uniform level diagram. The measurement performance is also investigated in terms of measurement range, accuracy, and fiber launch power. These results provide a path toward simplified and automated network management as another application of DBP.      
### 27.Fibro-CoSANet: Pulmonary Fibrosis Prognosis Prediction using a Convolutional Self Attention Network  [ :arrow_down: ](https://arxiv.org/pdf/2104.05889.pdf)
>  Idiopathic pulmonary fibrosis (IPF) is a restrictive interstitial lung disease that causes lung function decline by lung tissue scarring. Although lung function decline is assessed by the forced vital capacity (FVC), determining the accurate progression of IPF remains a challenge. To address this challenge, we proposed Fibro-CoSANet, a novel end-to-end multi-modal learning-based approach, to predict the FVC decline. Fibro-CoSANet utilized CT images and demographic information in convolutional neural network frameworks with a stacked attention layer. Extensive experiments on the OSIC Pulmonary Fibrosis Progression Dataset demonstrated the superiority of our proposed Fibro-CoSANet by achieving the new state-of-the-art modified Laplace Log-Likelihood score of -6.68. This network may benefit research areas concerned with designing networks to improve the prognostic accuracy of IPF. The source-code for Fibro-CoSANet is available at: \url{<a class="link-external link-https" href="https://github.com/zabir-nabil/Fibro-CoSANet" rel="external noopener nofollow">this https URL</a>}.      
### 28.Jittering Effects Analysis and Beam Training Design for UAV Millimeter Wave Communications  [ :arrow_down: ](https://arxiv.org/pdf/2104.05872.pdf)
>  Jittering effects significantly degrade the performance of UAV communications especially in millimeter-wave (mmWave) band. To investigate and mitigate the impacts of UAV jitter to mmWave communications, we firstly model UAV mmWave channel based on the geometric relationship between element antennas of the uniform planar arrays (UPAs) in receiver side and transmitter side, and we incorporate the jittering effects to our channel model through extracting the relationship between UAV attitude &amp; position and angle of arrival (AoA) &amp; angle of departure (AoD) of the UPAs. Then, based on the extracted relationship, we propose to utilize UAV navigation information to obtain a rough estimation of AoA and AoD, and we also analyze the impact of AoA and AoD estimation error to UAV beamforming. Finally, we propose a direction-constrained beam training scheme to refine the AoA/AoD estimation. Particularly, we construct a partially random sensing matrix to measure the channel within a narrow angle range that is centered at the aforementioned rough estimate of AoA/AoD. Numerical results show that our proposed UAV beam training scheme with navigation information is able to fast and accurately estimate the AoA/AoD fluctuation caused by UAV jitter.      
### 29.Evidence-based Prescriptive Analytics, CAUSAL Digital Twin and a Learning Estimation Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2104.05828.pdf)
>  Evidence-based Prescriptive Analytics (EbPA) is necessary to determine optimal operational set-points that will improve business productivity. EbPA results from what-if analysis and counterfactual experimentation on CAUSAL Digital Twins (CDTs) that quantify cause-effect relationships in the DYNAMICS of a system of connected assets. We describe the basics of Causality and Causal Graphs and develop a Learning Causal Digital Twin (LCDT) solution; our algorithm uses a simple recurrent neural network with some innovative modifications incorporating Causal Graph simulation. Since LCDT is a learning digital twin where parameters are learned online in real-time with minimal pre-configuration, the work of deploying digital twins will be significantly simplified. A proof-of-principle of LCDT was conducted using real vibration data from a system of bearings; results of causal factor estimation, what-if analysis study and counterfactual experiment are very encouraging.      
### 30.Model Reduction in Capacity Expansion Planning Problems via Renewable Generation Site Selection  [ :arrow_down: ](https://arxiv.org/pdf/2104.05792.pdf)
>  The accurate representation of variable renewable generation (RES, e.g., wind, solar PV) assets in capacity expansion planning (CEP) studies is paramount to capture spatial and temporal correlations that may exist between sites and impact both power system design and operation. However, it typically has a high computational cost. This paper proposes a method to reduce the spatial dimension of CEP problems while preserving an accurate representation of renewable energy sources. A two-stage approach is proposed to this end. In the first stage, relevant sites are identified via a screening routine that discards the locations with little impact on system design. In the second stage, the subset of relevant RES sites previously identified is used in a CEP problem to determine the optimal configuration of the power system. The proposed method is tested on a realistic EU case study and its performance is benchmarked against a CEP set-up in which the entire set of candidate RES sites is available. The method shows great promise, with the screening stage consistently identifying 90% of the optimal RES sites while discarding up to 54% of the total number of candidate locations. This leads to a peak memory reduction of up to 41% and solver runtime gains between 31% and 46%, depending on the weather year considered.      
### 31.Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling  [ :arrow_down: ](https://arxiv.org/pdf/2104.05778.pdf)
>  This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set.      
### 32.Staircase Selective Harmonic Elimination in Multilevel Inverters to Achieve Wide Output Voltage Range  [ :arrow_down: ](https://arxiv.org/pdf/2104.05759.pdf)
>  Multilevel inverters (MLIs) are popular because of their advantages such as improved output voltage quality, lower switching losses, low EMI, and ability to handle higher voltage and power levels. To generate the desired output voltage in the MLIs, there are different switching methods including multicarrier pulse-width modulation (PWM), selective harmonic elimination (SHE), and multilevel space-vector PWM (SVPWM) methods. The SHE method minimizes the number of switching and hence the switching losses. Also, the method eliminates or attenuates specified low-order harmonics and adjusts the fundamental output voltage to the desired value. However, problems arise in the low modulation indexes where the harmonic distortion of the output voltage increases considerably. To solve the problem, this paper proposes a method to adjust the DC voltage when the low output voltage is required. Therefore, the modulation index will increase and the output voltage quality will be improved. The proposed method is verified on a 7-level Cascaded H-Bridge (CHB) inverter using the SHE method solved by particle swarm optimization (PSO) algorithm.      
### 33.Real-time Forecast Models for TBM Load Parameters Based on Machine Learning Methods  [ :arrow_down: ](https://arxiv.org/pdf/2104.06353.pdf)
>  Because of the fast advance rate and the improved personnel safety, tunnel boring machines (TBMs) have been widely used in a variety of tunnel construction projects. The dynamic modeling of TBM load parameters (including torque, advance rate and thrust) plays an essential part in the design, safe operation and fault prognostics of this complex engineering system. In this paper, based on in-situ TBM operational data, we use the machine-learning (ML) methods to build the real-time forecast models for TBM load parameters, which can instantaneously provide the future values of the TBM load parameters as long as the current data are collected. To decrease the model complexity and improve the generalization, we also apply the least absolute shrinkage and selection (Lasso) method to extract the essential features of the forecast task. The experimental results show that the forecast models based on deep-learning methods, {\it e.g.}, recurrent neural network and its variants, outperform the ones based on the shallow-learning methods, {\it e.g.}, support vector regression and random forest. Moreover, the Lasso-based feature extraction significantly improves the performance of the resultant models.      
### 34.A Distributed Mixed-Integer Framework to Stochastic Optimal Microgrid Control  [ :arrow_down: ](https://arxiv.org/pdf/2104.06346.pdf)
>  In this paper, we consider distributed control of microgrids composed of storages, generators, renewable energy sources, critical and controllable loads. After recalling a Mixed-Integer Linear Programming (MILP) formulation of the optimal control problem associated to the microgrid, we show that this optimization problem can be recast as a distributed optimization problem. Following the approaches proposed in the literature, we formulate a distributed, two-stage stochastic optimization problem associated to the microgrid and solve this problem using a distributed approach for MILPs. The resulting algorithm is able to compute high-quality feasible solutions to the two-stage stochastic problem, for which we provide a theoretical upper bound on the constraint violation. Numerical computations show the efficacy of the proposed solution.      
### 35.Constraint-coupled Optimization with Unknown Costs: A Distributed Primal Decomposition Approach  [ :arrow_down: ](https://arxiv.org/pdf/2104.06341.pdf)
>  In this paper, we present a distributed algorithm for solving convex, constraint-coupled, optimization problems over peer-to-peer networks. We consider a network of processors that aim to cooperatively minimize the sum of local cost functions, subject to individual constraints and to global coupling constraints. The major assumption of this work is that the cost functions are unknown and must be learned online. We propose a fully distributed algorithm, based on a primal decomposition approach, that uses iteratively refined data-driven estimations of the cost functions over the iterations. The algorithm is scalable and maintains private information of agents. We prove that, asymptotically, the distributed algorithm provides the optimal solution of the problem even though the true cost functions are never used within the algorithm. The analysis requires an in-depth exploration of the primal decomposition approach and shows that the distributed algorithm can be thought of as an epsilon-subgradient method applied to a suitable reformulation of the original problem. Finally, numerical computations cor- roborate the theoretical findings and show the efficacy of the proposed approach.      
### 36.Terrain assessment for precision agriculture using vehicle dynamic modelling  [ :arrow_down: ](https://arxiv.org/pdf/2104.06326.pdf)
>  Advances in precision agriculture greatly rely on innovative control and sensing technologies that allow service units to increase their level of driving automation while ensuring at the same time high safety standards. This paper deals with automatic terrain estimation and classification that is performed simultaneously by an agricultural vehicle during normal operations. Vehicle mobility and safety, and the successful implementation of important agricultural tasks including seeding, ploughing, fertilising and controlled traffic depend or can be improved by a correct identification of the terrain that is traversed. The novelty of this research lies in that terrain estimation is performed by using not only traditional appearance-based features, that is colour and geometric properties, but also contact-based features, that is measuring physics-based dynamic effects that govern the vehicleeterrain interaction and that greatly affect its mobility. Experimental results obtained from an all-terrain vehicle operating on different surfaces are presented to validate the system in the field. It was shown that a terrain classifier trained with contact features was able to achieve a correct prediction rate of 85.1%, which is comparable or better than that obtained with approaches using traditional feature sets. To further improve the classification performance, all feature sets were merged in an augmented feature space, reaching, for these tests, 89.1% of correct predictions.      
### 37.Noisy quantum metrology with the assistance of indefinite causal order  [ :arrow_down: ](https://arxiv.org/pdf/2104.06284.pdf)
>  A generic qubit unitary operator affected by depolarizing noise is duplicated and inserted in a quantum switch process realizing a superposition of causal orders. The characterization of the resulting switched quantum channel is worked out for its action on the joint state of the probe-control qubit pair. The switched channel is then specifically investigated for the important metrological task of phase estimation on the noisy unitary operator, with the performance assessed by the Fisher information, classical or quantum. A comparison is made with conventional techniques of estimation where the noisy unitary is directly probed in a one-stage or two-stage cascade with definite order, or several uses of them with two or more qubits. In the switched channel with indefinite order, specific properties are reported, meaningful for estimation and not present with conventional techniques. It is shown that the control qubit, although it never directly interacts with the unitary, can nevertheless be measured alone for effective estimation, while discarding the probe qubit that interacts with the unitary. Also, measurement of the control qubit maintains the possibility of efficient estimation in difficult conditions where conventional estimation becomes less efficient, as with ill-configured input probes, or in blind situations when the axis of the unitary is unknown. Effective estimation by measuring the control qubit remains possible even when the input probe tends to align with the axis of the unitary, or with a fully depolarized input probe, while in these conditions conventional estimation gets inoperative. Measurement of the probe qubit of the switched channel is also shown to add useful capabilities for phase estimation. The results contribute to the analysis of switched quantum channels with indefinite order for information processing, and uncover new possibilities for qubit metrology.      
### 38.Multilingual Transfer Learning for Code-Switched Language and Speech Neural Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2104.06268.pdf)
>  In this thesis, we address the data scarcity and limitations of linguistic theory by proposing language-agnostic multi-task training methods. First, we introduce a meta-learning-based approach, meta-transfer learning, in which information is judiciously extracted from high-resource monolingual speech data to the code-switching domain. The meta-transfer learning quickly adapts the model to the code-switching task from a number of monolingual tasks by learning to learn in a multi-task learning fashion. Second, we propose a novel multilingual meta-embeddings approach to effectively represent code-switching data by acquiring useful knowledge learned in other languages, learning the commonalities of closely related languages and leveraging lexical composition. The method is far more efficient compared to contextualized pre-trained multilingual models. Third, we introduce multi-task learning to integrate syntactic information as a transfer learning strategy to a language model and learn where to code-switch. To further alleviate the aforementioned issues, we propose a data augmentation method using Pointer-Gen, a neural network using a copy mechanism to teach the model the code-switch points from monolingual parallel sentences. We disentangle the need for linguistic theory, and the model captures code-switching points by attending to input words and aligning the parallel words, without requiring any word alignments or constituency parsers. More importantly, the model can be effectively used for languages that are syntactically different, and it outperforms the linguistic theory-based models.      
### 39.CAN Coach: Vehicular Control through Human Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.06264.pdf)
>  This work addresses whether a human-in-the-loop cyber-physical system (HCPS) can be effective in improving the longitudinal control of an individual vehicle in a traffic flow. We introduce the CAN Coach, which is a system that gives feedback to the human-in-the-loop using radar data (relative speed and position information to objects ahead) that is available on the controller area network (CAN). Using a cohort of six human subjects driving an instrumented vehicle, we compare the ability of the human-in-the-loop driver to achieve a constant time-gap control policy using only human-based visual perception to the car ahead, and by augmenting human perception with audible feedback from CAN sensor data. The addition of CAN-based feedback reduces the mean time-gap error by an average of 73%, and also improves the consistency of the human by reducing the standard deviation of the time-gap error by 53%. We remove human perception from the loop using a ghost mode in which the human-in-the-loop is coached to track a virtual vehicle on the road, rather than a physical one. The loss of visual perception of the vehicle ahead degrades the performance for most drivers, but by varying amounts. We show that human subjects can match the velocity of the lead vehicle ahead with and without CAN-based feedback, but velocity matching does not offer regulation of vehicle spacing. The viability of dynamic time-gap control is also demonstrated. We conclude that (1) it is possible to coach drivers to improve performance on driving tasks using CAN data, and (2) it is a true HCPS, since removing human perception from the control loop reduces performance at the given control objective.      
### 40.On Determinism of Game Engines used for Simulation-based Autonomous Vehicle Verification  [ :arrow_down: ](https://arxiv.org/pdf/2104.06262.pdf)
>  Game engines are increasingly used as simulation platforms by the autonomous vehicle (AV) community to develop vehicle control systems and test environments. A key requirement for simulation-based development and verification is determinism, since a deterministic process will always produce the same output given the same initial conditions and event history. Thus, in a deterministic simulation environment, tests are rendered repeatable and yield simulation results that are trustworthy and straightforward to debug. However, game engines are seldom deterministic. This paper reviews and identifies the potential causes of non-deterministic behaviours in game engines. A case study using CARLA, an open-source autonomous driving simulation environment powered by Unreal Engine, is presented to highlight its inherent shortcomings in providing sufficient precision in experimental results. Different configurations and utilisations of the software and hardware are explored to determine an operational domain where the simulation precision is sufficiently low i.e.\ variance between repeated executions becomes negligible for development and testing work. Finally, a method of a general nature is proposed, that can be used to find the domains of permissible variance in game engine simulations for any given system configuration.      
### 41.Aliasing is your Ally: End-to-End Super-Resolution from Raw Image Bursts  [ :arrow_down: ](https://arxiv.org/pdf/2104.06191.pdf)
>  This presentation addresses the problem of reconstructing a high-resolution image from multiple lower-resolution snapshots captured from slightly different viewpoints in space and time. Key challenges for solving this problem include (i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw (noisy) images for maximal faithfulness to native camera data, and (iii) designing/learning an image prior (regularizer) well suited to the task. We address these three challenges with a hybrid algorithm building on the insight from Wronski et al. that aliasing is an ally in this setting, with parameters that can be learned end to end, while retaining the interpretability of classical approaches to inverse problems. The effectiveness of our approach is demonstrated on synthetic and real image bursts, setting a new state of the art on several benchmarks and delivering excellent qualitative results on real raw bursts captured by smartphones and prosumer cameras.      
### 42.Numerical Energy Analysis of In-wheel Motor Driven Autonomous Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2104.06189.pdf)
>  Autonomous electric vehicles are being widely studied nowadays as the future technology of ground transportation, while the autonomous electric vehicles based on conventional powertrain system limit their energy and power transmission efficiencies and may hinder their broad applications in future. Here we report a study on the energy consumption and efficiency improvement of a mid-size autonomous electric vehicle driven by in-wheel motors, through the development of a numerical energy model, validated with the actual driving data and implemented in a case study. The energy analysis was conducted under three driving conditions: flat road, upslope, and downslope driving to examine the energy consumption, with the energy-saving potential of the in-wheel-motor driven powertrain system systematically explored and discussed. Considering the energy recovery from the regenerative braking, energy consumption and regenerated energy were calculated in specific driving cycles based on vehicle dynamics and autonomous driving patterns. A case study was conducted using the baseline electric vehicle driving data in West Los Angeles. It was found that an in-wheel motor driven autonomous electric vehicle can save up to 17.5% of energy compared with a conventional electric vehicle during the slope driving. Using the efficiency maps of a commercial in-wheel motor, the numerical energy model and validated results obtained from this study are in line with actual situations, and can be used to support sustainable development of more energy-efficient autonomous electric vehicles in the future.      
### 43.Certified Control: An Architecture for Verifiable Safety of Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2104.06178.pdf)
>  Widespread adoption of autonomous cars will require greater confidence in their safety than is currently possible. Certified control is a new safety architecture whose goal is two-fold: to achieve a very high level of safety, and to provide a framework for justifiable confidence in that safety. The key idea is a runtime monitor that acts, along with sensor hardware and low-level control and actuators, as a small trusted base, ensuring the safety of the system as a whole. <br>Unfortunately, in current systems complex perception makes the verification even of a runtime monitor challenging. Unlike traditional runtime monitoring, therefore, a certified control monitor does not perform perception and analysis itself. Instead, the main controller assembles evidence that the proposed action is safe into a certificate that is then checked independently by the monitor. This exploits the classic gap between the costs of finding and checking. The controller is assigned the task of finding the certificate, and can thus use the most sophisticated algorithms available (including learning-enabled software); the monitor is assigned only the task of checking, and can thus run quickly and be smaller and formally verifiable. <br>This paper explains the key ideas of certified control and illustrates them with a certificate for LiDAR data and its formal verification. It shows how the architecture dramatically reduces the amount of code to be verified, providing an end-to-end safety analysis that would likely not be achievable in a traditional architecture.      
### 44.Visually Informed Binaural Audio Generation without Binaural Audios  [ :arrow_down: ](https://arxiv.org/pdf/2104.06162.pdf)
>  Stereophonic audio, especially binaural audio, plays an essential role in immersive viewing environments. Recent research has explored generating visually guided stereophonic audios supervised by multi-channel audio collections. However, due to the requirement of professional recording devices, existing datasets are limited in scale and variety, which impedes the generalization of supervised methods in real-world scenarios. In this work, we propose PseudoBinaural, an effective pipeline that is free of binaural recordings. The key insight is to carefully build pseudo visual-stereo pairs with mono data for training. Specifically, we leverage spherical harmonic decomposition and head-related impulse response (HRIR) to identify the relationship between spatial locations and received binaural audios. Then in the visual modality, corresponding visual cues of the mono data are manually placed at sound source positions to form the pairs. Compared to fully-supervised paradigms, our binaural-recording-free pipeline shows great stability in cross-dataset evaluation and achieves comparable performance under subjective preference. Moreover, combined with binaural recordings, our method is able to further boost the performance of binaural audio generation under supervised settings.      
### 45.Geometric Control of two Quadrotors Carrying a Rigid Rod with Elastic Cables  [ :arrow_down: ](https://arxiv.org/pdf/2104.06155.pdf)
>  This paper presents the design of a geometric trajectory tracking controller for the cooperative task of two quadrotor UAVs (unmanned aerial vehicles) carrying and transporting a rigid bar, which is attached to the quadrotors via inflexible elastic cables. The elasticity of the cables together with techniques of singular perturbation allows a reduction in the model to that of a similar model with inelastic cables. In this reduced model, we design a controller such that the rod exponentially tracks a given desired trajectory for its position and attitude, under some assumptions on initial error. We then show that exponential tracking in the reduced model corresponds to exponential tracking of the original elastic model. We also show that the previously defined control scheme provides uniform ultimate boundedness in the presence of unstructured bounded disturbances.      
### 46.Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept  [ :arrow_down: ](https://arxiv.org/pdf/2104.06104.pdf)
>  With the advent of direct models in automatic speech recognition (ASR), the formerly prevalent frame-wise acoustic modeling based on hidden Markov models (HMM) diversified into a number of modeling architectures like encoder-decoder attention models, transducer models and segmental models (direct HMM). While transducer models stay with a frame-level model definition, segmental models are defined on the level of label segments, directly. While (soft-)attention-based models avoid explicit alignment, transducer and segmental approach internally do model alignment, either by segment hypotheses or, more implicitly, by emitting so-called blank symbols. In this work, we prove that the widely used class of RNN-Transducer models and segmental models (direct HMM) are equivalent and therefore show equal modeling power. It is shown that blank probabilities translate into segment length probabilities and vice versa. In addition, we provide initial experiments investigating decoding and beam-pruning, comparing time-synchronous and label-/segment-synchronous search strategies and their properties using the same underlying model.      
### 47.NoiseVC: Towards High Quality Zero-Shot Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2104.06074.pdf)
>  Voice conversion (VC) is a task that transforms voice from target audio to source without losing linguistic contents, it is challenging especially when source and target speakers are unseen during training (zero-shot VC). Previous approaches require a pre-trained model or linguistic data to do the zero-shot conversion. Meanwhile, VC models with Vector Quantization (VQ) or Instance Normalization (IN) are able to disentangle contents from audios and achieve successful conversions. However, disentanglement in these models highly relies on heavily constrained bottleneck layers, thus, the sound quality is drastically sacrificed. In this paper, we propose NoiseVC, an approach that can disentangle contents based on VQ and Contrastive Predictive Coding (CPC). Additionally, Noise Augmentation is performed to further enhance disentanglement capability. We conduct several experiments and demonstrate that NoiseVC has a strong disentanglement ability with a small sacrifice of quality.      
### 48.Detecting Escalation Level from Speech with Transfer Learning and Acoustic-Lexical Information Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2104.06004.pdf)
>  Textual escalation detection has been widely applied to e-commerce companies' customer service systems to pre-alert and prevent potential conflicts. Similarly, in public areas such as airports and train stations, where many impersonal conversations frequently take place, acoustic-based escalation detection systems are also useful to enhance passengers' safety and maintain public order. To this end, we introduce a system based on acoustic-lexical features to detect escalation from speech, Voice Activity Detection (VAD) and label smoothing are adopted to further enhance the performance in our experiments. Considering a small set of training and development data, we also employ transfer learning on several well-known emotional detection datasets, i.e. RAVDESS, CREMA-D, to learn advanced emotional representations that can be applied to the escalation detection task. On the development set, our proposed system achieves 81.5% unweighted average recall (UAR) which significantly outperforms the baseline with 72.2% UAR.      
### 49.Temporal EigenPAC for dyslexia diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2104.05991.pdf)
>  Electroencephalography signals allow to explore the functional activity of the brain cortex in a non-invasive way. However, the analysis of these signals is not straightforward due to the presence of different artifacts and the very low signal-to-noise ratio. Cross-Frequency Coupling (CFC) methods provide a way to extract information from EEG, related to the synchronization among frequency bands. However, CFC methods are usually applied in a local way, computing the interaction between phase and amplitude at the same electrode. In this work we show a method to compute PAC features among electrodes to study the functional connectivity. Moreover, this has been applied jointly with Principal Component Analysis to explore patterns related to Dyslexia in 7-years-old children. The developed methodology reveals the temporal evolution of PAC-based connectivity. Directions of greatest variance computed by PCA are called eigenPACs here, since they resemble the classical \textit{eigenfaces} representation. The projection of PAC data onto the eigenPACs provide a set of features that has demonstrates their discriminative capability, specifically in the Beta-Gamma bands.      
### 50.Experiments of ASR-based mispronunciation detection for children and adult English learners  [ :arrow_down: ](https://arxiv.org/pdf/2104.05980.pdf)
>  Pronunciation is one of the fundamentals of language learning, and it is considered a primary factor of spoken language when it comes to an understanding and being understood by others. The persistent presence of high error rates in speech recognition domains resulting from mispronunciations motivates us to find alternative techniques for handling mispronunciations. In this study, we develop a mispronunciation assessment system that checks the pronunciation of non-native English speakers, identifies the commonly mispronounced phonemes of Italian learners of English, and presents an evaluation of the non-native pronunciation observed in phonetically annotated speech corpora. In this work, to detect mispronunciations, we used a phone-based ASR implemented using Kaldi. We used two non-native English labeled corpora; (i) a corpus of Italian adults contains 5,867 utterances from 46 speakers, and (ii) a corpus of Italian children consists of 5,268 utterances from 78 children. Our results show that the selected error model can discriminate correct sounds from incorrect sounds in both native and nonnative speech, and therefore can be used to detect pronunciation errors in non-native speech. The phone error rates show improvement in using the error language model. The ASR system shows better accuracy after applying the error model on our selected corpora.      
### 51.Load and Renewable-Following Control of Linearization-Free Differential Algebraic Equation Power System Models  [ :arrow_down: ](https://arxiv.org/pdf/2104.05957.pdf)
>  Electromechanical transients in power networks are mostly caused by mismatch between power consumption and production, causing generators to deviate from the nominal frequency. To that end, feedback control algorithms have been designed to perform frequency and load/renewables-following control. In particular, the literature addressed a plethora of grid- and frequency-control challenges with a focus on linearized, differential equation models whereby algebraic constraints (i.e., power flows) are eliminated. This is in contrast with the more realistic nonlinear differential algebraic equation (NDAE) models. Yet, as grids are increasingly pushed to their limits via intermittent renewables and varying loads, their physical states risk escaping operating regions due to either a poor prediction or sudden changes in renewables or demands -- deeming a feedback controller based on a linearization point virtually unusable. In lieu of linearized differential equation models, the objective of this paper is to design a simple, purely decentralized, linearization-free, feedback control law for NDAE models of power networks. The objective of such controller is to primarily stabilize frequency oscillations after a large, unknown disturbance in renewables or loads. Although the controller design involves advanced NDAE system theory, the controller itself is as simple as a decentralized proportional or linear quadratic regulator in its implementation. Case studies demonstrate that the proposed controller is able to stabilize dynamic and algebraic states under significant disturbances.      
### 52.Recurrent Equilibrium Networks: Unconstrained Learning of Stable and Robust Dynamical Models  [ :arrow_down: ](https://arxiv.org/pdf/2104.05942.pdf)
>  This paper introduces recurrent equilibrium networks (RENs), a new class of nonlinear dynamical models for applications in machine learning and system identification. The new model class has "built in" guarantees of stability and robustness: all models in the class are contracting -- a strong form of nonlinear stability -- and models can have prescribed Lipschitz bounds. RENs are otherwise very flexible: they can represent all stable linear systems, all previously-known sets of contracting recurrent neural networks, all deep feedforward neural networks, and all stable Wiener/Hammerstein models. RENs are parameterized directly by a vector in R^N, i.e. stability and robustness are ensured without parameter constraints, which simplifies learning since generic methods for unconstrained optimization can be used. The performance of the robustness of the new model set is evaluated on benchmark nonlinear system identification problems.      
### 53.An Adaptive Synaptic Array using Fowler-Nordheim Dynamic Analog Memory  [ :arrow_down: ](https://arxiv.org/pdf/2104.05926.pdf)
>  In this paper we present a synaptic array that uses dynamical states to implement an analog memory for energy-efficient training of machine learning (ML) systems. Each of the analog memory elements is a micro-dynamical system that is driven by the physics of Fowler-Nordheim (FN) quantum tunneling, whereas the system level learning modulates the state trajectory of the memory ensembles towards the optimal solution. We show that the extrinsic energy required for modulation can be matched to the dynamics of learning and weight decay leading to a significant reduction in the energy-dissipated during ML training. With the energy-dissipation as low as 5 fJ per memory update and a programming resolution up to 14 bits, the proposed synapse array could be used to address the energy-efficiency imbalance between the training and the inference phases observed in artificial intelligence (AI) systems.      
### 54.SRR-Net: A Super-Resolution-Involved Reconstruction Method for High Resolution MR Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2104.05901.pdf)
>  Improving the image resolution and acquisition speed of magnetic resonance imaging (MRI) is a challenging problem. There are mainly two strategies dealing with the speed-resolution trade-off: (1) $k$-space undersampling with high-resolution acquisition, and (2) a pipeline of lower resolution image reconstruction and image super-resolution. However, these approaches either have limited performance at certain high acceleration factor or suffer from the error accumulation of two-step structure. In this paper, we combine the idea of MR reconstruction and image super-resolution, and work on recovering HR images from low-resolution under-sampled $k$-space data directly. Particularly, the SR-involved reconstruction can be formulated as a variational problem, and a learnable network unrolled from its solution algorithm is proposed. A discriminator was introduced to enhance the detail refining performance. Experiment results using in-vivo HR multi-coil brain data indicate that the proposed SRR-Net is capable of recovering high-resolution brain images with both good visual quality and perceptual quality.      
### 55.Inertial Collaborative Localisation for Autonomous Vehicles using a Minimum Energy Filter  [ :arrow_down: ](https://arxiv.org/pdf/2104.05897.pdf)
>  Collaborative Localisation has been studied extensively in recent years as a way to improve pose estimation of unmanned aerial vehicles in challenging environments. However little attention has been paid toward advancing the underlying filter design beyond standard Extended Kalman Filter-based approaches. In this paper, we detail a discrete-time collaborative localisation filter using the deterministic minimum-energy framework. The filter incorporates measurements from an inertial measurement unit and models the effects of sensor bias and gravitational acceleration. We present a simulation based on real-world vehicle trajectories and IMU data that demonstrates how collaborative localisation can improve performance over single-vehicle methods.      
### 56.A Distributed and Resilient Bargaining Game for Weather-Predictive Microgrid Energy Cooperation  [ :arrow_down: ](https://arxiv.org/pdf/2104.05810.pdf)
>  A bargaining game is investigated for cooperative energy management in microgrids. This game incorporates a fully distributed and realistic cooperative power scheduling algorithm (CoDES) as well as a distributed Nash Bargaining Solution (NBS)-based method of allocating the overall power bill resulting from CoDES. A novel weather-based stochastic renewable generation (RG) prediction method is incorporated in the power scheduling. We demonstrate the proposed game using a 4-user grid-connected microgrid model with diverse user demands, storage, and RG profiles and examine the effect of weather prediction on day-ahead power scheduling and cost/profit allocation. Finally, the impact of users' ambivalence about cooperation and /or dishonesty on the bargaining outcome is investigated, and it is shown that the proposed game is resilient to malicious users' attempts to avoid payment of their fair share of the overall bill.      
### 57.Extremely Low Footprint End-to-End ASR System for Smart Device  [ :arrow_down: ](https://arxiv.org/pdf/2104.05784.pdf)
>  Recently, end-to-end (E2E) speech recognition has become popular, since it can integrate the acoustic, pronunciation and language models into a single neural network, as well as outperforms conventional models. Among E2E approaches, attention-based models, $e.g.$ Transformer, have emerged as being superior. The E2E models have opened the door of deployment of ASR on smart device, however it still suffers from large amount model parameters. This work proposes an extremely low footprint E2E ASR system for smart device, to achieve the goal of satisfying resource constraints without sacrificing recognition accuracy. We adopt cross-layer weight sharing to improve parameter-efficiency. We further exploit the model compression methods including sparsification and quantization, to reduce the memory storage and boost the decoding efficiency on smart device. We have evaluated our approach on the public AISHELL-1 and AISHELL-2 benchmarks. On the AISHELL-2 task, the proposed method achieves more than 10x compression (model size from 248MB to 24MB) while shuffer from small performance loss (CER from 6.49% to 6.92%).      
### 58.Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2104.05752.pdf)
>  A major focus of recent research in spoken language understanding (SLU) has been on the end-to-end approach where a single model can predict intents directly from speech inputs without intermediate transcripts. However, this approach presents some challenges. First, since speech can be considered as personally identifiable information, in some cases only automatic speech recognition (ASR) transcripts are accessible. Second, intent-labeled speech data is scarce. To address the first challenge, we propose a novel system that can predict intents from flexible types of inputs: speech, ASR transcripts, or both. We demonstrate strong performance for either modality separately, and when both speech and ASR transcripts are available, through system combination, we achieve better results than using a single input modality. To address the second challenge, we leverage a semantically robust pre-trained BERT model and adopt a cross-modal system that co-trains text embeddings and acoustic embeddings in a shared latent space. We further enhance this system by utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the text module on our target datasets. Our experiments show significant advantages for these pre-training and fine-tuning strategies, resulting in a system that achieves competitive intent-classification performance on Snips SLU and Fluent Speech Commands datasets.      
