# ArXiv eess --Mon, 5 Apr 2021
### 1.On the Generation of Long Binary Sequences with Record-Breaking PSL Values  [ :arrow_down: ](https://arxiv.org/pdf/2104.01154.pdf)
>  Binary sequences are widely used in various practical fields, such as telecommunications, radar technology, navigation, cryptography, measurement sciences, biology or industry. In this paper, a method to generate long binary sequences (LBS) with low peak sidelobe level (PSL) value is proposed. Having an LBS with length $n$, both the time and memory complexities of the proposed algorithm are $\mathcal{O}(n)$. During our experiments, we repeatedly reach better PSL values than the currently known state of art constructions, such as Legendre sequences, with or without rotations, Rudin-Shapiro sequences or m-sequences, with or without rotations, by always reaching a record-breaking PSL values strictly less than $\sqrt{n}$. Furthermore, the efficiency and simplicity of the proposed method are particularly beneficial to the lightweightness of the implementation, which allowed us to reach record-breaking PSL values for less than a second.      
### 2.Glioblastoma Multiforme Prognosis: MRI Missing Modality Generation, Segmentation and Radiogenomic Survival Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2104.01149.pdf)
>  The accurate prognosis of Glioblastoma Multiforme (GBM) plays an essential role in planning correlated surgeries and treatments. The conventional models of survival prediction rely on radiomic features using magnetic resonance imaging (MRI). In this paper, we propose a radiogenomic overall survival (OS) prediction approach by incorporating gene expression data with radiomic features such as shape, geometry, and clinical information. We exploit TCGA (The Cancer Genomic Atlas) dataset and synthesize the missing MRI modalities using a fully convolutional network (FCN) in a conditional Generative Adversarial Network (cGAN). Meanwhile, the same FCN architecture enables the tumor segmentation from the available and the synthesized MRI modalities. The proposed FCN architecture comprises octave convolution (OctConv) and a novel decoder, with skip connections in spatial and channel squeeze &amp; excitation (skip-scSE) block. The OctConv can process low and high-frequency features individually and improve model efficiency by reducing channel-wise redundancy. Skip-scSE applies spatial and channel-wise excitation to signify the essential features and reduces the sparsity in deeper layers learning parameters using skip connections. The proposed approaches are evaluated by comparative experiments with state-of-the-art models in synthesis, segmentation, and overall survival (OS) prediction. We observe that adding missing MRI modality improves the segmentation prediction, and expression levels of gene markers have a high contribution in the GBM prognosis prediction, and fused radiogenomic features boost the OS estimation.      
### 3.Linear Systems can be Hard to Learn  [ :arrow_down: ](https://arxiv.org/pdf/2104.01120.pdf)
>  In this paper, we investigate when system identification is statistically easy or hard, in the finite sample regime. Statistically easy to learn linear system classes have sample complexity that is polynomial with the system dimension. Most prior research in the finite sample regime falls in this category, focusing on systems that are directly excited by process noise. Statistically hard to learn linear system classes have worst-case sample complexity that is at least exponential with the system dimension, regardless of the identification algorithm. Using tools from minimax theory, we show that classes of linear systems can be hard to learn. Such classes include, for example, under-actuated or under-excited systems with weak coupling among the states. Having classified some systems as easy or hard to learn, a natural question arises as to what system properties fundamentally affect the hardness of system identifiability. Towards this direction, we characterize how the controllability index of linear systems affects the sample complexity of identification. More specifically, we show that the sample complexity of robustly controllable linear systems is upper bounded by an exponential function of the controllability index. This implies that identification is easy for classes of linear systems with small controllability index and potentially hard if the controllability index is large. Our analysis is based on recent statistical tools for finite sample analysis of system identification as well as a novel lower bound that relates controllability index with the least singular value of the controllability Gramian.      
### 4.Deep Manifold Learning for Dynamic MR Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2104.01102.pdf)
>  Purpose: To develop a deep learning method on a nonlinear manifold to explore the temporal redundancy of dynamic signals to reconstruct cardiac MRI data from highly undersampled measurements. <br>Methods: Cardiac MR image reconstruction is modeled as general compressed sensing (CS) based optimization on a low-rank tensor manifold. The nonlinear manifold is designed to characterize the temporal correlation of dynamic signals. Iterative procedures can be obtained by solving the optimization model on the manifold, including gradient calculation, projection of the gradient to tangent space, and retraction of the tangent space to the manifold. The iterative procedures on the manifold are unrolled to a neural network, dubbed as Manifold-Net. The Manifold-Net is trained using in vivo data with a retrospective electrocardiogram (ECG)-gated segmented bSSFP sequence. <br>Results: Experimental results at high accelerations demonstrate that the proposed method can obtain improved reconstruction compared with a compressed sensing (CS) method k-t SLR and two state-of-the-art deep learning-based methods, DC-CNN and CRNN. <br>Conclusion: This work represents the first study unrolling the optimization on manifolds into neural networks. Specifically, the designed low-rank manifold provides a new technical route for applying low-rank priors in dynamic MR imaging.      
### 5.Blind Exploration and Exploitation of Stochastic Experts  [ :arrow_down: ](https://arxiv.org/pdf/2104.01078.pdf)
>  We present blind exploration and exploitation (BEE) algorithms for identifying the most reliable stochastic expert based on formulations that employ posterior sampling, upper-confidence bounds, empirical Kullback-Leibler divergence, and minmax methods for the stochastic multi-armed bandit problem. Joint sampling and consultation of experts whose opinions depend on the hidden and random state of the world becomes challenging in the unsupervised, or blind, framework as feedback from the true state is not available. We propose an empirically realizable measure of expert competence that can be inferred instantaneously using only the opinions of other experts. This measure preserves the ordering of true competences and thus enables joint sampling and consultation of stochastic experts based on their opinions on dynamically changing tasks. Statistics derived from the proposed measure is instantaneously available allowing both blind exploration-exploitation and unsupervised opinion aggregation. We discuss how the lack of supervision affects the asymptotic regret of BEE architectures that rely on UCB1, KL-UCB, MOSS, IMED, and Thompson sampling. We demonstrate the performance of different BEE algorithms empirically and compare them to their standard, or supervised, counterparts.      
### 6.Prediction of Tuberculosis using U-Net and segmentation techniques  [ :arrow_down: ](https://arxiv.org/pdf/2104.01071.pdf)
>  One of the most serious public health problems in Peru and worldwide is Tuberculosis (TB), which is produced by a bacterium known as Mycobacterium tuberculosis. The purpose of this work is to facilitate and automate the diagnosis of tuberculosis using the MODS method and using lens-free microscopy, as it is easier to calibrate and easier to use by untrained personnel compared to lens microscopy. Therefore, we employed a U-Net network on our collected data set to perform automatic segmentation of cord shape bacterial accumulation and then predict tuberculosis. Our results show promising evidence for automatic segmentation of TB cords, and thus good accuracy for TB prediction.      
### 7.Channel Estimation for MIMO Space Time Coded OTFS under Doubly Selective Channels  [ :arrow_down: ](https://arxiv.org/pdf/2104.01023.pdf)
>  In this paper, we present a unique word (UW)-based channel estimation approach for multiple-input multiple-output (MIMO) systems under doubly dispersive channels, which is applied to orthogonal time frequency space (OTFS) with space time coding (STC). The OTFS modulation has been recently proposed as a robust technique under time varying channels due to its property of spreading the data symbols over time and frequency. Yet another relevant aspect is the employment of multiple antennas at the transmitter and receiver. Therefore, we consider an STC MIMO system with cyclic delay diversity at the transmitter and maximum ratio combining at the receiver, where we develop a UW-based channel estimation scheme for multiple transmit antennas. We show a recently proposed frame optimization scheme for SISO is directly applicable to MIMO. In addition, we evaluate numerically the frame error rate (FER) of OTFS and OFDM with 2x2 and 4x4 MIMO, where the time varying channel is estimated using the UW-based approach. The FER results reveal that OTFS becomes more advantageous than OFDM for MIMO-STC systems with higher order modulation and code rate.      
### 8.Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation  [ :arrow_down: ](https://arxiv.org/pdf/2104.00994.pdf)
>  This paper tackles automatically discovering phone-like acoustic units (AUD) from unlabeled speech data. Past studies usually proposed single-step approaches. We propose a two-stage approach: the first stage learns a subword-discriminative feature representation and the second stage applies clustering to the learned representation and obtains phone-like clusters as the discovered acoustic units. In the first stage, a recently proposed method in the task of unsupervised subword modeling is improved by replacing a monolingual out-of-domain (OOD) ASR system with a multilingual one to create a subword-discriminative representation that is more language-independent. In the second stage, segment-level k-means is adopted, and two methods to represent the variable-length speech segments as fixed-dimension feature vectors are compared. Experiments on a very low-resource Mboshi language corpus show that our approach outperforms state-of-the-art AUD in both normalized mutual information (NMI) and F-score. The multilingual ASR improved upon the monolingual ASR in providing OOD phone labels and in estimating the phone boundaries. A comparison of our systems with and without knowing the ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that the current approach can significantly benefit from improved phone boundary estimation.      
### 9.Brain Tumor Segmentation and Survival Prediction using 3D Attention UNet  [ :arrow_down: ](https://arxiv.org/pdf/2104.00985.pdf)
>  In this work, we develop an attention convolutional neural network (CNN) to segment brain tumors from Magnetic Resonance Images (MRI). Further, we predict the survival rate using various machine learning methods. We adopt a 3D UNet architecture and integrate channel and spatial attention with the decoder network to perform segmentation. For survival prediction, we extract some novel radiomic features based on geometry, location, the shape of the segmented tumor and combine them with clinical information to estimate the survival duration for each patient. We also perform extensive experiments to show the effect of each feature for overall survival (OS) prediction. The experimental results infer that radiomic features such as histogram, location, and shape of the necrosis region and clinical features like age are the most critical parameters to estimate the OS.      
### 10.Glioma Prognosis: Segmentation of the Tumor and Survival Prediction using Shape, Geometric and Clinical Information  [ :arrow_down: ](https://arxiv.org/pdf/2104.00980.pdf)
>  Segmentation of brain tumor from magnetic resonance imaging (MRI) is a vital process to improve diagnosis, treatment planning and to study the difference between subjects with tumor and healthy subjects. In this paper, we exploit a convolutional neural network (CNN) with hypercolumn technique to segment tumor from healthy brain tissue. Hypercolumn is the concatenation of a set of vectors which form by extracting convolutional features from multiple layers. Proposed model integrates batch normalization (BN) approach with hypercolumn. BN layers help to alleviate the internal covariate shift during stochastic gradient descent (SGD) training by zero-mean and unit variance of each mini-batch. Survival Prediction is done by first extracting features(Geometric, Fractal, and Histogram) from the segmented brain tumor data. Then, the number of days of overall survival is predicted by implementing regression on the extracted features using an artificial neural network (ANN). Our model achieves a mean dice score of 89.78%, 82.53% and 76.54% for the whole tumor, tumor core and enhancing tumor respectively in segmentation task and 67.90% in overall survival prediction task with the validation set of BraTS 2018 challenge. It obtains a mean dice accuracy of 87.315%, 77.04% and 70.22% for the whole tumor, tumor core and enhancing tumor respectively in the segmentation task and a 46.80% in overall survival prediction task in the BraTS 2018 test data set.      
### 11.Variational Deep Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2104.00965.pdf)
>  Convolutional neural networks (CNNs) have shown outstanding performance on image denoising with the help of large-scale datasets. Earlier methods naively trained a single CNN with many pairs of clean-noisy images. However, the conditional distribution of the clean image given a noisy one is too complicated and diverse, so that a single CNN cannot well learn such distributions. Therefore, there have also been some methods that exploit additional noise level parameters or train a separate CNN for a specific noise level parameter. These methods separate the original problem into easier sub-problems and thus have shown improved performance than the naively trained CNN. In this step, we raise two questions. The first one is whether it is an optimal approach to relate the conditional distribution only to noise level parameters. The second is what if we do not have noise level information, such as in a real-world scenario. To answer the questions and provide a better solution, we propose a novel Bayesian framework based on the variational approximation of objective functions. This enables us to separate the complicated target distribution into simpler sub-distributions. Eventually, the denoising CNN can conquer noise from each sub-distribution, which is generally an easier problem than the original. Experiments show that the proposed method provides remarkable performance on additive white Gaussian noise (AWGN) and real-noise denoising while requiring fewer parameters than recent state-of-the-art denoisers.      
### 12.INTERSPEECH 2021 ConferencingSpeech Challenge: Towards Far-field Multi-Channel Speech Enhancement for Video Conferencing  [ :arrow_down: ](https://arxiv.org/pdf/2104.00960.pdf)
>  The ConferencingSpeech 2021 challenge is proposed to stimulate research on far-field multi-channel speech enhancement for video conferencing. The challenge consists of two separate tasks: 1) Task 1 is multi-channel speech enhancement with single microphone array and focusing on practical application with real-time requirement and 2) Task 2 is multi-channel speech enhancement with multiple distributed microphone arrays, which is a non-real-time track and does not have any constraints so that participants could explore any algorithms to obtain high speech quality. Targeting the real video conferencing room application, the challenge database was recorded from real speakers and all recording facilities were located by following the real setup of conferencing room. In this challenge, we open-sourced the list of open source clean speech and noise datasets, simulation scripts, and a baseline system for participants to develop their own system. The final ranking of the challenge will be decided by the subjective evaluation which is performed using Absolute Category Ratings (ACR) to estimate Mean Opinion Score (MOS), speech MOS (S-MOS), and noise MOS (N-MOS). This paper describes the challenge, tasks, datasets, and subjective evaluation. The baseline system which is a complex ratio mask based neural network and its experimental results are also presented.      
### 13.Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2104.00931.pdf)
>  In this paper, we pose the current state-of-the-art voice conversion (VC) systems as two-encoder-one-decoder models. After comparing these models, we combine the best features and propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. This paper also introduces the GTA finetuning in VC, which significantly improves the quality and the speaker similarity of the outputs. Assem-VC outperforms the previous state-of-the-art approaches in both the naturalness and the speaker similarity on the VCTK dataset. As an objective result, the degree of speaker disentanglement of features such as phonetic posteriorgrams (PPG) is also explored. Our investigation indicates that many-to-many VC results are no longer distinct from human speech and similar quality can be achieved with any-to-many models. Audio samples are available at <a class="link-external link-https" href="https://mindslab-ai.github.io/assem-vc/" rel="external noopener nofollow">this https URL</a>      
### 14.Synchronized SCUBA: D2D Communication for Out-of-Sync Devices  [ :arrow_down: ](https://arxiv.org/pdf/2104.00809.pdf)
>  Device-to-device (D2D) communication is an essential component enabling connectivity for the Internet-of-Things (IoT). SCUBA, which stands for Sidelink Communication on Unlicensed Bands, is a novel medium access control protocol that facilitates D2D communications on the sidelink for IoT and machine-type communication (MTC) cellular devices. SCUBA includes support for direct peer-to-peer communication on the unlicensed bands by operating in a time division multiplexed manner to coexist with the underlying primary radio access technology, e.g., long term evolution - MTC (LTE-M). A fundamental requirement in the current version of SCUBA is that the communicating devices are to be synchronized with each other so that timing occasions of the devices can be accurately estimated by each other. However, when the devices are out of sync with each other, which may be caused due to one or more of the devices being out of cellular coverage region or are being served by different base stations, typically observed in mobile devices, operation of legacy SCUBA fails. To this end, we design synchronization methods to establish successful SCUBA links between devices that are out-of-sync with each other. Due to the inherent timing discovery embedded in our method, our solution also extends the operating range of SCUBA by eliminating its reliance on timing-agnostic communication. We analyze and compare the performance of our methods in terms of power consumption and the resultant impact on device battery life to show the potential of our solutions.      
### 15.Effect of Radiology Report Labeler Quality on Deep Learning Models for Chest X-Ray Interpretation  [ :arrow_down: ](https://arxiv.org/pdf/2104.00793.pdf)
>  Although deep learning models for chest X-ray interpretation are commonly trained on labels generated by automatic radiology report labelers, the impact of improvements in report labeling on the performance of chest X-ray classification models has not been systematically investigated. We first compare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of extracting accurate chest X-ray image labels from radiology reports, reporting that the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers. Next, after training image classification models using labels generated from the different radiology report labelers on one of the largest datasets of chest X-rays, we show that an image classification model trained on labels from the VisualCheXbert labeler outperforms image classification models trained on labels from the CheXpert and CheXbert labelers. Our work suggests that recent improvements in radiology report labeling can translate to the development of higher performing chest X-ray classification models.      
### 16.Keyword Transformer: A Self-Attention Model for Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2104.00769.pdf)
>  The Transformer architecture has been successful across many domains, including natural language processing, computer vision and speech recognition. In keyword spotting, self-attention has primarily been used on top of convolutional or recurrent encoders. We investigate a range of ways to adapt the Transformer architecture to keyword spotting and introduce the Keyword Transformer (KWT), a fully self-attentional architecture that exceeds state-of-the-art performance across multiple tasks without any pre-training or additional data. Surprisingly, this simple architecture outperforms more complex models that mix convolutional, recurrent and attentive layers. KWT can be used as a drop-in replacement for these models, setting two new benchmark records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on the 12 and 35-command tasks respectively.      
### 17.Radar Target Detection aided by Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2104.00768.pdf)
>  In this work, we consider the target detection problem in a sensing architecture where the radar is aided by a reconfigurable intelligent surface (RIS), that can be modeled as an array of sub-wavelength small reflective elements capable of imposing a tunable phase shift to the impinging waves and, ultimately, of providing the radar with an additional echo of the target. A theoretical analysis is carried out for closely- and widely-spaced (with respect to the target) radar and RIS and for different beampattern configurations, and some examples are provided to show that large gains can be achieved by the considered detection architecture.      
### 18.Two Modifications of the Unscented Kalman Filter that Specialize to the Kalman Filter for Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.00736.pdf)
>  Although the unscented Kalman filter (UKF) is applicable to nonlinear systems, it turns out that, for linear systems, UKF does not specialize to the classical Kalman filter. This situation suggests that it may be advantageous to modify UKF in such a way that, for linear systems, the Kalman filter is recovered. The ultimate goal is thus to develop modifications of UKF that specialize to the Kalman filter for linear systems and have improved accuracy for nonlinear systems. With this motivation, this paper presents two modifications of UKF that specialize to the Kalman filter for linear systems. The first modification (EUKF-A) requires the Jacobian of the dynamics map, whereas the second modification (EUKF-C) requires the Jacobian of the measurement map. For various nonlinear examples, the accuracy of EUKF-A and EUKF-C is compared to the accuracy of UKF.      
### 19.Optimal Strategies for Guarding a Compact and Convex Target Area: A Differential Game Approach  [ :arrow_down: ](https://arxiv.org/pdf/2104.00717.pdf)
>  We revisit the two-player planar target-defense game posed in [1], a special class of pursuit-evasion games in which the pursuer (or defender) strives to defend a stationary target area from the evader (or intruder) who desires to reach it, if possible, or approach it as close as possible. In this paper, the target area is assumed to be a compact and convex set. Unlike classical two-player pursuit-evasion games, this game involves two subgames: a capture game and an escape game. In the capture game, where capture is assured, the evader attempts to minimize the distance between her final position and the target area whereas the pursuer tries to maximize the same distance. In the escape game, where capture is not guaranteed, the evader attempts to maximize the distance between herself and the pursuer at the moment that she reaches the target for the first time. Our solution approach is based on Isaacs classical method in differential games. We first identify the barrier surface that demarcates the state space of the game into two subspaces, each of which corresponds to the two aforementioned subgames, by means of geometric arguments. Thereafter, we derive the optimal strategies for the players in each subspace. We show that, as long as the target area is compact and convex, the value of the game in each subspace is always continuously differentiable, and the proposed optimal strategies correspond to the unique saddle-point state-feedback strategies for the players. We illustrate our proposed solutions by means of numerical simulations.      
### 20.Microring Weight Banks Control beyond 8.5-bits Accuracy  [ :arrow_down: ](https://arxiv.org/pdf/2104.01164.pdf)
>  The microring resonators (MRR) weight bank, a ubiquitous element in silicon photonics providing weight addition, yet, has long suffered from the weakness of low accuracy, so that harming its potential in various promising applications. Here, we demonstrated a novel dithering method for MRR weight control, reaching an accuracy of up to 8.5 bits and a precision of up to 9.0 bits, which rivals that of the analog processors. Also, this method maintains good scalability to control multi-MRRs with little additional increase in searching time and system complexity, and has efficacious suppression to the inter-channel crosstalk. Given these features, this method could widely benefit the applications of MRR weight banks and facilitate the prevalence of the MRR in silicon photonics.      
### 21.An Audio-Based Deep Learning Framework ForBBC Television Programme Classification  [ :arrow_down: ](https://arxiv.org/pdf/2104.01161.pdf)
>  This paper proposes a deep learning framework for classification of BBC television programmes using audio. The audio is firstly transformed into spectrograms, which are fed into a pre-trained convolutional Neural Network (CNN), obtaining predicted probabilities of sound events occurring in the audio recording. Statistics for the predicted probabilities and detected sound events are then calculated to extract discriminative features representing the television programmes. Finally, the embedded features extracted are fed into a classifier for classifying the programmes into different genres. Our experiments are conducted over a dataset of 6,160 programmes belonging to nine genres labelled by the BBC. We achieve an average classification accuracy of 93.7% over 14-fold cross validation. This demonstrates the efficacy of the proposed framework for the task of audio-based classification of television programmes.      
### 22.PhyAug: Physics-Directed Data Augmentation for Deep Sensing Model Transfer in Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.01160.pdf)
>  Run-time domain shifts from training-phase domains are common in sensing systems designed with deep learning. The shifts can be caused by sensor characteristic variations and/or discrepancies between the design-phase model and the actual model of the sensed physical process. To address these issues, existing transfer learning techniques require substantial target-domain data and thus incur high post-deployment overhead. This paper proposes to exploit the first principle governing the domain shift to reduce the demand on target-domain data. Specifically, our proposed approach called PhyAug uses the first principle fitted with few labeled or unlabeled source/target-domain data pairs to transform the existing source-domain training data into augmented data for updating the deep neural networks. In two case studies of keyword spotting and DeepSpeech2-based automatic speech recognition, with 5-second unlabeled data collected from the target microphones, PhyAug recovers the recognition accuracy losses due to microphone characteristic variations by 37% to 72%. In a case study of seismic source localization with TDoA fngerprints, by exploiting the frst principle of signal propagation in uneven media, PhyAug only requires 3% to 8% of labeled TDoA measurements required by the vanilla fingerprinting approach in achieving the same localization accuracy.      
### 23.An active inference model of collective intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2104.01066.pdf)
>  To date, formal models of collective intelligence have lacked a plausible mathematical description of the relationship between local-scale interactions between highly autonomous sub-system components (individuals) and global-scale behavior of the composite system (the collective). In this paper we use the Active Inference Formulation (AIF), a framework for explaining the behavior of any non-equilibrium steady state system at any scale, to posit a minimal agent-based model that simulates the relationship between local individual-level interaction and collective intelligence (operationalized as system-level performance). We explore the effects of providing baseline AIF agents (Model 1) with specific cognitive capabilities: Theory of Mind (Model 2); Goal Alignment (Model 3), and Theory of Mind with Goal Alignment (Model 4). These stepwise transitions in sophistication of cognitive ability are motivated by the types of advancements plausibly required for an AIF agent to persist and flourish in an environment populated by other AIF agents, and have also recently been shown to map naturally to canonical steps in human cognitive ability. Illustrative results show that stepwise cognitive transitions increase system performance by providing complementary mechanisms for alignment between agents' local and global optima. Alignment emerges endogenously from the dynamics of interacting AIF agents themselves, rather than being imposed exogenously by incentives to agents' behaviors (contra existing computational models of collective intelligence) or top-down priors for collective behavior (contra existing multiscale simulations of AIF). These results shed light on the types of generic information-theoretic patterns conducive to collective intelligence in human and other complex adaptive systems.      
### 24.Hybrid and Generalized Bayesian Cramér-Rao Inequalities via Information Geometry  [ :arrow_down: ](https://arxiv.org/pdf/2104.01061.pdf)
>  Information geometry is the study of statistical models from a Riemannian geometric point of view. The Fisher information matrix plays the role of a Riemannian metric in this framework. This tool helps us obtain Cramér-Rao lower bound (CRLB). This chapter summarizes the recent results which extend this framework to more general Cramér-Rao inequalities. We apply Eguchi's theory to a generalized form of Czsiszár $f$-divergence to obtain a Riemannian metric that, at once, is used to obtain deterministic CRLB, Bayesian CRLB, and their generalizations.      
### 25.Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2104.01027.pdf)
>  Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. On a large-scale competitive setup, we show that pre-training on unlabeled in-domain data reduces the gap between models trained on in-domain and out-of-domain labeled data by 66%-73%. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. Code and models will be made available at <a class="link-external link-https" href="https://github.com/pytorch/fairseq" rel="external noopener nofollow">this https URL</a>.      
### 26.On Securing Cloud-hosted Cyber-physical Systems Using Trusted Execution Environments  [ :arrow_down: ](https://arxiv.org/pdf/2104.01011.pdf)
>  Recently, cloud control systems have gained increasing attention from the research community as a solution to implement networked cyber-physical systems (CPSs). Such an architecture can reduce deployment and maintenance costs albeit at the expense of additional security and privacy concerns. In this paper, first, we discuss state-of-the-art security solutions for cloud control systems and their limitations. Then, we propose a novel control architecture based on Trusted Execution Environments (TEE). We show that such an approach can potentially address major security and privacy issues for cloud-hosted control systems. Finally, we present an implementation setup based on Intel Software Guard Extensions (SGX) and validate its effectiveness on a testbed system.      
### 27.Data-driven balancing of linear dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.01006.pdf)
>  We present a novel reformulation of balanced truncation, a classical model reduction method. The principal innovation that we introduce comes through the use of system response data that has been either measured or computed, without reference to any prescribed realization of the original model. Data are represented by sampled values of the transfer function {or the impulse response} corresponding to the original model. We discuss parallels that our approach bears with the Loewner framework, another popular data-driven model reduction method. We illustrate our approach numerically in both continuous-time and discrete-time cases.      
### 28.Time Series Imaging for Link Layer Anomaly Classification in Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.00972.pdf)
>  The number of end devices that use the last mile wireless connectivity is dramatically increasing with the rise of smart infrastructures and require reliable functioning to support smooth and efficient business processes. To efficiently manage such massive wireless networks, more advanced and accurate network monitoring and malfunction detection solutions are required. In this paper, we perform a first time analysis of image-based representation techniques for wireless anomaly detection using recurrence plots and Gramian angular fields and propose a new deep learning architecture enabling accurate anomaly detection. We examine the relative performance of the proposed model and show that the image transformation of time series improves the performance of anomaly detection by up to 29% for binary classification and by up to 27% for multiclass classification. At the same time, the best performing model based on recurrence plot transformation leads to up to 55% increase compared to the state of the art where classical machine learning techniques are used. We also provide insights for the decisions of the classifier using an instance based approach enabled by insights into guided back-propagation. Our results demonstrate the potential of transformation of time series signals to images to improve classification performance compared to classification on raw time series data.      
### 29.Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered Language for Universal Phone Recognition Experiments  [ :arrow_down: ](https://arxiv.org/pdf/2104.00824.pdf)
>  There is growing interest in ASR systems that can recognize phones in a language-independent fashion. There is additionally interest in building language technologies for low-resource and endangered languages. However, there is a paucity of realistic data that can be used to test such systems and technologies. This paper presents a publicly available, phonetically transcribed corpus of 2255 utterances (words and short phrases) in the endangered Tangkhulic language East Tusom (no ISO 639-3 code), a Tibeto-Burman language variety spoken mostly in India. Because the dataset is transcribed in terms of phones, rather than phonemes, it is a better match for universal phone recognition systems than many larger (phonemically transcribed) datasets. This paper describes the dataset and the methodology used to produce it. It further presents basic benchmarks of state-of-the-art universal phone recognition systems on the dataset as baselines for future experiments.      
### 30.Deep Learning-based Codebook Design for Code-domain Non-Orthogonal Multiple Access Achieving a Single-User Bit Error Rate Performance  [ :arrow_down: ](https://arxiv.org/pdf/2104.00818.pdf)
>  The codebook design for code-domain non-orthogonal multiple access (CD-NOMA) can be considered as a constellation design for multi-user multi-dimensional modulation (MU-MDM). This paper proposes an autoencoder (AE)-based constellation design for MU-MDM with the objective of achieving a comparable bit error rate (BER) performance to single-user multi-dimensional modulation (SU-MDM), i.e., alleviating performance degradation in non-optimal AE design caused by overloading multiple users. Recognizing that various constraints in a receiver structure degrade a BER performance of the codebook design in the existing CD-NOMA, the MU-MDM design aims at global optimization on the common ground with SU-MDM by leveraging the agnosticism of the neural network-based multi-user decoder obtained from AE training, while mitigating the power normalization constraint and exploiting dense resource mapping in the MU-MDM AE structure. Moreover, as opposed to the existing loss function for MU-MDM which has failed to minimize BER for different levels of signal-to-noise ratio (SNR), a hyperparameterized loss function and proper training procedures are introduced to jointly optimize the signal points for MU-MDM constellation and their bit-to-symbol mapping. It has been demonstrated that the proposed design achieves a single-user BER bound with only 0.2dB loss, equivalently outperforming the existing CD-NOMA designs, while maintaining their overloading factor.      
### 31.A study on the effects of compression on hyperspectral image classification  [ :arrow_down: ](https://arxiv.org/pdf/2104.00788.pdf)
>  This paper presents a systematic study the effects of compression on hyperspectral pixel classification task. We use five dimensionality reduction methods -- PCA, KPCA, ICA, AE, and DAE -- to compress 301-dimensional hyperspectral pixels. Compressed pixels are subsequently used to perform pixel-based classifications. Pixel classification accuracies together with compression method, compression rates, and reconstruction errors provide a new lens to study the suitability of a compression method for the task of pixel-based classification. We use three high-resolution hyperspectral image datasets, representing three common landscape units (i.e. urban, transitional suburban, and forests) collected by the Remote Sensing and Spatial Ecosystem Modeling laboratory of the University of Toronto. We found that PCA, KPCA, and ICA post greater signal reconstruction capability; however, when compression rate is more than 90\% those methods showed lower classification scores. AE and DAE methods post better classification accuracy at 95\% compression rate, however decreasing again at 97\%, suggesting a sweet-spot at the 95\% mark. Our results demonstrate that the choice of a compression method with the compression rate are important considerations when designing a hyperspectral image classification pipeline.      
### 32.Reservoir-Based Distributed Machine Learning for Edge Operation  [ :arrow_down: ](https://arxiv.org/pdf/2104.00751.pdf)
>  We introduce a novel design for in-situ training of machine learning algorithms built into smart sensors, and illustrate distributed training scenarios using radio frequency (RF) spectrum sensors. Current RF sensors at the Edge lack the computational resources to support practical, in-situ training for intelligent signal classification. We propose a solution using Deepdelay Loop Reservoir Computing (DLR), a processing architecture that supports machine learning algorithms on resource-constrained edge-devices by leveraging delayloop reservoir computing in combination with innovative hardware. DLR delivers reductions in form factor, hardware complexity and latency, compared to the State-ofthe- Art (SoA) neural nets. We demonstrate DLR for two applications: RF Specific Emitter Identification (SEI) and wireless protocol recognition. DLR enables mobile edge platforms to authenticate and then track emitters with fast SEI retraining. Once delay loops separate the data classes, traditionally complex, power-hungry classification models are no longer needed for the learning process. Yet, even with simple classifiers such as Ridge Regression (RR), the complexity grows at least quadratically with the input size. DLR with a RR classifier exceeds the SoA accuracy, while further reducing power consumption by leveraging the architecture of parallel (split) loops. To authenticate mobile devices across large regions, DLR can be trained in a distributed fashion with very little additional processing and a small communication cost, all while maintaining accuracy. We illustrate how to merge locally trained DLR classifiers in use cases of interest.      
### 33.Graph Attention Networks for Channel Estimation in RIS-assisted Satellite IoT Communications  [ :arrow_down: ](https://arxiv.org/pdf/2104.00735.pdf)
>  Direct-to-satellite (DtS) communication has gained importance recently to support globally connected Internet of things (IoT) networks. However, relatively long distances of densely deployed satellite networks around the Earth cause a high path loss. In addition, since high complexity operations such as beamforming, tracking and equalization have to be performed in IoT devices partially, both the hardware complexity and the need for high-capacity batteries of IoT devices increase. The reconfigurable intelligent surfaces (RISs) have the potential to increase the energy-efficiency and to perform complex signal processing over the transmission environment instead of IoT devices. But, RISs need the information of the cascaded channel in order to change the phase of the incident signal. This study proposes graph attention networks (GATs) for the challenging channel estimation problem and examines the performance of DtS IoT networks for different RIS configurations under GAT channel estimation.      
### 34.Out of a hundred trials, how many errors does your speaker verifier make?  [ :arrow_down: ](https://arxiv.org/pdf/2104.00732.pdf)
>  Out of a hundred trials, how many errors does your speaker verifier make? For the user this is an important, practical question, but researchers and vendors typically sidestep it and supply instead the conditional error-rates that are given by the ROC/DET curve. We posit that the user's question is answered by the Bayes error-rate. We present a tutorial to show how to compute the error-rate that results when making Bayes decisions with calibrated likelihood ratios, supplied by the verifier, and an hypothesis prior, supplied by the user. For perfect calibration, the Bayes error-rate is upper bounded by min(EER,P,1-P), where EER is the equal-error-rate and P, 1-P are the prior probabilities of the competing hypotheses. The EER represents the accuracy of the verifier, while min(P,1-P) represents the hardness of the classification problem. We further show how the Bayes error-rate can be computed also for non-perfect calibration and how to generalize from error-rate to expected cost. We offer some criticism of decisions made by direct score thresholding. Finally, we demonstrate by analyzing error-rates of the recently published DCA-PLDA speaker verifier.      
### 35.Multi-rate attention architecture for fast streamable Text-to-speech spectrum modeling  [ :arrow_down: ](https://arxiv.org/pdf/2104.00705.pdf)
>  Typical high quality text-to-speech (TTS) systems today use a two-stage architecture, with a spectrum model stage that generates spectral frames and a vocoder stage that generates the actual audio. High-quality spectrum models usually incorporate the encoder-decoder architecture with self-attention or bi-directional long short-term (BLSTM) units. While these models can produce high quality speech, they often incur O($L$) increase in both latency and real-time factor (RTF) with respect to input length $L$. In other words, longer inputs leads to longer delay and slower synthesis speed, limiting its use in real-time applications. In this paper, we propose a multi-rate attention architecture that breaks the latency and RTF bottlenecks by computing a compact representation during encoding and recurrently generating the attention vector in a streaming manner during decoding. The proposed architecture achieves high audio quality (MOS of 4.31 compared to groundtruth 4.48), low latency, and low RTF at the same time. Meanwhile, both latency and RTF of the proposed system stay constant regardless of input lengths, making it ideal for real-time applications.      
### 36.Remote Sensing Image Classification with the SEN12MS Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2104.00704.pdf)
>  Image classification is one of the main drivers of the rapid developments in deep learning with convolutional neural networks for computer vision. So is the analogous task of scene classification in remote sensing. However, in contrast to the computer vision community that has long been using well-established, large-scale standard datasets to train and benchmark high-capacity models, the remote sensing community still largely relies on relatively small and often application-dependend datasets, thus lacking comparability. With this letter, we present a classification-oriented conversion of the SEN12MS dataset. Using that, we provide results for several baseline models based on two standard CNN architectures and different input data configurations. Our results support the benchmarking of remote sensing image classification and provide insights to the benefit of multi-spectral data and multi-sensor data fusion over conventional RGB imagery.      
