# ArXiv eess --Thu, 22 Apr 2021
### 1.Modeling Classroom Occupancy using Data of WiFi Infrastructure in a University Campus  [ :arrow_down: ](https://arxiv.org/pdf/2104.10667.pdf)
>  Universities worldwide are experiencing a surge in enrollments, therefore campus estate managers are seeking continuous data on attendance patterns to optimize the usage of classroom space. As a result, there is an increasing trend to measure classrooms attendance by employing various sensing technologies, among which pervasive WiFi infrastructure is seen as a low cost method. In a dense campus environment, the number of connected WiFi users does not well estimate room occupancy since connection counts are polluted by adjoining rooms, outdoor walkways, and network load balancing. <br>In this paper, we develop machine learning based models to infer classroom occupancy from WiFi sensing infrastructure. Our contributions are three-fold: (1) We analyze metadata from a dense and dynamic wireless network comprising of thousands of access points (APs) to draw insights into coverage of APs, behavior of WiFi connected users, and challenges of estimating room occupancy; (2) We propose a method to automatically map APs to classrooms using unsupervised clustering algorithms; and (3) We model classroom occupancy using a combination of classification and regression methods of varying algorithms. We achieve 84.6% accuracy in mapping APs to classrooms while the accuracy of our estimation for room occupancy is comparable to beam counter sensors with a symmetric Mean Absolute Percentage Error (sMAPE) of 13.10%.      
### 2.Artefact-removal algorithms for Fourier domain Quantum Optical Coherence Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2104.10655.pdf)
>  Quantum Optical Coherence Tomography (Q-OCT) is a non-classical equivalent of Optical Coherence Tomography and is able to provide a twofold axial resolution increase and immunity to resolution-degrading dispersion. The main drawback of Q-OCT are artefacts which are additional elements that clutter an A-scan and lead to a complete loss of structural information for multilayered objects. Whereas there are successful methods for artefact removal in Time-domain Q-OCT, no such scheme has been devised for Fourier-domain Q-OCT (Fd-Q-OCT), although the latter modality - through joint spectrum detection - outputs a lot of useful information on both the system and the imaged object. Here, we propose two algorithms which process a Fd-Q-OCT's joint spectrum into an artefact-free A-scan. We present the theoretical background of these algorithms and show their performance on computer-generated data. The limitations of both algorithms with regards to the experimental system and the imaged object are discussed.      
### 3.Deep Gaussian Processes for Biogeophysical Parameter Retrieval and Model Inversion  [ :arrow_down: ](https://arxiv.org/pdf/2104.10638.pdf)
>  Parameter retrieval and model inversion are key problems in remote sensing and Earth observation. Currently, different approximations exist: a direct, yet costly, inversion of radiative transfer models (RTMs); the statistical inversion with in situ data that often results in problems with extrapolation outside the study area; and the most widely adopted hybrid modeling by which statistical models, mostly nonlinear and non-parametric machine learning algorithms, are applied to invert RTM simulations. We will focus on the latter. Among the different existing algorithms, in the last decade kernel based methods, and Gaussian Processes (GPs) in particular, have provided useful and informative solutions to such RTM inversion problems. This is in large part due to the confidence intervals they provide, and their predictive accuracy. However, RTMs are very complex, highly nonlinear, and typically hierarchical models, so that often a shallow GP model cannot capture complex feature relations for inversion. This motivates the use of deeper hierarchical architectures, while still preserving the desirable properties of GPs. This paper introduces the use of deep Gaussian Processes (DGPs) for bio-geo-physical model inversion. Unlike shallow GP models, DGPs account for complicated (modular, hierarchical) processes, provide an efficient solution that scales well to big datasets, and improve prediction accuracy over their single layer counterpart. In the experimental section, we provide empirical evidence of performance for the estimation of surface temperature and dew point temperature from infrared sounding data, as well as for the prediction of chlorophyll content, inorganic suspended matter, and coloured dissolved matter from multispectral data acquired by the Sentinel-3 OLCI sensor. The presented methodology allows for more expressive forms of GPs in remote sensing model inversion problems.      
### 4.Programmable 3D snapshot microscopy with Fourier convolutional networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.10611.pdf)
>  3D snapshot microscopy enables volumetric imaging as fast as a camera allows by capturing a 3D volume in a single 2D camera image, and has found a variety of biological applications such as whole brain imaging of fast neural activity in larval zebrafish. The optimal microscope design for this optical 3D-to-2D encoding to preserve as much 3D information as possible is generally unknown and sample-dependent. Highly-programmable optical elements create new possibilities for sample-specific computational optimization of microscope parameters, e.g. tuning the collection of light for a given sample structure, especially using deep learning. This involves a differentiable simulation of light propagation through the programmable microscope and a neural network to reconstruct volumes from the microscope image. We introduce a class of global kernel Fourier convolutional neural networks which can efficiently integrate the globally mixed information encoded in a 3D snapshot image. We show in silico that our proposed global Fourier convolutional networks succeed in large field-of-view volume reconstruction and microscope parameter optimization where traditional networks fail.      
### 5.GAN-Based Data Augmentation and Anonymization for Skin-Lesion Analysis: A Critical Review  [ :arrow_down: ](https://arxiv.org/pdf/2104.10603.pdf)
>  Despite the growing availability of high-quality public datasets, the lack of training samples is still one of the main challenges of deep-learning for skin lesion analysis. Generative Adversarial Networks (GANs) appear as an enticing alternative to alleviate the issue, by synthesizing samples indistinguishable from real images, with a plethora of works employing them for medical applications. Nevertheless, carefully designed experiments for skin-lesion diagnosis with GAN-based data augmentation show favorable results only on out-of-distribution test sets. For GAN-based data anonymization $-$ where the synthetic images replace the real ones $-$ favorable results also only appear for out-of-distribution test sets. Because of the costs and risks associated with GAN usage, those results suggest caution in their adoption for medical applications.      
### 6.Using CNNs for AD classification based on spatial correlation of BOLD signals during the observation  [ :arrow_down: ](https://arxiv.org/pdf/2104.10596.pdf)
>  Resting state functional magnetic resonance images (fMRI) are commonly used for classification of patients as having Alzheimer's disease (AD), mild cognitive impairment (MCI), or being cognitive normal (CN). Most methods use time-series correlation of voxels signals during the observation period as a basis for the classification. In this paper we show that Convolutional Neural Network (CNN) classification based on spatial correlation of time-averaged signals yield a classification accuracy of up to 82% (sensitivity 86%, specificity 80%)for a data set with 429 subjects (246 cognitive normal and 183 Alzheimer patients). For the spatial correlation of time-averaged signal values we use voxel subdomains around center points of the 90 regions AAL atlas. We form the subdomains as sets of voxels along a Hilbert curve of a bounding box in which the brain is embedded with the AAL regions center points serving as subdomain seeds. The matrix resulting from the spatial correlation of the 90 arrays formed by the subdomain segments of the Hilbert curve yields a symmetric 90x90 matrix that is used for the classification based on two different CNN networks, a 4-layer CNN network with 3x3 filters and with 4, 8, 16, and 32 output channels respectively, and a 2-layer CNN network with 3x3 filters and with 4 and 8 output channels respectively. The results of the two networks are reported and compared.      
### 7.Rethinking annotation granularity for overcoming deep shortcut learning: A retrospective study on chest radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2104.10553.pdf)
>  Deep learning has demonstrated radiograph screening performances that are comparable or superior to radiologists. However, recent studies show that deep models for thoracic disease classification usually show degraded performance when applied to external data. Such phenomena can be categorized into shortcut learning, where the deep models learn unintended decision rules that can fit the identically distributed training and test set but fail to generalize to other distributions. A natural way to alleviate this defect is explicitly indicating the lesions and focusing the model on learning the intended features. In this paper, we conduct extensive retrospective experiments to compare a popular thoracic disease classification model, CheXNet, and a thoracic lesion detection model, CheXDet. We first showed that the two models achieved similar image-level classification performance on the internal test set with no significant differences under many scenarios. Meanwhile, we found incorporating external training data even led to performance degradation for CheXNet. Then, we compared the models' internal performance on the lesion localization task and showed that CheXDet achieved significantly better performance than CheXNet even when given 80% less training data. By further visualizing the models' decision-making regions, we revealed that CheXNet learned patterns other than the target lesions, demonstrating its shortcut learning defect. Moreover, CheXDet achieved significantly better external performance than CheXNet on both the image-level classification task and the lesion localization task. Our findings suggest improving annotation granularity for training deep learning systems as a promising way to elevate future deep learning-based diagnosis systems for clinical usage.      
### 8.Invertible Denoising Network: A Light Solution for Real Noise Removal  [ :arrow_down: ](https://arxiv.org/pdf/2104.10546.pdf)
>  Invertible networks have various benefits for image denoising since they are lightweight, information-lossless, and memory-saving during back-propagation. However, applying invertible models to remove noise is challenging because the input is noisy, and the reversed output is clean, following two different distributions. We propose an invertible denoising network, InvDN, to address this challenge. InvDN transforms the noisy input into a low-resolution clean image and a latent representation containing noise. To discard noise and restore the clean image, InvDN replaces the noisy latent representation with another one sampled from a prior distribution during reversion. The denoising performance of InvDN is better than all the existing competitive models, achieving a new state-of-the-art result for the SIDD dataset while enjoying less run time. Moreover, the size of InvDN is far smaller, only having 4.2% of the number of parameters compared to the most recently proposed DANet. Further, via manipulating the noisy latent representation, InvDN is also able to generate noise more similar to the original one. Our code is available at: <a class="link-external link-https" href="https://github.com/Yang-Liu1082/InvDN.git" rel="external noopener nofollow">this https URL</a>.      
### 9.Coded Divergent Waves for Fast Ultrasonic Imaging: Optimization and Comparative Performance Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2104.10526.pdf)
>  In this paper, we present the optimal use of coded signals in diverging wave transmission for fast ultrasonic imaging. The performance of coded imaging with diverging waves, quantified by SNR, CNR, speckle power and target signal strength, is optimized in terms of code length, wave profile, aperture size and field of view. We obtained one frame in 200 microseconds using coded diverging wave transmission, equivalent to very high 5000 frames/s rate, where the imaging depth is 7.5 cm. The performances of coded diverging wave transmission and conventional single focused phased array imaging are compared on a single frame basis. Complementary Golay sequences with code lengths ranging from 2 to 10 bits are used to code the signal. The signal strength and SNR obtained using synthetic transmit aperture and conventional single focused phased array imaging techniques, respectively, are used as reference in the performance analysis. We present a method to determine the optimum diverging wave profile to maximize the SNR across the entire viewing sector, which matches the received signal strength distribution to that of synthetic transmit aperture imaging. Optimum diverging wave profiles are determined for different sector angles and for different array apertures. For 90° sector, the SNR of 8-bit coded signal with optimized diverging wave profile is higher than that of conventional single focused phased array imaging at all depths and regions except the focal region, where it is 2-8 dB lower.      
### 10.A Two-Stage Attentive Network for Single Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2104.10488.pdf)
>  Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and contribute remarkable progress. However, most of the existing CNNs-based SISR methods do not adequately explore contextual information in the feature extraction stage and pay little attention to the final high-resolution (HR) image reconstruction step, hence hindering the desired SR performance. To address the above two issues, in this paper, we propose a two-stage attentive network (TSAN) for accurate SISR in a coarse-to-fine manner. Specifically, we design a novel multi-context attentive block (MCAB) to make the network focus on more informative contextual features. Moreover, we present an essential refined attention block (RAB) which could explore useful cues in HR space for reconstructing fine-detailed HR image. Extensive evaluations on four benchmark datasets demonstrate the efficacy of our proposed TSAN in terms of quantitative metrics and visual effects. Code is available at <a class="link-external link-https" href="https://github.com/Jee-King/TSAN" rel="external noopener nofollow">this https URL</a>.      
### 11.Hybrid Constructions of Binary Sequences with Low Autocorrelation Sidelobes  [ :arrow_down: ](https://arxiv.org/pdf/2104.10477.pdf)
>  In this work, a classical problem of the digital sequence design, or more precisely, finding binary sequences with optimal peak sidelobe level (PSL), is revisited. By combining some of our previous works, together with some mathematical insights, few hybrid heuristic algorithms were created. During our experiments, and by using the aforementioned algorithms, we were able to find PSL-optimal binary sequences for all those lengths, which were previously found during exhaustive searches by various papers throughout the literature. Then, by using a general-purpose computer, we further demonstrate the effectiveness of the proposed algorithms by revealing binary sequences with lengths between 106 and 300, the majority of which possess record-breaking PSL values. Then, by using some well-known algebraic constructions, we outline few strategies for finding highly competitive binary sequences, which could be efficiently optimized, in terms of PSL, by the proposed algorithms.      
### 12.Wide-Beam Array Antenna Power Gain Maximization via ADMM Framework  [ :arrow_down: ](https://arxiv.org/pdf/2104.10385.pdf)
>  This paper proposes two algorithms to maximize the minimum array power gain in a wide-beam mainlobe by solving the power gain pattern synthesis (PGPS) problem with and without sidelobe constraints. Firstly, the nonconvex PGPS problem is transformed into a nonconvex linear inequality optimization problem and then converted to an augmented Lagrangian problem by introducing auxiliary variables via the Alternating Direction Method of Multipliers (ADMM) framework. Next,the original intractable problem is converted into a series of nonconvex and convex subproblems. The nonconvex subproblems are solved by dividing their solution space into a finite set of smaller ones, in which the solution would be obtained pseudoanalytically. In such a way, the proposed algorithms are superior to the existing PGPS-based ones as their convergence can be theoretically guaranteed with a lower computational burden. Numerical examples with both isotropic element pattern (IEP) and active element pattern (AEP) arrays are simulated to show the effectiveness and superiority of the proposed algorithms by comparing with the related existing algorithms.      
### 13.Deep Learning Based Proactive Optimization for Indoor LiFi Systems with Channel Aging  [ :arrow_down: ](https://arxiv.org/pdf/2104.10384.pdf)
>  This paper investigates the channel aging problem of light-fidelity (LiFi) systems. In the LiFi physical layer, the majority of the optimization problems for mobile users are nonconvex and require the use of dual decomposition or heuristics techniques. Such techniques are based on iterative algorithms, and often, cause a high processing time at the physical layer. Hence, the obtained solutions are no longer optimal since the LiFi channels are evolving. In this paper, a proactive-optimization approach that can alleviate the LiFi channel aging problem is proposed. The core idea is to design a long-short-term memory (LSTM) network that is capable of predicting posterior positions and orientations of mobile users, which can be then used to predict their channel coefficients. Consequently, the obtained channel coefficients can be exploited for deriving near-optimal transmission-schemes prior to the intended service-time, which enables real-time service. Through various simulations, the performance of the designed LSTM model is evaluated in terms of prediction accuracy and time. Finally, the performance of the proposed PO approach is investigated in the sum rate maximization problem of multiuser cell-free LiFi systems with quality-of-service constraints, where a performance gap of less than 7% is achieved, while eliminating up to 100% of the online processing-time.      
### 14.Stochastic Model Predictive Control for Linear Systems with Unbounded Additive Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2104.10383.pdf)
>  This paper presents two stochastic model predictive control methods for linear time-invariant systems subject to unbounded additive uncertainties. The new methods are developed by formulating the chance constraints into deterministic form, which are treated in analogy with robust constraints, by using the probabilistic reachable set. Firstly, the probabilistically resolvable time-varying tube-based stochastic model predictive control algorithm is designed by employing the time-varying probabilistic reachable sets as the tubes. Secondly, by utilizing the probabilistic positively invariant set, the probabilistically resolvable constant tube-based stochastic model predictive control algorithm is developed by employing the constantly tightened constraints in the entire prediction horizons. In addition, to enhance the feasibility of the algorithms, the soft constraints are imposed to the state initializations. The algorithm feasibility and closed-loop stability results are provided. The efficacy of the approaches are demonstrated by means of numerical simulations.      
### 15.Control Contraction Metric Synthesis for Discrete-time Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.10352.pdf)
>  Flexible manufacturing has been the trend in the area of the modern chemical process nowadays. One of the essential characteristics of flexible manufacturing is to track time-varying target trajectories (e.g. diversity and quantity of products). A possible tool to achieve time-varying targets is contraction theory. However, the contraction theory was developed for continuous time systems and there lacks analysis and synthesis tools for discrete-time systems. This article develops a systematic approach to discrete-time contraction analysis and control synthesis using Discrete-time Control Contraction Metrics (DCCM) which can be implemented using Sum of Square (SOS) programming. The proposed approach is demonstrated by illustrative example.      
### 16.A Novel GCN based Indoor Localization System with Multiple Access Points  [ :arrow_down: ](https://arxiv.org/pdf/2104.10342.pdf)
>  With the rapid development of indoor location-based services (LBSs), the demand for accurate localization keeps growing as well. To meet this demand, we propose an indoor localization algorithm based on graph convolutional network (GCN). We first model access points (APs) and the relationships between them as a graph, and utilize received signal strength indication (RSSI) to make up fingerprints. Then the graph and the fingerprint will be put into GCN for feature extraction, and get classification by multilayer perceptron (MLP).In the end, experiments are performed under a 2D scenario and 3D scenario with floor prediction. In the 2D scenario, the mean distance error of GCN-based method is 11m, which improves by 7m and 13m compare with DNN-based and CNN-based schemes respectively. In the 3D scenario, the accuracy of predicting buildings and floors are up to 99.73% and 93.43% respectively. Moreover, in the case of predicting floors and buildings correctly, the mean distance error is 13m, which outperforms DNN-based and CNN-based schemes, whose mean distance errors are 34m and 26m respectively.      
### 17.Label-Synchronous Speech-to-Text Alignment for ASR Using Forward and Backward Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2104.10328.pdf)
>  This paper proposes a novel label-synchronous speech-to-text alignment technique for automatic speech recognition (ASR). The speech-to-text alignment is a problem of splitting long audio recordings with un-aligned transcripts into utterance-wise pairs of speech and text. Unlike conventional methods based on frame-synchronous prediction, the proposed method re-defines the speech-to-text alignment as a label-synchronous text mapping problem. This enables an accurate alignment benefiting from the strong inference ability of the state-of-the-art attention-based encoder-decoder models, which cannot be applied to the conventional methods. Two different Transformer models named forward Transformer and backward Transformer are respectively used for estimating an initial and final tokens of a given speech segment based on end-of-sentence prediction with teacher-forcing. Experiments using the corpus of spontaneous Japanese (CSJ) demonstrate that the proposed method provides an accurate utterance-wise alignment, that matches the manually annotated alignment with as few as 0.2% errors. It is also confirmed that a Transformer-based hybrid CTC/Attention ASR model using the aligned speech and text pairs as an additional training data reduces character error rates relatively up to 59.0%, which is significantly better than 39.0% reduction by a conventional alignment method based on connectionist temporal classification model.      
### 18.A Structure-Aware Relation Network for Thoracic Diseases Detection and Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2104.10326.pdf)
>  Instance level detection and segmentation of thoracic diseases or abnormalities are crucial for automatic diagnosis in chest X-ray images. Leveraging on constant structure and disease relations extracted from domain knowledge, we propose a structure-aware relation network (SAR-Net) extending Mask R-CNN. The SAR-Net consists of three relation modules: 1. the anatomical structure relation module encoding spatial relations between diseases and anatomical parts. 2. the contextual relation module aggregating clues based on query-key pair of disease RoI and lung fields. 3. the disease relation module propagating co-occurrence and causal relations into disease proposals. Towards making a practical system, we also provide ChestX-Det, a chest X-Ray dataset with instance-level annotations (boxes and masks). ChestX-Det is a subset of the public dataset NIH ChestX-ray14. It contains ~3500 images of 13 common disease categories labeled by three board-certified radiologists. We evaluate our SAR-Net on it and another dataset DR-Private. Experimental results show that it can enhance the strong baseline of Mask R-CNN with significant improvements. The ChestX-Det is released at <a class="link-external link-https" href="https://github.com/Deepwise-AILab/ChestX-Det-Dataset" rel="external noopener nofollow">this https URL</a>.      
### 19.Feedback Stabilization and Output Tracking for Discrete-Time Lipschitz Nonlinear Systems via Iterative Convex Approximations  [ :arrow_down: ](https://arxiv.org/pdf/2104.10320.pdf)
>  The stabilization of unstable nonlinear systems and tracking control are challenging engineering problems due to the encompassed nonlinearities in dynamic systems and their scale. In the past decades, numerous observer-based control designs for dynamic systems in which the nonlinearity belongs to Lipschitz functions have been proposed. However, most of them only focus on output feedback and consequently, state feedback design remains less developed. To that end, this paper is dedicated to the problem of full-state feedback controller design for discrete-time Lipschitz nonlinear systems. In addition, we present a simple iterative method for improving the convergence of the closed-loop performance. It is later demonstrated that our approach can be conveniently extended and utilized for output tracking.      
### 20.Visual Analysis Motivated Rate-Distortion Model for Image Coding  [ :arrow_down: ](https://arxiv.org/pdf/2104.10315.pdf)
>  Optimized for pixel fidelity metrics, images compressed by existing image codec are facing systematic challenges when used for visual analysis tasks, especially under low-bitrate coding. This paper proposes a visual analysis-motivated rate-distortion model for Versatile Video Coding (VVC) intra compression. The proposed model has two major contributions, a novel rate allocation strategy and a new distortion measurement model. We first propose the region of interest for machine (ROIM) to evaluate the degree of importance for each coding tree unit (CTU) in visual analysis. Then, a novel CTU-level bit allocation model is proposed based on ROIM and the local texture characteristics of each CTU. After an in-depth analysis of multiple distortion models, a visual analysis friendly distortion criteria is subsequently proposed by extracting deep feature of each coding unit (CU). To alleviate the problem of lacking spatial context information when calculating the distortion of each CU, we finally propose a multi-scale feature distortion (MSFD) metric using different neighboring pixels by weighting the extracted deep features in each scale. Extensive experimental results show that the proposed scheme could achieve up to 28.17\% bitrate saving under the same analysis performance among several typical visual analysis tasks such as image classification, object detection, and semantic segmentation.      
### 21.Distributed Cooperative Driving in Multi-Intersection Road Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.10313.pdf)
>  Cooperative driving at isolated intersections attracted great interest and had been well discussed in recent years. However, cooperative driving in multi-intersection road networks remains to be further investigated, because many algorithms for isolated intersection cannot be directly adopted for road networks. In this paper, we propose a distributed strategy to appropriately decompose the problem into small-scale sub-problems that address vehicle cooperation within limited temporal-spatial areas and meanwhile assure appropriate coordination between adjacent areas by specially designed information exchange. Simulation results demonstrate the efficiency-complexity balanced advantage of the proposed strategy under various traffic demand settings.      
### 22.Analyzing the Effect of Persistent Asset Switches on a Class of Hybrid-Inspired Optimization Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2104.10307.pdf)
>  Convex optimization challenges are currently pervasive in many science and engineering domains. In many applications of convex optimization, such as those involving multi-agent systems and resource allocation, the objective function can persistently switch during the execution of an optimization algorithm. Motivated by such applications, we analyze the effect of persistently switching objectives in continuous-time optimization algorithms. In particular, we take advantage of existing robust stability results for switched systems with distinct equilibria and extend these results to systems described by differential inclusions, making the results applicable to recent optimization algorithms that employ differential inclusions for improving efficiency and/or robustness. Within the framework of hybrid systems theory, we provide an accurate characterization, in terms of Omega-limit sets, of the set to which the optimization dynamics converge. Finally, by considering the switching signal to be constrained in its average dwell time, we establish semi-global practical asymptotic stability of these sets with respect to the dwell-time parameter.      
### 23.Amplifier-Coupled Tone Reservation for Minimization of OFDM Nonlinear Distortion  [ :arrow_down: ](https://arxiv.org/pdf/2104.10284.pdf)
>  Nonlinear distortion of an OFDM signal is a serious problem when it comes to energy-efficient Power Amplifier(PA) utilization. Typically, Peak-to-Average Power Ratio(PAPR) reduction algorithms and digital predistortion algorithms are used independently to fight the same phenomenon. This paper proposes an Amplifier-Coupled Tone Reservation (ACTR)algorithm for the reduction of nonlinear distortion power, utilizing knowledge on thep redistorted PA characteristic. The optimization problem is defined. Its convexity is proved. A computationally-efficient solution is presented. Finally, its performance is compared against two state-of-the-art TR algorithms by means of simulations and measurements. The results show the proposed solution is advantageous, both in terms of nonlinear distortion power and the required number of computations.      
### 24.TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatio-Temporal Single Image Super Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2104.10268.pdf)
>  Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from aremotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks(GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, thegenerated image still suffers from undesirable artifacts such as, the absence of texture-feature representationand high-frequency information. We propose a frequency domain-based spatio-temporal remote sensingsingle image super-resolution technique to reconstruct the HR image combined with generative adversarialnetworks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporatingWavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image hasbeen split into various frequency bands by using the WT, whereas, the transfer generative adversarial networkpredicts high-frequency components via a proposed architecture. Finally, the inverse transfer of waveletsproduces a reconstructed image with super-resolution. The model is first trained on an external DIV2 Kdataset and validated with the UC Merceed Landsat remote sensing dataset and Set14 with each image sizeof 256x256. Following that, transferred GANs are used to process spatio-temporal remote sensing images inorder to minimize computation cost differences and improve texture information. The findings are comparedqualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of theGPU memory during training and accelerated the execution of our simplified version by eliminating batchnormalization layers.      
### 25.Variance Reduction of Quadcopter Trajectory Tracking under Stochastic Wind Disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2104.10266.pdf)
>  We consider a quadcopter operating in a turbulent windy environment. The turbulent environment may be imposed on a quadcopter by structures, landscapes, terrains and most importantly by the unique physical phenomena in the lower atmosphere. Turbulence can negatively impact quadcopter's performance and operations. Modeling turbulence as a stochastic random input, we investigate control designs that can reduce the turbulence effects on the quadcopter's motion. In particular, we design a minimum cost variance (MCV) controller aiming to minimize the cost in terms of its weighted sum of mean and variance. We linearize the quadcopter dynamics and examine the MCV controller derived from a set of coupled algebraic Riccati equations (CARE) with full-state feedback. Our preliminary simulation results show reduction in variance and in mean trajectory tracking error compared to a traditional linear quadratic regulator (LQR).      
### 26.Broadband Radar Invisibility with Time-Dependent Metasurfaces  [ :arrow_down: ](https://arxiv.org/pdf/2104.10229.pdf)
>  Concealing objects from interrogation has been a primary objective since the integration of radars into surveillance systems. Metamaterial-based invisibility cloaking, which was considered a promising solution, did not yet succeed in delivering reliable performance against real radar systems, mainly due to its narrow operational bandwidth. Here we propose an approach, which addresses the issue from a signal-processing standpoint and, as a result, is capable of coping with the vast majority of unclassified radar systems by exploiting vulnerabilities in their design. In particular, we demonstrate complete concealment of a 0.25 square meter moving metal plate from an investigating radar system, operating in a broad frequency range approaching 20% bandwidth around the carrier of 1.5GHz. The key element of the radar countermeasure is a temporally modulated coating. This auxiliary structure is designed to dynamically and controllably adjust the reflected phase of the impinging radar signal, which acquires a user-defined Doppler shift. A special case of interest is imposing a frequency shift that compensates for the real Doppler signatures originating from the motion of the target. In this case the radar will consider the target static, even though it is moving. As a result, the reflected echo will be discarded by the clutter removal filter, which is an inherent part of any modern radar system that is designed to operate in real conditions. This signal-processing loophole allows rendering the target invisible to the radar even though it scatters electromagnetic radiation. This application claims the benefit of patent number 273995, filed on April 16 2020.      
### 27.Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.10219.pdf)
>  There has been significant recent interest in devising verification techniques for learning-enabled controllers (LECs) that manage safety-critical systems. Given the opacity and lack of interpretability of the neural policies that govern the behavior of such controllers, many existing approaches enforce safety properties through the use of shields, a dynamic monitoring and repair mechanism that ensures a LEC does not emit actions that would violate desired safety conditions. These methods, however, have shown to have significant scalability limitations because verification costs grow as problem dimensionality and objective complexity increase. In this paper, we propose a new automated verification pipeline capable of synthesizing high-quality safety shields even when the problem domain involves hundreds of dimensions, or when the desired objective involves stochastic perturbations, liveness considerations, and other complex non-functional properties. Our key insight involves separating safety verification from neural controller, using pre-computed verified safety shields to constrain neural controller training which does not only focus on safety. Experimental results over a range of realistic high-dimensional deep RL benchmarks demonstrate the effectiveness of our approach.      
### 28.Bias-Aware Loss for Training Image and Speech Quality Prediction Models from Multiple Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2104.10217.pdf)
>  The ground truth used for training image, video, or speech quality prediction models is based on the Mean Opinion Scores (MOS) obtained from subjective experiments. Usually, it is necessary to conduct multiple experiments, mostly with different test participants, to obtain enough data to train quality models based on machine learning. Each of these experiments is subject to an experiment-specific bias, where the rating of the same file may be substantially different in two experiments (e.g. depending on the overall quality distribution). These different ratings for the same distortion levels confuse neural networks during training and lead to lower performance. To overcome this problem, we propose a bias-aware loss function that estimates each dataset's biases during training with a linear function and considers it while optimising the network weights. We prove the efficiency of the proposed method by training and validating quality prediction models on synthetic and subjective image and speech quality datasets.      
### 29.Auto-FedAvg: Learnable Federated Averaging for Multi-Institutional Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2104.10195.pdf)
>  Federated learning (FL) enables collaborative model training while preserving each participant's privacy, which is particularly beneficial to the medical field. FedAvg is a standard algorithm that uses fixed weights, often originating from the dataset sizes at each client, to aggregate the distributed learned models on a server during the FL process. However, non-identical data distribution across clients, known as the non-i.i.d problem in FL, could make this assumption for setting fixed aggregation weights sub-optimal. In this work, we design a new data-driven approach, namely Auto-FedAvg, where aggregation weights are dynamically adjusted, depending on data distributions across data silos and the current training progress of the models. We disentangle the parameter set into two parts, local model parameters and global aggregation parameters, and update them iteratively with a communication-efficient algorithm. We first show the validity of our approach by outperforming state-of-the-art FL methods for image recognition on a heterogeneous data split of CIFAR-10. Furthermore, we demonstrate our algorithm's effectiveness on two multi-institutional medical image analysis tasks, i.e., COVID-19 lesion segmentation in chest CT and pancreas segmentation in abdominal CT.      
### 30.A Comparative Study of Using Spatial-Temporal Graph Convolutional Networks for Predicting Availability in Bike Sharing Schemes  [ :arrow_down: ](https://arxiv.org/pdf/2104.10644.pdf)
>  Accurately forecasting transportation demand is crucial for efficient urban traffic guidance, control and management. One solution to enhance the level of prediction accuracy is to leverage graph convolutional networks (GCN), a neural network based modelling approach with the ability to process data contained in graph based structures. As a powerful extension of GCN, a spatial-temporal graph convolutional network (ST-GCN) aims to capture the relationship of data contained in the graphical nodes across both spatial and temporal dimensions, which presents a novel deep learning paradigm for the analysis of complex time-series data that also involves spatial information as present in transportation use cases. In this paper, we present an Attention-based ST-GCN (AST-GCN) for predicting the number of available bikes in bike-sharing systems in cities, where the attention-based mechanism is introduced to further improve the performance of a ST-GCN. Furthermore, we also discuss the impacts of different modelling methods of adjacency matrices on the proposed architecture. Our experimental results are presented using two real-world datasets, Dublinbikes and NYC-Citi Bike, to illustrate the efficacy of our proposed model which outperforms the majority of existing approaches.      
### 31.Photothermal-SR-Net: A Customized Deep Unfolding Neural Network for Photothermal Super Resolution Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2104.10563.pdf)
>  This paper presents deep unfolding neural networks to handle inverse problems in photothermal radiometry enabling super resolution (SR) imaging. Photothermal imaging is a well-known technique in active thermography for nondestructive inspection of defects in materials such as metals or composites. A grand challenge of active thermography is to overcome the spatial resolution limitation imposed by heat diffusion in order to accurately resolve each defect. The photothermal SR approach enables to extract high-frequency spatial components based on the deconvolution with the thermal point spread function. However, stable deconvolution can only be achieved by using the sparse structure of defect patterns, which often requires tedious, hand-crafted tuning of hyperparameters and results in computationally intensive algorithms. On this account, Photothermal-SR-Net is proposed in this paper, which performs deconvolution by deep unfolding considering the underlying physics. This enables to super resolve 2D thermal images for nondestructive testing with a substantially improved convergence rate. Since defects appear sparsely in materials, Photothermal-SR-Net applies trained block-sparsity thresholding to the acquired thermal images in each convolutional layer. The performance of the proposed approach is evaluated and discussed using various deep unfolding and thresholding approaches applied to 2D thermal images. Subsequently, studies are conducted on how to increase the reconstruction quality and the computational performance of Photothermal-SR-Net is evaluated. Thereby, it was found that the computing time for creating high-resolution images could be significantly reduced without decreasing the reconstruction quality by using pixel binning as a preprocessing step.      
### 32.Multi-RAT for IoT: The Potential in Combining LoRaWAN and NB-IoT  [ :arrow_down: ](https://arxiv.org/pdf/2104.10536.pdf)
>  The broad range of requirements of Internet of Things applications has lead to the development of several dedicated communication technologies, each tailored to meet a specific feature set. A solution combining different wireless technologies in one device, can overcome the disadvantages of any individual technology. The design of such Multiple Radio Access Technology solutions based on the diverse characteristics of the technologies offers interesting opportunities. In this work we analyze the potential of combining LoRaWAN and NB-IoT in a Multi-RAT solution for IoT. To that end we evaluate key IoT node requirements in function of payload size and link quality: (1) energy efficiency, (2) coverage, (3) payload size, (4) latency performance, (5) Quality of Service, and (6) cost efficiency. Our theoretical assessment and experimental validation of these IoT features show the merits of a Multi-RAT solution. Notably, energy consumption in use cases with only sporadic large payload requirements, can be improved by a factor of at least 4 with respect to either single-mode technologies. Moreover, latency-critical messages can get delivered on time and coverage can be extended elegantly where needed.      
### 33.On Sampling-Based Training Criteria for Neural Language Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2104.10507.pdf)
>  As the vocabulary size of modern word-based language models becomes ever larger, many sampling-based training criteria are proposed and investigated. The essence of these sampling methods is that the softmax-related traversal over the entire vocabulary can be simplified, giving speedups compared to the baseline. A problem we notice about the current landscape of such sampling methods is the lack of a systematic comparison and some myths about preferring one over another. In this work, we consider Monte Carlo sampling, importance sampling, a novel method we call compensated partial summation, and noise contrastive estimation. Linking back to the three traditional criteria, namely mean squared error, binary cross-entropy, and cross-entropy, we derive the theoretical solutions to the training problems. Contrary to some common belief, we show that all these sampling methods can perform equally well, as long as we correct for the intended class posterior probabilities. Experimental results in language modeling and automatic speech recognition on Switchboard and LibriSpeech support our claim, with all sampling-based methods showing similar perplexities and word error rates while giving the expected speedups.      
### 34.HDR-Fuzz: Detecting Buffer Overruns using AddressSanitizer Instrumentation and Fuzzing  [ :arrow_down: ](https://arxiv.org/pdf/2104.10466.pdf)
>  Buffer-overruns are a prevalent vulnerability in software libraries and applications. Fuzz testing is one of the effective techniques to detect vulnerabilities in general. Greybox fuzzers such as AFL automatically generate a sequence of test inputs for a given program using a fitness-guided search process. A recently proposed approach in the literature introduced a buffer-overrun specific fitness metric called "headroom", which tracks how close each generated test input comes to exposing the vulnerabilities. That approach showed good initial promise, but is somewhat imprecise and expensive due to its reliance on conservative points-to analysis. Inspired by the approach above, in this paper we propose a new ground-up approach for detecting buffer-overrun vulnerabilities. This approach uses an extended version of ASAN (Address Sanitizer) that runs in parallel with the fuzzer, and reports back to the fuzzer test inputs that happen to come closer to exposing buffer-overrun vulnerabilities. The ASAN-style instrumentation is precise as it has no dependence on points-to analysis. We describe in this paper our approach, as well as an implementation and evaluation of the approach.      
### 35.Reinforcement Learning for Traffic Signal Control: Comparison with Commercial Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.10455.pdf)
>  Recently, Intelligent Transportation Systems are leveraging the power of increased sensory coverage and computing power to deliver data-intensive solutions achieving higher levels of performance than traditional systems. Within Traffic Signal Control (TSC), this has allowed the emergence of Machine Learning (ML) based systems. Among this group, Reinforcement Learning (RL) approaches have performed particularly well. Given the lack of industry standards in ML for TSC, literature exploring RL often lacks comparison against commercially available systems and straightforward formulations of how the agents operate. Here we attempt to bridge that gap. We propose three different architectures for TSC RL agents and compare them against the currently used commercial systems MOVA, SurTrac and Cyclic controllers and provide pseudo-code for them. The agents use variations of Deep Q-Learning and Actor Critic, using states and rewards based on queue lengths. Their performance is compared in across different map scenarios with variable demand, assessing them in terms of the global delay and average queue length. We find that the RL-based systems can significantly and consistently achieve lower delays when compared with existing commercial systems.      
### 36.Room adaptive conditioning method for sound event classification in reverberant environments  [ :arrow_down: ](https://arxiv.org/pdf/2104.10431.pdf)
>  Ensuring performance robustness for a variety of situations that can occur in real-world environments is one of the challenging tasks in sound event classification. One of the unpredictable and detrimental factors in performance, especially in indoor environments, is reverberation. To alleviate this problem, we propose a conditioning method that provides room impulse response (RIR) information to help the network become less sensitive to environmental information and focus on classifying the desired sound. Experimental results show that the proposed method successfully reduced performance degradation caused by the reverberation of the room. In particular, our proposed method works even with similar RIR that can be inferred from the room type rather than the exact one, which has the advantage of potentially being used in real-world applications.      
### 37.FD-JCAS Techniques for mmWave HetNets: Ginibre Point Process Modeling and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2104.10418.pdf)
>  In this paper, we study the co-design of full-duplex (FD) radio with joint communication and radar sensing (JCAS) techniques in millimeter-wave (mmWave) heterogeneous networks (HetNets). Spectral co-existence of radar and communication systems causes mutual interference between the two systems, compromising both the data exchange and sensing capabilities. Focusing on the detection performance, we propose a cooperative detection technique, which exploits the sensing information from multiple base stations (BSs), aiming at enhancing the probability of successfully detecting an object. Three combining rules are considered, namely the \textit{OR}, the \textit{Majority} and the \textit{AND} rule. In real-world network scenarios, the locations of the BSs are spatially correlated, exhibiting a repulsive behavior. Therefore, we model the spatial distribution of the BSs as a $\beta$-Ginibre point process ($\beta$-GPP), which can characterize the repulsion among the BSs. By using stochastic geometry tools, analytical expressions for the detection performance of $\beta$-GPP-based FD-JCAS systems are expressed for each of the considered combining rule. Furthermore, by considering temporal interference correlation, we evaluate the probability of successfully detecting an object over two different time slots. Our results demonstrate that our proposed technique can significantly improve the detection performance when compared to the conventional non-cooperative technique.      
### 38.Uplink Performance Analysis of Cell-Free mMIMO Systems under Channel Aging  [ :arrow_down: ](https://arxiv.org/pdf/2104.10404.pdf)
>  In this paper, we investigate the impact of channel aging on the uplink performance of a cell-free~(CF) massive multiple-input multiple-output (mMIMO) system with a minimum mean squared error (MMSE) receiver. To this end, we present a new model for the temporal evolution of the channel, which allows the channel to age at different rates at different access points (APs). Under this setting, we derive the deterministic equivalent of the per-user achievable signal-to-interference-plus-noise ratio (SINR). In addition to validating the theoretical expressions, our simulation results reveal that, {at low user mobilities,} the SINR of CF-mMIMO is nearly 5 dB higher than its cellular counterpart with the same number of antennas, and about 8 dB higher than that of an equivalent small-cell network with the same number of APs. {On the other hand, at very high user velocities, and when the channel between the UEs the different APs age at same rate, the relative impact of aging is higher for CF-mMIMO compared to cellular mMIMO. However, when the channel ages at the different APs with different rates, the effect of aging on CF-mMIMO is marginally mitigated, especially for larger frame durations.      
### 39.Game Theory to Study Interactions between Mobility Stakeholders  [ :arrow_down: ](https://arxiv.org/pdf/2104.10394.pdf)
>  Increasing urbanization and exacerbation of sustainability goals threaten the operational efficiency of current transportation systems and confront cities with complex choices with huge impact on future generations. At the same time, the rise of private, profit-maximizing Mobility Service Providers leveraging public resources, such as ride-hailing companies, entangles current regulation schemes. This calls for tools to study such complex socio-technical problems. In this paper, we provide a game-theoretic framework to study interactions between stakeholders of the mobility ecosystem, modeling regulatory aspects such as taxes and public transport prices, as well as operational matters for Mobility Service Providers such as pricing strategy, fleet sizing, and vehicle design. Our framework is modular and can readily accommodate different types of Mobility Service Providers, actions of municipalities, and low-level models of customers choices in the mobility system. Through both an analytical and a numerical case study for the city of Berlin, Germany, we showcase the ability of our framework to compute equilibria of the problem, to study fundamental tradeoffs, and to inform stakeholders and policy makers on the effects of interventions. Among others, we show tradeoffs between customers satisfaction, environmental impact, and public revenue, as well as the impact of strategic decisions on these metrics.      
### 40.Wireless Sensing With Deep Spectrogram Network and Primitive Based Autoregressive Hybrid Channel Model  [ :arrow_down: ](https://arxiv.org/pdf/2104.10378.pdf)
>  Human motion recognition (HMR) based on wireless sensing is a low-cost technique for scene understanding. Current HMR systems adopt support vector machines (SVMs) and convolutional neural networks (CNNs) to classify radar signals. However, whether a deeper learning model could improve the system performance is currently not known. On the other hand, training a machine learning model requires a large dataset, but data gathering from experiment is cost-expensive and time-consuming. Although wireless channel models can be adopted for dataset generation, current channel models are mostly designed for communication rather than sensing. To address the above problems, this paper proposes a deep spectrogram network (DSN) by leveraging the residual mapping technique to enhance the HMR performance. Furthermore, a primitive based autoregressive hybrid (PBAH) channel model is developed, which facilitates efficient training and testing dataset generation for HMR in a virtual environment. Experimental results demonstrate that the proposed PBAH channel model matches the actual experimental data very well and the proposed DSN achieves significantly smaller recognition error than that of CNN.      
### 41.Bipedal Walking on Constrained Footholds: Momentum Regulation via Vertical COM Control  [ :arrow_down: ](https://arxiv.org/pdf/2104.10367.pdf)
>  This paper presents an online walking gait synthesis and a feedback control methodology to enable stable walking on constrained footholds for bipedal robots. For this challenging task, the foot placement and center of pressure cannot be changed, which hinders the application of state-of-art stepping controllers or zero-moment-point (ZMP) based approaches for walking generation. As a result, this paper takes a different approach to modulate the change of the angular momentum about the foot-ground contact pivot at the discrete impact with vertical center of mass (COM) velocity. We utilize the underactuated Linear Inverted Pendulum (LIP) model for approximating the underactuated walking dynamics to provide the desired post-impact angular momentum for each step. Outputs are constructed via online optimization combined with closed-form polynomials and then tracked via a quadratic program based controller. This method is implemented online on two robot models, AMBER and Cassie, for which stable walking behaviors with constrained footholds are realized on flat ground, stairs, and randomly located stepping stones.      
### 42.Fixed-Point and Objective Convergence of Plug-and-Play Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2104.10348.pdf)
>  A standard model for image reconstruction involves the minimization of a data-fidelity term along with a regularizer, where the optimization is performed using proximal algorithms such as ISTA and ADMM. In plug-and-play (PnP) regularization, the proximal operator (associated with the regularizer) in ISTA and ADMM is replaced by a powerful image denoiser. Although PnP regularization works surprisingly well in practice, its theoretical convergence -- whether convergence of the PnP iterates is guaranteed and if they minimize some objective function -- is not completely understood even for simple linear denoisers such as nonlocal means. In particular, while there are works where either iterate or objective convergence is established separately, a simultaneous guarantee on iterate and objective convergence is not available for any denoiser to our knowledge. In this paper, we establish both forms of convergence for a special class of linear denoisers. Notably, unlike existing works where the focus is on symmetric denoisers, our analysis covers non-symmetric denoisers such as nonlocal means and almost any convex data-fidelity. The novelty in this regard is that we make use of the convergence theory of averaged operators and we work with a special inner product (and norm) derived from the linear denoiser; the latter requires us to appropriately define the gradient and proximal operators associated with the data-fidelity term. We validate our convergence results using image reconstruction experiments.      
### 43.Measuring economic activity from space: a case study using flying airplanes and COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2104.10345.pdf)
>  This work introduces a novel solution to measure economic activity through remote sensing for a wide range of spatial areas. We hypothesized that disturbances in human behavior caused by major life-changing events leave signatures in satellite imagery that allows devising relevant image-based indicators to estimate their impacts and support decision-makers. We present a case study for the COVID-19 coronavirus outbreak, which imposed severe mobility restrictions and caused worldwide disruptions, using flying airplane detection around the 30 busiest airports in Europe to quantify and analyze the lockdown's effects and post-lockdown recovery. Our solution won the Rapid Action Coronavirus Earth observation (RACE) upscaling challenge, sponsored by the European Space Agency and the European Commission, and now integrates the RACE dashboard. This platform combines satellite data and artificial intelligence to promote a progressive and safe reopening of essential activities. Code and CNN models are available at <a class="link-external link-https" href="https://github.com/maups/covid19-custom-script-contest" rel="external noopener nofollow">this https URL</a>      
### 44.CVLight: Deep Reinforcement Learning for Adaptive Traffic Signal Control with Connected Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2104.10340.pdf)
>  This paper develops a reinforcement learning (RL) scheme for adaptive traffic signal control (ATSC), called "CVLight", that leverages data collected only from connected vehicles (CV). Seven types of RL models are proposed within this scheme that contain various state and reward representations, including incorporation of CV delay and green light duration into state and the usage of CV delay as reward. To further incorporate information of both CV and non-CV into CVLight, an algorithm based on actor-critic, A2C-Full, is proposed where both CV and non-CV information is used to train the critic network, while only CV information is used to update the policy network and execute optimal signal timing. These models are compared at an isolated intersection under various CV market penetration rates. A full model with the best performance (i.e., minimum average travel delay per vehicle) is then selected and applied to compare with state-of-the-art benchmarks under different levels of traffic demands, turning proportions, and dynamic traffic demands, respectively. Two case studies are performed on an isolated intersection and a corridor with three consecutive intersections located in Manhattan, New York, to further demonstrate the effectiveness of the proposed algorithm under real-world scenarios. Compared to other baseline models that use all vehicle information, the trained CVLight agent can efficiently control multiple intersections solely based on CV data and can achieve a similar or even greater performance when the CV penetration rate is no less than 20%.      
### 45.Shadow Generation for Composite Image in Real-world Scenes  [ :arrow_down: ](https://arxiv.org/pdf/2104.10338.pdf)
>  Image composition targets at inserting a foreground object on a background image. Most previous image composition methods focus on adjusting the foreground to make it compatible with background while ignoring the shadow effect of foreground on the background. In this work, we focus on generating plausible shadow for the foreground object in the composite image. First, we contribute a real-world shadow generation dataset DESOBA by generating synthetic composite images based on paired real images and deshadowed images. Then, we propose a novel shadow generation network SGRNet, which consists of a shadow mask prediction stage and a shadow filling stage. In the shadow mask prediction stage, foreground and background information are thoroughly interacted to generate foreground shadow mask. In the shadow filling stage, shadow parameters are predicted to fill the shadow area. Extensive experiments on our DESOBA dataset and real composite images demonstrate the effectiveness of our proposed method.      
### 46.Efficient Sparse Coding using Hierarchical Riemannian Pursuit  [ :arrow_down: ](https://arxiv.org/pdf/2104.10314.pdf)
>  Sparse coding is a class of unsupervised methods for learning a sparse representation of the input data in the form of a linear combination of a dictionary and a sparse code. This learning framework has led to state-of-the-art results in various image and video processing tasks. However, classical methods learn the dictionary and the sparse code based on alternative optimizations, usually without theoretical guarantees for either optimality or convergence due to non-convexity of the problem. Recent works on sparse coding with a complete dictionary provide strong theoretical guarantees thanks to the development of the non-convex optimization. However, initial non-convex approaches learn the dictionary in the sparse coding problem sequentially in an atom-by-atom manner, which leads to a long execution time. More recent works seek to directly learn the entire dictionary at once, which substantially reduces the execution time. However, the associated recovery performance is degraded with a finite number of data samples. In this paper, we propose an efficient sparse coding scheme with a two-stage optimization. The proposed scheme leverages the global and local Riemannian geometry of the two-stage optimization problem and facilitates fast implementation for superb dictionary recovery performance by a finite number of samples without atom-by-atom calculation. We further prove that, with high probability, the proposed scheme can exactly recover any atom in the target dictionary with a finite number of samples if it is adopted to recover one atom of the dictionary. An application on wireless sensor data compression is also proposed. Experiments on both synthetic and real-world data verify the efficiency and effectiveness of the proposed scheme.      
### 47.Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices  [ :arrow_down: ](https://arxiv.org/pdf/2104.10299.pdf)
>  This work focuses on the analysis that whether 3D face models can be learned from only the speech inputs of speakers. Previous works for cross-modal face synthesis study image generation from voices. However, image synthesis includes variations such as hairstyles, backgrounds, and facial textures, that are arguably irrelevant to voice or without direct studies to show correlations. We instead investigate the ability to reconstruct 3D faces to concentrate on only geometry, which is more physiologically grounded. We propose both the supervised learning and unsupervised learning frameworks. Especially we demonstrate how unsupervised learning is possible in the absence of a direct voice-to-3D-face dataset under limited availability of 3D face scans when the model is equipped with knowledge distillation. To evaluate the performance, we also propose several metrics to measure the geometric fitness of two 3D faces based on points, lines, and regions. We find that 3D face shapes can be reconstructed from voices. Experimental results suggest that 3D faces can be reconstructed from voices, and our method can improve the performance over the baseline. The best performance gains (15% - 20%) on ear-to-ear distance ratio metric (ER) coincides with the intuition that one can roughly envision whether a speaker's face is overall wider or thinner only from a person's voice. See our project page for codes and data.      
### 48.Decoding the shift-invariant data: applications for band-excitation scanning probe microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2104.10207.pdf)
>  A shift-invariant variational autoencoder (shift-VAE) is developed as an unsupervised method for the analysis of spectral data in the presence of shifts along the parameter axis, disentangling the physically-relevant shifts from other latent variables. Using synthetic data sets, we show that the shift-VAE latent variables closely match the ground truth parameters. The shift VAE is extended towards the analysis of band-excitation piezoresponse force microscopy (BE-PFM) data, disentangling the resonance frequency shifts from the peak shape parameters in a model-free unsupervised manner. The extensions of this approach towards denoising of data and model-free dimensionality reduction in imaging and spectroscopic data are further demonstrated. This approach is universal and can also be extended to analysis of X-ray diffraction, photoluminescence, Raman spectra, and other data sets.      
