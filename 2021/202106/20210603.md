# ArXiv eess --Thu, 3 Jun 2021
### 1.Deep Clustering Activation Maps for Emphysema Subtyping  [ :arrow_down: ](https://arxiv.org/pdf/2106.01351.pdf)
>  We propose a deep learning clustering method that exploits dense features from a segmentation network for emphysema subtyping from computed tomography (CT) scans. Using dense features enables high-resolution visualization of image regions corresponding to the cluster assignment via dense clustering activation maps (dCAMs). This approach provides model interpretability. We evaluated clustering results on 500 subjects from the COPDGenestudy, where radiologists manually annotated emphysema sub-types according to their visual CT assessment. We achieved a 43% unsupervised clustering accuracy, outperforming our baseline at 41% and yielding results comparable to supervised classification at 45%. The proposed method also offers a better cluster formation than the baseline, achieving0.54 in silhouette coefficient and 0.55 in David-Bouldin scores.      
### 2.End-To-End Deep Learning-Based Adaptation Control for Frequency-Domain Adaptive System Identification  [ :arrow_down: ](https://arxiv.org/pdf/2106.01262.pdf)
>  We present a novel end-to-end deep learning-based adaptation control algorithm for frequency-domain adaptive system identification. The proposed method exploits a deep neural network to map observed signal features to corresponding step-sizes which control the filter adaptation. The parameters of the network are optimized in an end-to-end fashion by minimizing the average system distance of the adaptive filter. This avoids the need of explicit signal power spectral density estimation as required for model-based adaptation control and further auxiliary mechanisms to deal with model inaccuracies. The proposed algorithm achieves fast convergence and robust steady-state performance for scenarios characterized by non-white and non-stationary noise signals, time-varying environment changes and additional model inaccuracies.      
### 3.A 4-Element 800MHz-BW 29mW True-Time-Delay Spatial Signal Processor Enabling Fast Beam-Training with Data Communications  [ :arrow_down: ](https://arxiv.org/pdf/2106.01255.pdf)
>  Spatial signal processors (SSP) for emerging millimeter-wave wireless networks are critically dependent on link discovery. To avoid loss in communication, mobile devices need to locate narrow directional beams with millisecond latency. In this work, we demonstrate a true-time-delay (TTD) array with digitally reconfigurable delay elements enabling both fast beam-training at the receiver with wideband data communications. In beam-training mode, large delay-bandwidth products are implemented to accelerate beam training using frequency-dependent probing beams. In data communications mode, precise beam alignment is achieved to mitigate spatial effects during beam-forming for wideband signals. The 4-element switched-capacitor based time-interleaved array uses a compact closed-loop integrator for signal combining with the delay compensation implemented in the clock domain to achieve high precision and large delay range. Prototyped in TSMC 65nm CMOS, the TTD SSP successfully demonstrates unique frequency-to-angle mapping with 3.8ns maximum delay and 800MHz bandwidth in the beam-training mode. In the data communications mode, nearly 12dB uniform beamforming gain is achieved from 80MHz to 800MHz. The TTD SSP consumes 29mW at 1V supply achieving 122MB/s with 16-QAM at 9.8% EVM.      
### 4.Intelligent Surface Optimization in Terahertz under Two Manifestations of Molecular Re-radiation  [ :arrow_down: ](https://arxiv.org/pdf/2106.01225.pdf)
>  The operation of Terahertz (THz) communication can be significantly impacted by the interaction between the transmitted wave and the molecules in the atmosphere. In particular, it has been observed experimentally that the signal undergoes not only molecular absorption, but also molecular re-radiation. Two extreme modeling assumptions are prevalent in the literature, where the re-radiated energy is modeled in the first as additive Gaussian noise and in the second as a scattered component strongly correlated to the actual signal. Since the exact characterization is still an open problem, we provide in this paper the first comparative study of the performance of a reconfigurable intelligent surface (RIS) assisted THz system under these two extreme models of re-radiation. In particular, we employ an RIS to overcome the large pathloss by creating a virtual line-of-sight (LOS) path. We then develop an optimization framework for this setup and utilize the block-coordinate descent (BCD) method to iteratively optimize both RIS configuration vector and receive beamforming weight resulting in significant throughput gains for the user of interest compared to random RIS configurations. As expected, our results reveal that better throughput is achieved under the scattering assumption for the molecular re-radiation than the noise assumption.      
### 5.Opening the Black Box of Deep Neural Networks in Physical Layer Communication  [ :arrow_down: ](https://arxiv.org/pdf/2106.01124.pdf)
>  Deep Neural Network (DNN)-based physical layer techniques are attracting considerable interest due to their potential to enhance communication systems. However, most studies in the physical layer have tended to focus on the implement of DNN but not to theoretically understand how does a DNN work in a communication system. In this letter, we aim to quantitatively analyse why DNNs can achieve comparable performance in the physical layer comparing with traditional techniques and its cost in terms of computational complexity. We further investigate and also experimentally validate how information is flown in a DNN-based communication system under the information theoretic concepts.      
### 6.Design and Comparison of Reward Functions in Reinforcement Learning for Energy Management of Sensor Nodes  [ :arrow_down: ](https://arxiv.org/pdf/2106.01114.pdf)
>  Interest in remote monitoring has grown thanks to recent advancements in Internet-of-Things (IoT) paradigms. New applications have emerged, using small devices called sensor nodes capable of collecting data from the environment and processing it. However, more and more data are processed and transmitted with longer operational periods. At the same, the battery technologies have not improved fast enough to cope with these increasing needs. This makes the energy consumption issue increasingly challenging and thus, miniaturized energy harvesting devices have emerged to complement traditional energy sources. Nevertheless, the harvested energy fluctuates significantly during the node operation, increasing uncertainty in actually available energy resources. Recently, approaches in energy management have been developed, in particular using reinforcement learning approaches. However, in reinforcement learning, the algorithm's performance relies greatly on the reward function. In this paper, we present two contributions. First, we explore five different reward functions to identify the most suitable variables to use in such functions to obtain the desired behaviour. Experiments were conducted using the Q-learning algorithm to adjust the energy consumption depending on the energy harvested. Results with the five reward functions illustrate how the choice thereof impacts the energy consumption of the node. Secondly, we propose two additional reward functions able to find the compromise between energy consumption and a node performance using a non-fixed balancing parameter. Our simulation results show that the proposed reward functions adjust the node's performance depending on the battery level and reduce the learning time.      
### 7.Deep Learning based Full-reference and No-reference Quality Assessment Models for Compressed UGC Videos  [ :arrow_down: ](https://arxiv.org/pdf/2106.01111.pdf)
>  In this paper, we propose a deep learning based video quality assessment (VQA) framework to evaluate the quality of the compressed user's generated content (UGC) videos. The proposed VQA framework consists of three modules, the feature extraction module, the quality regression module, and the quality pooling module. For the feature extraction module, we fuse the features from intermediate layers of the convolutional neural network (CNN) network into final quality-aware feature representation, which enables the model to make full use of visual information from low-level to high-level. Specifically, the structure and texture similarities of feature maps extracted from all intermediate layers are calculated as the feature representation for the full reference (FR) VQA model, and the global mean and standard deviation of the final feature maps fused by intermediate feature maps are calculated as the feature representation for the no reference (NR) VQA model. For the quality regression module, we use the fully connected (FC) layer to regress the quality-aware features into frame-level scores. Finally, a subjectively-inspired temporal pooling strategy is adopted to pool frame-level scores into the video-level score. The proposed model achieves the best performance among the state-of-the-art FR and NR VQA models on the Compressed UGC VQA database and also achieves pretty good performance on the in-the-wild UGC VQA databases.      
### 8.Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy  [ :arrow_down: ](https://arxiv.org/pdf/2106.01100.pdf)
>  During lung cancer radiotherapy, the position of infrared reflective objects on the chest can be recorded to estimate the tumor location. However, radiotherapy systems usually have a latency inherent to robot control limitations that impedes the radiation delivery precision. Not taking this phenomenon into account may cause unwanted damage to healthy tissues and lead to side effects such as radiation pneumonitis. In this research, we use nine observation records of the three-dimensional position of three external markers on the chest and abdomen of healthy individuals breathing during intervals from 73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the recorded trajectories range from 6mm to 40mm in the superior-inferior direction. We forecast the location of each marker simultaneously with a horizon value (the time interval in advance for which the prediction is made) between 0.1s and 2.0s, using a recurrent neural network (RNN) trained with unbiased online recurrent optimization (UORO). We compare its performance with an RNN trained with real-time recurrent learning, least mean squares (LMS), and offline linear regression. Training and cross-validation are performed during the first minute of each sequence. On average, UORO achieves the lowest root-mean-square (RMS) and maximum error, equal respectively to 1.3mm and 8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core i9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s, and UORO for horizon values greater than 0.6s.      
### 9.Comparison of Convexificated SQCQP and PSO for the Optimal Transmission System Operation based on Incremental In-Phase and Quadrature Voltage Controlled Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2106.01080.pdf)
>  The optimal operation of electrical energy systems by solving a security constrained optimal power flow (SCOPF) problem is still a challenging research aspect. Especially, for conventional optimization methods like sequential quadratic constrained quadratic programming (SQCQP) the formulation of the incremental control variables like in-phase and quadrature voltage controlled transformers in a solver suitable way is complex. Compared to this, the implementation of these control variables within heuristic approaches like the particle swarm optimization (PSO) is simple but problem specific adaptations of the classic PSO algorithm are necessary to avoid an unfortunate swarm behavior and local convergence in bad results. The objective of this paper is to introduce a SQCQP and a modified PSO approach in detail to solve the SCOPF problem adequately under consideration of flexible incremental in-phase and quadrature transformers tap sets and to compare and benchmark the results of both approaches for an adapted IEEE 118-bus system. The casestudy shows that both approaches lead to suitable results of the SCOPF with individual advantages of the SQCQP concerning the quality and the reproducibility of the results while the PSO lead to faster solutions when the complexity of the investigation scenario increases.      
### 10.Comparison of Random Sampling and Heuristic Optimization-Based Methods for Determining the Flexibility Potential at Vertical System Interconnections  [ :arrow_down: ](https://arxiv.org/pdf/2106.01056.pdf)
>  In order to prevent conflicting or counteracting use of flexibility options, the coordination between distribution system operator and transmission system operator has to be strengthened. For this purpose, methods for the standardized description and identification of the aggregated flexibility potential of distribution grids are developed. Approaches for identifying the feasible operation region (FOR) of distribution grids can be categorized into two main classes: Random sampling/stochastic approaches and optimization-based approaches. While the latter have the advantage of working in real-world scenarios where no full grid models exist, when relying on naive sampling strategies, they suffer from poor coverage of the edges of the FOR due to convoluted distributions. In this paper, we tackle the problem from two different angles. First, we present a random sampling approach which mitigates the convolution problem by drawing sample values from a multivariate Dirichlet distribution. Second, we come up with a hybrid approach which solves the underlying optimal power flow problems of the optimization-based approach by means of a stochastic evolutionary optimization algorithm codenamed REvol. By means of synthetic feeders, we compare the two proposed FOR identification methods with regard to how well the FOR is covered and number of power flow calculations required.      
### 11.On Topology Inference for Networked Dynamical Systems: Principles and Performances  [ :arrow_down: ](https://arxiv.org/pdf/2106.01031.pdf)
>  Topology inference for networked dynamical systems (NDSs) plays a crucial role in many areas. Knowledge of the system topology can aid in detecting anomalies, spotting trends, predicting future behavior and so on. Different from the majority of pioneering works, this paper investigates the principles and performances of topology inference from the perspective of node causality and correlation. Specifically, we advocate a comprehensive analysis framework to unveil the mutual relationship, convergence and accuracy of the proposed methods and other benchmark methods, i.e., the Granger and ordinary least square (OLS) estimators. Our method allows for unknown observation noises, both asymptotic and marginal stabilities for NDSs, while encompasses a correlation-based modification design to alleviate performance degradation in small observation scale. To explicitly demonstrate the inference performance of the estimators, we leverage the concentration measure in Gaussian space, and derive the non-asymptotic rates of the inference errors for linear time-invariant (LTI) cases. Considering when the observations are not sufficient to support the estimators, we provide an excitation-based method to infer the one-hop and multi-hop neighbors with probability guarantees. Furthermore, we point out the theoretical results can be extended to switching topologies and nonlinear dynamics cases. Extensive simulations highlight the outperformance of the proposed method.      
### 12.Asymptotic Performance of TDOA Estimation using Satellites  [ :arrow_down: ](https://arxiv.org/pdf/2106.01025.pdf)
>  We present novel lower bounds on the localization error using a network of satellites randomly deployed on a sphere around Earth. Our new analysis approach characterizes the localization performance by its asymptotic behavior as the number of satellites gets large while assuming a dense network. Using the law of large numbers, we derive closed-form expressions for the asymptotic Cramer Rao bound (CRB) from which we draw valuable insights. The resulting expressions depend solely on the network statistics and are not a function of a particular network configuration. We consider two types of estimators. The first uses the exact statistical model, and hence employs both timing and amplitude information. The second estimator ignores the amplitudes and hence uses only time difference of arrival (TDOA) information. The asymptotic CRB indicates that for practical system setup, a TDOA estimator approaches the performance of the ideal estimator. For both estimators, the localization accuracy improves as satellites get closer to Earth. The latter finding is essential in light of the proliferation of low-Earth-orbit (LEO) satellites and motivates a further study of localization-performance in such networks. Besides, we show that the vertical localization accuracy is lower than the horizontal accuracy and is also more sensitive to the receiver field-of-view.      
### 13.Deep Reinforcement Learning-based UAV Navigation and Control: A Soft Actor-Critic with Hindsight Experience Replay Approach  [ :arrow_down: ](https://arxiv.org/pdf/2106.01016.pdf)
>  In this paper, we propose SACHER (soft actor-critic (SAC) with hindsight experience replay (HER)), which constitutes a class of deep reinforcement learning (DRL) algorithms. SAC is known as an off-policy model-free DRL algorithm based on the maximum entropy framework, which outperforms earlier DRL algorithms in terms of exploration, robustness and learning performance. However, in SAC, maximizing the entropy-augmented objective may degrade the optimality of the learning outcomes. HER is known as a sample-efficient replay method that enhances the performance of off-policy DRL algorithms by allowing them to learn from both failures and successes. We apply HER to SAC and propose SACHER to improve the learning performance of SAC. More precisely, SACHER achieves the desired optimal outcomes faster and more accurately than SAC, since HER improves the sample efficiency of SAC. We apply SACHER to the navigation and control problem of unmanned aerial vehicles (UAVs), where SACHER generates the optimal navigation path of the UAV under various obstacles in operation. Specifically, we show the effectiveness of SACHER in terms of the tracking error and cumulative reward in UAV operation by comparing them with those of state-of-the-art DRL algorithms, SAC and DDPG. Note that SACHER in UAV navigation and control problems can be applied to arbitrary models of UAVs.      
### 14.Refinement of Direction of Arrival Estimators by Majorization-Minimization Optimization on the Array Manifold  [ :arrow_down: ](https://arxiv.org/pdf/2106.01011.pdf)
>  We propose a generalized formulation of direction of arrival estimation that includes many existing methods such as steered response power, subspace, coherent and incoherent, as well as speech sparsity-based methods. Unlike most conventional methods that rely exclusively on grid search, we introduce a continuous optimization algorithm to refine DOA estimates beyond the resolution of the initial grid. The algorithm is derived from the majorization-minimization (MM) technique. We derive two surrogate functions, one quadratic and one linear. Both lead to efficient iterative algorithms that do not require hyperparameters, such as step size, and ensure that the DOA estimates never leave the array manifold, without the need for a projection step. In numerical experiments, we show that the accuracy after a few iterations of the MM algorithm nearly removes dependency on the resolution of the initial grid used. We find that the quadratic surrogate function leads to very fast convergence, but the simplicity of the linear algorithm is very attractive, and the performance gap small.      
### 15.Tips and Tricks to Improve CNN-based Chest X-ray Diagnosis: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2106.00997.pdf)
>  Convolutional Neural Networks (CNNs) intrinsically requires large-scale data whereas Chest X-Ray (CXR) images tend to be data/annotation-scarce, leading to over-fitting. Therefore, based on our development experience and related work, this paper thoroughly introduces tricks to improve generalization in the CXR diagnosis: how to (i) leverage additional data, (ii) augment/distillate data, (iii) regularize training, and (iv) conduct efficient segmentation. As a development example based on such optimization techniques, we also feature LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved radiologists/non-radiologists' nodule detection sensitivity by 0.100/0.131, respectively, while maintaining specificity.      
### 16.Convergent dynamics of optimal nonlinear damping control  [ :arrow_down: ](https://arxiv.org/pdf/2106.00962.pdf)
>  Following Demidovich's concept and definition of convergent systems, we analyze the optimal nonlinear damping control, recently proposed [1] for the second-order systems. Targeting the problem of output regulation, correspondingly tracking of $\mathcal{C}^1$-trajectories, it is shown that all solutions of the control system are globally uniformly asymptotically stable. The existence of the unique limit solution in the origin of the control error and its time derivative coordinates are shown in the sense of Demidovich's convergent dynamics. Explanative numerical examples are also provided along with analysis.      
### 17.Distributed Control-Estimation Synthesis for Stochastic Multi-Agent Systems via Virtual Interaction between Non-neighboring Agents  [ :arrow_down: ](https://arxiv.org/pdf/2106.00961.pdf)
>  This paper considers the optimal distributed control problem for a linear stochastic multi-agent system (MAS). Due to the distributed nature of MAS network, the information available to an individual agent is limited to its vicinity. From the entire MAS aspect, this imposes the structural constraint on the control law, making the optimal control law computationally intractable. This paper attempts to relax such a structural constraint by expanding the neighboring information for each agent to the entire MAS, enabled by the distributed estimation algorithm embedded in each agent. By exploiting the estimated information, each agent is not limited to interact with its neighborhood but further establishing the `virtual interactions' with the non-neighboring agents. Then the optimal distributed MAS control problem is cast as a synthesized control-estimation problem. An iterative optimization procedure is developed to find the control-estimation law, minimizing the global objective cost of MAS.      
### 18.Should We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2106.00949.pdf)
>  Although recent advances in deep learning technology improved automatic speech recognition (ASR), it remains difficult to recognize speech when it overlaps other people's voices. Speech separation or extraction is often used as a front-end to ASR to handle such overlapping speech. However, deep neural network-based speech enhancement can generate `processing artifacts' as a side effect of the enhancement, which degrades ASR performance. For example, it is well known that single-channel noise reduction for non-speech noise (non-overlapping speech) often does not improve ASR. Likewise, the processing artifacts may also be detrimental to ASR in some conditions when processing overlapping speech with a separation/extraction method, although it is usually believed that separation/extraction improves ASR. In order to answer the question `Do we always have to separate/extract speech from mixtures?', we analyze ASR performance on observed and enhanced speech at various noise and interference conditions, and show that speech enhancement degrades ASR under some conditions even for overlapping speech. Based on these findings, we propose a simple switching algorithm between observed and enhanced speech based on the estimated signal-to-interference ratio and signal-to-noise ratio. We demonstrated experimentally that such a simple switching mechanism can improve recognition performance when processing artifacts are detrimental to ASR.      
### 19.Self-supervised Lesion Change Detection and Localisation in Longitudinal Multiple Sclerosis Brain Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2106.00919.pdf)
>  Longitudinal imaging forms an essential component in the management and follow-up of many medical conditions. The presence of lesion changes on serial imaging can have significant impact on clinical decision making, highlighting the important role for automated change detection. Lesion changes can represent anomalies in serial imaging, which implies a limited availability of annotations and a wide variety of possible changes that need to be considered. Hence, we introduce a new unsupervised anomaly detection and localisation method trained exclusively with serial images that do not contain any lesion changes. Our training automatically synthesises lesion changes in serial images, introducing detection and localisation pseudo-labels that are used to self-supervise the training of our model. Given the rarity of these lesion changes in the synthesised images, we train the model with the imbalance robust focal Tversky loss. When compared to supervised models trained on different datasets, our method shows competitive performance in the detection and localisation of new demyelinating lesions on longitudinal magnetic resonance imaging in multiple sclerosis patients. Code for the models will be made available on GitHub.      
### 20.Concurrent Learning Based Tracking Control of Nonlinear Systems using Gaussian Process  [ :arrow_down: ](https://arxiv.org/pdf/2106.00910.pdf)
>  This paper demonstrates the applicability of the combination of concurrent learning as a tool for parameter estimation and non-parametric Gaussian Process for online disturbance learning. A control law is developed by using both techniques sequentially in the context of feedback linearization. The concurrent learning algorithm estimates the system parameters of structured uncertainty without requiring persistent excitation, which are used in the design of the feedback linearization law. Then, a non-parametric Gaussian Process learns unstructured uncertainty. The closed-loop system stability for the nth-order system is proven using the Lyapunov stability theorem. The simulation results show that the tracking error is minimized (i) when true values of model parameters have not been provided, (ii) in the presence of disturbances introduced once the parameters have converged to their true values and (iii) when system parameters have not converged to their true values in the presence of disturbances.      
### 21.Feedback Interconnected Mean-Field Estimation and Control  [ :arrow_down: ](https://arxiv.org/pdf/2106.00899.pdf)
>  Swarm robotic systems have foreseeable applications in the near future. Recently, there has been an increasing amount of literature that employs mean-field partial differential equations (PDEs) to model the time-evolution of the probability density of swarm robotic systems and uses mean-field feedback to design stable control laws that act on individuals such that their density converges to a target profile. However, it remains largely unexplored considering problems of how to estimate the mean-field density, how the density estimation algorithms affect the control performance, and whether the estimation performance in turn depends on the control algorithms. In this work, we focus on studying the interplay of these algorithms. Specially, we propose new mean-field control laws which use the real-time density and its gradient as feedback, and prove that they are globally input-to-state stable (ISS) to estimation errors. Then, we design filtering algorithms to obtain estimates of the density and its gradient, and prove that these estimates are convergent assuming the control laws are known. Finally, we show that the feedback interconnection of these estimation and control algorithms is still globally ISS, which is attributed to the bilinearity of the mean-field PDE system. An agent-based simulation is included to verify the stability of these algorithms and their feedback interconnection.      
### 22.Field Estimation using Robotic Swarms through Bayesian Regression and Mean-Field Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2106.00895.pdf)
>  Recent years have seen an increased interest in using mean-field density based modelling and control strategy for deploying robotic swarms. In this paper, we study how to dynamically deploy the robots subject to their physical constraints to efficiently measure and reconstruct certain unknown spatial field (e.g. the air pollution index over a city). Specifically, the evolution of the robots' density is modelled by mean-field partial differential equations (PDEs) which are uniquely determined by the robots' individual dynamics. Bayesian regression models are used to obtain predictions and return a variance function that represents the confidence of the prediction. We formulate a PDE constrained optimization problem based on this variance function to dynamically generate a reference density signal which guides the robots to uncertain areas to collect new data, and design mean-field feedback-based control laws such that the robots' density converges to this reference signal. We also show that the proposed feedback law is robust to density estimation errors in the sense of input-to-state stability. Simulations are included to verify the effectiveness of the algorithms.      
### 23.Passive Beamforming Design for Intelligent Reflecting Surface Assisted MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.00890.pdf)
>  Intelligent reflecting surfaces (IRSs) constitute passive devices, which are capable of adjusting the phase shifts of their reflected signals, and hence they are suitable for passive beamforming. In this paper, we conceive their design with the active beamforming action of multiple-input multipleoutput (MIMO) systems used at the access points (APs) for improving the beamforming gain, where both the APs and users are equipped with multiple antennas. Firstly, we decouple the optimization problem and design the active beamforming for a given IRS configuration. Then we transform the optimization problem of the IRS-based passive beamforming design into a tractable non-convex quadratically constrained quadratic program (QCQP). For solving the transformed problem, we give an approximate solution based on the technique of widely used semidefinite relaxation (SDR). We also propose a low-complexity iterative solution. We further prove that it can converge to a locally optimal value. Finally, considering the practical scenario of discrete phase shifts at the IRS, we give the quantization design for IRS elements on basis of the two solutions. Our simulation results demonstrate the superiority of the proposed solutions over the relevant benchmarks.      
### 24.A Neural Acoustic Echo Canceller Optimized Using An Automatic Speech Recognizer And Large Scale Synthetic Data  [ :arrow_down: ](https://arxiv.org/pdf/2106.00856.pdf)
>  We consider the problem of recognizing speech utterances spoken to a device which is generating a known sound waveform; for example, recognizing queries issued to a digital assistant which is generating responses to previous user inputs. Previous work has proposed building acoustic echo cancellation (AEC) models for this task that optimize speech enhancement metrics using both neural network as well as signal processing approaches. <br>Since our goal is to recognize the input speech, we consider enhancements which improve word error rates (WERs) when the predicted speech signal is passed to an automatic speech recognition (ASR) model. First, we augment the loss function with a term that produces outputs useful to a pre-trained ASR model and show that this augmented loss function improves WER metrics. Second, we demonstrate that augmenting our training dataset of real world examples with a large synthetic dataset improves performance. Crucially, applying SpecAugment style masks to the reference channel during training aids the model in adapting from synthetic to real domains. In experimental evaluations, we find the proposed approaches improve performance, on average, by 57% over a signal processing baseline and 45% over the neural AEC model without the proposed changes.      
### 25.Smart Online Charging Algorithm for Electric Vehicles via Customized Actor-Critic Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.00854.pdf)
>  With the advances in the Internet of Things technology, electric vehicles (EVs) have become easier to schedule in daily life, which is reshaping the electric load curve. It is important to design efficient charging algorithms to mitigate the negative impact of EV charging on the power grid. This paper investigates an EV charging scheduling problem to reduce the charging cost while shaving the peak charging load, under unknown future information about EVs, such as arrival time, departure time, and charging demand. First, we formulate an EV charging problem to minimize the electricity bill of the EV fleet and study the EV charging problem in an online setting without knowing future information. We develop an actor-critic learning-based smart charging algorithm (SCA) to schedule the EV charging against the uncertainties in EV charging behaviors. The SCA learns an optimal EV charging strategy with continuous charging actions instead of discrete approximation of charging. We further develop a more computationally efficient customized actor-critic learning charging algorithm (CALC) by reducing the state dimension and thus improving the computational efficiency. Finally, simulation results show that our proposed SCA can reduce EVs' expected cost by 24.03%, 21.49%, 13.80%, compared with the Eagerly Charging Algorithm, Online Charging Algorithm, RL-based Adaptive Energy Management Algorithm, respectively. CALC is more computationally efficient, and its performance is close to that of SCA with only a gap of 5.56% in the cost.      
### 26.Sparse, Efficient, and Semantic Mixture Invariant Training: Taming In-the-Wild Unsupervised Sound Separation  [ :arrow_down: ](https://arxiv.org/pdf/2106.00847.pdf)
>  Supervised neural network training has led to significant progress on single-channel sound separation. This approach relies on ground truth isolated sources, which precludes scaling to widely available mixture data and limits progress on open-domain tasks. The recent mixture invariant training (MixIT) method enables training on in-the wild data; however, it suffers from two outstanding problems. First, it produces models which tend to over-separate, producing more output sources than are present in the input. Second, the exponential computational complexity of the MixIT loss limits the number of feasible output sources. These problems interact: increasing the number of output sources exacerbates over-separation. In this paper we address both issues. To combat over-separation we introduce new losses: sparsity losses that favor fewer output sources and a covariance loss that discourages correlated outputs. We also experiment with a semantic classification loss by predicting weak class labels for each mixture. To extend MixIT to larger numbers of sources, we introduce an efficient approximation using a fast least-squares solution, projected onto the MixIT constraint set. Our experiments show that the proposed losses curtail over-separation and improve overall performance. The best performance is achieved using larger numbers of output sources, enabled by our efficient MixIT loss, combined with sparsity losses to prevent over-separation. On the FUSS test set, we achieve over 13 dB in multi-source SI-SNR improvement, while boosting single-source reconstruction SI-SNR by over 17 dB.      
### 27.Transmitter IQ Skew Calibration using Direct Detection  [ :arrow_down: ](https://arxiv.org/pdf/2106.00802.pdf)
>  We propose a transmitter skew calibration based on direct detection of coherent signals with estimation errors of +/-0.2ps, providing a reliable, accurate and low-cost scheme to calibrate skew for coherent transceivers. <br>In October 2019, this work was submitted / was exposed to Optical Fiber Communication Conference 2020 but an acceptance was not granted. We claimed the first time to use a direct detection-based feedback method for coherent transmitter calibration.      
### 28.Fourier Space Losses for Efficient Perceptual Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2106.00783.pdf)
>  Many super-resolution (SR) models are optimized for high performance only and therefore lack efficiency due to large model complexity. As large models are often not practical in real-world applications, we investigate and propose novel loss functions, to enable SR with high perceptual quality from much more efficient models. The representative power for a given low-complexity generator network can only be fully leveraged by strong guidance towards the optimal set of parameters. We show that it is possible to improve the performance of a recently introduced efficient generator architecture solely with the application of our proposed loss functions. In particular, we use a Fourier space supervision loss for improved restoration of missing high-frequency (HF) content from the ground truth image and design a discriminator architecture working directly in the Fourier domain to better match the target HF distribution. We show that our losses' direct emphasis on the frequencies in Fourier-space significantly boosts the perceptual image quality, while at the same time retaining high restoration quality in comparison to previously proposed loss functions for this task. The performance is further improved by utilizing a combination of spatial and frequency domain losses, as both representations provide complementary information during training. On top of that, the trained generator achieves comparable results with and is 2.4x and 48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow respectively.      
### 29.Resource allocation for D2D-Based AMI Communications Underlaying LTE Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.00782.pdf)
>  Smart meters are utilized to transmit the consumption information to the metering data management system for observing and management in smart grid advanced metering infrastructure systems. In the meantime, for efficient utilization for spectrum, Device-to-Device (D2D) communications underlaying LTE networks are a promising wireless communication technology for advanced metering infrastructure which supporting a technique for reusing the same radio resources (RRs) of LTE networks. Therefore, we examine the utilization of D2D communication technology for advanced metering infrastructure communications underlaying LTE networks. A novel approach is suggested for provisioning the mandatory communication between serving data concentrator and its set of smart meters using this technology. The suggested approach is dependent on two main stages. The group of permissible cellular user equipment reuse candidates for every smart meter is calculated with taking the quality of service demands for cellular user devices and smart meters into consideration in the first stage. The optimal RR allocation for every smart meter is determined based on maximizing the access rate of smart meters which can be accepted and operated in D2D reuse mode in the second stage. Simulation results prove the efficacy of the suggested approach for efficient advanced metering infrastructure communication underlaying LTE systems with accepting remarkable number of SMs and accomplishing outstanding throughput gain.      
### 30.Risk-sensitive safety analysis via state-space augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2106.00776.pdf)
>  Risk-sensitive safety analysis is a safety analysis method for stochastic systems on Borel spaces that uses a risk functional from finance called Conditional Value-at-Risk (CVaR). CVaR provides a particularly expressive way to quantify the safety of a control system, as it represents the average cost in a fraction of worst cases. In prior work, the notion of a risk-sensitive safe set was defined in terms of a non-standard optimal control problem, in which a maximum cost is assessed via CVaR. Here, we provide a method to compute risk-sensitive safe sets exactly in principle by utilizing a state-space augmentation technique. In addition, we prove the existence of an optimal pre-commitment policy under a measurable selection condition. The proposed framework assumes continuous system dynamics and cost functions, but is otherwise flexible. In particular, it can accommodate probabilistic control policies, fairly general disturbance distributions, and control-dependent, non-monotonic, and non-convex stage costs. We demonstrate how risk-sensitive safety analysis is useful for a stormwater infrastructure application. Our numerical examples are inspired by current challenges that cities face in managing precipitation uncertainty.      
### 31.Bilateral Spectrum Weighted Total Variation for Real-World Super-Resolution and Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2106.00768.pdf)
>  In this paper, we propose a regularization technique for real-world super-resolution and image denoising. Total variation (TV) regularization is adopted in many image processing applications to preserve the local smoothness. However, TV prior is prone to oversmoothness, staircasing effect, and contrast losses. Nonlocal TV (NLTV) mitigates the contrast losses by adaptively weighting the smoothness based on the similarity measure of image patches. Although it suppresses the noise effectively in the flat regions, it might leave residual noise surrounding the edges especially when the image is not oversmoothed. To address this problem, we propose the bilateral spectrum weighted total variation (BSWTV). Specially, we apply a locally adaptive shrink coefficient to the image gradients and employ the eigenvalues of the covariance matrix of the weighted image gradients to effectively refine the weighting map and suppress the residual noise. In conjunction with the data fidelity term derived from a mixed Poisson-Gaussian noise model, the objective function is decomposed and solved by the alternating direction method of multipliers (ADMM) algorithm. In order to remove outliers and facilitate the convergence stability, the weighting map is smoothed by a Gaussian filter with an iteratively decreased kernel width and updated in a momentum-based manner in each ADMM iteration. We benchmark our method with the state-of-the-art approaches on the public real-world datasets for super-resolution and image denoising. Experiments show that the proposed method obtains outstanding performance for super-resolution and achieves promising results for denoising on real-world images.      
### 32.A Simulation-Optimization Technique for Service Level Analysis in Conjunction with Reorder Point Estimation and Lead-Time consideration: A Case Study in Sea Port  [ :arrow_down: ](https://arxiv.org/pdf/2106.00767.pdf)
>  This study offers a step-by-step practical procedure from the analysis of the current status of the spare parts inventory system to advanced service-level analysis by virtue of simulation-optimization technique for a real-world case study associated with a seaport. The remarkable variety and immense diversity on one hand, and extreme complexities not only in consumption patterns but in the supply of spare parts in an international port with technically advance port operator machinery, on the other hand, have convinced the managers to deal with this issue in a structural framework. The huge available data require cleaning and classification to properly process them and derive reorder point (ROP) estimation, reorder quantity (ROQ) estimation, and associated service level analysis. Finally, from 247000 items used in 9 years long, 1416 inventory items are elected as a result of ABC analysis integrating with the Analytic Hierarchy Process (AHP), which led to the main items that need to be kept under strict inventory control. The ROPs and the pertinent quantities are simulated by Arena software for all the main items, each of which took approximately 30 minutes run-time on a personal computer to determine near-optimal estimations.      
### 33.Is good old GRAPPA dead?  [ :arrow_down: ](https://arxiv.org/pdf/2106.00753.pdf)
>  We perform a qualitative analysis of performance of XPDNet, a state-of-the-art deep learning approach for MRI reconstruction, compared to GRAPPA, a classical approach. We do this in multiple settings, in particular testing the robustness of the XPDNet to unseen settings, and show that the XPDNet can to some degree generalize well.      
### 34.Reconciling interoperability with efficient Verification and Validation within open source simulation environments  [ :arrow_down: ](https://arxiv.org/pdf/2106.01343.pdf)
>  A Cyber-Physical System (CPS) comprises physical as well as software subsystems. Simulation-based approaches are typically used to support design and Verification and Validation (V&amp;V) of CPSs in several domains such as: aerospace, defence, automotive, smart grid and healthcare. Accordingly, many simulation-based tools are available, and this poses huge interoperability challenges. To overcome them, in 2010 the Functional Mock-up Interface (FMI) was proposed as an open standard to support both Model Exchange (ME) and Co-Simulation (CS). Models adhering to such a standard are called Functional Mock-up Units (FMUs). FMUs play an essential role in defining complex CPSs through, e.g., the SSP standard. Simulation-based V&amp;V of CPSs typically requires exploring different scenarios (i.e., exogenous CPS input sequences), many of them showing a shared prefix. Accordingly, the simulator state at the end of a shared prefix is saved and then restored and used as a start state for the simulation of the next scenario. In this context, an important FMI feature is the capability to save and restore the internal FMU state on demand. Unfortunately, the implementation of this feature is not mandatory and it is available only within some commercial software. This motivates developing such a feature for open-source CPS simulation environments. In this paper, we focus on JModelica, an open-source modelling and simulation environment for CPSs defined in the Modelica language. We describe how we have endowed JModelica with our open-source implementation of the FMI 2.0 functions to save and restore internal states of FMUs for ME. Furthermore, we present results evaluating, through 934 benchmark models, correctness and efficiency of our extended JModelica. Our results show that simulation-based V&amp;V is, on average, 22 times faster with our get/set functionality than without it.      
### 35.Sound-to-Imagination: Unsupervised Crossmodal Translation Using Deep Dense Network Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2106.01266.pdf)
>  The motivation of our research is to develop a sound-to-image (S2I) translation system for enabling a human receiver to visually infer the occurrence of sound related events. We expect the computer to 'imagine' the scene from the captured sound, generating original images that picture the sound emitting source. Previous studies on similar topics opted for simplified approaches using data with low content diversity and/or strong supervision. Differently, we propose to perform unsupervised S2I translation using thousands of distinct and unknown scenes, with slightly pre-cleaned data, just enough to guarantee aural-visual semantic coherence. To that end, we employ conditional generative adversarial networks (GANs) with a deep densely connected generator. Besides, we implemented a moving-average adversarial loss to address GANs training instability. Though the specified S2I translation problem is quite challenging, we were able to generalize the translator model enough to obtain more than 14%, in average, of interpretable and semantically coherent images translated from unknown sounds. Additionally, we present a solution using informativity classifiers to perform quantitative evaluation of S2I translation.      
### 36.Improving low-resource ASR performance with untranscribed out-of-domain data  [ :arrow_down: ](https://arxiv.org/pdf/2106.01227.pdf)
>  Semi-supervised training (SST) is a common approach to leverage untranscribed/unlabeled speech data to improve automatic speech recognition performance in low-resource languages. However, if the available unlabeled speech is mismatched to the target domain, SST is not as effective, and in many cases performs worse than the original system. In this paper, we address the issue of low-resource ASR when only untranscribed out-of-domain speech data is readily available in the target language. Specifically, we look to improve performance on conversational/telephony speech (target domain) using web resources, in particular YouTube data, which more closely resembles news/topical broadcast data. Leveraging SST, we show that while in some cases simply pooling the out-of-domain data with the training data lowers word error rate (WER), in all cases, we see improvements if we train first with the out-of-domain data and then fine-tune the resulting model with the original training data. Using 2000 hours of speed perturbed YouTube audio in each target language, with semi-supervised transcripts, we show improvements on multiple languages/data sets, of up to 16.3% relative improvement in WER over the baseline systems and up to 7.4% relative improvement in WER over a system that simply pools the out-of-domain data with the training data.      
### 37.Refined method to extract frequency-noise components of lasers by delayed self-heterodyne  [ :arrow_down: ](https://arxiv.org/pdf/2106.01205.pdf)
>  An essential metric to quantify the stability of a laser is its frequency noise (FN). This metric yields information on the linewidth and on noise components which limit its usage for high precision purposes such as coherent communication. Its experimental determination relies on challenging optical phase measurements, for which dedicated commercial instruments have been developed. In contrast, this work presents a simple and cost-effective method for extracting FN features employing a delayed self-heterodyne (DSH) setup. Using delay lengths much shorter than the coherence length of the laser, the DSH trace reveals a correspondence with the FN power spectral density (PSD) measured with commercial instruments. Results are found for multiple lasers, with discrepancies in intense dither tone frequencies below 0.2 percent      
### 38.Exploring modality-agnostic representations for music classification  [ :arrow_down: ](https://arxiv.org/pdf/2106.01149.pdf)
>  Music information is often conveyed or recorded across multiple data modalities including but not limited to audio, images, text and scores. However, music information retrieval research has almost exclusively focused on single modality recognition, requiring development of separate models for each modality. Some multi-modal works require multiple coexisting modalities given to the model as inputs, constraining the use of these models to the few cases where data from all modalities are available. To the best of our knowledge, no existing model has the ability to take inputs from varying modalities, e.g. images or sounds, and classify them into unified music categories. We explore the use of cross-modal retrieval as a pretext task to learn modality-agnostic representations, which can then be used as inputs to classifiers that are independent of modality. We select instrument classification as an example task for our study as both visual and audio components provide relevant semantic information. We train music instrument classifiers that can take both images or sounds as input, and perform comparably to sound-only or image-only classifiers. Furthermore, we explore the case when there is limited labeled data for a given modality, and the impact in performance by using labeled data from other modalities. We are able to achieve almost 70% of best performing system in a zero-shot setting. We provide a detailed analysis of experimental results to understand the potential and limitations of the approach, and discuss future steps towards modality-agnostic classifiers.      
### 39.Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data Augmentation and Deep Ensemble Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.01132.pdf)
>  Deep Learning (DL) and specifically CNN models have become a de facto method for a wide range of vision tasks, outperforming traditional machine learning (ML) methods. Consequently, they drew a lot of attention in the neuroimaging field in particular for phenotype prediction or computer-aided diagnosis. However, most of the current studies often deal with small single-site cohorts, along with a specific pre-processing pipeline and custom CNN architectures, which make them difficult to compare to. We propose an extensive benchmark of recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM) pre-processing and quasi-raw images. Experiments were conducted on a large multi-site 3D brain anatomical MRI data-set comprising N=10k scans on 3 challenging tasks: age prediction, sex classification, and schizophrenia diagnosis. We found that all models provide significantly better predictions with VBM images than quasi-raw data. This finding evolved as the training set approaches 10k samples where quasi-raw data almost reach the performance of VBM. Moreover, we showed that linear models perform comparably with SOTA CNN on VBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter version that we proposed, provide a good compromise in terms of performance in all data regime. Therefore, we suggest to employ them as the architectures by default. Critically, we also showed that current CNN are still very biased towards the acquisition site, even when trained with N=10k multi-site images. In this context, VBM pre-processing provides an efficient way to limit this site effect. Surprisingly, we did not find any clear benefit from data augmentation techniques. Finally, we proved that deep ensemble learning is well suited to re-calibrate big CNN models without sacrificing performance.      
### 40.A robust controller for stable 3D pinching using tactile sensing  [ :arrow_down: ](https://arxiv.org/pdf/2106.01110.pdf)
>  This paper proposes a controller for stable grasping of unknown-shaped objects by two robotic fingers with tactile fingertips. The grasp is stabilised by rolling the fingertips on the contact surface and applying a desired grasping force to reach an equilibrium state. The validation is both in simulation and on a fully-actuated robot hand (the Shadow Modular Grasper) fitted with custom-built optical tactile sensors (based on the BRL TacTip). The controller requires the orientations of the contact surfaces, which are estimated by regressing a deep convolutional neural network over the tactile images. Overall, the grasp system is demonstrated to achieve stable equilibrium poses on a range of objects varying in shape and softness, with the system being robust to perturbations and measurement errors. This approach also has promise to extend beyond grasping to stable in-hand object manipulation with multiple fingers.      
### 41.NVC-Net: End-to-End Adversarial Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2106.00992.pdf)
>  Voice conversion has gained increasing popularity in many applications of speech synthesis. The idea is to change the voice identity from one speaker into another while keeping the linguistic content unchanged. Many voice conversion approaches rely on the use of a vocoder to reconstruct the speech from acoustic features, and as a consequence, the speech quality heavily depends on such a vocoder. In this paper, we propose NVC-Net, an end-to-end adversarial network, which performs voice conversion directly on the raw audio waveform of arbitrary length. By disentangling the speaker identity from the speech content, NVC-Net is able to perform non-parallel traditional many-to-many voice conversion as well as zero-shot voice conversion from a short utterance of an unseen target speaker. Importantly, NVC-Net is non-autoregressive and fully convolutional, achieving fast inference. Our model is capable of producing samples at a rate of more than 3600 kHz on an NVIDIA V100 GPU, being orders of magnitude faster than state-of-the-art methods under the same hardware configurations. Objective and subjective evaluations on non-parallel many-to-many voice conversion tasks show that NVC-Net obtains competitive results with significantly fewer parameters.      
### 42.End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2106.00952.pdf)
>  Information extraction from document images has received a lot of attention recently, due to the need for digitizing a large volume of unstructured documents such as invoices, receipts, bank transfers, etc. In this paper, we propose a novel deep learning architecture for end-to-end information extraction on the 2D character-grid embedding of the document, namely the \textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and spatial relations between 2D elements, our model leverages a specialized multi-stage encoder-decoders design, in conjunction with efficient uses of the self-attention mechanism and the box convolution. Experimental results on different datasets show that our model outperforms the baseline U-Net architecture by a large margin while using 40\% fewer parameters. Moreover, it also significantly improved the baseline in erroneous OCR and limited training data scenario, thus becomes practical for real-world applications.      
### 43.Finite-time bearing-only maneuver of acyclic leader-follower formations  [ :arrow_down: ](https://arxiv.org/pdf/2106.00951.pdf)
>  This letter proposes two finite-time bearing-based control laws for acyclic leader-follower formations. The leaders in formation move with a bounded continuous reference velocity and each follower controls its position with regard to three agents in the formation. The first control law uses only bearing vectors, and finite-time convergence is achieved by properly selecting two state-dependent control gains. The second control law requires both bearing vectors and communications between agents. Each agent simultaneously localizes and follows a virtual target. Finite-time convergence of the desired formation under both control laws is proved by mathematical induction and supported by numerical simulations.      
### 44.Teaching MPC: Which Way to the Promised Land?  [ :arrow_down: ](https://arxiv.org/pdf/2106.00944.pdf)
>  Since the earliest conceptualizations by Lee and Markus, and Propoi in the 1960s, Model Predictive Control (MPC) has become a major success story of systems and control with respect to industrial impact and with respect to continued and wide-spread research interest. The field has evolved from conceptually simple linear-quadratic (convex) settings in discrete and continuous time to nonlinear and distributed settings including hybrid, stochastic, and infinite-dimensional systems. Put differently, essentially the entire spectrum of dynamic systems can be considered in the MPC framework with respect to both -- system theoretic analysis and tailored numerics. Moreover, recent developments in machine learning also leverage MPC concepts and learning-based and data-driven MPC have become highly active research areas. <br>However, this evident and continued success renders it increasingly complex to live up to industrial expectations while enabling graduate students for state-of-the-art research in teaching MPC. Hence, this position paper attempts to trigger a discussion on teaching MPC. To lay the basis for a fruitful debate, we subsequently investigate the prospect of covering MPC in undergraduate courses; we comment on teaching textbooks; and we discuss the increasing complexity of research-oriented graduate teaching of~MPC.      
### 45.An arbitrary-order predefined-time exact differentiator for signals with exponential growth bound  [ :arrow_down: ](https://arxiv.org/pdf/2106.00822.pdf)
>  Constructing differentiation algorithms with a fixed-time convergence and a predefined Upper Bound on their Settling Time (\textit{UBST}), i.e., predefined-time differentiators, is attracting attention for solving estimation and control problems under time constraints. However, existing methods are limited to signals having an $n$-th Lipschitz derivative. Here, we introduce a general methodology to design $n$-th order predefined-time differentiators for a broader class of signals: for signals, whose $(n+1)$-th derivative is bounded by a function with bounded logarithmic derivative, i.e., whose $(n+1)$-th derivative grows at most exponentially. Our approach is based on a class of time-varying gains known as Time-Base Generators (\textit{TBG}). The only assumption to construct the differentiator is that the class of signals to be differentiated $n$-times have a $(n+1)$-th derivative bounded by a known function with a known bound for its $(n+1)$-th logarithmic derivative. We show how our methodology achieves an \textit{UBST} equal to the predefined time, better transient responses with smaller error peaks than autonomous predefined-time differentiators, and a \textit{TBG} gain that is bounded at the settling time instant.      
### 46.Exploring Exotic Counterpoint Compositions  [ :arrow_down: ](https://arxiv.org/pdf/2106.00806.pdf)
>  In this paper, first musical compositions are presented, which are created using the mathematical counterpoint theory of Guerino Mazzola and his collaborators. These compositions also use the RUBATO(R) software's components for counterpoint constructions. The present work aims at opening new "exotic" directions of contrapuntal composition in non-Fuxian worlds. The authors would like to receive first impressions about these compositions, which are available as scores and audio files.      
### 47.On Classification of MIMO Equalizers  [ :arrow_down: ](https://arxiv.org/pdf/2106.00795.pdf)
>  In this theoretical work, the DSP-perceived channel in optical coherent communications is first simplified, based on which we categorize linear MIMO equalizers into four classes according to their reference locations. The entire channel inverse can be represented by a complex conjugate-dependent system, coinciding with the widely linear equalization theory. Suboptimally removing FO dynamics, relatively static channel inverses parameterized with common device and channel parameters are presented for monitoring or calibration purposes.      
### 48.SWIPT with Intelligent Reflecting Surfaces under Spatial Correlation  [ :arrow_down: ](https://arxiv.org/pdf/2106.00771.pdf)
>  Intelligent reflecting surfaces (IRSs) can be beneficial to both information and energy transfer, due to the gains achieved by their multiple elements. In this work, we deal with the impact of spatial correlation between the IRS elements, in the context of simultaneous wireless information and power transfer. The performance is evaluated in terms of the average harvested energy and the outage probability for random and equal phase shifts. Closed-form analytical expressions for both metrics under spatial correlation are derived. Moreover, the optimal case is considered when the elements are uncorrelated and fully correlated. In the uncorrelated case, random and equal phase shifts provide the same performance. However, the performance of correlated elements attains significant gains when there are equal phase shifts. Finally, we show that correlation is always beneficial to energy transfer, whereas it is a degrading factor for information transfer under random and optimal configurations.      
### 49.On-Line Policy Iteration for Infinite Horizon Dynamic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2106.00746.pdf)
>  In this paper we propose an on-line policy iteration (PI) algorithm for finite-state infinite horizon discounted dynamic programming, whereby the policy improvement operation is done on-line, only for the states that are encountered during operation of the system. This allows the continuous updating/improvement of the current policy, thus resulting in a form of on-line PI that incorporates the improved controls into the current policy as new states and controls are generated. The algorithm converges in a finite number of stages to a type of locally optimal policy, and suggests the possibility of variants of PI and multiagent PI where the policy improvement is simplified. Moreover, the algorithm can be used with on-line replanning, and is also well-suited for on-line PI algorithms with value and policy approximations.      
### 50.Online Detection of Vibration Anomalies Using Balanced Spiking Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.00687.pdf)
>  Vibration patterns yield valuable information about the health state of a running machine, which is commonly exploited in predictive maintenance tasks for large industrial systems. However, the overhead, in terms of size, complexity and power budget, required by classical methods to exploit this information is often prohibitive for smaller-scale applications such as autonomous cars, drones or robotics. Here we propose a neuromorphic approach to perform vibration analysis using spiking neural networks that can be applied to a wide range of scenarios. We present a spike-based end-to-end pipeline able to detect system anomalies from vibration data, using building blocks that are compatible with analog-digital neuromorphic circuits. This pipeline operates in an online unsupervised fashion, and relies on a cochlea model, on feedback adaptation and on a balanced spiking neural network. We show that the proposed method achieves state-of-the-art performance or better against two publicly available data sets. Further, we demonstrate a working proof-of-concept implemented on an asynchronous neuromorphic processor device. This work represents a significant step towards the design and implementation of autonomous low-power edge-computing devices for online vibration monitoring.      
### 51.Prostate cancer histopathology with label-free multispectral deep UV microscopy quantifies phenotypes of tumor grade and aggressiveness  [ :arrow_down: ](https://arxiv.org/pdf/2106.00682.pdf)
>  Identifying prostate cancer patients that are harboring aggressive forms of prostate cancer remains a significant clinical challenge. To shed light on this problem, we develop an approach based on multispectral deep-ultraviolet (UV) microscopy that provides novel quantitative insight into the aggressiveness and grade of this disease. First, we find that UV spectral signatures from endogenous molecules give rise to a phenotypical continuum that differentiates critical structures of thin tissue sections with subcellular spatial resolution, including nuclei, cytoplasm, stroma, basal cells, nerves, and inflammation. Further, we show that this phenotypical continuum can be applied as a surrogate biomarker of prostate cancer malignancy, where patients with the most aggressive tumors show a ubiquitous glandular phenotypical shift. Lastly, we adapt a two-part Cycle-consistent Generative Adversarial Network to translate the label-free deep-UV images into virtual hematoxylin and eosin (H&amp;E) stained images. Agreement between the virtual H&amp;E images and the gold standard H&amp;E-stained tissue sections is evaluated by a panel of pathologists who find that the two modalities are in excellent agreement. This work has significant implications towards improving our ability to objectively quantify prostate cancer grade and aggressiveness, thus improving the management and clinical outcomes of prostate cancer patients. This same approach can also be applied broadly in other tumor types to achieve low-cost, stain-free, quantitative histopathological analysis.      
