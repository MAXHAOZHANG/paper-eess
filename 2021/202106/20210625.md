# ArXiv eess --Fri, 25 Jun 2021
### 1.Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI  [ :arrow_down: ](https://arxiv.org/pdf/2106.13188.pdf)
>  Current deep learning approaches for diffusion MRI modeling circumvent the need for densely-sampled diffusion-weighted images (DWIs) by directly predicting microstructural indices from sparsely-sampled DWIs. However, they implicitly make unrealistic assumptions of static $q$-space sampling during training and reconstruction. Further, such approaches can restrict downstream usage of variably sampled DWIs for usages including the estimation of microstructural indices or tractography. We propose a generative adversarial translation framework for high-quality DWI synthesis with arbitrary $q$-space sampling given commonly acquired structural images (e.g., B0, T1, T2). Our translation network linearly modulates its internal representations conditioned on continuous $q$-space information, thus removing the need for fixed sampling schemes. Moreover, this approach enables downstream estimation of high-quality microstructural maps from arbitrarily subsampled DWIs, which may be particularly important in cases with sparsely sampled DWIs. Across several recent methodologies, the proposed approach yields improved DWI synthesis accuracy and fidelity with enhanced downstream utility as quantified by the accuracy of scalar microstructure indices estimated from the synthesized images. Code is available at <a class="link-external link-https" href="https://github.com/mengweiren/q-space-conditioned-dwi-synthesis" rel="external noopener nofollow">this https URL</a>.      
### 2.Comparison between safety methods control barrier function vs. reachability analysis  [ :arrow_down: ](https://arxiv.org/pdf/2106.13176.pdf)
>  This report aims to compare two safety methods: control barrier function and Hamilton-Jacobi reachability analysis. We will consider the difference with a focus on the following aspects: generality of system dynamics, difficulty of construction and computation cost. A standard Dubins car model will be evaluated numerically to make the comparison more concrete.      
### 3.Augmented Synchronization: Bridging the Theory-Practice Gap in Transient Stability Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2106.13166.pdf)
>  Transient stability has been extensively studied in the framework of Lyapunov stability. Theorists are interested in the convergence of all states to certain post-disturbance equilibrium, while practitioners often assess transient stability solely based on convergence to nominal frequencies and steady-state voltages, which we refer to as augmented synchronization. Despite substantial results, conventional transient stability theories suffer from the stringent requirement of prior knowledge of the post-disturbance equilibrium. On the other hand, the wisdom from practice seems to provide an equilibrium-independent approach, although one intriguing and worrying question remains: does the convergence of frequencies and voltages always imply convergence of all states? This paper aims to bridge such a theory-practice gap by pursuing two objectives. First, we derive checkable conditions under which converging to augmented synchronization implies convergence of all states to the equilibrium set, and hence lay a theoretical foundation for the empirical wisdom. Second, we establish several theorems that answer under what conditions the power system can converge to augmented synchronization. These two results enable a new equilibrium-independent analytic for transient stability, which essentially re-defines the nominal motion of power systems as augmented synchronization rather than certain equilibrium. This new methodology is promising especially when the post-disturbance equilibrium is unknown or non-isolated equilibria exit, in which cases the conventional methods cannot apply. Rudimentary single-machine examples and the IEEE 9-bus benchmark system well verify our results and illustrate interesting implications.      
### 4.High-resolution Image Registration of Consecutive and Re-stained Sections in Histopathology  [ :arrow_down: ](https://arxiv.org/pdf/2106.13150.pdf)
>  We compare variational image registration in consectutive and re-stained sections from histopathology. We present a fully-automatic algorithm for non-parametric (nonlinear) image registration and apply it to a previously existing dataset from the ANHIR challenge (230 slide pairs, consecutive sections) and a new dataset (hybrid re-stained and consecutive, 81 slide pairs, ca. 3000 landmarks) which is made publicly available. Registration hyperparameters are obtained in the ANHIR dataset and applied to the new dataset without modification. In the new dataset, landmark errors after registration range from 13.2 micrometers for consecutive sections to 1 micrometer for re-stained sections. We observe that non-parametric registration leads to lower landmark errors in both cases, even though the effect is smaller in re-stained sections. The nucleus-level alignment after non-parametric registration of re-stained sections provides a valuable tool to generate automatic ground-truth for machine learning applications in histopathology.      
### 5.Power and Modulation Format Transfer Learning for Neural Network Equalizers in Coherent Optical Transmission Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.13144.pdf)
>  Transfer learning is proposed to adapt an NN-based nonlinear equalizer across different launch powers and modulation formats using a 450km TWC-fiber transmission. The result shows up to 92% reduction in epochs or 90% in the training dataset.      
### 6.Experimental Study of Deep Neural Network Equalizers Performance in Optical Links  [ :arrow_down: ](https://arxiv.org/pdf/2106.13133.pdf)
>  We propose a convolutional-recurrent channel equalizer and experimentally demonstrate 1dB Q-factor improvement both in single-channel and 96 x WDM, DP-16QAM transmission over 450km of TWC fiber. The new equalizer outperforms previous NN-based approaches and a 3-steps-per-span DBP.      
### 7.Modeling Magnetic Particle Imaging for Dynamic Tracer Distributions  [ :arrow_down: ](https://arxiv.org/pdf/2106.13102.pdf)
>  Magnetic Particle Imaging (MPI) is a promising tracer-based, functional medical imaging technique which measures the non-linear response of superparamagnetic iron oxide nanoparticles (SPION) to a dynamic magnetic field. <br>For image reconstruction, system matrices from time-consuming calibration scans are used predominantly. Finding modeled forward operators for magnetic particle imaging, which are able to compete with measured matrices in practice, is an ongoing topic of research. The existing models for magnetic particle imaging are by design not suitable for dynamic tracer concentrations. Neither modeled nor measured system matrices account for changes in the concentration during a single scanning cycle. <br>In this paper we present a new MPI forward model for dynamic concentrations. A standard model will be introduced briefly, followed by the changes due to the dynamic behavior of the tracer concentration. Furthermore, the relevance of this new extended model is examined by investigating the influence of the extension and example reconstructions with the new and the standard model.      
### 8.Partial Maximum Correntropy Regression for Robust Trajectory Decoding from Noisy Epidural Electrocorticographic Signals  [ :arrow_down: ](https://arxiv.org/pdf/2106.13086.pdf)
>  The Partial Least Square Regression (PLSR) algorithm exhibits exceptional competence for predicting continuous variables from inter-correlated brain recordings in brain-computer interfaces, which achieved successful prediction from epidural electrocorticography of macaques to three-dimensional continuous hand trajectories recently. Nevertheless, PLSR is in essence formulated based on the least square criterion, thus, being non-robust with respect to complicated noises consequently. The aim of the present study is to propose a robust version of PLSR. To this end, the maximum correntropy criterion is adopted to structure a new robust variant of PLSR, namely Partial Maximum Correntropy Regression (PMCR). Half-quadratic optimization technique is utilized to calculate the robust latent variables. We assess the proposed PMCR on a synthetic example and the public Neurotycho dataset. Compared with the conventional PLSR and the state-of-the-art variant, PMCR realized superior prediction competence on three different performance indicators with contaminated training set. The proposed PMCR was demonstrated as an effective approach for robust decoding from noisy brain measurements, which could reduce the performance degradation resulting from adverse noises, thus, improving the decoding robustness of brain-computer interfaces.      
### 9.Coherent, super resolved radar beamforming using self-supervised learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.13085.pdf)
>  High resolution automotive radar sensors are required in order to meet the high bar of autonomous vehicles needs and regulations. However, current radar systems are limited in their angular resolution causing a technological gap. An industry and academic trend to improve angular resolution by increasing the number of physical channels, also increases system complexity, requires sensitive calibration processes, lowers robustness to hardware malfunctions and drives higher costs. We offer an alternative approach, named Radar signal Reconstruction using Self Supervision (R2-S2), which significantly improves the angular resolution of a given radar array without increasing the number of physical channels. R2-S2 is a family of algorithms which use a Deep Neural Network (DNN) with complex range-Doppler radar data as input and trained in a self-supervised method using a loss function which operates in multiple data representation spaces. Improvement of 4x in angular resolution was demonstrated using a real-world dataset collected in urban and highway environments during clear and rainy weather conditions.      
### 10.Real Time State Estimation of Power Grids Using Convolutional Neural Networks and State Forecasting Via Recurrent Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.13084.pdf)
>  Power grids play a very important role in delivering electrical energy to homes, industries and other places that require it. Because of this increased demand they are facing a great challenge of voltage variations. This happens due to varied use of energy-consuming devices and appliances like electric vehicles, industrial consumption, occasional peak in energy demands etc. For these fluctuations in demands, it becomes extremely important to monitor the conditions at which the power grid operates. Once these conditions are known, the energy production can be manipulated to meet the demand. It has been found that the existing Power System State Estimation (PSSE) techniques may not be good in producing optimal Performance. Moreover, they are also expensive in terms of computational processing. To address this problem, this research proposes a state estimation method for power grids using Convolutional Neural Networks (CNN). It was found that the model produced an RMSE of 2.57 x 10-4, which was comparatively accurate than one of previous studies involved in making the estimation using a Prox Linear Model (2.97 x 10-4). <br>Furthermore, the research also proposes Power System State Forecasting for improving system awareness and resilience. The forecasting is carried out using a model of Recurrent Neural Network (RNN). This model helps in accounting for long-term nonlinear aspects present in data and based on that it does the forecasting. The proposed model forecasted with a RMSE of 2.53 x 10-3, which is comparatively equal to the previous study mentioned above (2.59 x 10-3).      
### 11.A Declarative Goal-oriented Framework for Smart Environments with LPaaS  [ :arrow_down: ](https://arxiv.org/pdf/2106.13083.pdf)
>  Smart environments powered by the Internet of Things aim at improving our daily lives by automatically tuning ambient parameters (e.g. temperature, interior light) and by achieving energy savings through self-managing cyber-physical systems. Commercial solutions, however, only permit setting simple target goals on those parameters and do not consider mediating conflicting goals among different users and/or system administrators, and feature limited compatibility across different IoT verticals. In this article, we propose a declarative framework to represent smart environments, user-set goals and customisable mediation policies to reconcile contrasting goals encompassing multiple IoT systems. An open-source Prolog prototype of the framework is showcased over two lifelike motivating examples.      
### 12.Artifact Detection and Correction in EEG data: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2106.13081.pdf)
>  Electroencephalography (EEG) has countless applications across many of fields. However, EEG applications are limited by low signal-to-noise ratios. Multiple types of artifacts contribute to the noisiness of EEG, and many techniques have been proposed to detect and correct these artifacts. These techniques range from simply detecting and rejecting artifact ridden segments, to extracting the noise component from the EEG signal. In this paper we review a variety of recent and classical techniques for EEG data artifact detection and correction with a focus on the last half-decade. We compare the strengths and weaknesses of the approaches and conclude with proposed future directions for the field.      
### 13.Integration of Communication and Sensing in 6G: a Joint Industrial and Academic Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2106.13023.pdf)
>  6G will likely be the first generation of mobile communication that will feature tight integration of localization and sensing with communication functionalities. Among several worldwide initiatives, the Hexa-X flagship project stands out as it brings together 25 key players from adjacent industries and academia, and has among its explicit goals to research fundamentally new radio access technologies and high-resolution localization and sensing. Such features will not only enable novel use cases requiring extreme localization performance, but also provide a means to support and improve communication functionalities. This paper provides an overview of the Hexa-X vision alongside the envisioned use cases. To close the required performance gap of these use cases with respect to 5G, several technical enablers will be discussed, together with the associated research challenges for the coming years.      
### 14.Rate Distortion Characteristic Modeling for Neural Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2106.12954.pdf)
>  End-to-end optimization capability offers neural image compression (NIC) superior lossy compression performance. However, distinct models are required to be trained to reach different points in the rate-distortion (R-D) space. In this paper, we consider the problem of R-D characteristic analysis and modeling for NIC. We make efforts to formulate the essential mathematical functions to describe the R-D behavior of NIC using deep network and statistical modeling. Thus continuous bit-rate points could be elegantly realized by leveraging such model via a single trained network. In this regard, we propose a plugin-in module to learn the relationship between the target bit-rate and the binary representation for the latent variable of auto-encoder. Furthermore, we model the rate and distortion characteristic of NIC as a function of the coding parameter $\lambda$ respectively. Our experiments show our proposed method is easy to adopt and obtains competitive coding performance with fixed-rate coding approaches, which would benefit the practical deployment of NIC. In addition, the proposed model could be applied to NIC rate control with limited bit-rate error using a single network.      
### 15.VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2106.12930.pdf)
>  Radiographs are used as the most important imaging tool for identifying spine anomalies in clinical practice. The evaluation of spinal bone lesions, however, is a challenging task for radiologists. This work aims at developing and evaluating a deep learning-based framework, named VinDr-SpineXR, for the classification and localization of abnormalities from spine X-rays. First, we build a large dataset, comprising 10,468 spine X-ray images from 5,000 studies, each of which is manually annotated by an experienced radiologist with bounding boxes around abnormal findings in 13 categories. Using this dataset, we then train a deep learning classifier to determine whether a spine scan is abnormal and a detector to localize 7 crucial findings amongst the total 13. The VinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies, which is kept separate from the training set. It demonstrates an area under the receiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%, 90.02%) for the image-level classification task and a mean average precision (mAP@0.5) of 33.56% for the lesion-level localization task. These results serve as a proof of concept and set a baseline for future research in this direction. To encourage advances, the dataset, codes, and trained deep learning models are made publicly available.      
### 16.Generalized Nash Equilibrium Seeking Algorithm Design for Distributed Constrained Multi-Cluster Games  [ :arrow_down: ](https://arxiv.org/pdf/2106.12926.pdf)
>  The multi-cluster games are addressed in this paper, where all players team up with the players in the cluster that they belong to, and compete against the players in other clusters to minimize the cost function of their own cluster. The decision of every player is constrained by coupling inequality constraints, local inequality constraints and local convex set constraints. Our problem extends well-known noncooperative game problems and resource allocation problems by considering the competition between clusters and the cooperation within clusters at the same time. Besides, without involving the resource allocation within clusters, the noncooperative game between clusters, and the aforementioned constraints, existing game algorithms as well as resource allocation algorithms cannot solve the problem. In order to seek the variational generalized Nash equilibrium (GNE) of the multi-cluster games, we design a distributed algorithm via gradient descent and projections. Moreover, we analyze the convergence of the algorithm with the help of variational analysis and Lyapunov stability theory. Under the algorithm, all players asymptotically converge to the variational GNE of the multi-cluster game. Simulation examples are presented to verify the effectiveness of the algorithm.      
### 17.Continuous-Time Deep Glioma Growth Models  [ :arrow_down: ](https://arxiv.org/pdf/2106.12917.pdf)
>  The ability to estimate how a tumor might evolve in the future could have tremendous clinical benefits, from improved treatment decisions to better dose distribution in radiation therapy. Recent work has approached the glioma growth modeling problem via deep learning and variational inference, thus learning growth dynamics entirely from a real patient data distribution. So far, this approach was constrained to predefined image acquisition intervals and sequences of fixed length, which limits its applicability in more realistic scenarios. We overcome these limitations by extending Neural Processes, a class of conditional generative models for stochastic time series, with a hierarchical multi-scale representation encoding including a spatio-temporal attention mechanism. The result is a learned growth model that can be conditioned on an arbitrary number of observations, and that can produce a distribution of temporally consistent growth trajectories on a continuous time axis. On a dataset of 379 patients, the approach successfully captures both global and finer-grained variations in the images, exhibiting superior performance compared to other learned growth models.      
### 18.A Systematic Collection of Medical Image Datasets for Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.12864.pdf)
>  The astounding success made by artificial intelligence (AI) in healthcare and other fields proves that AI can achieve human-like performance. However, success always comes with challenges. Deep learning algorithms are data-dependent and require large datasets for training. The lack of data in the medical imaging field creates a bottleneck for the application of deep learning to medical image analysis. Medical image acquisition, annotation, and analysis are costly, and their usage is constrained by ethical restrictions. They also require many resources, such as human expertise and funding. That makes it difficult for non-medical researchers to have access to useful and large medical data. Thus, as comprehensive as possible, this paper provides a collection of medical image datasets with their associated challenges for deep learning research. We have collected information of around three hundred datasets and challenges mainly reported between 2013 and 2020 and categorized them into four categories: head &amp; neck, chest &amp; abdomen, pathology &amp; blood, and ``others''. Our paper has three purposes: 1) to provide a most up to date and complete list that can be used as a universal reference to easily find the datasets for clinical image analysis, 2) to guide researchers on the methodology to test and evaluate their methods' performance and robustness on relevant datasets, 3) to provide a ``route'' to relevant algorithms for the relevant medical topics, and challenge leaderboards.      
### 19.Transform-Based Feature Map Compression for CNN Inference  [ :arrow_down: ](https://arxiv.org/pdf/2106.12850.pdf)
>  To achieve higher accuracy in machine learning tasks, very deep convolutional neural networks (CNNs) are designed recently. However, the large memory access of deep CNNs will lead to high power consumption. A variety of hardware-friendly compression methods have been proposed to reduce the data transfer bandwidth by exploiting the sparsity of feature maps. Most of them focus on designing a specialized encoding format to increase the compression ratio. Differently, we observe and exploit the sparsity distinction between activations in earlier and later layers to improve the compression ratio. We propose a novel hardware-friendly transform-based method named 1D-Discrete Cosine Transform on Channel dimension with Masks (DCT-CM), which intelligently combines DCT, masks, and a coding format to compress activations. The proposed algorithm achieves an average compression ratio of 2.9x (53% higher than the state-of-the-art transform-based feature map compression works) during inference on ResNet-50 with an 8-bit quantization scheme.      
### 20.NN2CAM: Automated Neural Network Mapping for Multi-Precision Edge Processing on FPGA-Based Cameras  [ :arrow_down: ](https://arxiv.org/pdf/2106.12840.pdf)
>  The record-breaking achievements of deep neural networks (DNNs) in image classification and detection tasks resulted in a surge of new computer vision applications during the past years. However, their computational complexity is restricting their deployment to powerful stationary or complex dedicated processing hardware, limiting their use in smart edge processing applications. We propose an automated deployment framework for DNN acceleration at the edge on field-programmable gate array (FPGA)-based cameras. The framework automatically converts an arbitrary-sized and quantized trained network into an efficient streaming-processing IP block that is instantiated within a generic adapter block in the FPGA. In contrast to prior work, the accelerator is purely logic and thus supports end-to-end processing on FPGAs without on-chip microprocessors. Our mapping tool features automatic translation from a trained Caffe network, arbitrary layer-wise fixed-point precision for both weights and activations, an efficient XNOR implementation for fully binary layers as well as a balancing mechanism for effective allocation of computational resources in the streaming dataflow. To present the performance of the system we employ this tool to implement two CNN edge processing networks on an FPGA-based high-speed camera with various precision settings showing computational throughputs of up to 337GOPS in low-latency streaming mode (no batching), running entirely on the camera.      
### 21.Orthogonal Time Frequency Space Modulation: A Discrete Zak Transform Approach  [ :arrow_down: ](https://arxiv.org/pdf/2106.12828.pdf)
>  In orthogonal time frequency space (OTFS) modulation, information-carrying symbols reside in the delay-Doppler (DD) domain. By operating in the DD domain, an appealing property for communication arises: time-frequency (TF) dispersive channels encountered in high mobility environments become time-invariant. The time-invariance of the channel in the DD domain enables efficient equalizers for time-frequency dispersive channels. In this paper, we propose an OTFS system based on the discrete Zak transform. The presented formulation not only allows an efficient implementation of OTFS but also simplifies the derivation and analysis of the input-output relation of TF dispersive channel in the DD domain.      
### 22.AVHYAS: A Free and Open Source QGIS Plugin for Advanced Hyperspectral Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2106.12776.pdf)
>  Advanced Hyperspectral Data Analysis Software (AVHYAS) plugin is a python3 based quantum GIS (QGIS) plugin designed to process and analyse hyperspectral (Hx) images. It is developed to guarantee full usage of present and future Hx airborne or spaceborne sensors and provides access to advanced algorithms for Hx data processing. The software is freely available and offers a range of basic and advanced tools such as atmospheric correction (for airborne AVIRISNG image), standard processing tools as well as powerful machine learning and Deep Learning interfaces for Hx data analysis.      
### 23.Deep Learning-Aided OFDM-Based Generalized Optical Quadrature Spatial Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2106.12770.pdf)
>  In this paper, we propose an orthogonal frequency division multiplexing (OFDM)-based generalized optical quadrature spatial modulation (GOQSM) technique for multiple-input multiple-output optical wireless communication (MIMO-OWC) systems. Considering the error propagation and noise amplification effects when applying maximum likelihood and maximum ratio combining (ML-MRC)-based detection, we further propose a deep neural network (DNN)-aided detection for OFDM-based GOQSM systems. The proposed DNN-aided detection scheme performs the GOQSM detection in a joint manner, which can efficiently eliminate the adverse effects of both error propagation and noise amplification. The obtained simulation results successfully verify the superiority of the deep learning-aided OFDM-based GOQSM technique for high-speed MIMO-OWC systems.      
### 24.SRIB-LEAP submission to Far-field Multi-Channel Speech Enhancement Challenge for Video Conferencing  [ :arrow_down: ](https://arxiv.org/pdf/2106.12763.pdf)
>  This paper presents the details of the SRIB-LEAP submission to the ConferencingSpeech challenge 2021. The challenge involved the task of multi-channel speech enhancement to improve the quality of far field speech from microphone arrays in a video conferencing room. We propose a two stage method involving a beamformer followed by single channel enhancement. For the beamformer, we incorporated self-attention mechanism as inter-channel processing layer in the filter-and-sum network (FaSNet), an end-to-end time-domain beamforming system. The single channel speech enhancement is done in log spectral domain using convolution neural network (CNN)-long short term memory (LSTM) based architecture. We achieved improvements in objective quality metrics - perceptual evaluation of speech quality (PESQ) of 0.5 on the noisy data. On subjective quality evaluation, the proposed approach improved the mean opinion score (MOS) by an absolute measure of 0.9 over the noisy audio.      
### 25.Control of a Mixed Autonomy Signalised Urban Intersection: An Action-Delayed Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2106.12755.pdf)
>  We consider a mixed autonomy scenario where the traffic intersection controller decides whether the traffic light will be green or red at each lane for multiple traffic-light blocks. The objective of the traffic intersection controller is to minimize the queue length at each lane and maximize the outflow of vehicles over each block. We consider that the traffic intersection controller informs the autonomous vehicle (AV) whether the traffic light will be green or red for the future traffic-light block. Thus, the AV can adapt its dynamics by solving an optimal control problem. We model the decision process of the traffic intersection controller as a deterministic delay Markov decision process owing to the delayed action by the traffic controller. We propose Reinforcement-learning based algorithm to obtain the optimal policy. We show - empirically - that our algorithm converges and reduces the energy costs of AVs drastically as the traffic controller communicates with the AVs.      
### 26.A Global Appearance and Local Coding Distortion based Fusion Framework for CNN based Filtering in Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2106.12746.pdf)
>  In-loop filtering is used in video coding to process the reconstructed frame in order to remove blocking artifacts. With the development of convolutional neural networks (CNNs), CNNs have been explored for in-loop filtering considering it can be treated as an image de-noising task. However, in addition to being a distorted image, the reconstructed frame is also obtained by a fixed line of block based encoding operations in video coding. It carries coding-unit based coding distortion of some similar characteristics. Therefore, in this paper, we address the filtering problem from two aspects, global appearance restoration for disrupted texture and local coding distortion restoration caused by fixed pipeline of coding. Accordingly, a three-stream global appearance and local coding distortion based fusion network is developed with a high-level global feature stream, a high-level local feature stream and a low-level local feature stream. Ablation study is conducted to validate the necessity of different features, demonstrating that the global features and local features can complement each other in filtering and achieve better performance when combined. To the best of our knowledge, we are the first one that clearly characterizes the video filtering process from the above global appearance and local coding distortion restoration aspects with experimental verification, providing a clear pathway to developing filter techniques. Experimental results demonstrate that the proposed method significantly outperforms the existing single-frame based methods and achieves 13.5%, 11.3%, 11.7% BD-Rate saving on average for AI, LDP and RA configurations, respectively, compared with the HEVC reference software.      
### 27.Perceptually Guided Adversarial Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2106.12731.pdf)
>  It is well known that carefully crafted imperceptible perturbations can cause state-of-the-art deep learning classification models to misclassify. Understanding and analyzing these adversarial perturbations play a crucial role in the design of robust convolutional neural networks. However, the reasons for the existence of adversarial perturbations and their mechanics are not well understood. In this work, we attempt to systematically answer the following question: do imperceptible adversarial perturbations focus on changing the regions of the image that are important for classification? Most current methods use $l_p$ distance to generate and characterize the imperceptibility of the adversarial perturbations. However, since $l_p$ distances only measure the pixel to pixel distances and do not consider the structure in the image, these methods do not provide a satisfactory answer to the above question. To address this issue, we propose a novel framework for generating adversarial perturbations by explicitly incorporating a ``perceptual quality ball" constraint in our formulation. Specifically, we pose the adversarial example generation problem as a tractable convex optimization problem, with constraints taken from a mathematically amenable variant of the popular SSIM index. We show that the perturbations generated by the proposed method result in a high fooling rate with minimal impact on perceptual quality compared to the norm bounded adversarial perturbations. Further, through SSIM maps, we show that the perceptually guided perturbations introduce changes specifically in the regions that contribute to classification decisions. We use networks trained on MNIST and CIFAR-10 datasets quantitative analysis, and MobileNetV2 trained on the ImageNet dataset for further qualitative analysis.      
### 28.ATP-Net: An Attention-based Ternary Projection Network For Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2106.12728.pdf)
>  Compressed Sensing (CS) theory simultaneously realizes the signal sampling and compression process, and can use fewer observations to achieve accurate signal recovery, providing a solution for better and faster transmission of massive data. In this paper, a ternary sampling matrix-based method with attention mechanism is proposed with the purpose to solve the problem that the CS sampling matrices in most cases are random matrices, which are irrelative to the sampled signal and need a large storage space. The proposed method consists of three components, i.e., ternary sampling, initial reconstruction and deep reconstruction, with the emphasis on the ternary sampling. The main idea of the ternary method (-1, 0, +1) is to introduce the attention mechanism to evaluate the importance of parameters at the sampling layer after the sampling matrix is binarized (-1, +1), followed by pruning weight of parameters, whose importance is below a predefined threshold, to achieve ternarization. Furthermore, a compressed sensing algorithm especially for image reconstruction is implemented, on the basis of the ternary sampling matrix, which is called ATP-Net, i.e., Attention-based ternary projection network. Experimental results show that the quality of image reconstruction by means of ATP-Net maintains a satisfactory level with the employment of the ternary sampling matrix, i.e., the average PSNR on Set11 is 30.4 when the sampling rate is 0.25, approximately 6% improvement compared with that of DR2-Net.      
### 29.A Novel Compact Dual-Band Antenna Design for WLAN Applications  [ :arrow_down: ](https://arxiv.org/pdf/2106.13232.pdf)
>  A novel and compact dual band planar antenna for 2.4/5.2/5.8-GHz wireless local area network(WLAN) applications is proposed and studied in this paper. The antenna comprises of a T-shaped and a F-shaped element to generate two resonant modes for dual band operation. The two elements can independently control the operating frequencies of the two excited resonant modes. The T-element which is fed directly by a 50 $\Omega$ microstrip line generates a frequency band at around 5.2 GHz and the antenna parameters can be adjusted to generate a frequency band at 5.8 GHz as well, thus covering the two higher bands of WLAN systems individually. By couple-feeding the F-element through the T-element, a frequency band can be generated at 2.4 GHz to cover the lower band of WLAN system. Hence, the two elements together are very compact with a total area of only 11$\times$6.5 mm$^{2}$. A thorough parametric study of key dimensions in the design has been performed and the results obtained have been used to present a generalized design approach. Plots of the return loss and radiation pattern have been given and discussed in detail to show that the design is a very promising candidate for WLAN applications.      
### 30.Resource Management for Transmit Power Minimization in UAV-Assisted RIS HetNets Supported by Dual Connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2106.13174.pdf)
>  This paper proposes a novel approach to improve the performance of a heterogeneous network (HetNet) supported by dual connectivity (DC) by adopting multiple unmanned aerial vehicles (UAVs) as passive relays that carry reconfigurable intelligent surfaces (RISs). More specifically, RISs are deployed under the UAVs termed as UAVs-RISs that operate over the micro-wave ($\mu$W) channel in the sky to sustain a strong line-of-sight (LoS) connection with the ground users. The macro-cell operates over the $\mu$W channel based on orthogonal multiple access (OMA), while small base stations (SBSs) operate over the millimeter-wave (mmW) channel based on non-orthogonal multiple access (NOMA). We study the problem of total transmit power minimization by jointly optimizing the trajectory/velocity of each UAV, RISs' phase shifts, subcarrier allocations, and active beamformers at each BS. The underlying problem is highly non-convex and the global optimal solution is intractable. To handle it, we decompose the original problem into two subproblems, i.e., a subproblem which deals with the UAVs' trajectories/velocities, RISs' phase shifts, and subcarrier allocations for $\mu$W; and a subproblem for active beamforming design and subcarrier allocation for mmW. In particular, we solve the first subproblem via the dueling deep Q-Network (DQN) learning approach by developing a distributed algorithm which leads to a better policy evaluation. Then, we solve the active beamforming design and subcarrier allocation for the mmW via the successive convex approximation (SCA) method. Simulation results exhibit the effectiveness of the proposed resource allocation scheme compared to other baseline schemes. In particular, it is revealed that by deploying UAVs-RISs, the transmit power can be reduced by 6 dBm while maintaining similar guaranteed QoS.      
### 31.Some Problems of Deployment and Navigation of Civilian Aerial Drones  [ :arrow_down: ](https://arxiv.org/pdf/2106.13162.pdf)
>  One of the biggest challenges is to determine the deployment and navigation of the drones to benefit the most for different applications. Many research questions have been raised about this topic. For example, drone-enabled wildlife monitoring has received much attention in recent years. Unfortunately, this approach results in significant disturbance to different species of wild animals. Moreover, with the capability of rapidly moving communication supply towards demand when required, the drone equipped with a base station, i.e., drone-cell, is becoming a promising solution for providing cellular networks to victims and rescue teams in disaster-affected areas. However, few studies have investigated the optimal deployments of multiple drone-cells with limited backhaul communication distances. In addition, the use of autonomous drones as flying interactors for many real-life applications has not been sufficiently discussed. With superior maneuverability, drone-enabled autonomous aerial interacting can potentially be used on shark attack prevention and animal herding. Nevertheless, previous studies of autonomous drones have not dealt with such applications in much detail. This report explores the solutions to all the mentioned research questions, with a particular focus on the deployment and navigation of the drones. Simulations have been conducted to verify the effectiveness of the proposed approaches. We believe that our findings in this report shed new light on the fundamental benefits of autonomous civilian drones.      
### 32.Shallow Representation is Deep: Learning Uncertainty-aware and Worst-case Random Feature Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2106.13066.pdf)
>  Random features is a powerful universal function approximator that inherits the theoretical rigor of kernel methods and can scale up to modern learning tasks. This paper views uncertain system models as unknown or uncertain smooth functions in universal reproducing kernel Hilbert spaces. By directly approximating the one-step dynamics function using random features with uncertain parameters, which are equivalent to a shallow Bayesian neural network, we then view the whole dynamical system as a multi-layer neural network. Exploiting the structure of Hamiltonian dynamics, we show that finding worst-case dynamics realizations using Pontryagin's minimum principle is equivalent to performing the Frank-Wolfe algorithm on the deep net. Various numerical experiments on dynamics learning showcase the capacity of our modeling methodology.      
### 33.Advancing biological super-resolution microscopy through deep learning: a brief review  [ :arrow_down: ](https://arxiv.org/pdf/2106.13064.pdf)
>  Super-resolution microscopy overcomes the diffraction limit of conventional light microscopy in spatial resolution. By providing novel spatial or spatio-temporal information on biological processes at nanometer resolution with molecular specificity, it plays an increasingly important role in life sciences. However, its technical limitations require trade-offs to balance its spatial resolution, temporal resolution, and light exposure of samples. Recently, deep learning has achieved breakthrough performance in many image processing and computer vision tasks. It has also shown great promise in pushing the performance envelope of super-resolution microscopy. In this brief Review, we survey recent advances in using deep learning to enhance performance of super-resolution microscopy. We focus primarily on how deep learning ad-vances reconstruction of super-resolution images. Related key technical challenges are discussed. Despite the challenges, deep learning is set to play an indispensable and transformative role in the development of super-resolution microscopy. We conclude with an outlook on how deep learning could shape the future of this new generation of light microscopy technology.      
### 34.Where are we in semantic concept extraction for Spoken Language Understanding?  [ :arrow_down: ](https://arxiv.org/pdf/2106.13045.pdf)
>  Spoken language understanding (SLU) topic has seen a lot of progress these last three years, with the emergence of end-to-end neural approaches. Spoken language understanding refers to natural language processing tasks related to semantic extraction from speech signal, like named entity recognition from speech or slot filling task in a context of human-machine dialogue. Classically, SLU tasks were processed through a cascade approach that consists in applying, firstly, an automatic speech recognition process, followed by a natural language processing module applied to the automatic transcriptions. These three last years, end-to-end neural approaches, based on deep neural networks, have been proposed in order to directly extract the semantics from speech signal, by using a single neural model. More recent works on self-supervised training with unlabeled data open new perspectives in term of performance for automatic speech recognition and natural language processing. In this paper, we present a brief overview of the recent advances on the French MEDIA benchmark dataset for SLU, with or without the use of additional data. We also present our last results that significantly outperform the current state-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for the last state-of-the-art system presented this year.      
### 35.AudioCLIP: Extending CLIP to Image, Text and Audio  [ :arrow_down: ](https://arxiv.org/pdf/2106.13043.pdf)
>  In the past, the rapidly evolving field of sound classification greatly benefited from the application of methods from other domains. Today, we observe the trend to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models. <br>In this work, we present an extension of the CLIP model that handles audio in addition to text and images. Our proposed model incorporates the ESResNeXt audio-model into the CLIP framework using the AudioSet dataset. Such a combination enables the proposed model to perform bimodal and unimodal classification and querying, while keeping CLIP's ability to generalize to unseen datasets in a zero-shot inference fashion. <br>AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task, out-performing other approaches by reaching accuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets. Further it sets new baselines in the zero-shot ESC-task on the same datasets 68.78% and 69.40%, respectively). <br>Finally, we also assess the cross-modal querying performance of the proposed model as well as the influence of full and partial training on the results. For the sake of reproducibility, our code is published.      
### 36.Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.13041.pdf)
>  Understanding the 3D world from 2D projected natural images is a fundamental challenge in computer vision and graphics. Recently, an unsupervised learning approach has garnered considerable attention owing to its advantages in data collection. However, to mitigate training limitations, typical methods need to impose assumptions for viewpoint distribution (e.g., a dataset containing various viewpoint images) or object shape (e.g., symmetric objects). These assumptions often restrict applications; for instance, the application to non-rigid objects or images captured from similar viewpoints (e.g., flower or bird images) remains a challenge. To complement these approaches, we propose aperture rendering generative adversarial networks (AR-GANs), which equip aperture rendering on top of GANs, and adopt focus cues to learn the depth and depth-of-field (DoF) effect of unlabeled natural images. To address the ambiguities triggered by unsupervised setting (i.e., ambiguities between smooth texture and out-of-focus blurs, and between foreground and background blurs), we develop DoF mixture learning, which enables the generator to learn real image distribution while generating diverse DoF images. In addition, we devise a center focus prior to guiding the learning direction. In the experiments, we demonstrate the effectiveness of AR-GANs in various datasets, such as flower, bird, and face images, demonstrate their portability by incorporating them into other 3D representation learning GANs, and validate their applicability in shallow DoF rendering.      
### 37.QASR: QCRI Aljazeera Speech Resource -- A Large Scale Annotated Arabic Speech Corpus  [ :arrow_down: ](https://arxiv.org/pdf/2106.13000.pdf)
>  We introduce the largest transcribed Arabic speech corpus, QASR, collected from the broadcast domain. This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others. QASR is suitable for training and evaluating speech recognition systems, acoustics- and/or linguistics- based Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other NLP modules for spoken data. In addition to QASR transcription, we release a dataset of 130M words to aid in designing and training a better language model. We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus. We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. We also report the first baseline for Arabic punctuation restoration. We make the corpus available for the research community.      
### 38.SofaMyRoom: a fast and multiplatform "shoebox" room simulator for binaural room impulse response dataset generation  [ :arrow_down: ](https://arxiv.org/pdf/2106.12992.pdf)
>  This paper introduces a shoebox room simulator able to systematically generate synthetic datasets of binaural room impulse responses (BRIRs) given an arbitrary set of head-related transfer functions (HRTFs). The evaluation of machine hearing algorithms frequently requires BRIR datasets in order to simulate the acoustics of any environment. However, currently available solutions typically consider only HRTFs measured on dummy heads, which poorly characterize the high variability in spatial sound perception. Our solution allows to integrate a room impulse response (RIR) simulator with different HRTF sets represented in Spatially Oriented Format for Acoustics (SOFA). The source code and the compiled binaries for different operating systems allow to both advanced and non-expert users to benefit from our toolbox, see <a class="link-external link-https" href="https://github.com/spatialaudiotools/sofamyroom/" rel="external noopener nofollow">this https URL</a> .      
### 39.Relationship between pulmonary nodule malignancy and surrounding pleurae, airways and vessels: a quantitative study using the public LIDC-IDRI dataset  [ :arrow_down: ](https://arxiv.org/pdf/2106.12991.pdf)
>  To investigate whether the pleurae, airways and vessels surrounding a nodule on non-contrast computed tomography (CT) can discriminate benign and malignant pulmonary nodules. The LIDC-IDRI dataset, one of the largest publicly available CT database, was exploited for study. A total of 1556 nodules from 694 patients were involved in statistical analysis, where nodules with average scorings &lt;3 and &gt;3 were respectively denoted as benign and malignant. Besides, 339 nodules from 113 patients with diagnosis ground-truth were independently evaluated. Computer algorithms were developed to segment pulmonary structures and quantify the distances to pleural surface, airways and vessels, as well as the counting number and normalized volume of airways and vessels near a nodule. Odds ratio (OR) and Chi-square (\chi^2) testing were performed to demonstrate the correlation between features of surrounding structures and nodule malignancy. A non-parametric receiver operating characteristic (ROC) analysis was conducted in logistic regression to evaluate discrimination ability of each structure. For benign and malignant groups, the average distances from nodules to pleural surface, airways and vessels are respectively (6.56, 5.19), (37.08, 26.43) and (1.42, 1.07) mm. The correlation between nodules and the counting number of airways and vessels that contact or project towards nodules are respectively (OR=22.96, \chi^2=105.04) and (OR=7.06, \chi^2=290.11). The correlation between nodules and the volume of airways and vessels are (OR=9.19, \chi^2=159.02) and (OR=2.29, \chi^2=55.89). The areas-under-curves (AUCs) for pleurae, airways and vessels are respectively 0.5202, 0.6943 and 0.6529. Our results show that malignant nodules are often surrounded by more pulmonary structures compared with benign ones, suggesting that features of these structures could be viewed as lung cancer biomarkers.      
### 40.A multi-center prospective evaluation of THEIA to detect diabetic retinopathy (DR) and diabetic macular edema (DME) in the New Zealand screening program  [ :arrow_down: ](https://arxiv.org/pdf/2106.12979.pdf)
>  Purpose: to assess the efficacy of THEIA, an artificial intelligence for screening diabetic retinopathy in a multi-center prospective study. To validate the potential application of THEIA as clinical decision making assistant in a national screening program. Methods: 902 patients were recruited from either an urban large eye hospital, or a semi-rural optometrist led screening provider, as they were attending their appointment as part of New Zealand Diabetic Screening programme. These clinics used a variety of retinal cameras and a range of operators. The de-identified images were then graded independently by three senior retinal specialists, and final results were aggregated using New Zealand grading scheme, which is then converted to referable\non-referable and Healthy\mild\more than mild\vision threatening categories. Results: compared to ground truth, THEIA achieved 100% sensitivity and [95.35%-97.44%] specificity, and negative predictive value of 100%. THEIA also did not miss any patients with more than mild or vision threatening disease. The level of agreement between the clinicians and the aggregated results was (k value: 0.9881, 0.9557, and 0.9175), and the level of agreement between THEIA and the aggregated labels was (k value: 0.9515). Conclusion: Our multi-centre prospective trial showed that THEIA does not miss referable disease when screening for diabetic retinopathy and maculopathy. It also has a very high level of granularity in reporting the disease level. Since THEIA is being tested on a variety of cameras, operating in a range of clinics (rural\urban, ophthalmologist-led\optometrist-led), we believe that it will be a suitable addition to a public diabetic screening program.      
### 41.Massive Wireless Energy Transfer with Multiple Power Beacons for very large Internet of Things  [ :arrow_down: ](https://arxiv.org/pdf/2106.12968.pdf)
>  The Internet of Things (IoT) comprises an increasing number of low-power and low-cost devices that autonomously interact with the surrounding environment. As a consequence of their popularity, future IoT deployments will be massive, which demands energy-efficient systems to extend their lifetime and improve the user experience. Radio frequency wireless energy transfer has the potential of powering massive IoT networks, thus eliminating the need for frequent battery replacement by using the so-called power beacons (PBs). In this paper, we provide a framework for minimizing the sum transmit power of the PBs using devices' positions information and their current battery state. Our strategy aims to reduce the PBs' power consumption and to mitigate the possible impact of the electromagnetic radiation on human health. We also present analytical insights for the case of very distant clusters and evaluate their applicability. Numerical results show that our proposed framework reduces the outage probability as the number of PBs and/or the energy demands increase.      
### 42.Speech is Silver, Silence is Golden: What do ASVspoof-trained Models Really Learn?  [ :arrow_down: ](https://arxiv.org/pdf/2106.12914.pdf)
>  The ASVspoof Dataset is one of the most established datasets for training and benchmarking systems designed for the detection of spoofed audio and audio deepfakes. However, we observe an uneven distribution of leading silence in dataset's training and test data, which hints at the target label: Bona-fide instances tend to have significantly longer leading silences than spoofed instances. This could be problematic, since a model may learn to only, or at least partially, base its decision on the length of the leading silence (similar to the issue with the Pascal VOC 2007 dataset, where all images of horses also contained a specific watermark). In this paper, we explore this phenomenon in depth. We train a number of networks on only a) the length of the leading silence and b) with and without leading silence. Results show that models trained on only the length of the leading silence perform suspiciously well: They achieve up to 85% percent accuracy and an equal error rate (EER) of 0.15 on the 'eval' split of the data. Conversely, when training strong models on the full audio files, we observe that removing leading silence during preprocessing dramatically worsens performance (EER increases from 0.05 to 0.2). This could indicate that previous work may, in part, have learned only to classify targets based on leading silence. We hope that by sharing these results, the ASV community can further evaluate this phenomenon.      
### 43.A Novel Compact Tri-Band Antenna Design for WiMAX, WLAN and Bluetooth Applications  [ :arrow_down: ](https://arxiv.org/pdf/2106.12884.pdf)
>  A novel and compact tri-band planar antenna for 2.4/5.2/5.8-GHz wireless local area network (WLAN), 2.3/3.5/5.5GHz Worldwide Interoperability for Microwave Access (WiMAX) and Bluetooth applications is proposed and studied in this paper. The antenna comprises of a L-shaped element which is coupled with a ground shorted parasitic resonator to generate three resonant modes for tri-band operation. The L-shaped element which is placed on top of the substrate is fed by a 50$\Omega$ microstrip feed line and is responsible for the generation of a wide band at 5.5 GHz. The parasitic resonator is placed on the other side of the substrate and is directly connected to the ground plane. The presence of the parasitic resonator gives rise to two additional resonant bands at 2.3 GHz and 3.5 GHz. Thus, together the two elements generate three resonant bands to cover WLAN, WiMAX and Bluetooth bands of operation. A thorough parametric study has been performed on the antenna and it has been found that the three bands can be tuned by varying certain dimensions of the antenna. Hence, the same design can be used for frequencies in adjacent bands as well with minor changes in its dimensions. Important antenna parameters such as return loss, radiation pattern and peak gains in the operating bands have been studied in detail to prove that the proposed design is a promising candidate for the aforementioned wireless technologies.      
### 44.Optimizing Intelligent Reflecting Surface-Base Station Association for Mobile Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.12883.pdf)
>  This paper studies a multi-Intelligent Reflecting Surfaces (IRSs)-assisted wireless network consisting of multiple base stations (BSs) serving a set of mobile users. We focus on the IRS-BS association problem in which multiple BSs compete with each other for controlling the phase shifts of a limited number of IRSs to maximize the long-term downlink data rate for the associated users. We propose MDLBI, a Multi-agent Deep Reinforcement Learning-based BS-IRS association scheme that optimizes the BS-IRS association as well as the phase-shift of each IRS when being associated with different BSs. MDLBI does not require information exchanging among BSs. Simulation results show that MDLBI achieves significant performance improvement and is scalable for large networking systems.      
### 45.Additive Phoneme-aware Margin Softmax Loss for Language Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2106.12851.pdf)
>  This paper proposes an additive phoneme-aware margin softmax (APM-Softmax) loss to train the multi-task learning network with phonetic information for language recognition. In additive margin softmax (AM-Softmax) loss, the margin is set as a constant during the entire training for all training samples, and that is a suboptimal method since the recognition difficulty varies in training samples. In additive angular margin softmax (AAM-Softmax) loss, the additional angular margin is set as a costant as well. In this paper, we propose an APM-Softmax loss for language recognition with phoneitc multi-task learning, in which the additive phoneme-aware margin is automatically tuned for different training samples. More specifically, the margin of language recognition is adjusted according to the results of phoneme recognition. Experiments are reported on Oriental Language Recognition (OLR) datasets, and the proposed method improves AM-Softmax loss and AAM-Softmax loss in different language recognition testing conditions.      
### 46.Multilingual transfer of acoustic word embeddings improves when training on languages related to the target zero-resource language  [ :arrow_down: ](https://arxiv.org/pdf/2106.12834.pdf)
>  Acoustic word embedding models map variable duration speech segments to fixed dimensional vectors, enabling efficient speech search and discovery. Previous work explored how embeddings can be obtained in zero-resource settings where no labelled data is available in the target language. The current best approach uses transfer learning: a single supervised multilingual model is trained using labelled data from multiple well-resourced languages and then applied to a target zero-resource language (without fine-tuning). However, it is still unclear how the specific choice of training languages affect downstream performance. Concretely, here we ask whether it is beneficial to use training languages related to the target. Using data from eleven languages spoken in Southern Africa, we experiment with adding data from different language families while controlling for the amount of data per language. In word discrimination and query-by-example search evaluations, we show that training on languages from the same family gives large improvements. Through finer-grained analysis, we show that training on even just a single related language gives the largest gain. We also find that adding data from unrelated languages generally doesn't hurt performance.      
### 47.Hamiltonian-based Neural ODE Networks on the SE(3) Manifold For Dynamics Learning and Control  [ :arrow_down: ](https://arxiv.org/pdf/2106.12782.pdf)
>  Accurate models of robot dynamics are critical for safe and stable control and generalization to novel operational conditions. Hand-designed models, however, may be insufficiently accurate, even after careful parameter tuning. This motivates the use of machine learning techniques to approximate the robot dynamics over a training set of state-control trajectories. The dynamics of many robots, including ground, aerial, and underwater vehicles, are described in terms of their SE(3) pose and generalized velocity, and satisfy conservation of energy principles. This paper proposes a Hamiltonian formulation over the SE(3) manifold of the structure of a neural ordinary differential equation (ODE) network to approximate the dynamics of a rigid body. In contrast to a black-box ODE network, our formulation guarantees total energy conservation by construction. We develop energy shaping and damping injection control for the learned, potentially under-actuated SE(3) Hamiltonian dynamics to enable a unified approach for stabilization and trajectory tracking with various platforms, including pendulum, rigid-body, and quadrotor systems.      
### 48.Density Constrained Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.12764.pdf)
>  We study constrained reinforcement learning (CRL) from a novel perspective by setting constraints directly on state density functions, rather than the value functions considered by previous works. State density has a clear physical and mathematical interpretation, and is able to express a wide variety of constraints such as resource limits and safety requirements. Density constraints can also avoid the time-consuming process of designing and tuning cost functions required by value function-based constraints to encode system specifications. We leverage the duality between density functions and Q functions to develop an effective algorithm to solve the density constrained RL problem optimally and the constrains are guaranteed to be satisfied. We prove that the proposed algorithm converges to a near-optimal solution with a bounded error even when the policy update is imperfect. We use a set of comprehensive experiments to demonstrate the advantages of our approach over state-of-the-art CRL methods, with a wide range of density constrained tasks as well as standard CRL benchmarks such as Safety-Gym.      
### 49.Bayesian Differential Privacy for Linear Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.12749.pdf)
>  Differential privacy is a privacy measure based on the difficulty of discriminating between similar input data. In differential privacy analysis, similar data usually implies that their distance does not exceed a predetermined threshold. It, consequently, does not take into account the difficulty of distinguishing data sets that are far apart, which often contain highly private information. This problem has been pointed out in the research on differential privacy for static data, and Bayesian differential privacy has been proposed, which provides a privacy protection level even for outlier data by utilizing the prior distribution of the data. In this study, we introduce this Bayesian differential privacy to dynamical systems, and provide privacy guarantees for distant input data pairs and reveal its fundamental property. For example, we design a mechanism that satisfies the desired level of privacy protection, which characterizes the trade-off between privacy and information utility.      
### 50.A Simultaneous Denoising and Dereverberation Framework with Target Decoupling  [ :arrow_down: ](https://arxiv.org/pdf/2106.12743.pdf)
>  Background noise and room reverberation are regarded as two major factors to degrade the subjective speech quality. In this paper, we propose an integrated framework to address simultaneous denoising and dereverberation under complicated scenario environments. It adopts a chain optimization strategy and designs four sub-stages accordingly. In the first two stages, we decouple the multi-task learning w.r.t. complex spectrum into magnitude and phase, and only implement noise and reverberation removal in the magnitude domain. Based on the estimated priors above, we further polish the spectrum in the third stage, where both magnitude and phase information are explicitly repaired with the residual learning. Due to the data mismatch and nonlinear effect of DNNs, the residual noise often exists in the DNN-processed spectrum. To resolve the problem, we adopt a light-weight algorithm as the post-processing module to capture and suppress the residual noise in the non-active regions. In the Interspeech 2021 Deep Noise Suppression (DNS) Challenge, our submitted system ranked top-1 for the real-time track in terms of Mean Opinion Score (MOS) with ITU-T P.835 framework      
### 51.Measuring and Optimizing System Reliability: A Stochastic Programming Approach  [ :arrow_down: ](https://arxiv.org/pdf/2106.12712.pdf)
>  We propose a computational framework to quantify (measure) and to optimize the reliability of complex systems. The approach uses a graph representation of the system that is subject to random failures of its components (nodes and edges). Under this setting, reliability is defined as the probability of finding a path between sources and sink nodes under random component failures and we show that this measure can be computed by solving a stochastic mixed-integer program. The stochastic programming setting allows us to account for system constraints and general probability distributions to characterize failures and allows us to derive optimization formulations that identify designs of maximum reliability. We also propose a strategy to approximately solve these problems in a scalable manner by using purely continuous formulations.      
### 52.A Computational Framework for Quantifying and Analyzing System Flexibility  [ :arrow_down: ](https://arxiv.org/pdf/2106.12706.pdf)
>  We present a computational framework for analyzing and quantifying system flexibility. Our framework incorporates new features that include: general uncertainty characterizations that are constructed using composition of sets, procedures for computing well-centered nominal points, and a procedure for identifying and ranking flexibility-limiting constraints and critical parameter values. These capabilities allow us to analyze the flexibility of complex systems such as distribution networks.      
### 53.A Mixed-Integer Conic Programming Formulation for Computing the Flexibility Index under Multivariate Gaussian Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2106.12702.pdf)
>  We present a methodology for computing the flexibility index when uncertainty is characterized using multivariate Gaussian random variables. Our approach computes the flexibility index by solving a mixed-integer conic program (MICP). This methodology directly characterizes ellipsoidal sets to capture correlations in contrast to previous methodologies that employ approximations. We also show that, under a Gaussian representation, the flexibility index can be used to obtain a lower bound for the so-called stochastic flexibility index (i.e., the probability of having feasible operation). Our results also show that the methodology can be generalized to capture different types of uncertainty sets.      
### 54.A Unifying Modeling Abstraction for Infinite-Dimensional Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2106.12689.pdf)
>  Infinite-dimensional optimization (InfiniteOpt) problems involve modeling components (variables, objectives, and constraints) that are functions defined over infinite-dimensional domains. Examples include continuous-time dynamic optimization (time is an infinite domain and components are a function of time), PDE optimization problems (space and time are infinite domains and components are a function of space-time), as well as stochastic and semi-infinite optimization (random space is an infinite domain and components are a function of such random space). InfiniteOpt problems also arise from combinations of these problem classes (e.g., stochastic PDE optimization). Given the infinite-dimensional nature of objectives and constraints, one often needs to define appropriate quantities (measures) to properly pose the problem. Moreover, InfiniteOpt problems often need to be transformed into a finite dimensional representation so that they can be solved numerically. In this work, we present a unifying abstraction that facilitates the modeling, analysis, and solution of InfiniteOpt problems. The proposed abstraction enables a general treatment of infinite-dimensional domains and provides a measure-centric paradigm to handle associated variables, objectives, and constraints. This abstraction allows us to transfer techniques across disciplines and with this identify new, interesting, and useful modeling paradigms (e.g., event constraints and risk measures defined over time domains). Our abstraction serves as the backbone of an intuitive Julia-based modeling package that we call InfiniteOpt.jl. We demonstrate the developments using diverse case studies arising in engineering.      
### 55.Conditional Deformable Image Registration with Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2106.12673.pdf)
>  Recent deep learning-based methods have shown promising results and runtime advantages in deformable image registration. However, analyzing the effects of hyperparameters and searching for optimal regularization parameters prove to be too prohibitive in deep learning-based methods. This is because it involves training a substantial number of separate models with distinct hyperparameter values. In this paper, we propose a conditional image registration method and a new self-supervised learning paradigm for deep deformable image registration. By learning the conditional features that correlated with the regularization hyperparameter, we demonstrate that optimal solutions with arbitrary hyperparameters can be captured by a single deep convolutional neural network. In addition, the smoothness of the resulting deformation field can be manipulated with arbitrary strength of smoothness regularization during inference. Extensive experiments on a large-scale brain MRI dataset show that our proposed method enables the precise control of the smoothness of the deformation field without sacrificing the runtime advantage or registration accuracy.      
### 56.Florida Wildlife Camera Trap Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2106.12628.pdf)
>  Trail camera imagery has increasingly gained popularity amongst biologists for conservation and ecological research. Minimal human interference required to operate camera traps allows capturing unbiased species activities. Several studies - based on human and wildlife interactions, migratory patterns of various species, risk of extinction in endangered populations - are limited by the lack of rich data and the time-consuming nature of manually annotating trail camera imagery. We introduce a challenging wildlife camera trap classification dataset collected from two different locations in Southwestern Florida, consisting of 104,495 images featuring visually similar species, varying illumination conditions, skewed class distribution, and including samples of endangered species, i.e. Florida panthers. Experimental evaluations with ResNet-50 architecture indicate that this image classification-based dataset can further push the advancements in wildlife statistical modeling. We will make the dataset publicly available.      
### 57.Dealing with training and test segmentation mismatch: FBK@IWSLT2021  [ :arrow_down: ](https://arxiv.org/pdf/2106.12607.pdf)
>  This paper describes FBK's system submission to the IWSLT 2021 Offline Speech Translation task. We participated with a direct model, which is a Transformer-based architecture trained to translate English speech audio data into German texts. The training pipeline is characterized by knowledge distillation and a two-step fine-tuning procedure. Both knowledge distillation and the first fine-tuning step are carried out on manually segmented real and synthetic data, the latter being generated with an MT system trained on the available corpora. Differently, the second fine-tuning step is carried out on a random segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce the performance drops occurring when a speech translation model trained on manually segmented data (i.e. an ideal, sentence-like segmentation) is evaluated on automatically segmented audio (i.e. actual, more realistic testing conditions). For the same purpose, a custom hybrid segmentation procedure that accounts for both audio content (pauses) and for the length of the produced segments is applied to the test data before passing them to the system. At inference time, we compared this procedure with a baseline segmentation method based on Voice Activity Detection (VAD). Our results indicate the effectiveness of the proposed hybrid approach, shown by a reduction of the gap with manual segmentation from 8.3 to 1.4 BLEU points.      
