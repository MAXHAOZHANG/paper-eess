# ArXiv eess --Thu, 10 Jun 2021
### 1.Implicit field learning for unsupervised anomaly detection in medical images  [ :arrow_down: ](https://arxiv.org/pdf/2106.05214.pdf)
>  We propose a novel unsupervised out-of-distribution detection method for medical images based on implicit fields image representations. In our approach, an auto-decoder feed-forward neural network learns the distribution of healthy images in the form of a mapping between spatial coordinates and probabilities over a proxy for tissue types. At inference time, the learnt distribution is used to retrieve, from a given test image, a restoration, i.e. an image maximally consistent with the input one but belonging to the healthy distribution. Anomalies are localized using the voxel-wise probability predicted by our model for the restored image. We tested our approach in the task of unsupervised localization of gliomas on brain MR images and compared it to several other VAE-based anomaly detection methods. Results show that the proposed technique substantially outperforms them (average DICE 0.640 vs 0.518 for the best performing VAE-based alternative) while also requiring considerably less computing time.      
### 2.Rethink Transfer Learning in Medical Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2106.05152.pdf)
>  Transfer learning (TL) with deep convolutional neural networks (DCNNs) has proved successful in medical image classification (MIC). However, the current practice is puzzling, as MIC typically relies only on low- and/or mid-level features that are learned in the bottom layers of DCNNs. Following this intuition, we question the current strategies of TL in MIC. In this paper, we perform careful experimental comparisons between shallow and deep networks for classification on two chest x-ray datasets, using different TL strategies. We find that deep models are not always favorable, and finetuning truncated deep models almost always yields the best performance, especially in data-poor regimes. <br>Project webpage: <a class="link-external link-https" href="https://github.com/sun-umn/Transfer-Learning-in-Medical-Imaging" rel="external noopener nofollow">this https URL</a> <br>Keywords: Transfer learning, Medical image classification, Feature hierarchy, Medical imaging, Evaluation metrics, Imbalanced data      
### 3.Time-Frequency Phase Retrieval for Audio -- The Effect of Transform Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2106.05148.pdf)
>  In audio processing applications, phase retrieval (PR) is often performed from the magnitude of short-time Fourier transform (STFT) coefficients. Although PR performance has been observed to depend on the considered STFT parameters and audio data, the extent of this dependence has not been systematically evaluated yet. To address this, we studied the performance of three PR algorithms for various types of audio content and various STFT parameters such as redundancy, time-frequency ratio, and the type of window. The quality of PR was studied in terms of objective difference grade and signal-to-noise ratio of the STFT magnitude, to provide auditory- and signal-based quality assessments. Our results show that PR quality improved with increasing redundancy, with a strong relevance of the time-frequency ratio. The effect of the audio content was smaller but still observable. The effect of the window was only significant for one of the PR algorithms. Interestingly, for a good PR quality, each of the three algorithms required a different set of parameters, demonstrating the relevance of individual parameter sets for a fair comparison across PR algorithms. Based on these results, we developed guidelines for optimizing STFT parameters for a given application.      
### 4.A multi-stage GAN for multi-organ chest X-ray image generation and segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2106.05132.pdf)
>  Multi-organ segmentation of X-ray images is of fundamental importance for computer aided diagnosis systems. However, the most advanced semantic segmentation methods rely on deep learning and require a huge amount of labeled images, which are rarely available due to both the high cost of human resources and the time required for labeling. In this paper, we present a novel multi-stage generation algorithm based on Generative Adversarial Networks (GANs) that can produce synthetic images along with their semantic labels and can be used for data augmentation. The main feature of the method is that, unlike other approaches, generation occurs in several stages, which simplifies the procedure and allows it to be used on very small datasets. The method has been evaluated on the segmentation of chest radiographic images, showing promising results. The multistage approach achieves state-of-the-art and, when very few images are used to train the GANs, outperforms the corresponding single-stage approach.      
### 5.Discrete-Time Accuracy Analysis of the Time-Domain Regular Perturbation Model for Unamplified Links  [ :arrow_down: ](https://arxiv.org/pdf/2106.05088.pdf)
>  The accuracy of a discrete-time channel model based on regular perturbation is numerically studied for unamplified links. We analyse the distance between discrete nonlinear interference points and show that such distance can be used to estimate the effective channel memory.      
### 6.Spatio-Temporal Dual-Stream Neural Network for Sequential Whole-Body PET Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2106.04961.pdf)
>  Sequential whole-body 18F-Fluorodeoxyglucose (FDG) positron emission tomography (PET) scans are regarded as the imaging modality of choice for the assessment of treatment response in the lymphomas because they detect treatment response when there may not be changes on anatomical imaging. Any computerized analysis of lymphomas in whole-body PET requires automatic segmentation of the studies so that sites of disease can be quantitatively monitored over time. State-of-the-art PET image segmentation methods are based on convolutional neural networks (CNNs) given their ability to leverage annotated datasets to derive high-level features about the disease process. Such methods, however, focus on PET images from a single time-point and discard information from other scans or are targeted towards specific organs and cannot cater for the multiple structures in whole-body PET images. In this study, we propose a spatio-temporal 'dual-stream' neural network (ST-DSNN) to segment sequential whole-body PET scans. Our ST-DSNN learns and accumulates image features from the PET images done over time. The accumulated image features are used to enhance the organs / structures that are consistent over time to allow easier identification of sites of active lymphoma. Our results show that our method outperforms the state-of-the-art PET image segmentation methods.      
### 7.Over-the-fiber Digital Predistortion Using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.04934.pdf)
>  We demonstrate, for the first time, experimental over-the-fiber training of transmitter neural networks (NNs) using reinforcement learning. Optical back-to-back training of a novel NN-based digital predistorter outperforms arcsine-based predistortion with up to 60\% bit-error-rate reduction.      
### 8.Continuous-discrete multiple target tracking with out-of-sequence measurements  [ :arrow_down: ](https://arxiv.org/pdf/2106.04898.pdf)
>  This paper derives the optimal Bayesian processing of an out-of-sequence (OOS) set of measurements in continuous-time for multiple target tracking. We consider a multi-target system modelled in continuous time that is discretised at the time steps when we receive the measurements, which are distributed according to the standard point target model. All information about this system at the sampled time steps is provided by the posterior density on the set of all trajectories. This density can be computed via the continuous-discrete trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. When we receive an OOS measurement, the optimal Bayesian processing performs a retrodiction step that adds trajectory information at the OOS measurement time stamp followed by an update step. After the OOS measurement update, the posterior remains in TPMBM form. We also provide a computationally lighter alternative based on a trajectory Poisson multi-Bernoulli filter. The effectiveness of the two approaches to handle OOS measurements is evaluated via simulations.      
### 9.Deep Interaction between Masking and Mapping Targets for Single-Channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2106.04878.pdf)
>  The most recent deep neural network (DNN) models exhibit impressive denoising performance in the time-frequency (T-F) magnitude domain. However, the phase is also a critical component of the speech signal that is easily overlooked. In this paper, we propose a multi-branch dilated convolutional network (DCN) to simultaneously enhance the magnitude and phase of noisy speech. A causal and robust monaural speech enhancement system is achieved based on the multi-objective learning framework of the complex spectrum and the ideal ratio mask (IRM) targets. In the process of joint learning, the intermediate estimation of IRM targets is used as a way of generating feature attention factors to realize the information interaction between the two targets. Moreover, the proposed multi-scale dilated convolution enables the DCN model to have a more efficient temporal modeling capability. Experimental results show that compared with other state-of-the-art models, this model achieves better speech quality and intelligibility with less computation.      
### 10.Fast Computational Ghost Imaging using Unpaired Deep Learning and a Constrained Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2106.04822.pdf)
>  The unpaired training can be the only option available for fast deep learning-based ghost imaging, where obtaining a high signal-to-noise ratio (SNR) image copy of each low SNR ghost image could be practically time-consuming and challenging. This paper explores the capabilities of deep learning to leverage computational ghost imaging when there is a lack of paired training images. The deep learning approach proposed here enables fast ghost imaging through reconstruction of high SNR images from faint and hastily shot ghost images using a constrained Wasserstein generative adversarial network. In the proposed approach, the objective function is regularized to enforce the generation of faithful and relevant high SNR images to the ghost copies. This regularization measures the distance between reconstructed images and the faint ghost images in a low-noise manifold generated by a shadow network. The performance of the constrained network is shown to be particularly important for ghost images with low SNR. The proposed pipeline is able to reconstruct high-quality images from the ghost images with SNR values not necessarily equal to the SNR of the training set.      
### 11.Detecting and Correcting IMU Movements During Joint Angle Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2106.04817.pdf)
>  Inertial measurement units (IMUs) increasingly function as a basic component of wearable sensor network (WSN)systems. IMU-based joint angle estimation (JAE) is a relatively typical usage of IMUs, with extensive applications. However, the issue that IMUs move with respect to their original placement during JAE is still a research gap, and limits the robustness of deploying the technique in real-world application scenarios. In this study, we propose to detect and correct the IMU movement online in a relatively computationally lightweight manner. Particularly, we first experimentally investigate the influence of IMU movements. Second, we design the metrics for detecting IMU movements by mathematically formulating how the IMU movement affects the IMU measurements. Third, we determine the optimal thresholds of metrics by synthetic IMU data from a significantly amended simulation model. Finally, a correction method is proposed to correct the effects of IMU movements. We demonstrate our method on both synthetic data and real-user data. The results demonstrate our method is a promising solution to detecting and correcting IMU movements during JAE.      
### 12.Deep learning based low-dose synchrotron radiation CT reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2106.04792.pdf)
>  Synchrotron radiation sources are widely used in various fields, among which computed tomography (CT) is one of the most important. The amount of effort expended by the operator varies depending on the subject. If the number of angles needed to be used can be greatly reduced under the condition of similar imaging effects, the working time and workload of the experimentalists will be greatly reduced. However, decreasing the sampling angle can produce serious artifacts and blur the details. We try to use a deep learning model which can build high quality reconstruction sparse data sampling from the angle of the image and ResAttUnet are put forward. ResAttUnet is roughly a symmetrical U-shaped network that incorporates similar mechanisms to ResNet and attention. In addition, the mixed precision is adopted to reduce the demand for video memory of the model and training time.      
### 13.Semi-Supervised Training with Pseudo-Labeling for End-to-End Neural Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2106.04764.pdf)
>  In this paper, we present a semi-supervised training technique using pseudo-labeling for end-to-end neural diarization (EEND). The EEND system has shown promising performance compared with traditional clustering-based methods, especially in the case of overlapping speech. However, to get a well-tuned model, EEND requires labeled data for all the joint speech activities of every speaker at each time frame in a recording. In this paper, we explore a pseudo-labeling approach that employs unlabeled data. First, we propose an iterative pseudo-label method for EEND, which trains the model using unlabeled data of a target condition. Then, we also propose a committee-based training method to improve the performance of EEND. To evaluate our proposed method, we conduct the experiments of model adaptation using labeled and unlabeled data. Experimental results on the CALLHOME dataset show that our proposed pseudo-label achieved a 37.4% relative diarization error rate reduction compared to a seed model. Moreover, we analyzed the results of semi-supervised adaptation with pseudo-labeling. We also show the effectiveness of our approach on the third DIHARD dataset.      
### 14.Modeling and Analysis of Switched-Capacitor Converters as a Multi-port Network for Covert Communication  [ :arrow_down: ](https://arxiv.org/pdf/2106.04761.pdf)
>  Switched-capacitor (SC) DC-DC voltage converters are widely used in power delivery and management of modern integrated circuits. Connected to a common supply voltage, SC converters exhibit cross-regulation/coupling effects among loads connected to different SC converter stages due to the shared components such as switches, capacitors, and parasitic elements. The coupling effects between SC converter stages can potentially be used in covert communication, where two or more entities (e.g., loads) illegitimately establish a communication channel to exchange malicious information stealthily. To qualitatively analyze the coupling effects, a novel modeling technique is proposed based on the multi-port network theory. The fast and slow switching limit (FSL and SSL) equivalent resistance concepts are used to analytically determine the impact of each design parameter such as switch resistance, flying capacitance, switching frequency, and parasitic resistance. A three-stage 2:1 SC converter supplying three different loads is considered as a case study to verify the proposed modeling technique.      
### 15.Job Dispatching Policies for Queueing Systems with Unknown Service Rates  [ :arrow_down: ](https://arxiv.org/pdf/2106.04707.pdf)
>  In multi-server queueing systems where there is no central queue holding all incoming jobs, job dispatching policies are used to assign incoming jobs to the queue at one of the servers. Classic job dispatching policies such as join-the-shortest-queue and shortest expected delay assume that the service rates and queue lengths of the servers are known to the dispatcher. In this work, we tackle the problem of job dispatching without the knowledge of service rates and queue lengths, where the dispatcher can only obtain noisy estimates of the service rates by observing job departures. This problem presents a novel exploration-exploitation trade-off between sending jobs to all the servers to estimate their service rates, and exploiting the currently known fastest servers to minimize the expected queueing delay. We propose a bandit-based exploration policy that learns the service rates from observed job departures. Unlike the standard multi-armed bandit problem where only one out of a finite set of actions is optimal, here the optimal policy requires identifying the optimal fraction of incoming jobs to be sent to each server. We present a regret analysis and simulations to demonstrate the effectiveness of the proposed bandit-based exploration policy.      
### 16.Towards Scalable Uncertainty Aware DNN-based Wireless Localisation  [ :arrow_down: ](https://arxiv.org/pdf/2106.04697.pdf)
>  Existing deep neural network (DNN) based wireless localization approaches typically do not capture uncertainty inherent in their estimates. In this work, we propose and evaluate variational and scalable DNN approaches to measure the uncertainty as a result of changing propagation conditions and the finite number of training samples. Furthermore, we show that data uncertainty is sufficient to capture the uncertainty due to non-line-of-sight (NLOS) and, model uncertainty improves the overall reliability. To assess the robustness due to channel conditions and out-of-set regions, we evaluate the methods on challenging massive multiple-input multiple-output (MIMO) scenarios.      
### 17.Exponentially-Weighted Energy Dispersion Index for the Nonlinear Interference Analysis of Finite-Blocklength Shaping  [ :arrow_down: ](https://arxiv.org/pdf/2106.04694.pdf)
>  A metric called exponentially-weighted energy dispersion index (EEDI) is proposed to explain the blocklength-dependent effective signal-to-noise ratio (SNR) in probabilistically shaped fiber-optic systems. EEDI is better than energy dispersion index (EDI) at capturing the dependency of the effective SNR on the blocklength for long-distance transmission.      
### 18.Mitigating Current Variation in Particle Beam Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2106.04686.pdf)
>  Particle beam microscopy uses a scanning beam of charged particles to create images of samples, and the quality of image reconstruction suffers when this beam current varies over time. Neither conventional reconstruction methods nor time-resolved sensing acknowledges beam current variation, although through sensitivity analysis, my project demonstrates that when the beam current variation is appreciable, time-resolved sensing has significant improvement compared to conventional methods in terms of image reconstruction quality, specifically mean-squared error (MSE). To more actively combat this unknown varying beam current's effects, my project further focuses on designing an algorithm that uses time-resolved sensing for even better image reconstruction quality in the presence of beam current variation. This algorithm works by simultaneously estimating the unknown beam current variation in addition to the underlying image, offering an alternative to more conventional methods, which exploit statistical assumptions of the image content without explicitly estimating the beam current. Using a concept of excess MSE due to beam current variation, this algorithm provides a factor of 7 improvement on average, which could lead to less expensive equipment in the future. Beyond improving the image estimation, this algorithm offers a novel estimation of the beam current, potentially providing more control in manufacturing and fabrication processes.      
### 19.TED-net: Convolution-free T2T Vision Transformer-based Encoder-decoder Dilation network for Low-dose CT Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2106.04650.pdf)
>  Low dose computed tomography is a mainstream for clinical applications. How-ever, compared to normal dose CT, in the low dose CT (LDCT) images, there are stronger noise and more artifacts which are obstacles for practical applications. In the last few years, convolution-based end-to-end deep learning methods have been widely used for LDCT image denoising. Recently, transformer has shown superior performance over convolution with more feature interactions. Yet its ap-plications in LDCT denoising have not been fully cultivated. Here, we propose a convolution-free T2T vision transformer-based Encoder-decoder Dilation net-work (TED-net) to enrich the family of LDCT denoising algorithms. The model is free of convolution blocks and consists of a symmetric encoder-decoder block with sole transformer. Our model is evaluated on the AAPM-Mayo clinic LDCT Grand Challenge dataset, and results show outperformance over the state-of-the-art denoising methods.      
### 20.SpeechBrain: A General-Purpose Speech Toolkit  [ :arrow_down: ](https://arxiv.org/pdf/2106.04624.pdf)
>  SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.      
### 21.Phase retrieval with physics informed zero-shot learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.04577.pdf)
>  Phase can be reliably estimated from a single diffracted intensity image, if a faithful prior information about the object is available. Examples include amplitude bounds, object support, sparsity in the spatial or a transform domain, deep image prior and the prior learnt from the labelled datasets by a deep neural network. Deep learning facilitates state of art reconstruction quality but requires a large labelled dataset (ground truth-measurement pair acquired in the same experimental conditions) for training. To alleviate this data requirement problem, this letter proposes a zero-shot learning method. The letter demonstrates that the object-prior learnt by a deep neural network while being trained for a denoising task can also be utilized for the phase retrieval, if the diffraction physics is effectively enforced on the network output. The letter additionally demonstrates that the incorporation of total variation in the proposed zero-shot framework facilitates the reconstruction of similar quality in lesser time (e.g. ~8.5 fold, for a test reported in this letter).      
### 22.Efficient input placement for the optimal control of network moments  [ :arrow_down: ](https://arxiv.org/pdf/2106.05265.pdf)
>  In this paper, we study the optimal control of the mean and variance of the network state vector. We develop an algorithm to optimize the control input placement subject to constraints on the state, which must be achieved at a given time threshold; seeking an input placement which moves the moment at minimum cost. First, we solve the state-selection problem for a number of variants of the first and second moment, and find solutions related to the eigenvalues of the systems' Gramian matrices. Our algorithm then uses this information to find a locally optimal input placement. This is a Generalization of the Projected Gradient Method (GPGM). We solve the problem for some common versions of these moments, including the mean state and versions of the second moment which induce discord, repel from a certain state, or encourage convergence. We then perform simulations, and discuss a measure of centrality based on the system flux -- a measure which describes what nodes are most important to optimal control of the average state.      
### 23.Intermittent Speech Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2106.05229.pdf)
>  A large number of Internet of Things (IoT) devices today are powered by batteries, which are often expensive to maintain and may cause serious environmental pollution. To avoid these problems, researchers have begun to consider the use of energy systems based on energy-harvesting units for such devices. However, the power harvested from an ambient source is fundamentally small and unstable, resulting in frequent power failures during the operation of IoT applications involving, for example, intermittent speech signals and the streaming of videos. This paper presents a deep-learning-based speech recovery system that reconstructs intermittent speech signals from self-powered IoT devices. Our intermittent speech recovery system (ISR) consists of three stages: interpolation, recovery, and combination. The experimental results show that our recovery system increases speech quality by up to 707.1%, while increasing speech intelligibility by up to 92.1%. Most importantly, our ISR system also enhances the WER scores by up to 65.6%. To the best of our knowledge, this study is one of the first to reconstruct intermittent speech signals from self-powered-sensing IoT devices. These promising results suggest that even though self powered microphone devices function with weak energy sources, our ISR system can still maintain the performance of most speech-signal-based applications.      
### 24.A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2106.05111.pdf)
>  End-to-end (E2E) modeling is advantageous for automatic speech recognition (ASR) especially for Japanese since word-based tokenization of Japanese is not trivial, and E2E modeling is able to model character sequences directly. This paper focuses on the latest E2E modeling techniques, and investigates their performances on character-based Japanese ASR by conducting comparative experiments. The results are analyzed and discussed in order to understand the relative advantages of long short-term memory (LSTM), and Conformer models in combination with connectionist temporal classification, transducer, and attention-based loss functions. Furthermore, the paper investigates on effectivity of the recent training techniques such as data augmentation (SpecAugment), variational noise injection, and exponential moving average. The best configuration found in the paper achieved the state-of-the-art character error rates of 4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ) eval1, eval2, and eval3 tasks, respectively. The system is also shown to be computationally efficient thanks to the efficiency of Conformer transducers.      
### 25.Gaussian Mixture Estimation from Weighted Samples  [ :arrow_down: ](https://arxiv.org/pdf/2106.05109.pdf)
>  We consider estimating the parameters of a Gaussian mixture density with a given number of components best representing a given set of weighted samples. We adopt a density interpretation of the samples by viewing them as a discrete Dirac mixture density over a continuous domain with weighted components. Hence, Gaussian mixture fitting is viewed as density re-approximation. In order to speed up computation, an expectation-maximization method is proposed that properly considers not only the sample locations, but also the corresponding weights. It is shown that methods from literature do not treat the weights correctly, resulting in wrong estimates. This is demonstrated with simple counterexamples. The proposed method works in any number of dimensions with the same computational load as standard Gaussian mixture estimators for unweighted samples.      
### 26.Multiple simultaneous solution representations in a population based evolutionary algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2106.05096.pdf)
>  The representation used for solutions in optimization can have a significant impact on the performance of the optimization method. Traditional population based evolutionary methods have homogeneous populations where all solutions use the same representation. If different representations are to be considered, different runs are required to investigate the relative performance. In this paper, we illustrate the use of a population based evolutionary method, Fresa, inspired by the propagation of Strawberry plants, which allows for multiple representations to co-exist in the population. <br>Fresa is implemented in the Julia language. Julia provides dynamic typing and multiple dispatch. In multiple dispatch, the function invoked is determined, dynamically at run time, by the types of the arguments passed to it. This enables a generic implementation of key steps in the plant propagation algorithm which allows for a heterogeneous population. The search procedure then leads to a competition between representations automatically. <br>A simple case study from the design of operating conditions for a batch reactor system is used to illustrate heterogeneous population based search.      
### 27.Agile wide-field imaging with selective high resolution  [ :arrow_down: ](https://arxiv.org/pdf/2106.05082.pdf)
>  Wide-field and high-resolution (HR) imaging is essential for various applications such as aviation reconnaissance, topographic mapping and safety monitoring. The existing techniques require a large-scale detector array to capture HR images of the whole field, resulting in high complexity and heavy cost. In this work, we report an agile wide-field imaging framework with selective high resolution that requires only two detectors. It builds on the statistical sparsity prior of natural scenes that the important targets locate only at small regions of interests (ROI), instead of the whole field. Under this assumption, we use a short-focal camera to image wide field with a certain low resolution, and use a long-focal camera to acquire the HR images of ROI. To automatically locate ROI in the wide field in real time, we propose an efficient deep-learning based multiscale registration method that is robust and blind to the large setting differences (focal, white balance, etc) between the two cameras. Using the registered location, the long-focal camera mounted on a gimbal enables real-time tracking of the ROI for continuous HR imaging. We demonstrated the novel imaging framework by building a proof-of-concept setup with only 1181 gram weight, and assembled it on an unmanned aerial vehicle for air-to-ground monitoring. Experiments show that the setup maintains 120$^{\circ}$ wide field-of-view (FOV) with selective 0.45$mrad$ instantaneous FOV.      
### 28.Towards a Framework for Nonlinear Predictive Control using Derivative-Free Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2106.05025.pdf)
>  The use of derivative-based solvers to compute solutions to optimal control problems with non-differentiable cost or dynamics often requires reformulations or relaxations that complicate the implementation or increase computational complexity. We present an initial framework for using the derivative-free Mesh Adaptive Direct Search (MADS) algorithm to solve Nonlinear Model Predictive Control problems with non-differentiable features without the need for reformulation. The MADS algorithm performs a structured search of the input space by simulating selected system trajectories and computing the subsequent cost value. We propose handling the path constraints and the Lagrange cost term by augmenting the system dynamics with additional states to compute the violation and cost value alongside the state trajectories, eliminating the need for reconstructing the state trajectories in a separate phase. We demonstrate the practicality of this framework by solving a robust rocket control problem, where the objective is to reach a target altitude as close as possible, given a system with uncertain parameters. This example uses a non-differentiable cost function and simulates two different system trajectories simultaneously, with each system having its own free final time.      
### 29.Satellite- and Cache-assisted UAV: A Joint Cache Placement, Resource Allocation, and Trajectory Optimization for 6G Aerial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.05016.pdf)
>  This paper considers LEO satellite- and cache-assisted UAV communications for content delivery in terrestrial networks, which shows great potential for next-generation systems to provide ubiquitous connectivity and high capacity. Specifically, caching is provided by the UAV to reduce backhaul congestion, and the LEO satellite supports the UAV's backhaul link. In this context, we aim to maximize the minimum achievable throughput per ground user (GU) by jointly optimizing cache placement, the UAV's resource allocation, and trajectory while cache capacity and flight time are limited. The formulated problem is challenging to solve directly due to its non-convexity and combinatorial nature. To find a solution, the problem is decomposed into three sub-problems: (1) cache placement optimization with fixed UAV resources and trajectory, followed by (2) the UAV resources optimization with fixed cache placement vector and trajectory, and finally, (3) we optimize the UAV trajectory with fixed cache placement and UAV resources. Based on the solutions of sub-problems, an efficient alternating algorithm is proposed utilizing the block coordinate descent (BCD) and successive convex approximation (SCA) methods. Simulation results show that the max-min throughput and total achievable throughput enhancement can be achieved by applying our proposed algorithm instead of other benchmark schemes.      
### 30.Optimal Inspection of Network Systems via Value of Information Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2106.04988.pdf)
>  This paper develops computable metrics to assign priorities for information collection on network systems made up by binary components. Components are worth inspecting because their condition state is uncertain and the system functioning depends on it. The Value of Information (VoI) allows assessing the impact of information in decision making under uncertainty, including the precision of the observation, the available actions and the expected economic loss. Some VoI-based metrics for system-level and component-level maintenance actions, defined as "global" and "local" metrics, respectively, are introduced, analyzed and applied to series and parallel systems. Their computationally complexity of applications to general networks is discussed and, to tame the complexity for the local metric assessment, a heuristic is presented and its performance is compared on some case studies.      
### 31.Information Avoidance and Overvaluation in Sequential Decision Making under Epistemic Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2106.04984.pdf)
>  Decision makers involved in the management of civil assets and systems usually take actions under constraints imposed by societal regulations. Some of these constraints are related to epistemic quantities, as the probability of failure events and the corresponding risks. Sensors and inspectors can provide useful information supporting the control process (e.g. the maintenance process of an asset), and decisions about collecting this information should rely on an analysis of its cost and value. When societal regulations encode an economic perspective that is not aligned with that of the decision makers, the Value of Information (VoI) can be negative (i.e., information sometimes hurts), and almost irrelevant information can even have a significant value (either positive or negative), for agents acting under these epistemic constraints. We refer to these phenomena as Information Avoidance (IA) and Information OverValuation (IOV). In this paper, we illustrate how to assess VoI in sequential decision making under epistemic constraints (as those imposed by societal regulations), by modeling a Partially Observable Markov Decision Processes (POMDP) and evaluating non optimal policies via Finite State Controllers (FSCs). We focus on the value of collecting information at current time, and on that of collecting sequential information, we illustrate how these values are related and we discuss how IA and IOV can occur in those settings.      
### 32.Unsupervised Automatic Speech Recognition: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2106.04897.pdf)
>  Automatic Speech Recognition (ASR) systems can be trained to achieve remarkable performance given large amounts of manually transcribed speech, but large labeled data sets can be difficult or expensive to acquire for all languages of interest. In this paper, we review the research literature to identify models and ideas that could lead to fully unsupervised ASR, including unsupervised segmentation of the speech signal, unsupervised mapping from speech segments to text, and semi-supervised models with nominal amounts of labeled examples. The objective of the study is to identify the limitations of what can be learned from speech data alone and to understand the minimum requirements for speech recognition. Identifying these limitations would help optimize the resources and efforts in ASR development for low-resource languages.      
### 33.RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2106.04833.pdf)
>  End-to-end simultaneous speech translation (SST), which directly translates speech in one language into text in another language in real-time, is useful in many scenarios but has not been fully investigated. In this work, we propose RealTranS, an end-to-end model for SST. To bridge the modality gap between speech and text, RealTranS gradually downsamples the input speech with interleaved convolution and unidirectional Transformer layers for acoustic modeling, and then maps speech features into text space with a weighted-shrinking operation and a semantic encoder. Besides, to improve the model performance in simultaneous scenarios, we propose a blank penalty to enhance the shrinking quality and a Wait-K-Stride-N strategy to allow local reranking during decoding. Experiments on public and widely-used datasets show that RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end models as well as cascaded models in diverse latency settings.      
### 34.Temporal Averaging LSTM-based Channel Estimation Scheme for IEEE 802.11p Standard  [ :arrow_down: ](https://arxiv.org/pdf/2106.04829.pdf)
>  In vehicular communications, reliable channel estimation is critical for the system performance due to the doubly-dispersive nature of vehicular channels. IEEE 802.11p standard allocates insufficient pilots for accurate channel tracking. Consequently, conventional IEEE 802.11p estimators suffer from a considerable performance degradation, especially in high mobility scenarios. Recently, deep learning (DL) techniques have been employed for IEEE 802.11p channel estimation. Nevertheless, these methods suffer either from performance degradation in very high mobility scenarios or from large computational complexity. In this paper, these limitations are solved using a long short term memory (LSTM)-based estimation. The proposed estimator employs an LSTM unit to estimate the channel, followed by temporal averaging (TA) processing as a noise alleviation technique. Moreover, the noise mitigation ratio is determined analytically, thus validating the TA processing ability in improving the overall performance. Simulation results reveal the performance superiority of the proposed schemes compared to recently proposed DL-based estimators, while recording a significant reduction in the computational complexity.      
### 35.Fractional order magnetic resonance fingerprinting in the human cerebral cortex  [ :arrow_down: ](https://arxiv.org/pdf/2106.04816.pdf)
>  Mathematical models are becoming increasingly important in magnetic resonance imaging (MRI), as they provide a mechanistic approach for making a link between tissue microstructure and signals acquired using the medical imaging instrument. The Bloch equations, which describes spin and relaxation in a magnetic field, is a set of integer order differential equations with a solution exhibiting mono-exponential behaviour in time. Parameters of the model may be estimated using a non-linear solver, or by creating a dictionary of model parameters from which MRI signals are simulated and then matched with experiment. We have previously shown the potential efficacy of a magnetic resonance fingerprinting (MRF) approach, i.e. dictionary matching based on the classical Bloch equations, for parcellating the human cerebral cortex. However, this classical model is unable to describe in full the mm-scale MRI signal generated based on an heterogenous and complex tissue micro-environment. The time-fractional order Bloch equations has been shown to provide, as a function of time, a good fit of brain MRI signals. We replaced the integer order Bloch equations with the previously reported time-fractional counterpart within the MRF framework and performed experiments to parcellate human gray matter, which is cortical brain tissue with different cyto-architecture at different spatial locations. Our findings suggest that the time-fractional order parameters, {\alpha} and {\beta}, potentially associate with the effect of interareal architectonic variability, hypothetically leading to more accurate cortical parcellation.      
### 36.Phase Retrieval using Single-Instance Deep Generative Prior  [ :arrow_down: ](https://arxiv.org/pdf/2106.04812.pdf)
>  Several deep learning methods for phase retrieval exist, but most of them fail on realistic data without precise support information. We propose a novel method based on single-instance deep generative prior that works well on complex-valued crystal data.      
### 37.Online Optimization in Games via Control Theory: Connecting Regret, Passivity and Poincaré Recurrence  [ :arrow_down: ](https://arxiv.org/pdf/2106.04748.pdf)
>  We present a novel control-theoretic understanding of online optimization and learning in games, via the notion of passivity. Passivity is a fundamental concept in control theory, which abstracts energy conservation and dissipation in physical systems. It has become a standard tool in analysis of general feedback systems, to which game dynamics belong. Our starting point is to show that all continuous-time Follow-the-Regularized-Leader (FTRL) dynamics, which includes the well-known Replicator Dynamic, are lossless, i.e. it is passive with no energy dissipation. Interestingly, we prove that passivity implies bounded regret, connecting two fundamental primitives of control theory and online optimization. <br>The observation of energy conservation in FTRL inspires us to present a family of lossless learning dynamics, each of which has an underlying energy function with a simple gradient structure. This family is closed under convex combination; as an immediate corollary, any convex combination of FTRL dynamics is lossless and thus has bounded regret. This allows us to extend the framework of Fox and Shamma (Games, 2013) to prove not just global asymptotic stability results for game dynamics, but Poincaré recurrence results as well. Intuitively, when a lossless game (e.g. graphical constant-sum game) is coupled with lossless learning dynamic, their interconnection is also lossless, which results in a pendulum-like energy-preserving recurrent behavior, generalizing the results of Piliouras and Shamma (SODA, 2014) and Mertikopoulos, Papadimitriou and Piliouras (SODA, 2018).      
### 38.Optical Networks for Composable Data Centers  [ :arrow_down: ](https://arxiv.org/pdf/2106.04738.pdf)
>  Composable data centers (DCs) have been proposed to enable greater efficiencies as the uptake of on-demand computing services grows. In this article we give an overview of composable DCs by discussing their enabling technologies, benefits, challenges, and research directions. We then describe a network for composable DCs that leverages optical communication technologies and components to implement a targeted design. Relative to the implementation of a generic design that requires a (high capacity) dedicated transceiver on each point-to-point link on a mesh optical fabric in a composable DC rack, the targeted design can significantly reduce capital expenditure (by up to 34 times) because fewer transceivers are used. This is achieved with little or no degradation of expected performance in composable DCs.      
### 39.Network Topologies for Composable Data Centres  [ :arrow_down: ](https://arxiv.org/pdf/2106.04699.pdf)
>  Suitable composable data center networks (DCNs) are essential to support the disaggregation of compute components in highly efficient next generation data centers (DCs). However, designing such composable DCNs can be challenging. A composable DCN that adopts a full mesh backplane between disaggregated compute components within a rack and employs dedicated interfaces on each point-to-point link is wasteful and expensive. In this paper, we propose and describe two (i.e., electrical, and electrical-optical) variants of a network for composable DC (NetCoD). NetCoD adopts a targeted design to reduce the number of transceivers required when a mesh physical backplane is deployed between disaggregated compute components in the same rack. The targeted design leverages optical communication techniques and components to achieve this with minimal or no network performance degradation. We formulate a MILP model to evaluate the performance of both variants of NetCoD in rack-scale composable DCs that implement different forms of disaggregation. The electrical-optical variant of NetCoD achieves similar performance as a reference network while utilizing fewer transceivers per compute node. The targeted adoption of optical technologies by both variants of NetCoD achieves greater (4 - 5 times greater) utilization of available network throughput than the reference network which implements a generic design. Under the various forms of disaggregation considered, both variant of NetCoD achieve near-optimal compute energy efficiency in the composable DC while satisfying both compute and network constraints. This is because marginal concession of optimal compute energy efficiency is often required to achieve overall optimal energy efficiency in composable DCs.      
### 40.Entropy of the Conditional Expectation under Gaussian Noise  [ :arrow_down: ](https://arxiv.org/pdf/2106.04677.pdf)
>  This paper considers an additive Gaussian noise channel with arbitrarily distributed finite variance input signals. It studies the differential entropy of the minimum mean-square error (MMSE) estimator and provides a new lower bound which connects the entropy of the input, output, and conditional mean. That is, the sum of entropies of the conditional mean and output is always greater than or equal to twice the input entropy. Various other properties such as upper bounds, asymptotics, Taylor series expansion, and connection to Fisher Information are obtained. An application of the lower bound in the remote-source coding problem is discussed, and extensions of the lower and upper bounds to the vector Gaussian channel are given.      
### 41.Sequential End-to-End Intent and Slot Label Classification and Localization  [ :arrow_down: ](https://arxiv.org/pdf/2106.04660.pdf)
>  Human-computer interaction (HCI) is significantly impacted by delayed responses from a spoken dialogue system. Hence, end-to-end (e2e) spoken language understanding (SLU) solutions have recently been proposed to decrease latency. Such approaches allow for the extraction of semantic information directly from the speech signal, thus bypassing the need for a transcript from an automatic speech recognition (ASR) system. In this paper, we propose a compact e2e SLU architecture for streaming scenarios, where chunks of the speech signal are processed continuously to predict intent and slot values. Our model is based on a 3D convolutional neural network (3D-CNN) and a unidirectional long short-term memory (LSTM). We compare the performance of two alignment-free losses: the connectionist temporal classification (CTC) method and its adapted version, namely connectionist temporal localization (CTL). The latter performs not only the classification but also localization of sequential audio events. The proposed solution is evaluated on the Fluent Speech Command dataset and results show our model ability to process incoming speech signal, reaching accuracy as high as 98.97 % for CTC and 98.78 % for CTL on single-label classification, and as high as 95.69 % for CTC and 95.28 % for CTL on two-label prediction.      
### 42.Probabilistic Neural Network to Quantify Uncertainty of Wind Power Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2106.04656.pdf)
>  Each year a growing number of wind farms are being added to power grids to generate electricity. The power curve of a wind turbine, which exhibits the relationship between generated power and wind speed, plays a major role in assessing the performance of a wind farm. Neural networks have been used for power curve estimation. However, they do not produce a confidence measure for their output, unless computationally prohibitive Bayesian methods are used. In this paper, a probabilistic neural network with Monte Carlo dropout is considered to quantify the model (epistemic) uncertainty of the power curve estimation. This approach offers a minimal increase in computational complexity over deterministic approaches. Furthermore, by incorporating a probabilistic loss function, the noise or aleatoric uncertainty in the data is estimated. The developed network captures both model and noise uncertainty which is found to be useful tools in assessing performance. Also, the developed network is compared with existing ones across a public domain dataset showing superior performance in terms of prediction accuracy.      
### 43.Optimising Hearing Aid Fittings for Speech in Noise with a Differentiable Hearing Loss Model  [ :arrow_down: ](https://arxiv.org/pdf/2106.04639.pdf)
>  Current hearing aids normally provide amplification based on a general prescriptive fitting, and the benefits provided by the hearing aids vary among different listening environments despite the inclusion of noise suppression feature. Motivated by this fact, this paper proposes a data-driven machine learning technique to develop hearing aid fittings that are customised to speech in different noisy environments. A differentiable hearing loss model is proposed and used to optimise fittings with back-propagation. The customisation is reflected on the data of speech in different noise with also the consideration of noise suppression. The objective evaluation shows the advantages of optimised custom fittings over general prescriptive fittings.      
