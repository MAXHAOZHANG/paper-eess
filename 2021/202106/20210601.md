# ArXiv eess --Tue, 1 Jun 2021
### 1.Automatic audiovisual synchronisation for ultrasound tongue imaging  [ :arrow_down: ](https://arxiv.org/pdf/2105.15162.pdf)
>  Ultrasound tongue imaging is used to visualise the intra-oral articulators during speech production. It is utilised in a range of applications, including speech and language therapy and phonetics research. Ultrasound and speech audio are recorded simultaneously, and in order to correctly use this data, the two modalities should be correctly synchronised. Synchronisation is achieved using specialised hardware at recording time, but this approach can fail in practice resulting in data of limited usability. In this paper, we address the problem of automatically synchronising ultrasound and audio after data collection. We first investigate the tolerance of expert ultrasound users to synchronisation errors in order to find the thresholds for error detection. We use these thresholds to define accuracy scoring boundaries for evaluating our system. We then describe our approach for automatic synchronisation, which is driven by a self-supervised neural network, exploiting the correlation between the two signals to synchronise them. We train our model on data from multiple domains with different speaker characteristics, different equipment, and different recording environments, and achieve an accuracy &gt;92.4% on held-out in-domain data. Finally, we introduce a novel resource, the Cleft dataset, which we gathered with a new clinical subgroup and for which hardware synchronisation proved unreliable. We apply our model to this out-of-domain data, and evaluate its performance subjectively with expert users. Results show that users prefer our model's output over the original hardware output 79.3% of the time. Our results demonstrate the strength of our approach and its ability to generalise to data from new domains.      
### 2.All-in-one: Certifiable Optimal Distributed Kalman Filter under Unknown Correlations  [ :arrow_down: ](https://arxiv.org/pdf/2105.15061.pdf)
>  The optimal fusion of estimates in a Distributed Kalman Filter (DKF) requires tracking of the complete network error covariance, which is a problem in terms of memory and communication bandwidth. A scalable alternative is to fuse estimates under unknown correlations, updating the local estimates and error covariance matrix as the solution of an optimisation problem. Unfortunately, this problem is NP-hard, forcing relaxations that lose optimality guarantees over the original problem. Motivated by this, we present the first Certifiable Optimal DKF (CO-DKF). Using only information from one-hop neighbours, CO-DKF solves the optimal fusion of estimates under unknown correlations by a tight Semidefinite Programming (SDP) relaxation. This particular relaxation allows to certify locally and in real time if the solution from the relaxed problem is the optimum of the original. In that case, we prove that CO-DKF is optimal in the Mean Square Error (MSE) sense. Additionally, we demonstrate that CO-DKF is a globally asymptotically stable estimator. Simulations show that CO-DKF outperforms other state-of-the-art DKF algorithms, specially in sparse, highly noisy setups.      
### 3.A Novel Automatic Modulation Classification Scheme Based on Multi-Scale Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.15037.pdf)
>  Automatic modulation classification enables intelligent communications and it is of crucial importance in today's and future wireless communication networks. Although many automatic modulation classification schemes have been proposed, they cannot tackle the intra-class diversity problem caused by the dynamic changes of the wireless communication environment. In order to overcome this problem, inspired by face recognition, a novel automatic modulation classification scheme is proposed by using the multi-scale network in this paper. Moreover, a novel loss function that combines the center loss and the cross entropy loss is exploited to learn both discriminative and separable features in order to further improve the classification performance. Extensive simulation results demonstrate that our proposed automatic modulation classification scheme can achieve better performance than the benchmark schemes in terms of the classification accuracy. The influence of the network parameters and the loss function with the two-stage training strategy on the classification accuracy of our proposed scheme are investigated.      
### 4.Feasibility Assessment of Multitasking in MRI Neuroimaging Analysis: Tissue Segmentation, Cross-Modality Conversion and Bias correction  [ :arrow_down: ](https://arxiv.org/pdf/2105.14986.pdf)
>  Neuroimaging is essential in brain studies for the diagnosis and identification of disease, structure, and function of the brain in its healthy and disease states. Literature shows that there are advantages of multitasking with some deep learning (DL) schemes in challenging neuroimaging applications. This study examines the feasibility of using multitasking in three different applications, including tissue segmentation, cross-modality conversion, and bias-field correction. These applications reflect five different scenarios in which multitasking is explored and 280 training and testing sessions conducted for empirical evaluations. Two well-known networks, U-Net as a well-known convolutional neural network architecture, and a closed architecture based on the conditional generative adversarial network are implemented. Different metrics such as the normalized cross-correlation coefficient and Dice scores are used for comparison of methods and results of the different experiments. Statistical analysis is also provided by paired t-test. The present study explores the pros and cons of these methods and their practical impacts on multitasking in different implementation scenarios. This investigation shows that bias correction and cross-modality conversion applications are significantly easier than the segmentation application, and having multitasking with segmentation is not reasonable if one of them is identified as the main target application. However, when the main application is the segmentation of tissues, multitasking with cross-modality conversion is beneficial, especially for the U-net architecture.      
### 5.Boosting the Performance of Video Compression Artifact Reduction with Reference Frame Proposals and Frequency Domain Information  [ :arrow_down: ](https://arxiv.org/pdf/2105.14962.pdf)
>  Many deep learning based video compression artifact removal algorithms have been proposed to recover high-quality videos from low-quality compressed videos. Recently, methods were proposed to mine spatiotemporal information via utilizing multiple neighboring frames as reference frames. However, these post-processing methods take advantage of adjacent frames directly, but neglect the information of the video itself, which can be exploited. In this paper, we propose an effective reference frame proposal strategy to boost the performance of the existing multi-frame approaches. Besides, we introduce a loss based on fast Fourier transformation~(FFT) to further improve the effectiveness of restoration. Experimental results show that our method achieves better fidelity and perceptual performance on MFQE 2.0 dataset than the state-of-the-art methods. And our method won Track 1 and Track 2, and was ranked the 2nd in Track 3 of NTIRE 2021 Quality enhancement of heavily compressed videos Challenge.      
### 6.SNIPS: Solving Noisy Inverse Problems Stochastically  [ :arrow_down: ](https://arxiv.org/pdf/2105.14951.pdf)
>  In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton's method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved.      
### 7.Self-Organized Residual Blocks for Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2105.14926.pdf)
>  It has become a standard practice to use the convolutional networks (ConvNet) with RELU non-linearity in image restoration and super-resolution (SR). Although the universal approximation theorem states that a multi-layer neural network can approximate any non-linear function with the desired precision, it does not reveal the best network architecture to do so. Recently, operational neural networks (ONNs) that choose the best non-linearity from a set of alternatives, and their "self-organized" variants (Self-ONN) that approximate any non-linearity via Taylor series have been proposed to address the well-known limitations and drawbacks of conventional ConvNets such as network homogeneity using only the McCulloch-Pitts neuron model. In this paper, we propose the concept of self-organized operational residual (SOR) blocks, and present hybrid network architectures combining regular residual and SOR blocks to strike a balance between the benefits of stronger non-linearity and the overall number of parameters. The experimental results demonstrate that the~proposed architectures yield performance improvements in both PSNR and perceptual metrics.      
### 8.Measurement placement in electric power transmission and distribution grids: Review of methods and opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2105.14917.pdf)
>  Sensing and measurement systems are a quintessential piece to the safe and reliable operation of electric power grids. Their strategic placement is of ultimate importance because it is not economically viable to install measurement systems on every node and branch of a power grid, though they need to be monitored. An overwhelming number of strategies have been developed to meet oftentimes multiple conflicting objectives. The prime challenge in formulating the problem lies in developing a heuristic or an optimization model that, though mathematically tractable and constrained in cost, leads to trustworthy technical solutions. Besides, large-scale, long-term deployments pose additional challenges because the boundary conditions change as technologies evolve. For instance, the advent of new technologies in sensing and measurement, as well as in communications and networking, might impact the cost and performance of available solutions and shift initially set conditions. Also, the placement strategies developed for transmission grids might not be suitable for distribution grids, and vice versa, due to unique characteristics. Therefore, the strategies need to be flexible, to a certain extent, because no two power grids are alike. Despite the extensive literature on the present topic, the focus of published works tends to be on a specific subject, such as optimal placement of measurements to assure observability in transmission grids. There is a dearth of work providing a comprehensive picture for developing optimal placement strategies. Because of the ongoing efforts on the modernization of electric power grids, there is a need to consolidate the status quo while exposing its limitations to inform policymakers, industry stakeholders, and researchers on the research-and-development needs to push the boundaries for innovation.      
### 9.Refined Deep Neural Network and U-Net for Polyps Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.14848.pdf)
>  The Medico: Multimedia Task 2020 focuses on developing an efficient and accurate computer-aided diagnosis system for automatic segmentation [3]. We participate in task 1, Polyps segmentation task, which is to develop algorithms for segmenting polyps on a comprehensive dataset. In this task, we propose methods combining Residual module, Inception module, Adaptive Convolutional neural network with U-Net model, and PraNet for semantic segmentation of various types of polyps in endoscopic images. We select 5 runs with different architecture and parameters in our methods. Our methods show potential results in accuracy and efficiency through multiple experiments, and our team is in the Top 3 best results with a Jaccard index of 0.765.      
### 10.Speaker Identification from Raw Waveform with LineNet  [ :arrow_down: ](https://arxiv.org/pdf/2105.14826.pdf)
>  Speaker Identification using i-vector has gradually been replaced by speaker Identification using deep learning. Speaker Identification based on Convolutional Neural Networks (CNNs) has been widely used in recent years, which learn low-level speech representations from raw waveforms. On this basis, a CNN architecture called SincNet proposes a kind of unique convolutional layer, which has achieved band-pass filters. Compared with standard CNNs, SincNet learns the low and high cutoff frequencies of each filter.This paper proposes an improved CNNs architecture called LineNet, which encourages the first convolutional layer to implement more specific filters than SincNet. LineNet parameterizes the frequency domain shape and can realize band-pass filters by learning some deformation points in frequency domain. Compared with standard CNN, LineNet can learn the characteristics of each filter. Compared with SincNet, LineNet can learn more characteristic parameters, instead of only low and high cutoff frequencies. This provides a personalized filter bank for different tasks. As a result, our experiments show that the LineNet converges faster than standard CNN and performs better than SincNet.      
### 11.Ultrasonic Array Characterization in Multiscattering and Attenuating Media Using Pin Targets  [ :arrow_down: ](https://arxiv.org/pdf/2105.14816.pdf)
>  This paper presents an approach to characterize ultrasonic imaging arrays using pin targets in commercial test phantoms. We used a 128-element phased array transducer operating at 7.5 MHz with a fractional bandwidth of %70. We also used a tissue-mimicking phantom in the measurements. This phantom consists of pin targets with a 50-micrometer diameter. We excited the transducer with pulsed and coded signals. We used Complementary Golay Sequences to code the transmitted signal and Binary Phase Shift Keying for modulation. We characterized the transducer array using the transfer function, line spread function, range resolution, and beam width in an attenuating and scattering medium. We showed that the pin targets, which are very thin compared to the diffraction-limited focus of the transducer array, are suitable for the transducer characterization under weak reflected signal conditions.      
### 12.Kurtosis-limited Sphere Shaping for Nonlinear Interference Noise Reduction in Optical Channels  [ :arrow_down: ](https://arxiv.org/pdf/2105.14794.pdf)
>  Nonlinear interference (NLI) generated during the propagation of an optical waveform through the fiber depends on the fourth order standardized moment of the channel input distribution, also known as kurtosis. Probabilistically-shaped inputs optimized for the linear Gaussian channel have a Gaussian-like distribution with high kurtosis. For optical channels, this leads to an increase in NLI power and consequently, a decrease in effective signal-to-noise ratio (SNR). In this work, we propose kurtosis-limited enumerative sphere shaping (K-ESS) as an algorithm to generate low-kurtosis shaped inputs. Numerical simulations demonstrate that with K-ESS, it is possible to increase the effective SNRs by 0.4 dB in a single-span single-channel scenario at 400 Gbit/s. K-ESS offers also a twofold decrease in frame error rate with respect to Gaussian-channel-optimal sphere shaping.      
### 13.BaMBNet: A Blur-aware Multi-branch Network for Defocus Deblurring  [ :arrow_down: ](https://arxiv.org/pdf/2105.14766.pdf)
>  The defocus deblurring raised from the finite aperture size and exposure time is an essential problem in the computational photography. It is very challenging because the blur kernel is spatially varying and difficult to estimate by traditional methods. Due to its great breakthrough in low-level tasks, convolutional neural networks (CNNs) have been introduced to the defocus deblurring problem and achieved significant progress. However, they apply the same kernel for different regions of the defocus blurred images, thus it is difficult to handle these nonuniform blurred images. To this end, this study designs a novel blur-aware multi-branch network (BaMBNet), in which different regions (with different blur amounts) should be treated differentially. In particular, we estimate the blur amounts of different regions by the internal geometric constraint of the DP data, which measures the defocus disparity between the left and right views. Based on the assumption that different image regions with different blur amounts have different deblurring difficulties, we leverage different networks with different capacities (\emph{i.e.} parameters) to process different image regions. Moreover, we introduce a meta-learning defocus mask generation algorithm to assign each pixel to a proper branch. In this way, we can expect to well maintain the information of the clear regions while recovering the missing details of the blurred regions. Both quantitative and qualitative experiments demonstrate that our BaMBNet outperforms the state-of-the-art methods. Source code will be available at <a class="link-external link-https" href="https://github.com/junjun-jiang/BaMBNet" rel="external noopener nofollow">this https URL</a>.      
### 14.Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network  [ :arrow_down: ](https://arxiv.org/pdf/2105.14758.pdf)
>  Low-dose CT has been a key diagnostic imaging modality to reduce the potential risk of radiation overdose to patient health. Despite recent advances, CNN-based approaches typically apply filters in a spatially invariant way and adopt similar pixel-level losses, which treat all regions of the CT image equally and can be inefficient when fine-grained structures coexist with non-uniformly distributed noises. To address this issue, we propose a Structure-preserving Kernel Prediction Network (StructKPN) that combines the kernel prediction network with a structure-aware loss function that utilizes the pixel gradient statistics and guides the model towards spatially-variant filters that enhance noise removal, prevent over-smoothing and preserve detailed structures for different regions in CT imaging. Extensive experiments demonstrated that our approach achieved superior performance on both synthetic and non-synthetic datasets, and better preserves structures that are highly desired in clinical screening and low-dose protocol optimization.      
### 15.Graph-signal Reconstruction and Blind Deconvolution for Structured Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2105.14747.pdf)
>  Key to successfully deal with complex contemporary datasets is the development of tractable models that account for the irregular structure of the information at hand. This paper provides a comprehensive view of several sampling, reconstruction, and recovery problems for signals defined on irregular domains that can be accurately represented by a graph. The workhorse assumption is that the (partially) observed signals can be modeled as the output of a graph filter to a structured (parsimonious) input graph signal. When either the input or the filter coefficients are known, this is tantamount to assuming that the signals of interest live on a subspace defined by the supporting graph. When neither is known, the model becomes bilinear. Upon imposing different priors and additional structure on either the input or the filter coefficients, a broad range of relevant problem formulations arise. The goal is then to leverage those priors, the structure of the supporting graph, and the samples of the signal of interest to recover: the signal at the non-sampled nodes (graph-signal interpolation), the input (deconvolution), the filter coefficients (system identification), or any combination thereof (blind deconvolution).      
### 16.An Adaptive Demand Response Framework using Price Elasticity Model in Distribution Networks: A Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2105.14741.pdf)
>  Price elasticity model (PEM) is an appealing and modest model for assessing the potential of flexible demand in DR. It measures the customers demand sensitivity through elasticity in relation to price variation. However, application of PEM in DR is partially apprehensible on attributing the adaptability and adjustability with intertemporal constraints in DR. Thus, this article presents an adaptive economic DR framework with attributes of DR via a dynamic elasticity approach to model customers sensitivity. This dynamic elasticity is modeled through the deterministic and stochastic approaches. Both approaches envision the notion of load recovery for shiftable/flexible loads to make the proposed DR framework adaptive and adjustable relative to price variation. In stochastic approach, a geometric Brownian motion is employed to emulate load recovery with inclusion of intertemporal constraint of load flexibility. The proposed mathematical model shows what should be the customers elasticity value to achieve the factual DR. The case study is carried out on standard IEEE 33 distribution system bus load data to assess technical and socio-economic impact of DR on customers and is also compared with the exiting model.      
### 17.Hierarchical Deep Network with Uncertainty-aware Semi-supervised Learning for Vessel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.14732.pdf)
>  The analysis of organ vessels is essential for computer-aided diagnosis and surgical planning. But it is not a easy task since the fine-detailed connected regions of organ vessel bring a lot of ambiguity in vessel segmentation and sub-type recognition, especially for the low-contrast capillary regions. Furthermore, recent two-staged approaches would accumulate and even amplify these inaccuracies from the first-stage whole vessel segmentation into the second-stage sub-type vessel pixel-wise classification. Moreover, the scarcity of manual annotation in organ vessels poses another challenge. In this paper, to address the above issues, we propose a hierarchical deep network where an attention mechanism localizes the low-contrast capillary regions guided by the whole vessels, and enhance the spatial activation in those areas for the sub-type vessels. In addition, we propose an uncertainty-aware semi-supervised training framework to alleviate the annotation-hungry limitation of deep models. The proposed method achieves the state-of-the-art performance in the benchmarks of both retinal artery/vein segmentation in fundus images and liver portal/hepatic vessel segmentation in CT images.      
### 18.Improving the Accuracy and Efficiency of Online Calibration for Simulation-based Dynamic Traffic Assignment  [ :arrow_down: ](https://arxiv.org/pdf/2105.14716.pdf)
>  Simulation-based Dynamic Traffic Assignment models have important applications in real-time traffic management and control. The efficacy of these systems rests on the ability to generate accurate estimates and predictions of traffic states, which necessitates online calibration. A widely used solution approach for online calibration is the Extended Kalman Filter (EKF), which -- although appealing in its flexibility to incorporate any class of parameters and measurements -- poses several challenges with regard to calibration accuracy and scalability, especially in congested situations for large-scale networks. This paper addresses these issues in turn so as to improve the accuracy and efficiency of EKF-based online calibration approaches for large and congested networks. First, the concept of state augmentation is revisited to handle violations of the Markovian assumption typically implicit in online applications of the EKF. Second, a method based on graph-coloring is proposed to operationalize the partitioned finite-difference approach that enhances scalability of the gradient computations. <br>Several synthetic experiments and a real world case study demonstrate that application of the proposed approaches yields improvements in terms of both prediction accuracy and computational performance. The work has applications in real-world deployments of simulation-based dynamic traffic assignment systems.      
### 19.CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2105.14711.pdf)
>  Spine-related diseases have high morbidity and cause a huge burden of social cost. Spine imaging is an essential tool for noninvasively visualizing and assessing spinal pathology. Segmenting vertebrae in computed tomography (CT) images is the basis of quantitative medical image analysis for clinical diagnosis and surgery planning of spine diseases. Current publicly available annotated datasets on spinal vertebrae are small in size. Due to the lack of a large-scale annotated spine image dataset, the mainstream deep learning-based segmentation methods, which are data-driven, are heavily restricted. In this paper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated from multiple sources for vertebra segmentation, which contains 1,005 CT volumes with over 11,100 labeled vertebrae belonging to different spinal conditions. Based on this dataset, we conduct several spinal vertebrae segmentation experiments to set the first benchmark. We believe that this large-scale dataset will facilitate further research in many spine-related image analysis tasks, including but not limited to vertebrae segmentation, labeling, 3D spine reconstruction from biplanar radiographs, image super-resolution, and enhancement.      
### 20.Joint Stabilization and Regret Minimization through Switching in Systems with Actuator Redundancy  [ :arrow_down: ](https://arxiv.org/pdf/2105.14709.pdf)
>  Adaptively controlling and minimizing regret in unknown dynamical systems while controlling the growth of the system state is crucial in real-world applications. In this work, we study the problem of stabilizing and regret minimization of linear dynamical systems with system-level actuator redundancy. We propose an optimism-based algorithm that utilizes the actuator redundancy and the possibility of switching between actuating modes to guarantee the boundedness of the state. This is in contrast to the prior works that may result in an exponential (in system dimension) explosion of the state of the system. We theoretically study the rate at which our algorithm learns a stabilizing controller and prove that it achieves a regret upper bound of $\mathcal{O}(\sqrt{T})$.      
### 21.Parkinsonian Chinese Speech Analysis towards Automatic Classification of Parkinson's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2105.14704.pdf)
>  Speech disorders often occur at the early stage of Parkinson's disease (PD). The speech impairments could be indicators of the disorder for early diagnosis, while motor symptoms are not obvious. In this study, we constructed a new speech corpus of Mandarin Chinese and addressed classification of patients with PD. We implemented classical machine learning methods with ranking algorithms for feature selection, convolutional and recurrent deep networks, and an end to end system. Our classification accuracy significantly surpassed state-of-the-art studies. The result suggests that free talk has stronger classification power than standard speech tasks, which could help the design of future speech tasks for efficient early diagnosis of the disease. Based on existing classification methods and our natural speech study, the automatic detection of PD from daily conversation could be accessible to the majority of the clinical population.      
### 22.Signal Acquisition of Luojia-1A Low Earth Orbit Navigation Augmentation System with Software Defined Receiver  [ :arrow_down: ](https://arxiv.org/pdf/2105.14671.pdf)
>  Low earth orbit (LEO) satellite navigation signal can be used as an opportunity signal in case of a Global navigation satellite system (GNSS) outage, or as an enhancement means of traditional GNSS positioning algorithms. No matter which service mode is used, signal acquisition is the prerequisite of providing enhanced LEO navigation service. Compared with the medium orbit satellite, the transit time of the LEO satellite is shorter. Thus, it is of great significance to expand the successful acquisition time range of the LEO signal. Previous studies on LEO signal acquisition are based on simulation data. However, signal acquisition research based on real data is very important. In this work, the signal characteristics of LEO satellite: power space density in free space and the Doppler shift of LEO satellite are individually studied. The unified symbol definitions of several integration algorithms based on the parallel search signal acquisition algorithm are given. To verify these algorithms for LEO signal acquisition, a software-defined receiver (SDR) is developed. The performance of those integration algorithms on expanding the successful acquisition time range is verified by the real data collected from the Luojia-1A satellite. The experimental results show that the integration strategy can expand the successful acquisition time range, and it will not expand indefinitely with the integration duration.      
### 23.Eco-Driving of Connected and Autonomous Vehicles with Sequence-to-Sequence Prediction of Target Vehicle Velocity  [ :arrow_down: ](https://arxiv.org/pdf/2105.14658.pdf)
>  The Eco-Driving control problem seeks to perform fuel efficient speed planning for a Connected and Autonomous Vehicle (CAV) that can exploit information available from advanced mapping, and from Vehicle-to-Everything (V2X) communication. The ability of an Eco-Driving strategy to adapt in real time to variable traffic scenarios where surrounding vehicles can be either connected or unconnected is critical for further development and deployment of this technology in the transportation sector. In this work, the Eco-Driving strategy, formulated as a receding-horizon optimal control problem, is integrated with a target vehicle speed prediction model and solved via Dynamic Programming (DP) to determine the optimal speed trajectory in the presence of a human-driven target vehicle. An encoder-decoder architecture analyzes the patterns in the target vehicle velocity recorded over a historic window using a Gated-Recurrent-Unit (GRU) based encoder and generates an estimate of the future velocity trajectory using the GRU based decoder. A sensitivity study is done to analyze the effect of the historical and prediction windows on the accuracy of the velocity predictor. The proposed Eco-Driving controller is evaluated through microscopic simulations using a traffic simulator.      
### 24.Human-level COVID-19 Diagnosis from Low-dose CT Scans Using a Two-stage Time-distributed Capsule Network  [ :arrow_down: ](https://arxiv.org/pdf/2105.14656.pdf)
>  Reverse transcription-polymerase chain reaction (RT-PCR) is currently the gold standard in COVID-19 diagnosis. It can, however, take days to provide the diagnosis, and false negative rate is relatively high. Imaging, in particular chest computed tomography (CT), can assist with diagnosis and assessment of this disease. Nevertheless, it is shown that standard dose CT scan gives significant radiation burden to patients, especially those in need of multiple scans. In this study, we consider low-dose and ultra-low-dose (LDCT and ULDCT) scan protocols that reduce the radiation exposure close to that of a single X-Ray, while maintaining an acceptable resolution for diagnosis purposes. Since thoracic radiology expertise may not be widely available during the pandemic, we develop an Artificial Intelligence (AI)-based framework using a collected dataset of LDCT/ULDCT scans, to study the hypothesis that the AI model can provide human-level performance. The AI model uses a two stage capsule network architecture and can rapidly classify COVID-19, community acquired pneumonia (CAP), and normal cases, using LDCT/ULDCT scans. The AI model achieves COVID-19 sensitivity of 89.5% +\- 0.11, CAP sensitivity of 95% +\- 0.11, normal cases sensitivity (specificity) of 85.7% +\- 0.16, and accuracy of 90% +\- 0.06. By incorporating clinical data (demographic and symptoms), the performance further improves to COVID-19 sensitivity of 94.3% +\- pm 0.05, CAP sensitivity of 96.7% +\- 0.07, normal cases sensitivity (specificity) of 91% +\- 0.09 , and accuracy of 94.1% +\- 0.03. The proposed AI model achieves human-level diagnosis based on the LDCT/ULDCT scans with reduced radiation exposure. We believe that the proposed AI model has the potential to assist the radiologists to accurately and promptly diagnose COVID-19 infection and help control the transmission chain during the pandemic.      
### 25.DimRad: A Radar-Based Perception System for Prosthetic Leg Barrier Traversing  [ :arrow_down: ](https://arxiv.org/pdf/2105.14634.pdf)
>  Lower extremity amputees face challenges in natural locomotion, which is partially compensated using powered assistive systems, e.g., micro-processor controlled prosthetic leg. In this paper, a radar-based perception system is proposed to assist prosthetic legs for autonomous obstacle traversing, focusing on multiple-step staircases. The presented perception system is composed of a radar module operating with a multiple-input-multiple-output (MIMO) configuration to localize consecutive stair corners. An inertial measurement unit (IMU) is integrated for coordinates correction due to the angular dis-positioning that occurs because of the knee angular motion. The captured information from both sensors is used for staircase dimensioning (depth and height). A shallow neural network (NN) is proposed to model the error due to the hardware limitations and enhance the dimension estimation accuracy (1 cm). The algorithm is implemented on a microcontroller subsystem of the radar kit to qualify the perception system for embedded integration in powered prosthetic legs.      
### 26.Robustness of electricity systems with nearly 100% share of renewables: a worst-case study  [ :arrow_down: ](https://arxiv.org/pdf/2105.14582.pdf)
>  Several research studies have shown that future sustainable electricity systems, mostly based on renewable generation and storage, are feasible with current technologies and costs. However, recent episodes of extreme weather conditions, probably associated with climate change, cast shades of doubt on whether the resulting generation portfolios are sufficiently robust to assure, at all times, a suitable balance between generation and demand, when adverse conditions are faced. To address this issue, this work elaborates a methodology intended to determine a sustainable electricity system that can endure extreme weather conditions, which are likely to occur. First, using hourly production and demand data from the last decade, along with estimates of new uses of electricity, a worst-case scenario is constructed, including the storage capacity and additional photovoltaic power which are needed to serve the demand on an hourly basis. Next, several key parameters which may have a significant influence on the LCOE are considered, and a sensitivity analysis is carried out to determine their real impact, significance and potential trends. The proposed methodology is then applied to the Spanish system. The results show that, under the hypotheses and conditions considered in this paper, it is possible to design a decarbonized electricity system that, taking advantage of existing sustainable assets, satisfies the long-term needs by providing a reliable supply at an average cost significantly lower than current market prices.      
### 27.Incorporating forecasting and peer-to-peer negotiation frameworks into a distributed model predictive control approach for meshed electric networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.14547.pdf)
>  Continuous integration of renewable energy sources into power networks is causing a paradigm shift in energy generation and distribution with regards to trading and control; the intermittent nature of renewable sources affects pricing of energy sold or purchased; the networks are subject to operational constraints, voltage limits at each node, rated capacities for the power electronic devices, current bounds for distribution lines. These economic and technical constraints coupled with intermittent renewable injection may pose a threat to system stability and performance. We propose a novel holistic approach to energy trading composed of a distributed predictive control framework to handle physical interactions, i,e., voltage constraints and power dispatch, together with a negotiation framework to determine pricing policies for energy transactions. We study the effect of forecasting generation and consumption on the overall network's performance and market behaviours. We provide a rigorous convergence analysis for both the negotiation framework and the distributed control. Lastly, we assess the impact of forecasting in the proposed system with the aid of testing scenarios.      
### 28.Effect of Camera's Focal Plane Array Fill Factor on Digital Image Correlation Measurement Accuracy  [ :arrow_down: ](https://arxiv.org/pdf/2105.14510.pdf)
>  The camera's focal plane array (FPA) fill factor is one of the parameters for digital cameras, though it is not widely known and usually not reported in specs sheets. The fill factor of an imaging sensor is defined as the ratio of a pixel's light sensitive area to its total theoretical area. It is generally believed that the lower fill factor may reduce the accuracy of photogrammetric measurements. But nevertheless, there are no studies addressing the effect of the imaging sensor's fill factor on digital image correlation (DIC) measurement accuracy. We report on research aiming to quantify the effect of fill factor on DIC measurements accuracy in terms of displacement error and strain error. We use rigid-body-translation experiments then numerically modify the recorded images to synthesize three different types of images with 1/4 of the original resolution. Each type of the synthesized images has different value of the fill factor; namely 100%, 50% and 25%. By performing DIC analysis with the same parameters on the three different types of synthesized images, the effect of fill factor on measurement accuracy may be realized. Our results show that the FPA's fill factor can have a significant effect on the accuracy of DIC measurements. This effect is clearly dependent on the type and characteristics of the speckle pattern. The fill factor has a clear effect on measurement error for low contrast speckle patterns and for high contrast speckle patterns (black dots on white background) with small dot size (3 pixels dot diameter). However, when the dot size is large enough (about 7 pixels dot diameter), the fill factor has very minor effect on measurement error.      
### 29.Intelligently Wireless Batteryless RF-Powered Reconfigurable Surface  [ :arrow_down: ](https://arxiv.org/pdf/2105.14475.pdf)
>  This work exploits commodity, ultra-low cost, commercial radio frequency identification tags (RFID) as the elements of a reconfigurable surface. Such batteryless tags are powered and controlled by a software-defined (SDR) reader, with properly modified software, so that a source-destination link is assisted, operating at a different carrier frequency. In terms of theory, the optimal gain and corresponding best element configuration is offered, with tractable polynomial complexity (instead of exponential) in number of elements. In terms of practice, a concrete way to design and prototype a wireless, batteryless, RF-powered, reconfigurable surface is offered and a proof-of-concept is experimentally demonstrated. It is also found that even with perfect channel estimation, the weak nature of backscattered links limits the performance gains, even for large number of surface elements. Impact of channel estimation errors is also studied. Future extensions at various carrier frequencies could be directly accommodated, through simple modifications in the antenna and matching network of each RFID tag/surface element.      
### 30.Model Aided Deep Learning Based MIMO OFDM Receiver With Nonlinear Power Amplifiers  [ :arrow_down: ](https://arxiv.org/pdf/2105.14458.pdf)
>  Multi-input multi-output orthogonal frequency division multiplexing (MIMO OFDM) is a key technology for mobile communication systems. However, due to the issue of high peak-to-average power ratio (PAPR), the OFDM symbols may suffer from nonlinear distortions of the power amplifier (PA) at the transmitters, which degrades the channel estimation and detection performances of the receivers. To mitigate the clipping distortions at the receivers end, we leverage deep learning (DL) and devise a DL based receiver which is aided by the traditional least square (LS) channel estimation and the zero-forcing (ZF) equalization models. Moreover, a data driven DL based receiver without explicit channel estimation is proposed and combined with the model aided DL based receiver to further improve the performance. Simulation results showcase that the proposed model aided DL based receiver has superior performance of bit error rate and has robustness over different levels of clipping distortions.      
### 31.Green Tethered UAVs for EMF-Aware Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.14359.pdf)
>  A prevalent theory circulating among the non-scientific community is that the intensive deployment of base stations over the territory significantly increases the level of electromagnetic field (EMF) exposure and affects population health. To alleviate this concern, in this work, we propose a network architecture that introduces tethered unmanned aerial vehicles (TUAVs) carrying green antennas to minimize the EMF exposure while guaranteeing a high data rate for users. In particular, each TUAV can attach itself to one of the possible ground stations at the top of some buildings. The location of the TUAVs, transmit power of user equipment and association policy are optimized to minimize the EMF exposure. Unfortunately, the problem turns out to be mixed-integer non-linear programming (MINLP), which is non-deterministic polynomial-time (NP) hard. We propose an efficient low-complexity algorithm composed of three submodules. Firstly, we propose an algorithm based on the greedy principle to determine the optimal association matrix between the users and base stations. Then, we offer two approaches, a modified K-mean and shrink and realign (SR) process, to associate each TUAV with a ground station. Finally, we put forward two algorithms based on the golden search and SR process to adjust the TUAV's position within the hovering area over the building. After that, we consider the dual problem that maximizes the sum rate while keeping the exposure below a predefined value, such as the level enforced by the regulation. Next, we perform extensive simulations to show the effectiveness of the proposed TUAVs to reduce the exposure compared to various architectures. Eventually, we show that TUAVs with green antennas can effectively mitigate the EMF exposure by more than 20% compared to fixed green small cells while achieving a higher data rate.      
### 32.Conditional Deep Convolutional Neural Networks for Improving the Automated Screening of Histopathological Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.14338.pdf)
>  Semantic segmentation of breast cancer metastases in histopathological slides is a challenging task. In fact, significant variation in data characteristics of histopathology images (domain shift) make generalization of deep learning to unseen data difficult. Our goal is to address this challenge by using a conditional Fully Convolutional Network (co-FCN) whose output can be conditioned at run time, and which can improve its performance when a properly selected set of reference slides are used to condition the output. We adapted to our task a co-FCN originally applied to organs segmentation in volumetric medical images and we trained it on the Whole Slide Images (WSIs) from three out of five medical centers present in the CAMELYON17 dataset. We tested the performance of the network on the WSIs of the remaining centers. We also developed an automated selection strategy for selecting the conditioning subset, based on an unsupervised clustering process applied to a target-specific set of reference patches, followed by a selection policy that relies on the cluster similarities with the input patch. We benchmarked our proposed method against a U-Net trained on the same dataset with no conditioning. The conditioned network shows better performance that the U-Net on the WSIs with Isolated Tumor Cells and micro-metastases from the medical centers used as test. Our contributions are an architecture which can be applied to the histopathology domain and an automated procedure for the selection of conditioning data.      
### 33.Covid-19 diagnosis from x-ray using neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.14333.pdf)
>  Corona virus or COVID-19 is a pandemic illness, which has influenced more than million of causalities worldwide and infected a few large number of individuals .Innovative instrument empowering quick screening of the COVID-19 contamination with high precision can be critically useful to the medical care experts. The primary clinical device presently being used for the analysis of COVID-19 is the Reverse record polymerase chain response as known as RT-PCR, which is costly, less-delicate and requires specific clinical work force. X-Ray imaging is an effectively available apparatus that can be a great option in the COVID-19 conclusion. This exploration was taken to examine the utility of computerized reasoning in the quick and exact recognition of COVID-19 from chest X-Ray pictures. The point of this paper is to propose a procedure for programmed recognition of COVID-19 from advanced chest X-Ray images applying pre-prepared profound learning calculations while boosting the discovery exactness. The point is to give over-focused on clinical experts a second pair of eyes through a learning picture characterization models. We distinguish an appropriate Convolutional Neural Network-CNN model through beginning similar investigation of a few mainstream CNN models.      
### 34.Self-Supervised Nonlinear Transform-Based Tensor Nuclear Norm for Multi-Dimensional Image Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2105.14320.pdf)
>  In this paper, we study multi-dimensional image recovery. Recently, transform-based tensor nuclear norm minimization methods are considered to capture low-rank tensor structures to recover third-order tensors in multi-dimensional image processing applications. The main characteristic of such methods is to perform the linear transform along the third mode of third-order tensors, and then compute tensor nuclear norm minimization on the transformed tensor so that the underlying low-rank tensors can be recovered. The main aim of this paper is to propose a nonlinear multilayer neural network to learn a nonlinear transform via the observed tensor data under self-supervision. The proposed network makes use of low-rank representation of transformed tensors and data-fitting between the observed tensor and the reconstructed tensor to construct the nonlinear transformation. Extensive experimental results on tensor completion, background subtraction, robust tensor completion, and snapshot compressive imaging are presented to demonstrate that the performance of the proposed method is better than that of state-of-the-art methods.      
### 35.Social Cost Optimization for Prosumer Community with Two Price-Package Incentives in Two-Settlement Based Electricity Market  [ :arrow_down: ](https://arxiv.org/pdf/2105.14286.pdf)
>  In this paper, we consider a future electricity market consisting of aggregated energy prosumers, who are equipped with local wind power plants (WPPs) to support (part of) their energy demands and can also trade energy with day-ahead market (DAM) and energy balancing market (EBM). In addition, an energy aggregator (EA) is established, who can provide the trading gateways between prosumers and the markets. The EA is responsible for making pricing strategies on the prosumers to influence their trading behaviours such that the social benefit of the prosumer community is improved. Specifically, two price packages are provided by the EA: wholesale price (WP) package and lump-sum (LS) package, which can be flexibly selected by prosumers based on their own preferences. Analytical energy-trading strategies will be derived for WP prosumers and LS prosumers based on non-cooperative games and Nash resource allocation strategies, respectively. In this work, a social cost optimization problem will be formulated for the EA, where the detailed WP/LS selection plans are unknown in advance. Consequently, a stochastic Stackelberg game between prosumers and the EA is formulated, and a two-level stochastic convex programming algorithm is proposed to minimize the expectation of the social cost. The performance of the proposed algorithm is demonstrated with a two-settlement based market model in the simulation.      
### 36.Development, Implementation, and Experimental Outdoor Evaluation of Quadcopter Controllers for Computationally Limited Embedded Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.14231.pdf)
>  Quadcopters are increasingly used for applications ranging from hobby to industrial products and services. This paper serves as a tutorial on the design, simulation, implementation, and experimental outdoor testing of digital quadcopter flight controllers, including Explicit Model Predictive Control, Linear Quadratic Regulator, and Proportional Integral Derivative. A quadcopter was flown in an outdoor testing facility and made to track an inclined, circular path at different tangential velocities under ambient wind conditions. Controller performance was evaluated via multiple metrics, such as position tracking error, velocity tracking error, and onboard computation time. Challenges related to the use of computationally limited embedded hardware and flight in an outdoor environment are addressed with proposed solutions.      
### 37.Evolving Deep Convolutional Neural Network by Hybrid Sine-Cosine and Extreme Learning Machine for Real-time COVID19 Diagnosis from X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.14192.pdf)
>  The COVID19 pandemic globally and significantly has affected the life and health of many communities. The early detection of infected patients is effective in fighting COVID19. Using radiology (X-Ray) images is perhaps the fastest way to diagnose the patients. Thereby, deep Convolutional Neural Networks (CNNs) can be considered as applicable tools to diagnose COVID19 positive cases. Due to the complicated architecture of a deep CNN, its real-time training and testing become a challenging problem. This paper proposes using the Extreme Learning Machine (ELM) instead of the last fully connected layer to address this deficiency. However, the parameters' stochastic tuning of ELM's supervised section causes the final model unreliability. Therefore, to cope with this problem and maintain network reliability, the sine-cosine algorithm was utilized to tune the ELM's parameters. The designed network is then benchmarked on the COVID-Xray-5k dataset, and the results are verified by a comparative study with canonical deep CNN, ELM optimized by cuckoo search, ELM optimized by genetic algorithm, and ELM optimized by whale optimization algorithm. The proposed approach outperforms comparative benchmarks with a final accuracy of 98.83% on the COVID-Xray-5k dataset, leading to a relative error reduction of 2.33% compared to a canonical deep CNN. Even more critical, the designed network's training time is only 0.9421 milliseconds and the overall detection test time for 3100 images is 2.721 seconds.      
### 38.DPLM: A Deep Perceptual Spatial-Audio Localization Metric  [ :arrow_down: ](https://arxiv.org/pdf/2105.14180.pdf)
>  Subjective evaluations are critical for assessing the perceptual realism of sounds in audio-synthesis driven technologies like augmented and virtual reality. However, they are challenging to set up, fatiguing for users, and expensive. In this work, we tackle the problem of capturing the perceptual characteristics of localizing sounds. Specifically, we propose a framework for building a general purpose quality metric to assess spatial localization differences between two binaural recordings. We model localization similarity by utilizing activation-level distances from deep networks trained for direction of arrival (DOA) estimation. Our proposed metric (DPLM) outperforms baseline metrics on correlation with subjective ratings on a diverse set of datasets, even without the benefit of any human-labeled training data.      
### 39.A Joint Intensity-Neuromorphic Event Imaging System for Resource Constrained Devices  [ :arrow_down: ](https://arxiv.org/pdf/2105.14164.pdf)
>  We present a novel adaptive multi-modal intensity-event algorithm to optimize an overall objective of object tracking under bit rate constraints for a host-chip architecture. The chip is a computationally resource constrained device acquiring high resolution intensity frames and events, while the host is capable of performing computationally expensive tasks. We develop a joint intensity-neuromorphic event rate-distortion compression framework with a quadtree (QT) based compression of intensity and events scheme. The data acquisition on the chip is driven by the presence of objects of interest in the scene as detected by an object detector. The most informative intensity and event data are communicated to the host under rate constraints, so that the best possible tracking performance is obtained. The detection and tracking of objects in the scene are done on the distorted data at the host. Intensity and events are jointly used in a fusion framework to enhance the quality of the distorted images, so as to improve the object detection and tracking performance. The performance assessment of the overall system is done in terms of the multiple object tracking accuracy (MOTA) score. Compared to using intensity modality only, there is an improvement in MOTA using both these modalities in different scenarios.      
### 40.Detailed Primary and Secondary Distribution System Model Enhancement Using AMI Data  [ :arrow_down: ](https://arxiv.org/pdf/2105.14161.pdf)
>  Reliable and accurate distribution system modeling, including the secondary network, is essential in examining distribution system performance with high penetration of distributed energy resources (DERs). This paper presents a highly automated, novel method to enhance the accuracy of utility distribution feeder models to capture their performance by matching simulation results with corresponding field measurements. The method is demonstrated using an actual feeder from an electrical utility with high penetration of DERs. The method proposed uses advanced metering infrastructure (AMI) voltage and derived active power measurements at the customer level, and data acquisition systems (DAS) measurements at the feeder-head, in conjunction with an AC optimal power flow (ACOPF) to estimate customer active and reactive power consumption over a time horizon, while accounting for unmetered loads. Additionally, the method proposed estimates both voltage magnitude and angle for each phase at the unbalanced distribution substation. Furthermore, the accuracy of the method developed is verified by comparing the time-series power flow results obtained from the enhancement algorithm with OpenDSS results. The proposed approach seamlessly manages the data available from the optimization procedure through the final model verification automatically.      
### 41.Intelligent Reconfigurable Surface-assisted Multi-UAV Networks: Efficient Resource Allocation with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2105.14142.pdf)
>  In this paper, we propose intelligent reconfigurable surface (IRS)-assisted unmanned aerial vehicles (UAVs) networks that can utilise both advantages of agility and reflection for enhancing the network's performance. To aim at maximising the energy efficiency (EE) of the considered networks, we jointly optimise the power allocation of the UAVs and the phaseshift matrix of the IRS. A deep reinforcement learning (DRL) approach is proposed for solving the continuous optimisation problem with time-varying channel gain in a centralised fashion. Moreover, a parallel learning approach is also proposed for reducing the information transmission requirement of the centralised approach. Numerical results show a significant improvement of our proposed schemes compared with the conventional approaches in terms of EE, flexibility, and processing time. Our proposed DRL methods for IRS-assisted UAV networks can be used for real-time applications due to their capability of instant decision-making and handling the time-varying channel with the dynamic environmental setting.      
### 42.Phoneme-Based Ratio Mask Estimation for Reverberant Speech Enhancement in Cochlear Implant Processors  [ :arrow_down: ](https://arxiv.org/pdf/2105.14135.pdf)
>  Cochlear implant (CI) users have considerable difficulty in understanding speech in reverberant listening environments. Time-frequency (T-F) masking is a common technique that aims to improve speech intelligibility by multiplying reverberant speech by a matrix of gain values to suppress T-F bins dominated by reverberation. Recently proposed mask estimation algorithms leverage machine learning approaches to distinguish between target speech and reverberant reflections. However, the spectro-temporal structure of speech is highly variable and dependent on the underlying phoneme. One way to potentially overcome this variability is to leverage explicit knowledge of phonemic information during mask estimation. This study proposes a phoneme-based mask estimation algorithm, where separate mask estimation models are trained for each phoneme. Sentence recognition tests were conducted in normal hearing listeners to determine whether a phoneme-based mask estimation algorithm is beneficial in the ideal scenario where perfect knowledge of the phoneme is available. The results showed that the phoneme-based masks improved the intelligibility of vocoded speech when compared to conventional phoneme-independent masks. The results suggest that a phoneme-based speech enhancement strategy may potentially benefit CI users in reverberant listening environments.      
### 43.Assessing the intelligibility of vocoded speech using a remote testing framework  [ :arrow_down: ](https://arxiv.org/pdf/2105.14120.pdf)
>  Over the past year, remote speech intelligibility testing has become a popular and necessary alternative to traditional in-person experiments due to the need for physical distancing during the COVID-19 pandemic. A remote framework was developed for conducting speech intelligibility tests with normal hearing listeners. In this study, subjects used their personal computers to complete sentence recognition tasks in anechoic and reverberant listening environments. The results obtained using this remote framework were compared with previously collected in-lab results, and showed higher levels of speech intelligibility among remote study participants than subjects who completed the test in the laboratory.      
### 44.Improved Convergence Rate for a Distributed Two-Time-Scale Gradient Method under Random Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2105.14089.pdf)
>  We study the so-called distributed two-time-scale gradient method for solving convex optimization problems over a network of agents when the communication bandwidth between the nodes is limited, and so information that is exchanged between the nodes must be quantized. Our main contribution is to provide a novel analysis, resulting to an improved convergence rate of this method as compared to the existing works. In particular, we show that the method converges at a rate $O(log_2 k/\sqrt k)$ to the optimal solution, when the underlying objective function is strongly convex and smooth. The key technique in our analysis is to consider a Lyapunov function that simultaneously captures the coupling of the consensus and optimality errors generated by the method.      
### 45.Classification of Brain Tumours in MR Images using Deep Spatiospatial Models  [ :arrow_down: ](https://arxiv.org/pdf/2105.14071.pdf)
>  A brain tumour is a mass or cluster of abnormal cells in the brain, which has the possibility of becoming life-threatening because of its ability to invade neighbouring tissues and also form metastases. An accurate diagnosis is essential for successful treatment planning and magnetic resonance imaging is the principal imaging modality for diagnostic of brain tumours and their extent. Deep Learning methods in computer vision applications have shown significant improvement in recent years, most of which can be credited to the fact that a sizeable amount of data is available to train models on, and the improvements in the model architectures yielding better approximations in a supervised setting. Classifying tumours using such deep learning methods has made significant progress with the availability of open datasets with reliable annotations. Typically those methods are either 3D models, which use 3D volumetric MRIs or even 2D models considering each slice separately. However, by treating the slice spatial dimension separately, spatiotemporal models can be employed as spatiospatial models for this task. These models have the capabilities of learning specific spatial and temporal relationship, while reducing computational costs. This paper uses two spatiotemporal models, ResNet (2+1)D and ResNet Mixed Convolution, to classify different types of brain tumours. It was observed that both these models performed superior to the pure 3D convolutional model, ResNet18. Furthermore, it was also observed that pre-training the models on a different, even unrelated dataset before training them for the task of tumour classification improves the performance. Finally, Pre-trained ResNet Mixed Convolution was observed to be the best model in these experiments, achieving a macro F1-score of 0.93 and a test accuracy of 96.98\%, while at the same time being the model with the least computational cost.      
### 46.Energy-Efficient Precoding in Electromagnetic Exposure-Constrained Uplink Multiuser MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2105.15174.pdf)
>  User electromagnetic (EM) exposure is continuously being exacerbated by the evolution of multi-antenna portable devices. To mitigate the effects of EM radiation, portable devices must satisfy tight regulations on user exposure level, generally measured by specific absorption rate (SAR). To this end, we investigate the SAR-aware uplink precoder design for the energy efficiency (EE) maximization in multiuser multiple-input multiple-output transmission exploiting statistical channel state information (CSI). As the objective function of the design problem is computationally demanding in the absence of closed form, we present an asymptotic approximation of the objective to facilitate the precoder design. An iterative algorithm based on Dinkelbach's method and sequential optimization is proposed to obtain an optimal solution of the asymptotic EE optimization problem. Based on the transformed problem, an iterative SAR-aware water-filing scheme is further conceived for the EE optimization precoding design with statistical CSI. Numerical results illustrate substantial performance improvements provided by our proposed SAR-aware energy-efficient transmission scheme over the traditional baseline schemes.      
### 47.Energy Efficiency Optimization for Multi-cell Massive MIMO: Centralized and Distributed Power Allocation Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2105.15161.pdf)
>  This paper investigates the energy efficiency (EE) optimization in downlink multi-cell massive multiple-input multiple-output (MIMO). In our research, the statistical channel state information (CSI) is exploited to reduce the signaling overhead. To maximize the minimum EE among the neighbouring cells, we design the transmit covariance matrices for each base station (BS). Specifically, optimization schemes for this max-min EE problem are developed, in the centralized and distributed ways, respectively. To obtain the transmit covariance matrices, we first find out the closed-form optimal transmit eigenmatrices for the BS in each cell, and convert the original transmit covariance matrices designing problem into a power allocation one. Then, to lower the computational complexity, we utilize an asymptotic approximation expression for the problem objective. Moreover, for the power allocation design, we adopt the minorization maximization method to address the non-convexity of the ergodic rate, and use Dinkelbach's transform to convert the max-min fractional problem into a series of convex optimization subproblems. To tackle the transformed subproblems, we propose a centralized iterative water-filling scheme. For reducing the backhaul burden, we further develop a distributed algorithm for the power allocation problem, which requires limited inter-cell information sharing. Finally, the performance of the proposed algorithms are demonstrated by extensive numerical results.      
### 48.SeReMAS: Self-Resilient Mobile AutonomousSystems Through Predictive Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2105.15105.pdf)
>  Edge computing enables Mobile Autonomous Systems (MASs) to execute continuous streams of heavy-duty mission-critical processing tasks, such as real-time obstacle detection and navigation. However, in practical applications, erratic patterns in channel quality, network load, and edge server load can interrupt the task flow execution, which necessarily leads to severe disruption of the system's key operations. Existing work has mostly tackled the problem with reactive approaches, which cannot guarantee task-level reliability. Conversely, in this paper we focus on learning-based predictive edge computing to achieve self-resilient task offloading. By conducting a preliminary experimental evaluation, we show that there is no dominant feature that can predict the edge-MAS system reliability, which calls for an ensemble and selection of weaker features. To tackle the complexity of the problem, we propose SeReMAS, a data-driven optimization framework. We first mathematically formulate a Redundant Task Offloading Problem (RTOP), where a MAS may connect to multiple edge servers for redundancy, and needs to select which server(s) to transmit its computing tasks in order to maximize the probability of task execution while minimizing channel and edge resource utilization. We then create a predictor based on Deep Reinforcement Learning (DRL), which produces the optimum task assignment based on application-, network- and telemetry-based features. We prototype SeReMAS on a testbed composed by a drone, mounting a PixHawk flight controller, a Jetson Nano board, and three 802.11n WiFi interfaces. We extensively evaluate SeReMAS by considering an application where one drone offloads high-resolution images for real-time analysis to three edge servers on the ground. Experimental results show that SeReMAS improves task execution probability by $17\%$ with respect to existing reactive-based approaches.      
### 49.Anchor Nodes Positioning for Self-localization in Wireless Sensor Networks using Belief Propagation and Evolutionary Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2105.15101.pdf)
>  Locating each node in a wireless sensor network is essential for starting the monitoring job and sending information about the area. One method that has been used in hard and inaccessible environments is randomly scattering each node in the area. In order to reduce the cost of using GPS at each node, some nodes should be equipped with GPS (anchors), Then using the belief propagation algorithm, locate other nodes. The number of anchor nodes must be reduced since they are expensive. Furthermore, the location of these nodes affects the algorithm's performance. Using multi-objective optimization, an algorithm is introduced in this paper that minimizes the estimated location error and the number of anchor nodes. According to simulation results, This algorithm proposes a set of solutions with less energy consumption and less error than similar algorithms.      
### 50.Multiple Sources Localization with Sparse Recovery under Log-normal Shadow Fading  [ :arrow_down: ](https://arxiv.org/pdf/2105.15097.pdf)
>  Localization based on received signal strength (RSS) has drawn great interest in the wireless sensor network (WSN). In this paper, we investigate the RSS-based multi-sources localization problem with unknown transmitted power under shadow fading. The log-normal shadowing effect is approximated through Fenton-Wilkinson (F-W) method and maximum likelihood estimation is adopted to optimize the RSS-based multiple sources localization problem. Moreover, we exploit a sparse recovery and weighted average of candidates (SR-WAC) based method to set up an initiation, which can efficiently approach a superior local optimal solution. It is shown from the simulation results that the proposed method has a much higher localization accuracy and outperforms the other      
### 51.Small-Scale Spatial-Temporal Correlation Modeling for Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2105.15096.pdf)
>  The reconfigurable intelligent surface (RIS) is an emerging promising candidate technology for the sixth-generation wireless networks, where the element spacing is usually of sub-wavelength. Only limited knowledge, however, has been gained about the spatial-temporal correlation behavior among the elements in an RIS. In this paper, we investigate the joint spatial-temporal correlation models for an RIS in a wireless communication system. Joint small-scale spatial-temporal correlation functions are provided and analyzed for both ideal isotropic scattering and more practical non-isotropic scattering environments, where the latter is studied by employing an angular distribution derived from real-world millimeter-wave measurements. Analytical and simulation results demonstrate that the joint spatial-temporal correlation can be represented by a four-dimensional sinc function under isotropic scattering, while the correlation is generally stronger with more fluctuation for non-isotropic scattering with various motion directions.      
### 52.Systematic investigation into generalization of COVID-19 CT deep learning models with Gabor ensemble for lung involvement scoring  [ :arrow_down: ](https://arxiv.org/pdf/2105.15094.pdf)
>  The COVID-19 pandemic has inspired unprecedented data collection and computer vision modelling efforts worldwide, focusing on diagnosis and stratification of COVID-19 from medical images. Despite this large-scale research effort, these models have found limited practical application due in part to unproven generalization of these models beyond their source study. This study investigates the generalizability of key published models using the publicly available COVID-19 Computed Tomography data through cross dataset validation. We then assess the predictive ability of these models for COVID-19 severity using an independent new dataset that is stratified for COVID-19 lung involvement. Each inter-dataset study is performed using histogram equalization, and contrast limited adaptive histogram equalization with and without a learning Gabor filter. The study shows high variability in the generalization of models trained on these datasets due to varied sample image provenances and acquisition processes amongst other factors. We show that under certain conditions, an internally consistent dataset can generalize well to an external dataset despite structural differences between these datasets with f1 scores up to 86%. Our best performing model shows high predictive accuracy for lung involvement score for an independent dataset for which expertly labelled lung involvement stratification is available. Creating an ensemble of our best model for disease positive prediction with our best model for disease negative prediction using a min-max function resulted in a superior model for lung involvement prediction with average predictive accuracy of 75% for zero lung involvement and 96% for 75-100% lung involvement with almost linear relationship between these stratifications.      
### 53.SDNet: mutil-branch for single image deraining using swin  [ :arrow_down: ](https://arxiv.org/pdf/2105.15077.pdf)
>  Rain streaks degrade the image quality and seriously affect the performance of subsequent computer vision tasks, such as autonomous driving, social security, etc. Therefore, removing rain streaks from a given rainy images is of great significance. Convolutional neural networks(CNN) have been widely used in image deraining tasks, however, the local computational characteristics of convolutional operations limit the development of image deraining tasks. Recently, the popular transformer has global computational features that can further facilitate the development of image deraining tasks. In this paper, we introduce Swin-transformer into the field of image deraining for the first time to study the performance and potential of Swin-transformer in the field of image deraining. Specifically, we improve the basic module of Swin-transformer and design a three-branch model to implement single-image rain removal. The former implements the basic rain pattern feature extraction, while the latter fuses different features to further extract and process the image features. In addition, we employ a jump connection to fuse deep features and shallow features. In terms of experiments, the existing public dataset suffers from image duplication and relatively homogeneous background. So we propose a new dataset Rain3000 to validate our model. Therefore, we propose a new dataset Rain3000 for validating our model. Experimental results on the publicly available datasets Rain100L, Rain100H and our dataset Rain3000 show that our proposed method has performance and inference speed advantages over the current mainstream single-image rain streaks removal models.The source code will be available at <a class="link-external link-https" href="https://github.com/H-tfx/SDNet" rel="external noopener nofollow">this https URL</a>.      
### 54.Boundary Output Feedback Stabilization of State Delayed Reaction-Diffusion PDEs  [ :arrow_down: ](https://arxiv.org/pdf/2105.15056.pdf)
>  This paper studies the boundary output feedback stabilization of general 1-D reaction-diffusion PDEs in the presence of a state delay in the reaction term. The control input applies through a Robin boundary condition while the system output is selected as a either Dirichlet or Neumann boundary trace. The control strategy takes the form of a finite-dimensional observer-based controller with feedback and observer gains that are computed in order to dominate the state delayed term. For any arbitrarily given value of the state delay, we show the exponential stability of the resulting closed-loop system provided the order of the observer is selected large enough.      
### 55.Scorpion detection and classification systems based on computer vision and deep learning for health security purposes  [ :arrow_down: ](https://arxiv.org/pdf/2105.15041.pdf)
>  In this paper, two novel automatic and real-time systems for the detection and classification of two genera of scorpions found in La Plata city (Argentina) were developed using computer vision and deep learning techniques. The object detection technique was implemented with two different methods, YOLO (You Only Look Once) and MobileNet, based on the shape features of the scorpions. High accuracy values of 88% and 91%, and high recall values of 90% and 97%, have been achieved for both models, respectively, which guarantees that they can successfully detect scorpions. In addition, the MobileNet method has been shown to have excellent performance to detect scorpions within an uncontrolled environment and to perform multiple detections. The MobileNet model was also used for image classification in order to successfully distinguish between dangerous scorpion (Tityus) and non-dangerous scorpion (Bothriurus) with the purpose of providing a health security tool. Applications for smartphones were developed, with the advantage of the portability of the systems, which can be used as a help tool for emergency services, or for biological research purposes. The developed systems can be easily scalable to other genera and species of scorpions to extend the region where these applications can be used.      
### 56.Singing Language Identification using a Deep Phonotactic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2105.15014.pdf)
>  Extensive works have tackled Language Identification (LID) in the speech domain, however their application to the singing voice trails and performances on Singing Language Identification (SLID) can be improved leveraging recent progresses made in other singing related tasks. This work presents a modernized phonotactic system for SLID on polyphonic music: phoneme recognition is performed with a Connectionist Temporal Classification (CTC)-based acoustic model trained with multilingual data, before language classification with a recurrent model based on the phonemes estimation. The full pipeline is trained and evaluated with a large and publicly available dataset, with unprecedented performances. First results of SLID with out-of-set languages are also presented.      
### 57.Safe Pontryagin Differentiable Programming  [ :arrow_down: ](https://arxiv.org/pdf/2105.14937.pdf)
>  We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic safe differentiable framework to solve a broad class of safety-critical learning and control tasks -- problems that require the guarantee of both immediate and long-term constraint satisfaction at any stage of the learning and control progress. In the spirit of interior-point methods, Safe PDP handles different types of state and input constraints by incorporating them into the cost and loss through barrier functions. We prove the following fundamental features of Safe PDP: first, both the constrained solution and its gradient in backward pass can be approximated by solving a more efficient unconstrained counterpart; second, the approximation for both the solution and its gradient can be controlled for arbitrary accuracy using a barrier parameter; and third, importantly, any intermediate results throughout the approximation and optimization are strictly respecting all constraints, thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of Safe PDP in solving various safe learning and control tasks, including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging control systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.      
### 58.LTL-Constrained Steady-State Policy Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2105.14894.pdf)
>  Decision-making policies for agents are often synthesized with the constraint that a formal specification of behaviour is satisfied. Here we focus on infinite-horizon properties. On the one hand, Linear Temporal Logic (LTL) is a popular example of a formalism for qualitative specifications. On the other hand, Steady-State Policy Synthesis (SSPS) has recently received considerable attention as it provides a more quantitative and more behavioural perspective on specifications, in terms of the frequency with which states are visited. Finally, rewards provide a classic framework for quantitative properties. In this paper, we study Markov decision processes (MDP) with the specification combining all these three types. The derived policy maximizes the reward among all policies ensuring the LTL specification with the given probability and adhering to the steady-state constraints. To this end, we provide a unified solution reducing the multi-type specification to a multi-dimensional long-run average reward. This is enabled by Limit-Deterministic Büchi Automata (LDBA), recently studied in the context of LTL model checking on MDP, and allows for an elegant solution through a simple linear programme. The algorithm also extends to the general $\omega$-regular properties and runs in time polynomial in the sizes of the MDP as well as the LDBA.      
### 59.Variational Autoencoders: A Harmonic Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2105.14866.pdf)
>  In this work we study Variational Autoencoders (VAEs) from the perspective of harmonic analysis. By viewing a VAE's latent space as a Gaussian Space, a variety of measure space, we derive a series of results that show that the encoder variance of a VAE controls the frequency content of the functions parameterised by the VAE encoder and decoder neural networks. In particular we demonstrate that larger encoder variances reduce the high frequency content of these functions. Our analysis allows us to show that increasing this variance effectively induces a soft Lipschitz constraint on the decoder network of a VAE, which is a core contributor to the adversarial robustness of VAEs. We further demonstrate that adding Gaussian noise to the input of a VAE allows us to more finely control the frequency content and the Lipschitz constant of the VAE encoder networks. To support our theoretical analysis we run experiments with VAEs with small fully-connected neural networks and with larger convolutional networks, demonstrating empirically that our theory holds for a variety of neural network architectures.      
### 60.Why does CTC result in peaky behavior?  [ :arrow_down: ](https://arxiv.org/pdf/2105.14849.pdf)
>  The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence.      
### 61.RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.14797.pdf)
>  Deep Neural Networks (DNNs) are ubiquitous in today's computer vision land-scape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to re-train the model. In this paper, we present RED, a data-free structured, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.      
### 62.Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR  [ :arrow_down: ](https://arxiv.org/pdf/2105.14779.pdf)
>  With the advent of globalization, there is an increasing demand for multilingual automatic speech recognition (ASR), handling language and dialectal variation of spoken content. Recent studies show its efficacy over monolingual systems. In this study, we design a large multilingual end-to-end ASR using self-attention based conformer architecture. We trained the system using Arabic (Ar), English (En) and French (Fr) languages. We evaluate the system performance handling: (i) monolingual (Ar, En and Fr); (ii) multi-dialectal (Modern Standard Arabic, along with dialectal variation such as Egyptian and Moroccan); (iii) code-switching -- cross-lingual (Ar-En/Fr) and dialectal (MSA-Egyptian dialect) test cases, and compare with current state-of-the-art systems. Furthermore, we investigate the influence of different embedding/character representations including character vs word-piece; shared vs distinct input symbol per language. Our findings demonstrate the strength of such a model by outperforming state-of-the-art monolingual dialectal Arabic and code-switching Arabic ASR.      
### 63.Multi-Objective LQG Design with Primal-Dual Method  [ :arrow_down: ](https://arxiv.org/pdf/2105.14760.pdf)
>  The goal of this paper is to study a multi-objective linear quadratic Gaussian (LQG) control problem. In particular, we consider an optimal control problem minimizing a quadratic cost over a finite time horizon for linear stochastic systems subject to control energy constraints. To solve the problem, we suggest an efficient bisection line search algorithm which is computationally efficient compared to other approaches such as the semidefinite programming. The main idea is to use the Lagrangian function and Karush-Kuhn-Tucker (KKT) optimality conditions to solve the constrained optimization problem. The Lagrange multiplier is searched using the bisection line search. Numerical examples are given to demonstrate the effectiveness of the proposed methods.      
### 64.Transfer Learning as an Enhancement for Reconfiguration Management of Cyber-Physical Production Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.14730.pdf)
>  Reconfiguration demand is increasing due to frequent requirement changes for manufacturing systems. Recent approaches aim at investigating feasible configuration alternatives from which they select the optimal one. This relies on processes whose behavior is not reliant on e.g. the production sequence. However, when machine learning is used, components' behavior depends on the process' specifics, requiring additional concepts to successfully conduct reconfiguration management. Therefore, we propose the enhancement of the comprehensive reconfiguration management with transfer learning. This provides the ability to assess the machine learning dependent behavior of the different CPPS configurations with reduced effort and further assists the recommissioning of the chosen one. A real cyber-physical production system from the discrete manufacturing domain is utilized to demonstrate the aforementioned proposal.      
### 65.Noise Classification Aided Attention-Based Neural Network for Monaural Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2105.14719.pdf)
>  This paper proposes an noise type classification aided attention-based neural network approach for monaural speech enhancement. The network is constructed based on a previous work by introducing a noise classification subnetwork into the structure and taking the classification embedding into the attention mechanism for guiding the network to make better feature extraction. Specifically, to make the network an end-to-end way, an audio encoder and decoder constructed by temporal convolution is used to make transformation between waveform and spectrogram. Additionally, our model is composed of two long short term memory (LSTM) based encoders, two attention mechanism, a noise classifier and a speech mask generator. Experiments show that, compared with OM-LSA and the previous work, the proposed noise classification aided attention-based approach can achieve better performance in terms of speech quality (PESQ). More promisingly, our approach has better generalization ability to unseen noise conditions.      
### 66.Multi-Scale Temporal Convolution Network for Classroom Voice Detection  [ :arrow_down: ](https://arxiv.org/pdf/2105.14717.pdf)
>  Teaching with the cooperation of expert teacher and assistant teacher, which is the so-called "double-teachers classroom", i.e., the course is giving by the expert online and presented through projection screen at the classroom, and the teacher at the classroom performs as an assistant for guiding the students in learning, is becoming more prevalent in today's teaching method for K-12 education. For monitoring the teaching quality, a microphone clipped on the assistant's neckline is always used for voice recording, then fed to the downstream tasks of automatic speech recognition (ASR) and neural language processing (NLP). However, besides its voice, there would be some other interfering voices, including the expert's one and the student's one. Here, we propose to extract the assistant' voices from the perspective of sound event detection, i.e., the voices are classified into four categories, namely the expert, the teacher, the mixture of them, and the background. To make frame-level identification, which is important for grabbing sensitive words for the downstream tasks, a multi-scale temporal convolution neural network is constructed with stacked dilated convolutions for considering both local and global properties. These features are concatenated and fed to a classification network constructed by three linear layers. The framework is evaluated on simulated data and real-world recordings, giving considerable performance in terms of precision and recall, compared with some classical classification methods.      
### 67.Emergence and algorithmic information dynamics of systems and observers  [ :arrow_down: ](https://arxiv.org/pdf/2105.14707.pdf)
>  Previous work has shown that perturbation analysis in algorithmic information dynamics can uncover generative causal processes of finite objects and quantify each of its element's information contribution to computably constructing the objects. One of the challenges for defining emergence is that the dependency on the observer's previous knowledge may cause a phenomenon to present itself as emergent for one observer at the same time that reducible for another observer. Thus, in order to quantify emergence of algorithmic information in computable generative processes, perturbation analyses may inherit such a problem of the dependency on the observer's previous formal knowledge. In this sense, by formalizing the act of observing as mutual perturbations, the emergence of algorithmic information becomes invariant, minimal, and robust to information costs and distortions, while it indeed depends on the observer. Then, we demonstrate that the unbounded increase of emergent algorithmic information implies asymptotically observer-independent emergence, which eventually overcomes any formal theory that any observer might devise. In addition, we discuss weak and strong emergence and analyze the concepts of observer-dependent emergence and asymptotically observer-independent emergence found in previous definitions and models in the literature of deterministic dynamical and computable systems.      
### 68.Image-to-Video Generation via 3D Facial Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2105.14678.pdf)
>  We present a versatile model, FaceAnime, for various video generation tasks from still images. Video generation from a single face image is an interesting problem and usually tackled by utilizing Generative Adversarial Networks (GANs) to integrate information from the input face image and a sequence of sparse facial landmarks. However, the generated face images usually suffer from quality loss, image distortion, identity change, and expression mismatching due to the weak representation capacity of the facial landmarks. In this paper, we propose to "imagine" a face video from a single face image according to the reconstructed 3D face dynamics, aiming to generate a realistic and identity-preserving face video, with precisely predicted pose and facial expression. The 3D dynamics reveal changes of the facial expression and motion, and can serve as a strong prior knowledge for guiding highly realistic face video generation. In particular, we explore face video prediction and exploit a well-designed 3D dynamic prediction network to predict a 3D dynamic sequence for a single face image. The 3D dynamics are then further rendered by the sparse texture mapping algorithm to recover structural details and sparse textures for generating face frames. Our model is versatile for various AR/VR and entertainment applications, such as face video retargeting and face video prediction. Superior experimental results have well demonstrated its effectiveness in generating high-fidelity, identity-preserving, and visually pleasant face video clips from a single source face image.      
### 69.A Minimax Lower Bound for Low-Rank Matrix-Variate Logistic Regression  [ :arrow_down: ](https://arxiv.org/pdf/2105.14673.pdf)
>  This paper considers the problem of matrix-variate logistic regression. The fundamental error threshold on estimating coefficient matrices in the logistic regression problem is found by deriving a lower bound on the minimax risk. The focus of this paper is on derivation of a minimax risk lower bound for low-rank coefficient matrices. The bound depends explicitly on the dimensions and distribution of the covariates, the rank and energy of the coefficient matrix, and the number of samples. The resulting bound is proportional to the intrinsic degrees of freedom in the problem, which suggests the sample complexity of the low-rank matrix logistic regression problem can be lower than that for vectorized logistic regression. \color{red}\color{black} The proof techniques utilized in this work also set the stage for development of minimax lower bounds for tensor-variate logistic regression problems.      
### 70.EchoFilter: End-to-End Neural Network for Acoustic Echo Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2105.14666.pdf)
>  Acoustic Echo Cancellation (AEC) whose aim is to suppress the echo originated from acoustic coupling between loudspeakers and microphones, plays a key role in voice interaction. Linear adaptive filter (AF) is always used for handling this problem. However, since there would be some severe effects in real scenarios, such nonlinear distortions, background noises, and microphone clipping, it would lead to considerable residual echo, giving poor performance in practice. In this paper, we propose an end-to-end network structure for echo cancellation, which is directly done on time-domain audio waveform. It is transformed to deep representation by temporal convolution, and modelled by Long Short-Term Memory (LSTM) for considering temporal property. Since time delay and severe reverberation may exist at the near-end with respect to the far-end, a local attention is employed for alignment. The network is trained using multitask learning by employing an auxiliary classification network for double-talk detection. Experiments show the superiority of our proposed method in terms of the echo return loss enhancement (ERLE) for single-talk periods and the perceptual evaluation of speech quality (PESQ) score for double-talk periods in background noise and nonlinear distortion scenarios.      
### 71.Federated Learning for Industrial Internet of Things in Future Industries  [ :arrow_down: ](https://arxiv.org/pdf/2105.14659.pdf)
>  The Industrial Internet of Things (IIoT) offers promising opportunities to transform the operation of industrial systems and becomes a key enabler for future industries. Recently, artificial intelligence (AI) has been widely utilized for realizing intelligent IIoT applications where AI techniques require centralized data collection and processing. However, this is not always feasible in realistic scenarios due to the high scalability of modern IIoT networks and growing industrial data confidentiality. Federated Learning (FL), as an emerging collaborative AI approach, is particularly attractive for intelligent IIoT networks by coordinating multiple IIoT devices and machines to perform AI training at the network edge while helping protect user privacy. In this article, we provide a detailed overview and discussions of the emerging applications of FL in key IIoT services and applications. A case study is also provided to demonstrate the feasibility of FL in IIoT. Finally, we highlight a range of interesting open research topics that need to be addressed for the full realization of FL-IIoT in industries.      
### 72.An iterative Jacobi-like algorithm to compute a few sparse eigenvalue-eigenvector pairs  [ :arrow_down: ](https://arxiv.org/pdf/2105.14642.pdf)
>  In this paper, we describe a new algorithm to compute the extreme eigenvalue/eigenvector pairs of a symmetric matrix. The proposed algorithm can be viewed as an extension of the Jacobi transformation method for symmetric matrix diagonalization to the case where we want to compute just a few eigenvalues/eigenvectors. The method is also particularly well suited for the computation of sparse eigenspaces. We show the effectiveness of the method for sparse low-rank approximations and show applications to random symmetric matrices, graph Fourier transforms, and with the sparse principal component analysis in image classification experiments.      
### 73.On the Controllers Based on Time Delay Estimation for Robotic Manipulators  [ :arrow_down: ](https://arxiv.org/pdf/2105.14615.pdf)
>  Assurance of asymptotic trajectory tracking in robotic manipulators with a continuous control law in the presence of unmodeled dynamics or external disturbance is a challenging problem. Recently, it is asserted that this aim is achieved by designing a traditional model-free controller together with time delay estimation (TDE) such that neither dynamical parameters nor conservative assumptions on the external disturbance are required. In this note, the purpose is to show that this claim is wrong. Additionally, modification of this method for some special cases is presented.      
### 74.Safety Embedded Differential Dynamic Programming using Discrete Barrier States  [ :arrow_down: ](https://arxiv.org/pdf/2105.14608.pdf)
>  Certified safe control is a growing challenge in robotics, especially when performance and safety objectives are desired to be concurrently achieved. In this work, we extend the barrier state (BaS) concept, recently proposed for stabilization of continuous time systems, to enforce safety for discrete time systems by creating a discrete barrier state (DBaS). The constructed DBaS is embedded into the discrete model of the safety-critical system in order to integrate safety objectives into performance objectives. We subsequently use the proposed technique to implement a safety embedded stabilizing control for nonlinear discrete systems. Furthermore, we employ the DBaS method to develop a safety embedded differential dynamic programming (DDP) technique to plan and execute safe optimal trajectories. The proposed algorithm is leveraged on a differential wheeled robot and on a quadrotor to safely perform several tasks including reaching, tracking and safe multi-quadrotor movement. The DBaS-based DDP (DBaS-DDP) is compared to the penalty method used in constrained DDP problems where it is shown that the DBaS-DDP consistently outperforms the penalty method.      
### 75.StyTr^2: Unbiased Image Style Transfer with Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2105.14576.pdf)
>  The goal of image style transfer is to render an image with artistic features guided by a style reference while maintaining the original content. Due to the locality and spatial invariance in CNNs, it is difficult to extract and maintain the global information of input images. Therefore, traditional neural style transfer methods are usually biased and content leak can be observed by running several times of the style transfer process with the same reference style image. To address this critical issue, we take long-range dependencies of input images into account for unbiased style transfer by proposing a transformer-based approach, namely StyTr^2. In contrast with visual transformers for other vision tasks, our StyTr^2 contains two different transformer encoders to generate domain-specific sequences for content and style, respectively. Following the encoders, a multi-layer transformer decoder is adopted to stylize the content sequence according to the style sequence. In addition, we analyze the deficiency of existing positional encoding methods and propose the content-aware positional encoding (CAPE) which is scale-invariant and more suitable for image style transfer task. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed StyTr^2 compared to state-of-the-art CNN-based and flow-based approaches.      
### 76.A Joint Power Splitting, Active and Passive Beamforming Optimization Framework for IRS Assisted MIMO SWIPT System  [ :arrow_down: ](https://arxiv.org/pdf/2105.14545.pdf)
>  This paper considers an intelligent reflecting surface (IRS) assisted multi-input multi-output (MIMO) power splitting (PS) based simultaneous wireless information and power transfer (SWIPT) system with multiple PS receivers (PSRs). The objective is to maximize the achievable data rate of the system by jointly optimizing the PS ratios at the PSRs, the active transmit beamforming (ATB) at the access point (AP), and the passive reflective beamforming (PRB) at the IRS, while the constraints on maximum transmission power at the AP, the reflective phase shift of each element at the IRS, the individual minimum harvested energy requirement of each PSR, and the domain of PS ratio of each PSR are all satisfied. For this unsolved problem, however, since the optimization variables are intricately coupled and the constraints are conflicting, the formulated problem is non-convex, and cannot be addressed by employing exist approaches directly. To this end, we propose a joint optimization framework to solve this problem. Particularly, we reformulate it as an equivalent form by employing the Lagrangian dual transform and the fractional programming transform, and decompose the transformed problem into several sub-problems. Then, we propose an alternate optimization algorithm by capitalizing on the dual sub-gradient method, the successive convex approximation method, and the penalty-based majorization-minimization approach, to solve the sub-problems iteratively, and obtain the optimal solutions in nearly closed-forms. Numerical simulation results verify the effectiveness of the IRS in SWIPT system and indicate that the proposed algorithm offers a substantial performance gain.      
### 77.Knowledge Transfer for Few-shot Segmentation of Novel White Matter Tracts  [ :arrow_down: ](https://arxiv.org/pdf/2105.14513.pdf)
>  Convolutional neural networks (CNNs) have achieved stateof-the-art performance for white matter (WM) tract segmentation based on diffusion magnetic resonance imaging (dMRI). These CNNs require a large number of manual delineations of the WM tracts of interest for training, which are generally labor-intensive and costly. The expensive manual delineation can be a particular disadvantage when novel WM tracts, i.e., tracts that have not been included in existing manual delineations, are to be analyzed. To accurately segment novel WM tracts, it is desirable to transfer the knowledge learned about existing WM tracts, so that even with only a few delineations of the novel WM tracts, CNNs can learn adequately for the segmentation. In this paper, we explore the transfer of such knowledge to the segmentation of novel WM tracts in the few-shot setting. Although a classic fine-tuning strategy can be used for the purpose, the information in the last task-specific layer for segmenting existing WM tracts is completely discarded. We hypothesize that the weights of this last layer can bear valuable information for segmenting the novel WM tracts and thus completely discarding the information is not optimal. In particular, we assume that the novel WM tracts can correlate with existing WM tracts and the segmentation of novel WM tracts can be predicted with the logits of existing WM tracts. In this way, better initialization of the last layer than random initialization can be achieved for fine-tuning. Further, we show that a more adaptive use of the knowledge in the last layer for segmenting existing WM tracts can be conveniently achieved by simply inserting a warmup stage before classic fine-tuning. The proposed method was evaluated on a publicly available dMRI dataset, where we demonstrate the benefit of our method for few-shot segmentation of novel WM tracts.      
### 78.Generating Ten BCI Commands Using Four Simple Motor Imageries  [ :arrow_down: ](https://arxiv.org/pdf/2105.14493.pdf)
>  The brain computer interface (BCI) systems are utilized for transferring information among humans and computers by analyzing electroencephalogram (EEG) recordings.The process of mentally previewing a motor movement without generating the corporal output can be described as motor imagery (MI).In this emerging research field, the number of commands is also limited in relation to the number of MI tasks; in the current literature, mostly two or four commands (classes) are studied. As a solution to this problem, it is recommended to use mental tasks as well as MI tasks. Unfortunately, the use of this approach reduces the classification performance of MI EEG signals. The fMRI analyses show that the resources in the brain associated with the motor imagery can be activated independently. It is assumed that the brain activity induced by the MI of the combination of body parts corresponds to the superposition of the activities generated during each body parts's simple MI. In this study, in order to create more than four BCI commands, we suggest to generate combined MI EEG signals artificially by using left hand, right hand, tongue, and feet motor imageries in pairs. A maximum of ten different BCI commands can be generated by using four motor imageries in pairs.This study aims to achieve high classification performances for BCI commands produced from four motor imageries by implementing a small-sized deep neural network (DNN).The presented method is evaluated on the four-class datasets of BCI Competitions III and IV, and an average classification performance of 81.8% is achieved for ten classes. The above assumption is also validated on a different dataset which consists of simple and combined MI EEG signals acquired in real time. Trained with the artificially generated combined MI EEG signals, DivFE resulted in an average of 76.5% success rate for the combined MI EEG signals acquired in real-time.      
### 79.Joint Training of the Superimposed Direct and Reflected Links in Reconfigurable Intelligent Surface Assisted Multiuser Communications  [ :arrow_down: ](https://arxiv.org/pdf/2105.14484.pdf)
>  In Reconfigurable intelligent surface (RIS)-assisted systems the acquisition of CSI and the optimization of the reflecting coefficients constitute a pair of salient design issues. In this paper, a novel channel training protocol is proposed, which is capable of achieving a flexible performance vs. signalling and pilot overhead as well as implementation complexity trade-off. More specifically, first of all, we conceive a holistic channel estimation protocol, which integrates the existing channel estimation techniques and passive beamforming design. Secondly, we propose a new channel training framework. In contrast to the conventional channel estimation arrangements, our new framework divides the training phase into several periods, where the superimposed end-to-end channel is estimated instead of separately estimating the direct BS-user channel and cascaded reflected BS-RIS-user channels. As a result, the reflecting coefficients of the RIS are optimized by comparing the objective function values over multiple training periods. Moreover, the theoretical performance of our channel training protocol is analyzed and compared to that under the optimal reflecting coefficients. In addition, the potential benefits of our channel training protocol in reducing the complexity, pilot overhead as well as signalling overhead are also detailed. Thirdly, we derive the theoretical performance of channel estimation protocols and our channel training protocol in the presence of noise for a SISO scenario, which provides useful insights into the impact of the noise on the overall RIS performance. Finally, our numerical simulations characterize the performance of the proposed protocols and verify our theoretical analysis. In particular, the simulation results demonstrate that our channel training protocol is more competitive than the channel estimation protocol at low signal-to-noise ratios.      
### 80.Communication efficient privacy-preserving distributed optimization using adaptive differential quantization  [ :arrow_down: ](https://arxiv.org/pdf/2105.14416.pdf)
>  Privacy issues and communication cost are both major concerns in distributed optimization. There is often a trade-off between them because the encryption methods required for privacy-preservation often incur expensive communication bandwidth. To address this issue, we, in this paper, propose a quantization-based approach to achieve both communication efficient and privacy-preserving solutions in the context of distributed optimization. By deploying an adaptive differential quantization scheme, we allow each node in the network to achieve its optimum solution with a low communication cost while keeping its private data unrevealed. Additionally, the proposed approach is general and can be applied in various distributed optimization methods, such as the primal-dual method of multipliers (PDMM) and the alternating direction method of multipliers (ADMM). Moveover, we consider two widely used adversary models: passive and eavesdropping. Finally, we investigate the properties of the proposed approach using different applications and demonstrate its superior performance in terms of several parameters including accuracy, privacy, and communication cost.      
### 81.An improved LogNNet classifier for IoT application  [ :arrow_down: ](https://arxiv.org/pdf/2105.14412.pdf)
>  The internet of things devices suffer of low memory while good accuracy is needed. Designing suitable algorithms is vital in this subject. This paper proposes a feed forward LogNNet neural network which uses a semi-linear Henon type discrete chaotic map to classify MNIST-10 dataset. The model is composed of reservoir part and trainable classifier. The aim of reservoir part is transforming the inputs to maximize the classification accuracy using a special matrix filing method and a time series generated by the chaotic map. The parameters of the chaotic map are optimized using particle swarm optimization with random immigrants. The results show that the proposed LogNNet/Henon classifier has higher accuracy and same RAM saving comparable to the original version of LogNNet and has broad prospects for implementation in IoT devices. In addition, the relation between the entropy and accuracy of the classification is investigated. It is shown that there exists a direct relation between the value of entropy and accuracy of the classification.      
### 82.A Matrix Autoencoder Framework to Align the Functional and Structural Connectivity Manifolds as Guided by Behavioral Phenotypes  [ :arrow_down: ](https://arxiv.org/pdf/2105.14409.pdf)
>  We propose a novel matrix autoencoder to map functional connectomes from resting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor Imaging (DTI), as guided by subject-level phenotypic measures. Our specialized autoencoder infers a low dimensional manifold embedding for the rs-fMRI correlation matrices that mimics a canonical outer-product decomposition. The embedding is simultaneously used to reconstruct DTI tractography matrices via a second manifold alignment decoder and to predict inter-subject phenotypic variability via an artificial neural network. We validate our framework on a dataset of 275 healthy individuals from the Human Connectome Project database and on a second clinical dataset consisting of 57 subjects with Autism Spectrum Disorder. We demonstrate that the model reliably recovers structural connectivity patterns across individuals, while robustly extracting predictive and interpretable brain biomarkers in a cross-validated setting. Finally, our framework outperforms several baselines at predicting behavioral phenotypes in both real-world datasets.      
### 83.On Centralized and Distributed Mirror Descent: Exponential Convergence Analysis Using Quadratic Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2105.14385.pdf)
>  Mirror descent (MD) is a powerful first-order optimization technique that subsumes several optimization algorithms including gradient descent (GD). In this work, we study the exact convergence rate of MD in both centralized and distributed cases for strongly convex and smooth problems. We view MD with a dynamical system lens and leverage quadratic constraints (QCs) to provide convergence guarantees based on the Lyapunov stability. For centralized MD, we establish a semi-definite programming (SDP) that certifies exponentially fast convergence of MD subject to a linear matrix inequality (LMI). We prove that the SDP always has a feasible solution that recovers the optimal GD rate. Next, we analyze the exponential convergence of distributed MD and characterize the rate using two LMIs. To the best of our knowledge, the exact (exponential) rate of distributed MD has not been previously explored in the literature. We present numerical results as a verification of our theory and observe that the richness of the Lyapunov function entails better (worst-case) convergence rates compared to existing works on distributed GD.      
### 84.Three-dimensional multimodal medical imaging system based on free-hand ultrasound and structured light  [ :arrow_down: ](https://arxiv.org/pdf/2105.14355.pdf)
>  We propose a three-dimensional (3D) multimodal medical imaging system that combines freehand ultrasound and structured light 3D reconstruction in a single coordinate system without requiring registration. To the best of our knowledge, these techniques have not been combined before as a multimodal imaging technique. The system complements the internal 3D information acquired with ultrasound, with the external surface measured with the structure light technique. Moreover, the ultrasound probe's optical tracking for pose estimation was implemented based on a convolutional neural network. Experimental results show the system's high accuracy and reproducibility, as well as its potential for preoperative and intraoperative applications. The experimental multimodal error, or the distance from two surfaces obtained with different modalities, was 0.12 mm. The code is available as a Github repository.      
### 85.Stability and Super-resolution of MUSIC and ESPRIT for Multi-snapshot Spectral Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2105.14304.pdf)
>  This paper studies the spectral estimation problem of estimating the locations of a fixed number of point sources given multiple snapshots of Fourier measurements collected by a uniform array of sensors. We prove novel non-asymptotic stability bounds for MUSIC and ESPRIT as a function of the noise standard deviation, number of snapshots, source amplitudes, and support. Our most general result is a perturbation bound of the signal space in terms of the minimum singular value of Fourier matrices. When the point sources are located in several separated clumps, we provide an explicit upper bound of the noise-space correlation perturbation error in MUSIC and the support error in ESPRIT in terms of a Super-Resolution Factor (SRF). The upper bound for ESPRIT is then compared with a new Cramér-Rao lower bound for the clumps model. As a result, we show that ESPRIT is comparable to that of the optimal unbiased estimator(s) in terms of the dependence on noise, number of snapshots and SRF. As a byproduct of our analysis, we discover several fundamental differences between the single-snapshot and multi-snapshot problems. Our theory is validated by numerical experiments.      
### 86.Applications of Epileptic Seizures Detection in Neuroimaging Modalities Using Deep Learning Techniques: Methods, Challenges, and Future Works  [ :arrow_down: ](https://arxiv.org/pdf/2105.14278.pdf)
>  Epileptic seizures are a type of neurological disorder that affect many people worldwide. Specialist physicians and neurologists take advantage of structural and functional neuroimaging modalities to diagnose various types of epileptic seizures. Neuroimaging modalities assist specialist physicians considerably in analyzing brain tissue and the changes made in it. One method to accelerate the accurate and fast diagnosis of epileptic seizures is to employ computer aided diagnosis systems (CADS) based on artificial intelligence (AI) and functional and structural neuroimaging modalities. AI encompasses a variety of areas, and one of its branches is deep learning (DL). Not long ago, and before the rise of DL algorithms, feature extraction was an essential part of every conventional machine learning method, yet handcrafting features limit these models' performances to the knowledge of system designers. DL methods resolved this issue entirely by automating the feature extraction and classification process; applications of these methods in many fields of medicine, such as the diagnosis of epileptic seizures, have made notable improvements. In this paper, a comprehensive overview of the types of DL methods exploited to diagnose epileptic seizures from various neuroimaging modalities has been studied. Additionally, rehabilitation systems and cloud computing in epileptic seizures diagnosis applications have been exactly investigated using various modalities.      
### 87.Performance of Dual-Hop Relaying for OWC System Over Foggy Channel with Pointing Errors and Atmospheric Turbulence  [ :arrow_down: ](https://arxiv.org/pdf/2105.14256.pdf)
>  Optical wireless communication (OWC) over atmospheric turbulence and pointing errors is a well-studied topic. Still, there is limited research on signal fading due to random fog and pointing errors in outdoor environments. In this paper, we analyze the performance of a decode-and-forward (DF) relaying under the combined effect of random fog, pointing errors, and atmospheric turbulence with a negligible line-of-sight (LOS) direct link. We consider a generalized model for the end-to-end channel with independent and not identically distributed (i.ni.d.) pointing errors, random fog with Gamma distributed attenuation coefficient, asymptotic exponentiated Weibull turbulence, and asymmetrical distance between the source and destination. We derive distribution functions of the signal-to-noise ratio (SNR), and then we develop analytical expressions of the outage probability, average SNR, ergodic rate, and average bit error rate (BER) in terms of OWC system parameters. We also develop simplified performance to provide insight on the system behavior analytically under various practically relevant scenarios. We demonstrate the mutual effects of channel impairments and pointing errors on the OWC performance, and show that the relaying system provides significant performance improvement compared with the direct transmissions, especially when pointing errors and fog becomes more pronounced.      
### 88.Biomimetic Control of Myoelectric Prosthetic Hand Based on a Lambda-type Muscle Model  [ :arrow_down: ](https://arxiv.org/pdf/2105.14215.pdf)
>  Myoelectric prosthetic hands are intended to replace the function of the amputee's lost arm. Therefore, developing robotic prosthetics that can mimic not only the appearance and functionality of humans but also characteristics unique to human movements is paramount. Although the impedance model was proposed to realize biomimetic control, this model cannot replicate the characteristics of human movements effectively because the joint angle always converges to the equilibrium position during muscle relaxation. This paper proposes a novel biomimetic control method for myoelectric prosthetic hands integrating the impedance model with the concept of the $\lambda$-type muscle model. The proposed method can dynamically control the joint equilibrium position, according to the state of the muscle, and can maintain the joint angle naturally during muscle relaxation. The effectiveness of the proposed method is evaluated through simulations and a series of experiments on non-amputee participants. The experimental results, based on comparison with the actual human joint angles, suggest that the proposed method has a better correlation with the actual human motion than the conventional methods. Additionally, the control experiments showed that the proposed method could achieve a natural prosthetic hand movement similar to that of a human, thereby allowing voluntary hand opening and closing movements.      
### 89.Prediction error quantification through probabilistic scaling  [ :arrow_down: ](https://arxiv.org/pdf/2105.14187.pdf)
>  In this paper, we address the probabilistic error quantification of a general class of prediction methods. We consider a given prediction model and show how to obtain, through a sample-based approach, a probabilistic upper bound on the absolute value of the prediction error. The proposed scheme is based on a probabilistic scaling methodology in which the number of required randomized samples is independent of the complexity of the prediction model. The methodology is extended to address the case in which the probabilistic uncertain quantification is required to be valid for every member of a finite family of predictors. We illustrate the results of the paper by means of a numerical example.      
### 90.3D U-NetR: Low Dose Computed Tomography Reconstruction via Deep Learning and 3 Dimensional Convolutions  [ :arrow_down: ](https://arxiv.org/pdf/2105.14130.pdf)
>  In this paper, we introduced a novel deep learning based reconstruction technique using the correlations of all 3 dimensions with each other by taking into account the correlation between 2-dimensional low-dose CT images. Sparse or noisy sinograms are back projected to the image domain with FBP operation, then denoising process is applied with a U-Net like 3 dimensional network called 3D U-NetR. Proposed network is trained with synthetic and real chest CT images, and 2D U-Net is also trained with the same dataset to prove the importance of the 3rd dimension. Proposed network shows better quantitative performance on SSIM and PSNR. More importantly, 3D U-NetR captures medically critical visual details that cannot be visualized by 2D network.      
### 91.Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2105.14125.pdf)
>  Many engineering problems have multiple objectives, and the overall aim is to optimize a non-linear function of these objectives. In this paper, we formulate the problem of maximizing a non-linear concave function of multiple long-term objectives. A policy-gradient based model-free algorithm is proposed for the problem. To compute an estimate of the gradient, a biased estimator is proposed. The proposed algorithm is shown to achieve convergence to within an $\epsilon$ of the global optima after sampling $\mathcal{O}(\frac{M^4\sigma^2}{(1-\gamma)^8\epsilon^4})$ trajectories where $\gamma$ is the discount factor and $M$ is the number of the agents, thus achieving the same dependence on $\epsilon$ as the policy gradient algorithm for the standard reinforcement learning.      
### 92.Asymptotically Optimal Bandits under Weighted Information  [ :arrow_down: ](https://arxiv.org/pdf/2105.14114.pdf)
>  We study the problem of regret minimization in a multi-armed bandit setup where the agent is allowed to play multiple arms at each round by spreading the resources usually allocated to only one arm. At each iteration the agent selects a normalized power profile and receives a Gaussian vector as outcome, where the unknown variance of each sample is inversely proportional to the power allocated to that arm. The reward corresponds to a linear combination of the power profile and the outcomes, resembling a linear bandit. By spreading the power, the agent can choose to collect information much faster than in a traditional multi-armed bandit at the price of reducing the accuracy of the samples. This setup is fundamentally different from that of a linear bandit -- the regret is known to scale as $\Theta(\sqrt{T})$ for linear bandits, while in this setup the agent receives a much more detailed feedback, for which we derive a tight $\log(T)$ problem-dependent lower-bound. We propose a Thompson-Sampling-based strategy, called Weighted Thompson Sampling (\WTS), that designs the power profile as its posterior belief of each arm being the best arm, and show that its upper bound matches the derived logarithmic lower bound. Finally, we apply this strategy to a problem of control and system identification, where the goal is to estimate the maximum gain (also called $\mathcal{H}_\infty$-norm) of a linear dynamical system based on batches of input-output samples.      
### 93.Necessary and Sufficient Conditions for Stability of Discrete-Time Switched Linear Systems with Ranged Dwell Time  [ :arrow_down: ](https://arxiv.org/pdf/2105.14113.pdf)
>  This paper deals with the stability analysis problem of discrete-time switched linear systems with ranged dwell time. A novel concept called L-switching-cycle is proposed, which contains sequences of multiple activation cycles satisfying the prescribed ranged dwell time constraint. Based on L-switching-cycle, two sufficient conditions are proposed to ensure the global uniform asymptotic stability of discrete-time switched linear systems. It is noted that two conditions are equivalent in stability analysis with the same $L$-switching-cycle. These two sufficient conditions can be viewed as generalizations of the clock-dependent Lyapunov and multiple Lyapunov function methods, respectively. Furthermore, it has been proven that the proposed L-switching-cycle can eventually achieve the nonconservativeness in stability analysis as long as a sufficiently long L-switching-cycle is adopted. A numerical example is provided to illustrate our theoretical results.      
### 94.Reinforcement Learning reveals fundamental limits on the mixing of active particles  [ :arrow_down: ](https://arxiv.org/pdf/2105.14105.pdf)
>  The control of far-from-equilibrium physical systems, including active materials, has emerged as an important area for the application of reinforcement learning (RL) strategies to derive control policies for physical systems. In active materials, non-linear dynamics and long-range interactions between particles prohibit closed-form descriptions of the system's dynamics and prevent explicit solutions to optimal control problems. Due to fundamental challenges in solving for explicit control strategies, RL has emerged as an approach to derive control strategies for far-from-equilibrium active matter systems. However, an important open question is how the mathematical structure and the physical properties of the active matter systems determine the tractability of RL for learning control policies. In this work, we show that RL can only find good strategies to the canonical active matter task of mixing for systems that combine attractive and repulsive particle interactions. Using mathematical results from dynamical systems theory, we relate the availability of both interaction types with the existence of hyperbolic dynamics and the ability of RL to find homogeneous mixing strategies. In particular, we show that for drag-dominated translational-invariant particle systems, hyperbolic dynamics and, therefore, mixing requires combining attractive and repulsive interactions. Broadly, our work demonstrates how fundamental physical and mathematical properties of dynamical systems can enable or constrain reinforcement learning-based control.      
### 95.Data-Driven Participation Factors for Nonlinear Systems Based on Koopman Mode Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/1806.01344.pdf)
>  This paper develops a novel data-driven technique to compute the participation factors for nonlinear systems based on the Koopman mode decomposition. Provided that certain conditions are satisfied, it is shown that the proposed technique generalizes the original definition of the linear mode-in-state participation factors. Two numerical examples are provided to demonstrate the performance of our approach: one relying on a canonical nonlinear dynamical system, and the other based on the two-area four-machine power system. The Koopman mode decomposition is capable of coping with a large class of nonlinearity, thereby making our technique able to deal with oscillations arising in practice due to nonlinearities while being fast to compute and compatible with real-time applications.      
