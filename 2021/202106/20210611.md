# ArXiv eess --Fri, 11 Jun 2021
### 1.Multiple Dynamic Pricing for Demand Response with Adaptive Clustering-based Customer Segmentation in Smart Grids  [ :arrow_down: ](https://arxiv.org/pdf/2106.05905.pdf)
>  In this paper, we propose a realistic multiple dynamic pricing approach to demand response in the retail market. First, an adaptive clustering-based customer segmentation framework is proposed to categorize customers into different groups to enable the effective identification of usage patterns. Second, customized demand models with important market constraints which capture the price-demand relationship explicitly, are developed for each group of customers to improve the model accuracy and enable meaningful pricing. Third, the multiple pricing based demand response is formulated as a profit maximization problem subject to realistic market constraints. The overall aim of the proposed scalable and practical method aims to achieve 'right' prices for 'right' customers so as to benefit various stakeholders in the system such as grid operators, customers and retailers. The proposed multiple pricing framework is evaluated via simulations based on real-world datasets.      
### 2.VLSI Systems for signal processing and Communications  [ :arrow_down: ](https://arxiv.org/pdf/2106.05896.pdf)
>  The growing advances in VLSI technology and design tools have exponentially expanded the application domain of digital signal processing over the past 10 years. This survey emphasises on the architectural and performance parameters of VLSI for DSP applications such as speech processing, wireless communication, analog to digital converters, etc      
### 3.A Wideband Dual Function Radar Communication System With Sparse Array and OFDM Waveforms  [ :arrow_down: ](https://arxiv.org/pdf/2106.05878.pdf)
>  A novel multiple-input multiple-output (MIMO) dual-function radar communication (DFRC) system is proposed. The system transmits wideband, orthogonal frequency division multiplexing (OFDM) waveforms using a small subset of the available antennas in each channel use. The proposed system assigns most carriers to antennas in a shared fashion, thus efficiently exploiting the available communication bandwidth, and a small set of subcarriers to active antennas in an exclusive fashion (private subcarriers). A novel target estimation approach is proposed to overcome the coupling of target parameters introduced by subcarrier sharing. The obtained parameters are further refined via an iterative approach, which formulates a sparse signal recovery problem based on the data of the private subcarriers. The system is endowed with beamforming capability, via waveform precoding and antenna selection. The precoding and antenna selection matrices are optimally co-designed to meet a joint sensing-communication system performance. The sparsity of the transmit array is exploited at the communication receiver to recover the transmitted information. The use of shared subcarriers enables high communication rate, while the sparse transmit array maintains low system hardware cost. The sensing problem is formulated by taking into account frequency selective fading, and a method is proposed to estimate the channel coefficients during the sensing process. The functionality of the proposed system is demonstrated via simulations.      
### 4.CoviLearn: A Machine Learning Integrated Smart X-Ray Device in Healthcare Cyber-Physical System for Automatic Initial Screening of COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2106.05861.pdf)
>  The pandemic of novel Coronavirus Disease 2019 (COVID-19) is widespread all over the world causing serious health problems as well as serious impact on the global economy. Reliable and fast testing of the COVID-19 has been a challenge for researchers and healthcare practitioners. In this work we present a novel machine learning (ML) integrated X-ray device in Healthcare Cyber-Physical System (H-CPS) or smart healthcare framework (called CoviLearn) to allow healthcare practitioners to perform automatic initial screening of COVID-19 patients. We propose convolutional neural network (CNN) models of X-ray images integrated into an X-ray device for automatic COVID-19 detection. The proposed CoviLearn device will be useful in detecting if a person is COVID-19 positive or negative by considering the chest X-ray image of individuals. CoviLearn will be useful tool doctors to detect potential COVID-19 infections instantaneously without taking more intrusive healthcare data samples, such as saliva and blood. COVID-19 attacks the endothelium tissues that support respiratory tract, X-rays images can be used to analyze the health of a patient lungs. As all healthcare centers have X-ray machines, it could be possible to use proposed CoviLearn X-rays to test for COVID-19 without the especial test kits. Our proposed automated analysis system CoviLearn which has 99% accuracy will be able to save valuable time of medical professionals as the X-ray machines come with a drawback as it needed a radiology expert.      
### 5.Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights  [ :arrow_down: ](https://arxiv.org/pdf/2106.05852.pdf)
>  Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the various linguistic peculiarities present in the language. The Sanskrit language is lexically productive, undergoes euphonic assimilation of phones at the word boundaries and exhibits variations in spelling conventions and in pronunciations. In this work, we propose the first large scale study of automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR dataset for Sanskrit, which faithfully captures several of the linguistic characteristics expressed by the language. We investigate the role of different acoustic model and language model units in ASR systems for Sanskrit. We also propose a new modelling unit, inspired by the syllable level unit selection, that captures character sequences from one vowel in the word to the next vowel. We also highlight the importance of choosing graphemic representations for Sanskrit and show the impact of this choice on word error rates (WER). Finally, we extend these insights from Sanskrit ASR for building ASR systems in two other Indic languages, Gujarati and Telugu. For both these languages, our experimental results show that the use of phonetic based graphemic representations in ASR results in performance improvements as compared to ASR systems that use native scripts.      
### 6.Construction of Differential-Cascaded Structures for Control of Robot Manipulators  [ :arrow_down: ](https://arxiv.org/pdf/2106.05832.pdf)
>  This paper focuses on the construction of differential-cascaded structures for control of nonlinear robot manipulators subjected to disturbances and unavailability of partial information of the desired trajectory. The proposed differential-cascaded structures rely on infinite differential series to handle the robustness with respect to time-varying disturbances and the partial knowledge of the desired trajectories for nonlinear robot manipulators. The long-standing problem of reliable adaptation in the presence of sustaining disturbances is solved by the proposed forwardstepping control with forwardstepping adaptation, and stacked reference dynamics yielding adaptive differential-cascaded structures have been proposed to facilitate the forwardstepping adaptation to both the uncertainty of robot dynamics and that of the frequencies of disturbances. A distinctive point of the proposed differential-cascaded approach is that the reference dynamics for design and analysis involve high-order quantities, but via degree-reduction implementation of the reference dynamics, the control typically involves only the low-order quantities, thus facilitating its applications to control of most physical systems. Our result relies on neither the explicit estimation of the disturbances or derivative and second derivative of the desired position nor the solutions to linear/nonlinear regulator equations, and the employed essential element is a differential-cascaded structure governing robot dynamics.      
### 7.Speaker-conversation factorial designs for diarization error analysis  [ :arrow_down: ](https://arxiv.org/pdf/2106.05792.pdf)
>  Speaker diarization accuracy can be affected by both acoustics and conversation characteristics. Determining the cause of diarization errors is difficult because speaker voice acoustics and conversation structure co-vary, and the interactions between acoustics, conversational structure, and diarization accuracy are complex. This paper proposes a methodology that can distinguish independent marginal effects of acoustic and conversation characteristics on diarization accuracy by remixing conversations in a factorial design. As an illustration, this approach is used to investigate gender-related and language-related accuracy differences with three diarization systems: a baseline system using subsegment x-vector clustering, a variant of it with shorter subsegments, and a third system based on a Bayesian hidden Markov model. Our analysis shows large accuracy disparities for the baseline system primarily due to conversational structure, which are partially mitigated in the other two systems. The illustration thus demonstrates how the methodology can be used to identify and guide diarization model improvements.      
### 8.Enabling Full Mutualism for Symbiotic Radio with Massive Backscatter Devices  [ :arrow_down: ](https://arxiv.org/pdf/2106.05789.pdf)
>  Symbiotic radio is a promising technology to achieve spectrum- and energy-efficient wireless communications, where the secondary backscatter device (BD) leverages not only the spectrum but also the power of the primary signals for its own information transmission. In return, the primary communication link can be enhanced by the additional multipaths created by the BD. This is known as the mutualism relationship of symbiotic radio. However, as the backscattering link is much weaker than the direct link due to double attenuations, the improvement of the primary link brought by one single BD is extremely limited. To address this issue and enable full mutualism of symbiotic radio, in this paper, we study symbiotic radio with massive number of BDs. For symbiotic radio multiple access channel (MAC) with successive interference cancellation (SIC), we first derive the achievable rate of both the primary and secondary communications, based on which a receive beamforming optimization problem is formulated and solved. Furthermore, considering the asymptotic regime of massive number of BDs, closed-form expressions are derived for the primary and the secondary communication rates, both of which are shown to be increasing functions of the number of BDs. This thus demonstrates that the mutualism relationship of symbiotic radio can be fully exploited with massive BD access.      
### 9.End-to-End Transmission Analysis of Simultaneous Wireless Information and Power Transfer using Resonant Beam  [ :arrow_down: ](https://arxiv.org/pdf/2106.05769.pdf)
>  Integrating the wireless power transfer (WPT) technology into the wireless communication system has been important for operational cost saving and power-hungry problem solving of electronic devices. In this paper, we propose a resonant beam simultaneous wireless information and power transfer (RB-SWIPT) system, which utilizes a gain medium and two retro-reflecting surfaces to enhance and retro-reflect energy, and allows devices to recharge their batteries and exchange information from the resonant beam wirelessly. To reveal the SWIPT mechanism and evaluate the SWIPT performance, we establish an analytical end-to-end (E2E) transmission model based on a modular approach and the electromagnetic field propagation. Then, the intra-cavity power intensity distribution, transmission loss, output power, and E2E efficiency can be obtained. The numerical evaluation illustrates that the exemplary RB-SWIPT system can provide about 4.20W electric power and 12.41bps/Hz spectral efficiency, and shorter transmission distance or larger retro-reflecting surface size can lead to higher E2E efficiency. The RB-SWIPT presents a new way for high-power, long-range WPT, and high-rate communication.      
### 10.A High Accuracy Image Hashing and Random Forest Classifier for Crack Detection in Concrete Surface Images  [ :arrow_down: ](https://arxiv.org/pdf/2106.05755.pdf)
>  Automatic detection of cracks in concrete surfaces based on image processing is a clear trend in modern civil engineering applications. Most infrastructure is made of concrete and cracks reveal degradation of the structural integrity of the facilities, which can lead to extreme structural failures. There are many approaches to overcome the difficulties in image-based crack detection, ranging from the pre-processing of the input image to the proper adjustment of efficient classifiers, passing through the essential feature selection step. This paper is related to the process of constructing features from images to allow a classifier to find the boundaries between images with and without cracks. The most common approaches to feature extraction are the convolutional techniques to extract relevant positional information from images and the filters for edge detection or background removal. Here we apply hashing techniques for the first time used for features extraction in this problem. The study of the classification capacity of hashes is carried out by comparing 5 different hash algorithms, 2 of which are based on wavelets. The effect of applying the z-transform on the images before calculating the hashes was also studied, which totals the study of 10 new features for this problem. A comparative study of 17 different algorithms from the scikit-learn library was carried out. The results show that 9 of the 10 features are relevant to the problem, as well as that the accuracy of the classifiers varied between 0.697 for the Naive-Bayes Gaussian classifier and 0.99 for the Random Forest (RF) classifier. The feature extraction algorithm developed in this work and the RF classifier algorithm is suitable for embedded applications, for example in inspection drones, as long as they are highly accurate and computationally light, both in terms of memory and processing time.      
### 11.Adaptive Robust Data-driven Building Control via Bi-level Reformulation: an Experimental Result  [ :arrow_down: ](https://arxiv.org/pdf/2106.05740.pdf)
>  In the era of digitalization, utilization of data-driven control approaches to minimize energy consumption of residential/commercial building is of far-reaching significance. Meanwhile, A number of recent approaches based on the application of Willems' fundamental lemma for data-driven controller design from input/output measurements are very promising for deterministic LTI systems. This paper addresses the key noise-free assumption, and extends these data-driven control schemes to adaptive building control with measured process noise and unknown measurement noise via a robust bilevel formulation, whose upper level ensures robustness and whose lower level guarantees prediction quality. Corresponding numerical improvements and an active excitation mechanism are proposed to enable a computationally efficient reliable operation. The efficacy of the proposed scheme is validated by a numerical example and a real-world experiment on a lecture hall on EPFL campus.      
### 12.Fastening the Initial Access in 5G NR Sidelink for 6G V2X Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.05716.pdf)
>  The ever-increasing demand for intelligent, automated, and connected mobility solutions pushes for the development of an innovative sixth Generation (6G) of cellular networks. A radical transformation on the physical layer of vehicular communications is planned, with a paradigm shift towards beam-based millimeter Waves or sub-Terahertz communications, which require precise beam pointing for guaranteeing the communication link, especially in high mobility. A key design aspect is a fast and proactive Initial Access (IA) algorithm to select the optimal beam to be used. In this work, we investigate alternative IA techniques to fasten the current fifth-generation (5G) standard, targeting an efficient 6G design. First, we discuss cooperative position-based schemes that rely on the position information. Then, motivated by the intuition of a non-uniform distribution of the communication directions due to road topology constraints, we design two Probabilistic Codebook (PCB) techniques of prioritized beams. In the first one, the PCBs are built leveraging past collected traffic information, while in the second one, we use the Hough Transform over the digital map to extract dominant road directions. We also show that the information coming from the angular probability distribution allows designing non-uniform codebook quantization, reducing the degradation of the performances compared to uniform one. Numerical simulation on realistic scenarios shows that PCBs-based beam selection outperforms the 5G standard in terms of the number of IA trials, with a performance comparable to position-based methods, without requiring the signaling of sensitive information.      
### 13.Outlier-Robust Filtering For Nonlinear Systems With Selective Observations Rejection  [ :arrow_down: ](https://arxiv.org/pdf/2106.05706.pdf)
>  This letter presents a novel outlier-robust filter for nonlinear dynamical systems. We consider a common case where measurements are obtained from independent sensors. The proposed method is devised by modifying the measurement model and subsequently using the theory of Variational Bayes and general Gaussian filtering. We treat the measurement outliers independently for independent observations leading to selective rejection of the corrupted observations during inference. By carrying out simulations for variable number of sensors we verify that an implementation of the proposed filter is computationally more efficient as compared to similar baseline methods.      
### 14.SNR Scaling Laws for Radio Sensing with Extremely Large-Scale MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2106.05651.pdf)
>  Mobile communication networks were designed to mainly support ubiquitous wireless communications, yet they are expected to also achieve radio sensing capabilities in the near future. Most prior studies on radar sensing focus on distant targets, which usually rely on far-field assumption with uniform plane wave (UPW) models. However, with ever-increasing antenna size, together with the growing need to also sense nearby targets, the far-field assumption may become invalid. This paper studies radar sensing with extremely large-scale (XL) antenna arrays, where a generic model that takes into account both spherical wavefront and amplitude variations across array elements is developed. Furthermore, new closed-form expressions of the sensing signal-to-noise ratios (SNRs) are derived for both XL-MIMO radar and XL-phased-array radar modes. Our results reveal that different from the conventional UPW model where the SNR scales linearly and unboundedly with N for MIMO radar and with MN for phased-array radar, with M and N being the transmit and receive antenna numbers, respectively, more practical SNR scaling laws are obtained. For XL-phased-array radar with optimal power allocation, the SNR increases with M and N with diminishing returns, governed by new parameters called the transmit and receive angular spans. On the other hand, for XL-MIMO radar, while the same SNR scaling as XL-phased-array radar is obeyed for N, the SNR first increases and then decreases with M.      
### 15.Contraction Analysis of Discrete-time Stochastic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.05635.pdf)
>  In this paper, we develop a novel contraction framework for stability analysis of discrete-time nonlinear systems with parameters following stochastic processes. For general stochastic processes, we first provide a sufficient condition for uniform incremental exponential stability (UIES) in the first moment with respect to a Riemannian metric. Then, focusing on the Euclidean distance, we present a necessary and sufficient condition for UIES in the second moment. By virtue of studying general stochastic processes, we can readily derive UIES conditions for special classes of processes, e.g., i.i.d. processes and Markov processes, which is demonstrated as selected applications of our results.      
### 16.Relational Data Selection for Data Augmentation of Speaker-dependent Multi-band MelGAN Vocoder  [ :arrow_down: ](https://arxiv.org/pdf/2106.05629.pdf)
>  Nowadays, neural vocoders can generate very high-fidelity speech when a bunch of training data is available. Although a speaker-dependent (SD) vocoder usually outperforms a speaker-independent (SI) vocoder, it is impractical to collect a large amount of data of a specific target speaker for most real-world applications. To tackle the problem of limited target data, a data augmentation method based on speaker representation and similarity measurement of speaker verification is proposed in this paper. The proposed method selects utterances that have similar speaker identity to the target speaker from an external corpus, and then combines the selected utterances with the limited target data for SD vocoder adaptation. The evaluation results show that, compared with the vocoder adapted using only limited target data, the vocoder adapted using augmented data improves both the quality and similarity of synthesized speech.      
### 17.RetinaNet Object Detector based on Analog-to-Spiking Neural Network Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2106.05624.pdf)
>  The paper proposes a method to convert a deep learning object detector into an equivalent spiking neural network. The aim is to provide a conversion framework that is not constrained to shallow network structures and classification problems as in state-of-the-art conversion libraries. The results show that models of higher complexity, such as the RetinaNet object detector, can be converted with limited loss in performance.      
### 18.Sparse Reconstruction of Chirplets for Automotive FMCW Radar Interference Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2106.05594.pdf)
>  Mutual interference in automotive radar scenarios is going to become a major concern as the density of vehicles with radar sensors in the roads increases. The present work tackles the problem of frequency modulated continuous wave (FMCW) to FMCW and continuous wave interference. In this context, we propose a signal processing technique to blindly identify and remove interference by using the fast Orthogonal Matching Pursuit (OMP) algorithm to project the interference signals in a reduced chirplet basis, and separate it from the target signal with minimal loss of information. Significant reduction of the noise-plus-interference levels are observed in both simulated and measured data, the later acquired with state of the art automotive sensors.      
### 19.CALTeC: Content-Adaptive Linear Tensor Completion for Collaborative Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2106.05531.pdf)
>  In collaborative intelligence, an artificial intelligence (AI) model is typically split between an edge device and the cloud. Feature tensors produced by the edge sub-model are sent to the cloud via an imperfect communication channel. At the cloud side, parts of the feature tensor may be missing due to packet loss. In this paper we propose a method called Content-Adaptive Linear Tensor Completion (CALTeC) to recover the missing feature data. The proposed method is fast, data-adaptive, does not require pre-training, and produces better results than existing methods for tensor data recovery in collaborative intelligence.      
### 20.SignalNet: A Low Resolution Sinusoid Decomposition and Estimation Network  [ :arrow_down: ](https://arxiv.org/pdf/2106.05490.pdf)
>  The detection and estimation of sinusoids is a fundamental signal processing task for many applications related to sensing and communications. While algorithms have been proposed for this setting, quantization is a critical, but often ignored modeling effect. In wireless communications, estimation with low resolution data converters is relevant for reduced power consumption in wideband receivers. Similarly, low resolution sampling in imaging and spectrum sensing allows for efficient data collection. In this work, we propose SignalNet, a neural network architecture that detects the number of sinusoids and estimates their parameters from quantized in-phase and quadrature samples. We incorporate signal reconstruction internally as domain knowledge within the network to enhance learning and surpass traditional algorithms in mean squared error and Chamfer error. We introduce a worst-case learning threshold for comparing the results of our network relative to the underlying data distributions. This threshold provides insight into why neural networks tend to outperform traditional methods and into the learned relationships between the input and output distributions. In simulation, we find that our algorithm is always able to surpass the threshold for three-bit data but often cannot exceed the threshold for one-bit data. We use the learning threshold to explain, in the one-bit case, how our estimators learn to minimize the distributional loss, rather than learn features from the data.      
### 21.Audiovisual transfer learning for audio tagging and sound event detection  [ :arrow_down: ](https://arxiv.org/pdf/2106.05408.pdf)
>  We study the merit of transfer learning for two sound recognition problems, i.e., audio tagging and sound event detection. Employing feature fusion, we adapt a baseline system utilizing only spectral acoustic inputs to also make use of pretrained auditory and visual features, extracted from networks built for different tasks and trained with external data. We perform experiments with these modified models on an audiovisual multi-label data set, of which the training partition contains a large number of unlabeled samples and a smaller amount of clips with weak annotations, indicating the clip-level presence of 10 sound categories without specifying the temporal boundaries of the active auditory events. For clip-based audio tagging, this transfer learning method grants marked improvements. Addition of the visual modality on top of audio also proves to be advantageous in this context. When it comes to generating transcriptions of audio recordings, the benefit of pretrained features depends on the requested temporal resolution: for coarse-grained sound event detection, their utility remains notable. But when more fine-grained predictions are required, performance gains are strongly reduced due to a mismatch between the problem at hand and the goals of the models from which the pretrained vectors were obtained.      
### 22.Generating MIMO Channels For 6G Virtual Worlds Using Ray-tracing Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2106.05377.pdf)
>  Some 6G use cases include augmented reality and high-fidelity holograms, with this information flowing through the network. Hence, it is expected that 6G systems can feed machine learning algorithms with such context information to optimize communication performance. This paper focuses on the simulation of 6G MIMO systems that rely on a 3-D representation of the environment as captured by cameras and eventually other sensors. We present new and improved Raymobtime datasets, which consist of paired MIMO channels and multimodal data. We also discuss tradeoffs between speed and accuracy when generating channels via ray-tracing. We finally provide results of beam selection and channel estimation to assess the impact of the improvements in the ray-tracing simulation methodology.      
### 23.5G MIMO Data for Machine Learning: Application to Beam-Selection using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.05370.pdf)
>  The increasing complexity of configuring cellular networks suggests that machine learning (ML) can effectively improve 5G technologies. Deep learning has proven successful in ML tasks such as speech processing and computational vision, with a performance that scales with the amount of available data. The lack of large datasets inhibits the flourish of deep learning applications in wireless communications. This paper presents a methodology that combines a vehicle traffic simulator with a raytracing simulator, to generate channel realizations representing 5G scenarios with mobility of both transceivers and objects. The paper then describes a specific dataset for investigating beams-election techniques on vehicle-to-infrastructure using millimeter waves. Experiments using deep learning in classification, regression and reinforcement learning problems illustrate the use of datasets generated with the proposed methodology      
### 24.Self-triggered min-max DMPC for asynchronous multi-agent systems with communication delays  [ :arrow_down: ](https://arxiv.org/pdf/2106.05358.pdf)
>  This paper studies the formation stabilization problem of asynchronous nonlinear multi-agent systems (MAS) subject to parametric uncertainties, external disturbances and bounded time-varying communication delays. A self-triggered min-max distributed model predictive control (DMPC) approach is proposed to handle these practical issues. At triggering instants, each agent solves a local min-max optimization problem based on local system states and predicted system states of neighbors, determines its next triggering instant and broadcasts its predicted state trajectory to its neighbors. As a result, the communication load is greatly alleviated while retaining robustness and comparable control performance compared to periodic algorithms. In order to handle time-varying delays, a novel consistency constraint is incorporated into each local optimization problem to restrict the deviation between the newest predicted states and previously broadcast predicted states. Consequently, each agent can utilize previously predicted states of its neighbors to achieve cooperation in the presence of time-varying delays and asynchronous communication induced by the distributed triggered scheduler. The recursive feasibility of the proposed algorithm and the closed-loop stability of MAS at triggering time instants are proven. Finally, numerical simulations are conducted to verify the efficiency of the proposed control method.      
### 25.First Demonstration of Field-Deployable Low Latency Hollow-core Cable Capable of Supporting &gt;1000km, 400Gb/s WDM Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2106.05343.pdf)
>  We report order-of-magnitude improvements in performance of field-deployable hollow-core fiber cables evidenced by a 38.4Tb/s (800Gb/s-x-48WDM-channels) 20.5km lab-trial using commercial terminal equipment and the demonstration of 1128km/126km reach in full-fill 400/800Gb/s WDM recirculating-loop experiments.      
### 26.Instantaneous Local Control Barrier Function: An Online Learning Approach for Collision Avoidance  [ :arrow_down: ](https://arxiv.org/pdf/2106.05341.pdf)
>  This paper presents a new formulation for provable safety under partial model uncertainty with guaranteed performance. A collision-free control strategy is developed for an uncertain multi-agent system that navigates through a prior unknown environment populated with static and dynamic obstacles. Our novel instantaneous local control barrier functions (IL-CBFs), constructed based on noisy data from limited horizon sensors online, are adopted to characterize potential agent-to-obstacle collisions. These data-based IL-CBFs further serve as the constraints of a quadratic programming (QP) optimization framework to generate safe control inputs. The required model information during the QP optimization process is identified within a finite time by our proposed parameter estimation update law. Numerical simulations based on the reach-avoid game and the formation keeping task are conducted to reveal the effectiveness of the proposed collision-free control strategy.      
### 27.Distributed Mean-Field Density Estimation for Large-Scale Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.05318.pdf)
>  This work studies how to estimate the mean-field density of large-scale systems in a distributed manner. Such problems are motivated by the recent swarm control technique that uses mean-field approximations to represent the collective effect of the swarm, wherein the mean-field density (and its gradient) is usually used in feedback control design. In the first part, we formulate the density estimation problem as a filtering problem of the associated mean-field partial differential equation (PDE), for which we employ kernel density estimation (KDE) to construct noisy observations and use filtering theory of PDE systems to design an optimal (centralized) density filter. It turns out that the covariance operator of observation noise depends on the unknown density. Hence, we use approximations for the covariance operator to obtain a suboptimal density filter, and prove that both the density estimates and their gradient are convergent and remain close to the optimal one using the notion of input-to-state stability (ISS). In the second part, we continue to study how to decentralize the density filter such that each agent can estimate the mean-field density based on only its own position and local information exchange with neighbors. We prove that the local density filter is also convergent and remains close to the centralized one in the sense of ISS. Simulation results suggest that the (centralized) suboptimal density filter is able to generate convergent density estimates, and the local density filter is able to converge and remain close to the centralized filter.      
### 28.PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2106.05933.pdf)
>  Recent work on speech self-supervised learning (speech SSL) demonstrated the benefits of scale in learning rich and transferable representations for Automatic Speech Recognition (ASR) with limited parallel data. It is then natural to investigate the existence of sparse and transferrable subnetworks in pre-trained speech SSL models that can achieve even better low-resource ASR performance. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, contrary to what LTH predicts, the discovered subnetworks yield minimal performance gain compared to the original dense network. In this work, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes subnetworks for much better ASR performance, while only requiring a single downstream finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks only needed to be slightly adjusted to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource English and multi-lingual ASR show (1) sparse subnetworks exist in pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. On the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We demonstrate PARP mitigates performance degradation in cross-lingual mask transfer, and investigate the possibility of discovering a single subnetwork for 10 spoken languages in one run.      
### 29.FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2106.05923.pdf)
>  Fetoscopy laser photocoagulation is a widely used procedure for the treatment of Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic multiple pregnancies due to placental vascular anastomoses. This procedure is particularly challenging due to limited field of view, poor manoeuvrability of the fetoscope, poor visibility due to fluid turbidity, variability in light source, and unusual position of the placenta. This may lead to increased procedural time and incomplete ablation, resulting in persistent TTTS. Computer-assisted intervention may help overcome these challenges by expanding the fetoscopic field of view through video mosaicking and providing better visualization of the vessel network. However, the research and development in this domain remain limited due to unavailability of high-quality data to encode the intra- and inter-procedure variability. Through the Fetoscopic Placental Vessel Segmentation and Registration (FetReg) challenge, we present a large-scale multi-centre dataset for the development of generalized and robust semantic segmentation and video mosaicking algorithms for the fetal environment with a focus on creating drift-free mosaics from long duration fetoscopy videos. In this paper, we provide an overview of the FetReg dataset, challenge tasks, evaluation metrics and baseline methods for both segmentation and registration. Baseline methods results on the FetReg dataset shows that our dataset poses interesting challenges, which can be modelled and competed for through our crowd-sourcing initiative of the FetReg challenge.      
### 30.SemSegLoss: A python package of loss functions for semantic segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2106.05844.pdf)
>  Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self-driving cars. In recent years, various research papers proposed different loss functions used in case of biased data, sparse segmentation, and unbalanced dataset. In this paper, we introduce SemSegLoss, a python package consisting of some of the well-known loss functions widely used for image segmentation. It is developed with the intent to help researchers in the development of novel loss functions and perform an extensive set of experiments on model architectures for various applications. The ease-of-use and flexibility of the presented package have allowed reducing the development time and increased evaluation strategies of machine learning models for semantic segmentation. Furthermore, different applications that use image segmentation can use SemSegLoss because of the generality of its functions. This wide range of applications will lead to the development and growth of AI across all industries.      
### 31.Niche to normality -- an interdisciplinary review of Vehicle-to-Grid  [ :arrow_down: ](https://arxiv.org/pdf/2106.05837.pdf)
>  Vehicle-to-Grid (V2G) capabilities, which enable electric vehicles to discharge power from their batteries for external uses, epitomise the coupling of the electricity and transport sectors. To thrive at the nexus of these large and well-established sectors V2G services must deliver technical, economic and social values to many stakeholders. In this Review we present a holistic and interdisciplinary examination of V2G services, highlighting the wide range of potential benefits as well as the challenges slowing the technology's evolution from niche trials to mainstream adoption. We find that benefits tend to be siloed by value proposition and stakeholder while the challenges tend to stem from stacking multiple values and connecting multiple stakeholders. Consequently, we identify key areas for future research, industry and policy activities that will accelerate and smoothen the realisation of V2G's potential as an essential pillar of clean transport-electricity systems.      
### 32.Improving multi-speaker TTS prosody variance with a residual encoder and normalizing flows  [ :arrow_down: ](https://arxiv.org/pdf/2106.05762.pdf)
>  Text-to-speech systems recently achieved almost indistinguishable quality from human speech. However, the prosody of those systems is generally flatter than natural speech, producing samples with low expressiveness. Disentanglement of speaker id and prosody is crucial in text-to-speech systems to improve on naturalness and produce more variable syntheses. This paper proposes a new neural text-to-speech model that approaches the disentanglement problem by conditioning a Tacotron2-like architecture on flow-normalized speaker embeddings, and by substituting the reference encoder with a new learned latent distribution responsible for modeling the intra-sentence variability due to the prosody. By removing the reference encoder dependency, the speaker-leakage problem typically happening in this kind of systems disappears, producing more distinctive syntheses at inference time. The new model achieves significantly higher prosody variance than the baseline in a set of quantitative prosody features, as well as higher speaker distinctiveness, without decreasing the speaker intelligibility. Finally, we observe that the normalized speaker embeddings enable much richer speaker interpolations, substantially improving the distinctiveness of the new interpolated speakers.      
### 33.Face mask detection using convolution neural network  [ :arrow_down: ](https://arxiv.org/pdf/2106.05728.pdf)
>  In the recent times, the Coronaviruses that are a big family of different viruses have become very common, contagious and dangerous to the whole human kind. It spreads human to human by exhaling the infection breath, which leaves droplets of the virus on different surface which is then inhaled by other person and catches the infection too. So it has become very important to protect ourselves and the people around us from this situation. We can take precautions such as social distancing, washing hands every two hours, using sanitizer, maintaining social distance and the most important wearing a mask. Public use of wearing a masks has become very common everywhere in the whole world now. From that the most affected and devastating condition is of India due to its extreme population in small area. This paper proposes a method to detect the face mask is put on or not for offices, or any other work place with a lot of people coming to work. We have used convolutional neural network for the same. The model is trained on a real world dataset and tested with live video streaming with a good accuracy. Further the accuracy of the model with different hyper parameters and multiple people at different distance and location of the frame is done.      
### 34.Outage Performance of $3$D Mobile UAV Caching for Hybrid Satellite-Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.05671.pdf)
>  In this paper, we consider a hybrid satellite-terrestrial network (HSTN) where a multiantenna satellite communicates with a ground user equipment (UE) with the help of multiple cache-enabled amplify-and-forward (AF) three-dimensional ($3$D) mobile unmanned aerial vehicle (UAV) relays. Herein, we employ the two fundamental most popular content (MPC) and uniform content (UC) caching schemes for two types of mobile UAV relays, namely fully $3$D and fixed height. Taking into account the multiantenna satellite links and the random $3$D distances between UAV relays and UE, we analyze the outage probability (OP) of considered system with MPC and UC caching schemes. We further carry out the corresponding asymptotic OP analysis to present the insights on achievable performance gains of two schemes for both types of $3$D mobile UAV relaying. Specifically, we show the following: (a) MPC caching dominates the UC and no caching schemes; (b) fully $3$D mobile UAV relaying outperforms its fixed height counterpart. We finally corroborate the theoretic analysis by simulations.      
### 35.U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2106.05642.pdf)
>  The unified streaming and non-streaming two-pass (U2) end-to-end model for speech recognition has shown great performance in terms of streaming capability, accuracy, real-time factor (RTF), and latency. In this paper, we present U2++, an enhanced version of U2 to further improve the accuracy. The core idea of U2++ is to use the forward and the backward information of the labeling sequences at the same time at training to learn richer information, and combine the forward and backward prediction at decoding to give more accurate recognition results. We also proposed a new data augmentation method called SpecSub to help the U2++ model to be more accurate and robust. Our experiments show that, compared with U2, U2++ shows faster convergence at training, better robustness to the decoding method, as well as consistent 5\% - 8\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we achieve a 4.63\% character error rate (CER) with a non-streaming setup and 5.05\% with a streaming setup with 320ms latency by U2++. To the best of our knowledge, 5.05\% is the best-published streaming result on the AISHELL-1 test set.      
### 36.C-GLISp: Preference-Based Global Optimization under Unknown Constraints with Applications to Controller Calibration  [ :arrow_down: ](https://arxiv.org/pdf/2106.05639.pdf)
>  Preference-based global optimization algorithms minimize an unknown objective function only based on whether the function is better, worse, or similar for given pairs of candidate optimization vectors. Such optimization problems arise in many real-life examples, such as finding the optimal calibration of the parameters of a control law. The calibrator can judge whether a particular combination of parameters leads to a better, worse, or similar closed-loop performance. Often, the search for the optimal parameters is also subject to unknown constraints. For example, the vector of calibration parameters must not lead to closed-loop instability. This paper extends an active preference learning algorithm introduced recently by the authors to handle unknown constraints. The proposed method, called C-GLISp, looks for an optimizer of the problem only based on preferences expressed on pairs of candidate vectors, and on whether a given vector is reported feasible and/or satisfactory. C-GLISp learns a surrogate of the underlying objective function based on the expressed preferences, and a surrogate of the probability that a sample is feasible and/or satisfactory based on whether each of the tested vectors was judged as such. The surrogate functions are used to propose a new candidate vector for testing and assessment iteratively. Numerical benchmarks and a semi-automated control calibration task demonstrate the effectiveness of C-GLISp, showing that it can reach near-optimal solutions within a small number of iterations.      
### 37.MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2106.05630.pdf)
>  Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.      
### 38.A Comparison and Combination of Unsupervised Blind Source Separation Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2106.05627.pdf)
>  Unsupervised blind source separation methods do not require a training phase and thus cannot suffer from a train-test mismatch, which is a common concern in neural network based source separation. The unsupervised techniques can be categorized in two classes, those building upon the sparsity of speech in the Short-Time Fourier transform domain and those exploiting non-Gaussianity or non-stationarity of the source signals. In this contribution, spatial mixture models which fall in the first category and independent vector analysis (IVA) as a representative of the second category are compared w.r.t. their separation performance and the performance of a downstream speech recognizer on a reverberant dataset of reasonable size. Furthermore, we introduce a serial concatenation of the two, where the result of the mixture model serves as initialization of IVA, which achieves significantly better WER performance than each algorithm individually and even approaches the performance of a much more complex neural network based technique.      
### 39.Multi-Dataset Benchmarks for Masked Identification using Contrastive Representation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.05596.pdf)
>  The COVID-19 pandemic has drastically changed accepted norms globally. Within the past year, masks have been used as a public health response to limit the spread of the virus. This sudden change has rendered many face recognition based access control, authentication and surveillance systems ineffective. Official documents such as passports, driving license and national identity cards are enrolled with fully uncovered face images. However, in the current global situation, face matching systems should be able to match these reference images with masked face images. As an example, in an airport or security checkpoint it is safer to match the unmasked image of the identifying document to the masked person rather than asking them to remove the mask. We find that current facial recognition techniques are not robust to this form of occlusion. <br>To address this unique requirement presented due to the current circumstance, we propose a set of re-purposed datasets and a benchmark for researchers to use. We also propose a contrastive visual representation learning based pre-training workflow which is specialized to masked vs unmasked face matching. We ensure that our method learns robust features to differentiate people across varying data collection scenarios. We achieve this by training over many different datasets and validating our result by testing on various holdout datasets. The specialized weights trained by our method outperform standard face recognition features for masked to unmasked face matching. We believe the provided synthetic mask generating code, our novel training approach and the trained weights from the masked face models will help in adopting existing face recognition systems to operate in the current global environment. We open-source all contributions for broader use by the research community.      
### 40.FRI-TEM: Time Encoding Sampling of Finite-Rate-of-Innovation Signals  [ :arrow_down: ](https://arxiv.org/pdf/2106.05564.pdf)
>  Classical sampling is based on acquiring signal amplitudes at specific points in time, with the minimal sampling rate dictated by the degrees of freedom in the signal. The samplers in this framework are controlled by a global clock that operates at a rate greater than or equal to the minimal sampling rate. At high sampling rates, clocks are power-consuming and prone to electromagnetic interference. An integrate-and-fire time encoding machine (IF-TEM) is an alternative power-efficient sampling mechanism which does not require a global clock. Here, the samples are irregularly spaced threshold-based samples. In this paper, we investigate the problem of sampling finite-rate-of-innovation (FRI) signals using an IF-TEM. We provide theoretical recovery guarantees for an FRI signal with arbitrary pulse shape and without any constraint on the minimum separation between the pulses. In particular, we show how to design a sampling kernel, IF-TEM, and recovery method such that the FRI signals are perfectly reconstructed. We then propose a modification to the sampling kernel to improve noise robustness. Our results enable designing low-cost and energy-efficient analog-to-digital converters for FRI signals.      
### 41.Independent Deeply Learned Tensor Analysis for Determined Audio Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2106.05529.pdf)
>  We address the determined audio source separation problem in the time-frequency domain. In independent deeply learned matrix analysis (IDLMA), it is assumed that the inter-frequency correlation of each source spectrum is zero, which is inappropriate for modeling nonstationary signals such as music signals. To account for the correlation between frequencies, independent positive semidefinite tensor analysis has been proposed. This unsupervised (blind) method, however, severely restrict the structure of frequency covariance matrices (FCMs) to reduce the number of model parameters. As an extension of these conventional approaches, we here propose a supervised method that models FCMs using deep neural networks (DNNs). It is difficult to directly infer FCMs using DNNs. Therefore, we also propose a new FCM model represented as a convex combination of a diagonal FCM and a rank-1 FCM. Our FCM model is flexible enough to not only consider inter-frequency correlation, but also capture the dynamics of time-varying FCMs of nonstationary signals. We infer the proposed FCMs using two DNNs: DNN for power spectrum estimation and DNN for time-domain signal estimation. An experimental result of separating music signals shows that the proposed method provides higher separation performance than IDLMA.      
### 42.Convex Risk Bounded Continuous-Time Trajectory Planning in Uncertain Nonconvex Environments  [ :arrow_down: ](https://arxiv.org/pdf/2106.05489.pdf)
>  In this paper, we address the trajectory planning problem in uncertain nonconvex static and dynamic environments that contain obstacles with probabilistic location, size, and geometry. To address this problem, we provide a risk bounded trajectory planning method that looks for continuous-time trajectories with guaranteed bounded risk over the planning time horizon. Risk is defined as the probability of collision with uncertain obstacles. Existing approaches to address risk bounded trajectory planning problems either are limited to Gaussian uncertainties and convex obstacles or rely on sampling-based methods that need uncertainty samples and time discretization. To address the risk bounded trajectory planning problem, we leverage the notion of risk contours to transform the risk bounded planning problem into a deterministic optimization problem. Risk contours are the set of all points in the uncertain environment with guaranteed bounded risk. The obtained deterministic optimization is, in general, nonlinear and nonconvex time-varying optimization. We provide convex methods based on sum-of-squares optimization to efficiently solve the obtained nonconvex time-varying optimization problem and obtain the continuous-time risk bounded trajectories without time discretization. The provided approach deals with arbitrary probabilistic uncertainties, nonconvex and nonlinear, static and dynamic obstacles, and is suitable for online trajectory planning problems.      
### 43.Differentiator for Noisy Sampled Signals with Best Worst-Case Accuracy  [ :arrow_down: ](https://arxiv.org/pdf/2106.05320.pdf)
>  This paper proposes a differentiator for sampled signals with bounded noise and bounded second derivative. It is based on a linear program derived from the available sample information and requires no further tuning beyond the noise and derivative bounds. A tight bound on the worst-case accuracy, i.e., the worst-case differentiation error, is derived, which is the best among all causal differentiators and is moreover shown to be obtained after a fixed number of sampling steps. Comparisons with the accuracy of existing high-gain and sliding-mode differentiators illustrate the obtained results.      
### 44.Raman spectral analysis of mixtures with one-dimensional convolutional neural network  [ :arrow_down: ](https://arxiv.org/pdf/2106.05316.pdf)
>  Recently, the combination of robust one-dimensional convolutional neural networks (1-D CNNs) and Raman spectroscopy has shown great promise in rapid identification of unknown substances with good accuracy. Using this technique, researchers can recognize a pure compound and distinguish it from unknown substances in a mixture. The novelty of this approach is that the trained neural network operates automatically without any pre- or post-processing of data. Some studies have attempted to extend this technique to the classification of pure compounds in an unknown mixture. However, the application of 1-D CNNs has typically been restricted to binary classifications of pure compounds. Here we will highlight a new approach in spectral recognition and quantification of chemical components in a multicomponent mixture. Two 1-D CNN models, RaMixNet I and II, have been developed for this purpose. The former is for rapid classification of components in a mixture while the latter is for quantitative determination of those constituents. In the proposed method, there is no limit to the number of compounds in a mixture. A data augmentation method is also introduced by adding random baselines to the Raman spectra. The experimental results revealed that the classification accuracy of RaMixNet I and II is 100% for analysis of unknown test mixtures; at the same time, the RaMixNet II model may achieve a regression accuracy of 88% for the quantification of each component.      
### 45.Decomposition, Compression, and Synthesis Based Video Coding: A Neural Approach Through Reference-Based Super Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2012.00650.pdf)
>  In pursuit of higher compression efficiency, a potential solution is the Down-Sampling based Video Coding (DSVC) where a input video is first downscaled for encoding at a relatively lower resolution, and then decoded frames are super-resolved through deep neural networks (DNNs). However, the coding gains are often bounded due to either uniform resolution sampling induced severe loss of high-frequency component, or insufficient information aggregation across non-uniformly sampled frames in existing DSVC methods. To address this, we propose to first decompose the input video into respective spatial texture frames (STFs) at its native spatial resolution that preserve the rich spatial details, and the other temporal motion frames (TMFs) at a lower spatial resolution that retain the motion smoothness; then compress them together using any popular video coder; and finally synthesize decoded STFs and TMFs for high-fidelity video reconstruction at the same resolution as its native input. This work simply applies the bicubic sampling in decomposition and Versatile Video Coding (VVC) compliant codec in compression, and puts the focus on the synthesis part. Such cross-resolution synthesis can be facilitated by Reference-based Super-Resolution (RefSR). Specifically, a motion compensation network (MCN) is devised on TMFs to efficiently align and aggregate temporal motion features that will be jointly processed with corresponding STFs using a texture transfer network (TTN) to better augment spatial details, by which the compression and resolution re-sampling noises can be effectively alleviated with better rate-distortion (R-D) efficiency, etc.      
