# ArXiv eess --Thu, 24 Jun 2021
### 1.Diabetic Retinopathy Detection using Ensemble Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.12545.pdf)
>  Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in diabetic patients. DR is a microvascular disease that affects the eye retina, which causes vessel blockage and therefore cuts the main source of nutrition for the retina tissues. Treatment for this visual disorder is most effective when it is detected in its earliest stages, as severe DR can result in irreversible blindness. Nonetheless, DR identification requires the expertise of Ophthalmologists which is often expensive and time-consuming. Therefore, automatic detection systems were introduced aiming to facilitate the identification process, making it available globally in a time and cost-efficient manner. However, due to the limited reliable datasets and medical records for this particular eye disease, the obtained predictions accuracies were relatively unsatisfying for eye specialists to rely on them as diagnostic systems. Thus, we explored an ensemble-based learning strategy, merging a substantial selection of well-known classification algorithms in one sophisticated diagnostic model. The proposed framework achieved the highest accuracy rates among all other common classification algorithms in the area. 4 subdatasets were generated to contain the top 5 and top 10 features of the Messidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies of 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original dataset respectively. The results imply the impressive performance of the subdataset, which significantly conduces to a less complex classification process      
### 2.Wavenumber-Division Multiplexing in Line-of-Sight Holographic MIMO Communications  [ :arrow_down: ](https://arxiv.org/pdf/2106.12531.pdf)
>  The ultimate performance of any wireless communication system is limited by electromagnetic principles and mechanisms. Motivated by this, we start from the first principles of wave propagation and consider a multiple-input multiple-output (MIMO) representation of a communication system between two spatially-continuous volumes of arbitrary shape and position. This is the concept of holographic MIMO communications. The analysis takes into account the electromagnetic noise field, generated by external sources, and the constraint on the physical radiated power. The electromagnetic MIMO model is particularized for a system with parallel linear sources and receivers in line-of-sight conditions. Inspired by orthogonal-frequency division-multiplexing, we assume that the spatially-continuous transmit currents and received fields are represented using the Fourier basis functions. In doing so, a wavenumber-division multiplexing (WDM) scheme is obtained whose properties are studied with the conventional tools of linear systems theory. Particularly, the interplay among the different system parameters (e.g., transmission range, wavelength, and sizes of source and receiver) in terms of number of communication modes and level of interference is studied. Due to the non-finite support of the electromagnetic channel, we prove that the interference-free condition can only be achieved when the receiver size grows to infinity. The spectral efficiency of WDM is evaluated via the singular-value decomposition architecture with water-filling and compared to that of a simplified architecture, which uses linear processing at the receiver and suboptimal power allocation.      
### 3.Weakly Supervised Registration of Prostate MRI and Histopathology Images  [ :arrow_down: ](https://arxiv.org/pdf/2106.12526.pdf)
>  The interpretation of prostate MRI suffers from low agreement across radiologists due to the subtle differences between cancer and normal tissue. Image registration addresses this issue by accurately mapping the ground-truth cancer labels from surgical histopathology images onto MRI. Cancer labels achieved by image registration can be used to improve radiologists' interpretation of MRI by training deep learning models for early detection of prostate cancer. A major limitation of current automated registration approaches is that they require manual prostate segmentations, which is a time-consuming task, prone to errors. This paper presents a weakly supervised approach for affine and deformable registration of MRI and histopathology images without requiring prostate segmentations. We used manual prostate segmentations and mono-modal synthetic image pairs to train our registration networks to align prostate boundaries and local prostate features. Although prostate segmentations were used during the training of the network, such segmentations were not needed when registering unseen images at inference time. We trained and validated our registration network with 135 and 10 patients from an internal cohort, respectively. We tested the performance of our method using 16 patients from the internal cohort and 22 patients from an external cohort. The results show that our weakly supervised method has achieved significantly higher registration accuracy than a state-of-the-art method run without prostate segmentations. Our deep learning framework will ease the registration of MRI and histopathology images by obviating the need for prostate segmentations.      
### 4.FoldIt: Haustral Folds Detection and Segmentation in Colonoscopy Videos  [ :arrow_down: ](https://arxiv.org/pdf/2106.12522.pdf)
>  Haustral folds are colon wall protrusions implicated for high polyp miss rate during optical colonoscopy procedures. If segmented accurately, haustral folds can allow for better estimation of missed surface and can also serve as valuable landmarks for registering pre-treatment virtual (CT) and optical colonoscopies, to guide navigation towards the anomalies found in pre-treatment scans. We present a novel generative adversarial network, FoldIt, for feature-consistent image translation of optical colonoscopy videos to virtual colonoscopy renderings with haustral fold overlays. A new transitive loss is introduced in order to leverage ground truth information between haustral fold annotations and virtual colonoscopy renderings. We demonstrate the effectiveness of our model on real challenging optical colonoscopy videos as well as on textured virtual colonoscopy videos with clinician-verified haustral fold annotations. All code and scripts to reproduce the experiments of this paper will be made available via our Computational Endoscopy Platform at <a class="link-external link-https" href="https://github.com/nadeemlab/CEP" rel="external noopener nofollow">this https URL</a>.      
### 5.High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.12511.pdf)
>  Left ventricular hypertrophy (LVH) results from chronic remodeling caused by a broad range of systemic and cardiovascular disease including hypertension, aortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early detection and characterization of LVH can significantly impact patient care but is limited by under-recognition of hypertrophy, measurement error and variability, and difficulty differentiating etiologies of LVH. To overcome this challenge, we present EchoNet-LVH - a deep learning workflow that automatically quantifies ventricular hypertrophy with precision equal to human experts and predicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model accurately measures intraventricular wall thickness (mean absolute error [MAE] 1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI 2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and classifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic cardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets from independent domestic and international healthcare systems, EchoNet-LVH accurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively) and detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy (AUC 0.89) on the domestic external validation site. Leveraging measurements across multiple heart beats, our model can more accurately identify subtle changes in LV geometry and its causal etiologies. Compared to human experts, EchoNet-LVH is fully automated, allowing for reproducible, precise measurements, and lays the foundation for precision diagnosis of cardiac hypertrophy. As a resource to promote further innovation, we also make publicly available a large dataset of 23,212 annotated echocardiogram videos.      
### 6.Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising  [ :arrow_down: ](https://arxiv.org/pdf/2106.12489.pdf)
>  Low-rankness is important in the hyperspectral image (HSI) denoising tasks. The tensor nuclear norm (TNN), defined based on the tensor singular value decomposition, is a state-of-the-art method to describe the low-rankness of HSI. However, TNN ignores some of the physical meanings of HSI in tackling the denoising tasks, leading to suboptimal denoising performance. In this paper, we propose the multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and the non-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the physical meaning of frequency components and reconsider their weights to improve the low-rank representation ability of TNN. Meanwhile, we also consider the correlation among two spatial dimensions and the spectral dimension of HSI and combine the above improvements to TNN to propose MFWTNN. Secondly, we use non-convex functions to approximate the rank function of the frequency tensor and propose the NonMFWTNN to relax the MFWTNN better. Besides, we adaptively choose bigger weights for slices mainly containing noise information and smaller weights for slices containing profile information. Finally, we develop the efficient alternating direction method of multiplier (ADMM) based algorithm to solve the proposed models, and the effectiveness of our models are substantiated in simulated and real HSI datasets.      
### 7.Bilateral Control of Teleoperators with Closed Architecture and Time-Varying Delay  [ :arrow_down: ](https://arxiv.org/pdf/2106.12470.pdf)
>  This paper investigates bilateral control of teleoperators with closed architecture and subjected to arbitrary bounded time-varying delay. A prominent challenge for bilateral control of such teleoperators lies in the closed architecture, especially in the context not involving interaction force/torque measurement. This yields the long-standing situation that most bilateral control rigorously developed in the literature is hard to be justified as applied to teleoperators with closed architecture. With a new class of dynamic feedback, we propose kinematic and adaptive dynamic controllers for teleoperators with closed architecture, and we show that the proposed kinematic and dynamic controllers are robust with respect to arbitrary bounded time-varying delay. In addition, by exploiting the input-output properties of an inverted form of the dynamics of robot manipulators with closed architecture, we remove the assumption of uniform exponential stability of a linear time-varying system due to the adaptation to the gains of the inner controller in demonstrating stability of the presented adaptive dynamic control. The application of the proposed approach is illustrated by the experimental results using a Phantom Omni and a UR10 robot.      
### 8.Decentralized Linear MMSE Equalizer Under Colored Noise for Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.12416.pdf)
>  Conventional uplink equalization in massive MIMO systems relies on a centralized baseband processing architecture. However, as the number of base station antennas increases, centralized baseband processing architectures encounter two bottlenecks, i.e., the tremendous data interconnection and the high-dimensional computation. To tackle these obstacles, decentralized baseband processing was proposed for uplink equalization, but only applicable to the scenarios with unpractical white Gaussian noise assumption. This paper presents an uplink linear minimum mean-square error (L-MMSE) equalization method in the daisy chain decentralized baseband processing architecture under colored noise assumption. The optimized L-MMSE equalizer is derived by exploiting the block coordinate descent method, which shows near-optimal performance both in theoretical and simulation while significantly mitigating the bottlenecks.      
### 9.STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.12407.pdf)
>  Fetal motion is unpredictable and rapid on the scale of conventional MR scan times. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and dynamics of fetal function, is limited to fast imaging techniques with compromises in image quality and resolution. Super-resolution for dynamic fetal MRI is still a challenge, especially when multi-oriented stacks of image slices for oversampling are not available and high temporal resolution for recording the dynamics of the fetus or placenta is desired. Further, fetal motion makes it difficult to acquire high-resolution images for supervised learning methods. To address this problem, in this work, we propose STRESS (Spatio-Temporal Resolution Enhancement with Simulated Scans), a self-supervised super-resolution framework for dynamic fetal MRI with interleaved slice acquisitions. Our proposed method simulates an interleaved slice acquisition along the high-resolution axis on the originally acquired data to generate pairs of low- and high-resolution images. Then, it trains a super-resolution network by exploiting both spatial and temporal correlations in the MR time series, which is used to enhance the resolution of the original data. Evaluations on both simulated and in utero data show that our proposed method outperforms other self-supervised super-resolution methods and improves image quality, which is beneficial to other downstream tasks and evaluations.      
### 10.Mitigating the Impact of Distributed Generations on Relay Coordination Using Fault Current Limiters  [ :arrow_down: ](https://arxiv.org/pdf/2106.12406.pdf)
>  The use of distributed generation resources, in addition to considerable benefits, causes some problems in the power system. One of the most critical problems in the case of disruption is increasing short-circuit current level in grids, which leads to change the protection devices settings in the downstream and upstream grid. By using fault current limiters (FCL), short-circuit currents in grids with distributed generation can be reduced to acceptable levels, so there is no needed to change the protection relays settings of the downstream grid (including distributed generations). However, by locating the FCL in the tie-feeder, the downstream grid is not more effective than the upstream grid and thus its reliability indices also will be changed. Therefore, this paper shows that by locating the unidirectional fault current limiter (UFCL) in the tie-feeder, the necessity of changing in the relay protection settings of upstream grids is prevented. In this paper, the proposed method is implemented, and its efficiency is reported in six scenarios.      
### 11.Gaussian Process-based Model Predictive Controller for Connected Vehicles with Uncertain Wireless Channel  [ :arrow_down: ](https://arxiv.org/pdf/2106.12366.pdf)
>  In this paper, we present a data-driven Model Predictive Controller that leverages a Gaussian Process to generate optimal motion policies for connected autonomous vehicles in regions with uncertainty in the wireless channel. The communication channel between the vehicles of a platoon can be easily influenced by numerous factors, e.g. the surrounding environment, and the relative states of the connected vehicles, etc. In addition, the trajectories of the vehicles depend significantly on the motion policies of the preceding vehicle shared via the wireless channel and any delay can impact the safety and optimality of its performance. In the presented algorithm, Gaussian Process learns the wireless channel model and is involved in the Model Predictive Controller to generate a control sequence that not only minimizes the conventional motion costs, but also minimizes the estimated delay of the wireless channel in the future. This results in a farsighted controller that maximizes the amount of transferred information beyond the controller's time horizon, which in turn guarantees the safety and optimality of the generated trajectories in the future. To decrease computational cost, the algorithm finds the reachable set from the current state and focuses on that region to minimize the size of the kernel matrix and related calculations. In addition, we present an efficient recursive approach to decrease the time complexity of developing the data-driven model and involving it in Model Predictive Control. We demonstrate the capability of the presented algorithm in a simulated scenario.      
### 12.Optimal Transmission Switching Problem Solving:Parallelization and Benchmarks  [ :arrow_down: ](https://arxiv.org/pdf/2106.12331.pdf)
>  The optimal transmission switching problem (OTSP) is an established problem of changing a power grid's topology to obtain an improved operation by controlling the switching status of transmission lines. This problem was proven to be NP-hard. Thus, different solution techniques have been proposed, that each has its advantages. These include mixed-integer formulations, which can be solved using branch-and-bound algorithms to obtain global optimal solutions, as well as heuristics methods. Whether optimal solutions can be found, however, largely depends on the specific size of a problem instance. <br>In this paper, we solve the full OTSP alongside reduced instances of it. A parallel algorithmic architecture is proposed where full OTSP formulation is run in parallel to another process that generates solutions to be injected into the solution procedure of the full OTSP during run time. Our method is tested on 14 instances from the pglib-opf library --the largest problem consisting of 13659 buses and 20467 branches. Our results show a good performance for large problem instances. If no optimal solution can be obtained for a test case within a few seconds, improvements can be reported consistently over off-the-shelf solvers' stock performance.      
### 13.Learning from Pseudo Lesion: A Self-supervised Framework for COVID-19 Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2106.12313.pdf)
>  The Coronavirus disease 2019 (COVID-19) has rapidly spread all over the world since its first report in December 2019 and thoracic computed tomography (CT) has become one of the main tools for its diagnosis. In recent years, deep learning-based approaches have shown impressive performance in myriad image recognition tasks. However, they usually require a large number of annotated data for training. Inspired by Ground Glass Opacity (GGO), a common finding in COIVD-19 patient's CT scans, we proposed in this paper a novel self-supervised pretraining method based on pseudo lesions generation and restoration for COVID-19 diagnosis. We used Perlin noise, a gradient noise based mathematical model, to generate lesion-like patterns, which were then randomly pasted to the lung regions of normal CT images to generate pseudo COVID-19 images. The pairs of normal and pseudo COVID-19 images were then used to train an encoder-decoder architecture based U-Net for image restoration, which does not require any labelled data. The pretrained encoder was then fine-tuned using labelled data for COVID-19 diagnosis task. Two public COVID-19 diagnosis datasets made up of CT images were employed for evaluation. Comprehensive experimental results demonstrated that the proposed self-supervised learning approach could extract better feature representation for COVID-19 diagnosis and the accuracy of the proposed method outperformed the supervised model pretrained on large scale images by 6.57% and 3.03% on SARS-CoV-2 dataset and Jinan COVID-19 dataset, respectively.      
### 14.Arbitrary Angle of Arrival in Radar Target Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2106.12308.pdf)
>  Automotive radar sensors play a key role in the current development of autonomous driving. Their ability to detect objects even under adverse conditions makes them indispensable for environment-sensing tasks in autonomous vehicles. The thorough and in-place validation of radar sensors demands for an integrative test system. Radar Target Simulators (RTS) are capable of performing over-the-air validation tests by creating artificial radar echos that are perceived as targets by the radar under test (RuT). Since the authenticity and credibility of these targets is based on the accuracy with which they are generated, their simulated position must be arbitrarily adjustable. In this paper, a new approach to synthesize virtual radar targets at an arbitrary angle of arrival is presented. The concept is based on the superposition of the returning signals of two adjacent RTS channels. A theoretical model describing the basic principle and its constraints is developed. A measurement campaign is conducted that verifies the practical functionality of the proposed scheme.      
### 15.Synthesis of Maximally Permissive Covert Attackers Against Unknown Supervisors by Using Observations  [ :arrow_down: ](https://arxiv.org/pdf/2106.12268.pdf)
>  In this paper, we consider the problem of synthesis of maximally permissive covert damage-reachable attackers in the setup where the model of the supervisor is unknown to the adversary but the adversary has recorded a (prefix-closed) finite set of observations of the runs of the closed-loop system. The synthesized attacker needs to ensure both the damage-reachability and the covertness against all the supervisors which are consistent with the given set of observations. There is a gap between the de facto maximal permissiveness, assuming the model of the supervisor is known, and the maximal permissiveness that can be attained with a limited knowledge of the model of the supervisor, from the adversary's point of view. We consider the setup where the attacker can exercise sensor replacement/deletion attacks and actuator enablement/disablement attacks. The solution methodology proposed in this work is to reduce the synthesis of maximally permissive covert damage-reachable attackers, given the model of the plant and the finite set of observations, to the synthesis of maximally permissive safe supervisors for certain transformed plant, which shows the decidability of the observation-assisted covert attacker synthesis problem. The effectiveness of our approach is illustrated on a water tank example adapted from the literature.      
### 16.Harmonic Power-Flow Study of Polyphase Grids with Converter-Interfaced Distributed Energy Resources, Part II: Model Library and Validation  [ :arrow_down: ](https://arxiv.org/pdf/2106.12255.pdf)
>  In Part I, a method for the Harmonic Power-Flow (HPF) study of three-phase power grids with Converter-Interfaced Distributed Energy Resources (CIDERs) is proposed. The method is based on generic and modular representations of the grid and the CIDERs, and explicitly accounts for coupling between harmonics. In Part II, the HPF method is validated. First, the applicability of the modeling framework is demonstrated on typical grid-forming and grid-following CIDERs. Then, the HPF method is implemented in Matlab and compared against time-domain simulations with Simulink. The accuracy of the models and the performance of the solution algorithm are assessed for individual resources and a modified version of the CIGRÉ low-voltage benchmark microgrid (i.e., with additional unbalanced components). The observed maximum errors are 6.3E-5 p.u. w.r.t. voltage magnitude, 1.3E-3 p.u. w.r.t. current magnitude, and 0.9 deg w.r.t. phase. Moreover, the scalability of the method is assessed w.r.t. the number of CIDERs and the maximum harmonic order ($\leqslant$25). For the maximum problem size, the execution time of the HPF method is 6.52 sec, which is 5 times faster than the time-domain simulation. The convergence of the method is robust w.r.t. the choice of the initial point, and multiplicity of solutions has not been observed.      
### 17.Harmonic Power-Flow Study of Polyphase Grids with Converter-Interfaced Distributed Energy Resources, Part I: Modelling Framework and Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2106.12253.pdf)
>  Power distribution systems are experiencing a large-scale integration of Converter-Interfaced Distributed Energy Resources (CIDERs). This complicates the analysis and mitigation of harmonics, whose creation and propagation are facilitated by the interactions of converters and their controllers through the grid. In this paper, a method for the calculation of the so-called Harmonic Power-Flow (HPF) in three-phase grids with CIDERs is proposed. The distinguishing feature of this HPF method is the generic and modular representation of the system components. Notably, as opposed to most of the existing approaches, the coupling between harmonics is explicitly considered. The HPF problem is formulated by combining the hybrid nodal equations of the grid with the closed-loop transfer functions of the CIDERs, and solved using the Newton-Raphson method. The grid components are characterized by compound electrical parameters, which allow to represent both transposed or non-transposed lines. The CIDERs are represented by modular linear time-periodic systems, which allows to treat both grid-forming and grid-following control laws. The method's accuracy and computational efficiency are confirmed via time-domain simulations of the CIGRÉ low-voltage benchmark microgrid. This paper is divided in two parts, which focus on the development (Part I) and the validation (Part II) of the proposed method.      
### 18.Lossless Point Cloud Attribute Compression with Normal-based Intra Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2106.12236.pdf)
>  The sparse LiDAR point clouds become more and more popular in various applications, e.g., the autonomous driving. However, for this type of data, there exists much under-explored space in the corresponding compression framework proposed by MPEG, i.e., geometry-based point cloud compression (G-PCC). In G-PCC, only the distance-based similarity is considered in the intra prediction for the attribute compression. In this paper, we propose a normal-based intra prediction scheme, which provides a more efficient lossless attribute compression by introducing the normals of point clouds. The angle between normals is used to further explore accurate local similarity, which optimizes the selection of predictors. We implement our method into the G-PCC reference software. Experimental results over LiDAR acquired datasets demonstrate that our proposed method is able to deliver better compression performance than the G-PCC anchor, with $2.1\%$ gains on average for lossless attribute coding.      
### 19.An alternative to PIs and PIDs: Intelligent proportional-derivative regulators  [ :arrow_down: ](https://arxiv.org/pdf/2106.12210.pdf)
>  This paper suggests to replace PIs and PIDs, which play a key role in control engineering, by intelligent Proportional-Derivative feedback loops, or iPDs, which are derived from model-free control. This standpoint is enhanced by a laboratory experiment.      
### 20.Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2106.12175.pdf)
>  Image denoising is of great importance for medical imaging system, since it can improve image quality for disease diagnosis and downstream image analyses. In a variety of applications, dynamic imaging techniques are utilized to capture the time-varying features of the subject, where multiple images are acquired for the same subject at different time points. Although signal-to-noise ratio of each time frame is usually limited by the short acquisition time, the correlation among different time frames can be exploited to improve denoising results with shared information across time frames. With the success of neural networks in computer vision, supervised deep learning methods show prominent performance in single-image denoising, which rely on large datasets with clean-vs-noisy image pairs. Recently, several self-supervised deep denoising models have been proposed, achieving promising results without needing the pairwise ground truth of clean images. In the field of multi-image denoising, however, very few works have been done on extracting correlated information from multiple slices for denoising using self-supervised deep learning methods. In this work, we propose Deformed2Self, an end-to-end self-supervised deep learning framework for dynamic imaging denoising. It combines single-image and multi-image denoising to improve image quality and use a spatial transformer network to model motion between different slices. Further, it only requires a single noisy image with a few auxiliary observations at different time frames for training and inference. Evaluations on phantom and in vivo data with different noise statistics show that our method has comparable performance to other state-of-the-art unsupervised or self-supervised denoising methods and outperforms under high noise levels.      
### 21.CxSE: Chest X-ray Slow Encoding CNN forCOVID-19 Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2106.12157.pdf)
>  The coronavirus continues to disrupt our everyday lives as it spreads at an exponential rate. It needs to be detected quickly in order to quarantine positive patients so as to avoid further spread. This work proposes a new convolutional neural network (CNN) architecture called 'slow Encoding CNN. The proposed model's best performance wrt Sensitivity, Positive Predictive Value (PPV) found to be SP=0.67, PP=0.98, SN=0.96, and PN=0.52 on AI AGAINST COVID19 - Screening X-ray images for COVID-19 Infections competition's test data samples. SP and PP stand for the Sensitivity and PPV of the COVID-19 positive class, while PN and SN stand for the Sensitivity and PPV of the COVID-19 negative class.      
### 22.Terahertz Wireless Communications with Flexible Index Modulation Aided Pilot Design  [ :arrow_down: ](https://arxiv.org/pdf/2106.12146.pdf)
>  Terahertz (THz) wireless communication is envisioned as a promising technology, which is capable of providing ultra-high-rate transmission up to Terabit per second. However, some hardware imperfections, which are generally neglected in the existing literature concerning lower data rates and traditional operating frequencies, cannot be overlooked in the THz systems. Hardware imperfections usually consist of phase noise, in-phase/quadrature imbalance, and nonlinearity of power amplifier. Due to the time-variant characteristic of phase noise, frequent pilot insertion is required, leading to decreased spectral efficiency. In this paper, to address this issue, a novel pilot design strategy is proposed based on index modulation (IM), where the positions of pilots are flexibly changed in the data frame, and additional information bits can be conveyed by indices of pilots. Furthermore, a turbo receiving algorithm is developed, which jointly performs the detection of pilot indices and channel estimation in an iterative manner. It is shown that the proposed turbo receiver works well even under the situation where the prior knowledge of channel state information is outdated. Analytical and simulation results validate that the proposed schemes achieve significant enhancement of bit-error rate performance and channel estimation accuracy, whilst attaining higher spectral efficiency in comparison with its classical counterpart.      
### 23.A Hybrid Genetic-Fuzzy Controller for a 14-inches Astronomical Telescope Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2106.12075.pdf)
>  The performance of on telescope depend strongly on its operating conditions. During pointing the telescope can move at a relatively high velocity, and the system can tolerate trajectory position errors higher than during tracking. On the contrary, during tracking Alt-Az telescopes generally move slower but still in a large dynamic range. In this case, the position errors must be as close to zero as possible. Tracking is one of the essential factors that affect the quality of astronomical observations. In this paper, a hybrid Genetic-Fuzzy approach to control the movement of a two-link direct-drive Celestron telescope is introduced. The proposed controller uses the Genetic algorithm (GA) for optimizing a fuzzy logic controller (FLC) to improve the tracking of the 14-inches Celestron telescope of the Kottamia Astronomical Observatory (KAO). The fuzzy logic input is a vector of the position error and its rate of change, and the output is torque. The GA objective function used here is the Integral Time Absolute Error (ITAE). The proposed method is compared with a conventional Proportional-Differential (PD) controller, an optimized PD controller with a GA, and a Fuzzy controller. The results show the effectiveness of the proposed controller to improve the dynamic response of the overall system.      
### 24.HydroPower Plant Planning for Resilience Improvement of Power Systems using Fuzzy-Neural based Genetic Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2106.12042.pdf)
>  This paper will propose a novel technique for optimize hydropower plant in small scale based on load frequency control (LFC) which use self-tuning fuzzy Proportional- Derivative (PD) method for estimation and prediction of planning. Due to frequency is not controlled by any dump load or something else, so this power plant is under dynamic frequency variations that will use PD controller which optimize by fuzzy rules and then with neural deep learning techniques and Genetic Algorithm optimization. The main purpose of this work is because to maintain frequency in small-hydropower plant at nominal value. So, proposed controller means Fuzzy PD optimization with Genetic Algorithm will be used for LFC in small scale of hydropower system. The proposed schema can be used in different designation of both diesel generator and mini-hydropower system at low stream flow. It is also possible to use diesel generator at the hydropower system which can be turn off when Consumer demand is higher than electricity generation. The simulation will be done in MATLAB/Simulink to represent and evaluate the performance of this control schema under dynamic frequency variations. Spiking Neural Network (SNN) used as the main deep learning techniques to optimizing this load frequency control which turns into Deep Spiking Neural Network (DSNN). Obtained results represented that the proposed schema has robust and high-performance frequency control in comparison to other methods.      
### 25.Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2106.12556.pdf)
>  This paper deals with the problem of localization in a cellular network in a dense urban scenario. Global Navigation Satellite Systems typically perform poorly in urban environments, where the likelihood of line-of-sight conditions between the devices and the satellites is low, and thus alternative localization methods are required for good accuracy. We present a deep learning method for localization, based merely on pathloss, which does not require any increase in computation complexity at the user devices with respect to the device standard operations, unlike methods that rely on time of arrival or angle of arrival information. In a wireless network, user devices scan the base station beacon slots and identify the few strongest base station signals for handover and user-base station association purposes. In the proposed method, the user to be localized simply reports such received signal strengths to a central processing unit, which may be located in the cloud. For each base station we have good approximation of the pathloss at every location in a dense grid in the map. This approximation is provided by RadioUNet, a deep learning-based simulator of pathloss functions in urban environment, that we have previously proposed and published. Using the estimated pathloss radio maps of all base stations and the corresponding reported signal strengths, the proposed deep learning algorithm can extract a very accurate localization of the user. The proposed method, called LocUNet, enjoys high robustness to inaccuracies in the estimated radio maps. We demonstrate this by numerical experiments, which obtain state-of-the-art results.      
### 26.Fine-Tuning StyleGAN2 For Cartoon Face Generation  [ :arrow_down: ](https://arxiv.org/pdf/2106.12445.pdf)
>  Recent studies have shown remarkable success in the unsupervised image to image (I2I) translation. However, due to the imbalance in the data, learning joint distribution for various domains is still very challenging. Although existing models can generate realistic target images, it's difficult to maintain the structure of the source image. In addition, training a generative model on large data in multiple domains requires a lot of time and computer resources. To address these limitations, we propose a novel image-to-image translation method that generates images of the target domain by finetuning a stylegan2 pretrained model. The stylegan2 model is suitable for unsupervised I2I translation on unbalanced datasets; it is highly stable, produces realistic images, and even learns properly from limited data when applied with simple fine-tuning techniques. Thus, in this paper, we propose new methods to preserve the structure of the source images and generate realistic images in the target domain. The code and results are available at <a class="link-external link-https" href="https://github.com/happy-jihye/Cartoon-StyleGan2" rel="external noopener nofollow">this https URL</a>      
### 27.A new Video Synopsis Based Approach Using Stereo Camera  [ :arrow_down: ](https://arxiv.org/pdf/2106.12362.pdf)
>  In today's world, the amount of data produced in every field has increased at an unexpected level. In the face of increasing data, the importance of data processing has increased remarkably. Our resource topic is on the processing of video data, which has an important place in increasing data, and the production of summary videos. Within the scope of this resource, a new method for anomaly detection with object-based unsupervised learning has been developed while creating a video summary. By using this method, the video data is processed as pixels and the result is produced as a video segment. The process flow can be briefly summarized as follows. Objects on the video are detected according to their type, and then they are tracked. Then, the tracking history data of the objects are processed, and the classifier is trained with the object type. Thanks to this classifier, anomaly behavior of objects is detected. Video segments are determined by processing video moments containing anomaly behaviors. The video summary is created by extracting the detected video segments from the original video and combining them. The model we developed has been tested and verified separately for single camera and dual camera systems.      
### 28.Computation Rate Maximization for Multiuser Mobile Edge Computing Systems With Dynamic Energy Arrivals  [ :arrow_down: ](https://arxiv.org/pdf/2106.12338.pdf)
>  This paper considers an energy harvesting (EH) based multiuser mobile edge computing (MEC) system, where each user utilizes the harvested energy from renewable energy sources to execute its computation tasks via computation offloading and local computing. Towards maximizing the system's weighted computation rate (i.e., the number of weighted users' computing bits within a finite time horizon) subject to the users' energy causality constraints due to dynamic energy arrivals, the decision for joint computation offloading and local computing over time is optimized {\em over time}. Assuming that the profile of channel state information and dynamic task arrivals at the users is known in advance, the weighted computation rate maximization problem becomes a convex optimization problem. Building on the Lagrange duality method, the well-structured optimal solution is analytically obtained. Both the users' local computing and offloading rates are shown to have a monotonically increasing structure. Numerical results show that the proposed design scheme can achieve a significant performance gain over the alternative benchmark schemes.      
### 29.Laboratory demonstration of the local oscillator concept for the Event Horizon Imager  [ :arrow_down: ](https://arxiv.org/pdf/2106.12316.pdf)
>  Black hole imaging challenges the 3rd generation space VLBI, the Very Long Baseline Interferometry, to operate on a 500 GHz band. The coherent integration timescale needed here is of 450 s though the available space oscillators cannot offer more than 10 s. Self-calibration methods might solve this issue in an interferometer formed by 3 antenna/satellite system, but the need in the 3rd satellite increases mission costs. A frequency transfer is of special interest to alleviate both performance and cost issues. A concept of 2-way optical frequency transfer is examined to investigate its suitability to enable space-to-space interferometry, in particular, to image the 'shadows' of black holes from space. The concept, promising on paper, has been demonstrated by tests. The laboratory test set-up is presented and the verification of the temporal stability using standard analysis tool as TimePod is given. The resulting Allan Deviation is dominated by the 1/$\tau$ phase noise trend since the frequency transfer timescale of interest is shorter than 0.2 s. This trend continues into longer integration times, as proven by the longest tests spanning over a few hours. The Allan Deviation between derived 103.2 GHz oscillators is $1.1\times10^{-14}/\tau$ within 10 ms &lt; $\tau$ &lt; 1,000 s that degrades twice towards the longest delay 0.2 s. The worst case satisfies the requirement with a margin of an order of magnitude. The obtained coherence in range of 0.997-0.9998 is beneficial for space VLBI at 557 GHz. The result is of special interest to future science missions for black hole imaging from space.      
### 30.Unsupervised Speech Enhancement using Dynamical Variational Auto-Encoders  [ :arrow_down: ](https://arxiv.org/pdf/2106.12271.pdf)
>  Dynamical variational auto-encoders (DVAEs) are a class of deep generative models with latent variables, dedicated to time series data modeling. DVAEs can be considered as extensions of the variational autoencoder (VAE) that include the modeling of temporal dependencies between successive observed and/or latent vectors in data sequences. Previous work has shown the interest of DVAEs and their better performance over the VAE for speech signals (spectrogram) modeling. Independently, the VAE has been successfully applied to speech enhancement in noise, in an unsupervised noise-agnostic set-up that does not require the use of a parallel dataset of clean and noisy speech samples for training, but only requires clean speech signals. In this paper, we extend those works to DVAE-based single-channel unsupervised speech enhancement, hence exploiting both speech signals unsupervised representation learning and dynamics modeling. We propose an unsupervised speech enhancement algorithm based on the most general form of DVAEs, that we then adapt to three specific DVAE models to illustrate the versatility of the framework. More precisely, we combine DVAE-based speech priors with a noise model based on nonnegative matrix factorization, and we derive a variational expectation-maximization (VEM) algorithm to perform speech enhancement. Experimental results show that the proposed approach based on DVAEs outperforms its VAE counterpart and a supervised speech enhancement baseline.      
### 31.Sentinel-1 and Sentinel-2 Spatio-Temporal Data Fusion for Clouds Removal  [ :arrow_down: ](https://arxiv.org/pdf/2106.12226.pdf)
>  The abundance of clouds, located both spatially and temporally, often makes remote sensing applications with optical images difficult or even impossible. In this manuscript, a novel method for clouds-corrupted optical image restoration has been presented and developed, based on a joint data fusion paradigm, where three deep neural networks have been combined in order to fuse spatio-temporal features extracted from Sentinel-1 and Sentinel-2 time-series of data. It is worth highlighting that both the code and the dataset have been implemented from scratch and made available to interested research for further analysis and investigation.      
### 32.A General Lotto game with asymmetric budget uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2106.12133.pdf)
>  We consider General Lotto games of asymmetric information where one player's resource endowment is randomly assigned one of two possible values, and the assignment is not revealed to the opponent. We completely characterize the Bayes-Nash equilibria for two such formulations -- namely, one in which the opponent's endowment is fixed and common knowledge, and another where the opponent has a per-unit cost to utilize resources. We then highlight the impact these characterizations have on resource allocation problems involving a central commander that decides how to assign available resources to two sub-colonels competing in separate Lotto games against respective opponents. We find that randomized assignments, which induce the Bayesian game interactions, do not offer strategic advantages over deterministic ones when the opponents have fixed resource endowments. However, this is not the case when the opponents have per-unit costs to utilize resources. We find the optimal randomized assignment strategy can actually improve the commander's payoff two-fold when compared to optimal deterministic assignments, and four-fold in settings where the commander also pays a per-unit cost for resources.      
### 33.Enrollment-less training for personalized voice activity detection  [ :arrow_down: ](https://arxiv.org/pdf/2106.12132.pdf)
>  We present a novel personalized voice activity detection (PVAD) learning method that does not require enrollment data during training. PVAD is a task to detect the speech segments of a specific target speaker at the frame level using enrollment speech of the target speaker. Since PVAD must learn speakers' speech variations to clarify the boundary between speakers, studies on PVAD used large-scale datasets that contain many utterances for each speaker. However, the datasets to train a PVAD model are often limited because substantial cost is needed to prepare such a dataset. In addition, we cannot utilize the datasets used to train the standard VAD because they often lack speaker labels. To solve these problems, our key idea is to use one utterance as both a kind of enrollment speech and an input to the PVAD during training, which enables PVAD training without enrollment speech. In our proposed method, called enrollment-less training, we augment one utterance so as to create variability between the input and the enrollment speech while keeping the speaker identity, which avoids the mismatch between training and inference. Our experimental results demonstrate the efficacy of the method.      
### 34.The Rate of Convergence of Variation-Constrained Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.12068.pdf)
>  Multi-layer feedforward networks have been used to approximate a wide range of nonlinear functions. An important and fundamental problem is to understand the learnability of a network model through its statistical risk, or the expected prediction error on future data. To the best of our knowledge, the rate of convergence of neural networks shown by existing works is bounded by at most the order of $n^{-1/4}$ for a sample size of $n$. In this paper, we show that a class of variation-constrained neural networks, with arbitrary width, can achieve near-parametric rate $n^{-1/2+\delta}$ for an arbitrarily small positive constant $\delta$. It is equivalent to $n^{-1 +2\delta}$ under the mean squared error. This rate is also observed by numerical experiments. The result indicates that the neural function space needed for approximating smooth functions may not be as large as what is often perceived. Our result also provides insight to the phenomena that deep neural networks do not easily suffer from overfitting when the number of neurons and learning parameters rapidly grow with $n$ or even surpass $n$. We also discuss the rate of convergence regarding other network parameters, including the input dimension, network layer, and coefficient norm.      
### 35.Experimental Quantum Computing to Solve Network DC Power Flow Problem  [ :arrow_down: ](https://arxiv.org/pdf/2106.12032.pdf)
>  Practical quantum computing applications to power grids are nonexistent at the moment. This paper investigates how a fundamental grid problem, namely DC power flow, can be solved using quantum computing. Power flow is the most widely used power system analysis technique, either as a stand-alone application or embedded in other applications; therefore, its fast and accurate solution is of utmost significance for grid operators. We base our studies on the Harrow-Hassidim-Lloyd (HHL) quantum algorithm, which has a proven theoretical speedup over classical algorithms in solving a system of linear equations. Practical studies on a quantum computer are conducted using the WSCC 9-bus system.      
