# ArXiv eess --Tue, 29 Jun 2021
### 1.Progressive Joint Low-light Enhancement and Noise Removal for Raw Images  [ :arrow_down: ](https://arxiv.org/pdf/2106.14844.pdf)
>  Low-light imaging on mobile devices is typically challenging due to insufficient incident light coming through the relatively small aperture, resulting in a low signal-to-noise ratio. Most of the previous works on low-light image processing focus either only on a single task such as illumination adjustment, color enhancement, or noise removal; or on a joint illumination adjustment and denoising task that heavily relies on short-long exposure image pairs collected from specific camera models, and thus these approaches are less practical and generalizable in real-world settings where camera-specific joint enhancement and restoration is required. To tackle this problem, in this paper, we propose a low-light image processing framework that performs joint illumination adjustment, color enhancement, and denoising. Considering the difficulty in model-specific data collection and the ultra-high definition of the captured images, we design two branches: a coefficient estimation branch as well as a joint enhancement and denoising branch. The coefficient estimation branch works in a low-resolution space and predicts the coefficients for enhancement via bilateral learning, whereas the joint enhancement and denoising branch works in a full-resolution space and performs joint enhancement and denoising in a progressive manner. In contrast to existing methods, our framework does not need to recollect massive data when being adapted to another camera model, which significantly reduces the efforts required to fine-tune our approach for practical usage. Through extensive experiments, we demonstrate its great potential in real-world low-light imaging applications when compared with current state-of-the-art methods.      
### 2.Gaussian Process Regression for Active Sensing Probabilistic Structural Health Monitoring: Experimental Assessment Across Multiple Damage and Loading Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2106.14841.pdf)
>  In the near future, Structural Health Monitoring (SHM) technologies will be capable of overcoming the drawbacks in the current maintenance and life-cycle management paradigms, namely: cost, increased downtime, less-than-optimal safety management paradigm and the limited applicability of fully-autonomous operations. In the context of SHM, one of the most challenging tasks is damage quantification. Current methods face accuracy and/or robustness issues when it comes to varying operating and environmental conditions. In addition, the damage/no-damage paradigm of current frameworks does not offer much information to maintainers on the ground for proper decision-making. In this study, a novel structural damage quantification framework is proposed based on widely-used Damage Indices (DIs) and Gaussian Process Regression Models (GPRMs). The novelty lies in calculating the probability of an incoming test DI point originating from a specific state, which allows for probability-educated decision-making. This framework is applied to three test cases: a Carbon Fiber-Reinforced Plastic (CFRP) coupon with attached weights as simulated damage, an aluminum coupon with a notch, and an aluminum coupon with attached weights as simulated damage under varying loading states. The state prediction method presented herein is applied to single-state quantification in the first two test cases, as well as the third one assuming the loading state is known. Finally, the proposed method is applied to the third test case assuming neither the damage size nor the load is known in order to predict both simultaneously from incoming DI test points. In applying this framework, two forms of GPRMs (standard and variational heteroscedastic) are used in order to critically assess their performances with respect to the three test cases.      
### 3.Hybrid zonotopes: a new set representation for reachability analysis of mixed logical dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.14831.pdf)
>  This article presents a new set representation named the hybrid zonotope. The hybrid zonotope is shown to be equivalent to $2^N$ constrained zonotopes through the addition of $N$ binary zonotope factors and is well-suited for the analysis of hybrid systems with both continuous and discrete states and inputs. The major contribution of this manuscript is a closed-form solution for exact forward reachable sets of linear mixed logical dynamical systems. This is given by a simple identity and does not require solving any optimization programs or taking set approximations. The proposed approach captures the worst-case exponential growth in the number of convex sets required to represent the nonconvex reachable set of a hybrid system while exhibiting only linear growth in the complexity of the hybrid zonotope set representation. To reduce both set representation complexity and the computational burden of reachability analysis, a binary tree is used to store only the combinations of binary factors of the hybrid zonotope that map to nonempty convex sets. The proposed approach is applied to an established benchmark example where the exact reachable set of a discrete-time hybrid system with six continuous and two discrete states is given by a single hybrid zonotope equivalent to the union of 657 constrained zonotopes, and is represented using only 283 continuous factors, 29 binary factors, and 177 linear equality constraints. Furthermore, the hybrid zonotope is closed under linear mappings, Minkowski sums, generalized intersections, and halfspace intersections.      
### 4.Analysis and Control of Autonomous Mobility-on-Demand Systems: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2106.14827.pdf)
>  Challenged by urbanization and increasing travel needs, existing transportation systems call for new mobility paradigms. In this article, we present the emerging concept of Autonomous Mobility-on-Demand, whereby centrally orchestrated fleets of autonomous vehicles provide mobility service to customers. We provide a comprehensive review of methods and tools to model and solve problems related to Autonomous Mobility-on-Demand systems. Specifically, we first identify problem settings for their analysis and control, both from the operational and the planning perspective. We then review modeling aspects, including transportation networks, transportation demand, congestion, operational constraints, and interactions with existing infrastructure. Thereafter, we provide a systematic analysis of existing solution methods and performance metrics, highlighting trends and trade-offs. Finally, we present various directions for further research.      
### 5.Analyzing Power Quality Implications of High Level Charging Rates of Electric Vehicle Within Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.14819.pdf)
>  This paper investigates the impact of the charging level of high penetration level of Electric Vehicles (EVs) on the power quality of the electricity distribution network. The EV owners tend to charge their EVs as fast as possible. The charging levels of EVs within the distribution network affect the voltage profile of buses of the network. In this paper, an exact Second-Order Cone Programming (SOCP) formulation of the full AC optimal power flow (ACOPF) problem of the distribution network is presented. The network includes solar generation units and EVs as Distributed Energy Resources (DERs). Different charging levels are considered to analyze the impact of EVs on the distribution network. The performance of the proposed model is illustrated for the modified IEEE-33 bus system for different charging levels for EVs. Besides, the impact of available solar power and battery degradation cost of EVs on the distribution network is investigated. It is illustrated that how EV charging will cause voltage deviation challenges for the distribution network.      
### 6.Channel Estimation for RIS-Aided Multiuser Millimeter-Wave Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.14792.pdf)
>  Channel estimation in the RIS-aided massive multiuser multiple-input single-output (MU-MISO) wireless communication systems is challenging due to the passive feature of RIS and the large number of reflecting elements that incur high channel estimation overhead. To address this issue, we propose a novel cascaded channel estimation strategy with low pilot overhead by exploiting the sparsity and the correlation of multiuser cascaded channels in millimeter-wave massive MISO systems. Based on the fact that the phsical positions of the BS, the RIS and users may not change in several or even tens of consecutive channel coherence blocks, we first estimate the full channel state information (CSI) including all the angle and gain information in the first coherence block, and then only re-estimate the channel gains in the remaining coherence blocks with much less pilot overhead. In the first coherence block, we propose a two-phase channel estimation method, in which the cascaded channel of one typical user is estimated in Phase I based on the linear correlation among cascaded paths, while the cascaded channels of other users are estimated in Phase II by utilizing the partial CSI of the common base station (BS)-RIS channel obtained in Phase I. The total theoretical minimum pilot overhead in the first coherence block is $8J-2+(K-1)\left\lceil (8J-2)/L\right\rceil $, where $K$, $L$ and $J$ denote the numbers of users, paths in the BS-RIS channel and paths in the RIS-user channel, respectively. In each of the remaining coherence blocks, the minimum pilot overhead is $JK$. Moreover, the training phase shift matrices at the RIS are optimized to improve the estimation performance.      
### 7.Mobile Microphone Array Speech Detection and Localization in Diverse Everyday Environments  [ :arrow_down: ](https://arxiv.org/pdf/2106.14787.pdf)
>  Joint sound event localization and detection (SELD) is an integral part of developing context awareness into communication interfaces of mobile robots, smartphones, and home assistants. For example, an automatic audio focus for video capture on a mobile phone requires robust detection of relevant acoustic events around the device and their direction. Existing SELD approaches have been evaluated using material produced in controlled indoor environments, or the audio is simulated by mixing isolated sounds to different spatial locations. This paper studies SELD of speech in diverse everyday environments, where the audio corresponds to typical usage scenarios of handheld mobile devices. In order to allow weighting the relative importance of localization vs. detection, we will propose a two-stage hierarchical system, where the first stage is to detect the target events, and the second stage is to localize them. <br>The proposed method utilizes convolutional recurrent neural network (CRNN) and is evaluated on a database of manually annotated microphone array recordings from various acoustic conditions. The array is embedded in a contemporary mobile phone form factor. The obtained results show good speech detection and localization accuracy of the proposed method in contrast to a non-hierarchical flat classification model.      
### 8.Tiled sparse coding in eigenspaces for the COVID-19 diagnosis in chest X-ray images  [ :arrow_down: ](https://arxiv.org/pdf/2106.14724.pdf)
>  The ongoing crisis of the COVID-19 (Coronavirus disease 2019) pandemic has changed the world. According to the World Health Organization (WHO), 4 million people have died due to this disease, whereas there have been more than 180 million confirmed cases of COVID-19. The collapse of the health system in many countries has demonstrated the need of developing tools to automatize the diagnosis of the disease from medical imaging. Previous studies have used deep learning for this purpose. However, the performance of this alternative highly depends on the size of the dataset employed for training the algorithm. In this work, we propose a classification framework based on sparse coding in order to identify the pneumonia patterns associated with different pathologies. Specifically, each chest X-ray (CXR) image is partitioned into different tiles. The most relevant features extracted from PCA are then used to build the dictionary within the sparse coding procedure. Once images are transformed and reconstructed from the elements of the dictionary, classification is performed from the reconstruction errors of individual patches associated with each image. Performance is evaluated in a real scenario where simultaneously differentiation between four different pathologies: control vs bacterial pneumonia vs viral pneumonia vs COVID-19. The accuracy when identifying the presence of pneumonia is 93.85%, whereas 88.11% is obtained in the 4-class classification context. The excellent results and the pioneering use of sparse coding in this scenario evidence the applicability of this approach as an aid for clinicians in a real-world environment.      
### 9.Weighted multi-level deep learning analysis and framework for processing breast cancer WSIs  [ :arrow_down: ](https://arxiv.org/pdf/2106.14708.pdf)
>  Prevention and early diagnosis of breast cancer (BC) is an essential prerequisite for the selection of proper treatment. The substantial pressure due to the increase of demand for faster and more precise diagnostic results drives for automatic solutions. In the past decade, deep learning techniques have demonstrated their power over several domains, and Computer-Aided (CAD) diagnostic became one of them. However, when it comes to the analysis of Whole Slide Images (WSI), most of the existing works compute predictions from levels independently. This is, however, in contrast to the histopathologist expert approach who requires to see a global architecture of tissue structures important in BC classification. <br>We present a deep learning-based solution and framework for processing WSI based on a novel approach utilizing the advantages of image levels. We apply the weighing of information extracted from several levels into the final classification of the malignancy. Our results demonstrate the profitability of global information with an increase of accuracy from 72.2% to 84.8%.      
### 10.Mixed-Spectrum Signals -- Discrete Approximations and Variance Expressions for Covariance Estimates  [ :arrow_down: ](https://arxiv.org/pdf/2106.14696.pdf)
>  The estimation of the covariance function of a stochastic process, or signal, is of integral importance for a multitude of signal processing applications. In this work, we derive closed-form expressions for the variance of covariance estimates for mixed-spectrum signals, i.e., spectra containing both absolutely continuous and singular parts. The results cover both finite-sample and asymptotic regimes, allowing for assessing the exact speed of convergence of estimates to their expectations, as well as their limiting behavior. As is shown, such covariance estimates may converge even for non-ergodic processes. Furthermore, we consider approximating signals with arbitrary spectral densities by sequences of singular spectrum, i.e., sinusoidal, processes, and derive the limiting behavior of covariance estimates as both the sample size and the number of sinusoidal components tend to infinity. We show that the asymptotic regime variance can be described by a time-frequency resolution product, with dramatically different behavior depending on how the sinusoidal approximation is constructed. In a few numerical examples we illustrate the theory and the corresponding implications for direction of arrival estimation.      
### 11.Anticipatory routing methods for an on-demand ridepooling mobility system  [ :arrow_down: ](https://arxiv.org/pdf/2106.14685.pdf)
>  One of the most relevant challenges regarding on-demand ridepooling relates to the spatial imbalances of the demand, which induce a mismatch between the position of the vehicles and the origins of the emerging requests. Most ridepooling models face this problem through rebalancing methods only, i.e., moving idle vehicles towards areas with high rejections rate, which is done independently from routing and vehicle-to-orders assignments, so that vehicles serving passengers (a large portion of the total fleet) remain unaffected. This paper introduces two types of techniques for anticipatory routing that affect how vehicles are assigned to users and how to route vehicles to serve such users, so that the whole operation of the system is modified to reach more efficient states for future requests. Both techniques do not require any assumption or exogenous knowledge about the future demand, as they depend only on current and recent requests. Firstly, we introduce rewards that reduce the cost of an assignment between a vehicle and a group of passengers if the vehicle gets routed towards a high-demand zone. Secondly, we include a small set of artificial requests, whose request times are in the near future and whose origins are sampled from a probability distribution that mimics observed generation rates. These artificial requests are to be assigned together with the real requests. <br>We test these techniques using a set of real rides from Manhattan. Introducing rewards can diminish the rejection rate to about nine-tenths of its original value. On the other hand, including future requests can reduce users' traveling times by about one-fifth, but increasing rejections. Both methods increase the vehicles-hour-traveled by about 10%. Spatial analysis reveals that vehicles are indeed moved towards the most demanded areas, such that the reduction in rejections rate is achieved mostly there.      
### 12.An Efficient Asynchronous Batch Bayesian Optimization Approach for Analog Circuit Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2106.14683.pdf)
>  In this paper, we propose EasyBO, an Efficient ASYnchronous Batch Bayesian Optimization approach for analog circuit synthesis. In this proposed approach, instead of waiting for the slowest simulations in the batch to finish, we accelerate the optimization procedure by asynchronously issuing the next query points whenever there is an idle worker. We introduce a new acquisition function that can better explore the design space for asynchronous batch Bayesian optimization. A new strategy is proposed to better balance the exploration and exploitation and guarantee the diversity of the query points. And a penalization scheme is proposed to further avoid redundant queries during the asynchronous batch optimization. The efficiency of optimization can thus be further improved. Compared with the state-of-the-art batch Bayesian optimization algorithm, EasyBO achieves up to 7.35 times speed-up without sacrificing the optimization results.      
### 13.FRaC: FMCW-Based Joint Radar-Communications System via Index Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2106.14671.pdf)
>  Dual function radar communications (DFRC) systems are attractive technologies for autonomous vehicles, which utilize electromagnetic waves to constantly sense the environment while simultaneously communicating with neighbouring devices. An emerging approach to implement DFRC systems is to embed information in radar waveforms via index modulation (IM). Implementation of DFRC schemes in vehicular systems gives rise to strict constraints in terms of cost, power efficiency, and hardware complexity. In this paper, we extend IM-based DFRC systems to utilize sparse arrays and frequency modulated continuous waveforms (FMCWs), which are popular in automotive radar for their simplicity and low hardware complexity. The proposed FMCW-based radar-communications system (FRaC) operates at reduced cost and complexity by transmitting with a reduced number of radio frequency modules, combined with narrowband FMCW signalling. This is achieved via array sparsification in transmission, formulating a virtual multiple-input multiple-output array by combining the signals in one coherent processing interval, in which the narrowband waveforms are transmitted in a randomized manner. Performance analysis and numerical results show that the proposed radar scheme achieves similar resolution performance compared with a wideband radar system operating with a large receive aperture, while requiring less hardware overhead. For the communications subsystem, FRaC achieves higher rates and improved error rates compared to dual-function signalling based on conventional phase modulation.      
### 14.ACN: Adversarial Co-training Network for Brain Tumor Segmentation with Missing Modalities  [ :arrow_down: ](https://arxiv.org/pdf/2106.14591.pdf)
>  Accurate segmentation of brain tumors from magnetic resonance imaging (MRI) is clinically relevant in diagnoses, prognoses and surgery treatment, which requires multiple modalities to provide complementary morphological and physiopathologic information. However, missing modality commonly occurs due to image corruption, artifacts, different acquisition protocols or allergies to certain contrast agents in clinical practice. Though existing efforts demonstrate the possibility of a unified model for all missing situations, most of them perform poorly when more than one modality is missing. In this paper, we propose a novel Adversarial Co-training Network (ACN) to solve this issue, in which a series of independent yet related models are trained dedicated to each missing situation with significantly better results. Specifically, ACN adopts a novel co-training network, which enables a coupled learning process for both full modality and missing modality to supplement each other's domain and feature representations, and more importantly, to recover the `missing' information of absent modalities. Then, two unsupervised modules, i.e., entropy and knowledge adversarial learning modules are proposed to minimize the domain gap while enhancing prediction reliability and encouraging the alignment of latent representations, respectively. We also adapt modality-mutual information knowledge transfer learning to ACN to retain the rich mutual information among modalities. Extensive experiments on BraTS2018 dataset show that our proposed method significantly outperforms all state-of-the-art methods under any missing situation.      
### 15.Learning to Sample: Data-Driven Sampling and Reconstruction of FRI Signals  [ :arrow_down: ](https://arxiv.org/pdf/2106.14500.pdf)
>  Finite-rate-of-innovation (FRI) signals are ubiquitous in applications such as radar, ultrasound, and time of flight imaging. Due to their finite degrees of freedom, FRI signals can be sampled at sub-Nyquist rates using appropriate sampling kernels and reconstructed using sparse-recovery algorithms. Typically, Fourier samples of the FRI signals are used for reconstruction. The reconstruction quality depends on the choice of Fourier samples and recovery method. In this paper, we consider to jointly optimize the choice of Fourier samples and reconstruction parameters. Our framework is a combination of a greedy subsampling algorithm and a learning-based sparse recovery method. Unlike existing techniques, the proposed algorithm can flexibly handle changes in the sampling rate and does not suffer from differentiability issues during training. Importantly, exact knowledge of the FRI pulse is not required. Numerical results show that, for a given number of samples, the proposed joint design leads to lower reconstruction error for FRI signals compared to independent data-driven design methods for both noisy and clean samples. Our learning to sample approach can be readily applied to other sampling setups as well including compressed sensing problems.      
### 16.Benchmarking convolutional neural networks for diagnosing Lyme disease from images  [ :arrow_down: ](https://arxiv.org/pdf/2106.14465.pdf)
>  Lyme disease is one of the most common infectious vector-borne diseases in the world. In the early stage, the disease manifests itself in most cases with erythema migrans (EM) skin lesions. Better diagnosis of these early forms would allow improving the prognosis by preventing the transition to a severe late form thanks to appropriate antibiotic therapy. Recent studies show that convolutional neural networks (CNNs) perform very well to identify skin lesions from the image but, there is not much work for Lyme disease prediction from EM lesion images. The main objective of this study is to extensively analyze the effectiveness of CNNs for diagnosing Lyme disease from images and to find out the best CNN architecture for the purpose. There is no publicly available EM image dataset for Lyme disease prediction mainly because of privacy concerns. In this study, we utilized an EM dataset consisting of images collected from Clermont-Ferrand University Hospital Center (CF-CHU) of France and the internet. CF-CHU collected the images from several hospitals in France. This dataset was labeled by expert dermatologists and infectiologists from CF-CHU. First, we benchmarked this dataset for twenty-three well-known CNN architectures in terms of predictive performance metrics, computational complexity metrics, and statistical significance tests. Second, to improve the performance of the CNNs, we used transfer learning from ImageNet pre-trained models as well as pre-trained the CNNs with the skin lesion dataset "Human Against Machine with 10000 training images (HAM1000)". In that process, we searched for the best performing number of layers to unfreeze during transfer learning fine-tuning for each of the CNNs. Third, for model explainability, we utilized Gradient-weighted Class Activation Mapping to visualize the regions of input that are significant to the CNNs for making predictions. Fourth, we provided guidelines for model selection based on predictive performance and computational complexity. Our study confirmed the effectiveness and potential of even some lightweight CNNs to be used for Lyme disease pre-scanner mobile applications. We also made all the trained models publicly available at <a class="link-external link-https" href="https://dappem.limos.fr/download.html" rel="external noopener nofollow">this https URL</a>, which can be used by others for transfer learning and building pre-scanners for Lyme disease.      
### 17.Data-driven Model Predictive and Reinforcement Learning Based Control for Building Energy Management: a Survey  [ :arrow_down: ](https://arxiv.org/pdf/2106.14450.pdf)
>  Building energy management is one of the core problems in modern power grids to reduce energy consumption while ensuring occupants' comfort. However, the building energy management system (BEMS) is now facing more challenges and uncertainties with the increasing penetration of renewables and complicated interactions between humans and buildings. Classical model predictive control (MPC) has shown its capacity to reduce building energy consumption, but it suffers from labor-intensive modelling and complex on-line control optimization. Recently, with the growing accessibility to the building control and automation data, data-driven solutions have attracted more research interest. This paper presents a compact review of the recent advances in data-driven MPC and reinforcement learning based control methods for BEMS. The main challenges in these approaches and insights on the selection of a control method are discussed.      
### 18.A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images  [ :arrow_down: ](https://arxiv.org/pdf/2106.14403.pdf)
>  We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice images. In this framework, the slice images of a CT-scan volume are first proprocessed using segmentation techniques to filter out images of closed lung, and to remove the useless background. Then a resampling method is used to select one or multiple sets of a fixed number of slice images for training and validation. A 3D CNN network with BERT is used to classify this set of selected slice images. In this network, an embedding feature is also extracted. In cases where there are more than one set of slice images in a volume, the features of all sets are extracted and pooled into a global feature vector for the whole CT-scan volume. A simple multiple-layer perceptron (MLP) network is used to further classify the aggregated feature vector. The models are trained and evaluated on the provided training and validation datasets. On the validation dataset, the accuracy is 0.9278 and the F1 score is 0.9261.      
### 19.Armoured Fighting Vehicle Team Performance Prediction against Missile Attacks with Directed Energy Weapons  [ :arrow_down: ](https://arxiv.org/pdf/2106.14381.pdf)
>  A recent study has introduced a procedure to quantify the survivability of a team of armoured fighting vehicles when it is subjected to a single missile attack. In particular this study investigated the concept of collaborative active protection systems, focusing on the case where vehicle defence is provided by high power radio frequency directed energy weapons. The purpose of the current paper is to demonstrate how this analysis can be extended to account for more than one missile threat. This is achieved by introducing a jump stochastic process whose states represent the number of missiles defeated at a given time instant. Analysis proceeds through consideration of the sojourn times of this stochastic process, and it is shown how consideration of these jump times can be related to transition probabilities of the auxiliary stochastic process. The latter probabilities are then related to the probabilities of detection and disruption of missile threats. The sum of these sojourn times can then be used to quantify the survivability of the team at any given time instant. Due to the fact that there is much interest in the application of high energy lasers in the context of this paper, the numerical examples will thus focus on such directed energy weapons for armoured fighting vehicle team defence.      
### 20.AutoEKF: Scalable System Identification for COVID-19 Forecasting from Large-Scale GPS Data  [ :arrow_down: ](https://arxiv.org/pdf/2106.14357.pdf)
>  We present an Extended Kalman Filter framework for system identification and control of a stochastic high-dimensional epidemic model. The scale and severity of the COVID-19 emergency have highlighted the need for accurate forecasts of the state of the pandemic at a high resolution. Mechanistic compartmental models are widely used to produce such forecasts and assist in the design of control and relief policies. Unfortunately, the scale and stochastic nature of many of these models often makes the estimation of their parameters difficult. With the goal of calibrating a high dimensional COVID-19 model using low-level mobility data, we introduce a method for tractable maximum likelihood estimation that combines tools from Bayesian inference with scalable optimization techniques from machine learning. The proposed approach uses automatic backward-differentiation to directly compute the gradient of the likelihood of COVID-19 incidence and death data. The likelihood of the observations is estimated recursively using an Extended Kalman Filter and can be easily optimized using gradient-based methods to compute maximum likelihood estimators. Our compartmental model is trained using GPS mobility data that measures the mobility patterns of millions of mobile phones across the United States. We show that, after calibrating against incidence and deaths data from the city of Philadelphia, our model is able to produce an accurate 30-day forecast of the evolution of the pandemic.      
### 21.On the Design of an Insurance Mechanism for Reliability Differentiation in Electricity Markets  [ :arrow_down: ](https://arxiv.org/pdf/2106.14351.pdf)
>  Securing an adequate supply of dispatchable resources is critical for keeping a power system reliable under high penetrations of variable generation. Traditional resource adequacy mechanisms are poorly suited to exploiting the growing flexibility and heterogeneity of load enabled by advancements in distributed resource and control technology. To address these challenges this paper develops a resource adequacy mechanism for the electricity sector utilising insurance risk management frameworks that is adapted to a future with variable generation and flexible demand. The proposed design introduces a central insurance scheme with prudential requirements that align diverse consumer reliability preferences with the financial objectives of an insurer-of-last-resort. We illustrate the benefits of the scheme in (i) differentiating load by usage to enable better management of the system during times of extreme scarcity, (ii) incentivising incremental investment in generation infrastructure that is aligned with consumer reliability preferences and (iii) improving overall reliability outcomes for consumers.      
### 22.A Cooperative Game Theory-based Approach to Under-frequency Load Shedding Control  [ :arrow_down: ](https://arxiv.org/pdf/2106.14330.pdf)
>  This paper proposes a cooperative game theory-based under-frequency load shedding (UFLS) approach for frequency stability and control in power systems. UFLS is a crucial factor for frequency stability and control especially in power grids with high penetration of renewable energy sources and restructured power systems. Conventional UFLS methods, most of which are off-line, usually shed fixed amounts of predetermined loads based on a predetermined schedule which can lead to over or under curtailment of load. This paper presents a co-operative game theory-based two-stage strategy to effectively and precisely determine locations and amounts of loads to be shed for UFLS control. In the first stage, the total amount of loads to be shed, also referred to as deficit in generation or the disturbance power, is computed using the initial rate of change of frequency (ROCOF) referred to the equivalent inertial center. In the second stage, the Shapley value, one of the solution concepts of cooperative game theory, is used to determine load shedding amounts and locations. The proposed method is implemented on the reduced 9-bus 3-machine Western Electricity Coordinating Council (WECC) system and simulated on Real-time Digital Simulators (RTDS). The results show that the proposed UFLS approach can effectively return the system to normal state after disturbances.      
### 23.Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs  [ :arrow_down: ](https://arxiv.org/pdf/2106.14324.pdf)
>  In order to objectively assess new medical imaging technologies via computer-simulations, it is important to account for all sources of variability that contribute to image data. One important source of variability that can significantly limit observer performance is associated with the variability in the ensemble of objects to-be-imaged. This source of variability can be described by stochastic object models (SOMs), which are generative models that can be employed to sample from a distribution of to-be-virtually-imaged objects. It is generally desirable to establish SOMs from experimental imaging measurements acquired by use of a well-characterized imaging system, but this task has remained challenging. Deep generative neural networks, such as generative adversarial networks (GANs) hold potential for such tasks. To establish SOMs from imaging measurements, an AmbientGAN has been proposed that augments a GAN with a measurement operator. However, the original AmbientGAN could not immediately benefit from modern training procedures and GAN architectures, which limited its ability to be applied to realistically sized medical image data. To circumvent this, in this work, a modified AmbientGAN training strategy is proposed that is suitable for modern progressive or multi-resolution training approaches such as employed in the Progressive Growing of GANs and Style-based GANs. AmbientGANs established by use of the proposed training procedure are systematically validated in a controlled way by use of computer-simulated measurement data corresponding to a stylized imaging system. Finally, emulated single-coil experimental magnetic resonance imaging data are employed to demonstrate the methods under less stylized conditions.      
### 24.Knee Osteoarthritis Severity Prediction using an Attentive Multi-Scale Deep Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2106.14292.pdf)
>  Knee Osteoarthritis (OA) is a destructive joint disease identified by joint stiffness, pain, and functional disability concerning millions of lives across the globe. It is generally assessed by evaluating physical symptoms, medical history, and other joint screening tests like radiographs, Magnetic Resonance Imaging (MRI), and Computed Tomography (CT) scans. Unfortunately, the conventional methods are very subjective, which forms a barrier in detecting the disease progression at an early stage. This paper presents a deep learning-based framework, namely OsteoHRNet, that automatically assesses the Knee OA severity in terms of Kellgren and Lawrence (KL) grade classification from X-rays. As a primary novelty, the proposed approach is built upon one of the most recent deep models, called the High-Resolution Network (HRNet), to capture the multi-scale features of knee X-rays. In addition, we have also incorporated an attention mechanism to filter out the counterproductive features and boost the performance further. Our proposed model has achieved the best multiclass accuracy of 71.74% and MAE of 0.311 on the baseline cohort of the OAI dataset, which is a remarkable gain over the existing best-published works. We have also employed the Gradient-based Class Activation Maps (Grad-CAMs) visualization to justify the proposed network learning.      
### 25.Outage Performance Analysis of Widely Linear Receivers in Uplink Multi-user MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.14288.pdf)
>  This paper considers the application of widely linear (WL) receivers in an uplink multi-user system using real-valued modulation schemes, where the cellular base station (BS) with multiple antennas provides connectivity for randomly deployed single-antenna users. The targeted use case is massive machine type communication (mMTC) with grant-free access in the uplink, where the network is required to host a large number of low data rate devices transmitting in an uncoordinated fashion. Four types of WL receivers are investigated, namely the WL zero-forcing (ZF) and the WL minimum meansquared error (MMSE) receivers, along with their enhanced versions employing successive interference cancellation (SIC) with channel-dependent ordering, i.e., the WL-ZF-SIC and WL-MMSE-SIC receivers. The outage performances of these receivers are analytically characterized in the high signal-to-noise ratio (SNR) regime and compared to those of conventional linear (CL) receivers using complex-valued modulation schemes. For the non-SIC receivers, we show that, when compared to the CL counterparts, the WL receivers yield a higher diversity gain when decoding the same number of users and have the same diversity gain but a decreased coding gain when the number of users is nearly doubled. The outage performance analysis of WL-SIC receivers is facilitated by the marginal distribution of ordered eigenvalues of a real-valued Wishart matrix. It is shown that the SIC operation with channel-dependent ordering brings no additional diversity gain to the WL receivers but instead increases the coding gain. Moreover, the coding gain of WL-SIC receivers grows as the number of users increases and even exceeds that of CL-SIC receivers under suitable conditions.      
### 26.Disconnectivity-Aware Energy-Efficient Cargo-UAV Trajectory Planning with Minimum Handoffs  [ :arrow_down: ](https://arxiv.org/pdf/2106.14276.pdf)
>  On-board battery consumption, cellular disconnectivity, and frequent handoff are key challenges for unmanned aerial vehicle (UAV) based delivery missions, a.k.a., cargo-UAV. Indeed, with the introduction of UAV technology into cargo shipping and logistics, designing energy-efficient paths becomes a serious issue for the next retail industry transformation. Typically, the latter has to guarantee uninterrupted or slightly interrupted cellular connectivity for the UAV's command and control through a small number of handoffs. In this paper, we formulate the trajectory planning as a multi-objective problem aiming to minimize both the UAV's energy consumption and the handoff rate, constrained by the UAV battery size and disconnectivity rate. Due to the problem's complexity, we propose a dynamic programming based solution. Through simulations, we demonstrate the efficiency of our approach in providing optimized UAV trajectories. Also, the impact of several parameters, such as the cargo-UAV altitude, disconnectivity rate, and type of environment, are investigated. The obtained results allow to draw recommendations and guidelines for cargo-UAV operations.      
### 27.Using deep learning to detect patients at risk for prostate cancer despite benign biopsies  [ :arrow_down: ](https://arxiv.org/pdf/2106.14256.pdf)
>  Background: Transrectal ultrasound guided systematic biopsies of the prostate is a routine procedure to establish a prostate cancer diagnosis. However, the 10-12 prostate core biopsies only sample a relatively small volume of the prostate, and tumour lesions in regions between biopsy cores can be missed, leading to a well-known low sensitivity to detect clinically relevant cancer. As a proof-of-principle, we developed and validated a deep convolutional neural network model to distinguish between morphological patterns in benign prostate biopsy whole slide images from men with and without established cancer. Methods: This study included 14,354 hematoxylin and eosin stained whole slide images from benign prostate biopsies from 1,508 men in two groups: men without an established prostate cancer (PCa) diagnosis and men with at least one core biopsy diagnosed with PCa. 80% of the participants were assigned as training data and used for model optimization (1,211 men), and the remaining 20% (297 men) as a held-out test set used to evaluate model performance. An ensemble of 10 deep convolutional neural network models was optimized for classification of biopsies from men with and without established cancer. Hyperparameter optimization and model selection was performed by cross-validation in the training data . Results: Area under the receiver operating characteristic curve (ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy level and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a specificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion: The developed model has the ability to detect men with risk of missed PCa due to under-sampling of the prostate. The proposed model has the potential to reduce the number of false negative cases in routine systematic prostate biopsies and to indicate men who could benefit from MRI-guided re-biopsy.      
### 28.MTrans: Multi-Modal Transformer for Accelerated MR Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2106.14248.pdf)
>  Accelerating multi-modal magnetic resonance (MR) imaging is a new and effective solution for fast MR imaging, providing superior performance in restoring the target modality from its undersampled counterpart with guidance from an auxiliary modality. However, existing works simply introduce the auxiliary modality as prior information, lacking in-depth investigations on the potential mechanisms for fusing two modalities. Further, they usually rely on the convolutional neural networks (CNNs), which focus on local information and prevent them from fully capturing the long-distance dependencies of global knowledge. To this end, we propose a multi-modal transformer (MTrans), which is capable of transferring multi-scale features from the target modality to the auxiliary modality, for accelerated MR imaging. By restructuring the transformer architecture, our MTrans gains a powerful ability to capture deep multi-modal information. More specifically, the target modality and the auxiliary modality are first split into two branches and then fused using a multi-modal transformer module. This module is based on an improved multi-head attention mechanism, named the cross attention module, which absorbs features from the auxiliary modality that contribute to the target modality. Our framework provides two appealing benefits: (i) MTrans is the first attempt at using improved transformers for multi-modal MR imaging, affording more global information compared with CNN-based methods. (ii) A new cross attention module is proposed to exploit the useful information in each branch at different scales. It affords both distinct structural information and subtle pixel-level information, which supplement the target modality effectively.      
### 29.Versatile Video Coding Standard: A Review from Coding Tools to Consumers Deployment  [ :arrow_down: ](https://arxiv.org/pdf/2106.14245.pdf)
>  The amount of video content and the number of applications based on multimedia information increase each day. The development of new video coding standards is a challenge to increase the compression rate and other important features with a reasonable increase in the computational load. Video Experts Team (JVET) of ITU-T and the JCT group within ISO/IEC have worked together to standardize the Versatile Video Coding, approved finally in July 2020 as ITU-T H.266 | MPEG-I - Part 3 (ISO/IEC 23090-3) standard. This paper overviews some interesting consumer electronic use cases, the compression tools described in the standard, the current available real time implementations and the first industrial trials done with this standard.      
### 30.Second-Order Perturbation Theory-Based Digital Predistortion for Fiber Nonlinearity Compensation  [ :arrow_down: ](https://arxiv.org/pdf/2106.14230.pdf)
>  The first-order (FO) perturbation theory-based nonlinearity compensation (PB-NLC) technique has been widely investigated to combat the detrimental effects of the intra-channel Kerr nonlinearity in polarization-multiplexed (Pol-Mux) optical fiber communication systems. However, the NLC performance of the FO-PB-NLC technique is significantly limited in highly nonlinear regimes of the Pol-Mux long-haul optical transmission systems. In this paper, we extend the FO theory to second-order (SO) to improve the NLC performance. This technique is referred to as the SO-PB-NLC. A detailed theoretical analysis is performed to derive the SO perturbative field for a Pol-Mux optical transmission system. Following that, we investigate a few simplifying assumptions to reduce the implementation complexity of the SO-PB-NLC technique. The numerical simulations for a single-channel system show that the SO-PB-NLC technique provides an improved bit-error-rate performance and increases the transmission reach, in comparison with the FO-PB-NLC technique. The complexity analysis demonstrates that the proposed SO-PB-NLC technique has a reduced computational complexity when compared to the digital back-propagation with one step per span.      
### 31.A Joint Technique for Nonlinearity Compensation in CO-OFDM Superchannel Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.14212.pdf)
>  We propose a technique combining the singlechannel digital-back-propagation (SC-DBP) with phaseconjugated-twin-wave (PCTW) to compensate nonlinearities in CO-OFDM superchannel systems. This exhibits a similar performance as multi-channel DBP while providing increased transmission reach compared to SC-DBP, PCTW, and linear dispersion compensation (LDC).      
### 32.A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images  [ :arrow_down: ](https://arxiv.org/pdf/2106.14207.pdf)
>  Diabetes foot ulceration (DFU) and amputation are a cause of significant morbidity. The prevention of DFU may be achieved by the identification of patients at risk of DFU and the institution of preventative measures through education and offloading. Several studies have reported that thermogram images may help to detect an increase in plantar temperature prior to DFU. However, the distribution of plantar temperature may be heterogeneous, making it difficult to quantify and utilize to predict outcomes. We have compared a machine learning-based scoring technique with feature selection and optimization techniques and learning classifiers to several state-of-the-art Convolutional Neural Networks (CNNs) on foot thermogram images and propose a robust solution to identify the diabetic foot. A comparatively shallow CNN model, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram image-based classification and the AdaBoost Classifier used 10 features and achieved an F1 score of 97 %. A comparison of the inference time for the best-performing networks confirmed that the proposed algorithm can be deployed as a smartphone application to allow the user to monitor the progression of the DFU in a home setting.      
### 33.A Spectrally Efficient Linear Polarization Coding Scheme for Fiber Nonlinearity Compensation in CO-OFDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.14205.pdf)
>  In this paper, we propose a linear polarization coding scheme (LPC) combined with the phase conjugated twin signals (PCTS) technique, referred to as LPC-PCTS, for fiber nonlinearity mitigation in coherent optical orthogonal frequency division multiplexing (CO-OFDM) systems. The LPC linearly combines the data symbols on the adjacent subcarriers of the OFDM symbol, one at full amplitude and the other at half amplitude. The linearly coded data is then transmitted as phase conjugate pairs on the same subcarriers of the two OFDM symbols on the two orthogonal polarizations. The nonlinear distortions added to these subcarriers are essentially anti-correlated, since they carry phase conjugate pairs of data. At the receiver, the coherent superposition of the information symbols received on these pairs of subcarriers eventually leads to the cancellation of the nonlinear distortions. We conducted numerical simulation of a single channel 200 Gb/s CO-OFDM system employing the LPCPCTS technique. The results show that a Q-factor improvement of 2.3 dB and 1.7 dB with and without the dispersion symmetry, respectively, when compared to the recently proposed phase conjugated subcarrier coding (PCSC) technique, at an average launch power of 3 dBm. In addition, our proposed LPCPCTS technique shows a significant performance improvement when compared to the 16-quadrature amplitude modulation (QAM) with phase conjugated twin waves (PCTW) scheme, at the same spectral efficiency, for an uncompensated transmission distance of 2800 km.      
### 34.Joint Mobile Charging and Coverage-Time Extension for Unmanned Aerial Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2106.14203.pdf)
>  In modern networks, the use of drones as mobile base stations (MBSs) has been discussed for coverage flexibility. However, the realization of drone-based networks raises several issues. One of the critical issues is drones are extremely power-hungry. To overcome this, we need to characterize a new type of drones, so-called charging drones, which can deliver energy to MBS drones. Motivated by the fact that the charging drones also need to be charged, we deploy ground-mounted charging towers for delivering energy to the charging drones. We introduce a new energy-efficiency maximization problem, which is partitioned into two independently separable tasks. More specifically, as our first optimization task, two-stage charging matching is proposed due to the inherent nature of our network model, where the first matching aims to schedule between charging towers and charging drones while the second matching solves the scheduling between charging drones and MBS drones. We analyze how to convert the formulation containing non-convex terms to another one only with convex terms. As our second optimization task, each MBS drone conducts energy-aware time-average transmit power allocation minimization subject to stability via Lyapunov optimization. Our solutions enable the MBS drones to extend their lifetimes; in turn, network coverage-time can be extended.      
### 35.Simple Wideband RCS Reduction by Phase Gradient Modulated Surface  [ :arrow_down: ](https://arxiv.org/pdf/2106.14202.pdf)
>  This paper presents the design and implementation of a simple, single-layer, broadband (97%, 11.3-32.3 GHz) Radar Cross Section Reduction (RCSR) Modulated Surface (MS). It uses modulation of the edge-length of the square patch (SP) radiators within adjacent unit cells. By using Sinusoidal Modulation (SM) of the edge length of the unit cells, the unit cells sequences with phase gradient, that plays an effective role in improving the RCSR, can be used for wideband RCSR achievement. The proposed structure with the dimension of 250*250mm2 that consists of 40 * 40 unit cells with period of 6mm printed on a RO4003 substrate of 1.6mm thickness and has been considered. Measurements on a prototype were conducted considering both mono- and bi-static arrangements for oblique incidences for both TM and TE polarization tests. A good agreement between simulation and measurement results proves the validity of the design criteria.      
### 36.BS-RIS-User Association and Beamforming Designs for RIS-aided Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.14197.pdf)
>  Reconfigurable intelligent surface (RIS) has been regarded as a revolutionary and promising technology owing to its powerful feature of adaptively shaping wireless propagation environment. However, as a frequency-selective device, the RIS can only effectively provide tunable phase-shifts for signals within a certain frequency band. Thus, base-station (BS)-RIS-user association is an important issue to maximize the efficiency and ability of the RIS in cellular networks. In this paper, we consider a RIS-aided cellular network and aim to maximize the sum-rate of downlink transmissions by designing BS-RIS-user association as well as the active and passive beamforming of BSs and RIS, respectively. A dynamically successive access algorithm is developed to design the user association. During the dynamical access process, an iterative algorithm is proposed to alternatively obtain the active and passive beamforming. Finally, the optimal BS-RIS association is obtained by an exhaustive search method. Simulation results illustrate the significant performance improvement of the proposed BS-RIS-user association and beamforming design algorithm.      
### 37.An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ  [ :arrow_down: ](https://arxiv.org/pdf/2106.14186.pdf)
>  During the last decade or so, there has been an insurgence in the deep learning community to solve health-related issues, particularly breast cancer. Following the Camelyon-16 challenge in 2016, several researchers have dedicated their time to build Convolutional Neural Networks (CNNs) to help radiologists and other clinicians diagnose breast cancer. In particular, there has been an emphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage breast cancer. Large companies have given their fair share of research into this subject, among these Google Deepmind who developed a model in 2020 that has proven to be better than radiologists themselves to diagnose breast cancer correctly. <br>We found that among the issues which exist, there is a need for an explanatory system that goes through the hidden layers of a CNN to highlight those pixels that contributed to the classification of a mammogram. We then chose an open-source, reasonably successful project developed by Prof. Shen, using the CBIS-DDSM image database to run our experiments on. It was later improved using the Resnet-50 and VGG-16 patch-classifiers, analytically comparing the outcome of both. The results showed that the Resnet-50 one converged earlier in the experiments. <br>Following the research by Montavon and Binder, we used the DeepTaylor Layer-wise Relevance Propagation (LRP) model to highlight those pixels and regions within a mammogram which contribute most to its classification. This is represented as a map of those pixels in the original image, which contribute to the diagnosis and the extent to which they contribute to the final classification. The most significant advantage of this algorithm is that it performs exceptionally well with the Resnet-50 patch classifier architecture.      
### 38.Residual Moment Loss for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2106.14178.pdf)
>  Location information is proven to benefit the deep learning models on capturing the manifold structure of target objects, and accordingly boosts the accuracy of medical image segmentation. However, most existing methods encode the location information in an implicit way, e.g. the distance transform maps, which describe the relative distance from each pixel to the contour boundary, for the network to learn. These implicit approaches do not fully exploit the position information (i.e. absolute location) of targets. In this paper, we propose a novel loss function, namely residual moment (RM) loss, to explicitly embed the location information of segmentation targets during the training of deep learning networks. Particularly, motivated by image moments, the segmentation prediction map and ground-truth map are weighted by coordinate information. Then our RM loss encourages the networks to maintain the consistency between the two weighted maps, which promotes the segmentation networks to easily locate the targets and extract manifold-structure-related features. We validate the proposed RM loss by conducting extensive experiments on two publicly available datasets, i.e., 2D optic cup and disk segmentation and 3D left atrial segmentation. The experimental results demonstrate the effectiveness of our RM loss, which significantly boosts the accuracy of segmentation networks.      
### 39.On Hyperspectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2106.14177.pdf)
>  In this article the author reviews José Bioucas-Dias' key contributions to hyperspectral unmixing (HU), in memory of him as an influential scholar and for his many beautiful ideas introduced to the hyperspectral community. Our story will start with vertex component analysis (VCA) -- one of the most celebrated HU algorithms, with more than 2,000 Google Scholar citations. VCA was pioneering, invented at a time when HU research just began to emerge, and it shows sharp insights on a then less-understood subject. Then we will turn to SISAL, another widely-used algorithm. SISAL is not only a highly successful algorithm, it is also a demonstration of its inventor's ingenuity on applied optimization and on smart formulation for practical noisy cases. Our tour will end with dependent component analysis (DECA), perhaps a less well-known contribution. DECA adopts a statistical inference framework, and the author's latest research indicates that such framework has great potential for further development, e.g., there are hidden connections between SISAL and DECA. The development of DECA shows foresight years ahead, in that regard.      
### 40.Model-assisted Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control  [ :arrow_down: ](https://arxiv.org/pdf/2106.14144.pdf)
>  As people spend up to 87% of their time indoors, intelligent Heating, Ventilation, and Air Conditioning (HVAC) systems in buildings are essential for maintaining occupant comfort and reducing energy consumption. Those HVAC systems in modern smart buildings rely on real-time sensor readings, which in practice often suffer from various faults and could also be vulnerable to malicious attacks. Such faulty sensor inputs may lead to the violation of indoor environment requirements (e.g., temperature, humidity, etc.) and the increase of energy consumption. While many model-based approaches have been proposed in the literature for building HVAC control, it is costly to develop accurate physical models for ensuring their performance and even more challenging to address the impact of sensor faults. In this work, we present a novel learning-based framework for sensor fault-tolerant HVAC control, which includes three deep learning based components for 1) generating temperature proposals with the consideration of possible sensor faults, 2) selecting one of the proposals based on the assessment of their accuracy, and 3) applying reinforcement learning with the selected temperature proposal. Moreover, to address the challenge of training data insufficiency in building-related tasks, we propose a model-assisted learning method leveraging an abstract model of building physical dynamics. Through extensive numerical experiments, we demonstrate that the proposed fault-tolerant HVAC control framework can significantly reduce building temperature violations under a variety of sensor fault patterns while maintaining energy efficiency.      
### 41.Sparse Control Synthesis for Uncertain Responsive Loads with Stochastic Stability Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2106.14143.pdf)
>  Recent studies have demonstrated the potential of flexible loads in providing frequency response services. However, uncertainty and variability in various weather-related and end-use behavioral factors often affect the demand-side control performance. This work addresses this problem with the design of a demand-side control to achieve frequency response under load uncertainties. Our approach involves modeling the load uncertainties via stochastic processes that appear as both multiplicative and additive to the system states in closed-loop power system dynamics. Extending the recently developed mean square exponential stability (MSES) results for stochastic systems, we formulate multi-objective linear matrix inequality (LMI)-based optimal control synthesis problems to not only guarantee stochastic stability, but also promote sparsity, enhance closed-loop transient performance, and maximize allowable uncertainties. The fundamental trade-off between the maximum allowable (\textit{critical}) uncertainty levels and the optimal stochastic stabilizing control efforts is established. Moreover, the sparse control synthesis problem is generalized to the realistic power systems scenario in which only partial-state measurements are available. Detailed numerical studies are carried out on IEEE 39-bus system to demonstrate the closed-loop stochastic stabilizing performance of the sparse controllers in enhancing frequency response under load uncertainties; as well as illustrate the fundamental trade-off between the allowable uncertainties and optimal control efforts.      
### 42.BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2106.14033.pdf)
>  The recurrent mechanism has recently been introduced into U-Net in various medical image segmentation tasks. Existing studies have focused on promoting network recursion via reusing building blocks. Although network parameters could be greatly saved, computational costs still increase inevitably in accordance with the pre-set iteration time. In this work, we study a multi-scale upgrade of a bi-directional skip connected network and then automatically discover an efficient architecture by a novel two-phase Neural Architecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method reduces the network computational cost by sifting out ineffective multi-scale features at different levels and iterations. We evaluate BiX-NAS on two segmentation tasks using three different medical image datasets, and the experimental results show that our BiX-NAS searched architecture achieves the state-of-the-art performance with significantly lower computational cost.      
### 43.Reduced Training Overhead for WLAN MU-MIMO Channel Feedback with Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2106.14022.pdf)
>  The WLAN packet format has a short training field (STF) for synchronization followed by a long training field (LTF) for channel estimation. To enable MIMO channel estimation, the LTF is repeated as many times as the number of spatial streams. For MU-MIMO, the CSI feedback in the 802.11ac/ax requires the access point (AP) to send a null data packet (NDP) where the HT/VHT/HE LTF is repeated as many times as the number of transmit antennas $N_{t}$. With each LTF being 4$\mu$s long in case of VHT and 12$\mu$s to 16$\mu$s long in case of High Efficiency WLAN (HEW), the length of NDP grows linearly with increasing $N_{t}$. Furthermore, the station (STA) with $N_{r}$ receive antennas needs to expend significant processing power to compute SVD per tone for the $N_{r}\times N_{t}$ channel matrix for generating the feedback bits, which again increases linearly with $N_{t}\cdot N_{r}$. To reduce the training and feedback overhead, this paper proposes a scheme based on Compressed Sensing that allows only a subset of tones per LTF to be transmitted in NDP, which can be used by STA to compute channel estimates that are then sent back without any further processing. Since AP knows the measurement matrix, the full dimension time domain channel estimates can be recovered by running the L1 minimization algorithms (OMP, CoSAMP). AP can further process the time domain channel estimates to generate the SVD precoding matrix.      
### 44.Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text  [ :arrow_down: ](https://arxiv.org/pdf/2106.14014.pdf)
>  Video represents the majority of internet traffic today leading to a continuous technological arms race between generating higher quality content, transmitting larger file sizes and supporting network infrastructure. Adding to this is the recent COVID-19 pandemic fueled surge in the use of video conferencing tools. Since videos take up substantial bandwidth (~100 Kbps to few Mbps), improved video compression can have a substantial impact on network performance for live and pre-recorded content, providing broader access to multimedia content worldwide. In this work, we present a novel video compression pipeline, called Txt2Vid, which substantially reduces data transmission rates by compressing webcam videos ("talking-head videos") to a text transcript. The text is transmitted and decoded into a realistic reconstruction of the original video using recent advances in deep learning based voice cloning and lip syncing models. Our generative pipeline achieves two to three orders of magnitude reduction in the bitrate as compared to the standard audio-video codecs (encoders-decoders), while maintaining equivalent Quality-of-Experience based on a subjective evaluation by users (n=242) in an online study. The code for this work is available at <a class="link-external link-https" href="https://github.com/tpulkit/txt2vid.git" rel="external noopener nofollow">this https URL</a>.      
### 45.An Audio Envelope Generator Derived from Industrial Process Control  [ :arrow_down: ](https://arxiv.org/pdf/2106.13966.pdf)
>  Audio envelopes serve a crucial role in ensuring the versatility of synthesizers in producing timbres. To this end, the Attack, Decay, Release and Sustain (ADSR) envelope generator and its derivatives have been established as a mainstay in modern music. However, there may be merit in exploring alternate techniques to produce envelopes that could not only resemble ADSR but also be used to create novel timbres. Consequently, an attempt is made in this research to formulate the framework of a new envelope generator by redefining the Proportional-Integral-Derivative (PID) algorithm used in feedback-based process control. Additionally, a detailed analysis is made on the modes of operation and the nature of envelopes thus generated to establish it as a potential harbinger of distinctive styles of music.      
### 46.A Chirp Spread Spectrum Modulation Scheme for Robust Power Line Communication  [ :arrow_down: ](https://arxiv.org/pdf/2106.13965.pdf)
>  This paper proposes the use of a LoRa like chirp spread spectrum physical layer as the basis for a new Power Line Communication modulation scheme suited for low-bandwidth communication. It is shown that robust communication can be established even in channels exhibiting both extreme multipath interference and low SNR (-40dB), with synchronisation requirements significantly reduced compared to conventional LoRa. ATP-EMTP simulations using frequency dependent line and transformer models, and simulations using artificial Rayleigh channels demonstrate the effectiveness of the new scheme in providing load data from LV feeders back to the MV primary substation. We further present experimental results based on a Field Programmable Gate Array hardware implementation of the proposed scheme.      
### 47.Adaptive Smooth Disturbance Observer-Based Fast Finite-Time Adaptive Backstepping Control for Attitude Tracking of a 3-DOF Helicopter  [ :arrow_down: ](https://arxiv.org/pdf/2106.13940.pdf)
>  In this paper, a novel adaptive smooth disturbance observer-based fast finite-time adaptive backstepping control scheme is presented for the attitude tracking of the 3-DOF helicopter system subject to compound disturbances. First, an adaptive smooth disturbance observer (ASDO) is proposed to estimate the composite disturbance, which owns the characteristics of smooth output, fast finite-time convergence, and adaptability to the disturbance of unknown derivative boundary. Then, a finite-time backstepping control protocol is construct to drive the elevation and pitch angles to track reference trajectories. To tackle the "explosion of complexity" and "singularity" problems in the conventional backstepping design framework, a fast finite-time command filter (FFTCF) is utilized to estimate the virtual control signal and its derivative. Moreover, a fractional power-based auxiliary dynamic system is introduced to compensate the error caused by the FFTCF estimation. Furthermore, an improved fractional power-based adaptive law with the $\sigma $-modification term is designed to attenuate the observer approximation error, such that the tracking performance is further enhanced. In terms of the fast finite-time stability theory, the signals of the closed-loop system are all fast finite-time bounded while the attitude tracking errors can fast converge to a sufficiently small region of the origin in finite time. Finally, a contrastive numerical simulation is carried out to validate the effectiveness and superiority of the designed control scheme.      
### 48.Partial Strong Structural Controllability  [ :arrow_down: ](https://arxiv.org/pdf/2106.13934.pdf)
>  In linear control theory, a structured system is a system whose entries of its system matrices are either fixed zero or indeterminate. This system is structurally controllable, if there exists a realization of it that is controllable, and is strongly structurally controllable (SSC), if for any nonzero values of the indeterminate entries, the corresponding system is controllable. This paper introduces a new controllability notion, termed partial strong structural controllability (PSSC), which naturally extends SSC and bridges the gap between structural controllability and SSC. Dividing the indeterminate entries into two categories, generic entries and unspecified entries, a system is PSSC, if for almost all values of the generic entries in the parameter space except for a set of measure zero, and any nonzero (complex) values of the unspecified entries, the corresponding system is controllable. We highlight that this notion generalizes the generic property embedded in the conventional structural controllability for single-input systems. We then give algebraic and (bipartite) graph-theoretic necessary and sufficient conditions for single-input systems to be PSSC. Conditions for multi-input systems are subsequently given for a particular case. We also extend our results to the case where the unspecified entries can take either nonzero values or zero/nonzero values. Finally, we show the established results can induce a new graph-theoretic criterion for SSC in maximum matchings over the system bipartite graph representations.      
### 49.POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.13867.pdf)
>  We propose POLAR, a \textbf{pol}ynomial \textbf{ar}ithmetic framework that leverages polynomial overapproximations with interval remainders for bounded-time reachability analysis of neural network-controlled systems (NNCSs). Compared with existing arithmetic approaches that use standard Taylor models, our framework uses a novel approach to iteratively overapproximate the neuron output ranges layer-by-layer with a combination of Bernstein polynomial interpolation for continuous activation functions and Taylor model arithmetic for the other operations. This approach can overcome the main drawback in the standard Taylor model arithmetic, i.e. its inability to handle functions that cannot be well approximated by Taylor polynomials, and significantly improve the accuracy and efficiency of reachable states computation for NNCSs. To further tighten the overapproximation, our method keeps the Taylor model remainders symbolic under the linear mappings when estimating the output range of a neural network. We show that POLAR can be seamlessly integrated with existing Taylor model flowpipe construction techniques, and demonstrate that POLAR significantly outperforms the current state-of-the-art techniques on a suite of benchmarks.      
### 50.A Photonic-Circuits-Inspired Compact Network: Toward Real-Time Wireless Signal Classification at the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2106.13865.pdf)
>  Machine learning (ML) methods are ubiquitous in wireless communication systems and have proven powerful for applications including radio-frequency (RF) fingerprinting, automatic modulation classification, and cognitive radio. However, the large size of ML models can make them difficult to implement on edge devices for latency-sensitive downstream tasks. In wireless communication systems, ML data processing at a sub-millisecond scale will enable real-time network monitoring to improve security and prevent infiltration. In addition, compact and integratable hardware platforms which can implement ML models at the chip scale will find much broader application to wireless communication networks. Toward real-time wireless signal classification at the edge, we propose a novel compact deep network that consists of a photonic-hardware-inspired recurrent neural network model in combination with a simplified convolutional classifier, and we demonstrate its application to the identification of RF emitters by their random transmissions. With the proposed model, we achieve 96.32% classification accuracy over a set of 30 identical ZigBee devices when using 50 times fewer training parameters than an existing state-of-the-art CNN classifier. Thanks to the large reduction in network size, we demonstrate real-time RF fingerprinting with 0.219 ms latency using a small-scale FPGA board, the PYNQ-Z1.      
### 51.PhysiNet: A Combination of Physics-based Model and Neural Network Model for Digital Twins  [ :arrow_down: ](https://arxiv.org/pdf/2106.14790.pdf)
>  As the real-time digital counterpart of a physical system or process, digital twins are utilized for system simulation and optimization. Neural networks are one way to build a digital twins model by using data especially when a physics-based model is not accurate or even not available. However, for a newly designed system, it takes time to accumulate enough data for neural network moded and only an approximate physics-based model is available. To take advantage of both models, this paper proposed a model that combines the physics-based model and the neural network model to improve the prediction accuracy for the whole life cycle of a system. The proposed model was able to automatically combine the models and boost their prediction performance. Experiments showed that the proposed hybrid model outperformed both the physics-based model and the neural network model.      
### 52.The Associative Multifractal Process: A Novel Model for Computer Network Traffic Flows  [ :arrow_down: ](https://arxiv.org/pdf/2106.14666.pdf)
>  A novel constructive mathematical model based on the multifractal formalism in order to accurately characterizing the localized fluctuations present in the course of traffic flows today high-speed computer networks is presented. The proposed model has the target to analyze self-similar second-order time series representative of traffic flows in terms of their roughness and impulsivity.      
### 53.Applications of Mechanism Design in Market-Based Demand-Side Management  [ :arrow_down: ](https://arxiv.org/pdf/2106.14659.pdf)
>  The intermittent nature of renewable energy resources creates extra challenges in the operation and control of the electricity grid. Demand flexibility markets can help in dealing with these challenges by introducing incentives for customers to modify their demand. Market-based demand-side management (DSM) have garnered serious attention lately due to its promising capability of maintaining the balance between supply and demand, while also keeping customer satisfaction at its highest levels. Many researchers have proposed using concepts from mechanism design theory in their approaches to market-based DSM. In this work, we provide a review of the advances in market-based DSM using mechanism design. We provide a categorisation of the reviewed literature and evaluate the strengths and weaknesses of each design criteria. We also study the utility function formulations used in the reviewed literature and provide a critique of the proposed indirect mechanisms. We show that despite the extensiveness of the literature on this subject, there remains concerns and challenges that should be addressed for the realistic implementation of such DSM approaches. We draw conclusions from our review and discuss possible future research directions.      
### 54.Rate and Power Adaptation for Multihop Regenerative Relaying Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.14638.pdf)
>  In this work, we provide a global framework analysis of a multi-hop relaying systems wherein the transmitter (TX) communicates with the receiver (RX) through a set of intermediary relays deployed either in series or in parallel. Regenerative based relaying scheme is assumed such as the repetition-coded decoded-and-forward (DF) wherein the decoding is threshold-based. To reflect a wide range of fading, we introduce the generalized $H$-function (also termed as Fox-$H$ function) distribution model which enables the modeling of radio-frequency (RF) fading like Weibull and Gamma, as well as the free-space optic (FSO) such as the Double Generalized Gamma and Málaga fading. In this context, we introduce various power and rate adaptation policies based on the channel state information (CSI) availability at TX and RX. Finally, we address the effects of relaying topology, number of relays and fading model, etc, on the performance reliability of each link adaptation policy.      
### 55.Blockchain and AI-based Solutions to Combat Coronavirus (COVID-19)-like Epidemics: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2106.14631.pdf)
>  The beginning of 2020 has seen the emergence of coronavirus outbreak caused by a novel virus called SARS-CoV-2. The sudden explosion and uncontrolled worldwide spread of COVID-19 show the limitations of existing healthcare systems in timely handling public health emergencies. In such contexts, innovative technologies such as blockchain and Artificial Intelligence (AI) have emerged as promising solutions for fighting coronavirus epidemic. In particular, blockchain can combat pandemics by enabling early detection of outbreaks, ensuring the ordering of medical data, and ensuring reliable medical supply chain during the outbreak tracing. Moreover, AI provides intelligent solutions for identifying symptoms caused by coronavirus for treatments and supporting drug manufacturing. Therefore, we present an extensive survey on the use of blockchain and AI for combating COVID-19 epidemics. First, we introduce a new conceptual architecture which integrates blockchain and AI for fighting COVID-19. Then, we survey the latest research efforts on the use of blockchain and AI for fighting COVID-19 in various applications. The newly emerging projects and use cases enabled by these technologies to deal with coronavirus pandemic are also presented. A case study is also provided using federated AI for COVID-19 detection. Finally, we point out challenges and future directions that motivate more research efforts to deal with future coronavirus-like epidemics.      
### 56.Optimized Wireless Control and Telemetry Network for Mobile Soccer Robots  [ :arrow_down: ](https://arxiv.org/pdf/2106.14617.pdf)
>  In a diverse set of robotics applications, including RoboCup categories, mobile robots require control commands to interact with surrounding environment correctly. These control commands should come wirelessly to not interfere in robots' movement; also, the communication has a set of requirements, including low latency and consistent delivery. This paper presents a complete communication architecture consisting of computer communication with a base station, which transmits the data to robots and returns robots telemetry to the computer. With the proposed communication, it is possible to send messages in less than 4.5ms for six robots with telemetry enables in all of them.      
### 57.Privacy-Preserving Image Acquisition Using Trainable Optical Kernel  [ :arrow_down: ](https://arxiv.org/pdf/2106.14577.pdf)
>  Preserving privacy is a growing concern in our society where sensors and cameras are ubiquitous. In this work, for the first time, we propose a trainable image acquisition method that removes the sensitive identity revealing information in the optical domain before it reaches the image sensor. The method benefits from a trainable optical convolution kernel which transmits the desired information while filters out the sensitive content. As the sensitive content is suppressed before it reaches the image sensor, it does not enter the digital domain therefore is unretrievable by any sort of privacy attack. This is in contrast with the current digital privacy-preserving methods that are all vulnerable to direct access attack. Also, in contrast with the previous optical privacy-preserving methods that cannot be trained, our method is data-driven and optimized for the specific application at hand. Moreover, there is no additional computation, memory, or power burden on the acquisition system since this processing happens passively in the optical domain and can even be used together and on top of the fully digital privacy-preserving systems. The proposed approach is adaptable to different digital neural networks and content. We demonstrate it for several scenarios such as smile detection as the desired attribute while the gender is filtered out as the sensitive content. We trained the optical kernel in conjunction with two adversarial neural networks where the analysis network tries to detect the desired attribute and the adversarial network tries to detect the sensitive content. We show that this method can reduce 65.1% of sensitive content when it is selected to be the gender and it only loses 7.3% of the desired content. Moreover, we reconstruct the original faces using the deep reconstruction method that confirms the ineffectiveness of reconstruction attacks to obtain the sensitive content.      
### 58.1.6 Tbps Classical Channel Coexistence With DV-QKD OverHollow Core Nested Antiresonant Nodeless Fibre (HC-NANF)  [ :arrow_down: ](https://arxiv.org/pdf/2106.14560.pdf)
>  We demonstrate for the first time the coexistence of a quantum-channel and 8x200 Gpbs 16-QAM optical channels with launching powers as high as -9dBm per channel in a 2 km HC-NANF. Comparative analysis with single-mode fibre reveals that the quantum channel could not be sustained at such power levels.      
### 59.Active Safety System for Semi-Autonomous Teleoperated Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2106.14554.pdf)
>  Autonomous cars can reduce road traffic accidents and provide a safer mode of transport. However, key technical challenges, such as safe navigation in complex urban environments, need to be addressed before deploying these vehicles on the market. Teleoperation can help smooth the transition from human operated to fully autonomous vehicles since it still has human in the loop providing the scope of fallback on driver. This paper presents an Active Safety System (ASS) approach for teleoperated driving. The proposed approach helps the operator ensure the safety of the vehicle in complex environments, that is, avoid collisions with static or dynamic obstacles. Our ASS relies on a model predictive control (MPC) formulation to control both the lateral and longitudinal dynamics of the vehicle. By exploiting the ability of the MPC framework to deal with constraints, our ASS restricts the controller's authority to intervene for lateral correction of the human operator's commands, avoiding counter-intuitive driving experience for the human operator. Further, we design a visual feedback to enhance the operator's trust over the ASS. In addition, we propose an MPC's prediction horizon data based novel predictive display to mitigate the effects of large latency in the teleoperation system. We tested the performance of the proposed approach on a high-fidelity vehicle simulator in the presence of dynamic obstacles and latency.      
### 60.R2RNet: Low-light Image Enhancement via Real-low to Real-normal Network  [ :arrow_down: ](https://arxiv.org/pdf/2106.14501.pdf)
>  Images captured in weak illumination conditions will seriously degrade the image quality. Solving a series of degradation of low-light images can effectively improve the visual quality of the image and the performance of high-level visual tasks. In this paper, we propose a novel Real-low to Real-normal Network for low-light image enhancement, dubbed R2RNet, based on the Retinex theory, which includes three subnets: a Decom-Net, a Denoise-Net, and a Relight-Net. These three subnets are used for decomposing, denoising, and contrast enhancement, respectively. Unlike most previous methods trained on synthetic images, we collect the first Large-Scale Real-World paired low/normal-light images dataset (LSRW dataset) for training. Our method can properly improve the contrast and suppress noise simultaneously. Extensive experiments on publicly available datasets demonstrate that our method outperforms the existing state-of-the-art methods by a large margin both quantitatively and visually. And we also show that the performance of the high-level visual task (\emph{i.e.} face detection) can be effectively improved by using the enhanced results obtained by our method in low-light conditions. Our codes and the LSRW dataset are available at: <a class="link-external link-https" href="https://github.com/abcdef2000/R2RNet" rel="external noopener nofollow">this https URL</a>.      
### 61.Unifying Classical and Bayesian Revealed Preference  [ :arrow_down: ](https://arxiv.org/pdf/2106.14486.pdf)
>  This paper establishes the equivalence between Bayesian revealed preference and classical revealed preference with non-linear budget constraints. Classical revealed preference tests for utility maximization given known budget constraints. Bayesian revealed preference tests for costly information acquisition given a utility function. Our main result shows that the key theorem in Caplin and Dean (2015) on Bayesian revealed preference is equivalent to Afriat-type feasibility inequalities for general (non-linear) budget sets. Our second result exploits this equivalence of classical and Bayesian revealed preference to construct a monotone convex information acquisition cost from decision maker's data in Bayesian revealed preference      
### 62.Operational Data Analytics in Practice: Experiences from Design to Deployment in Production HPC Environments  [ :arrow_down: ](https://arxiv.org/pdf/2106.14423.pdf)
>  As HPC systems grow in complexity, efficient and manageable operation is increasingly critical. Many centers are thus starting to explore the use of Operational Data Analytics (ODA) techniques, which extract knowledge from massive amounts of monitoring data and use it for control and visualization purposes. As ODA is a multi-faceted problem, much effort has gone into researching its separate aspects: however, accounts of production ODA experiences are still hard to come across. <br>In this work we aim to bridge the gap between ODA research and production use by presenting our experiences with ODA in production, involving in particular the control of cooling infrastructures and visualization of job data on two HPC systems. We cover the entire development process, from design to deployment, highlighting our insights in an effort to drive the community forward. We rely on open-source tools, which make for a generic ODA framework suitable for most scenarios.      
### 63.Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits  [ :arrow_down: ](https://arxiv.org/pdf/2106.14371.pdf)
>  Target speech separation is the process of filtering a certain speaker's voice out of speech mixtures according to the additional speaker identity information provided. Recent works have made considerable improvement by processing signals in the time domain directly. The majority of them take fully overlapped speech mixtures for training. However, since most real-life conversations occur randomly and are sparsely overlapped, we argue that training with different overlap ratio data benefits. To do so, an unavoidable problem is that the popularly used SI-SNR loss has no definition for silent sources. This paper proposes the weighted SI-SNR loss, together with the joint learning of target speech separation and personal VAD. The weighted SI-SNR loss imposes a weight factor that is proportional to the target speaker's duration and returns zero when the target speaker is absent. Meanwhile, the personal VAD generates masks and sets non-target speech to silence. Experiments show that our proposed method outperforms the baseline by 1.73 dB in terms of SDR on fully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely overlapped speech of clean and noisy conditions. Besides, with slight degradation in performance, our model could reduce the time costs in inference.      
### 64.Use of Variational Inference in Music Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2106.14323.pdf)
>  This work was developed aiming to employ Statistical techniques to the field of Music Emotion Recognition, a well-recognized area within the Signal Processing world, but hardly explored from the statistical point of view. Here, we opened several possibilities within the field, applying modern Bayesian Statistics techniques and developing efficient algorithms, focusing on the applicability of the results obtained. Although the motivation for this project was the development of a emotion-based music recommendation system, its main contribution is a highly adaptable multivariate model that can be useful interpreting any database where there is an interest in applying regularization in an efficient manner. Broadly speaking, we will explore what role a sound theoretical statistical analysis can play in the modeling of an algorithm that is able to understand a well-known database and what can be gained with this kind of approach.      
### 65.Change Detection for Geodatabase Updating  [ :arrow_down: ](https://arxiv.org/pdf/2106.14309.pdf)
>  The geodatabase (vectorized data) nowadays becomes a rather standard digital city infrastructure; however, updating geodatabase efficiently and economically remains a fundamental and practical issue in the geospatial industry. The cost of building a geodatabase is extremely high and labor intensive, and very often the maps we use have several months and even years of latency. One solution is to develop more automated methods for (vectorized) geospatial data generation, which has been proven a difficult task in the past decades. An alternative solution is to first detect the differences between the new data and the existing geospatial data, and then only update the area identified as changes. The second approach is becoming more favored due to its high practicality and flexibility. A highly relevant technique is change detection. This article aims to provide an overview the state-of-the-art change detection methods in the field of Remote Sensing and Geomatics to support the task of updating geodatabases. Data used for change detection are highly disparate, we therefore structure our review intuitively based on the dimension of the data, being 1) change detection with 2D data; 2) change detection with 3D data. Conclusions will be drawn based on the reviewed efforts in the field, and we will share our outlooks of the topic of updating geodatabases.      
### 66.Concentration of Contractive Stochastic Approximation and Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2106.14308.pdf)
>  Using a martingale concentration inequality, concentration bounds `from time $n_0$ on' are derived for stochastic approximation algorithms with contractive maps and both martingale difference and Markov noises. These are applied to reinforcement learning algorithms, in particular to asynchronous Q-learning and TD(0).      
### 67.Representation Based Regression for Object Distance Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2106.14208.pdf)
>  In this study, we propose a novel approach to predict the distances of the detected objects in an observed scene. The proposed approach modifies the recently proposed Convolutional Support Estimator Networks (CSENs). CSENs are designed to compute a direct mapping for the Support Estimation (SE) task in a representation-based classification problem. We further propose and demonstrate that representation-based methods (sparse or collaborative representation) can be used in well-designed regression problems. To the best of our knowledge, this is the first representation-based method proposed for performing a regression task by utilizing the modified CSENs; and hence, we name this novel approach as Representation-based Regression (RbR). The initial version of CSENs has a proxy mapping stage (i.e., a coarse estimation for the support set) that is required for the input. In this study, we improve the CSEN model by proposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly optimize the so-called proxy mapping stage along with convolutional layers. The experimental evaluations using the KITTI 3D Object Detection distance estimation dataset show that the proposed method can achieve a significantly improved distance estimation performance over all competing methods. Finally, the software implementations of the methods are publicly shared at <a class="link-external link-https" href="https://github.com/meteahishali/CSENDistance" rel="external noopener nofollow">this https URL</a>.      
### 68.Open, Sesame! Introducing Access Control to Voice Services  [ :arrow_down: ](https://arxiv.org/pdf/2106.14191.pdf)
>  Personal voice assistants (VAs) are shown to be vulnerable against record-and-replay, and other acoustic attacks which allow an adversary to gain unauthorized control of connected devices within a smart home. Existing defenses either lack detection and management capabilities or are too coarse-grained to enable flexible policies on par with other computing interfaces. In this work, we present Sesame, a lightweight framework for edge devices which is the first to enable fine-grained access control of smart-home voice commands. Sesame combines three components: Automatic Speech Recognition, Natural Language Understanding (NLU) and a Policy module. We implemented Sesame on Android devices and demonstrate that our system can enforce security policies for both Alexa and Google Home in real-time (362ms end-to-end inference time), with a lightweight (&lt;25MB) NLU model which exhibits minimal accuracy loss compared to its non-compact equivalent.      
### 69.Low power in-situ AI Calibration of a 3 Axial Magnetic Sensor  [ :arrow_down: ](https://arxiv.org/pdf/2106.14151.pdf)
>  Magnetic surveys are conventionally performed by scanning a domain with a portable scalar magnetic sensor. Unfortunately, scalar magnetometers are expensive, power consuming and bulky. In many applications, calibrated vector magnetometers can be used to perform magnetic surveys. In recent years algorithms based on artificial intelligence (AI) achieve state-of-the-art results in many modern applications. In this work we investigate an AI algorithm for the classical scalar calibration of magnetometers. A simple, low cost method for performing a magnetic survey is presented. The method utilizes a low power consumption sensor with an AI calibration procedure that improves the common calibration methods and suggests an alternative to the conventional technology and algorithms. The setup of the survey system is optimized for quick deployment in-situ right before performing the magnetic survey. We present a calibration method based on a procedure of rotating the sensor in the natural earth magnetic field for an optimal time period. This technique can deal with a constant field offset and non-orthogonality issues and does not require any external reference. The calibration is done by finding an estimator that yields the calibration parameters and produces the best geometric fit to the sensor readings. A comprehensive model considering the physical, algorithmic and hardware properties of the magnetometer of the survey system is presented. The geometric ellipsoid fitting approach is parametrically tested. The calibration procedure reduced the root-mean-squared noise from the order of 104 nT to less than 10 nT with variance lower than 1 nT in a complete 360 degrees rotation in the natural earth magnetic field.      
### 70.Capacity Analysis of Public Blockchain  [ :arrow_down: ](https://arxiv.org/pdf/2106.14149.pdf)
>  As distributed ledgers, blockchains run consensus protocols which trade capacity for consistency, especially in non-ideal networks with incomplete connectivity and erroneous links. Existing studies on the tradeoff between capacity and consistency are only qualitative or rely on specific assumptions. This paper presents discrete-time Markov chain models to quantify the capacity of Proof-of-Work based public blockchains in non-ideal networks. The comprehensive model is collapsed to be ergodic under the eventual consistency of blockchains, achieving tractability and efficient evaluations of blockchain capacity. A closed-form expression for the capacity is derived in the case of two miners. Another important aspect is that we extend the ergodic model to analyze the capacity under strong consistency, evaluating the robustness of blockchains against double-spending attacks. Validated by simulations, the proposed models are accurate and reveal the effect of link quality and the distribution of mining rates on blockchain capacity and the ratio of stale blocks.      
### 71.Query-graph with Cross-gating Attention Model for Text-to-Audio Grounding  [ :arrow_down: ](https://arxiv.org/pdf/2106.14136.pdf)
>  In this paper, we address the text-to-audio grounding issue, namely, grounding the segments of the sound event described by a natural language query in the untrimmed audio. This is a newly proposed but challenging audio-language task, since it requires to not only precisely localize all the on- and off-sets of the desired segments in the audio, but to perform comprehensive acoustic and linguistic understandings and reason the multimodal interactions between the audio and query. To tackle those problems, the existing method treats the query holistically as a single unit by a global query representation, which fails to highlight the keywords that contain rich semantics. Besides, this method has not fully exploited interactions between the query and audio. Moreover, since the audio and queries are arbitrary and variable in length, many meaningless parts of them are not filtered out in this method, which hinders the grounding of the desired segments. <br>To this end, we propose a novel Query Graph with Cross-gating Attention (QGCA) model, which models the comprehensive relations between the words in query through a novel query graph. Besides, to capture the fine-grained interactions between audio and query, a cross-modal attention module that assigns higher weights to the keywords is introduced to generate the snippet-specific query representations. Finally, we also design a cross-gating module to emphasize the crucial parts as well as weaken the irrelevant ones in the audio and query. We extensively evaluate the proposed QGCA model on the public Audiogrounding dataset with significant improvements over several state-of-the-art methods. Moreover, further ablation study shows the consistent effectiveness of different modules in the proposed QGCA model.      
### 72.Online Cognitive Data Sensing and Processing Optimization in Energy-harvesting Edge Computing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2106.14113.pdf)
>  Mobile edge computing (MEC) has recently become a prevailing technique to alleviate the intensive computation burden in Internet of Things (IoT) networks. However, the limited device battery capacity and stringent spectrum resource significantly restrict the data processing performance of MEC-enabled IoT networks. To address the two performance limitations, we consider in this paper an MEC-enabled IoT system with an energy harvesting (EH) wireless device (WD) which opportunistically accesses the licensed spectrum of an overlaid primary communication link for task offloading. We aim to maximize the long-term average sensing rate of the WD subject to quality of service (QoS) requirement of primary link, average power constraint of MEC server (MS) and data queue stability of both MS and WD. We formulate the problem as a multi-stage stochastic optimization and propose an online algorithm named PLySE that applies the perturbed Lyapunov optimization technique to decompose the original problem into per-slot deterministic optimization problems. For each per-slot problem, we derive the closed-form optimal solution of data sensing and processing control to facilitate low-complexity real-time implementation. Interestingly, our analysis finds that the optimal solution exhibits an threshold-based structure. Simulation results collaborate with our analysis and demonstrate more than 46.7\% data sensing rate improvement of the proposed PLySE over representative benchmark methods.      
### 73.Inferring a Continuous Distribution of Atom Coordinates from Cryo-EM Images using VAEs  [ :arrow_down: ](https://arxiv.org/pdf/2106.14108.pdf)
>  Cryo-electron microscopy (cryo-EM) has revolutionized experimental protein structure determination. Despite advances in high resolution reconstruction, a majority of cryo-EM experiments provide either a single state of the studied macromolecule, or a relatively small number of its conformations. This reduces the effectiveness of the technique for proteins with flexible regions, which are known to play a key role in protein function. Recent methods for capturing conformational heterogeneity in cryo-EM data model it in volume space, making recovery of continuous atomic structures challenging. Here we present a fully deep-learning-based approach using variational auto-encoders (VAEs) to recover a continuous distribution of atomic protein structures and poses directly from picked particle images and demonstrate its efficacy on realistic simulated data. We hope that methods built on this work will allow incorporation of stronger prior information about protein structure and enable better understanding of non-rigid protein structures.      
### 74.Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2106.14070.pdf)
>  Highly constrained manipulation tasks continue to be challenging for autonomous robots as they require high levels of precision, typically less than 1mm, which is often incompatible with what can be achieved by traditional perception systems. This paper demonstrates that the combination of state-of-the-art object tracking with passively adaptive mechanical hardware can be leveraged to complete precision manipulation tasks with tight, industrially-relevant tolerances (0.25mm). The proposed control method closes the loop through vision by tracking the relative 6D pose of objects in the relevant workspace. It adjusts the control reference of both the compliant manipulator and the hand to complete object insertion tasks via within-hand manipulation. Contrary to previous efforts for insertion, our method does not require expensive force sensors, precision manipulators, or time-consuming, online learning, which is data hungry. Instead, this effort leverages mechanical compliance and utilizes an object agnostic manipulation model of the hand learned offline, off-the-shelf motion planning, and an RGBD-based object tracker trained solely with synthetic data. These features allow the proposed system to easily generalize and transfer to new tasks and environments. This paper describes in detail the system components and showcases its efficacy with extensive experiments involving tight tolerance peg-in-hole insertion tasks of various geometries as well as open-world constrained placement tasks.      
### 75.Semi-Supervised Deep Ensembles for Blind Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2106.14008.pdf)
>  Ensemble methods are generally regarded to be better than a single model if the base learners are deemed to be "accurate" and "diverse." Here we investigate a semi-supervised ensemble learning strategy to produce generalizable blind image quality assessment models. We train a multi-head convolutional network for quality prediction by maximizing the accuracy of the ensemble (as well as the base learners) on labeled data, and the disagreement (i.e., diversity) among them on unlabeled data, both implemented by the fidelity loss. We conduct extensive experiments to demonstrate the advantages of employing unlabeled data for BIQA, especially in model generalization and failure identification.      
### 76.A Trust-Centric Privacy-Preserving Blockchain for Dynamic Spectrum Management in IoT Networks  [ :arrow_down: ](https://arxiv.org/pdf/2106.13958.pdf)
>  In this paper, we propose a trust-centric privacy-preserving blockchain for dynamic spectrum access in IoT networks. To be specific, we propose a trust evaluation mechanism to evaluate the trustworthiness of sensing nodes and design a Proof-of-Trust (PoT) consensus mechanism to build a scalable blockchain with high transaction-per-second (TPS). Moreover, a privacy protection scheme is proposed to protect sensors' real-time geolocatioin information when they upload sensing data to the blockchain. Two smart contracts are designed to make the whole procedure (spectrum sensing, spectrum auction, and spectrum allocation) run automatically. Simulation results demonstrate the expected computation cost of the PoT consensus algorithm for reliable sensing nodes is low, and the cooperative sensing performance is improved with the help of trust value evaluation <a class="link-external link-http" href="http://mechanism.In" rel="external noopener nofollow">this http URL</a> addition, incentivization and security are also analyzed, which show that our design not only can encourage nodes' participation, but also resist to many kinds of attacks which are frequently encountered in trust-based blockchain systems.      
### 77.Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2106.13952.pdf)
>  In this paper, we propose a spectral-spatial graph reasoning network (SSGRN) for hyperspectral image (HSI) classification. Concretely, this network contains two parts that separately named spatial graph reasoning subnetwork (SAGRN) and spectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral graph contexts, respectively. Different from the previous approaches implementing superpixel segmentation on the original image or attempting to obtain the category features under the guide of label image, we perform the superpixel segmentation on intermediate features of the network to adaptively produce the homogeneous regions to get the effective descriptors. Then, we adopt a similar idea in spectral part that reasonably aggregating the channels to generate spectral descriptors for spectral graph contexts capturing. All graph reasoning procedures in SAGRN and SEGRN are achieved through graph convolution. To guarantee the global perception ability of the proposed methods, all adjacent matrices in graph reasoning are obtained with the help of non-local self-attention mechanism. At last, by combining the extracted spatial and spectral graph contexts, we obtain the SSGRN to achieve a high accuracy classification. Extensive quantitative and qualitative experiments on three public HSI benchmarks demonstrate the competitiveness of the proposed methods compared with other state-of-the-art approaches.      
### 78.Unified Simultaneous Wireless Information and Power Transfer for IoT: Signaling and Architecture with Deep Learning Adaptive Control  [ :arrow_down: ](https://arxiv.org/pdf/2106.13937.pdf)
>  In this paper, we propose a unified SWIPT signal and its architecture design in order to take advantage of both single tone and multi-tone signaling by adjusting only the power allocation ratio of a unified signal. For this, we design a novel unified and integrated receiver architecture for the proposed unified SWIPT signaling, which consumes low power with an envelope detection. To relieve the computational complexity of the receiver, we propose an adaptive control algorithm by which the transmitter adjusts the communication mode through temporal convolutional network (TCN) based asymmetric processing. To this end, the transmitter optimizes the modulation index and power allocation ratio in short-term scale while updating the mode switching threshold in long-term scale. We demonstrate that the proposed unified SWIPT system improves the achievable rate under the self-powering condition of low-power IoT devices. Consequently it is foreseen to effectively deploy low-power IoT networks that concurrently supply both information and energy wirelessly to the devices by using the proposed unified SWIPT and adaptive control algorithm in place at the transmitter side.      
### 79.Optical MIMO Communication Using Holographic Spectral Multiplexing of Pulsed Ultrashort Laser  [ :arrow_down: ](https://arxiv.org/pdf/2106.13896.pdf)
>  In this paper, we introduce Holographic Spectral Multiplexing (HSM) as a novel technique to enable multiple-input multiple-output (MIMO) communication in optical networks. HSM uses the spectral space of ultrashort laser pulses to create line codes in the form of 2D holograms. The pulse processing is performed in the temporal Fourier domain by spatially dispersing the pulse frequency components in a spectral processing device (SPD). The 2D holograms are composed of the patterns of intensity disparities that an SLM inscribes on the spectrally-decomposed Fourier plane of the pulse. The holographic line codes defined in this way transform the ultrashort laser pulses into high-entropy data symbols, hence, enhance the communication's spectral efficiency. Unlike conventional optical multiplexing schemes (e.g., TDM, WDM, or SDM), HSM does not physically or abstractly separate the communication propagation space into subchannels. Rather, HSM realizes a MIMO communication paradigm by allowing the photonic waves under the pulse envelope to propagate in the same space so they scatter and interfere by chromatic dispersion. This allows HSM to form beams between the pixels of SLM at the sender and receiver sides and optimize the beam to adapt to channel scattering situations. In this way, HSM delivers a rate gain that in the best case exponentially increases the information rate of communication.      
### 80.Deconstruction and reconstruction of image-degrading effects in the human abdomen using Fullwave: phase aberration, multiple reverberation, and trailing reverberation  [ :arrow_down: ](https://arxiv.org/pdf/2106.13890.pdf)
>  Ultrasound image degradation in the human body is complex and occurs due to the distortion of the wave as it propagates to and from the target. Here, we establish a simulation based framework that deconstructs the sources of image degradation into a separable parameter space that includes phase aberration from speed variation, multiple reverberations, and trailing reverberation. These separable parameters are then used to reconstruct images with known and independently modulable amounts of degradation using methods that depend on the additive or multiplicative nature of the degradation. Experimental measurements and Fullwave simulations in the human abdomen demonstrate this calibrated process in abdominal imaging by matching relevant imaging metrics such as phase aberration, reverberation strength, speckle brightness and coherence length. Applications of the reconstruction technique are illustrated for beamforming strategies (phase aberration correction, spatial coherence imaging), in a standard abdominal environment, as well as in impedance ranges much higher than those naturally occurring in the body.      
### 81.Semi-Supervised Raw-to-Raw Mapping  [ :arrow_down: ](https://arxiv.org/pdf/2106.13883.pdf)
>  The raw-RGB colors of a camera sensor vary due to the spectral sensitivity differences across different sensor makes and models. This paper focuses on the task of mapping between different sensor raw-RGB color spaces. Prior work addressed this problem using a pairwise calibration to achieve accurate color mapping. Although being accurate, this approach is less practical as it requires: (1) capturing pair of images by both camera devices with a color calibration object placed in each new scene; (2) accurate image alignment or manual annotation of the color calibration object. This paper aims to tackle color mapping in the raw space through a more practical setup. Specifically, we present a semi-supervised raw-to-raw mapping method trained on a small set of paired images alongside an unpaired set of images captured by each camera device. Through extensive experiments, we show that our method achieves better results compared to other domain adaptation alternatives in addition to the single-calibration solution. We have generated a new dataset of raw images from two different smartphone cameras as part of this effort. Our dataset includes unpaired and paired sets for our semi-supervised training and evaluation.      
### 82.Transflower: probabilistic autoregressive dance generation with multimodal attention  [ :arrow_down: ](https://arxiv.org/pdf/2106.13871.pdf)
>  Dance requires skillful composition of complex movements that follow rhythmic, tonal and timbral features of music. Formally, generating dance conditioned on a piece of music can be expressed as a problem of modelling a high-dimensional continuous motion signal, conditioned on an audio signal. In this work we make two contributions to tackle this problem. First, we present a novel probabilistic autoregressive architecture that models the distribution over future poses with a normalizing flow conditioned on previous poses as well as music context, using a multimodal transformer encoder. Second, we introduce the currently largest 3D dance-motion dataset, obtained with a variety of motion-capture technologies, and including both professional and casual dancers. Using this dataset, we compare our new model against two baselines, via objective metrics and a user study, and show that both the ability to model a probability distribution, as well as being able to attend over a large motion and music context are necessary to produce interesting, diverse, and realistic dance that matches the music.      
### 83.Nonuniform Defocus Removal for Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2106.13864.pdf)
>  We propose and study the single-frame anisoplanatic deconvolution problem associated with image classification using machine learning algorithms, named the nonuniform defocus removal (NDR) problem. Mathematical analysis of the NDR problem is done and the so-called defocus removal (DR) algorithm for solving it is proposed. Global convergence of the DR algorithm is established without imposing any unverifiable assumption. Numerical results on simulation data show significant features of DR including solvability, noise robustness, convergence, model insensitivity and computational efficiency. Physical relevance of the NDR problem and practicability of the DR algorithm are tested on experimental data. Back to the application that originally motivated the investigation of the NDR problem, we show that the DR algorithm can improve the accuracy of classifying distorted images using convolutional neural networks. The key difference of this paper compared to most existing works on single-frame anisoplanatic deconvolution is that the new method does not require the data image to be decomposable into isoplanatic subregions. Therefore, solution approaches partitioning the image into isoplanatic zones are not applicable to the NDR problem and those handling the entire image such as the DR algorithm need to be developed and analyzed.      
### 84.Decomposition of transition systems into sets of synchronizing state machines  [ :arrow_down: ](https://arxiv.org/pdf/2106.13852.pdf)
>  Transition systems (TS) and Petri nets (PN) are important models of computation ubiquitous in formal methods for modeling systems. An important problem is how to extract from a given TS a PN whose reachability graph is equivalent (with a suitable notion of equivalence) to the original TS. <br>This paper addresses the decomposition of transition systems into synchronizing state machines (SMs), which are a class of Petri nets where each transition has one incoming and one outgoing arc and all markings have exactly one token. This is an important case of the general problem of extracting a PN from a TS. The decomposition is based on the theory of regions, and it is shown that a property of regions called excitation-closure is a sufficient condition to guarantee the equivalence between the original TS and a decomposition into SMs. <br>An efficient algorithm is provided which solves the problem by reducing its critical steps to the maximal independent set problem (to compute a minimal set of irredundant SMs) or to satisfiability (to merge the SMs). We report experimental results that show a good trade-off between quality of results vs. computation time.      
### 85.A CNN Segmentation-Based Approach to Object Detection and Tracking in Ultrasound Scans with Application to the Vagus Nerve Detection  [ :arrow_down: ](https://arxiv.org/pdf/2106.13849.pdf)
>  Ultrasound scanning is essential in several medical diagnostic and therapeutic applications. It is used to visualize and analyze anatomical features and structures that influence treatment plans. However, it is both labor intensive, and its effectiveness is operator dependent. Real-time accurate and robust automatic detection and tracking of anatomical structures while scanning would significantly impact diagnostic and therapeutic procedures to be consistent and efficient. In this paper, we propose a deep learning framework to automatically detect and track a specific anatomical target structure in ultrasound scans. Our framework is designed to be accurate and robust across subjects and imaging devices, to operate in real-time, and to not require a large training set. It maintains a localization precision and recall higher than 90% when trained on training sets that are as small as 20% in size of the original training set. The framework backbone is a weakly trained segmentation neural network based on U-Net. We tested the framework on two different ultrasound datasets with the aim to detect and track the Vagus nerve, where it outperformed current state-of-the-art real-time object detection networks.      
