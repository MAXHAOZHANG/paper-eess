# ArXiv eess --Fri, 1 Jan 2021
### 1.Universal Adaptive Control for Uncertain Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.15815.pdf)
>  Precise motion planning and control require accurate models which are often difficult, expensive, or time-consuming to obtain. Online model learning is an attractive approach that can handle model variations while achieving the desired level of performance. However, several model learning methods developed within adaptive nonlinear control are limited to certain systems or types of uncertainties. In particular, the so-called unmatched uncertainties pose significant problems for existing methods if the system is not in a particular form. This work presents an adaptive control framework for nonlinear systems with unmatched uncertainties that addresses several of the limitations of existing methods through two key innovations. The first is leveraging contraction theory and a new type of contraction metric that, when coupled with an adaptation law, is able to track feasible trajectories generated by an adapting reference model. The second is a natural modulation of the learning rate so the closed-loop system remains stable during learning transients. The proposed approach is more general than existing methods as it is able to handle unmatched uncertainties while only requiring the system be nominally contracting in closed-loop. Additionally, it can be used with learned feedback policies that are known to be contracting in some metric, facilitating transfer learning and bridging the sim2real gap. Simulation results demonstrate the effectiveness of the method.      
### 2.Estimating Uncertainty in Neural Networks for Cardiac MRI Segmentation: A Benchmark Study  [ :arrow_down: ](https://arxiv.org/pdf/2012.15772.pdf)
>  Convolutional neural networks (CNNs) have demonstrated promise in automated cardiac magnetic resonance imaging segmentation. However, when using CNNs in a large real world dataset, it is important to quantify segmentation uncertainty in order to know which segmentations could be problematic. In this work, we performed a systematic study of Bayesian and non-Bayesian methods for estimating uncertainty in segmentation neural networks. We evaluated Bayes by Backprop (BBB), Monte Carlo (MC) Dropout, and Deep Ensembles in terms of segmentation accuracy, probability calibration, uncertainty on out-of-distribution images, and segmentation quality control. We tested these algorithms on datasets with various distortions and observed that Deep Ensembles outperformed the other methods except for images with heavy noise distortions. For segmentation quality control, we showed that segmentation uncertainty is correlated with segmentation accuracy. With the incorporation of uncertainty estimates, we were able to reduce the percentage of poor segmentation to 5% by flagging 31% to 48% of the most uncertain images for manual review, substantially lower than random review of the results without using neural network uncertainty.      
### 3.Exploiting Shared Knowledge from Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.15564.pdf)
>  The novel Coronavirus disease (COVID-19) is a highly contagious virus and has spread all over the world, posing an extremely serious threat to all countries. Automatic lung infection segmentation from computed tomography (CT) plays an important role in the quantitative analysis of COVID-19. However, the major challenge lies in the inadequacy of annotated COVID-19 datasets. Currently, there are several public non-COVID lung lesion segmentation datasets, providing the potential for generalizing useful information to the related COVID-19 segmentation task. In this paper, we propose a novel relation-driven collaborative learning model for annotation-efficient COVID-19 CT lung infection segmentation. The network consists of encoders with the same architecture and a shared decoder. The general encoder is adopted to capture general lung lesion features based on multiple non-COVID lesions, while the target encoder is adopted to focus on task-specific features of COVID-19 infections. Features extracted from the two parallel encoders are concatenated for the subsequent decoder part. To thoroughly exploit shared knowledge between COVID and non-COVID lesions, we develop a collaborative learning scheme to regularize the relation consistency between extracted features of given input. Other than existing consistency-based methods that simply enforce the consistency of individual predictions, our method enforces the consistency of feature relation among samples, encouraging the model to explore semantic information from both COVID-19 and non-COVID cases. Extensive experiments on one public COVID-19 dataset and two public non-COVID datasets show that our method achieves superior segmentation performance compared with existing methods in the absence of sufficient high-quality COVID-19 annotations.      
### 4.Colonoscopy Polyp Detection: Domain Adaptation From Medical Report Images to Real-time Videos  [ :arrow_down: ](https://arxiv.org/pdf/2012.15531.pdf)
>  Automatic colorectal polyp detection in colonoscopy video is a fundamental task, which has received a lot of attention. Manually annotating polyp region in a large scale video dataset is time-consuming and expensive, which limits the development of deep learning techniques. A compromise is to train the target model by using labeled images and infer on colonoscopy videos. However, there are several issues between the image-based training and video-based inference, including domain differences, lack of positive samples, and temporal smoothness. To address these issues, we propose an Image-video-joint polyp detection network (Ivy-Net) to address the domain gap between colonoscopy images from historical medical reports and real-time videos. In our Ivy-Net, a modified mixup is utilized to generate training data by combining the positive images and negative video frames at the pixel level, which could learn the domain adaptive representations and augment the positive samples. Simultaneously, a temporal coherence regularization (TCR) is proposed to introduce the smooth constraint on feature-level in adjacent frames and improve polyp detection by unlabeled colonoscopy videos. For evaluation, a new large colonoscopy polyp dataset is collected, which contains 3056 images from historical medical reports of 889 positive patients and 7.5-hour videos of 69 patients (28 positive). The experiments on the collected dataset demonstrate that our Ivy-Net achieves the state-of-the-art result on colonoscopy video.      
### 5.Linear-Quadratic regulators for internal boundary control of lane-free automated vehicle traffic  [ :arrow_down: ](https://arxiv.org/pdf/2012.15519.pdf)
>  Lane-free vehicle movement has been recently proposed for connected automated vehicles (CAV) due to various potential advantages. One such advantage stems from the fact that incremental changes of the road width in lane-free traffic lead to corresponding incremental changes of the traffic flow capacity. Based on this property, the concept of internal boundary control was recently introduced to flexibly share the total road width and capacity among the two traffic directions of a highway in real-time, in response to the prevailing traffic conditions, so as to maximize the cross-road (both directions) infrastructure utilization. Feedback-based Linear-Quadratic regulators with or without Integral action (LQI and LQ regulators) are appropriately developed in this paper to efficiently address the internal boundary control problem. Simulation investigations, involving a realistic highway stretch and different demand scenarios, demonstrate that the proposed simple regulators are robust and similarly efficient as an open-loop nonlinear constrained optimal control solution, while circumventing the need for accurate modelling and external demand prediction      
### 6.Bayesian Federated Learning over Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.15486.pdf)
>  Federated learning is a privacy-preserving and distributed training method using heterogeneous data sets stored at local devices. Federated learning over wireless networks requires aggregating locally computed gradients at a server where the mobile devices send statistically distinct gradient information over heterogenous communication links. This paper proposes a Bayesian federated learning (BFL) algorithm to aggregate the heterogeneous quantized gradient information optimally in the sense of minimizing the mean-squared error (MSE). The idea of BFL is to aggregate the one-bit quantized local gradients at the server by jointly exploiting i) the prior distributions of the local gradients, ii) the gradient quantizer function, and iii) channel distributions. Implementing BFL requires high communication and computational costs as the number of mobile devices increases. To address this challenge, we also present an efficient modified BFL algorithm called scalable-BFL (SBFL). In SBFL, we assume a simplified distribution on the local gradient. Each mobile device sends its one-bit quantized local gradient together with two scalar parameters representing this distribution. The server then aggregates the noisy and faded quantized gradients to minimize the MSE. We provide a convergence analysis of SBFL for a class of non-convex loss functions. Our analysis elucidates how the parameters of communication channels and the gradient priors affect convergence. From simulations, we demonstrate that SBFL considerably outperforms the conventional sign stochastic gradient descent algorithm when training and testing neural networks using MNIST data sets over heterogeneous wireless networks.      
### 7.Economic Evaluation of Transformer Loss of Life Mitigation by Energy Storage and PV Generation  [ :arrow_down: ](https://arxiv.org/pdf/2012.15450.pdf)
>  High penetration of plug-in electric vehicles (PEVs) can potentially put the utility assets such as transformer under overload stress causing decrease in their lifetime. The decrease in PV and battery energy storage system (BESS) prices has made them viable solutions to mitigate this situation. In this paper, the economic aspect of their optimal coordination is studied to assess transformer hottest spot temperature (HST) and loss of life. Monte Carlo simulation is employed to provide synthetic data of PEVs load in a residential complex and model their stochastic behavior. For load, temperature, energy price and PV generation, real data for City of College Station, Texas, USA in 2018 is acquired and a case study is developed for one year. The results illustrate using BESS and PV is economically effective and mitigates distribution transformer loss of life.      
### 8.Survey of the Detection and Classification of Pulmonary Lesions via CT and X-Ray  [ :arrow_down: ](https://arxiv.org/pdf/2012.15442.pdf)
>  In recent years, the prevalence of several pulmonary diseases, especially the coronavirus disease 2019 (COVID-19) pandemic, has attracted worldwide attention. These diseases can be effectively diagnosed and treated with the help of lung imaging. With the development of deep learning technology and the emergence of many public medical image datasets, the diagnosis of lung diseases via medical imaging has been further improved. This article reviews pulmonary CT and X-ray image detection and classification in the last decade. It also provides an overview of the detection of lung nodules, pneumonia, and other common lung lesions based on the imaging characteristics of various lesions. Furthermore, this review introduces 26 commonly used public medical image datasets, summarizes the latest technology, and discusses current challenges and future research directions.      
### 9.Adaptive filters for the moving target indicator system  [ :arrow_down: ](https://arxiv.org/pdf/2012.15440.pdf)
>  Adaptive algorithms belong to an important class of algorithms used in radar target detection to overcome prior uncertainty of interference covariance. The contamination of the empirical covariance matrix by the useful signal leads to significant degradation of performance of this class of adaptive algorithms. Regularization, also known in radar literature as sample covariance loading, can be used to combat both ill conditioning of the original problem and contamination of the empirical covariance by the desired signal for the adaptive algorithms based on sample covariance matrix inversion. However, the optimum value of loading factor cannot be derived unless strong assumptions are made regarding the structure of covariance matrix and useful signal penetration model. Similarly, least mean square algorithm with linear constraint or without constraint, is also sensitive to the contamination of the learning sample with the target signal. We synthesize two approaches to improve the convergence of adaptive algorithms and protect them from the contamination of the learning sample with the signal from the target. The proposed approach is based on the maximization of empirical signal to interference plus noise ratio (SINR). Its effectiveness is demonstrated using simulated data.      
### 10.New Bag of Deep Visual Words based features to classify chest x-ray images for COVID-19 diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2012.15413.pdf)
>  Because the infection by Severe Acute Respiratory Syndrome Coronavirus 2 (COVID-19) causes the pneumonia-like effect in the lungs, the examination of chest x-rays can help to diagnose the diseases. For automatic analysis of images, they are represented in machines by a set of semantic features. Deep Learning (DL) models are widely used to extract features from images. General deep features may not be appropriate to represent chest x-rays as they have a few semantic regions. Though the Bag of Visual Words (BoVW) based features are shown to be more appropriate for x-ray type of images, existing BoVW features may not capture enough information to differentiate COVID-19 infection from other pneumonia-related infections. In this paper, we propose a new BoVW method over deep features, called Bag of Deep Visual Words (BoDVW), by removing the feature map normalization step and adding deep features normalization step on the raw feature maps. This helps to preserve the semantics of each feature map that may have important clues to differentiate COVID-19 from pneumonia. We evaluate the effectiveness of our proposed BoDVW features in chest x-rays classification using Support Vector Machine (SVM) to diagnose COVID-19. Our results on a publicly available COVID-19 x-ray dataset reveal that our features produce stable and prominent classification accuracy, particularly differentiating COVID-19 infection from other pneumonia, in shorter computation time compared to the state-of-the-art methods. Thus, our method could be a very useful tool for quick diagnosis of COVID-19 patients on a large scale.      
### 11.Wi-Chlorian: Wireless sensing and localization of contact forces on a space continuum  [ :arrow_down: ](https://arxiv.org/pdf/2012.15412.pdf)
>  Contact force is a natural way for humans to interact with the physical world around us. However, most of our interactions with the digital world are largely based on a simple binary sense of touch (contact or no contact). Similarly, when interacting with robots to perform complex tasks, such as surgery, we need to acquire the rich force information and contact location, to aid in the task. To address these issues, we present the design and fabrication of Wi-Chlorian, which is a 'wireless' sensors that can be attached to an object or robot, like a sticker. Wi-Chlorian's sensor transduces force magnitude and location into phase changes of an incident RF signal, which is reflected back to enable measurement of force and contact location. Wi-Chlorian's sensor is designed to support wide-band frequencies all the way up to 3GHz.We evaluate the force sensing wirelessly in different environments, including in-body like, and achieve force ac-curacy of 0.3N and contact location accuracy of 0.6mm.      
### 12.Two New Approaches to Optical IRSs: Schemes and Comparative Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.15398.pdf)
>  Oriented to the point-to-multipoint free space optical communication (FSO) scenarios, this paper analyzes the micro-mirror array and phased array-type optical intelligent reflecting surface (OIRS) in terms of control mode, power efficiency, and beam splitting. We build the physical models of the two types of OIRSs. Based on the models, the closed form solution of OIRSs' output power density distribution and power efficiency, along with their control algorithms have been derived. Then we propose the algorithms of beam splitting and multi-beam power allocation for two types of OIRSs. The channel fading in FSO system and the comparison of two types of OIRSs in actual systems are discussed according to the analytical results. Experiments and simulations are both presented to verify the feasibility of models and algorithms.      
### 13.FREA-Unet: Frequency-aware U-net for Modality Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2012.15397.pdf)
>  While Positron emission tomography (PET) imaging has been widely used in diagnosis of number of diseases, it has costly acquisition process which involves radiation exposure to patients. However, magnetic resonance imaging (MRI) is a safer imaging modality that does not involve patient's exposure to radiation. Therefore, a need exists for an efficient and automated PET image generation from MRI data. In this paper, we propose a new frequency-aware attention U-net for generating synthetic PET images. Specifically, we incorporate attention mechanism into different U-net layers responsible for estimating low/high frequency scales of the image. Our frequency-aware attention Unet computes the attention scores for feature maps in low/high frequency layers and use it to help the model focus more on the most important regions, leading to more realistic output images. Experimental results on 30 subjects from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset demonstrate good performance of the proposed model in PET image synthesis that achieved superior performance, both qualitative and quantitative, over current state-of-the-arts.      
### 14.Indoor Air Quality Improvement  [ :arrow_down: ](https://arxiv.org/pdf/2012.15387.pdf)
>  Poor indoor air quality can contribute to the development of various chronic respiratory diseases such as asthma, heart disease, and lung cancer. Since air quality is extremely difficult for humans to detect though sensory processing, there is a need for efficient ventilation systems that can provide a healthier environment. In this paper, we have designed an energy efficient ventilation system that predicts sensor occupancy patterns based on historical data to improve indoor air quality.      
### 15.H2NF-Net for Brain Tumor Segmentation using Multimodal MR Imaging: 2nd Place Solution to BraTS Challenge 2020 Segmentation Task  [ :arrow_down: ](https://arxiv.org/pdf/2012.15318.pdf)
>  In this paper, we propose a Hybrid High-resolution and Non-local Feature Network (H2NF-Net) to segment brain tumor in multimodal MR images. Our H2NF-Net uses the single and cascaded HNF-Nets to segment different brain tumor sub-regions and combines the predictions together as the final segmentation. We trained and evaluated our model on the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 dataset. The results on the test set show that the combination of the single and cascaded models achieved average Dice scores of 0.78751, 0.91290, and 0.85461, as well as Hausdorff distances ($95\%$) of 26.57525, 4.18426, and 4.97162 for the enhancing tumor, whole tumor, and tumor core, respectively. Our method won the second place in the BraTS 2020 challenge segmentation task out of nearly 80 participants.      
### 16.MRI brain tumor segmentation and uncertainty estimation using 3D-UNet architectures  [ :arrow_down: ](https://arxiv.org/pdf/2012.15294.pdf)
>  Automation of brain tumor segmentation in 3D magnetic resonance images (MRIs) is key to assess the diagnostic and treatment of the disease. In recent years, convolutional neural networks (CNNs) have shown improved results in the task. However, high memory consumption is still a problem in 3D-CNNs. Moreover, most methods do not include uncertainty information, which is especially critical in medical diagnosis. This work studies 3D encoder-decoder architectures trained with patch-based techniques to reduce memory consumption and decrease the effect of unbalanced data. The different trained models are then used to create an ensemble that leverages the properties of each model, thus increasing the performance. We also introduce voxel-wise uncertainty information, both epistemic and aleatoric using test-time dropout (TTD) and data-augmentation (TTA) respectively. In addition, a hybrid approach is proposed that helps increase the accuracy of the segmentation. The model and uncertainty estimation measurements proposed in this work have been used in the BraTS'20 Challenge for task 1 and 3 regarding tumor segmentation and uncertainty estimation.      
### 17.Automated Crater Detection from Co-registered Optical Images, Elevation Maps and Slope Maps using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.15281.pdf)
>  Impact craters are formed as a result of continuous impacts on the surface of planetary bodies. This paper proposes a novel way of simultaneously utilizing optical images, digital elevation maps (DEMs), and slope maps for automatic crater detection on the lunar surface. Mask R-CNN, tuned for the crater detection task, is utilized in this paper. Two catalogs, namely, Head-LROC and Robbins, are used for the performance evaluation. Exhaustive analysis of the detection results on the lunar surface has been performed with respect to both Head-LROC and Robbins catalog. With the Head-LROC catalog, which has relatively strict crater markings and larger possibility of missing craters, recall value of 94.28\% has been obtained as compared to 88.03\% for the baseline method. However, with respect to a manually marked exhaustive crater catalog based on relatively liberal marking, significant precision and recall values are obtained for different crater size ranges. The generalization capability of the proposed method in terms of crater detection on a different terrain with different input data type is also evaluated. We show that the proposed model trained on the lunar surface with optical images, DEMs and corresponding slope maps can be used to detect craters on the Martian surface even with entirely different input data type, such as thermal IR images from the Martian surface.      
### 18.Smart Rewritings of the Basic Equations for Quantitative Non-Linear Inverse Scattering  [ :arrow_down: ](https://arxiv.org/pdf/2012.15252.pdf)
>  Nonlinearity arising from mutual interactions is one of the two main difficulties to be addressed in inverse scattering. In this paper, we review and describe under a common rationale some approaches which have been introduced in literature in order to counteract nonlinearity. In particular, we focus on possible rewritings of the Lippman Schwinger basic equation such to reduce the degree of nonlinearity of inverse scattering problem. In detail, three different rewritings are discussed and compared by emphasizing similarities and the differences, and in the same rewriting spirit, we also summarize and discuss the Virtual Experiments framework. Then, some possible joint exploitations of the above concepts are introduced, discussed and tested against numerical examples.      
### 19.Automatic Polyp Segmentation using U-Net-ResNet50  [ :arrow_down: ](https://arxiv.org/pdf/2012.15247.pdf)
>  Polyps are the predecessors to colorectal cancer which is considered as one of the leading causes of cancer-related deaths worldwide. Colonoscopy is the standard procedure for the identification, localization, and removal of colorectal polyps. Due to variability in shape, size, and surrounding tissue similarity, colorectal polyps are often missed by the clinicians during colonoscopy. With the use of an automatic, accurate, and fast polyp segmentation method during the colonoscopy, many colorectal polyps can be easily detected and removed. The ``Medico automatic polyp segmentation challenge'' provides an opportunity to study polyp segmentation and build an efficient and accurate segmentation algorithm. We use the U-Net with pre-trained ResNet50 as the encoder for the polyp segmentation. The model is trained on Kvasir-SEG dataset provided for the challenge and tested on the organizer's dataset and achieves a dice coefficient of 0.8154, Jaccard of 0.7396, recall of 0.8533, precision of 0.8532, accuracy of 0.9506, and F2 score of 0.8272, demonstrating the generalization ability of our model.      
### 20.DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.15245.pdf)
>  Colonoscopy is the gold standard for examination and detection of colorectal polyps. Localization and delineation of polyps can play a vital role in treatment (e.g., surgical planning) and prognostic decision making. Polyp segmentation can provide detailed boundary information for clinical analysis. Convolutional neural networks have improved the performance in colonoscopy. However, polyps usually possess various challenges, such as intra-and inter-class variation and noise. While manual labeling for polyp assessment requires time from experts and is prone to human error (e.g., missed lesions), an automated, accurate, and fast segmentation can improve the quality of delineated lesion boundaries and reduce missed rate. The Endotect challenge provides an opportunity to benchmark computer vision methods by training on the publicly available Hyperkvasir and testing on a separate unseen dataset. In this paper, we propose a novel architecture called ``DDANet'' based on a dual decoder attention network. Our experiments demonstrate that the model trained on the Kvasir-SEG dataset and tested on an unseen dataset achieves a dice coefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of 0.8577, demonstrating the generalization ability of our model.      
### 21.Medico Multimedia Task at MediaEval 2020: Automatic Polyp Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.15244.pdf)
>  Colorectal cancer is the third most common cause of cancer worldwide. According to Global cancer statistics 2018, the incidence of colorectal cancer is increasing in both developing and developed countries. Early detection of colon anomalies such as polyps is important for cancer prevention, and automatic polyp segmentation can play a crucial role for this. Regardless of the recent advancement in early detection and treatment options, the estimated polyp miss rate is still around 20\%. Support via an automated computer-aided diagnosis system could be one of the potential solutions for the overlooked polyps. Such detection systems can help low-cost design solutions and save doctors time, which they could for example use to perform more patient examinations. In this paper, we introduce the 2020 Medico challenge, provide some information on related work and the dataset, describe the task and evaluation metrics, and discuss the necessity of organizing the Medico challenge.      
### 22.Learning to Optimize Energy Efficiency in Energy Harvesting Wireless Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.15203.pdf)
>  We study wireless power transmission by an energy source to multiple energy harvesting nodes with the aim to maximize the energy efficiency. The source transmits energy to the nodes using one of the available power levels in each time slot and the nodes transmit information back to the energy source using the harvested energy. The source does not have any channel state information and it only knows whether a received codeword from a given node was successfully decoded or not. With this limited information, the source has to learn the optimal power level that maximizes the energy efficiency of the network. We model the problem as a stochastic Multi-Armed Bandits problem and develop an Upper Confidence Bound based algorithm, which learns the optimal transmit power of the energy source that maximizes the energy efficiency. Numerical results validate the performance guarantees of the proposed algorithm and show significant gains compared to the benchmark schemes.      
### 23.A GA-based Approach to Eco-driving of Electric Vehicles Considering Regenerative Braking  [ :arrow_down: ](https://arxiv.org/pdf/2012.15195.pdf)
>  As the deployment of low carbon transportation technologies, specifically electric vehicles (EVs), is increasing, the concept of their eco-driving is gaining significant attention. Contrary to the eco-driving techniques used in conventional internal combustion engine vehicles that do not have the capability of regenerative braking, this paper proposes a genetic algorithm (GA)-based eco-driving technique for EVs considering regenerative braking. In the proposed approach, the optimal or near-optimal combination of variables in the driving cycle of EVs is searched using GA. The proposed approach starts by generating an initial population of chromosomes, where all variables under consideration are encoded in each chromosome. This population of chromosomes is passed through crossover, mutation, and elitist-based selection over a certain number of generations, which results in a driving cycle with the least energy consumption. The proposed method is verified using two case studies. The results of the case studies show the capability of the proposed method in computing the minimum energy driving cycle.      
### 24.Exploring Large Context for Cerebral Aneurysm Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.15136.pdf)
>  Automated segmentation of aneurysms from 3D CT is important for the diagnosis, monitoring, and treatment planning of the cerebral aneurysm disease. This short paper briefly presents the main technique details of the aneurysm segmentation method in the MICCAI 2020 CADA challenge. The main contribution is that we configure the 3D U-Net with a large patch size, which can obtain the large context. Our method ranked second on the MICCAI 2020 CADA testing dataset with an average Jaccard of 0.7593. Our code and trained models are publicly available at \url{<a class="link-external link-https" href="https://github.com/JunMa11/CADA2020" rel="external noopener nofollow">this https URL</a>}.      
### 25.Fast Hyperspectral Image Recovery via Non-iterative Fusion of Dual-Camera Compressive Hyperspectral Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2012.15104.pdf)
>  Coded aperture snapshot spectral imaging (CASSI) is a promising technique to capture the three-dimensional hyperspectral image (HSI) using a single coded two-dimensional (2D) measurement, in which algorithms are used to perform the inverse problem. Due to the ill-posed nature, various regularizers have been exploited to reconstruct the 3D data from the 2D measurement. Unfortunately, the accuracy and computational complexity are unsatisfied. One feasible solution is to utilize additional information such as the RGB measurement in CASSI. Considering the combined CASSI and RGB measurement, in this paper, we propose a new fusion model for the HSI reconstruction. We investigate the spectral low-rank property of HSI composed of a spectral basis and spatial coefficients. Specifically, the RGB measurement is utilized to estimate the coefficients, meanwhile the CASSI measurement is adopted to provide the orthogonal spectral basis. We further propose a patch processing strategy to enhance the spectral low-rank property of HSI. The proposed model neither requires non-local processing or iteration, nor the spectral sensing matrix of the RGB detector. Extensive experiments on both simulated and real HSI dataset demonstrate that our proposed method outperforms previous state-of-the-art not only in quality but also speeds up the reconstruction more than 5000 times.      
### 26.Visual counterexample explanation for model checking with Oeritte  [ :arrow_down: ](https://arxiv.org/pdf/2012.15097.pdf)
>  Despite being one of the most reliable approaches for ensuring system correctness, model checking requires auxiliary tools to fully avail. In this work, we tackle the issue of its results being hard to interpret and present Oeritte, a tool for automatic visual counterexample explanation for function block diagrams. To learn what went wrong, the user can inspect a parse tree of the violated LTL formula and a table view of a counterexample, where important variables are highlighted. Then, on the function block diagram of the system under verification, they can receive a visualization of causality relationships between the calculated values of interest and intermediate results or inputs of the function block diagram. Thus, Oeritte serves to decrease formal model and specification debugging efforts along with making model checking more utilizable for complex industrial systems.      
### 27.Analysis of Frequency Agile Radar from the Phase Transition Theory  [ :arrow_down: ](https://arxiv.org/pdf/2012.15089.pdf)
>  FAR has improved anti-jamming performance over traditional pulse-Doppler radars under complex electromagnetic circumstances. To reconstruct the range-Doppler information in FAR, many compressed sensing (CS) methods including standard and block sparse recovery have been applied. In this paper, we study phase transitions of range-Doppler recovery in FAR using CS. In particular, we derive closed-form phase transition curves associated with block sparse recovery and complex Gaussian matrices, based on prior results of standard sparse recovery under real Gaussian matrices. We further approximate the obtained curves with elementary functions of radar and target parameters, facilitating practical applications of these curves. Our results indicate that block sparse recovery outperforms the standard counterpart when targets occupy more than one range cell, which are often referred to as extended targets. Simulations validate the availability of these curves and their approximations in FAR, which benefit the design of the radar parameters.      
### 28.Unpaired Image Enhancement with Quality-Attention Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2012.15052.pdf)
>  In this work, we aim to learn an unpaired image enhancement model, which can enrich low-quality images with the characteristics of high-quality images provided by users. We propose a quality attention generative adversarial network (QAGAN) trained on unpaired data based on the bidirectional Generative Adversarial Network (GAN) embedded with a quality attention module (QAM). The key novelty of the proposed QAGAN lies in the injected QAM for the generator such that it learns domain-relevant quality attention directly from the two domains. More specifically, the proposed QAM allows the generator to effectively select semantic-related characteristics from the spatial-wise and adaptively incorporate style-related attributes from the channel-wise, respectively. Therefore, in our proposed QAGAN, not only discriminators but also the generator can directly access both domains which significantly facilitates the generator to learn the mapping function. Extensive experimental results show that, compared with the state-of-the-art methods based on unpaired learning, our proposed method achieves better performance in both objective and subjective evaluations.      
### 29.VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations  [ :arrow_down: ](https://arxiv.org/pdf/2012.15029.pdf)
>  Most of the existing chest X-ray datasets include labels from a list of findings without specifying their locations on the radiographs. This limits the development of machine learning algorithms for the detection and localization of chest abnormalities. In this work, we describe a dataset of more than 100,000 chest X-ray scans that were retrospectively collected from two major hospitals in Vietnam. Out of this raw data, we release 18,000 images that were manually annotated by a total of 17 experienced radiologists with 22 local labels of rectangles surrounding abnormalities and 6 global labels of suspected diseases. The released dataset is divided into a training set of 15,000 and a test set of 3,000. Each scan in the training set was independently labeled by 3 radiologists, while each scan in the test set was labeled by the consensus of 5 radiologists. We designed and built a labeling platform for DICOM images to facilitate these annotation procedures. All images are made publicly available in DICOM format in company with the labels of the training set. The labels of the test set are hidden at the time of writing this paper as they will be used for benchmarking machine learning algorithms on an open platform.      
### 30.A Review of Machine Learning Techniques for Applied Eye Fundus and Tongue Digital Image Processing with Diabetes Management System  [ :arrow_down: ](https://arxiv.org/pdf/2012.15025.pdf)
>  Diabetes is a global epidemic and it is increasing at an alarming rate. The International Diabetes Federation (IDF) projected that the total number of people with diabetes globally may increase by 48%, from 425 million (year 2017) to 629 million (year 2045). Moreover, diabetes had caused millions of deaths and the number is increasing drastically. Therefore, this paper addresses the background of diabetes and its complications. In addition, this paper investigates innovative applications and past researches in the areas of diabetes management system with applied eye fundus and tongue digital images. Different types of existing applied eye fundus and tongue digital image processing with diabetes management systems in the market and state-of-the-art machine learning techniques from previous literature have been reviewed. The implication of this paper is to have an overview in diabetic research and what new machine learning techniques can be proposed in solving this global epidemic.      
### 31.Control Barriers in Bayesian Learning of System Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2012.14964.pdf)
>  This paper focuses on learning a model of system dynamics online while satisfying safety constraints. Our objective is to avoid offline system identification or hand-specified models and allow a system to safely and autonomously estimate and adapt its own model during operation. Given streaming observations of the system state, we use Bayesian learning to obtain a distribution over the system dynamics. Specifically, we use a matrix variate Gaussian process (MVGP) regression approach with efficient covariance factorization to learn the drift and input gain terms of a nonlinear control-affine system. The MVGP distribution is then used to optimize the system behavior and ensure safety with high probability, by specifying control Lyapunov function (CLF) and control barrier function (CBF) chance constraints. We show that a safe control policy can be synthesized for systems with arbitrary relative degree and probabilistic CLF-CBF constraints by solving a second order cone program (SOCP). Finally, we extend our design to a self-triggering formulation, adaptively determining the time at which a new control input needs to be applied in order to guarantee safety.      
### 32.Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: theory, implementation and analysis on standard tasks  [ :arrow_down: ](https://arxiv.org/pdf/2012.14952.pdf)
>  The recently proposed VBx diarization method uses a Bayesian hidden Markov model to find speaker clusters in a sequence of x-vectors. In this work we perform an extensive comparison of performance of the VBx diarization with other approaches in the literature and we show that VBx achieves superior performance on three of the most popular datasets for evaluating diarization: CALLHOME, AMI and DIHARDII datasets. Further, we present for the first time the derivation and update formulae for the VBx model, focusing on the efficiency and simplicity of this model as compared to the previous and more complex BHMM model working on frame-by-frame standard Cepstral features. Together with this publication, we release the recipe for training the x-vector extractors used in our experiments on both wide and narrowband data, and the VBx recipes that attain state-of-the-art performance on all three datasets. Besides, we point out the lack of a standardized evaluation protocol for AMI dataset and we propose a new protocol for both Beamformed and Mix-Headset audios based on the official AMI partitions and transcriptions.      
### 33.Dynamic State Estimation for Power System Control and Protection  [ :arrow_down: ](https://arxiv.org/pdf/2012.14927.pdf)
>  Dynamic state estimation (DSE) accurately tracks the dynamics of a power system and provides the evolution of the system state in real-time. This paper focuses on the control and protection applications of DSE, comprehensively presenting different facets of control and protection challenges arising in modern power systems. It is demonstrated how these challenges are effectively addressed with DSE-enabled solutions. As precursors to these solutions, reformulation of DSE considering both synchrophasor and sampled value measurements and comprehensive comparisons of DSE and observers have been presented. The usefulness and necessity of DSE based solutions in ensuring system stability, reliable protection and security, and resilience by revamping of control and protection methods are shown through examples, practical applications, and suggestions for further development.      
### 34.Predictive Multi-Microgrid Generation Maintenance: Formulation and Impact on Operations &amp; Resilience  [ :arrow_down: ](https://arxiv.org/pdf/2012.14926.pdf)
>  Industrial sensor data provides significant insights into the failure risks of microgrid generation assets. In traditional applications, these sensor-driven risks are used to generate alerts that initiate maintenance actions without considering their impact on operational aspects. The focus of this paper is to propose a framework that i) builds a seamless integration between sensor data and operational &amp; maintenance drivers, and ii) demonstrates the value of this integration for improving multiple aspects of microgrid operations. The proposed framework offers an integrated stochastic optimization model that jointly optimizes operations and maintenance in a multi-microgrid setting. Maintenance decisions identify optimal crew routing, opportunistic maintenance, and repair schedules as a function of dynamically evolving sensor-driven predictions on asset life. Operational decisions identify commitment and generation from a fleet of distributed energy resources, storage, load management, as well as power transactions with the main grid and neighboring microgrids. Operational uncertainty from renewable generation, demand, and market prices are explicitly modeled through scenarios in the optimization model. We use the structure of the model to develop a decomposition-based solution algorithm to ensure computational scalability. The proposed model provides significant improvements in reliability and enhances a range of operational outcomes, including costs, renewables, generation availability, and resilience.      
### 35.Infinite-Horizon Linear-Quadratic-Gaussian Control with Costly Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2012.14925.pdf)
>  In this paper, we consider an infinite horizon Linear-Quadratic-Gaussian control problem with controlled and costly measurements. A control strategy and a measurement strategy are co-designed to optimize the trade-off among control performance, actuating costs, and measurement costs. We address the co-design and co-optimization problem by establishing a dynamic programming equation with controlled lookahead. By leveraging the dynamic programming equation, we fully characterize the optimal control strategy and the measurement strategy analytically. The optimal control is linear in the state estimate that depends on the measurement strategy. We prove that the optimal measurement strategy is independent of the measured state and is periodic. And the optimal period length is determined by the cost of measurements and system parameters. We demonstrate the potential application of the co-design and co-optimization problem in an optimal self-triggered control paradigm. Two examples are provided to show the effectiveness of the optimal measurement strategy in reducing the overhead of measurements while keeping the system performance.      
### 36.Tube-enhanced Multi-stage MPC for Flexible Robust Control of Constrained Linear Systems with Additive and Parametric Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2012.14848.pdf)
>  The trade-off between optimality and complexity has been one of the most important challenges in the field of robust Model Predictive Control (MPC). To address the challenge, we propose a flexible robust MPC scheme by synergizing the multi-stage and tube-based MPC approaches. The key idea is to exploit the non-conservatism of the multi-stage MPC and the simplicity of the tube-based MPC. The proposed scheme provides two options for the user to determine the trade-off depending on the application: the choice of the robust horizon and the classification of the uncertainties. Beyond the robust horizon, the branching of the scenario-tree employed in multi-stage MPC is avoided with the help of tubes. The growth of the problem size with respect to the number of uncertainties is reduced by handling \emph{small} uncertainties via an invariant tube that can be computed offline. This results in linear growth of the problem size beyond the robust horizon and no growth of the problem size concerning small magnitude uncertainties. The proposed approach helps to achieve a desired trade-off between optimality and complexity compared to existing robust MPC approaches. We show that the proposed approach is robustly asymptotically stable. Its advantages are demonstrated for a CSTR example.      
### 37.Resource Allocation for NOMA-based LPWA Networks Powered by Energy Harvesting  [ :arrow_down: ](https://arxiv.org/pdf/2012.14834.pdf)
>  In this paper, we explore perpetual, scalable, Low-powered Wide-area networks (LPWA). Specifically we focus on the uplink transmissions of non-orthogonal multiple access (NOMA)-based LPWA networks consisting of multiple self-powered nodes and a NOMA-based single gateway. The self-powered LPWA nodes use the "harvest-then-transmit" protocol where they harvest energy from ambient sources (solar and radio frequency signals), then transmit their signals. The main features of the studied LPWA network are different transmission times-on-air, multiple uplink transmission attempts, and duty cycle restrictions. The aim of this work is to maximize the time-averaged sum of the uplink transmission rates by optimizing the transmission time-on-air allocation, the energy harvesting time allocation and the power allocation; subject to a maximum transmit power and to the availability of the harvested energy. We propose a low complex solution which decouples the optimization problem into three sub-problems: we assign the LPWA node transmission times (using either the fair or unfair approaches), we optimize the energy harvesting (EH) times using a one-dimensional search method, and optimize the transmit powers using a concave-convex (CCCP) procedure. In the simulation results, we focus on Long Range (LoRa) networks as a practical example LPWA network. We validate our proposed solution and we observe a $15\%$ performance improvement when using NOMA.      
### 38.Quality-driven Variable Frame-Rate for Green Video Coding in Broadcast Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.14796.pdf)
>  The Digital Video Broadcasting (DVB) has proposed to introduce the Ultra-High Definition services in three phases: UHD-1 phase 1, UHD-1 phase 2 and UHD-2. The UHD-1 phase 2 specification includes several new features such as High Dynamic Range (HDR) and High Frame-Rate (HFR). It has been shown in several studies that HFR (+100 fps) enhances the perceptual quality and that this quality enhancement is content-dependent. On the other hand, HFR brings several challenges to the transmission chain including codec complexity increase and bit-rate overhead, which may delay or even prevent its deployment in the broadcast echo-system. In this paper, we propose a Variable Frame Rate (VFR) solution to determine the minimum (critical) frame-rate that preserves the perceived video quality of HFR video. The frame-rate determination is modeled as a 3-class classification problem which consists in dynamically and locally selecting one frame-rate among three: 30, 60 and 120 frames per second. Two random forests classifiers are trained with a ground truth carefully built by experts for this purpose. The subjective results conducted on ten HFR video contents, not included in the training set, clearly show the efficiency of the proposed solution enabling to locally determine the lowest possible frame-rate while preserving the quality of the HFR content. Moreover, our VFR solution enables significant bit-rate savings and complexity reductions at both encoder and decoder sides.      
### 39.Detection of Lexical Stress Errors in Non-native (L2) English with Data Augmentation and Attention  [ :arrow_down: ](https://arxiv.org/pdf/2012.14788.pdf)
>  This paper describes two novel complementary techniques that improve the detection of lexical stress errors in non-native (L2) English speech: attention-based feature extraction and data augmentation based on Neural Text-To-Speech (TTS). In a classical approach, audio features are usually extracted from fixed regions of speech such as syllable nucleus. We propose an attention-based deep learning model that automatically derives optimal syllable-level representation from frame-level and phoneme-level audio features. Training this model is challenging because of the limited amount of incorrect stress patterns. To solve this problem, we propose to augment the training set with incorrectly stressed words generated with Neural TTS. Combining both techniques achieves 94.8\% precision and 49.2\% recall for the detection of incorrectly stressed words in L2 English speech of Slavic speakers.      
### 40.Semi-supervised Cardiac Image Segmentation via Label Propagation and Style Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2012.14785.pdf)
>  Accurate segmentation of cardiac structures can assist doctors to diagnose diseases, and to improve treatment planning, which is highly demanded in the clinical practice. However, the shortage of annotation and the variance of the data among different vendors and medical centers restrict the performance of advanced deep learning methods. In this work, we present a fully automatic method to segment cardiac structures including the left (LV) and right ventricle (RV) blood pools, as well as for the left ventricular myocardium (MYO) in MRI volumes. Specifically, we design a semi-supervised learning method to leverage unlabelled MRI sequence timeframes by label propagation. Then we exploit style transfer to reduce the variance among different centers and vendors for more robust cardiac image segmentation. We evaluate our method in the M&amp;Ms challenge 7 , ranking 2nd place among 14 competitive teams.      
### 41.Development and evaluation of a 3D annotation software for interactive COVID-19 lesion segmentation in chest CT  [ :arrow_down: ](https://arxiv.org/pdf/2012.14752.pdf)
>  Segmentation of COVID-19 lesions from chest CT scans is of great importance for better diagnosing the disease and investigating its extent. However, manual segmentation can be very time consuming and subjective, given the lesions' large variation in shape, size and position. On the other hand, we still lack large manually segmented datasets that could be used for training machine learning-based models for fully automatic segmentation. In this work, we propose a new interactive and user-friendly tool for COVID-19 lesion segmentation, which works by alternating automatic steps (based on level-set segmentation and statistical shape modeling) with manual correction steps. The present software was tested by two different expertise groups: one group of three radiologists and one of three users with an engineering background. Promising segmentation results were obtained by both groups, which achieved satisfactory agreement both between- and within-group. Moreover, our interactive tool was shown to significantly speed up the lesion segmentation process, when compared to fully manual segmentation. Finally, we investigated inter-observer variability and how it is strongly influenced by several subjective factors, showing the importance for AI researchers and clinical doctors to be aware of the uncertainty in lesion segmentation results.      
### 42.Leveraging AI and Intelligent Reflecting Surface for Energy-Efficient Communication in 6G IoT  [ :arrow_down: ](https://arxiv.org/pdf/2012.14716.pdf)
>  The ever-increasing data traffic, various delay-sensitive services, and the massive deployment of energy-limited Internet of Things (IoT) devices have brought huge challenges to the current communication networks, motivating academia and industry to move to the sixth-generation (6G) network. With the powerful capability of data transmission and processing, 6G is considered as an enabler for IoT communication with low latency and energy cost. In this paper, we propose an artificial intelligence (AI) and intelligent reflecting surface (IRS) empowered energy-efficiency communication system for 6G IoT. First, we design a smart and efficient communication architecture including the IRS-aided data transmission and the AI-driven network resource management mechanisms. Second, an energy efficiency-maximizing model under given transmission latency for 6G IoT system is formulated, which jointly optimizes the settings of all communication participants, i.e. IoT transmission power, IRS-reflection phase shift, and BS detection matrix. Third, a deep reinforcement learning (DRL) empowered network resource control and allocation scheme is proposed to solve the formulated optimization model. Based on the network and channel status, the DRL-enabled scheme facilities the energy-efficiency and low-latency communication. Finally, experimental results verified the effectiveness of our proposed communication system for 6G IoT.      
### 43.Universal Silicon Microwave Photonic Spectral Shaper  [ :arrow_down: ](https://arxiv.org/pdf/2012.14696.pdf)
>  Optical modulation plays arguably the utmost important role in microwave photonic (MWP) systems. Precise synthesis of modulated optical spectra dictates virtually all aspects of MWP system quality including loss, noise figure, linearity, and the types of functionality that can be executed. But for such a critical function, the versatility to generate and transform analog optical modulation is severely lacking, blocking the pathways to truly unique MWP functions including ultra-linear links and low-loss high rejection filters. Here we demonstrate versatile RF photonic spectrum synthesis in an all-integrated silicon photonic circuit, enabling electrically-tailorable universal analog modulation transformation. We show a series of unprecedented RF filtering experiments through monolithic integration of the spectrum-synthesis circuit with a network of reconfigurable ring resonators.      
### 44.Annotation-Efficient Learning for Medical Image Segmentation based on Noisy Pseudo Labels and Adversarial Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.14584.pdf)
>  Despite that deep learning has achieved state-of-the-art performance for medical image segmentation, its success relies on a large set of manually annotated images for training that are expensive to acquire. In this paper, we propose an annotation-efficient learning framework for segmentation tasks that avoids annotations of training images, where we use an improved Cycle-Consistent Generative Adversarial Network (GAN) to learn from a set of unpaired medical images and auxiliary masks obtained either from a shape model or public datasets. We first use the GAN to generate pseudo labels for our training images under the implicit high-level shape constraint represented by a Variational Auto-encoder (VAE)-based discriminator with the help of the auxiliary masks, and build a Discriminator-guided Generator Channel Calibration (DGCC) module which employs our discriminator's feedback to calibrate the generator for better pseudo labels. To learn from the pseudo labels that are noisy, we further introduce a noise-robust iterative learning method using noise-weighted Dice loss. We validated our framework with two situations: objects with a simple shape model like optic disc in fundus images and fetal head in ultrasound images, and complex structures like lung in X-Ray images and liver in CT images. Experimental results demonstrated that 1) Our VAE-based discriminator and DGCC module help to obtain high-quality pseudo labels. 2) Our proposed noise-robust learning method can effectively overcome the effect of noisy pseudo labels. 3) The segmentation performance of our method without using annotations of training images is close or even comparable to that of learning from human annotations.      
### 45.Synchronization with prescribed transient behavior: Heterogeneous multi-agent systems under funnel coupling  [ :arrow_down: ](https://arxiv.org/pdf/2012.14580.pdf)
>  In this paper, we introduce a nonlinear time-varying coupling law, which can be designed in a fully decentralized manner and achieves approximate synchronization with arbitrary precision, under only mild assumptions on the individual vector fields and the underlying graph structure. Meanwhile, we consider undirected graphs and scalar input affine systems for simplicity. The proposed coupling law is motivated by the so called funnel control method studied in adaptive control under the observation that arbitrary precision synchronization can be achieved for heterogeneous multi-agent systems by the high-gain coupling, and thus, we follow to call our coupling law as `(node-wise) funnel coupling.' By getting out of the conventional proof technique in the funnel control study, we now can obtain even asymptotic synchronization with the same funnel coupling law. Moreover, the emergent collective behavior that arises for a heterogeneous multi-agent system when enforcing arbitrary precision synchronization by the proposed funnel coupling law, has been analyzed in this paper. In particular, we introduce a single scalar dynamics called `emergent dynamics' that is capable of illustrating the emergent synchronized behavior by its solution trajectory. Characterization of the emergent dynamics is important because, for instance, one can design the emergent dynamics first such that the solution trajectory behaves as desired, and then, provide a design guideline to each agent so that the constructed vector fields yield the desired emergent dynamics. A particular example illustrating the utility of the emergent dynamics is given also in the paper as a distributed median solver.      
### 46.Ensembled ResUnet for Anatomical Brain Barriers Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.14567.pdf)
>  Accuracy segmentation of brain structures could be helpful for glioma and radiotherapy planning. However, due to the visual and anatomical differences between different modalities, the accurate segmentation of brain structures becomes challenging. To address this problem, we first construct a residual block based U-shape network with a deep encoder and shallow decoder, which can trade off the framework performance and efficiency. Then, we introduce the Tversky loss to address the issue of the class imbalance between different foreground and the background classes. Finally, a model ensemble strategy is utilized to remove outliers and further boost performance.      
### 47.Myocardial Segmentation of Cardiac MRI Sequences with Temporal Consistency for Coronary Artery Disease Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2012.14564.pdf)
>  Coronary artery disease (CAD) is the most common cause of death globally, and its diagnosis is usually based on manual myocardial segmentation of Magnetic Resonance Imaging (MRI) sequences. As the manual segmentation is tedious, time-consuming and with low applicability, automatic myocardial segmentation using machine learning techniques has been widely explored recently. However, almost all the existing methods treat the input MRI sequences independently, which fails to capture the temporal information between sequences, e.g., the shape and location information of the myocardium in sequences along time. In this paper, we propose a myocardial segmentation framework for sequence of cardiac MRI (CMR) scanning images of left ventricular cavity, right ventricular cavity, and myocardium. Specifically, we propose to combine conventional networks and recurrent networks to incorporate temporal information between sequences to ensure temporal consistent. We evaluated our framework on the Automated Cardiac Diagnosis Challenge (ACDC) dataset. Experiment results demonstrate that our framework can improve the segmentation accuracy by up to 2% in Dice coefficient.      
### 48.Cascaded Framework for Automatic Evaluation of Myocardial Infarction from Delayed-Enhancement Cardiac MRI  [ :arrow_down: ](https://arxiv.org/pdf/2012.14556.pdf)
>  Automatic evaluation of myocardium and pathology plays an important role in the quantitative analysis of patients suffering from myocardial infarction. In this paper, we present a cascaded convolutional neural network framework for myocardial infarction segmentation and classification in delayed-enhancement cardiac MRI. Specifically, we first use a 2D U-Net to segment the whole heart, including the left ventricle and the myocardium. Then, we crop the whole heart as a region of interest (ROI). Finally, a new 2D U-Net is used to segment the infraction and no-reflow areas in the whole heart ROI. The segmentation method can be applied to the classification task where the segmentation results with the infraction or no-reflow areas are classified as pathological cases. Our method took second place in the MICCAI 2020 EMIDEC segmentation task with Dice scores of 86.28%, 62.24%, and 77.76% for myocardium, infraction, and no-reflow areas, respectively, and first place in the classification task with an accuracy of 92%.      
### 49.Magneto-Mechanical Transmitters for Ultra-LowFrequency Near-field Communication  [ :arrow_down: ](https://arxiv.org/pdf/2012.14548.pdf)
>  Electromagnetic signals in the ultra-low frequency (ULF) range below 3 kHz are well suited for underwater and underground wireless communication thanks to low signal attenuation and high penetration depth. However, it is challenging to design ULF transmitters that are simultaneously compact and energy efficient using traditional approaches, e.g., using coils or dipole antennas. Recent works have considered magneto-mechanical alternatives, in which ULF magnetic fields are generated using the motion of permanent magnets, since they enable extremely compact ULF transmitters that can operate with low energy consumption and are suitable for human-portable applications. Here we explore the design and operating principles of resonant magneto-mechanical transmitters (MMT) that operate over frequencies spanning a few 10's of Hz up to 1 kHz. We experimentally demonstrate two types of MMT designs using both single-rotor and multi-rotor architectures. We study the nonlinear electro-mechanical dynamics of MMTs using point dipole approximation and magneto-static simulations. We further experimentally explore techniques to control the operation frequency and demonstrate amplitude modulation up to 10 bits-per-second.      
### 50.Seamless Active Morphing Wing Simultaneous Gust and Maneuver Load Alleviation  [ :arrow_down: ](https://arxiv.org/pdf/2012.14520.pdf)
>  This paper deals with the simultaneous gust and maneuver load alleviation problem of a seamless active morphing wing. The incremental nonlinear dynamic inversion with quadratic programming control allocation and virtual shape functions (denoted as INDI-QP-V) is proposed to fulfill this goal. The designed control allocator provides an optimal solution while satisfying actuator position constraints, rate constraints, and relative position constraints. Virtual shape functions ensure the smoothness of the morphing wing at every moment. In the presence of model uncertainties, external disturbances, and control allocation errors, the closed-loop stability is guaranteed in the Lyapunov sense. Wind tunnel tests demonstrate that INDI-QP-V can make the seamless wing morph actively to resist ``1-cos'' gusts and modify the spanwise lift distribution at the same time. The wing root shear force and bending moment have been alleviated by more than 44% despite unexpected actuator fault and nonlinear backlash. Moreover, during the experiment, all the input constraints were satisfied, the wing shape was smooth all the time, and the control law was executed in real time. Furthermore, as compared to the linear quadratic Gaussian (LQG) control, the hardware implementation of INDI-QP-V is easier; the robust performance of INDI-QP-V is also superior.      
### 51.Comparison of different CNNs for breast tumor classification from ultrasound images  [ :arrow_down: ](https://arxiv.org/pdf/2012.14517.pdf)
>  Breast cancer is one of the deadliest cancer worldwide. Timely detection could reduce mortality rates. In the clinical routine, classifying benign and malignant tumors from ultrasound (US) imaging is a crucial but challenging task. An automated method, which can deal with the variability of data is therefore needed. <br>In this paper, we compared different Convolutional Neural Networks (CNNs) and transfer learning methods for the task of automated breast tumor classification. The architectures investigated in this study were VGG-16 and Inception V3. Two different training strategies were investigated: the first one was using pretrained models as feature extractors and the second one was to fine-tune the pre-trained models. A total of 947 images were used, 587 corresponded to US images of benign tumors and 360 with malignant tumors. 678 images were used for the training and validation process, while 269 images were used for testing the models. <br>Accuracy and Area Under the receiver operating characteristic Curve (AUC) were used as performance metrics. The best performance was obtained by fine tuning VGG-16, with an accuracy of 0.919 and an AUC of 0.934. The obtained results open the opportunity to further investigation with a view of improving cancer detection.      
### 52.SASSI -- Super-Pixelated Adaptive Spatio-Spectral Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2012.14495.pdf)
>  We introduce a novel video-rate hyperspectral imager with high spatial, and temporal resolutions. Our key hypothesis is that spectral profiles of pixels in a super-pixel of an oversegmented image tend to be very similar. Hence, a scene-adaptive spatial sampling of an hyperspectral scene, guided by its super-pixel segmented image, is capable of obtaining high-quality reconstructions. To achieve this, we acquire an RGB image of the scene, compute its super-pixels, from which we generate a spatial mask of locations where we measure high-resolution spectrum. The hyperspectral image is subsequently estimated by fusing the RGB image and the spectral measurements using a learnable guided filtering approach. Due to low computational complexity of the superpixel estimation step, our setup can capture hyperspectral images of the scenes with little overhead over traditional snapshot hyperspectral cameras, but with significantly higher spatial and spectral resolutions. We validate the proposed technique with extensive simulations as well as a lab prototype that measures hyperspectral video at a spatial resolution of $600 \times 900$ pixels, at a spectral resolution of 10 nm over visible wavebands, and achieving a frame rate at $18$fps.      
### 53.Real-time Webcam Heart-Rate and Variability Estimation with Clean Ground Truth for Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2012.15846.pdf)
>  Remote photo-plethysmography (rPPG) uses a camera to estimate a person's heart rate (HR). Similar to how heart rate can provide useful information about a person's vital signs, insights about the underlying physio/psychological conditions can be obtained from heart rate variability (HRV). HRV is a measure of the fine fluctuations in the intervals between heart beats. However, this measure requires temporally locating heart beats with a high degree of precision. We introduce a refined and efficient real-time rPPG pipeline with novel filtering and motion suppression that not only estimates heart rates, but also extracts the pulse waveform to time heart beats and measure heart rate variability. This unsupervised method requires no rPPG specific training and is able to operate in real-time. We also introduce a new multi-modal video dataset, VicarPPG 2, specifically designed to evaluate rPPG algorithms on HR and HRV estimation. We validate and study our method under various conditions on a comprehensive range of public and self-recorded datasets, showing state-of-the-art results and providing useful insights into some unique aspects. Lastly, we make available CleanerPPG, a collection of human-verified ground truth peak/heart-beat annotations for existing rPPG datasets. These verified annotations should make future evaluations and benchmarking of rPPG algorithms more accurate, standardized and fair.      
### 54.Timely Communication in Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.15831.pdf)
>  We consider a federated learning framework in which a parameter server (PS) trains a global model by using $n$ clients without actually storing the client data centrally at a cloud server. Focusing on a setting where the client datasets are highly changing and temporal in nature, we investigate the timeliness of model updates and propose a novel timely communication scheme. Under the proposed scheme, at each iteration, the PS waits for $m$ available clients and sends them the current model. Then, the PS uses the local updates of the earliest $k$ out of $m$ clients to update the global model at each iteration. We find the average age of information experienced by each client and characterize the age-optimal $m$ and $k$ values for a given $n$. Our results indicate that, in addition to ensuring timeliness, the proposed communication scheme results in significantly smaller average iteration times compared to random client selection without hurting the convergence of the global learning task.      
### 55.iGOS++: Integrated Gradient Optimized Saliency by Bilateral Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2012.15783.pdf)
>  The black-box nature of the deep networks makes the explanation for "why" they make certain predictions extremely challenging. Saliency maps are one of the most widely-used local explanation tools to alleviate this problem. One of the primary approaches for generating saliency maps is by optimizing a mask over the input dimensions so that the output of the network is influenced the most by the masking. However, prior work only studies such influence by removing evidence from the input. In this paper, we present iGOS++, a framework to generate saliency maps that are optimized for altering the output of the black-box system by either removing or preserving only a small fraction of the input. Additionally, we propose to add a bilateral total variation term to the optimization that improves the continuity of the saliency map especially under high resolution and with thin object parts. The evaluation results from comparing iGOS++ against state-of-the-art saliency map methods show significant improvement in locating salient regions that are directly interpretable by humans. We utilized iGOS++ in the task of classifying COVID-19 cases from x-ray images and discovered that sometimes the CNN network is overfitted to the characters printed on the x-ray images when performing classification. Fixing this issue by data cleansing significantly improved the precision and recall of the classifier.      
### 56.An analytic physically motivated model of the mammalian cochlea  [ :arrow_down: ](https://arxiv.org/pdf/2012.15750.pdf)
>  We develop an analytic model of the mammalian cochlea. We use a mixed physical-phenomenological approach by utilizing existing work on the physics of classical box-representations of the cochlea, and behavior of recent data-derived wavenumber estimates. Spatial variation is incorporated through a single independent variable that combines space and frequency. We arrive at closed-form expressions for the organ of Corti velocity, its impedance, the pressure difference across the organ of Corti, and its wavenumber. We perform model tests using real and imaginary parts of chinchilla data from multiple locations and for multiple variables. The model also predicts impedances that are qualitatively consistent with current literature. For implementation, the model can leverage existing efforts for both filter bank and filter cascade models that target improved algorithmic or analog circuit efficiencies. The simplicity of the cochlear model, its small number of model constants, its ability to capture the variation of tuning, its closed-form expressions for physically-interrelated variables, and the form of these expressions that allows for easily determining one variable from another make the model appropriate for analytic and digital auditory filter implementations as discussed here, as well as for extracting macromechanical insights regarding how the cochlea works.      
### 57.Incentivizing Routing Choices for Safe and Efficient Transportation in the Face of the COVID-19 Pandemic  [ :arrow_down: ](https://arxiv.org/pdf/2012.15749.pdf)
>  The COVID-19 pandemic has severely affected many aspects of people's daily lives. While many countries are in a re-opening stage, some effects of the pandemic on people's behaviors are expected to last much longer, including how they choose between different transport options. Experts predict considerably delayed recovery of the public transport options, as people try to avoid crowded places. In turn, significant increases in traffic congestion are expected, since people are likely to prefer using their own vehicles or taxis as opposed to riskier and more crowded options such as the railway. In this paper, we propose to use financial incentives to set the tradeoff between risk of infection and congestion to achieve safe and efficient transportation networks. To this end, we formulate a network optimization problem to optimize taxi fares. For our framework to be useful in various cities and times of the day without much designer effort, we also propose a data-driven approach to learn human preferences about transport options, which is then used in our taxi fare optimization. Our user studies and simulation experiments show our framework is able to minimize congestion and risk of infection.      
### 58.Design Of Two Stage CMOS Operational Amplifier in 180nm Technology  [ :arrow_down: ](https://arxiv.org/pdf/2012.15737.pdf)
>  In this paper a CMOS two stage operational amplifier has been presented which operates at 1.8 V power supply at 0.18 micron (i.e., 180 nm) technology and whose input is depended on Bias Current. The op-amp provides a gain of 63dB and a bandwidth of 140 kHz for a load of 1 pF. This op-amp has a Common Mode gain of -25 dB, an output slew rate of 32 $V / \mu s$, and a output voltage swing. The power consumption for the op-amp is $300\mu W$.      
### 59.EfficientNet-Absolute Zero for Continuous Speech Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2012.15695.pdf)
>  Keyword spotting is a process of finding some specific words or phrases in recorded speeches by computers. Deep neural network algorithms, as a powerful engine, can handle this problem if they are trained over an appropriate dataset. To this end, the football keyword dataset (FKD), as a new keyword spotting dataset in Persian, is collected with crowdsourcing. This dataset contains nearly 31000 samples in 18 classes. The continuous speech synthesis method proposed to made FKD usable in the practical application which works with continuous speeches. Besides, we proposed a lightweight architecture called EfficientNet-A0 (absolute zero) by applying the compound scaling method on EfficientNet-B0 for keyword spotting task. Finally, the proposed architecture is evaluated with various models. It is realized that EfficientNet-A0 and Resnet models outperform other models on this dataset.      
### 60.Simulation and Control of Deformable Autonomous Airships in Turbulent Wind  [ :arrow_down: ](https://arxiv.org/pdf/2012.15684.pdf)
>  Abstract. Fixed wing and multirotor UAVs are common in the field of robotics. Solutions for simulation and control of these vehicles are ubiquitous. This is not the case for airships, a simulation of which needs to address unique properties, i) dynamic deformation in response to aerodynamic and control forces, ii) high susceptibility to wind and turbulence at low airspeed, iii) high variability in airship designs regarding placement, direction and vectoring of thrusters and control surfaces. We present a flexible framework for modeling, simulation and control of airships, based on the Robot operating system (ROS), simulation environment (Gazebo) and commercial off the shelf (COTS) electronics, both of which are open source. Based on simulated wind and deformation, we predict substantial effects on controllability, verified in real world flight experiments. All our code is shared as open source, for the benefit of the community and to facilitate lighter-than-air vehicle (LTAV) research. <a class="link-external link-https" href="https://github.com/robot-perception-group/airship_simulation" rel="external noopener nofollow">this https URL</a>      
### 61.Vehicular Network Slicing for Reliable Access and Deadline-Constrained Data Offloading: A Multi-Agent On-Device Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.15545.pdf)
>  Efficient data offloading plays a pivotal role in computational-intensive platforms as data rate over wireless channels is fundamentally limited. On top of that, high mobility adds an extra burden in vehicular edge networks (VENs), bolstering the desire for efficient user-centric solutions. Therefore, unlike the legacy inflexible network-centric approach, this paper exploits a software-defined flexible, open, and programmable networking platform for an efficient user-centric, fast, reliable, and deadline-constrained offloading solution in VENs. In the proposed model, each active vehicle user (VU) is served from multiple low-powered access points (APs) by creating a noble virtual cell (VC). A joint node association, power allocation, and distributed resource allocation problem is formulated. As centralized learning is not practical in many real-world problems, following the distributed nature of autonomous VUs, each VU is considered an edge learning agent. To that end, considering practical location-aware node associations, a joint radio and power resource allocation non-cooperative stochastic game is formulated. Leveraging reinforcement learning's (RL) efficacy, a multi-agent RL (MARL) solution is proposed where the edge learners aim to learn the Nash equilibrium (NE) strategies to solve the game efficiently. Besides, real-world map data, with a practical microscopic mobility model, are used for the simulation. Results suggest that the proposed user-centric approach can deliver remarkable performances in VENs. Moreover, the proposed MARL solution delivers near-optimal performances with approximately 3% collision probabilities in case of distributed random access in the uplink.      
### 62.Multiple Plans are Better than One: Diverse Stochastic Planning  [ :arrow_down: ](https://arxiv.org/pdf/2012.15485.pdf)
>  In planning problems, it is often challenging to fully model the desired specifications. In particular, in human-robot interaction, such difficulty may arise due to human's preferences that are either private or complex to model. Consequently, the resulting objective function can only partially capture the specifications and optimizing that may lead to poor performance with respect to the true specifications. Motivated by this challenge, we formulate a problem, called diverse stochastic planning, that aims to generate a set of representative -- small and diverse -- behaviors that are near-optimal with respect to the known objective. In particular, the problem aims to compute a set of diverse and near-optimal policies for systems modeled by a Markov decision process. We cast the problem as a constrained nonlinear optimization for which we propose a solution relying on the Frank-Wolfe method. We then prove that the proposed solution converges to a stationary point and demonstrate its efficacy in several planning problems.      
### 63.Learned Multi-Resolution Variable-Rate Image Compression with Octave-based Residual Blocks  [ :arrow_down: ](https://arxiv.org/pdf/2012.15463.pdf)
>  Recently deep learning-based image compression has shown the potential to outperform traditional codecs. However, most existing methods train multiple networks for multiple bit rates, which increase the implementation complexity. In this paper, we propose a new variable-rate image compression framework, which employs generalized octave convolutions (GoConv) and generalized octave transposed-convolutions (GoTConv) with built-in generalized divisive normalization (GDN) and inverse GDN (IGDN) layers. Novel GoConv- and GoTConv-based residual blocks are also developed in the encoder and decoder networks. Our scheme also uses a stochastic rounding-based scalar quantization. To further improve the performance, we encode the residual between the input and the reconstructed image from the decoder network as an enhancement layer. To enable a single model to operate with different bit rates and to learn multi-rate image features, a new objective function is introduced. Experimental results show that the proposed framework trained with variable-rate objective function outperforms the standard codecs such as H.265/HEVC-based BPG and state-of-the-art learning-based variable-rate methods.      
### 64.Text-Free Image-to-Speech Synthesis Using Learned Segmental Units  [ :arrow_down: ](https://arxiv.org/pdf/2012.15454.pdf)
>  In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.      
### 65.Generative Adversarial Network for Image Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2012.15446.pdf)
>  This chapter reviews recent developments of generative adversarial networks (GAN)-based methods for medical and biomedical image synthesis tasks. These methods are classified into conditional GAN and Cycle-GAN according to the network architecture designs. For each category, a literature survey is given, which covers discussions of the network architecture designs, highlights important contributions and identifies specific challenges.      
### 66.SharpGAN: Receptive Field Block Net for Dynamic Scene Deblurring  [ :arrow_down: ](https://arxiv.org/pdf/2012.15432.pdf)
>  When sailing at sea, the smart ship will inevitably produce swaying motion due to the action of wind, wave and current, which makes the image collected by the visual sensor appear motion blur. This will have an adverse effect on the object detection algorithm based on the vision sensor, thereby affect the navigation safety of the smart ship. In order to remove the motion blur in the images during the navigation of the smart ship, we propose SharpGAN, a new image deblurring method based on the generative adversarial network. First of all, the Receptive Field Block Net (RFBNet) is introduced to the deblurring network to strengthen the network's ability to extract the features of blurred image. Secondly, we propose a feature loss that combines different levels of image features to guide the network to perform higher-quality deblurring and improve the feature similarity between the restored images and the sharp image. Finally, we propose to use the lightweight RFB-s module to improve the real-time performance of deblurring network. Compared with the existing deblurring methods on large-scale real sea image datasets and large-scale deblurring datasets, the proposed method not only has better deblurring performance in visual perception and quantitative criteria, but also has higher deblurring efficiency.      
### 67.Curriculum-based Deep Reinforcement Learning for Quantum Control  [ :arrow_down: ](https://arxiv.org/pdf/2012.15427.pdf)
>  Deep reinforcement learning has been recognized as an efficient technique to design optimal strategies for different complex systems without prior knowledge of the control landscape. To achieve a fast and precise control for quantum systems, we propose a novel deep reinforcement learning approach by constructing a curriculum consisting of a set of intermediate tasks defined by a fidelity threshold. Tasks among a curriculum can be statically determined using empirical knowledge or adaptively generated with the learning process. By transferring knowledge between two successive tasks and sequencing tasks according to their difficulties, the proposed curriculum-based deep reinforcement learning (CDRL) method enables the agent to focus on easy tasks in the early stage, then move onto difficult tasks, and eventually approaches the final task. Numerical simulations on closed quantum systems and open quantum systems demonstrate that the proposed method exhibits improved control performance for quantum systems and also provides an efficient way to identify optimal strategies with fewer control pulses.      
### 68.Heterogeneous recovery from large scale power failures  [ :arrow_down: ](https://arxiv.org/pdf/2012.15420.pdf)
>  Large-scale power failures are induced by nearly all natural disasters from hurricanes to wild fires. A fundamental problem is whether and how recovery guided by government policies is able to meet the challenge of a wide range of disruptions. Prior research on this problem is scant due to lack of sharing large-scale granular data at the operational energy grid, stigma of revealing limitations of services, and complex recovery coupled with policies and customers. As such, both quantification and firsthand information are lacking on capabilities and fundamental limitation of energy services in response to extreme events. Furthermore, government policies that guide recovery are often sidelined by prior study. This work studies the fundamental problem through the lens of recovery guided by two commonly adopted policies. We develop data analysis on unsupervised learning from non-stationary data. The data span failure events, from moderate to extreme, at the operational distribution grid during the past nine years in two service regions at the state of New York and Massachusetts. We show that under the prioritization policy favoring large failures, recovery exhibits a surprising scaling property which counteracts failure scaling on the infrastructure vulnerability. However, heterogeneous recovery widens with the severity of failure events: large failures that cannot be prioritized increase customer interruption time by 47 folds. And, prolonged small failures dominate the entire temporal evolution of recovery.      
### 69.Algorithms for Learning Graphs in Financial Markets  [ :arrow_down: ](https://arxiv.org/pdf/2012.15410.pdf)
>  In the past two decades, the field of applied finance has tremendously benefited from graph theory. As a result, novel methods ranging from asset network estimation to hierarchical asset selection and portfolio allocation are now part of practitioners' toolboxes. In this paper, we investigate the fundamental problem of learning undirected graphical models under Laplacian structural constraints from the point of view of financial market times series data. In particular, we present natural justifications, supported by empirical evidence, for the usage of the Laplacian matrix as a model for the precision matrix of financial assets, while also establishing a direct link that reveals how Laplacian constraints are coupled to meaningful physical interpretations related to the market index factor and to conditional correlations between stocks. Those interpretations lead to a set of guidelines that practitioners should be aware of when estimating graphs in financial markets. In addition, we design numerical algorithms based on the alternating direction method of multipliers to learn undirected, weighted graphs that take into account stylized facts that are intrinsic to financial data such as heavy tails and modularity. We illustrate how to leverage the learned graphs into practical scenarios such as stock time series clustering and foreign exchange network estimation. The proposed graph learning algorithms outperform the state-of-the-art methods in an extensive set of practical experiments. Furthermore, we obtain theoretical and empirical convergence results for the proposed algorithms. Along with the developed methodologies for graph learning in financial markets, we release an R package, called fingraph, accommodating the code and data to obtain all the experimental results.      
### 70.Generalized Operating Procedure for Deep Learning: an Unconstrained Optimal Design Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2012.15391.pdf)
>  Deep learning (DL) has brought about remarkable breakthrough in processing images, video and speech due to its efficacy in extracting highly abstract representation and learning very complex functions. However, there is seldom operating procedure reported on how to make it for real use cases. In this paper, we intend to address this problem by presenting a generalized operating procedure for DL from the perspective of unconstrained optimal design, which is motivated by a simple intension to remove the barrier of using DL, especially for those scientists or engineers who are new but eager to use it. Our proposed procedure contains seven steps, which are project/problem statement, data collection, architecture design, initialization of parameters, defining loss function, computing optimal parameters, and inference, respectively. Following this procedure, we build a multi-stream end-to-end speaker verification system, in which the input speech utterance is processed by multiple parallel streams within different frequency range, so that the acoustic modeling can be more robust resulting from the diversity of features. Trained with VoxCeleb dataset, our experimental results verify the effectiveness of our proposed operating procedure, and also show that our multi-stream framework outperforms single-stream baseline with 20 % relative reduction in minimum decision cost function (minDCF).      
### 71.Model Free Reinforcement Learning Algorithm for Stationary Mean field Equilibrium for Multiple Types of Agents  [ :arrow_down: ](https://arxiv.org/pdf/2012.15377.pdf)
>  We consider a multi-agent Markov strategic interaction over an infinite horizon where agents can be of multiple types. We model the strategic interaction as a mean-field game in the asymptotic limit when the number of agents of each type becomes infinite. Each agent has a private state; the state evolves depending on the distribution of the state of the agents of different types and the action of the agent. Each agent wants to maximize the discounted sum of rewards over the infinite horizon which depends on the state of the agent and the distribution of the state of the leaders and followers. We seek to characterize and compute a stationary multi-type Mean field equilibrium (MMFE) in the above game. We characterize the conditions under which a stationary MMFE exists. Finally, we propose Reinforcement learning (RL) based algorithm using policy gradient approach to find the stationary MMFE when the agents are unaware of the dynamics. We, numerically, evaluate how such kind of interaction can model the cyber attacks among defenders and adversaries, and show how RL based algorithm can converge to an equilibrium.      
### 72.A Review into Data Science and Its Approaches in Mechanical Engineering  [ :arrow_down: ](https://arxiv.org/pdf/2012.15358.pdf)
>  Nowadays it is inevitable to use intelligent systems to improve the performance and optimization of different components of devices or factories. Furthermore, it's so essential to have appropriate predictions to make better decisions in businesses, medical studies, and engineering studies, etc. One of the newest and most widely used of these methods is a field called Data Science that all of the scientists, engineers, and factories need to learn and use in their careers. This article briefly introduced data science and reviewed its methods, especially it's usages in mechanical engineering and challenges and ways of developing data science in mechanical engineering. In the introduction, different definitions of data science and its background in technology reviewed. In the following, data science methodology which is the process that a data scientist needs to do in its works been discussed. Further, some researches in the mechanical engineering area that used data science methods in their studies, are reviewed. Eventually, it has been discussed according to the subjects that have been reviewed in the article, why it is necessary to use data science in mechanical engineering researches and projects.      
### 73.Multi-view Temporal Alignment for Non-parallel Articulatory-to-Acoustic Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2012.15184.pdf)
>  Articulatory-to-acoustic (A2A) synthesis refers to the generation of audible speech from captured movement of the speech articulators. This technique has numerous applications, such as restoring oral communication to people who cannot longer speak due to illness or injury. Most successful techniques so far adopt a supervised learning framework, in which time-synchronous articulatory-and-speech recordings are used to train a supervised machine learning algorithm that can be used later to map articulator movements to speech. This, however, prevents the application of A2A techniques in cases where parallel data is unavailable, e.g., a person has already lost her/his voice and only articulatory data can be captured. In this work, we propose a solution to this problem based on the theory of multi-view learning. The proposed algorithm attempts to find an optimal temporal alignment between pairs of non-aligned articulatory-and-acoustic sequences with the same phonetic content by projecting them into a common latent space where both views are maximally correlated and then applying dynamic time warping. Several variants of this idea are discussed and explored. We show that the quality of speech generated in the non-aligned scenario is comparable to that obtained in the parallel scenario.      
### 74.Analysis of Truck Driver Behavior to Design Different Lane Change Styles in Automated Driving  [ :arrow_down: ](https://arxiv.org/pdf/2012.15164.pdf)
>  Lane change is a very demanding driving task and number of traffic accidents are induced by mistaken maneuvers. An automated lane change system has the potential to reduce driver workload and to improve driving safety. One challenge is how to improve driver acceptance on the automated system. From the viewpoint of human factors, an automated system with different styles would improve user acceptance as the drivers can adapt the style to different driving situations. This paper proposes a method to design different lane change styles in automated driving by analysis and modeling of truck driver behavior. A truck driving simulator experiment with 12 participants was conducted to identify the driver model parameters and three lane change styles were classified as the aggressive, medium, and conservative ones. The proposed automated lane change system was evaluated by another truck driving simulator experiment with the same 12 participants. Moreover, the effect of different driving styles on driver experience and acceptance was evaluated. The evaluation results demonstrate that the different lane change styles could be distinguished by the drivers; meanwhile, the three styles were overall evaluated as acceptable on safety issues and reliable by the human drivers. This study provides insight into designing the automated driving system with different driving styles and the findings can be applied to commercial automated trucks.      
### 75.An Analysis of Scatter Characteristics in X-ray CT Spectral Correction  [ :arrow_down: ](https://arxiv.org/pdf/2012.15125.pdf)
>  X-ray scatter remains a major physics challenge in volumetric computed tomography (CT), whose physical and statistical behaviors have been commonly leveraged in order to eliminate its impact on CT image quality. In this work, we conduct an in-depth derivation of how the scatter distribution and scatter to primary ratio (SPR) will change during the spectral correction, leading to an interesting finding on the property of scatter: when applying the spectral correction before scatter is removed, the impact of SPR on a CT projection will be scaled by the first derivative of the mapping function; while the scatter distribution in the transmission domain will be scaled by the product of the first derivative of the mapping function and a natural exponential of the projection difference before and after the mapping. Such a characterization of scatter's behavior provides an analytic approach of compensating for the SPR as well as approximating the change of scatter distribution after spectral correction, even though both of them might be significantly distorted as the linearization mapping function in spectral correction could vary a lot from one detector pixel to another. We conduct an evaluation of SPR compensations on a Catphan phantom and an anthropomorphic chest phantom to validate the characteristics of scatter. In addition, this scatter property is also directly adopted into CT imaging using a spectral modulator with flying focal spot technology (SMFFS) as an example to demonstrate its potential in practical applications.      
### 76.Sub-sampled Cross-component Prediction for Emerging Video Coding Standards  [ :arrow_down: ](https://arxiv.org/pdf/2012.15067.pdf)
>  Cross-component linear model (CCLM) prediction has been repeatedly proven to be effective in reducing the inter-channel redundancies in video compression. Essentially speaking, the linear model is identically trained by employing accessible luma and chroma reference samples at both encoder and decoder, elevating the level of operational complexity due to the least square regression or max-min based model parameter derivation. In this paper, we investigate the capability of the linear model in the context of sub-sampled based cross-component correlation mining, as a means of significantly releasing the operation burden and facilitating the hardware and software design for both encoder and decoder. In particular, the sub-sampling ratios and positions are elaborately designed by exploiting the spatial correlation and the inter-channel correlation. Extensive experiments verify that the proposed method is characterized by its simplicity in operation and robustness in terms of rate-distortion performance, leading to the adoption by Versatile Video Coding (VVC) standard and the third generation of Audio Video Coding Standard (AVS3).      
### 77.Elastic Net based Feature Ranking and Selection  [ :arrow_down: ](https://arxiv.org/pdf/2012.14982.pdf)
>  Feature selection is important in data representation and intelligent diagnosis. Elastic net is one of the most widely used feature selectors. However, the features selected are dependant on the training data, and their weights dedicated for regularized regression are irrelevant to their importance if used for feature ranking, that degrades the model interpretability and extension. In this study, an intuitive idea is put at the end of multiple times of data splitting and elastic net based feature selection. It concerns the frequency of selected features and uses the frequency as an indicator of feature importance. After features are sorted according to their frequency, linear support vector machine performs the classification in an incremental manner. At last, a compact subset of discriminative features is selected by comparing the prediction performance. Experimental results on breast cancer data sets (BCDR-F03, WDBC, GSE 10810, and GSE 15852) suggest that the proposed framework achieves competitive or superior performance to elastic net and with consistent selection of fewer features. How to further enhance its consistency on high-dimension small-sample-size data sets should be paid more attention in our future work. The proposed framework is accessible online (<a class="link-external link-https" href="https://github.com/NicoYuCN/elasticnetFR" rel="external noopener nofollow">this https URL</a>).      
### 78.Consensus-Based Distributed Computation of Link-Based Network Metrics  [ :arrow_down: ](https://arxiv.org/pdf/2012.14971.pdf)
>  Average consensus algorithms have wide applications in distributed computing systems where all the nodes agree on the average value of their initial states by only exchanging information with their local neighbors. In this letter, we look into link-based network metrics which are polynomial functions of pair-wise node attributes defined over the links in a network. Different from node-based average consensus, such link-based metrics depend on both the distribution of node attributes and the underlying network topology. We propose a general algorithm using the weighted average consensus protocol for the distributed computation of link-based network metrics and provide the convergence conditions and convergence rate analysis.      
### 79.Decentralized Control with Graph Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.14906.pdf)
>  Dynamical systems consisting of a set of autonomous agents face the challenge of having to accomplish a global task, relying only on local information. While centralized controllers are readily available, they face limitations in terms of scalability and implementation, as they do not respect the distributed information structure imposed by the network system of agents. Given the difficulties in finding optimal decentralized controllers, we propose a novel framework using graph neural networks (GNNs) to learn these controllers. GNNs are well-suited for the task since they are naturally distributed architectures and exhibit good scalability and transferability properties. The problems of flocking and multi-agent path planning are explored to illustrate the potential of GNNs in learning decentralized controllers.      
### 80.An LQR-assisted Control Algorithm for an Under-actuated In-pipe Robot in Water Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.14879.pdf)
>  To address the operational challenges of in-pipe robots in large pipes of water distribution systems (WDS), in this research, a control algorithm is proposed for our previously designed robot [4]. Our size adaptable robot has an under-actuated modular design that can be used for both leak detection and quality monitoring. First, nonlinear dynamical governing equations of the robot are derived with the definition of two perpendicular planes, and two sets of states are defined for the robot for stabilization and mobilization. For stabilization, we calculated the auxiliary system matrices and designed a stabilizer controller based on the linear quadratic regulator (LQR) controller, and combined it with a proportional-integral-derivative (PID) based controller for mobilization. The controller scheme is validated with simulation in MATLAB in various operation conditions in three iterations. The simulation results show that the controller can stabilize the robot inside the pipe by converging the stabilizing states to zero and keeping them in zero with initial values between -25 degree and +25 degree and tracking velocities of 10cm/s, 30cm/s, and 50cm/s which makes the robot agile and dexterous for operation in pipelines.      
### 81.Accelerated NMR Spectroscopy: Merge Optimization with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.14830.pdf)
>  Multi-dimensional NMR spectroscopy is an invaluable biophysical tool in studies of structure, interactions, and dynamics of large molecules like proteins and nuclear acids. Non-uniform sampling is a powerful approach for shortening measurement time and increasing spectra resolution. Several methods have been established for spectra reconstruction from the undersampled data, typical approaches include model-based optimization and data-driven deep learning. The former is well theoretically grounded and provides high-quality spectra, while the latter has a huge advantage in reconstruction time potential and push further limits of spectra quality and analysis. Combining the merits of the two, we propose a model-inspired deep learning, for reliable, high-quality, and ultra-fast spectra reconstruction, exemplified by multi-dimensional spectra of several representative proteins. We demonstrate that the proposed network needs very few parameters, and shows very high robustness in respect to dissimilarity of the training and target data in the spectra size, type, and sampling level. This work can be considered as a proof-of-concept of merging optimization with deep learning in NMR spectroscopy.      
### 82.Quality-Driven Dynamic VVC Frame Partitioning for Efficient Parallel Processing  [ :arrow_down: ](https://arxiv.org/pdf/2012.14792.pdf)
>  VVC is the next generation video coding standard, offering coding capability beyond HEVC standard. The high computational complexity of the latest video coding standards requires high-level parallelism techniques, in order to achieve real-time and low latency encoding and decoding. HEVC and VVC include tile grid partitioning that allows to process simultaneously rectangular regions of a frame with independent threads. The tile grid may be further partitioned into a horizontal sub-grid of Rectangular Slices (RSs), increasing the partitioning flexibility. The dynamic Tile and Rectangular Slice (TRS) partitioning solution proposed in this paper benefits from this flexibility. The TRS partitioning is carried-out at the frame level, taking into account both spatial texture of the content and encoding times of previously encoded frames. The proposed solution searches the best partitioning configuration that minimizes the trade-off between multi-thread encoding time and encoding quality loss. Experiments prove that the proposed solution, compared to uniform TRS partitioning, significantly decreases multi-thread encoding time, with slightly better encoding quality.      
### 83.Improved Segmentation and Detection Sensitivity of Diffusion-Weighted Brain Infarct Lesions with Synthetically Enhanced Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.14771.pdf)
>  Purpose: To compare the segmentation and detection performance of a deep learning model trained on a database of human-labelled clinical diffusion-weighted (DW) stroke lesions to a model trained on the same database enhanced with synthetic DW stroke lesions. Methods: In this institutional review board approved study, a stroke database of 962 cases (mean age 65+/-17 years, 255 males, 449 scans with DW positive stroke lesions) and a normal database of 2,027 patients (mean age 38+/-24 years,1088 females) were obtained. Brain volumes with synthetic DW stroke lesions were produced by warping the relative signal increase of real strokes to normal brain volumes. A generic 3D U-Net was trained on four different databases to generate four different models: (a) 375 neuroradiologist-labeled clinical DW positive stroke cases(CDB);(b) 2,000 synthetic cases(S2DB);(c) CDB+2,000 synthetic cases(CS2DB); or (d) CDB+40,000 synthetic cases(CS40DB). The models were tested on 20%(n=192) of the cases of the stroke database, which were excluded from the training set. Segmentation accuracy was characterized using Dice score and lesion volume of the stroke segmentation, and statistical significance was tested using a paired, two-tailed, Student's t-test. Detection sensitivity and specificity was compared to three neuroradiologists. Results: The performance of the 3D U-Net model trained on the CS40DB(mean Dice 0.72) was better than models trained on the CS2DB (0.70,P &lt;0.001) or the CDB(0.65,P&lt;0.001). The deep learning model was also more sensitive (91%[89%-93%]) than each of the three human readers(84%[81%-87%],78%[75%-81%],and 79%[76%-82%]), but less specific(75%[72%-78%] vs for the three human readers (96%[94%-97%],92%[90%-94%] and 89%[86%-91%]). Conclusion: Deep learning training for segmentation and detection of DW stroke lesions was significantly improved by enhancing the training set with synthetic lesions.      
### 84.Data-driven audio recognition: a supervised dictionary approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.14761.pdf)
>  Machine hearing is an emerging area. Motivated by the need of a principled framework across domain applications for machine listening, we propose a generic and data-driven representation learning approach. For this sake, a novel and efficient supervised dictionary learning method is presented. Experiments are performed on both computational auditory scene (East Anglia and Rouen) and synthetic music chord recognition datasets. Obtained results show that our method is capable to reach state-of-the-art hand-crafted features for both applications      
### 85.Advances in deep learning methods for pavement surface crack detection and identification with visible light visual images  [ :arrow_down: ](https://arxiv.org/pdf/2012.14704.pdf)
>  Compared to NDT and health monitoring method for cracks in engineering structures, surface crack detection or identification based on visible light images is non-contact, with the advantages of fast speed, low cost and high precision. Firstly, typical pavement (concrete also) crack public data sets were collected, and the characteristics of sample images as well as the random variable factors, including environmental, noise and interference etc., were summarized. Subsequently, the advantages and disadvantages of three main crack identification methods (i.e., hand-crafted feature engineering, machine learning, deep learning) were compared. Finally, from the aspects of model architecture, testing performance and predicting effectiveness, the development and progress of typical deep learning models, including self-built CNN, transfer learning(TL) and encoder-decoder(ED), which can be easily deployed on embedded platform, were reviewed. The benchmark test shows that: 1) It has been able to realize real-time pixel-level crack identification on embedded platform: the entire crack detection average time cost of an image sample is less than 100ms, either using the ED method (i.e., FPCNet) or the TL method based on InceptionV3. It can be reduced to less than 10ms with TL method based on MobileNet (a lightweight backbone base network). 2) In terms of accuracy, it can reach over 99.8% on CCIC which is easily identified by human eyes. On SDNET2018, some samples of which are difficult to be identified, FPCNet can reach 97.5%, while TL method is close to 96.1%. <br>To the best of our knowledge, this paper for the first time comprehensively summarizes the pavement crack public data sets, and the performance and effectiveness of surface crack detection and identification deep learning methods for embedded platform, are reviewed and evaluated.      
### 86.Reinforcement Learning for Control of Valves  [ :arrow_down: ](https://arxiv.org/pdf/2012.14668.pdf)
>  This paper compares reinforcement learning (RL) with PID (proportional-integral-derivative) strategy for control of nonlinear valves using a unified framework. RL is an autonomous learning mechanism that learns by interacting with its environment. It is gaining increasing attention in the world of control systems as a means of building optimal-controllers for challenging dynamic and nonlinear processes. Published RL research often uses open-source tools (Python and OpenAI Gym environments) which could be difficult to adapt and apply by practicing industrial engineers, we therefore used MathWorks tools. MATLAB's recently launched (R2019a) Reinforcement Learning Toolbox was used to develop the valve controller; trained using the DDPG (Deep Deterministic Policy-Gradient) algorithm and Simulink to simulate the nonlinear valve and setup the experimental test-bench to evaluate the RL and PID controllers. Results indicate that the RL controller is extremely good at tracking the signal with speed and produces a lower error with respect to the reference signals. The PID, however, is better at disturbance rejection and hence provides a longer life for the valves. Experiential learnings gained from this research are corroborated against published research. It is known that successful machine learning involves tuning many hyperparameters and significant investment of time and efforts. We introduce ``Graded Learning" as a simplified, application oriented adaptation of the more formal and algorithmic ``Curriculum for Reinforcement Learning''. It is shown via experiments that it helps converge the learning task of complex non-linear real world systems.      
### 87.The Adaptive Dynamic Programming Toolbox  [ :arrow_down: ](https://arxiv.org/pdf/2012.14654.pdf)
>  The paper develops the Adaptive Dynamic Programming Toolbox (ADPT), which solves optimal control problems for continuous-time nonlinear systems. Based on the adaptive dynamic programming technique, the ADPT computes optimal feedback controls from the system dynamics in the model-based working mode, or from measurements of trajectories of the system in the model-free working mode without the requirement of knowledge of the system model. Multiple options are provided such that the ADPT can accommodate various customized circumstances. Compared to other popular software toolboxes for optimal control, the ADPT enjoys its computational precision and speed, which is illustrated with its applications to a satellite attitude control problem.      
### 88.Deferrable Load Scheduling under Demand Charge: A Block Model-Predictive Control Approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.14624.pdf)
>  Optimal scheduling of deferrable electrical loads can reshape the aggregated load profile to achieve higher operational efficiency and reliability. This paper studies deferrable load scheduling under demand charge that imposes a penalty on the peak consumption over a billing period. Such a terminal cost poses challenges in real-time dispatch when demand forecasts are inaccurate. A block model-predictive control approach is proposed by breaking demand charge into a sequence of stage costs. The problem of charging electric vehicles is used to illustrate the efficacy of the proposed approach. Numerical examples show that the block model-predictive control outperforms benchmark methods in various settings.      
### 89.Performance Analysis of 2-Step Random Access with CDMA in Machine-Type Communication  [ :arrow_down: ](https://arxiv.org/pdf/2012.14603.pdf)
>  There is a growing interest in the transition from 4-step random access to 2-step random access in machine-type communication (MTC), since 2-step random access is well-suited to short message delivery in various Internet of Things (IoT) applications. In this paper, we study a 2-step random access approach that uses code division multiple access (CDMA) to form multiple channels for data packet transmissions with a spreading factor less than the number of channels. As a result, the length of data transmission phase in 2-step random access can be shorter at the cost of multiuser interference. To see how the decrease of the length of data transmission phase can improve the spectral efficiency, we derive the throughput as well as the spectral efficiency. From the results of analysis, we can show that the 2-step CDMA-based random access approach can have a higher spectral efficiency than conventional 2-step approach with orthogonal channel allocations, which means that the performance of MTC can be improved by successfully transmitting more data packets per unit time using CDMA. This is also confirmed by simulation results.      
### 90.Detecting COVID-19 from Breathing and Coughing Sounds using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.14553.pdf)
>  The COVID-19 pandemic has affected the world unevenly; while industrial economies have been able to produce the tests necessary to track the spread of the virus and mostly avoided complete lockdowns, developing countries have faced issues with testing capacity. In this paper, we explore the usage of deep learning models as a ubiquitous, low-cost, pre-testing method for detecting COVID-19 from audio recordings of breathing or coughing taken with mobile devices or via the web. We adapt an ensemble of Convolutional Neural Networks that utilise raw breathing and coughing audio and spectrograms to classify if a speaker is infected with COVID-19 or not. The different models are obtained via automatic hyperparameter tuning using Bayesian Optimisation combined with HyperBand. The proposed method outperforms a traditional baseline approach by a large margin. Ultimately, it achieves an Unweighted Average Recall (UAR) of 74.9%, or an Area Under ROC Curve (AUC) of 80.7% by ensembling neural networks, considering the best test set result across breathing and coughing in a strictly subject independent manner. In isolation, breathing sounds thereby appear slightly better suited than coughing ones (76.1% vs 73.7% UAR).      
### 91.Source Identification for Mixtures of Product Distributions  [ :arrow_down: ](https://arxiv.org/pdf/2012.14540.pdf)
>  We give an algorithm for source identification of a mixture of $k$ product distributions on $n$ bits. This is a fundamental problem in machine learning with many applications. Our algorithm identifies the source parameters of an identifiable mixture, given, as input, approximate values of multilinear moments (derived, for instance, from a sufficiently large sample), using $2^{O(k^2)} n^{O(k)}$ arithmetic operations. Our result is the first explicit bound on the computational complexity of source identification of such mixtures. The running time improves previous results by Feldman, O'Donnell, and Servedio (FOCS 2005) and Chen and Moitra (STOC 2019) that guaranteed only learning the mixture (without parametric identification of the source). Our analysis gives a quantitative version of a qualitative characterization of identifiable sources that is due to Tahmasebi, Motahari, and Maddah-Ali (ISIT 2018).      
