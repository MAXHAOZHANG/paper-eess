# ArXiv eess --Wed, 20 Jan 2021
### 1.Implementing Admittance Relaying for Microgrid Protection  [ :arrow_down: ](https://arxiv.org/pdf/2101.07783.pdf)
>  The rapid increase of distributed energy resources has led to the widespread deployment of microgrids. These flexible and efficient local energy grids are able to operate in both grid-connected mode and islanded mode; they are interfaced to the main power system by a fast semiconductor switch and commonly make use of inverter-interfaced generation. This paper focuses on inverter interfaced microgrids, which present a challenge for protection as they do not provide the high short-circuit current necessary for conventional time-overcurrent protection. The application of admittance relaying for the protection of inverter-interfaced microgrids is investigated as a potential solution. The comparison of analytical and simulated results of performed four experiments prove the suitability of admittance relaying for microgrids protection.      
### 2.Dynamic State Estimation for Radial Microgrid Protection  [ :arrow_down: ](https://arxiv.org/pdf/2101.07774.pdf)
>  Microgrids are localized electrical grids with control capability that are able to disconnect from the traditional grid to operate autonomously. They strengthen grid resilience, help mitigate grid disturbances, and support a flexible grid by enabling the integration of distributed energy resources. Given the likely presence of critical loads, the proper protection of microgrids is of vital importance; however, this is complicated in the case of inverter-interfaced microgrids where low fault currents preclude the use of conventional time-overcurrent protection. This paper introduces and investigates the application of dynamic state estimation, a generalization of differential protection, for the protection of radial portions of microgrids (or distribution networks); both phasor-based and dynamic approaches are investigated for protection. It is demonstrated through experiments on three case-study systems that dynamic state estimation is capable of correctly identifying model parameters for both normal and faulted operation.      
### 3.Magnification Generalization for Histopathology Image Embedding  [ :arrow_down: ](https://arxiv.org/pdf/2101.07757.pdf)
>  Histopathology image embedding is an active research area in computer vision. Most of the embedding models exclusively concentrate on a specific magnification level. However, a useful task in histopathology embedding is to train an embedding space regardless of the magnification level. Two main approaches for tackling this goal are domain adaptation and domain generalization, where the target magnification levels may or may not be introduced to the model in training, respectively. Although magnification adaptation is a well-studied topic in the literature, this paper, to the best of our knowledge, is the first work on magnification generalization for histopathology image embedding. We use an episodic trainable domain generalization technique for magnification generalization, namely Model Agnostic Learning of Semantic Features (MASF), which works based on the Model Agnostic Meta-Learning (MAML) concept. Our experimental results on a breast cancer histopathology dataset with four different magnification levels show the proposed method's effectiveness for magnification generalization.      
### 4.Motion Estimation of Connected and Automated Vehicles under Communication Delay and Packet Loss of V2X Communications  [ :arrow_down: ](https://arxiv.org/pdf/2101.07756.pdf)
>  The emergence of the connected and automated vehicle (CAV) technology enables numerous advanced applications in our transportation system, benefiting our daily travels in terms of safety, mobility, and sustainability. However, vehicular communication technologies such as Dedicated Short-Range Communications (DSRC) or Cellular-Based Vehicle-to-Everything (C-V2X) communications unavoidably introduce issues like communication delay and packet loss, which will downgrade the performances of any CAV applications. In this study, we propose a consensus-based motion estimation methodology to estimate the vehicle motion when the vehicular communication environment is not ideal. This methodology is developed based on the consensus-based feedforward/feedback motion control algorithm, estimating the position and speed of a CAV in the presence of communication delay and packet loss. The simulation study is conducted in a traffic scenario of unsignalized intersections, where CAVs coordinate with each other through V2X communications and cross intersections without any full stop. Game engine-based human-in-the-loop simulation results shows the proposed motion estimation methodology can cap the position estimation error to 0.5 m during periodic packet loss and time-variant communication delay.      
### 5.A survey on shape-constraint deep learning for medical image segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2101.07721.pdf)
>  Since the advent of U-Net, fully convolutional deep neural networks and its many variants have completely changed the modern landscape of deep learning based medical image segmentation. However, the over dependence of these methods on pixel level classification and regression has been identified early on as a problem. Especially when trained on medical databases with sparse available annotation, these methods are prone to generate segmentation artifacts such as fragmented structures, topological inconsistencies and islands of pixel. These artefacts are especially problematic in medical imaging since segmentation is almost always a pre-processing step for some downstream evaluation. The range of possible downstream evaluations is rather big, for example surgical planning, visualization, shape analysis, prognosis, treatment planning etc. However, one common thread across all these downstream tasks is the demand of anatomical consistency. To ensure the segmentation result is anatomically consistent, approaches based on Markov/ Conditional Random Fields, Statistical Shape Models are becoming increasingly popular over the past 5 years. In this review paper, a broad overview of recent literature on bringing anatomical constraints for medical image segmentation is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed and potential future work is elaborated. We review the most relevant papers published until the submission date. For quick access, important details such as the underlying method, datasets and performance are tabulated.      
### 6.Predicting Pneumonia and Region Detection from X-Ray Images using Deep Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2101.07717.pdf)
>  Biomedical images are increasing drastically. Along the way, many machine learning algorithms have been proposed to predict and identify various kinds of diseases. One such disease is Pneumonia which is an infection caused by both bacteria and viruses through the inflammation of a person's lung air sacs. In this paper, an algorithm was proposed that receives x-ray images as input and verifies whether this patient is infected by Pneumonia as well as specific region of the lungs that the inflammation has occurred at. The algorithm is based on the transfer learning mechanism where pre-trained ResNet-50 (Convolutional Neural Network) was used followed by some custom layer for making the prediction. The model has achieved an accuracy of 90.6 percent which confirms that the model is effective and can be implemented for the detection of Pneumonia in patients. Furthermore, a class activation map is used for the detection of the infected region in the lungs. Also, PneuNet was developed so that users can access more easily and use the services.      
### 7.Meningioma segmentation in T1-weighted MRI leveraging global context and attention mechanisms  [ :arrow_down: ](https://arxiv.org/pdf/2101.07715.pdf)
>  Meningiomas are the most common type of primary brain tumor, accounting for approximately 30% of all brain tumors. A substantial number of these tumors are never surgically removed but rather monitored over time. Automatic and precise meningioma segmentation is therefore beneficial to enable reliable growth estimation and patient-specific treatment planning. In this study, we propose the inclusion of attention mechanisms over a U-Net architecture: (i) Attention-gated U-Net (AGUNet) and (ii) Dual Attention U-Net (DAUNet), using a 3D MRI volume as input. Attention has the potential to leverage the global context and identify features' relationships across the entire volume. To limit spatial resolution degradation and loss of detail inherent to encoder-decoder architectures, we studied the impact of multi-scale input and deep supervision components. The proposed architectures are trainable end-to-end and each concept can be seamlessly disabled for ablation studies. The validation studies were performed using a 5-fold cross validation over 600 T1-weighted MRI volumes from St. Olavs University Hospital, Trondheim, Norway. For the best performing architecture, an average Dice score of 81.6% was reached for an F1-score of 95.6%. With an almost perfect precision of 98%, meningiomas smaller than 3ml were occasionally missed hence reaching an overall recall of 93%. Leveraging global context from a 3D MRI volume provided the best performances, even if the native volume resolution could not be processed directly. Overall, near-perfect detection was achieved for meningiomas larger than 3ml which is relevant for clinical use. In the future, the use of multi-scale designs and refinement networks should be further investigated to improve the performance. A larger number of cases with meningiomas below 3ml might also be needed to improve the performance for the smallest tumors.      
### 8.Multi-target detection with rotations  [ :arrow_down: ](https://arxiv.org/pdf/2101.07709.pdf)
>  We consider the multi-target detection problem of estimating a two-dimensional target image from a large noisy measurement image that contains many randomly rotated and translated copies of the target image. Motivated by single-particle cryo-electron microscopy, we focus on the low signal-to-noise regime, where it is difficult to estimate the locations and orientations of the target images in the measurement. Our approach uses autocorrelation analysis to estimate rotationally and translationally invariant features of the target image. We demonstrate that, regardless of the level of noise, our technique can be used to recover the target image when the measurement is sufficiently large.      
### 9.A Novel Cluster Classify Regress Model Predictive Controller Formulation; CCR-MPC  [ :arrow_down: ](https://arxiv.org/pdf/2101.07655.pdf)
>  In this work, we develop a novel data-driven model predictive controller using advanced techniques in the field of machine learning. The objective is to regulate control signals to adjust the desired internal room setpoint temperature, affected indirectly by the external weather states. The methodology involves developing a time-series machine learning model with either a Long Short Term Memory model (LSTM) or a Gradient Boosting Algorithm (XGboost), capable of forecasting this weather states for any desired time horizon and concurrently optimising the control signals to the desired set point. The supervised learning model for mapping the weather states together with the control signals to the room temperature is constructed using a previously developed methodology called Cluster Classify regress (CCR), which is similar in style but scales better to high dimensional dataset than the well-known Mixture-of-Experts. The overall method called CCR-MPC involves a combination of a time series model for weather states prediction, CCR for forwarding and any numerical optimisation method for solving the inverse problem. Forward uncertainty quantification (Forward-UQ) leans towards the regression model in the CCR and is attainable using a Bayesian deep neural network or a Gaussian process (GP). For this work, in the CCR modulation, we employ K-means clustering for Clustering, XGboost classifier for Classification and 5th order polynomial regression for Regression. Inverse UQ can also be obtained by using an I-ES approach for solving the inverse problem or even the well-known Markov chain Monte Carlo (MCMC) approach. The developed CCR-MPC is elegant, and as seen on the numerical experiments is able to optimise the controller to attain the desired setpoint temperature.      
### 10.Unsupervised Domain Adaptation from Axial toShort-Axis Multi-Slice Cardiac MR Images byIncorporating Pretrained Task Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.07653.pdf)
>  Anisotropic multi-slice Cardiac Magnetic Resonance (CMR) Images are conventionally acquired in patient-specific short-axis (SAX) orientation. In specific cardiovascular diseases that affect right ventricular (RV) morphology, acquisitions in standard axial (AX) orientation are preferred by some investigators, due to potential superiority in RV volume measurement for treatment planning. Unfortunately, due to the rare occurrence of these diseases, data in this domain is scarce. Recent research in deep learning-based methods mainly focused on SAX CMR images and they had proven to be very successful. In this work, we show that there is a considerable domain shift between AX and SAX images, and therefore, direct application of existing models yield sub-optimal results on AX samples. We propose a novel unsupervised domain adaptation approach, which uses task-related probabilities in an attention mechanism. Beyond that, cycle consistency is imposed on the learned patient-individual 3D rigid transformation to improve stability when automatically re-sampling the AX images to SAX orientations. The network was trained on 122 registered 3D AX-SAX CMR volume pairs from a multi-centric patient cohort. A mean 3D Dice of $0.86\pm{0.06}$ for the left ventricle, $0.65\pm{0.08}$ for the myocardium, and $0.77\pm{0.10}$ for the right ventricle could be achieved. This is an improvement of $25\%$ in Dice for RV in comparison to direct application on axial slices. To conclude, our pre-trained task module has neither seen CMR images nor labels from the target domain, but is able to segment them after the domain gap is reduced. Code: <a class="link-external link-https" href="https://github.com/Cardio-AI/3d-mri-domain-adaptation" rel="external noopener nofollow">this https URL</a>      
### 11.A Note on Order and Index Reduction for Descriptor Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.07649.pdf)
>  We present order reduction results for linear time invariant descriptor systems. Results are given for both forced and unforced systems as well methods for constructing the reduced order systems. Our results establish a precise connection between classical and new results on this topic, and lead to an elementary construction of quasi-Weierstrass forms for a descriptor system. Examples are given to illustrate the usefulness of our results.      
### 12.Improved Coefficients for the Karagiannidis-Lioumpas Approximations and Bounds to the Gaussian Q-Function  [ :arrow_down: ](https://arxiv.org/pdf/2101.07631.pdf)
>  We revisit the Karagiannidis-Lioumpas (KL) approximation of the Q-function by optimizing its coefficients in terms of absolute error, relative error and total error. For minimizing the maximum absolute/relative error, we describe the targeted uniform error functions by sets of nonlinear equations so that the optimized coefficients are the solutions thereof. The total error is minimized with numerical search. We also introduce an extra coefficient in the KL approximation to achieve significantly tighter absolute and total error at the expense of unbounded relative error. Furthermore, we extend the KL expression to lower and upper bounds with optimized coefficients that minimize the error measures in the same way as for the approximations.      
### 13.A Lightweight Structure Aimed to Utilize Spatial Correlation for Sparse-View CT Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2101.07613.pdf)
>  Sparse-view computed tomography (CT) is known as a widely used approach to reduce radiation dose while accelerating imaging through lowered projection views and correlated calculations. However, its severe imaging noise and streaking artifacts turn out to be a major issue in the low dose protocol. In this paper, we propose a dual-domain deep learning-based method that breaks through the limitations of currently prevailing algorithms that merely process single image slices. Since the scanned object usually contains a high degree of spatial continuity, the obtained consecutive imaging slices embody rich information that is largely unexplored. Therefore, we establish a cascade model named LS-AAE which aims to tackle the above problem. In addition, in order to adapt to the social trend of lightweight medical care, our model adopts the inverted residual with linear bottleneck in the module design to make it mobile and lightweight (reduce model parameters to one-eighth of its original) without sacrificing its performance. In our experiments, sparse sampling is conducted at intervals of 4°, 8° and 16°, which appears to be a challenging sparsity that few scholars have attempted before. Nevertheless, our method still exhibits its robustness and achieves the state-of-the-art performance by reaching the PSNR of 40.305 and the SSIM of 0.948, while ensuring high model mobility. Particularly, it still exceeds other current methods when the sampling rate is one-fourth of them, thereby demonstrating its remarkable superiority.      
### 14.Comparative Evaluation of 3D and 2D Deep Learning Techniques for Semantic Segmentation in CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2101.07612.pdf)
>  Image segmentation plays a pivotal role in several medical-imaging applications by assisting the segmentation of the regions of interest. Deep learning-based approaches have been widely adopted for semantic segmentation of medical data. In recent years, in addition to 2D deep learning architectures, 3D architectures have been employed as the predictive algorithms for 3D medical image data. In this paper, we propose a 3D stack-based deep learning technique for segmenting manifestations of consolidation and ground-glass opacities in 3D Computed Tomography (CT) scans. We also present a comparison based on the segmentation results, the contextual information retained, and the inference time between this 3D technique and a traditional 2D deep learning technique. We also define the area-plot, which represents the peculiar pattern observed in the slice-wise areas of the pathology regions predicted by these deep learning models. In our exhaustive evaluation, 3D technique performs better than the 2D technique for the segmentation of CT scans. We get dice scores of 79% and 73% for the 3D and the 2D techniques respectively. The 3D technique results in a 5X reduction in the inference time compared to the 2D technique. Results also show that the area-plots predicted by the 3D model are more similar to the ground truth than those predicted by the 2D model. We also show how increasing the amount of contextual information retained during the training can improve the 3D model's performance.      
### 15.Deep Learning Models for Calculation of Cardiothoracic Ratio from Chest Radiographs for Assisted Diagnosis of Cardiomegaly  [ :arrow_down: ](https://arxiv.org/pdf/2101.07606.pdf)
>  We propose an automated method based on deep learning to compute the cardiothoracic ratio and detect the presence of cardiomegaly from chest radiographs. We develop two separate models to demarcate the heart and chest regions in an X-ray image using bounding boxes and use their outputs to calculate the cardiothoracic ratio. We obtain a sensitivity of 0.96 at a specificity of 0.81 with a mean absolute error of 0.0209 on a held-out test dataset and a sensitivity of 0.84 at a specificity of 0.97 with a mean absolute error of 0.018 on an independent dataset from a different hospital. We also compare three different segmentation model architectures for the proposed method and observe that Attention U-Net yields better results than SE-Resnext U-Net and EfficientNet U-Net. By providing a numeric measurement of the cardiothoracic ratio, we hope to mitigate human subjectivity arising out of visual assessment in the detection of cardiomegaly.      
### 16.Real-Time Limited-View CT Inpainting and Reconstruction with Dual Domain Based on Spatial Information  [ :arrow_down: ](https://arxiv.org/pdf/2101.07594.pdf)
>  Low-dose Computed Tomography is a common issue in reality. Current reduction, sparse sampling and limited-view scanning can all cause it. Between them, limited-view CT is general in the industry due to inevitable mechanical and physical limitation. However, limited-view CT can cause serious imaging problem on account of its massive information loss. Thus, we should effectively utilize the scant prior information to perform completion. It is an undeniable fact that CT imaging slices are extremely dense, which leads to high continuity between successive images. We realized that fully exploit the spatial correlation between consecutive frames can significantly improve restoration results in video inpainting. Inspired by this, we propose a deep learning-based three-stage algorithm that hoist limited-view CT imaging quality based on spatial information. In stage one, to better utilize prior information in the Radon domain, we design an adversarial autoencoder to complement the Radon data. In the second stage, a model is built to perform inpainting based on spatial continuity in the image domain. At this point, we have roughly restored the imaging, while its texture still needs to be finely repaired. Hence, we propose a model to accurately restore the image in stage three, and finally achieve an ideal inpainting result. In addition, we adopt FBP instead of SART-TV to make our algorithm more suitable for real-time use. In the experiment, we restore and reconstruct the Radon data that has been cut the rear one-third part, they achieve PSNR of 40.209, SSIM of 0.943, while precisely present the texture.      
### 17.Using StyleGAN for Visual Interpretability of Deep Learning Models on Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2101.07563.pdf)
>  As AI-based medical devices are becoming more common in imaging fields like radiology and histology, interpretability of the underlying predictive models is crucial to expand their use in clinical practice. Existing heatmap-based interpretability methods such as GradCAM only highlight the location of predictive features but do not explain how they contribute to the prediction. In this paper, we propose a new interpretability method that can be used to understand the predictions of any black-box model on images, by showing how the input image would be modified in order to produce different predictions. A StyleGAN is trained on medical images to provide a mapping between latent vectors and images. Our method identifies the optimal direction in the latent space to create a change in the model prediction. By shifting the latent representation of an input image along this direction, we can produce a series of new synthetic images with changed predictions. We validate our approach on histology and radiology images, and demonstrate its ability to provide meaningful explanations that are more informative than GradCAM heatmaps. Our method reveals the patterns learned by the model, which allows clinicians to build trust in the model's predictions, discover new biomarkers and eventually reveal potential biases.      
### 18.Electrocardiogram Classification and Visual Diagnosis of Atrial Fibrillation with DenseECG  [ :arrow_down: ](https://arxiv.org/pdf/2101.07535.pdf)
>  Atrial Fibrillation (AF) is a common cardiac arrhythmia affecting a large number of people around the world. If left undetected, it will develop into chronic disability or even early mortality. However, patients who have this problem can barely feel its presence, especially in its early stage. A non-invasive, automatic, and effective detection method is therefore needed to help early detection so that medical intervention can be implemented in time to prevent its progression. <br>Electrocardiogram (ECG), which records the electrical activities of the heart, has been widely used for detecting the presence of AF. However, due to the subtle patterns of AF, the performance of detection models have largely depended on complicated data pre-processing and expertly engineered features. In our work, we developed DenseECG, an end-to-end model based on 5 layers 1D densely connected convolutional neural network. We trained our model using the publicly available dataset from 2017 PhysioNet Computing in Cardiology(CinC) Challenge containing 8528 single-lead ECG recordings of short-term heart rhythms (9-61s). Our trained model was able to outperform the other state-of-the-art AF detection models on this dataset without complicated data pre-processing and expert-supervised feature engineering.      
### 19.Structure Synthesis of Op-Amps by Functional Block Composition  [ :arrow_down: ](https://arxiv.org/pdf/2101.07517.pdf)
>  This paper presents a method to automatically synthesize the structure of an operational amplifier. It is positioned between approaches with fixed design plans and a small search space of structures and approaches with generic structural production rules and a large search space with technically impractical structures. Based on [1], the presented approach develops a hierarchical functional composition graph that spans a search space of thousands of technically meaningful structure variants for single-output, fully-differential and complementary operational amplifiers. The search algorithm is a combined heuristic and enumerative process. The evaluation is based on circuit sizing with a library of behavioral equations of functional blocks [2]. Formalizing the knowledge of functional blocks in op-amps for structural synthesis and sizing inherently reduces the search space and lessens the number of created topologies not fulfilling the specifications. Experimental results for the three op-amp classes are presented.      
### 20.Deep Learning Based Channel Covariance Matrix Estimation with User Location and Scene Images  [ :arrow_down: ](https://arxiv.org/pdf/2101.07471.pdf)
>  Channel covariance matrix (CCM) is one critical parameter for designing the communications systems. In this paper, a novel framework of the deep learning (DL) based CCM estimation is proposed that exploits the perception of the transmission environment without any channel sample or the pilot signals. Specifically, as CCM is affected by the user's movement within a specific environment, we design a deep neural network (DNN) to predict CCM from user location and user speed, and the corresponding estimation method is named as ULCCME. A location denoising method is further developed to reduce the positioning error and improve the robustness of ULCCME. For cases when user location information is not available, we propose an interesting way that uses the environmental 3D images to predict the CCM, and the corresponding estimation method is named as SICCME. Simulation results show that both the proposed methods are effective and will benefit the subsequent channel estimation between the transceivers.      
### 21.COVID-Net CT-2: Enhanced Deep Neural Networks for Detection of COVID-19 from Chest CT Images Through Bigger, More Diverse Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.07433.pdf)
>  The COVID-19 pandemic continues to rage on, with multiple waves causing substantial harm to health and economies around the world. Motivated by the use of CT imaging at clinical institutes around the world as an effective complementary screening method to RT-PCR testing, we introduced COVID-Net CT, a neural network tailored for detection of COVID-19 cases from chest CT images as part of the open source COVID-Net initiative. However, one potential limiting factor is restricted quantity and diversity given the single nation patient cohort used. In this study, we introduce COVID-Net CT-2, enhanced deep neural networks for COVID-19 detection from chest CT images trained on the largest quantity and diversity of multinational patient cases in research literature. We introduce two new CT benchmark datasets, the largest comprising a multinational cohort of 4,501 patients from at least 15 countries. We leverage explainability to investigate the decision-making behaviour of COVID-Net CT-2, with the results for select cases reviewed and reported on by two board-certified radiologists with over 10 and 30 years of experience, respectively. The COVID-Net CT-2 neural networks achieved accuracy, COVID-19 sensitivity, and COVID-19 positive predictive value of 98.1%/96.2%/96.7% and 97.9%/95.7%/96.4%, respectively. Explainability-driven performance validation shows that COVID-Net CT-2's decision-making behaviour is consistent with radiologist interpretation by leveraging correct, clinically relevant critical factors. The results are promising and suggest the strong potential of deep neural networks as an effective tool for computer-aided COVID-19 assessment. While not a production-ready solution, we hope the open-source, open-access release of COVID-Net CT-2 and benchmark datasets will continue to enable researchers, clinicians, and citizen data scientists alike to build upon them.      
### 22.Learning Efficient, Explainable and Discriminative Representations for Pulmonary Nodules Classification  [ :arrow_down: ](https://arxiv.org/pdf/2101.07429.pdf)
>  Automatic pulmonary nodules classification is significant for early diagnosis of lung cancers. Recently, deep learning techniques have enabled remarkable progress in this field. However, these deep models are typically of high computational complexity and work in a black-box manner. To combat these challenges, in this work, we aim to build an efficient and (partially) explainable classification model. Specially, we use \emph{neural architecture search} (NAS) to automatically search 3D network architectures with excellent accuracy/speed trade-off. Besides, we use the convolutional block attention module (CBAM) in the networks, which helps us understand the reasoning process. During training, we use A-Softmax loss to learn angularly discriminative representations. In the inference stage, we employ an ensemble of diverse neural networks to improve the prediction accuracy and robustness. We conduct extensive experiments on the LIDC-IDRI database. Compared with previous state-of-the-art, our model shows highly comparable performance by using less than 1/40 parameters. Besides, empirical study shows that the reasoning process of learned networks is in conformity with physicians' diagnosis. Related code and results have been released at: <a class="link-external link-https" href="https://github.com/fei-hdu/NAS-Lung" rel="external noopener nofollow">this https URL</a>.      
### 23.Compressive Spectral Image Reconstruction using Deep Prior and Low-Rank Tensor Representation  [ :arrow_down: ](https://arxiv.org/pdf/2101.07424.pdf)
>  Compressive spectral imaging (CSI) has emerged as an alternative spectral image acquisition technology, which reduces the number of measurements at the cost of requiring a recovery process. In general, the reconstruction methods are based on hand-crafted priors used as regularizers in optimization algorithms or recent deep neural networks employed as an image generator to learn a non-linear mapping from the low-dimensional compressed measurements to the image space. However, these data-driven methods need many spectral images to obtain good performance. In this work, a deep recovery framework for CSI without training data is presented. The proposed method is based on the fact that the structure of some deep neural networks and an appropriated low-dimensional structure are sufficient to impose a structure of the underlying spectral image from CSI. We analyzed the low-dimension structure via the Tucker representation, modeled in the first net layer. The proposed scheme is obtained by minimizing the $\ell_2$-norm distance between the compressive measurements and the predicted measurements, and the desired recovered spectral image is formed just before the forward operator. Simulated and experimental results verify the effectiveness of the proposed method.      
### 24.Improved parallel WaveGAN vocoder with perceptually weighted spectrogram loss  [ :arrow_down: ](https://arxiv.org/pdf/2101.07412.pdf)
>  This paper proposes a spectral-domain perceptual weighting technique for Parallel WaveGAN-based text-to-speech (TTS) systems. The recently proposed Parallel WaveGAN vocoder successfully generates waveform sequences using a fast non-autoregressive WaveNet model. By employing multi-resolution short-time Fourier transform (MR-STFT) criteria with a generative adversarial network, the light-weight convolutional networks can be effectively trained without any distillation process. To further improve the vocoding performance, we propose the application of frequency-dependent weighting to the MR-STFT loss function. The proposed method penalizes perceptually-sensitive errors in the frequency domain; thus, the model is optimized toward reducing auditory noise in the synthesized speech. Subjective listening test results demonstrate that our proposed method achieves 4.21 and 4.26 TTS mean opinion scores for female and male Korean speakers, respectively.      
### 25.Deep-Learning Driven Noise Reduction for Reduced Flux Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2101.07376.pdf)
>  Deep neural networks have received considerable attention in clinical imaging, particularly with respect to the reduction of radiation risk. Lowering the radiation dose by reducing the photon flux inevitably results in the degradation of the scanned image quality. Thus, researchers have sought to exploit deep convolutional neural networks (DCNNs) to map low-quality, low-dose images to higher-dose, higher-quality images thereby minimizing the associated radiation hazard. Conversely, computed tomography (CT) measurements of geomaterials are not limited by the radiation dose. In contrast to the human body, however, geomaterials may be comprised of high-density constituents causing increased attenuation of the X-Rays. Consequently, higher dosage images are required to obtain an acceptable scan quality. The problem of prolonged acquisition times is particularly severe for micro-CT based scanning technologies. Depending on the sample size and exposure time settings, a single scan may require several hours to complete. This is of particular concern if phenomena with an exponential temperature dependency are to be elucidated. A process may happen too fast to be adequately captured by CT scanning. To address the aforementioned issues, we apply DCNNs to improve the quality of rock CT images and reduce exposure times by more than 60\%, simultaneously. We highlight current results based on micro-CT derived datasets and apply transfer learning to improve DCNN results without increasing training time. The approach is applicable to any computed tomography technology. Furthermore, we contrast the performance of the DCNN trained by minimizing different loss functions such as mean squared error and structural similarity index.      
### 26.Feature Fusion of Raman Chemical Imaging and Digital Histopathology using Machine Learning for Prostate Cancer Detection  [ :arrow_down: ](https://arxiv.org/pdf/2101.07342.pdf)
>  The diagnosis of prostate cancer is challenging due to the heterogeneity of its presentations, leading to the over diagnosis and treatment of non-clinically important disease. Accurate diagnosis can directly benefit a patient's quality of life and prognosis. Towards addressing this issue, we present a learning model for the automatic identification of prostate cancer. While many prostate cancer studies have adopted Raman spectroscopy approaches, none have utilised the combination of Raman Chemical Imaging (RCI) and other imaging modalities. This study uses multimodal images formed from stained Digital Histopathology (DP) and unstained RCI. The approach was developed and tested on a set of 178 clinical samples from 32 patients, containing a range of non-cancerous, Gleason grade 3 (G3) and grade 4 (G4) tissue microarray samples. For each histological sample, there is a pathologist labelled DP - RCI image pair. The hypothesis tested was whether multimodal image models can outperform single modality baseline models in terms of diagnostic accuracy. Binary non-cancer/cancer models and the more challenging G3/G4 differentiation were investigated. Regarding G3/G4 classification, the multimodal approach achieved a sensitivity of 73.8% and specificity of 88.1% while the baseline DP model showed a sensitivity and specificity of 54.1% and 84.7% respectively. The multimodal approach demonstrated a statistically significant 12.7% AUC advantage over the baseline with a value of 85.8% compared to 73.1%, also outperforming models based solely on RCI and median Raman spectra. Feature fusion of DP and RCI does not improve the more trivial task of tumour identification but does deliver an observed advantage in G3/G4 discrimination. Building on these promising findings, future work could include the acquisition of larger datasets for enhanced model generalization.      
### 27.Component Importance and Interdependence Analysis for Transmission, Distribution and Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.07306.pdf)
>  For critical infrastructure restoration planning, the real-time scheduling and coordination of system restoration efforts, the key in decision-making is to prioritize those critical components that are out of service during the restoration. For this purpose, there is a need for component importance analysis. While it has been investigated extensively for individual systems, component importance considering interdependence among transmission, distribution and communication (T&amp;D&amp;C) systems has not been systematically analyzed and widely adopted. In this study, we propose a component importance assessment method in the context of interdependence between T&amp;D&amp;C networks. Analytic methods for multilayer networks and a set of metrics have been applied for assessing the component importance and interdependence between T&amp;D&amp;C networks based on their physical characteristics. The proposed methodology is further validated with integrated synthetic Illinois regional transmission, distribution, and communication (T&amp;D&amp;C) systems, the results reveal the unique characteristics of component/node importance, which may be strongly affected by the network topology and cross-domain node mapping.      
### 28.Visualizing Missing Surfaces In Colonoscopy Videos using Shared Latent Space Representations  [ :arrow_down: ](https://arxiv.org/pdf/2101.07280.pdf)
>  Optical colonoscopy (OC), the most prevalent colon cancer screening tool, has a high miss rate due to a number of factors, including the geometry of the colon (haustral fold and sharp bends occlusions), endoscopist inexperience or fatigue, endoscope field of view, etc. We present a framework to visualize the missed regions per-frame during the colonoscopy, and provides a workable clinical solution. Specifically, we make use of 3D reconstructed virtual colonoscopy (VC) data and the insight that VC and OC share the same underlying geometry but differ in color, texture and specular reflections, embedded in the OC domain. A lossy unpaired image-to-image translation model is introduced with enforced shared latent space for OC and VC. This shared latent space captures the geometric information while deferring the color, texture, and specular information creation to additional Gaussian noise input. This additional noise input can be utilized to generate one-to-many mappings from VC to OC and OC to OC.      
### 29.Developing a Deep Neural Network to Denoise Time-Resolved In Situ ETEM Movies of Catalyst Nanoparticles  [ :arrow_down: ](https://arxiv.org/pdf/2101.07770.pdf)
>  A deep learning-based convolutional neural network has been developed to denoise atomic-resolution in situ TEM image datasets of catalyst nanoparticles acquired on high speed, direct electron counting detectors, where the signal is severely limited by shot noise. The network was applied to a model catalyst of CeO2-supported Pt nanoparticles. We leverage multislice simulation to generate a large and flexible dataset for training and testing the network. The proposed network outperforms state-of-the-art denoising methods by a significant margin both on simulated and experimental test data. Factors contributing to the performance are identified, including most importantly (a) the geometry of the images used during training and (b) the size of the network's receptive field. Through a gradient-based analysis, we investigate the mechanisms used by the network to denoise experimental images. This shows the network exploits information on the surrounding structure and that it adapts its filtering approach when it encounters atomic-level defects at the catalyst surface. Extensive analysis has been done to characterize the network's ability to correctly predict the exact atomic structure at the catalyst surface. Finally, we develop an approach based on the log-likelihood ratio test that provides an quantitative measure of uncertainty regarding the atomic-level structure in the network-denoised image.      
### 30.Towards duration robust weakly supervised sound event detection  [ :arrow_down: ](https://arxiv.org/pdf/2101.07687.pdf)
>  Sound event detection (SED) is the task of tagging the absence or presence of audio events and their corresponding interval within a given audio clip. While SED can be done using supervised machine learning, where training data is fully labeled with access to per event timestamps and duration, our work focuses on weakly-supervised sound event detection (WSSED), where prior knowledge about an event's duration is unavailable. Recent research within the field focuses on improving localization performance for specific datasets regarding specific evaluation metrics. Specifically, well-performing event-level localization work requires fully labeled development subsets to obtain event duration estimates, which significantly benefits localization performance. Moreover, well-performing segment-level localization models output predictions at a coarse-scale, hindering their deployment on datasets containing very short events. This work proposes a duration robust CRNN (CDur) framework, which aims to achieve competitive performance in terms of segment- and event-level localization. In the meantime, this paper proposes a new post-processing strategy named Triple Threshold and investigates two data augmentation methods along with a label smoothing method within the scope of WSSED. Evaluation of our model is done on three publicly available datasets: Task 4 of the DCASE2017 and 2018 datasets, as well as URBAN-SED. Our model outperforms other approaches on the DCASE2018 and URBAN-SED datasets without requiring prior duration knowledge. In particular, our model is capable of similar performance to strongly-labeled supervised models on the URBAN-SED dataset. Lastly, we run a series of ablation experiments to reveal that without post-processing, our model's localization performance drop is significantly lower compared with other approaches.      
### 31.A framework to compare music generative models using automatic evaluation metrics extended to rhythm  [ :arrow_down: ](https://arxiv.org/pdf/2101.07669.pdf)
>  To train a machine learning model is necessary to take numerous decisions about many options for each process involved, in the field of sequence generation and more specifically of music composition, the nature of the problem helps to narrow the options but at the same time, some other options appear for specific challenges. This paper takes the framework proposed in a previous research that did not consider rhythm to make a series of design decisions, then, rhythm support is added to evaluate the performance of two RNN memory cells in the creation of monophonic music. The model considers the handling of music transposition and the framework evaluates the quality of the generated pieces using automatic quantitative metrics based on geometry which have rhythm support added as well.      
### 32.Ptychography Intensity Interferometry Imaging for Dynamic Distant Object  [ :arrow_down: ](https://arxiv.org/pdf/2101.07662.pdf)
>  As a promising lensless imaging method for distance objects, intensity interferometry imaging (III) had been suffering from the unreliable phase retrieval process, hindering the development of III for decades. Recently, the introduction of the ptychographic detection in III overcame this challenge, and a method called ptychographic III (PIII) was proposed. We here experimentally demonstrate that PIII can image a dynamic distance object. A reasonable image for the moving object can be retrieved with only two speckle patterns for each probe, and only 10 to 20 iterations are needed. Meanwhile, PIII exhibits robust to the inaccurate information of the probe. Furthermore, PIII successfully recovers the image through a fog obfuscating the imaging light path, under which a conventional camera relying on lenses fails to provide a recognizable image.      
### 33.Improve Global Glomerulosclerosis Classification with Imbalanced Data using CircleMix Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2101.07654.pdf)
>  The classification of glomerular lesions is a routine and essential task in renal pathology. Recently, machine learning approaches, especially deep learning algorithms, have been used to perform computer-aided lesion characterization of glomeruli. However, one major challenge of developing such methods is the naturally imbalanced distribution of different lesions. In this paper, we propose CircleMix, a novel data augmentation technique, to improve the accuracy of classifying globally sclerotic glomeruli with a hierarchical learning strategy. Different from the recently proposed CutMix method, the CircleMix augmentation is optimized for the ball-shaped biomedical objects, such as glomeruli. 6,861 glomeruli with five classes (normal, periglomerular fibrosis, obsolescent glomerulosclerosis, solidified glomerulosclerosis, and disappearing glomerulosclerosis) were employed to develop and evaluate the proposed methods. From five-fold cross-validation, the proposed CircleMix augmentation achieved superior performance (Balanced Accuracy=73.0%) compared with the EfficientNet-B0 baseline (Balanced Accuracy=69.4%)      
### 34.Max-Min Fairness Based on Cooperative-NOMA Clustering for Ultra-Reliable and Low-Latency Communications  [ :arrow_down: ](https://arxiv.org/pdf/2101.07641.pdf)
>  In this paper, the performance of a cooperative relaying technique in a non-orthogonal multiple access (NOMA) system, briefly named cooperative NOMA (C-NOMA), is considered in short packet communications with finite blocklength (FBL) codes. We examine the performance of a decode-and-forward (DF) relaying along with selection combining (SC) and maximum ratio combining (MRC) strategies at the receiver. Our goal is user clustering based on C-NOMA to maximize fair throughput in a DL-NOMA scenario. In each cluster, the user with a stronger channel (strong user) acts as a relay for the other one (weak user), and optimal power and blocklength are allocated to achieve max-min throughput.      
### 35.UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data  [ :arrow_down: ](https://arxiv.org/pdf/2101.07597.pdf)
>  In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.      
### 36.Single-RF Multi-User Communication Through Reconfigurable Intelligent Surfaces: An Information-Theoretic Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2101.07556.pdf)
>  Reconfigurable intelligent surfaces (RISs) are typically used in multi-user systems to mitigate interference among active transmitters. In contrast, this paper studies a setting with a conventional active encoder as well as a passive encoder that modulates the reflection pattern of the RIS. The RIS hence serves the dual purpose of improving the rate of the active encoder and of enabling communication from the second encoder. The capacity region is characterized, and information-theoretic insights regarding the trade-offs between the rates of the two encoders are derived by focusing on the high- and low-power regimes.      
### 37.UAV-Enabled Cooperative Jamming for Covert Communications  [ :arrow_down: ](https://arxiv.org/pdf/2101.07502.pdf)
>  In this paper a novel unmanned aerial vehicle aided (UAV) cooperative jamming scheme is proposed for covert communications. We first analyze the detection performance of the system to obtain the minimum error detection probability of the eavesdropper and then determine the transmission rate as the objective function by analyzing the transmission outage probability of the communication. The problem formulate is non-convex that is difficult to solve. To solve this, two efficient algorithms are proposed for general signal to interference plus noise ratio (SINR) and high SINR, respectively. The first algorithm applying the block coordinate descent (BCD) to decompose the problem into two subproblems and then solve them by successive convex approximation (SCA). For the second algorithm, we use a geometric method(GM) based on the Apollonius of Sphere to solve the optimization problem. The proposed scheme can enhance the covert performance significantly. Simulations verify that the proposed joint design can enhance the covert transmission rate of the considered system as compared to the benchmark schemes.      
### 38.Automated Verification and Synthesis of Stochastic Hybrid Systems: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2101.07491.pdf)
>  Stochastic hybrid systems have received significant attentions as a relevant modelling framework describing many systems, from engineering to the life sciences: they enable the study of numerous applications, including transportation networks, biological systems and chemical reaction networks, smart energy and power grids, and beyond. Automated verification and policy synthesis for stochastic hybrid systems can be inherently challenging: this is due to the heterogeneity of their dynamics (presence of continuous and discrete components), the presence of uncertainty, and in some applications the large dimension of state and input sets. Over the past few years, a few hundred articles have investigated these models, and developed diverse and powerful approaches to mitigate difficulties encountered in the analysis and synthesis of such complex stochastic systems. In this survey, we overview the most recent results in the literature and discuss different approaches, including (in)finite abstractions, verification and synthesis for temporal logic specifications, stochastic similarity relations, (control) barrier certificates, compositional techniques, and a selection of results on continuous-time stochastic systems; we finally survey recently developed software tools that implement the discussed approaches. Throughout the manuscript we discuss a few open topics to be considered as potential future research directions: we hope that this survey will guide younger researchers through a comprehensive understanding of the various challenges, tools, and solutions in this enticing and rich scientific area.      
### 39.Learning Control of Quantum Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.07461.pdf)
>  This paper provides a brief introduction to learning control of quantum systems. In particular, the following aspects are outlined, including gradient-based learning for optimal control of quantum systems, evolutionary computation for learning control of quantum systems, learning-based quantum robust control, and reinforcement learning for quantum control.      
### 40.Wide Color Gamut Image Content Characterization: Method, Evaluation, and Applications  [ :arrow_down: ](https://arxiv.org/pdf/2101.07451.pdf)
>  In this paper, we propose a novel framework to characterize a wide color gamut image content based on perceived quality due to the processes that change color gamut, and demonstrate two practical use cases where the framework can be applied. We first introduce the main framework and implementation details. Then, we provide analysis for understanding of existing wide color gamut datasets with quantitative characterization criteria on their characteristics, where four criteria, i.e., coverage, total coverage, uniformity, and total uniformity, are proposed. Finally, the framework is applied to content selection in a gamut mapping evaluation scenario in order to enhance reliability and robustness of the evaluation results. As a result, the framework fulfils content characterization for studies where quality of experience of wide color gamut stimuli is involved.      
### 41.Ambiguity of Objective Image Quality Metrics: A New Methodology for Performance Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2101.07439.pdf)
>  Objective image quality metrics try to estimate the perceptual quality of the given image by considering the characteristics of the human visual system. However, it is possible that the metrics produce different quality scores even for two images that are perceptually indistinguishable by human viewers, which have not been considered in the existing studies related to objective quality assessment. In this paper, we address the issue of ambiguity of objective image quality assessment. We propose an approach to obtain an ambiguity interval of an objective metric, within which the quality score difference is not perceptually significant. In particular, we use the visual difference predictor, which can consider viewing conditions that are important for visual quality perception. In order to demonstrate the usefulness of the proposed approach, we conduct experiments with 33 state-of-the-art image quality metrics in the viewpoint of their accuracy and ambiguity for three image quality databases. The results show that the ambiguity intervals can be applied as an additional figure of merit when conventional performance measurement does not determine superiority between the metrics. The effect of the viewing distance on the ambiguity interval is also shown.      
### 42.Swarm Herding: A Leader-Follower Framework For Multi-Robot Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2101.07416.pdf)
>  A leader-follower framework is proposed for multi-robot navigation of large scale teams where the leader agents corral the follower agents. A group of leaders is modeled as a 2D deformable object where discrete masses (i.e., leader robots) are interconnected by springs and dampers. A time-varying domain is defined by the positions of leaders while the external forces induce deformations of the domain from its nominal configuration. The team of followers is performing coverage over the time-varying domain by employing a perspective transformation that maps between the nominal and deformed configurations. A decentralized control strategy is proposed where a leader only takes local sensing information and information about its neighbors (connected by virtual springs and dampers), and a follower only needs partial information about leaders and information about its Delaunay neighbors.      
### 43.Galaxy Image Translation with Semi-supervised Noise-reconstructed Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.07389.pdf)
>  Image-to-image translation with Deep Learning neural networks, particularly with Generative Adversarial Networks (GANs), is one of the most powerful methods for simulating astronomical images. However, current work is limited to utilizing paired images with supervised translation, and there has been rare discussion on reconstructing noise background that encodes instrumental and observational effects. These limitations might be harmful for subsequent scientific applications in astrophysics. Therefore, we aim to develop methods for using unpaired images and preserving noise characteristics in image translation. In this work, we propose a two-way image translation model using GANs that exploits both paired and unpaired images in a semi-supervised manner, and introduce a noise emulating module that is able to learn and reconstruct noise characterized by high-frequency features. By experimenting on multi-band galaxy images from the Sloan Digital Sky Survey (SDSS) and the Canada France Hawaii Telescope Legacy Survey (CFHT), we show that our method recovers global and local properties effectively and outperforms benchmark image translation models. To our best knowledge, this work is the first attempt to apply semi-supervised methods and noise reconstruction techniques in astrophysical studies.      
### 44.Switched Systems as Hybrid Programs  [ :arrow_down: ](https://arxiv.org/pdf/2101.06195.pdf)
>  Real world systems of interest often feature interactions between discrete and continuous dynamics. Various hybrid system formalisms have been used to model and analyse this combination of dynamics, ranging from mathematical descriptions, e.g., using impulsive differential equations and switching, to automata-theoretic and language-based approaches. This paper bridges two such formalisms by showing how various classes of switched systems can be modeled using the language of hybrid programs from differential dynamic logic (dL). The resulting models enable the formal specification and verification of switched systems using dL and its existing deductive verification tools such as KeYmaera X. Switched systems also provide a natural avenue for the generalization of dL's deductive proof theory for differential equations. The completeness results for switched system invariants proved in this paper enable effective safety verification of those systems in dL.      
