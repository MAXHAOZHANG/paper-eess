# ArXiv eess --Fri, 29 Jan 2021
### 1.Use Non-Energy-Curtailment Resources for Primary Frequency Response in Future Low-Inertia Power Grids  [ :arrow_down: ](https://arxiv.org/pdf/2101.12092.pdf)
>  Power grid primary frequency response will be significantly impaired by Photovoltaic (PV) penetration increase because of the decrease in inertia and governor response. PV inertia and governor emulation requires reserving PV output and leads to solar energy waste. This paper exploits current grid resources and explores energy storage for primary frequency response under high PV penetration at the interconnection level. Based on the actual models of the U.S. Eastern Interconnection grid and the Texas grid, effects of multiple factors associated with primary frequency response, including the governor ratio, governor deadband, droop rate, and fast load response, are assessed under high PV penetration scenarios. In addition, performance of batteries and supercapacitors using different control strategies is studied in the two interconnections. The paper quantifies the potential of various resources to improve interconnection-level primary frequency response under high PV penetration without curtailing solar output.      
### 2.Risk-sensitive safety analysis using Conditional Value-at-Risk  [ :arrow_down: ](https://arxiv.org/pdf/2101.12086.pdf)
>  This paper develops a safety analysis method for stochastic systems that is sensitive to the possibility and severity of rare harmful outcomes. We define risk-sensitive safe sets as sub-level sets of the solution to a non-standard optimal control problem, where a random maximum cost is assessed using the Conditional Value-at-Risk (CVaR) functional. The solution to the control problem represents the maximum extent of constraint violation of the state trajectory, averaged over the $\alpha\cdot 100$% worst cases, where $\alpha \in (0,1]$. This problem is well-motivated but difficult to solve in a tractable fashion because temporal decompositions for risk functionals generally depend on the history of the system's behavior. Our primary theoretical contribution is to derive under-approximations to risk-sensitive safe sets, which are computationally tractable. Our method provides a novel, theoretically guaranteed, parameter-dependent upper bound to the CVaR of a maximum cost without the need to augment the state space. For a fixed parameter value, the solution to only one Markov decision process problem is required to obtain the under-approximations for any family of risk-sensitivity levels. In addition, we propose a second definition for risk-sensitive safe sets and provide a tractable method for their estimation without using a parameter-dependent upper bound. The second definition is expressed in terms of a new coherent risk functional, which is inspired by CVaR. We demonstrate our primary theoretical contribution using numerical examples of a thermostatically controlled load system and a stormwater system.      
### 3.Quantitative Resilience of Linear Driftless Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.12063.pdf)
>  This paper introduces the notion of quantitative resilience of a control system. Following prior work, we study systems enduring a loss of control authority over some of their actuators. Such a malfunction results in actuators producing possibly undesirable inputs over which the controller has real-time readings but no control. By definition, a system is resilient if it can still reach a target after a loss of control authority. However, after a malfunction a resilient system might be significantly slower to reach a target compared to its initial capabilities. We quantify this loss of performance through the new concept of quantitative resilience. We define this metric as the maximal ratio of the minimal times required to reach any target for the initial and malfunctioning systems. NaÃ¯ve computation of quantitative resilience directly from the definition is a time-consuming task as it requires solving four nested, possibly nonlinear, optimization problems. The main technical contribution of this work is to provide an efficient method to compute quantitative resilience. Relying on control theory and on two novel geometric results we reduce the computation of quantitative resilience to a single linear optimization problem. We illustrate our method on two numerical examples: an opinion dynamics scenario and a trajectory controller for low-thrust spacecrafts.      
### 4.Uncertainty aware and explainable diagnosis of retinal disease  [ :arrow_down: ](https://arxiv.org/pdf/2101.12041.pdf)
>  Deep learning methods for ophthalmic diagnosis have shown considerable success in tasks like segmentation and classification. However, their widespread application is limited due to the models being opaque and vulnerable to making a wrong decision in complicated cases. Explainability methods show the features that a system used to make prediction while uncertainty awareness is the ability of a system to highlight when it is not sure about the decision. This is one of the first studies using uncertainty and explanations for informed clinical decision making. We perform uncertainty analysis of a deep learning model for diagnosis of four retinal diseases - age-related macular degeneration (AMD), central serous retinopathy (CSR), diabetic retinopathy (DR), and macular hole (MH) using images from a publicly available (OCTID) dataset. Monte Carlo (MC) dropout is used at the test time to generate a distribution of parameters and the predictions approximate the predictive posterior of a Bayesian model. A threshold is computed using the distribution and uncertain cases can be referred to the ophthalmologist thus avoiding an erroneous diagnosis. The features learned by the model are visualized using a proven attribution method from a previous study. The effects of uncertainty on model performance and the relationship between uncertainty and explainability are discussed in terms of clinical significance. The uncertainty information along with the heatmaps make the system more trustworthy for use in clinical settings.      
### 5.(Stochastic) Model Predictive Control -- a Simulation Example  [ :arrow_down: ](https://arxiv.org/pdf/2101.12020.pdf)
>  This brief introduction to Model Predictive Control specifically addresses stochastic Model Predictive Control, where probabilistic constraints are considered. A simple linear system subject to uncertainty serves as an example. The Matlab code for this stochastic Model Predictive Control example is available online.      
### 6.Multi-Antenna Joint Radar and Communications: Precoder Optimization and Weighted Sum-Rate vs Probing Power Tradeoff  [ :arrow_down: ](https://arxiv.org/pdf/2101.11957.pdf)
>  In order to further exploit the potential of joint multi-antenna radar-communication (RadCom) system, we propose two transmission techniques respectively based on separated and shared antenna deployments. Both techniques are designed to maximize the weighted sum rate (WSR) and the probing power at target's location under average power constraints at the antennas such that the system can simultaneously communicate with downlink users and detect the target within the same frequency band. Based on a Weighted Minimized Mean Square Errors (WMMSE) method, the separated deployment transmission is designed via semidefinite programming (SDP) while the shared deployment problem is solved by majorization-minimization (MM) algorithm. Numerical results show that the shared deployment outperforms the separated deployment in radar beamforming. The tradeoffs between WSR and probing power at target are compared among both proposed transmissions and two practically simpler dual-function implementations i.e., time division and frequency division. Results show that although the separated deployment enables spectrum sharing, it experiences a performance loss compared with frequency division, while the shared deployment outperforms both and surpasses time division in certain conditions.      
### 7.Neural Particle Image Velocimetry  [ :arrow_down: ](https://arxiv.org/pdf/2101.11950.pdf)
>  In the past decades, great progress has been made in the field of optical and particle-based measurement techniques for experimental analysis of fluid flows. Particle Image Velocimetry (PIV) technique is widely used to identify flow parameters from time-consecutive snapshots of particles injected into the fluid. The computation is performed as post-processing of the experimental data via proximity measure between particles in frames of reference. However, the post-processing step becomes problematic as the motility and density of the particles increases, since the data emerges in extreme rates and volumes. Moreover, existing algorithms for PIV either provide sparse estimations of the flow or require large computational time frame preventing from on-line use. The goal of this manuscript is therefore to develop an accurate on-line algorithm for estimation of the fine-grained velocity field from PIV data. As the data constitutes a pair of images, we employ computer vision methods to solve the problem. In this work, we introduce a convolutional neural network adapted to the problem, namely Volumetric Correspondence Network (VCN) which was recently proposed for the end-to-end optical flow estimation in computer vision. The network is thoroughly trained and tested on a dataset containing both synthetic and real flow data. Experimental results are analyzed and compared to that of conventional methods as well as other recently introduced methods based on neural networks. Our analysis indicates that the proposed approach provides improved efficiency also keeping accuracy on par with other state-of-the-art methods in the field. We also verify through a-posteriori tests that our newly constructed VCN schemes are reproducing well physically relevant statistics of velocity and velocity gradients.      
### 8.An Explainable AI System for Automated COVID-19 Assessment and Lesion Categorization from CT-scans  [ :arrow_down: ](https://arxiv.org/pdf/2101.11943.pdf)
>  COVID-19 infection caused by SARS-CoV-2 pathogen is a catastrophic pandemic outbreak all over the world with exponential increasing of confirmed cases and, unfortunately, deaths. In this work we propose an AI-powered pipeline, based on the deep-learning paradigm, for automated COVID-19 detection and lesion categorization from CT scans. We first propose a new segmentation module aimed at identifying automatically lung parenchyma and lobes. Next, we combined such segmentation network with classification networks for COVID-19 identification and lesion categorization. We compare the obtained classification results with those obtained by three expert radiologists on a dataset consisting of 162 CT scans. Results showed a sensitivity of 90\% and a specificity of 93.5% for COVID-19 detection, outperforming those yielded by the expert radiologists, and an average lesion categorization accuracy of over 84%. Results also show that a significant role is played by prior lung and lobe segmentation that allowed us to enhance performance by over 20 percent points. The interpretation of the trained AI models, moreover, reveals that the most significant areas for supporting the decision on COVID-19 identification are consistent with the lesions clinically associated to the virus, i.e., crazy paving, consolidation and ground glass. This means that the artificial models are able to discriminate a positive patient from a negative one (both controls and patients with interstitial pneumonia tested negative to COVID) by evaluating the presence of those lesions into CT scans. Finally, the AI models are integrated into a user-friendly GUI to support AI explainability for radiologists, which is publicly available at <a class="link-external link-http" href="http://perceivelab.com/covid-ai" rel="external noopener nofollow">this http URL</a>.      
### 9.Automated Insulin Delivery for Type 1 Diabetes Mellitus Patients using Gaussian Process-based Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2101.11887.pdf)
>  The human insulin-glucose metabolism is a time-varying process, which is partly caused by the changing insulin sensitivity of the body. This insulin sensitivity follows a circadian rhythm and its effects should be anticipated by any automated insulin delivery system. This paper presents an extension of our previous work on automated insulin delivery by developing a controller suitable for humans with Type 1 Diabetes Mellitus. Furthermore, we enhance the controller with a new kernel function for the Gaussian Process and deal with noisy measurements, as well as, the noisy training data for the Gaussian Process, arising therefrom. This enables us to move the proposed control algorithm, a combination of Model Predictive Controller and a Gaussian Process, closer towards clinical application. Simulation results on the University of Virginia/Padova FDA-accepted metabolic simulator are presented for a meal schedule with random carbohydrate sizes and random times of carbohydrate uptake to show the performance of the proposed control scheme.      
### 10.A review of deep-learning techniques for SAR image restoration  [ :arrow_down: ](https://arxiv.org/pdf/2101.11852.pdf)
>  The speckle phenomenon remains a major hurdle for the analysis of SAR images. The development of speckle reduction methods closely follows methodological progress in the field of image restoration. The advent of deep neural networks has offered new ways to tackle this longstanding problem. Deep learning for speckle reduction is a very active research topic and already shows restoration performances that exceed that of the previous generations of methods based on the concepts of patches, sparsity, wavelet transform or total variation minimization. The objective of this paper is to give an overview of the most recent works and point the main research directions and current challenges of deep learning for SAR image restoration.      
### 11.Windowed total variation denoising and noise variance monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2101.11850.pdf)
>  We proposed a real time Total-Variation denosing method with an automatic choice of hyper-parameter $\lambda$, and the good performance of this method provides a large application field. In this article, we adapt the developed method to the non stationary signal in using the sliding window, and propose a noise variance monitoring method. The simulated results show that our proposition follows well the variation of noise variance.      
### 12.Chronological age estimation of lateral cephalometric radiographs with deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.11805.pdf)
>  The traditional manual age estimation method is crucial labor based on many kinds of the X-Ray image. Some current studies have shown that lateral cephalometric(LC) images can be used to estimate age. However, these methods are based on manually measuring some image features and making age estimates based on experience or scoring. Therefore, these methods are time-consuming and labor-intensive, and the effect will be affected by subjective opinions. In this work, we propose a saliency map-enhanced age estimation method, which can automatically perform age estimation based on LC images. Meanwhile, it can also show the importance of each region in the image for age estimation, which undoubtedly increases the method's Interpretability. Our method was tested on 3014 LC images from 4 to 40 years old. The MEA of the experimental result is 1.250, which is less than the result of the state-of-the-art benchmark because it performs significantly better in the age group with fewer data. Besides, our model is trained in each area with a high contribution to age estimation in LC images, so the effect of these different areas on the age estimation task was verified. Consequently, we conclude that the proposed saliency map enhancements chronological age estimation method of lateral cephalometric radiographs can work well in chronological age estimation task, especially when the amount of data is small. Besides, compared with traditional deep learning, our method is also interpretable.      
### 13.An Overview of Machine Learning Techniques for Radiowave Propagation Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2101.11760.pdf)
>  We give an overview of recent developments in the modeling of radiowave propagation, based on machine learning algorithms. We identify the input and output specification and the architecture of the model as the main challenges associated with machine learning-driven propagation models. Relevant papers are discussed and categorized based on their approach to each of these challenges. Emphasis is given on presenting the prospects and open problems in this promising and rapidly evolving area.      
### 14.Automated femur segmentation from computed tomography images using a deep neural network  [ :arrow_down: ](https://arxiv.org/pdf/2101.11742.pdf)
>  Osteoporosis is a common bone disease that occurs when the creation of new bone does not keep up with the loss of old bone, resulting in increased fracture risk. Adults over the age of 50 are especially at risk and see their quality of life diminished because of limited mobility, which can lead to isolation and depression. We are developing a robust screening method capable of identifying individuals predisposed to hip fracture to address this clinical challenge. The method uses finite element analysis and relies on segmented computed tomography (CT) images of the hip. Presently, the segmentation of the proximal femur requires manual input, which is a tedious task, prone to human error, and severely limits the practicality of the method in a clinical context. Here we present a novel approach for segmenting the proximal femur that uses a deep convolutional neural network to produce accurate, automated, robust, and fast segmentations of the femur from CT scans. The network architecture is based on the renowned u-net, which consists of a downsampling path to extract increasingly complex features of the input patch and an upsampling path to convert the acquired low resolution image into a high resolution one. Skipped connections allow us to recover critical spatial information lost during downsampling. The model was trained on 30 manually segmented CT images and was evaluated on 200 ground truth manual segmentations. Our method delivers a mean Dice similarity coefficient (DSC) and 95th percentile Hausdorff distance (HD95) of 0.990 and 0.981 mm, respectively.      
### 15.A Multi-Scale Conditional Deep Model for Tumor Cell Ratio Counting  [ :arrow_down: ](https://arxiv.org/pdf/2101.11731.pdf)
>  We propose a method to accurately obtain the ratio of tumor cells over an entire histological slide. We use deep fully convolutional neural network models trained to detect and classify cells on images of H&amp;E-stained tissue sections. Pathologists' labels consisting of exhaustive nuclei locations and tumor regions were used to trained the model in a supervised fashion. We show that combining two models, each working at a different magnification allows the system to capture both cell-level details and surrounding context to enable successful detection and classification of cells as either tumor-cell or normal-cell. Indeed, by conditioning the classification of a single cell on a multi-scale context information, our models mimic the process used by pathologists who assess cell neoplasticity and tumor extent at different microscope magnifications. The ratio of tumor cells can then be readily obtained by counting the number of cells in each class. To analyze an entire slide, we split it into multiple tiles that can be processed in parallel. The overall tumor cell ratio can then be aggregated. We perform experiments on a dataset of 100 slides with lung tumor specimens from both resection and tissue micro-array (TMA). We train fully-convolutional models using heavy data augmentation and batch normalization. On an unseen test set, we obtain an average mean absolute error on predicting the tumor cell ratio of less than 6%, which is significantly better than the human average of 20% and is key in properly selecting tissue samples for recent genetic panel tests geared at prescribing targeted cancer drugs. We perform ablation studies to show the importance of training two models at different magnifications and to justify the choice of some parameters, such as the size of the receptive field.      
### 16.Artificial Intelligence Driven UAV-NOMA-MEC in Next Generation Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.11681.pdf)
>  Driven by the unprecedented high throughput and low latency requirements in next-generation wireless networks, this paper introduces an artificial intelligence (AI) enabled framework in which unmanned aerial vehicles (UAVs) use non-orthogonal multiple access (NOMA) and mobile edge computing (MEC) techniques to service terrestrial mobile users (MUs). The proposed framework enables the terrestrial MUs to offload their computational tasks simultaneously, intelligently, and flexibly, thus enhancing their connectivity as well as reducing their transmission latency and their energy consumption. To this end, the fundamentals of this framework are first introduced. Then, a number of communication and AI techniques are proposed to improve the quality of experiences of terrestrial MUs. To this end, federated learning and reinforcement learning are introduced for intelligent task offloading and computing resource allocation. For each learning technique, motivations, challenges, and representative results are introduced. Finally, several key technical challenges and open research issues of the proposed framework are summarized.      
### 17.High Resolution, Deep Imaging Using Confocal Time-of-flight Diffuse Optical Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2101.11680.pdf)
>  Light scattering by tissue severely limits both how deep beneath the surface one can image, and at what spatial resolution one can obtain from these images. Diffuse optical tomography (DOT) has emerged as one of the most powerful techniques for imaging deep within tissue -- well beyond the conventional $\sim$ 10-15 mean scattering lengths tolerated by ballistic imaging techniques such as confocal and two-photon microscopy. Unfortunately, existing DOT systems are quite limited and achieve only centimeter-scale resolution. Furthermore, they also suffer from slow acquisition times and extremely slow reconstruction speeds making real-time imaging infeasible. We show that time-of-flight diffuse optical tomography (ToF-DOT) and its confocal variant (CToF-DOT), by exploiting the photon travel time information, allow us to achieve millimeter spatial resolution in the highly scattered diffusion regime ($&gt;50$ mean free paths). In addition, we demonstrate that two additional innovations: focusing on confocal measurements, and multiplexing the illumination sources allow us to significantly reduce the scan time to acquire measurements. Finally, we also rely on a novel convolutional approximation that allows us to develop a fast reconstruction algorithm achieving a 100 $\times$ speedup in reconstruction time compared to traditional DOT reconstruction techniques. Together, we believe that these technical advances, serve as the first step towards real-time, millimeter resolution, deep tissue imaging using diffuse optical tomography.      
### 18.Bounds on mutual information of mixture data for classification tasks  [ :arrow_down: ](https://arxiv.org/pdf/2101.11670.pdf)
>  The data for many classification problems, such as pattern and speech recognition, follow mixture distributions. To quantify the optimum performance for classification tasks, the Shannon mutual information is a natural information-theoretic metric, as it is directly related to the probability of error. The mutual information between mixture data and the class label does not have an analytical expression, nor any efficient computational algorithms. We introduce a variational upper bound, a lower bound, and three estimators, all employing pair-wise divergences between mixture components. We compare the new bounds and estimators with Monte Carlo stochastic sampling and bounds derived from entropy bounds. To conclude, we evaluate the performance of the bounds and estimators through numerical simulations.      
### 19.Optimal Utilization Strategy of the LiFePO$_4$ Battery Storage  [ :arrow_down: ](https://arxiv.org/pdf/2101.11659.pdf)
>  The paper provides a comprehensive battery storage modelling approach, which accounts for operation- and degradation-aware characteristics, i.e., variable efficiency, internal resistance growth, and capacity fade. Based on the available experimental data from the literature, we build mixed-integer linear programming compatible lithium iron phosphate (LiFePO$_4$) battery model that can be used in problems related to various applications, i.e., power system, smart grid, and vehicular applications. Such formulation allows finding the globally optimal solution using off-the-shelf academic and commercial solvers. In the numerical study, the proposed modelling approach has been applied to realistic scenarios of peak-shaving, where the importance of considering the developed models is explicitly demonstrated. For instance, a time-varying operation strategy is required to obtain the optimal utilization of the LiFePO$_4$ battery storage. Particularly, during the battery operational lifetime its optimal average SoC may change by up to $20\%$, while the duration of charging process may increase by $75\%$. Finally, using the same LiFePO$_4$ benchmark model from the literature, we compare the results of using the proposed approach to the state-of-the-art in the optimal sizing and scheduling problems. The proposed approach led to a $12.1\%$ reduction of battery investment and operating costs compared to the state-of-the-art method.      
### 20.Easy-GT: Open-Source Software to Facilitate Making the Ground Truth for White Blood Cells Nucleus  [ :arrow_down: ](https://arxiv.org/pdf/2101.11654.pdf)
>  The nucleus of white blood cells (WBCs) plays a significant role in their detection and classification. Appropriate feature extraction of the nucleus is necessary to fit a suitable artificial intelligence model to classify WBCs. Therefore, designing a method is needed to segment the nucleus accurately. The detected nuclei should be compared with the ground truths identified by a hematologist to obtain a proper performance evaluation of the nucleus segmentation method. It is a time-consuming and tedious task for experts to establish the ground truth manually. This paper presents an intelligent open-source software called Easy-GT to create the ground truth of WBCs nucleus faster and easier. This software first detects the nucleus by employing a new otsus thresholding based method with a dice similarity coefficient (DSC) of 95.42 %; the hematologist can then create a more accurate ground truth, using the designed buttons to modify the threshold value. This software can speed up ground truths forming process more than six times.      
### 21.Steady-State Models of STATCOM and UPFC using Flexible Holomorphic Embedding  [ :arrow_down: ](https://arxiv.org/pdf/2101.11613.pdf)
>  To investigate the effect and ability of FACTS devices using the Fast and Flexible Holomorphic Embedding technique (FFHE), it is necessary to develop an embedded system for these devices. Therefore, this paper presents FFHE based embedded system for STATCOM and UPFC. The embedded system is also proposed for their controlling modes. The introduced embedded system for STATCOM and UPFC is flexible which allows to take any state as an initial guess instead of fixed state, which leads towards the reduced runtime and decreases the required number of terms, as compared to the standard Holomorphic Embedded Load-Flow method (HELM). To demonstrate the effectiveness and practicability, the proposed models of STATCOM and UPFC have been tested for several cases. Further, the developed recursive formulas for power balance equations, devices' physical constraints, and their controlling modes are thoroughly investigated and examined. From several tests, it is found that the proposed embedded system requires less execution time and reduces the error at higher rate.      
### 22.Adversarial Attacks on Deep Learning Based Power Allocation in a Massive MIMO Network  [ :arrow_down: ](https://arxiv.org/pdf/2101.12090.pdf)
>  Deep learning (DL) is becoming popular as a new tool for many applications in wireless communication systems. However, for many classification tasks (e.g., modulation classification) it has been shown that DL-based wireless systems are susceptible to adversarial examples; adversarial examples are well-crafted malicious inputs to the neural network (NN) with the objective to cause erroneous outputs. In this paper, we extend this to regression problems and show that adversarial attacks can break DL-based power allocation in the downlink of a massive multiple-input-multiple-output (maMIMO) network. Specifically, we extend the fast gradient sign method (FGSM), momentum iterative FGSM, and projected gradient descent adversarial attacks in the context of power allocation in a maMIMO system. We benchmark the performance of these attacks and show that with a small perturbation in the input of the NN, the white-box attacks can result in infeasible solutions up to 86%. Furthermore, we investigate the performance of black-box attacks. All the evaluations conducted in this work are based on an open dataset and NN models, which are publicly available.      
### 23.Modeling Spatial Nonstationarity via Deformable Convolutions for Deep Traffic Flow Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2101.12010.pdf)
>  Deep neural networks are being increasingly used for short-term traffic flow prediction. Existing convolution-based approaches typically partition an underlying territory into grid-like spatial units, and employ standard convolutions to learn spatial dependence among the units. However, standard convolutions with fixed geometric structures cannot fully model the nonstationary characteristics of local traffic flows. To overcome the deficiency, we introduce deformable convolution that augments the spatial sampling locations with additional offsets, to enhance the modeling capability of spatial nonstationarity. On this basis, we design a deep deformable convolutional residual network, namely DeFlow-Net, that can effectively model global spatial dependence, local spatial nonstationarity, and temporal periodicity of traffic flows. Furthermore, to fit better with convolutions, we suggest to first aggregate traffic flows according to pre-conceived regions of interest, then dispose to sequentially organized raster images for network input. Extensive experiments on real-world traffic flows demonstrate that DeFlow-Net outperforms existing solutions using standard convolutions, and spatial partition by pre-conceived regions further enhances the performance. Finally, we demonstrate the advantage of DeFlow-Net in maintaining spatial autocorrelation, and reveal the impacts of partition shapes and scales on deep traffic flow prediction.      
### 24.A Machine Learning Challenge for Prognostic Modelling in Head and Neck Cancer Using Multi-modal Data  [ :arrow_down: ](https://arxiv.org/pdf/2101.11935.pdf)
>  Accurate prognosis for an individual patient is a key component of precision oncology. Recent advances in machine learning have enabled the development of models using a wider range of data, including imaging. Radiomics aims to extract quantitative predictive and prognostic biomarkers from routine medical imaging, but evidence for computed tomography radiomics for prognosis remains inconclusive. We have conducted an institutional machine learning challenge to develop an accurate model for overall survival prediction in head and neck cancer using clinical data etxracted from electronic medical records and pre-treatment radiological images, as well as to evaluate the true added benefit of radiomics for head and neck cancer prognosis. Using a large, retrospective dataset of 2,552 patients and a rigorous evaluation framework, we compared 12 different submissions using imaging and clinical data, separately or in combination. The winning approach used non-linear, multitask learning on clinical data and tumour volume, achieving high prognostic accuracy for 2-year and lifetime survival prediction and outperforming models relying on clinical data only, engineered radiomics and deep learning. Combining all submissions in an ensemble model resulted in improved accuracy, with the highest gain from a image-based deep learning model. Our results show the potential of machine learning and simple, informative prognostic factors in combination with large datasets as a tool to guide personalized cancer care.      
### 25.Joint Transmission Scheme and Coded Content Placement in Cluster-centric UAV-aided Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.11787.pdf)
>  Recently, as a consequence of the COVID-19 pandemic, dependence on telecommunication for remote learning/working and telemedicine has significantly increased. In this context, preserving high Quality of Service (QoS) and maintaining low latency communication are of paramount importance. Development of an Unmanned Aerial Vehicles (UAV)-aided heterogeneous cellular network is a promising solution to satisfy the aforementioned requirements. There are, however, key challenges ahead, on the one hand, it is challenging to optimally increase content diversity in caching nodes to mitigate the network's traffic over the backhaul. On the other hand is the challenge of attenuated UAVs' signal in indoor environments, which increases users' access delay and UAVs' energy consumption. To address these challenges, we incorporate UAVs, as mobile caching nodes, together with Femto Access points (FAPs) to increase the network's coverage in both indoor and outdoor environments. Referred to as the Cluster-centric and Coded UAV-aided Femtocaching (CCUF) framework, a two-phase clustering framework is proposed for optimal FAPs' formation and UAVs' deployment. The proposed CCUF leads to an increase in the cache diversity, a reduction in the users' access delay, and significant reduction in UAVs' energy consumption. To mitigate the inter-cell interference in edge areas, the Coordinated Multi-Point (CoMP) approach is integrated within the CCUF framework. In contrary to existing works, we analytically compute the optimal number of FAPs in each cluster to increase the cache-hit probability of coded content placement. Furthermore, the optimal number of coded contents to be stored in each caching node is computed to increase the cache-hit-ratio, Signal-to-Interference-plus-Noise Ratio (SINR), and cache diversity and decrease the users' access delay and cache redundancy for different content popularity profiles.      
### 26.Assessing the applicability of Deep Learning-based visible-infrared fusion methods for fire imagery  [ :arrow_down: ](https://arxiv.org/pdf/2101.11745.pdf)
>  Early wildfire detection is of paramount importance to avoid as much damage as possible to the environment, properties, and lives. Deep Learning (DL) models that can leverage both visible and infrared information have the potential to display state-of-the-art performance, with lower false-positive rates than existing techniques. However, most DL-based image fusion methods have not been evaluated in the domain of fire imagery. Additionally, to the best of our knowledge, no publicly available dataset contains visible-infrared fused fire images. There is a growing interest in DL-based image fusion techniques due to their reduced complexity. Due to the latter, we select three state-of-the-art, DL-based image fusion techniques and evaluate them for the specific task of fire image fusion. We compare the performance of these methods on selected metrics. Finally, we also present an extension to one of the said methods, that we called FIRe-GAN, that improves the generation of artificial infrared images and fused ones on selected metrics.      
### 27.Convergence Analysis of Fixed Point Chance Constrained Optimal Power Flow Problems  [ :arrow_down: ](https://arxiv.org/pdf/2101.11740.pdf)
>  For optimal power flow problems with chance constraints, a particularly effective method is based on a fixed point iteration applied to a sequence of deterministic power flow problems. However, a priori, the convergence of such an approach is not necessarily guaranteed. This article analyses the convergence conditions for this fixed point approach, and reports numerical experiments including for large IEEE networks.      
### 28.A Case Study of Deep Learning Based Multi-Modal Methods for Predicting the Age-Suitability Rating of Movie Trailers  [ :arrow_down: ](https://arxiv.org/pdf/2101.11704.pdf)
>  In this work, we explore different approaches to combine modalities for the problem of automated age-suitability rating of movie trailers. First, we introduce a new dataset containing videos of movie trailers in English downloaded from IMDB and YouTube, along with their corresponding age-suitability rating labels. Secondly, we propose a multi-modal deep learning pipeline addressing the movie trailer age suitability rating problem. This is the first attempt to combine video, audio, and speech information for this problem, and our experimental results show that multi-modal approaches significantly outperform the best mono and bimodal models in this task.      
### 29.Robust and Efficient Single-Pixel Image Classificationwith Nonlinear Optics  [ :arrow_down: ](https://arxiv.org/pdf/2101.11696.pdf)
>  We present a hybrid image classifier by mode-selective image upconversion, single pixel photodetection, and deep learning, aiming at fast processing a large number of pixels. It utilizes partial Fourier transform to extract the signature features of images in both the original and Fourier domains, thereby significantly increasing the classification accuracy and robustness. Tested on the MNIST handwritten digit images, it boosts the accuracy from 81.25% to 99.23%, and achieves an 83% accuracy for highly contaminated images whose signal-to-noise ratio is only -17 dB. Our approach could prove useful for fast lidar data processing, high resolution image recognition, occluded target identification, atmosphere monitoring, and so on.      
### 30.Hadamard Powers and the Identification of Mixtures of Products  [ :arrow_down: ](https://arxiv.org/pdf/2101.11688.pdf)
>  The Hadamard Power of a matrix is the matrix consisting of all Hadamard products of subsets of its rows. We obtain several results concerning when a Hadamard Power has full column rank. This question in turn is central to the following problem: given a mixture of $k$ product distributions on a list of binary random variables $X_1,\ldots,X_n$, can the probability model be identified from the joint statistics of the $X_i$.      
### 31.G-MIND: An End-to-End Multimodal Imaging-Genetics Framework for Biomarker Identification and Disease Classification  [ :arrow_down: ](https://arxiv.org/pdf/2101.11656.pdf)
>  We propose a novel deep neural network architecture to integrate imaging and genetics data, as guided by diagnosis, that provides interpretable biomarkers. Our model consists of an encoder, a decoder and a classifier. The encoder learns a non-linear subspace shared between the input data modalities. The classifier and the decoder act as regularizers to ensure that the low-dimensional encoding captures predictive differences between patients and controls. We use a learnable dropout layer to extract interpretable biomarkers from the data, and our unique training strategy can easily accommodate missing data modalities across subjects. We have evaluated our model on a population study of schizophrenia that includes two functional MRI (fMRI) paradigms and Single Nucleotide Polymorphism (SNP) data. Using 10-fold cross validation, we demonstrate that our model achieves better classification accuracy than baseline methods, and that this performance generalizes to a second dataset collected at a different site. In an exploratory analysis we further show that the biomarkers identified by our model are closely associated with the well-documented deficits in schizophrenia.      
