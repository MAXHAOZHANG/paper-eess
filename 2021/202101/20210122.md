# ArXiv eess --Fri, 22 Jan 2021
### 1.Expectation-Maximization Regularized DeepLearning for Weakly Supervised Tumor Segmentation for Glioblastoma  [ :arrow_down: ](https://arxiv.org/pdf/2101.08757.pdf)
>  We present an Expectation-Maximization (EM) Regularized Deep Learning (EMReDL) model for the weakly supervised tumor segmentation. The proposed framework was tailored to glioblastoma, a type of malignant tumor characterized by its diffuse infiltration into the surrounding brain tissue, which poses significant challenge to treatment target and tumor burden estimation based on conventional structural MRI. Although physiological MRI can provide more specific information regarding tumor infiltration, the relatively low resolution hinders a precise full annotation. This has motivated us to develop a weakly supervised deep learning solution that exploits the partial labelled tumor regions. <br>EMReDL contains two components: a physiological prior prediction model and EM-regularized segmentation model. The physiological prior prediction model exploits the physiological MRI by training a classifier to generate a physiological prior map. This map was passed to the segmentation model for regularization using the EM algorithm. We evaluated the model on a glioblastoma dataset with the available pre-operative multiparametric MRI and recurrence MRI. EMReDL was shown to effectively segment the infiltrated tumor from the partially labelled region of potential infiltration. The segmented core and infiltrated tumor showed high consistency with the tumor burden labelled by experts. The performance comparison showed that EMReDL achieved higher accuracy than published state-of-the-art models. On MR spectroscopy, the segmented region showed more aggressive features than other partial labelled region. The proposed model can be generalized to other segmentation tasks with partial labels, with the CNN architecture flexible in the framework.      
### 2.Graph-based many-to-one dynamic ride-matching for shared mobility services in congested networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.08657.pdf)
>  On-demand shared mobility systems require matching of one (one-to-one) or multiple riders (many- to-one) to a vehicle based on real-time information. We propose a novel graph-based algorithm (GMO- Match) for dynamic many-to-one matching problem in the presence of traffic congestion. The proposed algorithm, which is an iterative two-step method, provides high service quality and is efficient in terms of computational complexity. GMOMatch starts with a one-to-one matching in step 1 and is followed by solving a maximum weight matching problem is step 2 to combine the travel requests. To evaluate the performance of the proposed algorithm, it is compared with a ride-matching algorithm by IBM (Simonetto et al., 2019). Both algorithms are implemented in a micro-traffic simulator to assess their performance and also their impacts on traffic congestion. Downtown Toronto road network is chosen as the study area. In comparison to IBM algorithm, GMOMatch improves the service quality and traffic travel time by 32% and 4%, respectively. A sensitivity analysis is also conducted over different parameters to show their impacts on the service quality.      
### 3.Noisy-target Training: A Training Strategy for DNN-based Speech Enhancement without Clean Speech  [ :arrow_down: ](https://arxiv.org/pdf/2101.08625.pdf)
>  Deep neural network (DNN)-based speech enhancement ordinarily requires clean speech signals as the training target. However, collecting clean signals is very costly because they must be recorded in a studio. This requirement currently restricts the amount of training data for speech enhancement less than 1/1000 of that of speech recognition which does not need clean signals. Increasing the amount of training data is important for improving the performance, and hence the requirement of clean signals should be relaxed. In this paper, we propose a training strategy that does not require clean signals. The proposed method only utilizes noisy signals for training, which enables us to use a variety of speech signals in the wild. Our experimental results showed that the proposed method can achieve the performance similar to that of a DNN trained with clean signals.      
### 4.Path Loss Modeling and Measurements for Reconfigurable Intelligent Surfaces in the Millimeter-Wave Frequency Band  [ :arrow_down: ](https://arxiv.org/pdf/2101.08607.pdf)
>  Reconfigurable intelligent surfaces (RISs) provide an interface between the electromagnetic world of the wireless propagation environment and the digital world of information science. Simple yet sufficiently accurate path loss models for RISs are an important basis for theoretical analysis and optimization of RIS-assisted wireless communication systems. In this paper, we refine our previously proposed free-space path loss model for RISs to make it simpler, more applicable, and easier to use. In the proposed path loss model, the impact of the radiation patterns of the antennas and unit cells of the RIS is formulated in terms of an angle-dependent loss factor. The refined model gives more accurate estimates of the path loss of RISs comprised of unit cells with a deep sub-wavelength size. The free-space path loss model of the sub-channel provided by a single unit cell is also explicitly provided. In addition, two fabricated RISs, which are designed to operate in the millimeter-wave (mmWave) band, are utilized to carry out a measurement campaign in order to characterize and validate the proposed path loss model for RIS-assisted wireless communications. The measurement results corroborate the proposed analytical model. The proposed refined path loss model for RISs reveals that the reflecting capability of a single unit cell is proportional to its physical aperture and to an angle-dependent factor. In particular, the far-field beamforming gain provided by an RIS is mainly determined by the total area of the surface and by the angles of incidence and reflection.      
### 5.Monitoring nonstationary processes based on recursive cointegration analysis and elastic weight consolidation  [ :arrow_down: ](https://arxiv.org/pdf/2101.08579.pdf)
>  This paper considers the problem of nonstationary process monitoring under frequently varying operating conditions. Traditional approaches generally misidentify the normal dynamic deviations as faults and thus lead to high false alarms. Besides, they generally consider single relatively steady operating condition and suffer from the catastrophic forgetting issue when learning successive operating conditions. In this paper, recursive cointegration analysis (RCA) is first proposed to distinguish the real faults from normal systems changes, where the model is updated once a new normal sample arrives and can adapt to slow change of cointegration relationship. Based on the long-term equilibrium information extracted by RCA, the remaining short-term dynamic information is monitored by recursive principal component analysis (RPCA). Thus a comprehensive monitoring framework is built. When the system enters a new operating condition, the RCA-RPCA model is rebuilt to deal with the new condition. Meanwhile, elastic weight consolidation (EWC) is employed to settle the `catastrophic forgetting' issue inherent in RPCA, where significant information of influential parameters is enhanced to avoid the abrupt performance degradation for similar modes. The effectiveness of the proposed method is illustrated by a practical industrial system.      
### 6.GhostSR: Learning Ghost Features for Efficient Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2101.08525.pdf)
>  Modern single image super-resolution (SISR) system based on convolutional neural networks (CNNs) achieves fancy performance while requires huge computational costs. The problem on feature redundancy is well studied in visual recognition task, but rarely discussed in SISR. Based on the observation that many features in SISR models are also similar to each other, we propose to use shift operation to generate the redundant features (i.e., Ghost features). Compared with depth-wise convolution which is not friendly to GPUs or NPUs, shift operation can bring practical inference acceleration for CNNs on common hardware. We analyze the benefits of shift operation for SISR and make the shift orientation learnable based on Gumbel-Softmax trick. For a given pre-trained model, we first cluster all filters in each convolutional layer to identify the intrinsic ones for generating intrinsic features. Ghost features will be derived by moving these intrinsic features along a specific orientation. The complete output features are constructed by concatenating the intrinsic and ghost features together. Extensive experiments on several benchmark models and datasets demonstrate that both the non-compact and lightweight SISR models embedded in our proposed module can achieve comparable performance to that of their baselines with large reduction of parameters, FLOPs and GPU latency. For instance, we reduce the parameters by 47%, FLOPs by 46% and GPU latency by 41% of EDSR x2 network without significant performance degradation.      
### 7.Weighted Fuzzy-Based PSNR for Watermarking  [ :arrow_down: ](https://arxiv.org/pdf/2101.08502.pdf)
>  One of the problems of conventional visual quality evaluation criteria such as PSNR and MSE is the lack of appropriate standards based on the human visual system (HVS). They are calculated based on the difference of the corresponding pixels in the original and manipulated image. Hence, they practically do not provide a correct understanding of the image quality. Watermarking is an image processing application in which the image's visual quality is an essential criterion for its evaluation. Watermarking requires a criterion based on the HVS that provides more accurate values than conventional measures such as PSNR. This paper proposes a weighted fuzzy-based criterion that tries to find essential parts of an image based on the HVS. Then these parts will have larger weights in computing the final value of PSNR. We compare our results against standard PSNR, and our experiments show considerable consequences.      
### 8.A Study of F0 Modification for X-Vector Based Speech Pseudonymization Across Gender  [ :arrow_down: ](https://arxiv.org/pdf/2101.08478.pdf)
>  Speech pseudonymization aims at altering a speech signal to map the identifiable personal characteristics of a given speaker to another identity. In other words, it aims to hide the source speaker identity while preserving the intelligibility of the spoken content. This study takes place in the VoicePrivacy 2020 challenge framework, where the baseline system performs pseudonymization by modifying x-vector information to match a target speaker while keeping the fundamental frequency (F0) unchanged. We propose to alter other paralin-guistic features, here F0, and analyze the impact of this modification across gender. We found that the proposed F0 modification always improves pseudonymization We observed that both source and target speaker genders affect the performance gain when modifying the F0.      
### 9.Arabic Speech Recognition by End-to-End, Modular Systems and Human  [ :arrow_down: ](https://arxiv.org/pdf/2101.08454.pdf)
>  Recent advances in automatic speech recognition (ASR) have achieved accuracy levels comparable to human transcribers, which led researchers to debate if the machine has reached human performance. Previous work focused on the English language and modular hidden Markov model-deep neural network (HMM-DNN) systems. In this paper, we perform a comprehensive benchmarking for end-to-end transformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the Arabic language and its dialects. For the HSR, we evaluate linguist performance and lay-native speaker performance on a new dataset collected as a part of this study. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new performance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our results suggest that human performance in the Arabic language is still considerably better than the machine with an absolute WER gap of 3.6% on average.      
### 10.Privacy-Preserving Distributed Optimal Power Flow with Partially Homomorphic Encryption  [ :arrow_down: ](https://arxiv.org/pdf/2101.08395.pdf)
>  Distribution grid agents are obliged to exchange and disclose their states explicitly to neighboring regions to enable distributed optimal power flow dispatch. However, the states contain sensitive information of individual agents, such as voltage and current measurements. These measurements can be inferred by adversaries, such as other participating agents or eavesdroppers. To address the issue, we propose a privacy-preserving distributed optimal power flow (OPF) algorithm based on partially homomorphic encryption (PHE). First of all, we exploit the alternating direction method of multipliers (ADMM) to solve the OPF in a distributed fashion. In this way, the dual update of ADMM can be encrypted by PHE. We further relax the augmented term of the primal update of ADMM with the $\ell_1$-norm regularization. In addition, we transform the relaxed ADMM with the $\ell_1$-norm regularization to a semidefinite program (SDP), and prove that this transformation is exact. The SDP can be solved locally with only the sign messages from neighboring agents, which preserves the privacy of the primal update. At last, we strictly prove the privacy preservation guarantee of the proposed algorithm. Numerical case studies validate the effectiveness and exactness of the proposed approach.      
### 11.Learning Ultrasound Rendering from Cross-Sectional Model Slices for Simulated Training  [ :arrow_down: ](https://arxiv.org/pdf/2101.08339.pdf)
>  Purpose. Given the high level of expertise required for navigation and interpretation of ultrasound images, computational simulations can facilitate the training of such skills in virtual reality. With ray-tracing based simulations, realistic ultrasound images can be generated. However, due to computational constraints for interactivity, image quality typically needs to be compromised. <br>Methods. We propose herein to bypass any rendering and simulation process at interactive time, by conducting such simulations during a non-time-critical offline stage and then learning image translation from cross-sectional model slices to such simulated frames. We use a generative adversarial framework with a dedicated generator architecture and input feeding scheme, which both substantially improve image quality without increase in network parameters. Integral attenuation maps derived from cross-sectional model slices, texture-friendly strided convolutions, providing stochastic noise and input maps to intermediate layers in order to preserve locality are all shown herein to greatly facilitate such translation task. <br>Results. Given several quality metrics, the proposed method with only tissue maps as input is shown to provide comparable or superior results to a state-of-the-art that uses additional images of low-quality ultrasound renderings. An extensive ablation study shows the need and benefits from the individual contributions utilized in this work, based on qualitative examples and quantitative ultrasound similarity metrics. To that end, a local histogram statistics based error metric is proposed and demonstrated for visualization of local dissimilarities between ultrasound images.      
### 12.Chest X-ray lung and heart segmentation based on minimal training sets  [ :arrow_down: ](https://arxiv.org/pdf/2101.08309.pdf)
>  As the COVID-19 pandemic aggravated the excessive workload of doctors globally, the demand for computer aided methods in medical imaging analysis increased even further. Such tools can result in more robust diagnostic pipelines which are less prone to human errors. In our paper, we present a deep neural network to which we refer to as Attention BCDU-Net, and apply it to the task of lung and heart segmentation from chest X-ray (CXR) images, a basic but ardous step in the diagnostic pipeline, for instance for the detection of cardiomegaly. We show that the fine-tuned model exceeds previous state-of-the-art results, reaching $98.1\pm 0.1\%$ Dice score and $95.2\pm 0.1\%$ IoU score on the dataset of Japanese Society of Radiological Technology (JSRT). Besides that, we demonstrate the relative simplicity of the task by attaining surprisingly strong results with training sets of size 10 and 20: in terms of Dice score, $97.0\pm 0.8\%$ and $97.3\pm 0.5$, respectively, while in terms of IoU score, $92.2\pm 1.2\%$ and $93.3\pm 0.4\%$, respectively. To achieve these scores, we capitalize on the mixup augmentation technique, which yields a remarkable gain above $4\%$ IoU score in the size 10 setup.      
### 13.Run-Time Safety Monitoring of Neural-Network-Enabled Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.08297.pdf)
>  Complex dynamical systems rely on the correct deployment and operation of numerous components, with state-of-the-art methods relying on learning-enabled components in various stages of modeling, sensing, and control at both offline and online levels. This paper addresses the run-time safety monitoring problem of dynamical systems embedded with neural network components. A run-time safety state estimator in the form of an interval observer is developed to construct lower-bound and upper-bound of system state trajectories in run time. The developed run-time safety state estimator consists of two auxiliary neural networks derived from the neural network embedded in dynamical systems, and observer gains to ensure the positivity, namely the ability of estimator to bound the system state in run time, and the convergence of the corresponding error dynamics. The design procedure is formulated in terms of a family of linear programming feasibility problems. The developed method is illustrated by a numerical example and is validated with evaluations on an adaptive cruise control system.      
### 14.Acceleration Measurement Enhances the Bandwidth of Disturbance Observer in Motion Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.08259.pdf)
>  The trade-off between the noise-sensitivity and the performance of disturbance estimation is well-known in the Disturbance Observer- (DOb-) based motion control systems. As the bandwidth of the DOb increases, not only the performance but also the frequency range of disturbance estimation improves yet the motion controller becomes more sensitive to the noise of measurement system. This trade-off is generally explained by considering only the noise of sensors such as encoders. However, the digital implementation of the robust motion controller may significantly influence the noise sensitivity and performance of disturbance estimation in practice. This paper shows that the conventional DOb implemented by estimating velocity is subject to waterbed effect when the design parameters (i.e., sampling-time, nominal plant parameters and the bandwidth of the DOb) are not properly tuned in the digital motion controller synthesis. Therefore, the bandwidth of disturbance estimation is limited by waterbed effect in addition to the noise of velocity measurement system. To facilitate the digital motion controller synthesis, the design constraints of the conventional DOb are analytically derived in this paper. When the digital motion controller is implemented by estimating acceleration, waterbed effect does not occur, and good robust stability and performance can be achieved for all values of the design parameters of the acceleration measurement-based DOb. The bandwidth of disturbance estimation, however, cannot be freely increased due to the noise of acceleration sensors in practice. By employing Bode Integral Theorem in the discrete-time domain, the design constraints of the DOb-based digital motion control systems are clearly explained and it is shown that acceleration measurement can be used to enhance the bandwidth of the DOb, i.e., the performance and frequency range of disturbance estimation.      
### 15.Model-based Policy Search for Partially Measurable Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.08740.pdf)
>  In this paper, we propose a Model-Based Reinforcement Learning (MBRL) algorithm for Partially Measurable Systems (PMS), i.e., systems where the state can not be directly measured, but must be estimated through proper state observers. The proposed algorithm, named Monte Carlo Probabilistic Inference for Learning COntrol for Partially Measurable Systems (MC-PILCO4PMS), relies on Gaussian Processes (GPs) to model the system dynamics, and on a Monte Carlo approach to update the policy parameters. W.r.t. previous GP-based MBRL algorithms, MC-PILCO4PMS models explicitly the presence of state observers during policy optimization, allowing to deal PMS. The effectiveness of the proposed algorithm has been tested both in simulation and in two real systems.      
### 16.Multi-robot energy autonomy with wind and constrained resources  [ :arrow_down: ](https://arxiv.org/pdf/2101.08697.pdf)
>  One aspect of the ever-growing need for long term autonomy of multi-robot systems, is ensuring energy sufficiency. In particular, in scenarios where charging facilities are limited, battery-powered robots need to coordinate to share access. In this work we extend previous results by considering robots that carry out a generic mission while sharing a single charging station, while being affected by air drag and wind fields. Our mission-agnostic framework based on control barrier functions (CBFs) ensures energy sufficiency (i.e., maintaining all robots above a certain voltage threshold) and proper coordination (i.e., ensuring mutually exclusive use of the available charging station). Moreover, we investigate the feasibility requirements of the system in relation to individual robots' properties, as well as air drag and wind effects. We show simulation results that demonstrate the effectiveness of the proposed framework.      
### 17.Data Processing for Short-Term Solar Irradiance Forecasting using Ground-Based Infrared Images  [ :arrow_down: ](https://arxiv.org/pdf/2101.08694.pdf)
>  The generation of energy in a power grid which uses Photovoltaic (PV) systems depends on the projection of shadows from moving clouds in the Troposphere. This investigation proposes an efficient method of data processing for the statistical quantification of cloud features using long-wave infrared (IR) images and Global Solar Irradiance (GSI) measurements. The IR images are obtained using a data acquisition system (DAQ) mounted on a solar tracker. We explain how to remove cyclostationary biases in GSI measurements. Seasonal trends are removed from the GSI time series, using the theoretical GSI to obtain the Clear-Sky Index (CSI) time series. We introduce an atmospheric model to remove from IR images both the effect of atmosphere scatter irradiance and the effect of the Sun's direct irradiance. Scattering is produced by water spots and dust particles on the germanium lens of the enclosure. We explain how to remove the scattering effect produced by the germanium lens attached to the DAQ enclosure window of the IR camera. An atmospheric condition model classifies the sky-conditions in four different categories: clear-sky, cumulus, stratus and nimbus. When an IR image is classified in the category of clear-sky, it is used to model the scattering effect of the germanium lens.      
### 18.Streaming from the Air: Enabling High Data-rate 5G Cellular Links for Drone Streaming Applications  [ :arrow_down: ](https://arxiv.org/pdf/2101.08681.pdf)
>  Enabling high data-rate uplink cellular connectivity for drones is a challenging problem, since a flying drone has a higher likelihood of having line-of-sight propagation to base stations that terrestrial UEs normally do not have line-of-sight to. This may result in uplink inter-cell interference and uplink performance degradation for the neighboring ground UEs when drones transmit at high data-rates (e.g., video streaming). We address this problem from a cellular operator's standpoint to support drone-sourced video streaming of a point of interest. We propose a low-complexity, closed-loop control system for Open-RAN architectures that jointly optimizes the drone's location in space and its transmission directionality to support video streaming and minimize its uplink interference impact on the network. We prototype and experimentally evaluate the proposed control system on an outdoor multi-cell RAN testbed. Furthermore, we perform a large-scale simulation assessment of the proposed system on the actual cell deployment topologies and cell load profiles of a major US cellular carrier. The proposed Open-RAN-based control achieves an average 19% network capacity gain over traditional BS-constrained control solutions and satisfies the application data-rate requirements of the drone (e.g., to stream an HD video).      
### 19.Regularization via deep generative models: an analysis point of view  [ :arrow_down: ](https://arxiv.org/pdf/2101.08661.pdf)
>  This paper proposes a new way of regularizing an inverse problem in imaging (e.g., deblurring or inpainting) by means of a deep generative neural network. Compared to end-to-end models, such approaches seem particularly interesting since the same network can be used for many different problems and experimental conditions, as soon as the generative model is suited to the data. Previous works proposed to use a synthesis framework, where the estimation is performed on the latent vector, the solution being obtained afterwards via the decoder. Instead, we propose an analysis formulation where we directly optimize the image itself and penalize the latent vector. We illustrate the interest of such a formulation by running experiments of inpainting, deblurring and super-resolution. In many cases our technique achieves a clear improvement of the performance and seems to be more robust, in particular with respect to initialization.      
### 20.LEAF: A Learnable Frontend for Audio Classification  [ :arrow_down: ](https://arxiv.org/pdf/2101.08596.pdf)
>  Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.      
### 21.A Joint Diagonalization Based Efficient Approach to Underdetermined Blind Audio Source Separation Using the Multichannel Wiener Filter  [ :arrow_down: ](https://arxiv.org/pdf/2101.08563.pdf)
>  This paper presents a computationally efficient approach to blind source separation (BSS) of audio signals, applicable even when there are more sources than microphones (i.e., the underdetermined case). When there are as many sources as microphones (i.e., the determined case), BSS can be performed computationally efficiently by independent component analysis (ICA). Unfortunately, however, ICA is basically inapplicable to the underdetermined case. Another BSS approach using the multichannel Wiener filter (MWF) is applicable even to this case, and encompasses full-rank spatial covariance analysis (FCA) and multichannel non-negative matrix factorization (MNMF). However, these methods require massive numbers of matrix inversions to design the MWF, and are thus computationally inefficient. To overcome this drawback, we exploit the well-known property of diagonal matrices that matrix inversion amounts to mere inversion of the diagonal elements and can thus be performed computationally efficiently. This makes it possible to drastically reduce the computational cost of the above matrix inversions based on a joint diagonalization (JD) idea, leading to computationally efficient BSS. Specifically, we restrict the N spatial covariance matrices (SCMs) of all N sources to a class of (exactly) jointly diagonalizable matrices. Based on this approach, we present FastFCA, a computationally efficient extension of FCA. We also present a unified framework for underdetermined and determined audio BSS, which highlights a theoretical connection between FastFCA and other methods. Moreover, we reveal that FastFCA can be regarded as a regularized version of approximate joint diagonalization (AJD).      
### 22.Boosting in Univariate Nonparametric Maximum Likelihood Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2101.08505.pdf)
>  Nonparametric maximum likelihood estimation is intended to infer the unknown density distribution while making as few assumptions as possible. To alleviate the over parameterization in nonparametric data fitting, smoothing assumptions are usually merged into the estimation. In this paper a novel boosting-based method is introduced to the nonparametric estimation in univariate cases. We deduce the boosting algorithm by the second-order approximation of nonparametric log-likelihood. Gaussian kernel and smooth spline are chosen as weak learners in boosting to satisfy the smoothing assumptions. Simulations and real data experiments demonstrate the efficacy of the proposed approach.      
### 23.Effect of Window Size for Detection of Abnormalities in Respiratory Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2101.08495.pdf)
>  The recording of respiratory sounds was of significant benefit in the diagnosis of abnormalities in respiratory sounds. The duration of the sounds used in the diagnosis affects the speed of the diagnosis. In this study, the effect of window size on diagnosis of abnormalities in respiratory sounds and the most efficient recording time for diagnosis were analyzed. First, window size was applied to each sound in the data set consisting of normal and abnormal breathing sounds, 0.5 second and from 1 to 20 seconds Increased by 1 second. Then, the data applied to window size was inferred feature extraction with Mel Frequency Cepstral Coefficient (MFCC) and the performance of each window was calculated using the leave one-out classifier and the k-nearest neighbor (KNN) algorithm. As a result, it was determined that the data was significant with an average performance of 92.06% in the records between 2 and 10 seconds.      
### 24.Online End-to-End Neural Diarization Handling Overlapping Speech and Flexible Numbers of Speakers  [ :arrow_down: ](https://arxiv.org/pdf/2101.08473.pdf)
>  This paper proposes an online end-to-end diarization that can handle overlapping speech and flexible numbers of speakers. The end-to-end neural speaker diarization (EEND) model has already achieved significant improvement when compared with conventional clustering-based methods. However, the original EEND has two limitations: i) EEND does not perform well in online scenarios; ii) the number of speakers must be fixed in advance. This paper solves both problems by applying a modified extension of the speaker-tracing buffer method that deals with variable numbers of speakers. Experiments on CALLHOME and DIHARD II datasets show that the proposed online method achieves comparable performance to the offline EEND method. Compared with the state-of-the-art online method based on a fully supervised approach (UIS-RNN), the proposed method shows better performance on the DIHARD II dataset.      
### 25.Hybrid Beamforming for Terahertz Wireless Communications: Challenges, Architectures, and Open Problems  [ :arrow_down: ](https://arxiv.org/pdf/2101.08469.pdf)
>  Terahertz (THz) communications are regarded as a pillar technology for the sixth generation (6G) wireless systems, by offering multi-ten-GHz bandwidth. To overcome the short transmission distance and huge propagation loss, ultra-massive (UM) MIMO systems that employ sub-millimeter wavelength antennas array are proposed to enable an enticingly high array gain. In the UM-MIMO systems, hybrid beamforming stands out for its great potential in promisingly high data rate and reduced power consumption. In this paper, challenges and features of the THz hybrid beamforming design are investigated, in light of the distinctive THz peculiarities. Specifically, we demonstrate that the spatial degree-of-freedom (SDoF) is less than 5, which is caused by the extreme sparsity of the THz channel. The blockage problem caused by the huge reflection and scattering losses, as high as 15 dB or over, is studied. Moreover, we analyze the challenges led by the array containing 1024 or more antennas, including the requirement for intelligent subarray architecture, strict energy efficiency, and propagation characterization based on spherical-wave propagation mechanisms. Owning up to hundreds of GHz bandwidth, beam squint effect could cause over 5~dB array gain loss, when the fractional bandwidth exceeds 10%. Inspired by these facts, three novel THz-specific hybrid beamforming architectures are presented, including widely-spaced multi-subarray, dynamic array-of-subarrays, and true-time-delay-based architectures. We also demonstrate the potential data rate, power consumption, and array gain capabilities for THz communications. As a roadmap of THz hybrid beamforming design, multiple open problems and potential research directions are elaborated.      
### 26.Turkish Voice Commands based Chess Game using Gammatone Cepstral Coefficients  [ :arrow_down: ](https://arxiv.org/pdf/2101.08441.pdf)
>  This study was carried out to enable individuals with limited mobility skills to play chess in real time and to play games with the individuals around them without being under any social distress or stress. Voice recordings were taken from 50 people (23 men and 27 women). While recording the sound, 29 words from each person were used which are determined as necessary for playing the game. Mel Frequency Coefficients (MFCC) and Gammatone Cepstral Coefficients (GTCC) qualification methods were used. In addition, k-NN, Naive Bayes and Neural Network classification methods were used for classification. Two different classification procedures were applied, namely, person-based and general. While the performance rate in person-based classification ranged from 75% to 98%, a performance over 84% was achieved in general classification.      
### 27.Effect of Deep Learning Feature Inference Techniques on Respiratory Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2101.08438.pdf)
>  Analysis of respiratory sounds increases its importance every day. Many different methods are available in the analysis, and new techniques are continuing to be developed to further improve these methods. Features are extracted from audio signals and trained using different machine learning techniques. The use of deep learning, which is a different method and has increased in recent years, also shows its influence in this field. Deep learning techniques applied to the image of audio signals give good results and continue to be developed. In this study, image filters were applied to the values obtained from audio signals and the results of the features formed from this were examined in machine learning and deep learning techniques. Their results were compared with the results of methods that had previously achieved good results.      
### 28.Analysis of Information Flow Through U-Nets  [ :arrow_down: ](https://arxiv.org/pdf/2101.08427.pdf)
>  Deep Neural Networks (DNNs) have become ubiquitous in medical image processing and analysis. Among them, U-Nets are very popular in various image segmentation tasks. Yet, little is known about how information flows through these networks and whether they are indeed properly designed for the tasks they are being proposed for. In this paper, we employ information-theoretic tools in order to gain insight into information flow through U-Nets. In particular, we show how mutual information between input/output and an intermediate layer can be a useful tool to understand information flow through various portions of a U-Net, assess its architectural efficiency, and even propose more efficient designs.      
### 29.TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2101.08398.pdf)
>  Topological Data Analysis (TDA) has emerged recently as a robust tool to extract and compare the structure of datasets. TDA identifies features in data such as connected components and holes and assigns a quantitative measure to these features. Several studies reported that topological features extracted by TDA tools provide unique information about the data, discover new insights, and determine which feature is more related to the outcome. On the other hand, the overwhelming success of deep neural networks in learning patterns and relationships has been proven on a vast array of data applications, images in particular. To capture the characteristics of both powerful tools, we propose \textit{TDA-Net}, a novel ensemble network that fuses topological and deep features for the purpose of enhancing model generalizability and accuracy. We apply the proposed \textit{TDA-Net} to a critical application, which is the automated detection of COVID-19 from CXR images. The experimental results showed that the proposed network achieved excellent performance and suggests the applicability of our method in practice.      
### 30.An Information-Theoretic Analysis of the Impact of Task Similarity on Meta-Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.08390.pdf)
>  Meta-learning aims at optimizing the hyperparameters of a model class or training algorithm from the observation of data from a number of related tasks. Following the setting of Baxter [1], the tasks are assumed to belong to the same task environment, which is defined by a distribution over the space of tasks and by per-task data distributions. The statistical properties of the task environment thus dictate the similarity of the tasks. The goal of the meta-learner is to ensure that the hyperparameters obtain a small loss when applied for training of a new task sampled from the task environment. The difference between the resulting average loss, known as meta-population loss, and the corresponding empirical loss measured on the available data from related tasks, known as meta-generalization gap, is a measure of the generalization capability of the meta-learner. In this paper, we present novel information-theoretic bounds on the average absolute value of the meta-generalization gap. Unlike prior work [2], our bounds explicitly capture the impact of task relatedness, the number of tasks, and the number of data samples per task on the meta-generalization gap. Task similarity is gauged via the Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences. We illustrate the proposed bounds on the example of ridge regression with meta-learned bias.      
### 31.BOOSTR: A Dataset for Accelerator Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.08359.pdf)
>  BOOSTR (Booster Operation Optimization Sequential Time-Series for Reinforcement) was created to provide cycle-by-cycle time series of readings and settings from instruments and controllable devices of the Booster, Fermilab's Rapid-Cycling Synchrotron (RCS) operating at 15 Hz. BOOSTR provides a time series from 55 device readings and settings which pertain most directly to the high-precision regulation of the Booster's Gradient Magnet Power Supply (GMPS). To our knowledge, this is the first known dataset of accelerator device parameters made publicly available. We are releasing it in the hopes that it can be used to demonstrate aspects of artificial intelligence for advanced control systems, such as reinforcement learning and autonomous anomaly detection.      
### 32.Nonparametric clustering for image segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2101.08345.pdf)
>  Image segmentation aims at identifying regions of interest within an image, by grouping pixels according to their properties. This task resembles the statistical one of clustering, yet many standard clustering methods fail to meet the basic requirements of image segmentation: segment shapes are often biased toward predetermined shapes and their number is rarely determined automatically. Nonparametric clustering is, in principle, free from these limitations and turns out to be particularly suitable for the task of image segmentation. This is also witnessed by several operational analogies, as, for instance, the resort to topological data analysis and spatial tessellation in both the frameworks. We discuss the application of nonparametric clustering to image segmentation and provide an algorithm specific for this task. Pixel similarity is evaluated in terms of density of the color representation and the adjacency structure of the pixels is exploited to introduce a simple, yet effective method to identify image segments as disconnected high-density regions. The proposed method works both to segment an image and to detect its boundaries and can be seen as a generalization to color images of the class of thresholding methods.      
### 33.Location Management in IP-based Future LEO Satellite Networks: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2101.08336.pdf)
>  Future integrated terrestrial, aerial, and space networks will involve thousands of Low Earth Orbit (LEO) satellites forming a network of mega-constellations, which will play a significant role in providing communication and Internet services everywhere, at any time, and for everything. Due to its very large scale and highly dynamic nature, future LEO satellite networks (SatNets) management is a very complicated and crucial process, especially the mobility management aspect and its two components location management and handover management. In this article, we present a comprehensive and critical review of the state-of-the-art research in LEO SatNets location management. First, we give an overview of the Internet Engineering Task Force (IETF) mobility management standards (e.g., Mobile IPv6 and Proxy Mobile IPv6) and discuss their location management techniques limitations in the environment of future LEO SatNets. We highlight future LEO SatNets mobility characteristics and their challenging features and describe two unprecedented future location management scenarios. A taxonomy of the available location management solutions for LEO SatNets is presented, where the solutions are classified into three approaches. For each of the reviewed approaches, the "Issues to consider" section draws attention to important points that should be considered in future LEO SatNets location management. To identify the gaps, the current state of LEO SatNets location management is summarized and the challenges facing future LEO SatNets location management are discussed. Worthy research directions are recommended. This article is providing a road map for researchers and industry to shape the future of LEO SatNets location management.      
### 34.Dictionary-Sparse Recovery From Heavy-Tailed Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2101.08298.pdf)
>  The recovery of signals that are sparse not in a basis, but rather sparse with respect to an over-complete dictionary is one of the most flexible settings in the field of compressed sensing with numerous applications. As in the standard compressed sensing setting, it is possible that the signal can be reconstructed efficiently from few, linear measurements, for example by the so-called $\ell_1$-synthesis method. <br>However, it has been less well-understood which measurement matrices provably work for this setting. Whereas in the standard setting, it has been shown that even certain heavy-tailed measurement matrices can be used in the same sample complexity regime as Gaussian matrices, comparable results are only available for the restrictive class of sub-Gaussian measurement vectors as far as the recovery of dictionary-sparse signals via $\ell_1$-synthesis is concerned. <br>In this work, we fill this gap and establish optimal guarantees for the recovery of vectors that are (approximately) sparse with respect to a dictionary via the $\ell_1$-synthesis method from linear, potentially noisy measurements for a large class of random measurement matrices. In particular, we show that random measurements that fulfill only a small-ball assumption and a weak moment assumption, such as random vectors with i.i.d. Student-$t$ entries with a logarithmic number of degrees of freedom, lead to comparable guarantees as (sub-)Gaussian measurements. Our results apply for a large class of both random and deterministic dictionaries. <br>As a corollary of our results, we also obtain a slight improvement on the weakest assumption on a measurement matrix with i.i.d. rows sufficient for uniform recovery in standard compressed sensing, improving on results by Mendelson and Lecué and Dirksen, Lecué and Rauhut.      
### 35.The Diagnosis of Asthma using Hilbert-Huang Transform and Deep Learning on Lung Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2101.08288.pdf)
>  Lung auscultation is the most effective and indispensable method for diagnosing various respiratory disorders by using the sounds from the airways during inspirium and exhalation using a stethoscope. In this study, the statistical features are calculated from intrinsic mode functions that are extracted by applying the HilbertHuang Transform to the lung sounds from 12 different auscultation regions on the chest and back. The classification of the lung sounds from asthma and healthy subjects is performed using Deep Belief Networks (DBN). The DBN classifier model with two hidden layers has been tested using 5-fold cross validation method. The proposed DBN separated lung sounds from asthmatic and healthy subjects with high classification performance rates of 84.61%, 85.83%, and 77.11% for overall accuracy, sensitivity, and selectivity, respectively using frequencytime analysis.      
### 36.DyLoc: Dynamic Localization for Massive MIMO Using Predictive Recurrent Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.07848.pdf)
>  This paper presents a data-driven localization framework with high precision in time-varying complex multipath environments, such as dense urban areas and indoors, where GPS and model-based localization techniques come short. We consider the angle-delay profile (ADP), a linear transformation of channel state information (CSI), in massive MIMO systems and show that ADPs preserve users' motion when stacked temporally. We discuss that given a static environment, future frames of ADP time-series are predictable employing a video frame prediction algorithm. We express that a deep convolutional neural network (DCNN) can be employed to learn the background static scattering environment. To detect foreground changes in the environment, corresponding to path blockage or addition, we introduce an algorithm taking advantage of the trained DCNN. Furthermore, we present DyLoc, a data-driven framework to recover distorted ADPs due to foreground changes and to obtain precise location estimations. We evaluate the performance of DyLoc in several dynamic scenarios employing DeepMIMO dataset to generate geo-tagged CSI datasets for indoor and outdoor environments. We show that previous DCNN-based techniques fail to perform with desirable accuracy in dynamic environments, while DyLoc pursues localization precisely. Moreover, simulations show that as the environment gets richer in terms of the number of multipath, DyLoc gets more robust to foreground changes.      
