# ArXiv eess --Wed, 27 Jan 2021
### 1.The Wireless Control Bus: Enabling Efficient Multi-hop Event-Triggered Control with Concurrent Transmissions  [ :arrow_down: ](https://arxiv.org/pdf/2101.10961.pdf)
>  Event-triggered control (ETC) holds the potential to significantly improve the efficiency of wireless networked control systems. Unfortunately, its real-world impact has hitherto been hampered by the lack of a network stack able to transfer its benefits from theory to practice specifically by supporting the latency and reliability requirements of the aperiodic communication ETC induces. This is precisely the contribution of this paper. <br>Our Wireless Control Bus (WCB) exploits carefully orchestrated network-wide floods of concurrent transmissions to minimize overhead during quiescent, steady-state periods, and ensures timely and reliable collection of sensor readings and dissemination of actuation commands when an ETC triggering condition is violated. Using a cyber-physical testbed emulating a water distribution system controlled over a real-world multi-hop wireless network, we show that ETC over WCB achieves the same quality of periodic control at a fraction of the energy costs, therefore unleashing and concretely demonstrating its full potential for the first time.      
### 2.EEG-Inception: An Accurate and Robust End-to-End Neural Network for EEG-based Motor Imagery Classification  [ :arrow_down: ](https://arxiv.org/pdf/2101.10932.pdf)
>  Classification of EEG-based motor imagery (MI) is a crucial non-invasive application in the brain-computer interface (BCI) research. This paper proposes a convolutional neural network (CNN) architecture for accurate and robust EEG-based MI classification that outperforms the state-of-the-art methods. The proposed CNN model, namely EEG-Inception, is built on the backbone of the Inception-Time network, which showed to be highly efficient and accurate for time-series classification. Also, the proposed network is an end-to-end classification, for it takes the raw EEG signals as the input and does not require complex EEG signal-preprocessing. Furthermore, this paper proposes a novel data augmentation method for EEG signals to enhance the accuracy, at least by 3%, and reduce overfitting with limited BCI datasets. The proposed model outperformed all the state-of-the-art methods by achieving the average accuracy of 88.4% and 88.6% on the 2008 BCI Competition IV 2a (four-classes) and 2b datasets (binary-classes), respectively. Furthermore, it takes less than 0.025 seconds to test a sample, which is suitable for real-time processing. Moreover, the classification standard deviation for nine different subjects achieved the lowest value of 5.5 for the 2b dataset and 7.1 for the 2a dataset, validating that the proposed method is highly robust. From the experiment results, it can be inferred that the EEG-Inception network exhibits a strong potential as a subject-independent classifier for EEG-based MI tasks.      
### 3.Learning-Based Patch-Wise Metal Segmentation with Consistency Check  [ :arrow_down: ](https://arxiv.org/pdf/2101.10914.pdf)
>  Metal implants that are inserted into the patient's body during trauma interventions cause heavy artifacts in 3D X-ray acquisitions. Metal Artifact Reduction (MAR) methods, whose first step is always a segmentation of the present metal objects, try to remove these artifacts. Thereby, the segmentation is a crucial task which has strong influence on the MAR's outcome. This study proposes and evaluates a learning-based patch-wise segmentation network and a newly proposed Consistency Check as post-processing step. The combination of the learned segmentation and Consistency Check reaches a high segmentation performance with an average IoU score of 0.924 on the test set. Furthermore, the Consistency Check proves the ability to significantly reduce false positive segmentations whilst simultaneously ensuring consistent segmentations.      
### 4.Artificial Intelligence for Satellite Communication: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2101.10899.pdf)
>  Satellite communication offers the prospect of service continuity over uncovered and under-covered areas, service ubiquity, and service scalability. However, several challenges must first be addressed to realize these benefits, as the resource management, network control, network security, spectrum management, and energy usage of satellite networks are more challenging than that of terrestrial networks. Meanwhile, artificial intelligence (AI), including machine learning, deep learning, and reinforcement learning, has been steadily growing as a research field and has shown successful results in diverse applications, including wireless communication. In particular, the application of AI to a wide variety of satellite communication aspects have demonstrated excellent potential, including beam-hopping, anti-jamming, network traffic forecasting, channel modeling, telemetry mining, ionospheric scintillation detecting, interference managing, remote sensing, behavior modeling, space-air-ground integrating, and energy managing. This work thus provides a general overview of AI, its diverse sub-fields, and its state-of-the-art algorithms. Several challenges facing diverse aspects of satellite communication systems are then discussed, and their proposed and potential AI-based solutions are presented. Finally, an outlook of field is drawn, and future steps are suggested.      
### 5.Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yoloxóchitl Mixtec  [ :arrow_down: ](https://arxiv.org/pdf/2101.10877.pdf)
>  "Transcription bottlenecks", created by a shortage of effective human transcribers are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which unlike Hidden Markov Model ASR systems, eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yoloxóchitl Mixtec EL corpus. First, we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation.      
### 6.Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2101.10876.pdf)
>  In this paper, we demonstrate how deep autoencoders can be generalized to the case of inpainting and denoising, even when no clean training data is available. In particular, we show how neural networks can be trained to perform all of these tasks simultaneously. While, deep autoencoders implemented by way of neural networks have demonstrated potential for denoising and anomaly detection, standard autoencoders have the drawback that they require access to clean data for training. However, recent work in Robust Deep Autoencoders (RDAEs) shows how autoencoders can be trained to eliminate outliers and noise in a dataset without access to any clean training data. Inspired by this work, we extend RDAEs to the case where data are not only noisy and have outliers, but also only partially observed. Moreover, the dataset we train the neural network on has the properties that all entries have noise, some entries are corrupted by large mistakes, and many entries are not even known. Given such an algorithm, many standard tasks, such as denoising, image inpainting, and unobserved entry imputation can all be accomplished simultaneously within the same framework. Herein we demonstrate these techniques on standard machine learning tasks, such as image inpainting and denoising for the MNIST and CIFAR10 datasets. However, these approaches are not only applicable to image processing problems, but also have wide ranging impacts on datasets arising from real-world problems, such as manufacturing and network processing, where noisy, partially observed data naturally arise.      
### 7.B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows  [ :arrow_down: ](https://arxiv.org/pdf/2101.10870.pdf)
>  Human Activity Recognition (HAR), based on machine and deep learning algorithms is considered one of the most promising technologies to monitor professional and daily life activities for different categories of people (e.g., athletes, elderly, kids, employers) in order to provide a variety of services related, for example to well-being, empowering of technical performances, prevention of risky situation, and educational purposes. However, the analysis of the effectiveness and the efficiency of HAR methodologies suffers from the lack of a standard workflow, which might represent the baseline for the estimation of the quality of the developed pattern recognition models. This makes the comparison among different approaches a challenging task. In addition, researchers can make mistakes that, when not detected, definitely affect the achieved results. To mitigate such issues, this paper proposes an open-source automatic and highly configurable framework, named B-HAR, for the definition, standardization, and development of a baseline framework in order to evaluate and compare HAR methodologies. It implements the most popular data processing methods for data preparation and the most commonly used machine and deep learning pattern recognition models.      
### 8.A Raspberry Pi based Traumatic Brain Injury Detection System for Single-Channel Electroencephalogram  [ :arrow_down: ](https://arxiv.org/pdf/2101.10869.pdf)
>  Traumatic Brain Injury (TBI) is a common cause of death and disability. However, existing tools for TBI diagnosis are either subjective or require extensive clinical setup and expertise. The increasing affordability and reduction in size of relatively high-performance computing systems combined with promising results from TBI related machine learning research make it possible to create compact and portable systems for early detection of TBI. This work describes a Raspberry Pi based portable, real-time data acquisition, and automated processing system that uses machine learning to efficiently identify TBI and automatically score sleep stages from a single-channel Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and verification of the system that can digitize EEG signal using an Analog to Digital Converter (ADC) and perform real-time signal classification to detect the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN) and XGBoost based predictive models to evaluate the performance and demonstrate the versatility of the system to operate with multiple types of predictive models. We achieve a peak classification accuracy of more than 90% with a classification time of less than 1 s across 16 s - 64 s epochs for TBI vs control conditions. This work can enable development of systems suitable for field use without requiring specialized medical equipment for early TBI detection applications and TBI research. Further, this work opens avenues to implement connected, real-time TBI related health and wellness monitoring systems.      
### 9.Deep neural network-based automatic metasurface design with a wide frequency range  [ :arrow_down: ](https://arxiv.org/pdf/2101.10866.pdf)
>  Beyond the scope of conventional metasurface which necessitates plenty of computational resources and time, an inverse design approach using machine learning algorithms promises an effective way for metasurfaces design. In this paper, benefiting from Deep Neural Network (DNN), an inverse design procedure of a metasurface in an ultra-wide working frequency band is presented where the output unit cell structure can be directly computed by a specified design target. To reach the highest working frequency, for training the DNN, we consider 8 ring-shaped patterns to generate resonant notches at a wide range of working frequencies from 4 to 45 GHz. We propose two network architectures. In one architecture, we restricted the output of the DNN, so the network can only generate the metasurface structure from the input of 8 ring-shaped patterns. This approach drastically reduces the computational time, while keeping the network's accuracy above 91\%. We show that our model based on DNN can satisfactorily generate the output metasurface structure with an average accuracy of over 90\% in both network architectures. Determination of the metasurface structure directly without time-consuming optimization procedures, having an ultra-wide working frequency, and high average accuracy equip an inspiring platform for engineering projects without the need for complex electromagnetic theory.      
### 10.Multi-objective design of multilayer microwave dielectric filters using artificial bee colony algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2101.10858.pdf)
>  Artificial bee colony algorithm (ABC) developed by inspiring the foraging phenomena of the natural honey bees is a simple and powerful metaheuristic optimization algorithm. The performance of single objective ABC performance has been well demonstrated by implemented to different design optimization problems from elec-trical and mechanical to civil engineering. Its efficacy is anticipated in the multi-objective design optimization of multilayer microwave dielectric filters (MMDFs) which is a challenging multi-objective problem falls into the context of electromagnetic (EM) design in electrical engineering. The MMDF is composed of superimposed multiple dielectric layers in order to pass or stop the EM wave at the desired frequency bands. An accurate dual-objective EM model of MMDF is constituted to be the total reflection coefficients for oblique incident wave angular range with transverse electric (TE) or transverse magnetic (TM) polarizations. The objective functions depend on the total reflection (TR) at the pass and stop band re-gions. Three types of the MDFs that are low pass (LP), high pass (HP) and band pass (BP) MMDFs are optimally designed across the constituted dual-objective EM model with a frequency dependent material database through using the multi-objective ABC (MO-ABC) strategy. The Pareto optimal solutions are refined within the possible solutions for synchronously minimizing the objective func-tions. The global optimal MMDF designs are selected from the Pare-to optimal solutions by providing the trade-off therein between the objective functions. The performance study regarding the frequen-cy characteristics of the designed MMDFs are well presented via numerical and graphical results. It is hence demonstrated that MO-ABC is even versatile and robust in the multi-objective design of MMDFs.      
### 11.Sinc-based convolutional neural networks for EEG-BCI-based motor imagery classification  [ :arrow_down: ](https://arxiv.org/pdf/2101.10846.pdf)
>  Brain-Computer Interfaces (BCI) based on motor imagery translate mental motor images recognized from the electroencephalogram (EEG) to control commands. EEG patterns of different imagination tasks, e.g. hand and foot movements, are effectively classified with machine learning techniques using band power features. Recently, also Convolutional Neural Networks (CNNs) that learn both effective features and classifiers simultaneously from raw EEG data have been applied. However, CNNs have two major drawbacks: (i) they have a very large number of parameters, which thus requires a very large number of training examples; and (ii) they are not designed to explicitly learn features in the frequency domain. To overcome these limitations, in this work we introduce Sinc-EEGNet, a lightweight CNN architecture that combines learnable band-pass and depthwise convolutional filters. Experimental results obtained on the publicly available BCI Competition IV Dataset 2a show that our approach outperforms reference methods in terms of classification accuracy.      
### 12.Visible light communication-based monitoring for indoor environments using unsupervised learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.10838.pdf)
>  Visible Light Communication~(VLC) systems provide not only illumination and data communication, but also indoor monitoring services if the effect that different events create on the received optical signal is properly tracked. For this purpose, the Channel State Information that a VLC receiver computes to equalize the subcarriers of the OFDM signal can be also reused to train an Unsupervised Learning classifier. This way, different clusters can be created on the collected CSI data, which could be then mapped into relevant events to-be-monitored in the indoor environments, such as the presence of a new object in a given position or the change of the position of a given object. When compared to supervised learning algorithms, the proposed approach does not need to add tags in the training data, simplifying notably the implementation of the machine learning classifier. The practical validation the monitoring approach was done with the aid of a software-defined VLC link based on OFDM, in which a copy of the intensity modulated signal coming from a Phosphor-converted LED was captured by a pair of Photodetectors~(PDs). The performance evaluation of the experimental VLC-based monitoring demo achieved a positioning accuracy in the few-centimeter-range, without the necessity of deploying a large number of sensors and/or adding a VLC-enabled sensor on the object to-be-tracked.      
### 13.Loss Ensembles for Extremely Imbalanced Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2101.10815.pdf)
>  This short paper briefly presents our methodology details of automatic intracranial aneurysms segmentation from brain MR scans. We use ensembles of multiple models trained from different loss functions. Our method ranked first place in the ADAM challenge segmentation task. The code and trained models are publicly available at <a class="link-external link-https" href="https://github.com/JunMa11/ADAM2020" rel="external noopener nofollow">this https URL</a>.      
### 14.Robust Scheduling of Virtual Power Plant under Exogenous and Endogenous Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2101.10800.pdf)
>  Virtual power plant (VPP) provides a flexible solution to distributed energy resources integration by aggregating renewable generation units, conventional power plants, energy storages, and flexible demands. This paper proposes a novel model for determining the optimal offering strategy in the day-ahead energy-reserve market and the optimal self-scheduling plan. It considers exogenous uncertainties (or called decision-independent uncertainties, DIUs) associated with market clearing prices and available wind power generation, as well as the endogenous uncertainties (or called decision-dependent uncertainties, DDUs) pertaining to real-time reserve deployment requests. A tractable solution method based on strong duality theory, McCormick relaxation, and the Benders' decomposition to solve the proposed stochastic adaptive robust optimization with DDUs formulation is developed. Simulation results demonstrate the applicability of the proposed approach.      
### 15.ImageCHD: A 3D Computed Tomography Image Dataset for Classification of Congenital Heart Disease  [ :arrow_down: ](https://arxiv.org/pdf/2101.10799.pdf)
>  Congenital heart disease (CHD) is the most common type of birth defect, which occurs 1 in every 110 births in the United States. CHD usually comes with severe variations in heart structure and great artery connections that can be classified into many types. Thus highly specialized domain knowledge and the time-consuming human process is needed to analyze the associated medical images. On the other hand, due to the complexity of CHD and the lack of dataset, little has been explored on the automatic diagnosis (classification) of CHDs. In this paper, we present ImageCHD, the first medical image dataset for CHD classification. ImageCHD contains 110 3D Computed Tomography (CT) images covering most types of CHD, which is of decent size Classification of CHDs requires the identification of large structural changes without any local tissue changes, with limited data. It is an example of a larger class of problems that are quite difficult for current machine-learning-based vision methods to solve. To demonstrate this, we further present a baseline framework for the automatic classification of CHD, based on a state-of-the-art CHD segmentation method. Experimental results show that the baseline framework can only achieve a classification accuracy of 82.0\% under a selective prediction scheme with 88.4\% coverage, leaving big room for further improvement. We hope that ImageCHD can stimulate further research and lead to innovative and generic solutions that would have an impact in multiple domains. Our dataset is released to the public compared with existing medical imaging datasets.      
### 16.Data-Driven Set-Based Estimation using Matrix Zonotopes with Set Containment Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2101.10784.pdf)
>  We propose a method to perform set-based state estimation of an unknown dynamical system using a data-driven set propagation function. Our method comes with set-containment guarantees, making it applicable to the estimation of safety-critical systems. The method consists of two phases: (1) an offline learning phase where we collect noisy state-input data to determine a function to propagate the state-set ahead in time; and (2) an online estimation phase consisting of a time update and a measurement update. It is assumed that sets bound measurement noise and disturbances, but we assume no knowledge of their statistical properties. These sets are described using zonotopes, allowing efficient propagation and intersection operations. We propose two approaches to perform the measurement update. The method is extended to constrained zonotopes. Simulations show that the proposed estimator yields state sets comparable in volume to the confidence bounds obtained by a Kalman filter approach, but with the addition of state set-containment guarantees. We observe that using constrained zonotopes yields smaller sets, but with higher computational cost compared to unconstrained zonotopes.      
### 17.Quasi Static Atmospheric Model for Aircraft Trajectory Prediction and Flight Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2101.10744.pdf)
>  Aircraft trajectory prediction requires the determination of the atmospheric properties (pressure, temperature, and density) encountered by the aircraft during its flight. This is accomplished by employing a tabulated prediction published by a meteorological service, a static atmosphere model that does not consider the atmosphere variation with time or horizontal position, such as the International Civil Aviation Organization (ICAO) Standard Atmosphere (ISA), or a variation to the later so it better conforms with the expected flight conditions. This article proposes an easy-to-use quasi static model that introduces temperature and pressure variations while respecting all the hypotheses of the ISA model, resulting in more realistic trajectory predictions as the obtained atmospheric properties bear a higher resemblance with the local conditions encountered during the flight. The proposed model relies on two parameters, the temperature and pressure offsets, and converges to the ISA model when both are zero. Expressions to obtain the atmospheric properties are established, and their dependencies with both parameters explained. The author calls this model INSA (ICAO Non Standard Atmosphere) and releases its C++ implementation as open-source software.      
### 18.Real-time Video Streaming and Control of Cellular-Connected UAV System  [ :arrow_down: ](https://arxiv.org/pdf/2101.10736.pdf)
>  Unmanned aerial vehicles (UAVs) play an increasingly important role in military, public, and civilian applications, where providing connectivity to UAVs is crucial for its real-time control, video streaming, and data collection. Considering that cellular networks offer wide area, high speed, and secure wireless connectivity, cellular-connected UAVs have been considered as an appealing solution to provide UAV connectivity with enhanced reliability, coverage, throughput, and security. Due to the nature of UAVs mobility, the throughput and the End-to-End (E2E) delay of UAVs communication under various flight heights, video resolutions, and transmission frequencies remain unknown. To evaluate these parameters, we develop a cellular-connected UAV testbed based on the Long Term Evolution (LTE) network with its uplink video transmission and downlink control\&amp;command (CC) transmission. We also design algorithms for sending control signal and controlling UAV. The indoor experimental results provide fundamental insights for the cellular-connected UAV system design from the perspective of transmission frequency, adaptability, link outage, and transmission protocol, respectively.      
### 19.A Distributed Implementation of Steady-State Kalman Filter  [ :arrow_down: ](https://arxiv.org/pdf/2101.10689.pdf)
>  This paper studies the distributed state estimation in sensor network, where $m$ sensors are deployed to infer the $n$-dimensional state of a linear time-invariant (LTI) Gaussian system. By a lossless decomposition of optimal steady-state Kalman filter, we show that the problem of distributed estimation can be reformulated as synchronization of homogeneous linear systems. Based on such decomposition, a distributed estimator is proposed, where each sensor node runs a local filter using only its own measurement and fuses the local estimate of each node with a consensus algorithm. We show that the average of the estimate from all sensors coincides with the optimal Kalman estimate. Numerical examples are provided in the end to illustrate the performance of the proposed scheme.      
### 20.Discrete Adaptive Control Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2101.10675.pdf)
>  The main purpose of a control allocator is to distribute a total control effort among redundant actuators. This paper proposes a discrete adaptive control allocator for over-actuated sampled-data systems in the presence of actuator uncertainty. The proposed method does not require uncertainty estimation or persistency of excitation. Furthermore, the presented algorithm employs a closed loop reference model, which provides fast convergence without introducing excessive oscillations. To generate the total control signal, an LQR controller with reference tracking is used to guarantee the outer loop asymptotic stability. The discretized version of the Aerodata Model in Research Environment (ADMIRE) is used as an over-actuated system, to demonstrate the efficacy of the proposed method.      
### 21.Leveraging 3D Information in Unsupervised Brain MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2101.10674.pdf)
>  Automatic segmentation of brain abnormalities is challenging, as they vary considerably from one pathology to another. Current methods are supervised and require numerous annotated images for each pathology, a strenuous task. To tackle anatomical variability, Unsupervised Anomaly Detection (UAD) methods are proposed, detecting anomalies as outliers of a healthy model learned using a Variational Autoencoder (VAE). Previous work on UAD adopted a 2D approach, meaning that MRIs are processed as a collection of independent slices. Yet, it does not fully exploit the spatial information contained in MRI. Here, we propose to perform UAD in a 3D fashion and compare 2D and 3D VAEs. As a side contribution, we present a new loss function guarantying a robust training. Learning is performed using a multicentric dataset of healthy brain MRIs, and segmentation performances are estimated on White-Matter Hyperintensities and tumors lesions. Experiments demonstrate the interest of 3D methods which outperform their 2D counterparts.      
### 22.Efficient Multi-objective Evolutionary 3D Neural Architecture Search for COVID-19 Detection with Chest CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2101.10667.pdf)
>  COVID-19 pandemic has spread globally for months. Due to its long incubation period and high testing cost, there is no clue showing its spread speed is slowing down, and hence a faster testing method is in dire need. This paper proposes an efficient Evolutionary Multi-objective neural ARchitecture Search (EMARS) framework, which can automatically search for 3D neural architectures based on a well-designed search space for COVID-19 chest CT scan classification. Within the framework, we use weight sharing strategy to significantly improve the search efficiency and finish the search process in 8 hours. We also propose a new objective, namely potential, which is of benefit to improve the search process's robustness. With the objectives of accuracy, potential, and model size, we find a lightweight model (3.39 MB), which outperforms three baseline human-designed models, i.e., ResNet3D101 (325.21 MB), DenseNet3D121 (43.06 MB), and MC3\_18 (43.84 MB). Besides, our well-designed search space enables the class activation mapping algorithm to be easily embedded into all searched models, which can provide the interpretability for medical diagnosis by visualizing the judgment based on the models to locate the lesion areas.      
### 23.Semi-supervised source localization in reverberant environments with deep generative modeling  [ :arrow_down: ](https://arxiv.org/pdf/2101.10636.pdf)
>  A semi-supervised approach to acoustic source localization in reverberant environments, based on deep generative modeling, is proposed. Localization in reverberant environments remains an open challenge. Even with large data volumes, the number of labels available for supervised learning in reverberant environments is usually small. We address this issue by performing semi-supervised learning (SSL) with convolutional variational autoencoders (VAEs) on speech signals in reverberant environments. The VAE is trained to generate the phase of relative transfer functions (RTFs) between microphones, in parallel with a direction of arrival (DOA) classifier based on RTF-phase, on both labeled and unlabeled RTF samples. In learning to perform these tasks, the VAE-SSL explicitly learns to separate the physical causes of the RTF-phase (i.e., source location) from distracting signal characteristics such as noise and speech activity. Relative to existing semi-supervised localization methods in acoustics, VAE-SSL is effectively an end-to-end processing approach which relies on minimal preprocessing of RTF-phase features. The VAE-SSL approach is compared with the steered response power with phase transform (SRP-PHAT) and fully supervised CNNs. We find that VAE-SSL can outperform both SRP-PHAT and CNN in label-limited scenarios. Further, the trained VAE-SSL system can generate new RTF-phase samples, which shows the VAE-SSL approach learns the physics of the acoustic environment. The generative modeling in VAE-SSL thus provides a means of interpreting the learned representations.      
### 24.Robust Finite-Time Consensus Subject to Unknown Communication Time Delays Based on Delay-Dependent Criteria  [ :arrow_down: ](https://arxiv.org/pdf/2101.10627.pdf)
>  In this paper, robust finite-time consensus of a group of nonlinear multi-agent systems in the presence of communication time delays is considered. In particular, appropriate delay-dependent strategies which are less conservative are suggested. Sufficient conditions for finite-time consensus in the presence of deterministic and stochastic disturbances are presented. The communication delays don't need to be time invariant, uniform, symmetric, or even known. The only required condition is that all delays satisfy a known upper bound. The consensus algorithm is appropriate for agents with partial access to neighbor agents' signals. The Lyapunov-Razumikhin theorem for finite-time convergence is used to prove the results. Simulation results on a group of mobile robot manipulators as the agents of the system are presented.      
### 25.A Survey and Analysis on Automated Glioma Brain Tumor Segmentation and Overall Patient Survival Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2101.10599.pdf)
>  Glioma is the most deadly brain tumor with high mortality. Treatment planning by human experts depends on the proper diagnosis of physical symptoms along with Magnetic Resonance(MR) image analysis. Highly variability of a brain tumor in terms of size, shape, location, and a high volume of MR images makes the analysis time-consuming. Automatic segmentation methods achieve a reduction in time with excellent reproducible results. The article aims to survey the advancement of automated methods for Glioma brain tumor segmentation. It is also essential to make an objective evaluation of various models based on the benchmark. Therefore, the 2012 - 2019 BraTS challenges database evaluates state-of-the-art methods. The complexity of tasks under the challenge has grown from segmentation (Task1) to overall survival prediction (Task 2) to uncertainty prediction for classification (Task 3). The paper covers the complete gamut of brain tumor segmentation using handcrafted features to deep neural network models for Task 1. The aim is to showcase a complete change of trends in automated brain tumor models. The paper also covers end to end joint models involving brain tumor segmentation and overall survival prediction. All the methods are probed, and parameters that affect performance are tabulated and analyzed.      
### 26.Glioblastoma Multiforme Patient Survival Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2101.10589.pdf)
>  Glioblastoma Multiforme is a very aggressive type of brain tumor. Due to spatial and temporal intra-tissue inhomogeneity, location and the extent of the cancer tissue, it is difficult to detect and dissect the tumor regions. In this paper, we propose survival prognosis models using four regressors operating on handcrafted image-based and radiomics features. We hypothesize that the radiomics shape features have the highest correlation with survival prediction. The proposed approaches were assessed on the Brain Tumor Segmentation (BraTS-2020) challenge dataset. The highest accuracy of image features with random forest regressor approach was 51.5\% for the training and 51.7\% for the validation dataset. The gradient boosting regressor with shape features gave an accuracy of 91.5\% and 62.1\% on training and validation datasets respectively. It is better than the BraTS 2020 survival prediction challenge winners on the training and validation datasets. Our work shows that handcrafted features exhibit a strong correlation with survival prediction. The consensus based regressor with gradient boosting and radiomics shape features is the best combination for survival prediction.      
### 27.Towards a Secure and Resilient All-Renewable Energy Grid for Smart Cities  [ :arrow_down: ](https://arxiv.org/pdf/2101.10570.pdf)
>  The concept of smart cities is driven by the need to enhance citizens' quality of life. It is estimated that 70% of the world population will live in urban areas by 2050. The electric grid is the energy backbone of smart city deployments. An electric energy system immune to adverse events, both cyber and physical risks, and able to support the integration of renewable sources will drive a transformational development approach for future smart cities. This article describes how the future electric energy system with 100% electricity supply from renewable energy sources requires the "birth of security and resiliency" incorporated with its ecosystem.      
### 28.Self-Calibrating Indoor Localization with Crowdsourcing Fingerprints and Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.10527.pdf)
>  Precise indoor localization is one of the key requirements for fifth Generation (5G) and beyond, concerning various wireless communication systems, whose applications span different vertical sectors. Although many highly accurate methods based on signal fingerprints have been lately proposed for localization, their vast majority faces the problem of degrading performance when deployed in indoor systems, where the propagation environment changes rapidly. In order to address this issue, the crowdsourcing approach has been adopted, according to which the fingerprints are frequently updated in the respective database via user reporting. However, the late crowdsourcing techniques require precise indoor floor plans and fail to provide satisfactory accuracy. In this paper, we propose a low-complexity self-calibrating indoor crowdsourcing localization system that combines historical with frequently updated fingerprints for high precision user positioning. We present a multi-kernel transfer learning approach which exploits the inner relationship between the original and updated channel measurements. Our indoor laboratory experimental results with the proposed approach and using Nexus 5 smartphones at 2.4GHz with 20MHz bandwidth have shown the feasibility of about one meter level accuracy with a reasonable fingerprint update overhead.      
### 29.Autonomous Vehicle-to-Grid Design for Provision of Frequency Control Ancillary Service and Distribution Voltage Regulation  [ :arrow_down: ](https://arxiv.org/pdf/2101.10518.pdf)
>  We develop a system-level design for the provision of Ancillary Service (AS) for control of electric power grids by in-vehicle batteries, suitably applied to Electric Vehicles (EVs) operated in a sharing service. The provision is called in this paper the multi-objective AS: primary frequency control in a transmission grid and voltage amplitude regulation in a distribution grid connected to EVs. The design is based on the ordinary differential equation model of distribution voltage, which has been recently introduced as a new physics-based model, and is utilized in this paper for assessing and regulating the impact of spatiotemporal charging/charging of a large population of EVs to a distribution grid. Effectiveness of the autonomous V2G design is evaluated with numerical simulations of realistic models for transmission and distribution grids with synthetic operation data on EVs in a sharing service. In addition, we present a hardware-in-the-loop test for evaluating its feasibility in a situation where inevitable latency is involved due to power, control, and communication equipments.      
### 30.Compositional Cyber-Physical Systems Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2101.10484.pdf)
>  Assuring the correct behavior of cyber-physical systems requires significant modeling effort, particularly during early stages of the engineering and design process when a system is not yet available for testing or verification of proper behavior. A primary motivation for `getting things right' in these early design stages is that altering the design is significantly less costly and more effective than when hardware and software have already been developed. Engineering cyber-physical systems requires the construction of several different types of models, each representing a different view, which include stakeholder requirements, system behavior, and the system architecture. Furthermore, each of these models can be represented at different levels of abstraction. Formal reasoning has improved the precision and expanded the available types of analysis in assuring correctness of requirements, behaviors, and architectures. However, each is usually modeled in distinct formalisms and corresponding tools. Currently, this disparity means that a system designer must manually check that the different models are in agreement. Manually editing and checking models is error prone, time consuming, and sensitive to any changes in the design of the models themselves. Wiring diagrams and related theory provide a means for formally organizing these different but related modeling views, resulting in a compositional modeling language for cyber-physical systems. Such a categorical language can make concrete the relationship between different model views, thereby managing complexity, allowing hierarchical decomposition of system models, and formally proving consistency between models.      
### 31.State Estimation for a Zero-Dimensional Electrochemical Model of Lithium-Sulfur Batteries  [ :arrow_down: ](https://arxiv.org/pdf/2101.10436.pdf)
>  Lithium-sulfur (Li-S) batteries have become one of the most attractive alternatives over conventional Li-ion batteries due to their high theoretical specific energy density (2500 Wh/kg for Li-S vs. $\sim$250 Wh/kg for Li-ion). Accurate state estimation in Li-S batteries is urgently needed for safe and efficient operation. To the best of the authors' knowledge, electrochemical model-based observers have not been reported for Li-S batteries, primarily due to the complex dynamics that make state observer design a challenging problem. In this work, we demonstrate a state estimation scheme based on a zero-dimensional electrochemical model for Li-S batteries. The nonlinear differential-algebraic equation (DAE) model is incorporated into an extend Kalman filter. This observer design estimates both differential and algebraic states that represent the dynamic behavior inside the cell, from voltage and current measurements only. The effectiveness of the proposed estimation algorithm is illustrated by numerical simulation results. Our study unlocks how an electrochemical model can be utilized for practical state estimation of Li-S batteries.      
### 32.Test and Evaluation Framework for Multi-Agent Systems of Autonomous Intelligent Agents  [ :arrow_down: ](https://arxiv.org/pdf/2101.10430.pdf)
>  Test and evaluation is a necessary process for ensuring that engineered systems perform as intended under a variety of conditions, both expected and unexpected. In this work, we consider the unique challenges of developing a unifying test and evaluation framework for complex ensembles of cyber-physical systems with embedded artificial intelligence. We propose a framework that incorporates test and evaluation throughout not only the development life cycle, but continues into operation as the system learns and adapts in a noisy, changing, and contended environment. The framework accounts for the challenges of testing the integration of diverse systems at various hierarchical scales of composition while respecting that testing time and resources are limited. A generic use case is provided for illustrative purposes and research directions emerging as a result of exploring the use case via the framework are suggested.      
### 33.Computing Robust Forward Invariant Sets of Multidimensional Non-linear Systems via Geometric Deformation of Polytopes  [ :arrow_down: ](https://arxiv.org/pdf/2101.10407.pdf)
>  This paper develops and implements an algorithm to compute sequences of polytopic Robust Forward Invariant Sets (RFIS) that can parametrically vary in size between the maximal and minimal RFIS of a nonlinear dynamical system. This is done through a novel computational approach that geometrically deforms a polytope into an invariant set using a sequence of homeomorphishms, based on an invariance condition that only needs to be satisfied at a finite set of test points. For achieving this, a fast computational test is developed to establish if a given polytopic set is an RFIS. The geometric nature of the proposed approach makes it applicable for arbitrary Lipschitz continuous nonlinear systems in the presence of bounded additive disturbances. The versatility of the proposed approach is presented through simulation results on a variety of nonlinear dynamical systems in two and three dimensions, for which, sequences of invariant sets are computed.      
### 34.Learning-'N-Flying: A Learning-based, Decentralized Mission Aware UAS Collision Avoidance Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2101.10404.pdf)
>  Urban Air Mobility, the scenario where hundreds of manned and Unmanned Aircraft System (UAS) carry out a wide variety of missions (e.g. moving humans and goods within the city), is gaining acceptance as a transportation solution of the future. One of the key requirements for this to happen is safely managing the air traffic in these urban airspaces. Due to the expected density of the airspace, this requires fast autonomous solutions that can be deployed online. We propose Learning-'N-Flying (LNF) a multi-UAS Collision Avoidance (CA) framework. It is decentralized, works on-the-fly and allows autonomous UAS managed by different operators to safely carry out complex missions, represented using Signal Temporal Logic, in a shared airspace. We initially formulate the problem of predictive collision avoidance for two UAS as a mixed-integer linear program, and show that it is intractable to solve online. Instead, we first develop Learning-to-Fly (L2F) by combining: a) learning-based decision-making, and b) decentralized convex optimization-based control. LNF extends L2F to cases where there are more than two UAS on a collision path. Through extensive simulations, we show that our method can run online (computation time in the order of milliseconds), and under certain assumptions has failure rates of less than 1% in the worst-case, improving to near 0% in more relaxed operations. We show the applicability of our scheme to a wide variety of settings through multiple case studies.      
### 35.Quality Assessment of Super-Resolved Omnidirectional Image Quality Using Tangential Views  [ :arrow_down: ](https://arxiv.org/pdf/2101.10396.pdf)
>  Omnidirectional images (ODIs), also known as 360-degree images, enable viewers to explore all directions of a given 360-degree scene from a fixed point. Designing an immersive imaging system with ODI is challenging as such systems require very large resolution coverage of the entire 360 viewing space to provide an enhanced quality of experience (QoE). Despite remarkable progress on single image super-resolution (SISR) methods with deep-learning techniques, no study for quality assessments of super-resolved ODIs exists to analyze the quality of such SISR techniques. This paper proposes an objective, full-reference quality assessment framework which studies quality measurement for ODIs generated by GAN-based and CNN-based SISR methods. The quality assessment framework offers to utilize tangential views to cope with the spherical nature of a given ODIs. The generated tangential views are distortion-free and can be efficiently scaled to high-resolution spherical data for SISR quality measurement. We extensively evaluate two state-of-the-art SISR methods using widely used full-reference SISR quality metrics adapted to our designed framework. In addition, our study reveals that most objective metric show high performance over CNN based SISR, while subjective tests favors GAN-based architectures.      
### 36.Optimal Network Topology of Multi-Agent Systems subject to Computation and Communication Latency  [ :arrow_down: ](https://arxiv.org/pdf/2101.10394.pdf)
>  We study minimum-variance feedback-control design for a networked control system with retarded dynamics, where inter-agent communication is subject to latency. In particular, we prove that such control design can be solved efficiently for circular formations and compute near-optimal control gains in closed form. Also, we show that the centralized control architecture is in general suboptimal when the feedback delays increase with the number of communication links, and propose a control-driven optimization of the network topology.      
### 37.A Joint Learning and Communication Framework for Multi-Agent Reinforcement Learning over Noisy Channels  [ :arrow_down: ](https://arxiv.org/pdf/2101.10369.pdf)
>  We propose a novel formulation of the "effectiveness problem" in communications, put forth by Shannon and Weaver in their seminal work [2], by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework. Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment can also communicate with each other over a noisy communication channel. The noisy communication channel is considered explicitly as part of the dynamics of the environment and the message each agent sends is part of the action that the agent can take. As a result, the agents learn not only to collaborate with each other but also to communicate "effectively" over a noisy channel. This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the "learning to communicate" framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free. We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP. This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems.      
### 38.Estimates for weighted homogeneous delay systems: A Lyapunov-Krasovskii-Razumikhin approach  [ :arrow_down: ](https://arxiv.org/pdf/2101.10365.pdf)
>  In this paper, we present estimates for solutions and for the attraction domain of the trivial solution for systems with delayed and nonlinear weighted homogeneous right-hand side of positive degree. The results are achieved via a generalization of the Lyapunov-Krasovskii functional construction presented recently for homogeneous systems with standard dilation. Along with the classical approach for the calculation of the estimates within the Lyapunov-Krasovskii framework, we develop a novel approach which combines the use of Lyapunov-Krasovskii functionals with ideas of the Razumikhin framework. More precisely, a lower bound for the functional on a special set of functions inspired by the Razumikhin condition is constructed, and an additional condition imposed on the solution of the comparison equation ensures that this bound can be used to estimate all solutions in a certain neighbourhood of the trivial one. An example shows that this approach yields less conservative estimates in comparison with the classical one.      
### 39.Fire Risk Analysis By Using Sentinel-2 Data: The Case Study Of The Vesuvius In Campania, Italy  [ :arrow_down: ](https://arxiv.org/pdf/2101.10352.pdf)
>  As sadly known, forest fires are part of a set of natural disasters that have always affected regions of the world typically characterized by a tropical climate with long periods of drought. However, due to climate changes of the recent years, other regions of our planet that were not affected by this plague have also had to deal with this phenomenon. One of them is certainly the Italian peninsula, and especially the regions of southern Italy. For this reason, the scientific community, and in particular that one of the remote sensing, plays an important role in the development of reliable techniques to provide useful support to the competent authorities. Therefore, in this work, the capability of the Normalized Differential Water Index (NDWI), derived from spaceborne remote sensing (RS) data, is assessed to monitor the forest fires occurred on a specific study area during the summer of 2017: the volcano Vesuvius, near Naples (in Campania, Italy). In particular, the index is obtained from Sentinel-2 multispectral images of the European Space Agency (ESA), which are free of charge and open accessible. Moreover, the twin Sentinel-2 (S-2) sensors allows to overcome some restrictions on time delivery and high frequency observation. These requirements are goodly matched by other spaceborne sensors, such as MODIS and VIIRS satellites, but at the expense of a lower spatial resolution.      
### 40.A Receding Horizon Approach for Simultaneous Active Learning and Control using Gaussian Processes  [ :arrow_down: ](https://arxiv.org/pdf/2101.10351.pdf)
>  This paper proposes a receding horizon active learning and control problem for dynamical systems in which Gaussian Processes (GPs) are utilized to model the system dynamics. The active learning objective in the optimization problem is presented by the exact conditional differential entropy of GP predictions at multiple steps ahead, which is equivalent to the log determinant of the GP predictive covariance matrix. The resulting non-convex and complex optimization problem is solved by the Successive Convex Programming algorithm that exploits the first-order approximations of non-convex functions. Simulation results of an autonomous racing car example verify that using the proposed method can significantly improve data quality for model learning while solving time is highly promising for real-time applications.      
### 41.Robustness of Iteratively Pre-Conditioned Gradient-Descent Method: The Case of Distributed Linear Regression Problem  [ :arrow_down: ](https://arxiv.org/pdf/2101.10967.pdf)
>  This paper considers the problem of multi-agent distributed linear regression in the presence of system noises. In this problem, the system comprises multiple agents wherein each agent locally observes a set of data points, and the agents' goal is to compute a linear model that best fits the collective data points observed by all the agents. We consider a server-based distributed architecture where the agents interact with a common server to solve the problem; however, the server cannot access the agents' data points. We consider a practical scenario wherein the system either has observation noise, i.e., the data points observed by the agents are corrupted, or has process noise, i.e., the computations performed by the server and the agents are corrupted. In noise-free systems, the recently proposed distributed linear regression algorithm, named the Iteratively Pre-conditioned Gradient-descent (IPG) method, has been claimed to converge faster than related methods. In this paper, we study the robustness of the IPG method, against both the observation noise and the process noise. We empirically show that the robustness of the IPG method compares favorably to the state-of-the-art algorithms.      
### 42.RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated Content  [ :arrow_down: ](https://arxiv.org/pdf/2101.10955.pdf)
>  Blind or no-reference video quality assessment of user-generated content (UGC) has become a trending, challenging, unsolved problem. Accurate and efficient video quality predictors suitable for this content are thus in great demand to achieve more intelligent analysis and processing of UGC videos. Previous studies have shown that natural scene statistics and deep learning features are both sufficient to capture spatial distortions, which contribute to a significant aspect of UGC video quality issues. However, these models are either incapable or inefficient for predicting the quality of complex and diverse UGC videos in practical applications. Here we introduce an effective and efficient video quality model for UGC content, which we dub the Rapid and Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime. RAPIQUE combines and leverages the advantages of both quality-aware scene statistics features and semantics-aware deep convolutional features, allowing us to design the first general and efficient spatial and temporal (space-time) bandpass statistics model for video quality modeling. Our experimental results on recent large-scale UGC video quality databases show that RAPIQUE delivers top performances on all the datasets at a considerably lower computational expense. We hope this work promotes and inspires further efforts towards practical modeling of video quality problems for potential real-time and low-latency applications. To promote public usage, an implementation of RAPIQUE has been made freely available online: \url{<a class="link-external link-https" href="https://github.com/vztu/RAPIQUE" rel="external noopener nofollow">this https URL</a>}.      
### 43.Towards Universal Physical Attacks On Cascaded Camera-Lidar 3D Object Detection Models  [ :arrow_down: ](https://arxiv.org/pdf/2101.10747.pdf)
>  We propose a universal and physically realizable adversarial attack on a cascaded multi-modal deep learning network (DNN), in the context of self-driving cars. DNNs have achieved high performance in 3D object detection, but they are known to be vulnerable to adversarial attacks. These attacks have been heavily investigated in the RGB image domain and more recently in the point cloud domain, but rarely in both domains simultaneously - a gap to be filled in this paper. We use a single 3D mesh and differentiable rendering to explore how perturbing the mesh's geometry and texture can reduce the robustness of DNNs to adversarial attacks. We attack a prominent cascaded multi-modal DNN, the Frustum-Pointnet model. Using the popular KITTI benchmark, we showed that the proposed universal multi-modal attack was successful in reducing the model's ability to detect a car by nearly 73%. This work can aid in the understanding of what the cascaded RGB-point cloud DNN learns and its vulnerability to adversarial attacks.      
### 44.Privacy-preserving Channel Estimation in Cell-free Hybrid Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.10703.pdf)
>  We consider a cell-free hybrid massive multiple-input multiple-output (MIMO) system with $K$ users and $M$ access points (APs), each with $N_a$ antennas and $N_r&lt; N_a$ radio frequency (RF) chains. When $K\ll M{N_a}$, efficient uplink channel estimation and data detection with reduced number of pilots can be performed based on low-rank matrix completion. However, such a scheme requires the central processing unit (CPU) to collect received signals from all APs, which may enable the CPU to infer the private information of user locations. We therefore develop and analyze privacy-preserving channel estimation schemes under the framework of differential privacy (DP). As the key ingredient of the channel estimator, two joint differentially private noisy matrix completion algorithms based respectively on Frank-Wolfe iteration and singular value decomposition are presented. We provide an analysis on the tradeoff between the privacy and the channel estimation error. In particular, we show that the estimation error can be mitigated while maintaining the same privacy level by increasing the payload size with fixed pilot size; and the scaling laws of both the privacy-induced and privacy-independent error components in terms of payload size are characterized. Simulation results are provided to further demonstrate the tradeoff between privacy and channel estimation performance.      
### 45.Analytical Bounds for Dynamic Multi-Channel Discrimination  [ :arrow_down: ](https://arxiv.org/pdf/2101.10694.pdf)
>  The ability to precisely discriminate multiple quantum channels is fundamental to achieving quantum enhancements in data-readout, target detection, pattern recognition, and more. Optimal discrimination protocols often rely on entanglement shared between an incident probe and a protected idler-mode. While these protocols can be highly advantageous over classical ones, the storage of idler-modes is extremely challenging in practice. In this work, we explicitly define idler-free protocols based on the use of multipartite entangled probe states. We show that using non-disjoint distributions of multipartite probe states over multi-channels can be described by dynamic discrimination protocols. Analytical bounds for their error probabilities are derived for arbitrary pattern size, revealing idler-free protocols that display performance close to idler-assistance for powerful, near-term quantum sensing applications.      
### 46.Joint Estimation of Location and Scatter in Complex Elliptical Distributions: A robust semiparametric and computationally efficient $R$-estimator of the shape matrix  [ :arrow_down: ](https://arxiv.org/pdf/2101.10671.pdf)
>  The joint estimation of the location vector and the shape matrix of a set of independent and identically Complex Elliptically Symmetric (CES) distributed observations is investigated from both the theoretical and computational viewpoints. This joint estimation problem is framed in the original context of semiparametric models allowing us to handle the (generally unknown) density generator as an \textit{infinite-dimensional} nuisance parameter. In the first part of the paper, a computationally efficient and memory saving implementation of the robust and semiparmaetric efficient $R$-estimator for shape matrices is derived. Building upon this result, in the second part, a joint estimator, relying on the Tyler's $M$-estimator of location and on the $R$-estimator of shape matrix, is proposed and its Mean Squared Error (MSE) performance compared with the Semiparametric Cramér-Rao Bound (CSCRB).      
### 47.Ensembling complex network 'perspectives' for mild cognitive impairment detection with artificial neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.10629.pdf)
>  In this paper, we propose a novel method for mild cognitive impairment detection based on jointly exploiting the complex network and the neural network paradigm. In particular, the method is based on ensembling different brain structural "perspectives" with artificial neural networks. On one hand, these perspectives are obtained with complex network measures tailored to describe the altered brain connectivity. In turn, the brain reconstruction is obtained by combining diffusion-weighted imaging (DWI) data to tractography algorithms. On the other hand, artificial neural networks provide a means to learn a mapping from topological properties of the brain to the presence or absence of cognitive decline. The effectiveness of the method is studied on a well-known benchmark data set in order to evaluate if it can provide an automatic tool to support the early disease diagnosis. Also, the effects of balancing issues are investigated to further assess the reliability of the complex network approach to DWI data.      
### 48.On the distributions of some statistics related to adaptive filters trained with $t$-distributed samples  [ :arrow_down: ](https://arxiv.org/pdf/2101.10609.pdf)
>  In this paper we analyze the behavior of adaptive filters or detectors when they are trained with $t$-distributed samples rather than Gaussian distributed samples. More precisely we investigate the impact on the distribution of some relevant statistics including the signal to noise ratio loss and the Gaussian generalized likelihood ratio test. Some properties of partitioned complex $F$ distributed matrices are derived which enable to obtain statistical representations in terms of independent chi-square distributed random variables. These representations are compared with their Gaussian counterparts and numerical simulations illustrate and quantify the induced degradation.      
### 49.Robust and Secure Sum-Rate Maximization for Multiuser MISO Downlink Systems with Self-sustainable IRS  [ :arrow_down: ](https://arxiv.org/pdf/2101.10549.pdf)
>  This paper investigates robust and secure multiuser multiple-input single-output (MISO) downlink communications assisted by a self-sustainable intelligent reflection surface (IRS), which can simultaneously reflect and harvest energy from the received signals. We study the joint design of beamformers at an access point (AP) and the phase shifts as well as the energy harvesting schedule at the IRS for maximizing the system sum-rate. The design is formulated as a non-convex optimization problem taking into account the wireless energy harvesting capability of IRS elements, secure communications, and the robustness against the impact of channel state information (CSI) imperfection. Subsequently, we propose a computationally-efficient iterative algorithm to obtain a suboptimal solution to the design problem. In each iteration, S-procedure and the successive convex approximation are adopted to handle the intermediate optimization problem. Our simulation results unveil that: 1) there is a non-trivial trade-off between the system sum-rate and the self-sustainability of the IRS; 2) the performance gain achieved by the proposed scheme is saturated with a large number of energy harvesting IRS elements; 3) an IRS equipped with small bit-resolution discrete phase shifters is sufficient to achieve a considerable system sum-rate of the ideal case with continuous phase shifts.      
### 50.Comparison of Optimization Methods with Application to a Network Containing Malicious Agents  [ :arrow_down: ](https://arxiv.org/pdf/2101.10546.pdf)
>  There are different methods of solving unconstrained optimization problems, but there have been disparities in convergence speed for most of these methods. First-order methods such as the steepest descent method are very common in solving unconstrained problems but second-order methods such as the Newton-type methods enable faster convergence especially for quadratic functions. In this paper, We compare and analyse first and second order methods in solving two unconstrained problems that differ by a multiplicative perturbation parameter to show how malicious agents in a network can cause disruption. We also explore the advantages and disadvantages of the steepest descent, Newton, and Conjugate gradient methods in terms of their convergence attributes by comparing a strictly convex function with a banana-type function.      
### 51.Ear Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2101.10540.pdf)
>  Ear recognition can be described as a revived scientific field. Ear biometrics were long believed to not be accurate enough and held a secondary place in scientific research, being seen as only complementary to other types of biometrics, due to difficulties in measuring correctly the ear characteristics and the potential occlusion of the ear by hair, clothes and ear jewellery. However, recent research has reinstated them as a vivid research field, after having addressed these problems and proven that ear biometrics can provide really accurate identification and verification results. Several 2D and 3D imaging techniques, as well as acoustical techniques using sound emission and reflection, have been developed and studied for ear recognition, while there have also been significant advances towards a fully automated recognition of the ear. Furthermore, ear biometrics have been proven to be mostly non-invasive, adequately permanent and accurate, and hard to spoof and counterfeit. Moreover, different ear recognition techniques have proven to be as effective as face recognition ones, thus providing the opportunity for ear recognition to be used in identification and verification applications. Finally, even though some issues still remain open and require further research, the scientific field of ear biometrics has proven to be not only viable, but really thriving.      
### 52.Hyperspectral Image Classification: Artifacts of Dimension Reduction on Hybrid CNN  [ :arrow_down: ](https://arxiv.org/pdf/2101.10532.pdf)
>  Convolutional Neural Networks (CNN) has been extensively studied for Hyperspectral Image Classification (HSIC) more specifically, 2D and 3D CNN models have proved highly efficient in exploiting the spatial and spectral information of Hyperspectral Images. However, 2D CNN only considers the spatial information and ignores the spectral information whereas 3D CNN jointly exploits spatial-spectral information at a high computational cost. Therefore, this work proposed a lightweight CNN (3D followed by 2D-CNN) model which significantly reduces the computational cost by distributing spatial-spectral feature extraction across a lighter model alongside a preprocessing that has been carried out to improve the classification results. Five benchmark Hyperspectral datasets (i.e., SalinasA, Salinas, Indian Pines, Pavia University, Pavia Center, and Botswana) are used for experimental evaluation. The experimental results show that the proposed pipeline outperformed in terms of generalization performance, statistical significance, and computational complexity, as compared to the state-of-the-art 2D/3D CNN models except commonly used computationally expensive design choices.      
### 53.ADMM-based Adaptive Sampling Strategy for Nonholonomic Mobile Robotic Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.10500.pdf)
>  This paper discusses the adaptive sampling problem in a nonholonomic mobile robotic sensor network for efficiently monitoring a spatial field. It is proposed to employ Gaussian process to model a spatial phenomenon and predict it at unmeasured positions, which enables the sampling optimization problem to be formulated by the use of the log determinant of a predicted covariance matrix at next sampling locations. The control, movement and nonholonomic dynamics constraints of the mobile sensors are also considered in the adaptive sampling optimization problem. In order to tackle the nonlinearity and nonconvexity of the objective function in the optimization problem we first exploit the linearized alternating direction method of multipliers (L-ADMM) method that can effectively simplify the objective function, though it is computationally expensive since a nonconvex problem needs to be solved exactly in each iteration. We then propose a novel approach called the successive convexified ADMM (SC-ADMM) that sequentially convexify the nonlinear dynamic constraints so that the original optimization problem can be split into convex subproblems. It is noted that both the L-ADMM algorithm and our SC-ADMM approach can solve the sampling optimization problem in either a centralized or a distributed manner. We validated the proposed approaches in 1000 experiments in a synthetic environment with a real-world dataset, where the obtained results suggest that both the L-ADMM and SC- ADMM techniques can provide good accuracy for the monitoring purpose. However, our proposed SC-ADMM approach computationally outperforms the L-ADMM counterpart, demonstrating its better practicality.      
### 54.DNC-Aided SCL-Flip Decoding of Polar Codes  [ :arrow_down: ](https://arxiv.org/pdf/2101.10498.pdf)
>  Successive-cancellation list (SCL) decoding of polar codes is promising towards practical adoptions. However, the performance is not satisfactory with moderate code length. Variety of flip algorithms are developed to solve this problem. The key for successful flip is to accurately identify error bit positions. However, state-of-the-art flip strategies, including heuristic and deep-learning-aided (DL-aided) approaches, are not effective in handling long-distance dependencies in sequential SCL decoding. In this work, we propose a new DNC-aided flip decoding with differentiable neural computer (DNC). New action and state encoding are developed for better training and inference efficiency. The proposed method consists of two phases: i) a flip DNC (F-DNC) is exploited to rank most likely flip positions for multi-bit flipping; ii) if multi-bit flipping fails, a flip-validate DNC (FV-DNC) is used to re-select error position and assist single-bit flipping successively. Training methods are designed accordingly for the two DNCs. Simulation results show that proposed DNC-aided SCL-Flip (DNC-SCLF) decoding can effectively improve the error-correction performance and reduce number of decoding attempts compared to prior works.      
### 55.Real-time Non-line-of-sight Imaging with Two-step Deep Remapping  [ :arrow_down: ](https://arxiv.org/pdf/2101.10492.pdf)
>  Conventional imaging only records the photons directly sent from the object to the detector, while non-line-of-sight (NLOS) imaging takes the indirect light into account. To explore the NLOS surroundings, most NLOS solutions employ a transient scanning process, followed by a back-projection based algorithm to reconstruct the NLOS scenes. However, the transient detection requires sophisticated apparatus, with long scanning time and low robustness to ambient environment, and the reconstruction algorithms typically cost tens of minutes with high demand on memory and computational resources. Here we propose a new NLOS solution to address the above defects, with innovations on both detection equipment and reconstruction algorithm. We apply inexpensive commercial Lidar for detection, with much higher scanning speed and better compatibility to real-world imaging tasks. Our reconstruction framework is deep learning based, consisting of a variational autoencoder and a compression neural network. The generative feature and the two-step reconstruction strategy of the framework guarantee high fidelity of NLOS imaging. The overall detection and reconstruction process allows for real-time responses, with state-of-the-art reconstruction performance. We have experimentally tested the proposed solution on both a synthetic dataset and real objects, and further demonstrated our method to be applicable for full-color NLOS imaging.      
### 56.Classification of Schizophrenia from Functional MRI Using Large-scale Extended Granger Causality  [ :arrow_down: ](https://arxiv.org/pdf/2101.10471.pdf)
>  The literature manifests that schizophrenia is associated with alterations in brain network connectivity. We investigate whether large-scale Extended Granger Causality (lsXGC) can capture such alterations using resting-state fMRI data. Our method utilizes dimension reduction combined with the augmentation of source time-series in a predictive time-series model for estimating directed causal relationships among fMRI time-series. The lsXGC is a multivariate approach since it identifies the relationship of the underlying dynamic system in the presence of all other time-series. Here lsXGC serves as a biomarker for classifying schizophrenia patients from typical controls using a subset of 62 subjects from the Centers of Biomedical Research Excellence (COBRE) data repository. We use brain connections estimated by lsXGC as features for classification. After feature extraction, we perform feature selection by Kendall's tau rank correlation coefficient followed by classification using a support vector machine. As a reference method, we compare our results with cross-correlation, typically used in the literature as a standard measure of functional connectivity. We cross-validate 100 different training/test (90%/10%) data split to obtain mean accuracy and a mean Area Under the receiver operating characteristic Curve (AUC) across all tested numbers of features for lsXGC. Our results demonstrate a mean accuracy range of [0.767, 0.940] and a mean AUC range of [0.861, 0.983] for lsXGC. The result of lsXGC is significantly higher than the results obtained with the cross-correlation, namely mean accuracy of [0.721, 0.751] and mean AUC of [0.744, 0.860]. Our results suggest the applicability of lsXGC as a potential biomarker for schizophrenia.      
### 57.Machine Learning Coupled Trajectory and Communication Design for UAV-Facilitated Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.10454.pdf)
>  Augmenting wireless networks with Unmanned Aerial Vehicles (UAVs), commonly referred to as drones, offers a promising avenue for providing reliable, cost-effective, and on-demand wireless services to desired areas. However, existing UAV communication and trajectory schemes are inefficient as they assume limited drone mobility and static transmission power. Furthermore, they tend to rely upon convex approximations to highly non-linear functions and fail to adopt a combination of heuristic and convex methods. This paper considers a Multi-UAV system where UAV-mounted mobile base stations serve users on the ground. An iterative approach using block gradient descent is used to jointly optimize user scheduling, UAV trajectories, and transmission power for maximizing throughput over all users. Subsequently, an innovative technique for initial trajectory predictions was developed using a K-means clustering algorithm for partitioning users into subgroups and a genetic algorithm for initializing shortest flight paths within clusters. Finally, convex optimization solvers such as MATLAB's Fmincon are used for fine-tuning parameters. Extensive simulation and optimization results demonstrate a 33.57%, 87.4%, and 53.2% increase in system throughput for the 1, 2, and 3 UAV scenarios respectively when compared to existing trajectory and communication design schemes. Furthermore, the K-means and genetic algorithm reveal additional improvements in throughput by around 15%. Our results note diminished increases in throughput for increases in UAV trajectory period as the period approaches higher values. Further research into joint adoption of convex and non-convex schemes as well as consideration of environment-dependent channel models would allow for a faster and more optimal deployment of UAVs.      
### 58.Environment-Adaptive Multiple Access for Distributed V2X Network: A Reinforcement Learning Framework  [ :arrow_down: ](https://arxiv.org/pdf/2101.10447.pdf)
>  The huge research interest in cellular vehicle-to-everything (C-V2X) communications in recent days is attributed to their ability to schedule multiple access more efficiently as compared to its predecessor technology, i.e., dedicated short-range communications (DSRC). However, one of the foremost issues still remaining is the need for the V2X to operate stably in a highly dynamic environment. This paper proposes a way to exploit the dynamicity. That is, we propose a resource allocation mechanism adaptive to the environment, which can be an efficient solution for air interface congestion that a V2X network often suffers from. Specifically, the proposed mechanism aims at granting a higher chance of transmission to a vehicle with a higher crash risk. As such, the channel access is prioritized to those with urgent needs. The proposed framework is established based on reinforcement learning (RL), which is modeled as a contextual multi-armed bandit (MAB). Importantly, the framework is designed to operate at a vehicle autonomously without any assistance from a central entity, which, henceforth, is expected to make a particular fit to distributed V2X network such as C-V2X mode 4.      
### 59.Dairy Cow rumination detection: A deep learning approach  [ :arrow_down: ](https://arxiv.org/pdf/2101.10445.pdf)
>  Cattle activity is an essential index for monitoring health and welfare of the ruminants. Thus, changes in the livestock behavior are a critical indicator for early detection and prevention of several diseases. Rumination behavior is a significant variable for tracking the development and yield of animal husbandry. Therefore, various monitoring methods and measurement equipment have been used to assess cattle behavior. However, these modern attached devices are invasive, stressful and uncomfortable for the cattle and can influence negatively the welfare and diurnal behavior of the animal. Multiple research efforts addressed the problem of rumination detection by adopting new methods by relying on visual features. However, they only use few postures of the dairy cow to recognize the rumination or feeding behavior. In this study, we introduce an innovative monitoring method using Convolution Neural Network (CNN)-based deep learning models. The classification process is conducted under two main labels: ruminating and other, using all cow postures captured by the monitoring camera. Our proposed system is simple and easy-to-use which is able to capture long-term dynamics using a compacted representation of a video in a single 2D image. This method proved efficiency in recognizing the rumination behavior with 95%, 98% and 98% of average accuracy, recall and precision, respectively.      
### 60.GnetSeg: Semantic Segmentation Model Optimized on a 224mW CNN Accelerator Chip at the Speed of 318FPS  [ :arrow_down: ](https://arxiv.org/pdf/2101.10444.pdf)
>  Semantic segmentation is the task to cluster pixels on an image belonging to the same class. It is widely used in the real-world applications including autonomous driving, medical imaging analysis, industrial inspection, smartphone camera for person segmentation and so on. Accelerating the semantic segmentation models on the mobile and edge devices are practical needs for the industry. Recent years have witnessed the wide availability of CNN (Convolutional Neural Networks) accelerators. They have the advantages on power efficiency, inference speed, which are ideal for accelerating the semantic segmentation models on the edge devices. However, the CNN accelerator chips also have the limitations on flexibility and memory. In addition, the CPU load is very critical because the CNN accelerator chip works as a co-processor with a host CPU. In this paper, we optimize the semantic segmentation model in order to fully utilize the limited memory and the supported operators on the CNN accelerator chips, and at the same time reduce the CPU load of the CNN model to zero. The resulting model is called GnetSeg. Furthermore, we propose the integer encoding for the mask of the GnetSeg model, which minimizes the latency of data transfer between the CNN accelerator and the host CPU. The experimental result shows that the model running on the 224mW chip achieves the speed of 318FPS with excellent accuracy for applications such as person segmentation.      
### 61.A metric for evaluating 3D reconstruction and mapping performance with no ground truthing  [ :arrow_down: ](https://arxiv.org/pdf/2101.10402.pdf)
>  It is not easy when evaluating 3D mapping performance because existing metrics require ground truth data that can only be collected with special instruments. In this paper, we propose a metric, dense map posterior (DMP), for this evaluation. It can work without any ground truth data. Instead, it calculates a comparable value, reflecting a map posterior probability, from dense point cloud observations. In our experiments, the proposed DMP is benchmarked against ground truth-based metrics. Results show that DMP can provide a similar evaluation capability. The proposed metric makes evaluating different methods more flexible and opens many new possibilities, such as self-supervised methods and more available datasets.      
### 62.Learning to falsify automated driving vehicles with prior knowledge  [ :arrow_down: ](https://arxiv.org/pdf/2101.10377.pdf)
>  While automated driving technology has achieved a tremendous progress, the scalable and rigorous testing and verification of safe automated and autonomous driving vehicles remain challenging. This paper proposes a learning-based falsification framework for testing the implementation of an automated or self-driving function in simulation. We assume that the function specification is associated with a violation metric on possible scenarios. Prior knowledge is incorporated to limit the scenario parameter variance and in a model-based falsifier to guide and improve the learning process. For an exemplary adaptive cruise controller, the presented framework yields non-trivial falsifying scenarios with higher reward, compared to scenarios obtained by purely learning-based or purely model-based falsification approaches.      
### 63.Enhanced Normalized Conjugate Beamforming for Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2101.10363.pdf)
>  In cell-free massive multiple-input multiple-output (MIMO) the fluctuations of the channel gain from the access points to a user are large due to the distributed topology of the system. Because of these fluctuations, data decoding schemes that treat the channel as deterministic perform inefficiently. A way to reduce the channel fluctuations is to design a precoding scheme that equalizes the effective channel gain seen by the users. Conjugate beamforming (CB) poorly contributes to harden the effective channel at the users. In this work, we propose a variant of CB dubbed enhanced normalized CB (ECB), in that the precoding vector consists of the conjugate of the channel estimate normalized by its squared norm. For this scheme, we derive an exact closed-form expression for an achievable downlink spectral efficiency (SE), accounting for channel estimation errors, pilot reuse and user's lack of channel state information (CSI), assuming independent Rayleigh fading channels. We also devise an optimal max-min fairness power allocation based only on large-scale fading quantities. ECB greatly boosts the channel hardening enabling the users to reliably decode data relying only on statistical CSI. As the provided effective channel is nearly deterministic, acquiring CSI at the users does not yield a significant gain.      
### 64.Regret-Optimal Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2101.10357.pdf)
>  We consider the problem of filtering in linear state-space models (e.g., the Kalman filter setting) through the lens of regret optimization. Different assumptions on the driving disturbance and the observation noise sequences give rise to different estimators: in the stochastic setting to the celebrated Kalman filter, and in the deterministic setting of bounded energy disturbances to $H_\infty$ estimators. In this work, we formulate a novel criterion for filter design based on the concept of regret between the estimation error energy of a clairvoyant estimator that has access to all future observations (a so-called smoother) and a causal one that only has access to current and past observations. The regret-optimal estimator is chosen to minimize this worst-case difference across all bounded-energy noise sequences. The resulting estimator is adaptive in the sense that it aims to mimic the behavior of the clairvoyant estimator, irrespective of what the realization of the noise will be and thus interpolates between the stochastic and deterministic approaches. We provide a solution for the regret estimation problem at two different levels. First, we provide a solution at the operator level by reducing it to the Nehari problem. Second, for state-space models, we explicitly find the estimator that achieves the optimal regret. From a computational perspective, the regret-optimal estimator can be easily implemented by solving three Riccati equations and a single Lyapunov equation. For a state-space model of dimension $n$, the regret-optimal estimator has a state-space structure of dimension $3n$. We demonstrate the applicability and efficacy of the estimator in a variety of problems and observe that the estimator has average and worst-case performances that are simultaneously close to their optimal values. We therefore argue that regret-optimality is a viable approach to estimator design.      
