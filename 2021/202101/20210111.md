# ArXiv eess --Mon, 11 Jan 2021
### 1.Impact of High PV Penetration on the Inter-area Oscillations in the U.S. Eastern Interconnection  [ :arrow_down: ](https://arxiv.org/pdf/2101.03159.pdf)
>  This study explores the impact of high PV penetration on the inter-area oscillation modes of large-scale power grids. A series of dynamic models with various PV penetration levels are developed based on a detailed model representing the U.S. Eastern Interconnection (EI). Transient simulations are performed to investigate the change of inter-area oscillation modes with PV penetration. The impact of PV control strategies and parameter settings on inter-area oscillations is studied. This study finds that as PV increases, the damping of the dominant oscillation mode decreases monotonically. It is also observed that the mode shape varies with the PV control strategy and new oscillation modes may emerge under inappropriate parameter settings in PV plant controls.      
### 2.GRAPPA-GANs for Parallel MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2101.03135.pdf)
>  k-space undersampling is a standard technique to accelerate MR image acquisitions. Reconstruction techniques including GeneRalized Autocalibrating Partial Parallel Acquisition(GRAPPA) and its variants are utilized extensively in clinical and research settings. A reconstruction model combining GRAPPA with a conditional generative adversarial network (GAN) was developed and tested on multi-coil human brain images from the fastMRI dataset. For various acceleration rates, GAN and GRAPPA reconstructions were compared in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). For an acceleration rate of R=4, PSNR improved from 33.88 using regularized GRAPPA to 37.65 using GAN. GAN consistently outperformed GRAPPA for various acceleration rates.      
### 3.Explainable Systematic Analysis for Synthetic Aperture Sonar Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2101.03134.pdf)
>  In this work, we present an in-depth and systematic analysis using tools such as local interpretable model-agnostic explanations (LIME) (<a class="link-https" data-arxiv-id="1602.04938" href="https://arxiv.org/abs/1602.04938">arXiv:1602.04938</a>) and divergence measures to analyze what changes lead to improvement in performance in fine tuned models for synthetic aperture sonar (SAS) data. We examine the sensitivity to factors in the fine tuning process such as class imbalance. Our findings show not only an improvement in seafloor texture classification, but also provide greater insight into what features play critical roles in improving performance as well as a knowledge of the importance of balanced data for fine tuning deep learning models for seafloor classification in SAS imagery.      
### 4.Knowledge AI: New Medical AI Solution for Medical image Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2101.03063.pdf)
>  The implementation of medical AI has always been a problem. The effect of traditional perceptual AI algorithm in medical image processing needs to be improved. Here we propose a method of knowledge AI, which is a combination of perceptual AI and clinical knowledge and experience. Based on this method, the geometric information mining of medical images can represent the experience and information and evaluate the quality of medical images.      
### 5.A review for Tone-mapping Operators on Wide Dynamic Range Image  [ :arrow_down: ](https://arxiv.org/pdf/2101.03003.pdf)
>  The dynamic range of our normal life can exceeds 120 dB, however, the smart-phone cameras and the conventional digital cameras can only capture a dynamic range of 90 dB, which sometimes leads to loss of details for the recorded image. Now, some professional hardware applications and image fusion algorithms have been devised to take wide dynamic range (WDR), but unfortunately existing devices cannot display WDR image. Tone mapping (TM) thus becomes an essential step for exhibiting WDR image on our ordinary screens, which convert the WDR image into low dynamic range (LDR) image. More and more researchers are focusing on this topic, and give their efforts to design an excellent tone mapping operator (TMO), showing detailed images as the same as the perception that human eyes could receive. Therefore, it is important for us to know the history, development, and trend of TM before proposing a practicable TMO. In this paper, we present a comprehensive study of the most well-known TMOs, which divides TMOs into traditional and machine learning-based category.      
### 6.Necessary and Sufficient Conditions for Harmonic Control in Continuous Time  [ :arrow_down: ](https://arxiv.org/pdf/2101.02987.pdf)
>  In this paper, we revisit the concepts and tools of harmonic analysis and control and provide a rigorous mathematical answer to the following question: when does an harmonic control has a representative in the time domain ? By representative we mean a control in the time domain that leads by sliding Fourier decomposition to exactly the same harmonic control. Harmonic controls that do not have such representatives lead to erroneous results in practice. The main results of this paper are: a one-to-one correspondence between ad hoc functional spaces guaranteeing the existence of a representative, a strict equivalence between the Carath{Ã©}orody solutions of a differential system and the solutions of the associated harmonic differential model, and as a consequence, a general harmonic framework for Linear Time Periodic (LTP) systems and bilinear affine systems. The proposed framework allows to design globally stabilizing harmonic control laws. We illustrate the proposed approach on a single-phase rectifier bridge. Through this example, we show how one can design stabilizing control laws that guarantee periodic disturbance rejection and low harmonic content.      
### 7.Application of Machine Learning to Performance Assessment for a class of PID-based Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.02939.pdf)
>  In this paper, a novel machine learning derived control performance assesment (CPA) classification system is proposed. It is dedicated for a class of PID-based control loops with processes exhibiting second order plus delay time (SOPDT) dynamical properties. The proposed concept is based on deriving and combining a number of different, diverse control performance indices (CPIs) that separately do not provide sufficient information about the control performance. However, when combined together and used as discriminative features of the assessed control system, they can provide consistent and accurate CPA information. This concept is discussed in terms of the introduced extended set of CPIs, comprehensive performance assessment of different machine learning based classification methods and practical applicability of the suggested solution. The latter is shown and verified by practical application of the proposed approach to a CPA system for a laboratory heat exchange and ditribution setup.      
### 8.An Open Source Power System Simulator in Python for Efficient Prototyping of WAMPAC Applications  [ :arrow_down: ](https://arxiv.org/pdf/2101.02937.pdf)
>  An open source software package for performing dynamic RMS simulation of small to medium-sized power systems is presented, written entirely in the Python programming language. The main objective is to facilitate fast prototyping of new wide area monitoring, control and protection applications for the future power system by enabling seamless integration with other tools available for Python in the open source community, e.g. for signal processing, artificial intelligence, communication protocols etc. The focus is thus transparency and expandability rather than computational efficiency and performance. The main purpose of this paper, besides presenting the code and some results, is to share interesting experiences with the power system community, and thus stimulate wider use and further development. Two interesting conclusions at the current stage of development are as follows: First, the simulation code is fast enough to emulate real-time simulation for small and medium-size grids with a time step of 5 ms, and allows for interactive feedback from the user during the simulation. Second, the simulation code can be uploaded to an online Python interpreter, edited, run and shared with anyone with a compatible internet browser. Based on this, we believe that the presented simulation code could be a valuable tool, both for researchers in early stages of prototyping real-time applications, and in the educational setting, for students developing intuition for concepts and phenomena through real-time interaction with a running power system model.      
### 9.Real-Time Distributed Automation Of Road Intersections  [ :arrow_down: ](https://arxiv.org/pdf/2101.02925.pdf)
>  The topic of this paper is the design of a fully distributed and real-time capable control scheme for the automation of road intersections. State of the art Vehicle-to-Vehicle (V2V) communication technology is adopted. Vehicles distributively negotiate crossing priorities by a Consensus-Based Auction Algorithm (CBAA-M). Then, each agent solves a nonlinear Model Predictive Control (MPC) problem that computes the optimal trajectory avoiding collisions with higher priority vehicles and deciding the crossing order. The scheme is shown to be real-time capable and able to respond to sudden priority changes, e.g. if a vehicle gets an emergency call. Simulations reinforce theoretical results.      
### 10.Predicting Semen Motility using three-dimensional Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.02888.pdf)
>  Manual and computer aided methods to perform semen analysis are time-consuming, requires extensive training and prone to human error. The use of classical machine learning and deep learning based methods using videos to perform semen analysis have yielded good results. The state-of-the-art method uses regular convolutional neural networks to perform quality assessments on a video of the provided sample. In this paper we propose an improved deep learning based approach using three-dimensional convolutional neural networks to predict sperm motility from microscopic videos of the semen sample. We make use of the VISEM dataset that consists of video and tabular data of semen samples collected from 85 participants. We were able to achieve good results from significantly less data points. Our models indicate that deep learning based automatic semen analysis may become a valuable and effective tool in fertility and IVF labs.      
### 11.Nash Equilibrium Seeking for High-order Multi-agent Systems with Unknown Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2101.02883.pdf)
>  In this paper, we consider a Nash equilibrium seeking problem for a class of high-order multi-agent systems with unknown dynamics. Different from existing results for single integrators, we aim to steer the outputs of this class of uncertain high-order agents to the Nash equilibrium of some noncooperative game in a distributed manner. To overcome the difficulties brought by the high-order structure, unknown nonlinearities, and the regulation requirement, we first introduce a virtual player for each agent and solve an auxiliary noncooperative game for them. Then, we develop a distributed adaptive protocol by embedding this auxiliary game dynamics into some proper tracking controller for the original agent to resolve this problem. We also discuss the parameter convergence problem under certain persistence of excitation condition. The efficacy of our algorithms is verified by numerical examples.      
### 12.Deep Convolutional Neural Network based Classification of Alzheimer's Disease using MRI data  [ :arrow_down: ](https://arxiv.org/pdf/2101.02876.pdf)
>  Alzheimer's disease (AD) is a progressive and incurable neurodegenerative disease which destroys brain cells and causes loss to patient's memory. An early detection can prevent the patient from further damage of the brain cells and hence avoid permanent memory loss. In past few years, various automatic tools and techniques have been proposed for diagnosis of AD. Several methods focus on fast, accurate and early detection of the disease to minimize the loss to patients mental health. Although machine learning and deep learning techniques have significantly improved medical imaging systems for AD by providing diagnostic performance close to human level. But the main problem faced during multi-class classification is the presence of highly correlated features in the brain structure. In this paper, we have proposed a smart and accurate way of diagnosing AD based on a two-dimensional deep convolutional neural network (2D-DCNN) using imbalanced three-dimensional MRI dataset. Experimental results on Alzheimer Disease Neuroimaging Initiative magnetic resonance imaging (MRI) dataset confirms that the proposed 2D-DCNN model is superior in terms of accuracy, efficiency, and robustness. The model classifies MRI into three categories: AD, mild cognitive impairment, and normal control: and has achieved 99.89% classification accuracy with imbalanced classes. The proposed model exhibits noticeable improvement in accuracy as compared to the state-fo-the-art methods.      
### 13.FENet: A Frequency Extraction Network for Obstructive Sleep Apnea Detection  [ :arrow_down: ](https://arxiv.org/pdf/2101.02873.pdf)
>  Obstructive Sleep Apnea (OSA) is a highly prevalent but inconspicuous disease that seriously jeopardizes the health of human beings. Polysomnography (PSG), the gold standard of detecting OSA, requires multiple specialized sensors for signal collection, hence patients have to physically visit hospitals and bear the costly treatment for a single detection. Recently, many single-sensor alternatives have been proposed to improve the cost efficiency and convenience. Among these methods, solutions based on RR-interval (i.e., the interval between two consecutive pulses) signals reach a satisfactory balance among comfort, portability and detection accuracy. In this paper, we advance RR-interval based OSA detection by considering its real-world practicality from energy perspectives. As photoplethysmogram (PPG) pulse sensors are commonly equipped on smart wrist-worn wearable devices (e.g., smart watches and wristbands), the energy efficiency of the detection model is crucial to fully support an overnight observation on patients. This creates challenges as the PPG sensors are unable to keep collecting continuous signals due to the limited battery capacity on smart wrist-worn devices. Therefore, we propose a novel Frequency Extraction Network (FENet), which can extract features from different frequency bands of the input RR-interval signals and generate continuous detection results with downsampled, discontinuous RR-interval signals. With the help of the one-to-multiple structure, FENet requires only one-third of the operation time of the PPG sensor, thus sharply cutting down the energy consumption and enabling overnight diagnosis. Experimental results on real OSA datasets reveal the state-of-the-art performance of FENet.      
### 14.Disturbance Observer  [ :arrow_down: ](https://arxiv.org/pdf/2101.02859.pdf)
>  Disturbance observer is an inner-loop output-feedback controller whose role is to reject external disturbances and to make the outer-loop baseline controller robust against plant's uncertainties. Therefore, the closed-loop system with the DOB approximates the nominal closed-loop by the baseline controller and the nominal plant model with no disturbances. This article presents how the disturbance observer works under what conditions, and how one can design a disturbance observer to guarantee robust stability and to recover the nominal performance not only in the steady-state but also for the transient response under large uncertainty and disturbance.      
### 15.Learning Low-Correlation GPS Spreading Codes with a Policy Gradient Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2101.02850.pdf)
>  With the birth of the next-generation GPS III constellation and the upcoming launch of the Navigation Technology Satellite-3 (NTS-3) testing platform to explore future technologies for GPS, we are indeed entering a new era of satellite navigation. Correspondingly, it is time to revisit the design methods of the GPS spreading code families. In this work, we develop a Gaussian policy gradient-based reinforcement learning algorithm which constructs high-quality families of spreading code sequences. We demonstrate the ability of our algorithm to achieve better mean-squared auto- and cross-correlation than well-chosen families of equal-length Gold codes and Weil codes. Furthermore, we compare our algorithm with an analogous genetic algorithm implementation assigned the same code evaluation metric. To the best of the authors' knowledge, this is the first work to explore using a machine learning / reinforcement learning approach to design navigation spreading codes.      
### 16.Distributionally Consistent Simulation of Naturalistic Driving Environment for Autonomous Vehicle Testing  [ :arrow_down: ](https://arxiv.org/pdf/2101.02828.pdf)
>  Microscopic traffic simulation provides a controllable, repeatable, and efficient testing environment for autonomous vehicles (AVs). To evaluate AVs' safety performance unbiasedly, ideally, the probability distributions of the joint state space of all vehicles in the simulated naturalistic driving environment (NDE) needs to be consistent with those from the real-world driving environment. However, although human driving behaviors have been extensively investigated in the transportation engineering field, most existing models were developed for traffic flow analysis without consideration of distributional consistency of driving behaviors, which may cause significant evaluation biasedness for AV testing. To fill this research gap, a distributionally consistent NDE modeling framework is proposed. Using large-scale naturalistic driving data, empirical distributions are obtained to construct the stochastic human driving behavior models under different conditions, which serve as the basic behavior models. To reduce the model errors caused by the limited data quantity and mitigate the error accumulation problem during the simulation, an optimization framework is designed to further enhance the basic models. Specifically, the vehicle state evolution is modeled as a Markov chain and its stationary distribution is twisted to match the distribution from the real-world driving environment. In the case study of highway driving environment using real-world naturalistic driving data, the distributional accuracy of the generated NDE is validated. The generated NDE is further utilized to test the safety performance of an AV model to validate its effectiveness.      
### 17.Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images  [ :arrow_down: ](https://arxiv.org/pdf/2101.02824.pdf)
>  In the last few years, image denoising has benefited a lot from the fast development of neural networks. However, the requirement of large amounts of noisy-clean image pairs for supervision limits the wide use of these models. Although there have been a few attempts in training an image denoising model with only single noisy images, existing self-supervised denoising approaches suffer from inefficient network training, loss of useful information, or dependence on noise modeling. In this paper, we present a very simple yet effective method named Neighbor2Neighbor to train an effective image denoising model with only noisy images. Firstly, a random neighbor down-sampler is proposed for the generation of training image pairs. In detail, input and target used to train a network are images down-sampled from the same noisy image, satisfying the requirement that paired pixels of paired images are neighbors and have very similar appearance with each other. Secondly, a denoising network is trained on down-sampled training pairs generated in the first stage, with a proposed regularizer as additional loss for better performance. The proposed Neighbor2Neighbor framework is able to enjoy the progress of state-of-the-art supervised denoising networks in network architecture design. Moreover, it avoids heavy dependence on the assumption of the noise distribution. We explain our approach from a theoretical perspective and further validate it through extensive experiments, including synthetic experiments with different noise distributions in sRGB space and real-world experiments on a denoising benchmark dataset in raw-RGB space.      
### 18.Single Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2101.02802.pdf)
>  This study presents a chronological overview of the single image super-resolution problem. We first define the problem thoroughly and mention some of the serious challenges. Then the problem formulation and the performance metrics are defined. We give an overview of the previous methods relying on reconstruction based solutions and then continue with the deep learning approaches. We pick 3 landmark architectures and present their results quantitatively. We see that the latest proposed network gives favorable output compared to the previous methods.      
### 19.Secrecy Rate Maximization for Hardware Impaired Untrusted Relaying Network with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.02749.pdf)
>  This paper investigates the physical layer security design of an untrusted relaying network where the source node coexists with a multi-antenna eavesdropper (Eve). While the communication relies on untrustworthy relay nodes to increase reliability, we aim to protect the confidentiality of information against combined eavesdropping attacks performed by both untrusted relay nodes and Eve. Taking into account the hardware impairments, and power budget constraints, this paper presents a novel approach to jointly optimize relay beamformer and transmit powers aimed at maximizing average secrecy rate (ASR). The resultant optimization problem is non-convex, and a suboptimal solution is obtained through the sequential parametric convex approximation (SPCA) method. In order to prevent any failure due to infeasibility, we propose an iterative initialization algorithm to find the feasible initial point of the original problem. To satisfy low-latency as one of the main key performance indicators (KPI) required in beyond 5G (B5G) communications, a computationally efficient data-driven approach is developed exploiting a deep learning model to improve the ASR while the computational burden is significantly reduced. Simulation results assess the effect of different system parameters on the ASR performance as well as the effectiveness of the proposed deep learning solution in large-scale cases.      
### 20.Learning Guided Electron Microscopy with Active Acquisition  [ :arrow_down: ](https://arxiv.org/pdf/2101.02746.pdf)
>  Single-beam scanning electron microscopes (SEM) are widely used to acquire massive data sets for biomedical study, material analysis, and fabrication inspection. Datasets are typically acquired with uniform acquisition: applying the electron beam with the same power and duration to all image pixels, even if there is great variety in the pixels' importance for eventual use. Many SEMs are now able to move the beam to any pixel in the field of view without delay, enabling them, in principle, to invest their time budget more effectively with non-uniform imaging. <br>In this paper, we show how to use deep learning to accelerate and optimize single-beam SEM acquisition of images. Our algorithm rapidly collects an information-lossy image (e.g. low resolution) and then applies a novel learning method to identify a small subset of pixels to be collected at higher resolution based on a trade-off between the saliency and spatial diversity. We demonstrate the efficacy of this novel technique for active acquisition by speeding up the task of collecting connectomic datasets for neurobiology by up to an order of magnitude.      
### 21.VisualVoice: Audio-Visual Speech Separation with Cross-Modal Consistency  [ :arrow_down: ](https://arxiv.org/pdf/2101.03149.pdf)
>  We introduce a new approach for audio-visual speech separation. Given a video, the goal is to extract the speech associated with a face in spite of simultaneous background sounds and/or other human speakers. Whereas existing methods focus on learning the alignment between the speaker's lip movements and the sounds they generate, we propose to leverage the speaker's face appearance as an additional prior to isolate the corresponding vocal qualities they are likely to produce. Our approach jointly learns audio-visual speech separation and cross-modal speaker embeddings from unlabeled video. It yields state-of-the-art results on five benchmark datasets for audio-visual speech separation and enhancement, and generalizes well to challenging real-world videos of diverse scenarios. Our video results and code: <a class="link-external link-http" href="http://vision.cs.utexas.edu/projects/VisualVoice/" rel="external noopener nofollow">this http URL</a>.      
### 22.User-friendly automatic transcription of low-resource languages: Plugging ESPnet into Elpis  [ :arrow_down: ](https://arxiv.org/pdf/2101.03027.pdf)
>  This paper reports on progress integrating the speech recognition toolkit ESPnet into Elpis, a web front-end originally designed to provide access to the Kaldi automatic speech recognition toolkit. The goal of this work is to make end-to-end speech recognition models available to language workers via a user-friendly graphical interface. Encouraging results are reported on (i) development of an ESPnet recipe for use in Elpis, with preliminary results on data sets previously used for training acoustic models with the Persephone toolkit along with a new data set that had not previously been used in speech recognition, and (ii) incorporating ESPnet into Elpis along with UI enhancements and a CUDA-supported Dockerfile.      
### 23.On the Turnpike to Design of Deep Neural Nets: Explicit Depth Bounds  [ :arrow_down: ](https://arxiv.org/pdf/2101.03000.pdf)
>  It is well-known that the training of Deep Neural Networks (DNN) can be formalized in the language of optimal control. In this context, this paper leverages classical turnpike properties of optimal control problems to attempt a quantifiable answer to the question of how many layers should be considered in a DNN. The underlying assumption is that the number of neurons per layer -- i.e., the width of the DNN -- is kept constant. Pursuing a different route than the classical analysis of approximation properties of sigmoidal functions, we prove explicit bounds on the required depths of DNNs based on asymptotic reachability assumptions and a dissipativity-inducing choice of the regularization terms in the training problem. Numerical results obtained for the two spiral task data set for classification indicate that the proposed estimates can provide non-conservative depth bounds.      
### 24.Physical Layer Security based Key Management for LoRaWAN  [ :arrow_down: ](https://arxiv.org/pdf/2101.02975.pdf)
>  Within this the work applicability of Physical LayerSecurity (PHYSEC) based key management within Long RangeWide Area Network (LoRaWAN) is proposed and evaluatedusing an experimental testbed. Since Internet of Things (IoT)technologies have been arising in past years, they have as wellattracted attention for possible cyber attacks. While LoRaWANalready provides many of the features needed in order to ensuresecurity goals such as data confidentiality and integrity, it lacksin measures such as secure key management and distributionschemes. Since conventional solutions are not feasible here, e.g.due to constraints on payload size and power consumption, wepropose the usage of PHYSEC based session key management,which can provide the respective measures in a more lightweightway. The results derived from our testbed show that it can be apromising alternative approach.      
### 25.Maximizing Information Gain for the Characterization of Biomolecular Circuits  [ :arrow_down: ](https://arxiv.org/pdf/2101.02924.pdf)
>  Quantitatively predictive models of biomolecular circuits are important tools for the design of synthetic biology and molecular communication circuits. The information content of typical time-lapse single-cell data for the inference of kinetic parameters is not only limited by measurement uncertainty and intrinsic stochasticity, but also by the employed perturbations. Novel microfluidic devices enable the synthesis of temporal chemical concentration profiles. The informativeness of a perturbation can be quantified based on mutual information. We propose an approximate method to perform optimal experimental design of such perturbation profiles. To estimate the mutual information we perform a multivariate log-normal approximation of the joint distribution over parameters and observations and scan the design space using Metropolis-Hastings sampling. The method is demonstrated by finding optimal perturbation sequences for synthetic case studies on a gene expression model with varying reporter characteristics.      
### 26.A Four-Stage Data Augmentation Approach to ResNet-Conformer Based Acoustic Modeling for Sound Event Localization and Detection  [ :arrow_down: ](https://arxiv.org/pdf/2101.02919.pdf)
>  In this paper, we propose a novel four-stage data augmentation approach to ResNet-Conformer based acoustic modeling for sound event localization and detection (SELD). First, we explore two spatial augmentation techniques, namely audio channel swapping (ACS) and multi-channel simulation (MCS), to deal with data sparsity in SELD. ACS and MDS focus on augmenting the limited training data with expanding direction of arrival (DOA) representations such that the acoustic models trained with the augmented data are robust to localization variations of acoustic sources. Next, time-domain mixing (TDM) and time-frequency masking (TFM) are also investigated to deal with overlapping sound events and data diversity. Finally, ACS, MCS, TDM and TFM are combined in a step-by-step manner to form an effective four-stage data augmentation scheme. Tested on the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 data sets, our proposed augmentation approach greatly improves the system performance, ranking our submitted system in the first place in the SELD task of DCASE 2020 Challenge. Furthermore, we employ a ResNet-Conformer architecture to model both global and local context dependencies of an audio sequence to yield further gains over those architectures used in the DCASE 2020 SELD evaluations.      
### 27.Primal-dual $\varepsilon$-Subgradient Method for Distributed Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2101.02880.pdf)
>  This paper studies the distributed optimization problem where the objective functions might be nondifferentiable and subject to heterogeneous set constraints. Unlike existing subgradient methods, we focus on the case when the exact subgradients of the local objective functions can not be accessed by the agents. To solve this problem, we propose a projected primal-dual dynamics using only the objective function's approximate subgradients. We first prove that the formulated optimization problem can only be solved with an approximate error depending upon the accuracy of the available subgradients. Then, we show the exact solvability of this optimization problem if the accumulated approximation error is not too large. After that, we also give a novel componentwise normalized variant to improve the transient behavior of the convergent sequence. The effectiveness of our algorithms is verified by a numerical example.      
### 28.ADiag: Graph Neural Network Based Diagnosis of Alzheimer's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2101.02870.pdf)
>  Alzheimer's Disease (AD) is the most widespread neurodegenerative disease, affecting over 50 million people across the world. While its progression cannot be stopped, early and accurate diagnostic testing can drastically improve quality of life in patients. Currently, only qualitative means of testing are employed in the form of scoring performance on a battery of cognitive tests. The inherent disadvantage of this method is that the burden of an accurate diagnosis falls on the clinician's competence. Quantitative methods like MRI scan assessment are inaccurate at best,due to the elusive nature of visually observable changes in the brain. In lieu of these disadvantages to extant methods of AD diagnosis, we have developed ADiag, a novel quantitative method to diagnose AD through GraphSAGE Network and Dense Differentiable Pooling (DDP) analysis of large graphs based on thickness difference between different structural regions of the cortex. Preliminary tests of ADiag have revealed a robust accuracy of 83%, vastly outperforming other qualitative and quantitative diagnostic techniques.      
### 29.Towards High Data-Rate Diffusive Molecular Communications: Performance Enhancement Strategies  [ :arrow_down: ](https://arxiv.org/pdf/2101.02869.pdf)
>  Diffusive molecular communications (DiMC) have recently gained attention as a candidate for nano- to micro- and macro-scale communications due to its simplicity and energy efficiency. As signal propagation is solely enabled by Brownian motion mechanics, DiMC faces severe inter-symbol interference (ISI), which limits reliable and high data-rate communications. Herein, recent literature on DiMC performance enhancement strategies is surveyed; key research directions are identified. Signaling design and associated design constraints are presented. Classical and novel transceiver designs are reviewed with an emphasis on methods for ISI mitigation and performance-complexity tradeoffs. Key parameter estimation strategies such as synchronization and channel estimation are considered in conjunction with asynchronous and timing error robust receiver methods. Finally, source and channel coding in the context of DiMC is presented.      
### 30.Probabilistic Graph Attention Network with Conditional Kernels for Pixel-Wise Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2101.02843.pdf)
>  Multi-scale representations deeply learned via convolutional neural networks have shown tremendous importance for various pixel-level prediction problems. In this paper we present a novel approach that advances the state of the art on pixel-level prediction in a fundamental aspect, i.e. structured multi-scale features learning and fusion. In contrast to previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture, and simply fusing the features with weighted averaging or concatenation, we propose a probabilistic graph attention network structure based on a novel Attention-Gated Conditional Random Fields (AG-CRFs) model for learning and fusing multi-scale representations in a principled manner. In order to further improve the learning capacity of the network structure, we propose to exploit feature dependant conditional kernels within the deep probabilistic framework. Extensive experiments are conducted on four publicly available datasets (i.e. BSDS500, NYUD-V2, KITTI, and Pascal-Context) and on three challenging pixel-wise prediction problems involving both discrete and continuous labels (i.e. monocular depth estimation, object contour prediction, and semantic segmentation). Quantitative and qualitative results demonstrate the effectiveness of the proposed latent AG-CRF model and the overall probabilistic graph attention network with feature conditional kernels for structured feature learning and pixel-wise prediction.      
### 31.Free-Breathing Water, Fat, $R_2^{\star}$ and $B_0$ Field Mapping of the Liver Using Multi-Echo Radial FLASH and Regularized Model-based Reconstruction (MERLOT)  [ :arrow_down: ](https://arxiv.org/pdf/2101.02788.pdf)
>  Purpose: To achieve free-breathing quantitative fat and $R_2^{\star}$ mapping of the liver using a generalized model-based iterative reconstruction, dubbed as MERLOT. Methods: For acquisition, we use a multi-echo radial FLASH sequence that acquires multiple echoes with different complementary radial spoke encodings. We investigate real-time single-slice and volumetric multi-echo radial FLASH acquisition. For the latter, the sampling scheme is extended to a volumetric stack-of-stars acquisition. Model-based reconstruction based on generalized nonlinear inversion is used to jointly estimate water, fat, $R_2^{\star}$, $B_0$ field inhomogeneity, and coil sensitivity maps from the multi-coil multi-echo radial spokes. Spatial smoothness regularization is applied onto the B 0 field and coil sensitivity maps, whereas joint sparsity regularization is employed for the other parameter maps. The method integrates calibration-less parallel imaging and compressed sensing and was implemented in BART. For the volumetric acquisition, the respiratory motion is resolved with self-gating using SSA-FARY. The quantitative accuracy of the proposed method was validated via numerical simulation, the NIST phantom, a water/fat phantom, and in in-vivo liver studies. Results: For real-time acquisition, the proposed model-based reconstruction allowed acquisition of dynamic liver fat fraction and $R_2^{\star}$ maps at a temporal resolution of 0.3 s per frame. For the volumetric acquisition, whole liver coverage could be achieved in under 2 minutes using the self-gated motion-resolved reconstruction. Conclusion: The proposed multi-echo radial sampling sequence achieves fast k -space coverage and is robust to motion. The proposed model-based reconstruction yields spatially and temporally resolved liver fat fraction, $R_2^{\star}$ and $B_0$ field maps at high undersampling factor and with volume coverage.      
### 32.Measurement of breast-tissue x-ray attenuation by spectral mammography: first results on cyst fluid  [ :arrow_down: ](https://arxiv.org/pdf/2101.02762.pdf)
>  Knowledge of x-ray attenuation is essential for developing and evaluating x-ray imaging technologies. For instance, techniques to better characterize cysts at mammography screening would be highly desirable to reduce recalls, but the development is hampered by the lack of attenuation data for cysts. We have developed a method to measure x-ray attenuation of tissue samples using a prototype photon-counting spectral mammography unit. The method was applied to measure the attenuation of 50 samples of breast cyst fluid and 50 samples of water. Spectral (energy-resolved) images of the samples were acquired and the image signal was mapped to equivalent thicknesses of two known reference materials, which can be used to derive the x-ray attenuation as a function of energy. The attenuation of cyst fluid was found to be significantly different from water. There was a relatively large natural spread between different samples of cyst fluid, whereas the homogeneity of each individual sample was found to be good; the variation within samples did not reach above the quantum noise floor. The spectral method proved stable between several measurements on the same sample. Further, chemical analysis and elemental attenuation calculation were used to validate the spectral measurement on a subset of the samples. The two methods agreed within the precision of the elemental attenuation calculation over the mammographic energy range.      
### 33.Volumetric breast-density measurement using spectral photon-counting tomosynthesis: First clinical results  [ :arrow_down: ](https://arxiv.org/pdf/2101.02758.pdf)
>  Measurements of breast density have the potential to improve the efficiency and reduce the cost of screening mammography through personalized screening. Breast density has traditionally been evaluated from the dense area in a mammogram, but volumetric assessment methods, which measure the volumetric fraction of fibro-glandular tissue in the breast, are potentially more consistent and physically sound. The purpose of the present study is to evaluate a method for measuring the volumetric breast density using photon-counting spectral tomosynthesis. The performance of the method was evaluated using phantom measurements and clinical data from a small population (n=18). The precision was determined to 2.4 percentage points (pp) of volumetric breast density. Strong correlations were observed between contralateral (R^2=0.95) and ipsilateral (R^2=0.96) breast-density measurements. The measured breast density was anti-correlated to breast thickness, as expected, and exhibited a skewed distribution in the range [3.7%, 55%] and with a median of 18%. We conclude that the method yields promising results that are consistent with expectations. The relatively high precision of the method may enable novel applications such as treatment monitoring.      
### 34.Measurement of breast-tissue x-ray attenuation by spectral imaging: fresh and fixed normal and malignant tissue  [ :arrow_down: ](https://arxiv.org/pdf/2101.02755.pdf)
>  Knowledge of x-ray attenuation is essential for developing and evaluating x-ray imaging technologies. In mammography, measurement of breast density, dose estimation, and differentiation between cysts and solid tumours are example applications requiring accurate data on tissue attenuation. Published attenuation data are, however, sparse and cover a relatively wide range. To supplement available data we have previously measured the attenuation of cyst fluid and solid lesions using photon-counting spectral mammography. The present study aims to measure the attenuation of normal adipose and glandular tissue, and to measure the effect of formalin fixation, a major uncertainty in published data. A total of 27 tumour specimens, seven fibro-glandular tissue specimens, and 15 adipose tissue specimens were included. Spectral (energy-resolved) images of the samples were acquired and the image signal was mapped to equivalent thicknesses of two known reference materials, from which x-ray attenuation as a function of energy can be derived. The spread in attenuation between samples was relatively large, partly because of natural variation. The variation of malignant and glandular tissue was similar, whereas that of adipose tissue was lower. Formalin fixation slightly altered the attenuation of malignant and glandular tissue, whereas the attenuation of adipose tissue was not significantly affected. The difference in attenuation between fresh tumour tissue and cyst fluid was smaller than has previously been measured for fixed tissue, but the difference was still significant and discrimination of these two tissue types is still possible. The difference between glandular and malignant tissue was close-to significant; it is reasonable to expect a significant difference with a larger set of samples. [cropped]      
### 35.Heatmap-based 2D Landmark Detection with a Varying Number of Landmarks  [ :arrow_down: ](https://arxiv.org/pdf/2101.02737.pdf)
>  Mitral valve repair is a surgery to restore the function of the mitral valve. To achieve this, a prosthetic ring is sewed onto the mitral annulus. Analyzing the sutures, which are punctured through the annulus for ring implantation, can be useful in surgical skill assessment, for quantitative surgery and for positioning a virtual prosthetic ring model in the scene via augmented reality. This work presents a neural network approach which detects the sutures in endoscopic images of mitral valve repair and therefore solves a landmark detection problem with varying amount of landmarks, as opposed to most other existing deep learning-based landmark detection approaches. The neural network is trained separately on two data collections from different domains with the same architecture and hyperparameter settings. The datasets consist of more than 1,300 stereo frame pairs each, with a total over 60,000 annotated landmarks. The proposed heatmap-based neural network achieves a mean positive predictive value (PPV) of 66.68$\pm$4.67% and a mean true positive rate (TPR) of 24.45$\pm$5.06% on the intraoperative test dataset and a mean PPV of 81.50\pm5.77\% and a mean TPR of 61.60$\pm$6.11% on a dataset recorded during surgical simulation. The best detection results are achieved when the camera is positioned above the mitral valve with good illumination. A detection from a sideward view is also possible if the mitral valve is well perceptible.      
