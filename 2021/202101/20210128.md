# ArXiv eess --Thu, 28 Jan 2021
### 1.Synthetic Generation of Three-Dimensional Cancer Cell Models from Histopathological Images  [ :arrow_down: ](https://arxiv.org/pdf/2101.11600.pdf)
>  Synthetic generation of three-dimensional cell models from histopathological images enables an enhanced understanding of cell mutation, spatial context, and progression of cancer, necessary for clinical assessment and optimal treatment. Classical reconstruction algorithms based on image registration of consecutive slides of stained tissues are prone to errors and often not suitable for the training of three-dimensional segmentation algorithms. We propose a novel framework to generate synthetic three-dimensional histological models based on a generator-discriminator pattern optimizing constrained features that construct a 3D model via a blender interface exploiting smooth shape continuity typical for biological specimens. Simultaneously a discriminator is trained to distinguish between the original cell patches and projections of the three-dimensional model. To capture the spatial context of entire cell clusters we deploy a similar architecture expanded by style transfer capability. Models based on clusters of adjoined embedding points are generated to analyze the characteristic cell mutation process of breast cancer.      
### 2.An Interpretation of Regularization by Denoising and its Application with the Back-Projected Fidelity Term  [ :arrow_down: ](https://arxiv.org/pdf/2101.11599.pdf)
>  The vast majority of image recovery tasks are ill-posed problems. As such, methods that are based on optimization use cost functions that consist of both fidelity and prior (regularization) terms. A recent line of works imposes the prior by the Regularization by Denoising (RED) approach, which exploits the good performance of existing image denoising engines. Yet, the relation of RED to explicit prior terms is still not well understood, as previous work requires too strong assumptions on the denoisers. In this paper, we make two contributions. First, we show that the RED gradient can be seen as a (sub)gradient of a prior function--but taken at a denoised version of the point. As RED is typically applied with a relatively small noise level, this interpretation indicates a similarity between RED and traditional gradients. This leads to our second contribution: We propose to combine RED with the Back-Projection (BP) fidelity term rather than the common Least Squares (LS) term that is used in previous works. We show that the advantages of BP over LS for image deblurring and super-resolution, which have been demonstrated for traditional gradients, carry on to the RED approach.      
### 3.Modeling of Nonlinear Interference Power for Dual-Polarization 4D Formats  [ :arrow_down: ](https://arxiv.org/pdf/2101.11580.pdf)
>  We assess the accuracy of a recently introduced nonlinear interference model for general dual-polarization 4D formats.~ Unlike previous models for polarization-multiplexed 2D formats, an average gap from split-step Fourier simulations within 0.1 dB is demonstrated.      
### 4.Tractable higher-order under-approximating AE extensions for non-linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.11536.pdf)
>  We consider the problem of under and over-approximating the image of general vector-valued functions over bounded sets, and apply the proposed solution to the estimation of reachable sets of uncertain non-linear discrete-time dynamical systems. Such a combination of under and over-approximations is very valuable for the verification of properties of embedded and cyber-physical controlled systems. Over-approximations prove properties correct, while under-approximations can be used for falsification. Coupled, they provide a measure of the conservatism of the analysis. This work introduces a general framework relying on computations of robust ranges of vector-valued functions. This framework allows us to extend for under-approximation many precision refinements that are classically used for over-approximations, such as affine approximations, Taylor models, quadrature formulae and preconditioning methods. We end by evaluating the efficiency and precision of our approach, focusing on the application to the analysis of discrete-time dynamical systems with inputs and disturbances, on different examples from the literature.      
### 5.Practical Utility PV Multilevel Inverter Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2101.11524.pdf)
>  Multilevel inverters are used to improve powerquality and reduce component stresses. This paper describesand compares two multilevel cascaded three phase inverterimplementations with two different modulation techniques: PhaseShifted Pulse Width Modulation, and Nearest Level Control.Further analysis will show required number of inverter levelswith respect to modulation techniques to provide desired powerand power quality to resistive load or grid. Cascaded inverterwill be designed and simulated to draw power from PV cells.      
### 6.Parameter and density estimation from real-world traffic data: A kinetic compartmental approach  [ :arrow_down: ](https://arxiv.org/pdf/2101.11485.pdf)
>  The main motivation of this work is to assess the validity of a LWR traffic flow model to model measurements obtained from trajectory data, and propose extensions of this model to improve it. A formulation for a discrete dynamical system is proposed aiming at reproducing the evolution in time of the density of vehicles along a road, as observed in the measurements. This system is formulated as a chemical reaction network where road cells are interpreted as compartments, the transfer of vehicles from one cell to the other is seen as a chemical reaction between adjacent compartment and the density of vehicles is seen as a concentration of reactant. Several degrees of flexibility on the parameters of this system, which basically consist of the reaction rates between the compartments, can be considered: a constant value or a function depending on time and/or space. Density measurements coming from trajectory data are then interpreted as observations of the states of this system at consecutive times. Optimal reaction rates for the system are then obtained by minimizing the discrepancy between the output of the system and the state measurements. This approach was tested both on simulated and real data, proved successful in recreating the complexity of traffic flows despite the assumptions on the flux-density relation.      
### 7.VOTE400(Voide Of The Elderly 400 Hours): A Speech Dataset to Study Voice Interface for Elderly-Care  [ :arrow_down: ](https://arxiv.org/pdf/2101.11469.pdf)
>  This paper introduces a large-scale Korean speech dataset, called VOTE400, that can be used for analyzing and recognizing voices of the elderly people. The dataset includes about 300 hours of continuous dialog speech and 100 hours of read speech, both recorded by the elderly people aged 65 years or over. A preliminary experiment showed that speech recognition system trained with VOTE400 can outperform conventional systems in speech recognition of elderly people's voice. This work is a multi-organizational effort led by ETRI and MINDs Lab Inc. for the purpose of advancing the speech recognition performance of the elderly-care robots.      
### 8.Impact of High Penetration of Inverter-based Generation on Electromechanical Wave Propagation in Power Grids  [ :arrow_down: ](https://arxiv.org/pdf/2101.11454.pdf)
>  A power system electromechanical wave propagates from the disturbance location to the rest of system, influencing various types of protections. In addition, since more power-electronics-interfaced generation and energy storage devices are being integrated into power systems, electromechanical wave propagation speeds in the future power systems are likely to change accordingly. In this paper, GPS-synchronized measurement data from a wide-area synchrophasor measurement system FNET/GridEye are used to analyze the characteristics of electromechanical wave propagation in the U.S. Eastern Interconnection (EI) system. Afterwards, high levels of photovoltaic (PV) penetration are modeled in the EI to investigate the influences of a typical power-electronics--interfaced resource on the electromechanical wave propagation speed. The result shows a direct correlation between the local penetration level of inverter-based generation and the electromechanical wave propagation speed.      
### 9.Robust Instability Radius for Multi-agent Dynamical Systems with Cyclic Structure  [ :arrow_down: ](https://arxiv.org/pdf/2101.11452.pdf)
>  This paper is concerned with robust instability analysis for linear multi-agent dynamical systems with cyclic structure. This relates to interesting and important periodic oscillation phenomena in biology and neuronal science, since the nonlinear phenomena often occur when the linearized model around an equilibrium point is unstable. We first make a problem setting on the analysis and define the notion of robust instability radius (RIR) as a quantitative measure for maximum allowable stable dynamic perturbation in terms of the H-infinity norm. After showing lower bounds of the RIR, we derive the exact RIR, which is analytic and scalable, for first order time-lag agents. Finally, we make a remark on the potential applicability to some classes of higher order systems.      
### 10.Spectrum Sharing for 6G Integrated Satellite-Terrestrial Communication Networks Based on NOMA and Cognitive Radio  [ :arrow_down: ](https://arxiv.org/pdf/2101.11418.pdf)
>  The explosive growth of bandwidth hungry Internet applications has led to the rapid development of new generation mobile network technologies that are expected to provide broadband access to the Internet in a pervasive manner. For example, 6G networks are capable of providing high-speed network access by exploiting higher frequency spectrum; high-throughout satellite communication services are also adopted to achieve pervasive coverage in remote and isolated areas. In order to enable seamless access, Integrated Satellite-Terrestrial Communication Networks (ISTCN) has emerged as an important research area. ISTCN aims to provide high speed and pervasive network services by integrating broadband terrestrial mobile networks with satellite communication networks. As terrestrial mobile networks began to use higher frequency spectrum (between 3GHz to 40GHz) which overlaps with that of satellite communication (4GHz to 8GHz for C band and 26GHz to 40GHz for Ka band), there are opportunities and challenges. On one hand, satellite terminals can potentially access terrestrial networks in an integrated manner; on the other hand, there will be more congestion and interference in this spectrum, hence more efficient spectrum management techniques are required. In this paper, we propose a new technique to improve spectrum sharing performance by introducing Non-orthogonal Frequency Division Multiplexing (NOMA) and Cognitive Radio (CR) in the spectrum sharing of ISTCN. In essence, NOMA technology improves spectrum efficiency by allowing different users to transmit on the same carrier and distinguishing users by user power levels while CR technology improves spectrum efficiency through dynamic spectrum sharing. Furthermore, some open researches and challenges in ISTCN will be discussed.      
### 11.A Two-Functional-Network Framework of Opinion Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2101.11415.pdf)
>  A common trait involving the opinion dynamics in social networks is an anchor on interacting network to characterize the opinion formation process among participating social actors, such as information flow, cooperative and antagonistic influence, etc. Nevertheless, interacting networks are generally public for social groups, as well as other individuals who may be interested in. This blocks a more precise interpretation of the opinion formation process since social actors always have complex feeling, motivation and behavior, even beliefs that are personally private. In this paper, we formulate a general configuration on describing how individual's opinion evolves in a distinct fashion. It consists of two functional networks: interacting network and appraisal network. Interacting network inherits the operational properties as DeGroot iterative opinion pooling scheme while appraisal network, forming a belief system, quantifies certain cognitive orientation to interested individuals' beliefs, over which the adhered attitudes may have the potential to be antagonistic. We explicitly show that cooperative appraisal network always leads to consensus in opinions. Antagonistic appraisal network, however, causes opinion cluster. It is verified that antagonistic appraisal network affords to guarantee consensus by imposing some extra restrictions. They hence bridge a gap between the consensus and the clusters in opinion dynamics. We further attain a gauge on the appraisal network by means of the random convex optimization approach. Moreover, we extend our results to the case of mutually interdependent issues.      
### 12.Identification of particle mixtures using machine-learning-assisted laser diffraction analysis  [ :arrow_down: ](https://arxiv.org/pdf/2101.11402.pdf)
>  We demonstrate a smart laser-diffraction analysis technique for particle mixture identification. We retrieve information about the size, geometry, and ratio concentration of two-component heterogeneous particle mixtures with an efficiency above 92%. In contrast to commonly-used laser diffraction schemes -- in which a large number of detectors is needed -- our machine-learning-assisted protocol makes use of a single far-field diffraction pattern, contained within a small angle ($\sim 0.26^{\circ}$) around the light propagation axis. Because of its reliability and ease of implementation, our work may pave the way towards the development of novel smart identification technologies for sample classification and particle contamination monitoring in industrial manufacturing processes.      
### 13.B-spline Parameterized Joint Optimization of Reconstruction and K-space Trajectories (BJORK) for Accelerated 2D MRI  [ :arrow_down: ](https://arxiv.org/pdf/2101.11369.pdf)
>  Optimizing k-space sampling trajectories is a challenging topic for fast magnetic resonance imaging (MRI). This work proposes to optimize a reconstruction algorithm and sampling trajectories jointly concerning image reconstruction quality. We parameterize trajectories with quadratic B-spline kernels to reduce the number of parameters and enable multi-scale optimization, which may help to avoid sub-optimal local minima. The algorithm includes an efficient non-Cartesian unrolled neural network-based reconstruction and an accurate approximation for backpropagation through the non-uniform fast Fourier transform (NUFFT) operator to accurately reconstruct and back-propagate multi-coil non-Cartesian data. Penalties on slew rate and gradient amplitude enforce hardware constraints. Sampling and reconstruction are trained jointly using large public datasets. To correct the potential eddy-current effect introduced by the curved trajectory, we use a pencil-beam trajectory mapping technique. In both simulations and in-vivo experiments, the learned trajectory demonstrates significantly improved image quality compared to previous model-based and learning-based trajectory optimization methods for 20x acceleration factors. Though trained with neural network-based reconstruction, the proposed trajectory also leads to improved image quality with compressed sensing-based reconstruction.      
### 14.Anti-Aliasing Add-On for Deep Prior Seismic Data Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2101.11361.pdf)
>  Data interpolation is a fundamental step in any seismic processing workflow. Among machine learning techniques recently proposed to solve data interpolation as an inverse problem, Deep Prior paradigm aims at employing a convolutional neural network to capture priors on the data in order to regularize the inversion. However, this technique lacks of reconstruction precision when interpolating highly decimated data due to the presence of aliasing. In this work, we propose to improve Deep Prior inversion by adding a directional Laplacian as regularization term to the problem. This regularizer drives the optimization towards solutions that honor the slopes estimated from the interpolated data low frequencies. We provide some numerical examples to showcase the methodology devised in this manuscript, showing that our results are less prone to aliasing also in presence of noisy and corrupted data.      
### 15.Low-Complexity Joint CFO and CIR Estimation for RIS-aided wireless communications using OFDM  [ :arrow_down: ](https://arxiv.org/pdf/2101.11348.pdf)
>  Accurate channel estimation is essential for achieving the performance gains offered by reconfigurable intelligent surface (RIS)-assisted wireless communications. Recently, a large number of channel estimation methods for RIS-assisted wireless communications have been proposed. However, none of the existing methods takes into account the influence of carrier frequency offset (CFO). In general, CFO can significantly degrade channel estimation for orthogonal frequency division multiplexing (OFDM) systems, since it breaks the orthogonality of subcarriers. Motivated by this, we investigate the effect of CFO on channel estimation for RIS-aided OFDM systems. Furthermore, we propose a joint CFO and channel impulse response (CIR) estimation method for RIS-aided OFDM systems. Simulation results demonstrate the effectiveness of our proposed joint CFO and CIR estimation method, and also demonstrate that the use of the time domain for estimation in this context results in a factor of 10 improvement in the mean-squared error (MSE) performance of channel estimation. Finally, the total computational complexity of the proposed method, including both CFO and channel estimation, is lower than the complexity of the conventional frequency-domain channel estimation method without CFO estimation.      
### 16.Low-Power Audio Keyword Spotting using Tsetlin Machines  [ :arrow_down: ](https://arxiv.org/pdf/2101.11336.pdf)
>  The emergence of Artificial Intelligence (AI) driven Keyword Spotting (KWS) technologies has revolutionized human to machine interaction. Yet, the challenge of end-to-end energy efficiency, memory footprint and system complexity of current Neural Network (NN) powered AI-KWS pipelines has remained ever present. This paper evaluates KWS utilizing a learning automata powered machine learning algorithm called the Tsetlin Machine (TM). Through significant reduction in parameter requirements and choosing logic over arithmetic based processing, the TM offers new opportunities for low-power KWS while maintaining high learning efficacy. In this paper we explore a TM based keyword spotting (KWS) pipeline to demonstrate low complexity with faster rate of convergence compared to NNs. Further, we investigate the scalability with increasing keywords and explore the potential for enabling low-power on-chip KWS.      
### 17.Doppler Estimation for High-Velocity Targets Using Subpulse Processing and the Classic Chinese Remainder Theorem  [ :arrow_down: ](https://arxiv.org/pdf/2101.11311.pdf)
>  In pulsed Doppler radars, the classic Chinese remainder theorem (CCRT) is a common method to resolve Doppler ambiguities caused by fast-moving targets. Another issue concerning high-velocity targets is related to the loss in the signal-to-noise ratio (SNR) after performing range compression. In particular, this loss can be partially mitigated by the use of subpulse processing (SP). Modern radars combine these techniques in order to reliably unfold the target velocity. However, the presence of background noise may compromise the Doppler estimates. Hence, a rigorous statistical analysis is imperative. In this work, we provide a comprehensive analysis on Doppler estimation. In particular, we derive novel closed-form expressions for the probability of detection (PD) and probability of false alarm (PFA). To this end, we consider the newly introduce SP along with the CCRT. A comparison analysis between SP and the classic pulse processing (PP) technique is also carried out. Numerical results and Monte-Carlo simulations corroborate the validity of our expressions and show that the SP-plus-CCRT technique helps to greatly reduce the PFA compared to previous studies, thereby improving radar detection.      
### 18.Distributed Learning over Markovian Fading Channels for Stable Spectrum Access  [ :arrow_down: ](https://arxiv.org/pdf/2101.11292.pdf)
>  We consider the problem of multi-user spectrum access in wireless networks. The bandwidth is divided into K orthogonal channels, and M users aim to access the spectrum. Each user chooses a single channel for transmission at each time slot. The state of each channel is modeled by a restless unknown Markovian process. Previous studies have analyzed a special case of this setting, in which each channel yields the same expected rate for all users. By contrast, we consider a more general and practical model, where each channel yields a different expected rate for each user. This model adds a significant challenge of how to efficiently learn a channel allocation in a distributed manner to yield a global system-wide objective. We adopt the stable matching utility as the system objective, which is known to yield strong performance in multichannel wireless networks, and develop a novel Distributed Stable Strategy Learning (DSSL) algorithm to achieve the objective. We prove theoretically that DSSL converges to the stable matching allocation, and the regret, defined as the loss in total rate with respect to the stable matching solution, has a logarithmic order with time. Finally, simulation results demonstrate the strong performance of the DSSL algorithm.      
### 19.Steady-State Model of VSC based FACTS Devices using Flexible Holomorphic Embedding: (SSSC and IPFC)  [ :arrow_down: ](https://arxiv.org/pdf/2101.11289.pdf)
>  For proper planning, operation, control, and protection of the power system, the development of a suitable steady-state mathematical model of FACTS devices is a key issue. The Fast and Flexible Holomorphic Embedding (FFHE) method converges faster and provides the flexibility to use any state as an initial guess. But to investigate the effect and ability of FACTS devices using FFHE technique, it is necessary to develop an embedded system for these devices. Therefore, this paper presents an FFHE-based embedded system for VSC-based FACTS controllers, such as SSSC and IPFC. The embedded system is also proposed for their controlling modes. <br>The introduced embedded system is flexible which allows to take any state as an initial guess instead of fixed state, which leads towards the reduced runtime and decreases the required number of terms, as compared to standard HELM. To demonstrate the effectiveness and practicability, the proposed FFHE-based models of FACTS devices have been tested for several cases. Further, the developed recursive formulas for power balance equations, devices' physical constraints, and their controlling modes are thoroughly investigated and examined. From several tests, it is found that the proposed FFHE-based FACTS models require less execution time and reduce the error at higher rate.      
### 20.ViaPPS: A Mobile Pavement Profiling System  [ :arrow_down: ](https://arxiv.org/pdf/2101.11267.pdf)
>  Ensuring safety levels on roads is an imperative task for road authorities. Significant amounts of time and money are spent on performing road inspections every year. In order to ensure efficiency of road inventories, road practitioners are in need of reliable systems that are fast and are designed to comply with high standards at lower costs. To date, the 3D Mobile Mapping Systems is the most efficient, productive and accurate system used during evaluation of pavement inventories. This paper presents ViaPPS, a 3D Mobile Mapping System from ViaTech AS. ViaPPS offers accurate georeferenced data by combining the perception and navigation sensors. This paper showcases results from multiple mobile systems after the harmonization process which is held once a year to evaluate reliability and repeatability of the measurements.      
### 21.Automatic Segmentation of Gross Target Volume of Nasopharynx Cancer using Ensemble of Multiscale Deep Neural Networks with Spatial Attention  [ :arrow_down: ](https://arxiv.org/pdf/2101.11254.pdf)
>  Radiotherapy is the main treatment modality for nasopharynx cancer. Delineation of Gross Target Volume (GTV) from medical images such as CT and MRI images is a prerequisite for radiotherapy. As manual delineation is time-consuming and laborious, automatic segmentation of GTV has a potential to improve this process. Currently, most of the deep learning-based automatic delineation methods of GTV are mainly performed on medical images like CT images. However, it is challenged by the low contrast between the pathology regions and surrounding soft tissues, small target region, and anisotropic resolution of clinical CT images. To deal with these problems, we propose a 2.5D Convolutional Neural Network (CNN) to handle the difference of inplane and through-plane resolution. Furthermore, we propose a spatial attention module to enable the network to focus on small target, and use channel attention to further improve the segmentation performance. Moreover, we use multi-scale sampling method for training so that the networks can learn features at different scales, which are combined with a multi-model ensemble method to improve the robustness of segmentation results. We also estimate the uncertainty of segmentation results based on our model ensemble, which is of great importance for indicating the reliability of automatic segmentation results for radiotherapy planning.      
### 22.Segmentation of common and internal carotid arteries from 3D ultrasound images using adaptive triple U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2101.11252.pdf)
>  Objective: Vessel-wall-volume (VWV) and localized vessel-wall-thickness (VWT) measured from 3D ultrasound (US) carotid images are sensitive to anti-atherosclerotic effects of medical/dietary treatments. VWV and VWT measurements require the lumen-intima (LIB) and media-adventitia boundaries (MAB) at the common and internal carotid arteries (CCA and ICA). However, most existing segmentation techniques were capable of automating only CCA segmentation. An approach capable of segmenting the MAB and LIB from the CCA and ICA was required to accelerate VWV and VWT quantification. Methods: Segmentation for CCA and ICA were performed independently using the proposed two-channel U-Net, which was driven by a novel loss function known as the adaptive triple Dice loss (ADTL). A test-time augmentation (TTA) approach is used, in which segmentation was performed three times based on axial images and its flipped versions; the final segmentation was generated by pixel-wise majority voting. Results: Experiments involving 224 3DUS volumes produce a Dice-similarity-coefficient (DSC) of 95.1%$\pm$4.1% and 91.6%$\pm$6.6% for the MAB and LIB, in the CCA, respectively, and 94.2%$\pm$3.3% and 89.0%$\pm$8.1% for the MAB and LIB, in the ICA, respectively. TTA and ATDL independently contributed to a statistically significant improvement to all boundaries except the LIB in ICA. The total time required to segment the entire 3DUS volume (CCA+ICA) is 1.4s. Conclusion: The proposed two-channel U-Net with ADTL and TTA can segment the CCA and ICA accurately and efficiently from the 3DUS volume. Significance: Our approach has the potential to accelerate the transition of 3DUS measurements of carotid atherosclerosis to clinical research.      
### 23.Smooth Attitude Tracking Control of a 3-DOF Helicopter with Guaranteed Performance  [ :arrow_down: ](https://arxiv.org/pdf/2101.11241.pdf)
>  This paper presents a new prescribed performance control scheme for the attitude tracking of the three degree-of-freedom (3-DOF) helicopter system with lumped disturbances under mechanical constraints. First, a novel prescribed performance function is defined to guarantee that the tracking error performance has a small overshoot in the transient process and converges to an arbitrary small region within a predetermined time in the steady-state process without knowing the initial tracking error in advance. Then, based on the novel prescribed performance function, an error transformation combined with the smooth finite-time control method we proposed before is employed to drive the elevation and pitch angles to track given desired trajectories with guaranteed tracking performance. The theoretical analysis of finite-time Lyapunov stability indicates that the closed-loop system is fast finite-time uniformly ultimately boundedness. Finally, comparative experiment results illustrate the effectiveness and superiority of the proposed control scheme.      
### 24.A Game Theory Based Ramp Merging Strategy for Connected and Automated Vehicles in the Mixed Traffic: A Unity-SUMO Integrated Platform  [ :arrow_down: ](https://arxiv.org/pdf/2101.11237.pdf)
>  Ramp merging is considered as one of the major causes of traffic congestion and accidents because of its chaotic nature. With the development of connected and automated vehicle (CAV) technology, cooperative ramp merging has become one of the popular solutions to this problem. In a mixed traffic situation, CAVs will not only interact with each other, but also handle complicated situations with human-driven vehicles involved. In this paper, a game theory-based ramp merging strategy has been developed for the optimal merging coordination of CAVs in the mixed traffic, which determines dynamic merging sequence and corresponding longitudinal/lateral control. This strategy improves the safety and efficiency of the merging process by ensuring a safe inter-vehicle distance among the involved vehicles and harmonizing the speed of CAVs in the traffic stream. To verify the proposed strategy, mixed traffic simulations under different penetration rates and different congestion levels have been carried out on an innovative Unity-SUMO integrated platform, which connects a game engine-based driving simulator with a traffic simulator. This platform allows the human driver to participate in the simulation, and also equip CAVs with more realistic sensing systems. In the traffic flow level simulation test, Unity takes over the sensing and control of all CAVs in the simulation, while SUMO handles the behavior of all legacy vehicles. The results show that the average speed of traffic flow can be increased up to 110%, and the fuel consumption can be reduced up to 77%, respectively.      
### 25.Statistical guided-waves-based SHM via stochastic non-parametric time series models  [ :arrow_down: ](https://arxiv.org/pdf/2101.11208.pdf)
>  Damage detection in active-sensing, guided-waves-based Structural Health Monitoring (SHM) has evolved through multiple eras of development during the past decades. Nevertheless, there still exists a number of challenges facing the current state-of-the-art approaches, including low damage sensitivity, lack of robustness to uncertainties, need for user-defined thresholds, and non-uniform response across a sensor network. In this work, a novel statistical framework is proposed for active-sensing SHM based on the use of ultrasonic guided waves. This framework is based on stochastic non-parametric time series models and their corresponding statistical properties in order to readily provide healthy confidence bounds and enable accurate and robust damage detection via the use of appropriate statistical decision making tests. Three such methods and corresponding statistical quantities along with decision making schemes are formulated and experimentally assessed via the use of three structural coupons: an Al plate with a growing notch, a Carbon fiber-reinforced plastic (CFRP) plate with added weights to simulate local damages, and the CFRP panel used in the Open Guided Waves project, all outfitted with piezoelectric transducers and a pitch-catch configuration. The performance of the proposed methods is compared to that of state-of-the-art time-domain damage indices. The results demonstrate the increased sensitivity and robustness of the proposed methods, with better tracking capability of damage evolution compared to conventional approaches, even for damage-non-intersecting actuator-sensor paths. Overall, the proposed statistics in this study promise greater damage sensitivity across different components, with enhanced robustness to uncertainties, as well as user-friendly application.      
### 26.Cyber-Physical Queueing-Network Model for Risk Management in Next-Generation Emergency Response Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.11198.pdf)
>  Queueing-network models are developed for enhanced and next-generation 911 (E911 and NG911) systems, which capture both their cyber-components (communications, data-processing) and physical-world-elements (call takers, vehicle dispatch). The models encompass both the call-processing and dispatch functions of 911, and can represent the interdependencies between multiple PSAPs enabled by NG911. An instantiation of the model for a future NG911 system for Charlotte, North Carolina, is developed and used to assess performance metrics. Representation of cyber-threats (e.g. Distributed Denial-of-Service attacks) within the queueing-network model is undertaken. Based on these representations, the model is used for analysis of holistic threat impacts, as a step toward risk and vulnerability assessment for future emergency response systems.      
### 27.GymD2D: A Device-to-Device Underlay Cellular Offload Evaluation Platform  [ :arrow_down: ](https://arxiv.org/pdf/2101.11188.pdf)
>  Cellular offloading in device-to-device communication is a challenging optimisation problem in which the improved allocation of radio resources can increase spectral efficiency, energy efficiency, throughout and reduce latency. The academic community have explored different optimisation methods on these problems and initial results are encouraging. However, there exists significant friction in the lack of a simple, configurable, open-source framework for cellular offload research. Prior research utilises a variety of network simulators and system models, making it difficult to compare results. In this paper we present GymD2D, a framework for experimentation with physical layer resource allocation problems in device-to-device communication. GymD2D allows users to simulate a variety of cellular offload scenarios and to extend its behaviour to meet their research needs. GymD2D provides researchers an evaluation platform to compare, share and build upon previous research. We evaluated GymD2D with state-of-the-art deep reinforcement learning and demonstrate these algorithms provide significant efficiency improvements.      
### 28.Functional Observers with Linear Error Dynamics for Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.11148.pdf)
>  In this work, the problem of designing observers for estimating a single nonlinear functional of the state is formulated for general nonlinear systems. Notions of functional observer linearization are also formulated, in terms achieving exactly linear error dynamics in transformed coordinates and with prescribed rate of decay of the error. Necessary and sufficient conditions for the existence of a lower-order functional observer with linear dynamics are derived. The results provide a direct generalization of Luenberger's linear theory of functional observers to nonlinear systems.      
### 29.Boosting Segmentation Performance across datasets using histogram specification with application to pelvic bone segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2101.11135.pdf)
>  Accurate segmentation of the pelvic CTs is crucial for the clinical diagnosis of pelvic bone diseases and for planning patient-specific hip surgeries. With the emergence and advancements of deep learning for digital healthcare, several methodologies have been proposed for such segmentation tasks. But in a low data scenario, the lack of abundant data needed to train a Deep Neural Network is a significant bottle-neck. In this work, we propose a methodology based on modulation of image tonal distributions and deep learning to boost the performance of networks trained on limited data. The strategy involves pre-processing of test data through histogram specification. This simple yet effective approach can be viewed as a style transfer methodology. The segmentation task uses a U-Net configuration with an EfficientNet-B0 backbone, optimized using an augmented BCE-IoU loss function. This configuration is validated on a total of 284 images taken from two publicly available CT datasets, TCIA (a cancer imaging archive) and the Visible Human Project. The average performance measures for the Dice coefficient and Intersection over Union are 95.7% and 91.9%, respectively, give strong evidence for the effectiveness of the approach, which is highly competitive with state-of-the-art methodologies.      
### 30.Route Choice-based Socio-Technical Macroscopic Traffic Model  [ :arrow_down: ](https://arxiv.org/pdf/2101.11133.pdf)
>  Human route choice is undeniably one of the key contributing factors towards traffic dynamics. However, most existing macroscopic traffic models are typically concerned with driving behavior and do not incorporate human route choice behavior models in their formulation. In this paper, we propose a socio-technical macroscopic traffic model that characterizes the traffic states using human route choice attributes. Essentially, such model provides a framework for capturing the Cyber-Physical-Social coupling in smart transportation systems. To derive this model, we first use Cumulative Prospect Theory (CPT) to model the human passengers' route choice under bounded rationality. These choices are assumed to be influenced by traffic alerts and other incomplete traffic information. Next, we assume that the vehicles are operating under a non-cooperative cruise control scenario. Accordingly, human route choice segregates the traffic into multiple classes where each class corresponds to a specific route between an origin-destination pair. Thereafter, we derive a Mean Field Game (MFG) limit of this non-cooperative game to obtain a macroscopic model which embeds the human route choice attribute. Finally, we analyze the mathematical characteristics of the proposed model and present simulation studies to illustrate the model behavior.      
### 31.Steady State Modeling for Variable Frequency AC Power Flow  [ :arrow_down: ](https://arxiv.org/pdf/2101.11113.pdf)
>  Advantages of operating portions of a power system at frequencies different from the standard 50 or 60 Hz have been demonstrated in the low frequency AC (LFAC) and high voltage DC (HVDC) literature. Branches constrained by stability or thermal limits can benefit from increased capacity and flexibility. Since advances in power electronics enable the choice of an operating frequency, tools are needed to make this choice. In order to quantify the advantages as functions of frequency, this paper provides models for steady state calculations with frequency as a variable and validates the modeling assumptions. It then introduces an analytical quantification of the power flow capacity of a transmission branch as a function of frequency, demonstrating different active constraints across the range of frequency. The modeling and power flow calculations are demonstrated for a practical transmission line using manufacturer data. The models presented here provide a foundation for system level studies with variable frequency using optimal power flow.      
### 32.Blind Estimation of Reflectivity Profile Under Bayesian Setting Using MCMC Methods  [ :arrow_down: ](https://arxiv.org/pdf/2101.11079.pdf)
>  In this paper, we study the problem of inverse electromagnetic scattering to recover multilayer human tissue profiles using ultrawideband radar systems. We pose the recovery problem as a blind deconvolution problem, in which we simultaneously estimate both the transmitted pulse and the underlying dielectric and geometric properties of the one-dimensional tissue profile. We propose comprehensive Bayesian Markov Chain Monte Carlo methods, where the sampler parameters are adaptively updated to maintain desired acceptance ratios. We present the recovery performance of the proposed algorithms on simulated synthetic measurements. We also derive theoretical bounds for the estimation of dielectric properties and provide minimum achievable mean-square-errors for unbiased estimators.      
### 33.New Findings on GLRT Radar Detection of Nonfluctuating Targets via Phased Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2101.11077.pdf)
>  This paper addresses the standard generalized likelihood ratio test (GLRT) detection problem of weak signals in background noise. In so doing, we consider a nonfluctuating target embedded in complex white Gaussian noise (CWGN), in which the amplitude of the target echo and the noise power are assumed to be unknown. Important works have analyzed the performance for the referred scenario and proposed GLRT-based detectors. Such detectors are projected at an early stage (i.e., prior to the formation of a post-beamforming scalar waveform), thereby imposing high demands on hardware, processing, and data storage. From a hardware perspective, most radar systems fail to meet these strong requirements. In fact, due to hardware and computational constraints, most radars use a combination of analog and digital beamformers (sums) before any estimation or further pre-processing. The rationale behind this study is to derive a GLRT detector that meets the hardware and system requirements. In this work, we design and analyze a more practical and easy-to-implement GLRT detector, which is projected after the analog beamforming. The performance of the proposed detector is analyzed and the probabilities of detection (PD) and false alarm (PFA) are derived in closed form. Moreover, we show that in the low signal-to-noise ratio (SNR) regime, the post-beamforming GLRT detector performs better than both the classic pre-beamforming GLRT detector and the square-law detector. This finding suggests that if the signals are weak, instead of processing the signals separately, we first must to reinforce the overall signal and then assembling the system's detection statistic. At last, the SNR losses are quantified, in which the superiority of the post-beamforming GLRT detector was evidenced as the number of antennas and samples increase.      
### 34.Patient Diversion Across Primary Health Centers Using Real Time Delay Predictors  [ :arrow_down: ](https://arxiv.org/pdf/2101.11074.pdf)
>  In the current work, we consider diversion of childbirth patients who arrive seeking emergency admission to public primary health centers (PHCs). PHCs are the first point of contact for an Indian patient with formal medical care, and offer medical care on an outpatient basis, and limited inpatient and childbirth care. In this context, real-time prediction of the wait time of the arriving patient becomes important in order to determine whether the patient must be diverted to another PHC or not. We study this problem using a discrete event simulation that we develop of medical care operations in two PHCs in India. We approximate the labour room service at each PHC as an M/G/1 queueing system and show how the accuracy of real-time delay predictors impacts the extent of the change in operational outcomes at each PHC. We simulate patient diversion using actual delays as well as the delay estimates generated by various delay predictors based on the state of the system such as queue-length, elapsed service time, and observed delay histories. The simulation of the diversion process also incorporates travel time between the PHCs. We also propose a new delay predictor that incorporates information regarding the system state as well as the service time distribution. We compare the operational outcomes at both PHCs without diversion and with diversion using the above delay predictors. We show numerically that more accurate delay predictors lead to more equitable distribution of resources involved in provision of childbirth care across both PHCs.      
### 35.Effects of Image Size on Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.11508.pdf)
>  The question is: what size of the region of interest is likely to lead to better training outcomes? To answer this: The U-net is used for semantic segmentation. Image interpolation algorithms are used to double the cropped image size and create new datasets. Depending on the selected image interpolation algorithm category, non-original classes are created in the ground truth images thus a filtering strategy is introduced to remove such spurious classes. Evaluation results of effects on the myocardium segmentation and quantification of the myocardial infarction are provided and discussed.      
### 36.Utilizing Uncertainty Estimation in Deep Learning Segmentation of Fluorescence Microscopy Images with Missing Markers  [ :arrow_down: ](https://arxiv.org/pdf/2101.11476.pdf)
>  Fluorescence microscopy images contain several channels, each indicating a marker staining the sample. Since many different marker combinations are utilized in practice, it has been challenging to apply deep learning based segmentation models, which expect a predefined channel combination for all training samples as well as at inference for future application. Recent work circumvents this problem using a modality attention approach to be effective across any possible marker combination. However, for combinations that do not exist in a labeled training dataset, one cannot have any estimation of potential segmentation quality if that combination is encountered during inference. Without this, not only one lacks quality assurance but one also does not know where to put any additional imaging and labeling effort. We herein propose a method to estimate segmentation quality on unlabeled images by (i) estimating both aleatoric and epistemic uncertainties of convolutional neural networks for image segmentation, and (ii) training a Random Forest model for the interpretation of uncertainty features via regression to their corresponding segmentation metrics. Additionally, we demonstrate that including these uncertainty measures during training can provide an improvement on segmentation performance.      
### 37.Denoising Single Voxel Magnetic Resonance Spectroscopy with Deep Learning on Repeatedly Sampled In Vivo Data  [ :arrow_down: ](https://arxiv.org/pdf/2101.11442.pdf)
>  Objective: Magnetic Resonance Spectroscopy (MRS) is a noninvasive tool to reveal metabolic information. One challenge of MRS is the relatively low Signal-Noise Ratio (SNR) due to low concentrations of metabolites. To improve the SNR, the most common approach is to average signals that are acquired in multiple times. The data acquisition time, however, is increased by multiple times accordingly, resulting in the scanned objects uncomfortable or even unbearable. Methods: By exploring the multiple sampled data, a deep learning denoising approach is proposed to learn a mapping from the low SNR signal to the high SNR one. Results: Results on simulated and in vivo data show that the proposed method significantly reduces the data acquisition time with slightly compromised metabolic accuracy. Conclusion: A deep learning denoising method was proposed to significantly shorten the time of data acquisition, while maintaining signal accuracy and reliability. Significance: Provide a solution of the fundamental low SNR problem in MRS with artificial intelligence.      
### 38.Variable Petri Nets for Mobility  [ :arrow_down: ](https://arxiv.org/pdf/2101.11379.pdf)
>  Mobile computing systems, service-based systems and some other systems with mobile interacting components have recently received much attention. However, because of their characteristics such as mobility and disconnection, it is difficult to model and analyze them by using a structure-fixed model. This work proposes a new Petri net model called Variable Petri Net (VPN) for modeling and analyzing these systems. The definition, firing rule, and related analysis technology of VPN are introduced in detail. In a VPN, the possible interaction interfaces are abstracted as a new kind of places called virtual places, and the occurrences of (dis)connections are described by new functions, which makes it appropriate to describe the component collaboration in systems and realize the scalability and pluggability of systems. Moreover, to overcome the shortcoming that markings cannot reflect link capability of a system, VPNs add a constraint function along with a marking to represent a complete system configuration. Several examples are used to demonstrate the newly proposed model and method.      
### 39.On Massive IoT Connectivity with Temporally-Correlated User Activity  [ :arrow_down: ](https://arxiv.org/pdf/2101.11344.pdf)
>  This paper considers joint device activity detection and channel estimation in Internet of Things (IoT) networks, where a large number of IoT devices exist but merely a random subset of them become active for short-packet transmission at each time slot. In particular, we propose to leverage the temporal correlation in user activity, i.e., a device active at the previous time slot is more likely to be still active at the current moment, to improve the detection performance. Despite the temporally-correlated user activity in consecutive time slots, it is challenging to unveil the connection between the activity pattern estimated previously, which is imperfect but the only available side information (SI), and the true activity pattern at the current moment due to the unknown estimation error. In this work, we manage to tackle this challenge under the framework of approximate message passing (AMP). Specifically, thanks to the state evolution, the correlation between the activity pattern estimated by AMP at the previous time slot and the real activity pattern at the previous and current moment is quantified explicitly. Based on the well-defined temporal correlation, we further manage to embed this useful SI into the design of the minimum mean-squared error (MMSE) denoisers and log-likelihood ratio (LLR) test based activity detectors under the AMP framework. Theoretical comparison between the SI-aided AMP algorithm and its counterpart without utilizing temporal correlation is provided. Moreover, numerical results are given to show the significant gain in activity detection accuracy brought by the SI-aided algorithm.      
### 40.Joint Source-Channel Coding for Semantics-Aware Grant-Free Radio Access in IoT Fog Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.11309.pdf)
>  A fog-radio access network (F-RAN) architecture is studied for an Internet-of-Things (IoT) system in which wireless sensors monitor a number of multi-valued events and transmit in the uplink using grant-free random access to multiple edge nodes (ENs). Each EN is connected to a central processor (CP) via a finite-capacity fronthaul link. In contrast to conventional information-agnostic protocols based on separate source-channel (SSC) coding, where each device uses a separate codebook, this paper considers an information-centric approach based on joint source-channel (JSC) coding via a non-orthogonal generalization of type-based multiple access (TBMA). By leveraging the semantics of the observed signals, all sensors measuring the same event share the same codebook (with non-orthogonal codewords), and all such sensors making the same local estimate of the event transmit the same codeword. The F-RAN architecture directly detects the events values without first performing individual decoding for each device. Cloud and edge detection schemes based on Bayesian message passing are designed and trade-offs between cloud and edge processing are assessed.      
### 41.Inadequacy of Linear Methods for Minimal Sensor Placement and Feature Selection in Nonlinear Systems; a New Approach Using Secants  [ :arrow_down: ](https://arxiv.org/pdf/2101.11162.pdf)
>  Sensor placement and feature selection are critical steps in engineering, modeling, and data science that share a common mathematical theme: the selected measurements should enable solution of an inverse problem. Most real-world systems of interest are nonlinear, yet the majority of available techniques for feature selection and sensor placement rely on assumptions of linearity or simple statistical models. We show that when these assumptions are violated, standard techniques can lead to costly over-sensing without guaranteeing that the desired information can be recovered from the measurements. In order to remedy these problems, we introduce a novel data-driven approach for sensor placement and feature selection for a general type of nonlinear inverse problem based on the information contained in secant vectors between data points. Using the secant-based approach, we develop three efficient greedy algorithms that each provide different types of robust, near-minimal reconstruction guarantees. We demonstrate them on two problems where linear techniques consistently fail: sensor placement to reconstruct a fluid flow formed by a complicated shock-mixing layer interaction and selecting fundamental manifold learning coordinates on a torus.      
### 42.Towards low loss non-volatile phase change materials in mid index waveguides  [ :arrow_down: ](https://arxiv.org/pdf/2101.11127.pdf)
>  Photonic integrated circuits currently use platform intrinsic thermo-optic and electro-optic effects to implement dynamic functions such as switching, modulation and other processing. Currently, there is a drive to implement field programmable photonic circuits, a need which is only magnified by new neuromorphic and quantum computing applications. The most promising non-volatile photonic components employ phase change materials such as GST and GSST, which had their origin in electronic memory. However, in the optical domain, these compounds introduce significant losses potentially preventing a large number of applications. Here, we evaluate the use of two newly introduced low loss phase change materials, Sb2S3 and Sb2Se3, on a silicon nitride photonic platform. We focus the study on Mach-Zehnder interferometers that operate at the O and C bands to demonstrate the performance of the system. Our measurements show an insertion loss below 0.04 dB/um for Sb2S3 and lower than 0.09 dB/um for Sb2Se3 cladded devices for both amorphous and crystalline phases. The effective refractive index contrast for Sb2S3 on SiNx was measured to be 0.05 at 1310 nm and 0.02 at 1550 nm, whereas for Sb2Se3, it was 0.03 at 1310 nm and 0.05 at 1550 nm highlighting the performance of the integrated device.      
### 43.A Coding Theory Perspective on MultiplexedMolecular Profiling of Biological Tissues  [ :arrow_down: ](https://arxiv.org/pdf/2101.11123.pdf)
>  High-throughput and quantitative experimental technologies are experiencing rapid advances in the biological sciences. One important recent technique is multiplexed fluorescence in situ hybridization (mFISH), which enables the identification and localization of large numbers of individual strands of RNA within single cells. Core to that technology is a coding problem: with each RNA sequence of interest being a codeword, how to design a codebook of probes, and how to decode the resulting noisy measurements? Published work has relied on assumptions of uniformly distributed codewords and binary symmetric channels for decoding and to a lesser degree for code construction. Here we establish that both of these assumptions are inappropriate in the context of mFISH experiments and substantial decoding performance gains can be obtained by using more appropriate, less classical, assumptions. We propose a more appropriate asymmetric channel model that can be readily parameterized from data and use it to develop a maximum a posteriori (MAP) decoders. We show that false discovery rate for rare RNAs, which is the key experimental metric, is vastly improved with MAP decoders even when employed with the existing sub-optimal codebook. Using an evolutionary optimization methodology, we further show that by permuting the codebook to better align with the prior, which is an experimentally straightforward procedure, significant further improvements are possible.      
### 44.Exact and Approximate Heterogeneous Bayesian Decentralized Data Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2101.11116.pdf)
>  In Bayesian peer-to-peer decentralized data fusion for static and dynamic systems, the underlying estimated or communicated distributions are frequently assumed to be homogeneous between agents. This requires each agent to process and communicate the full global joint distribution, and thus leads to high computation and communication costs irrespective of relevancy to specific local objectives. This work considers a family of heterogeneous decentralized fusion problems, where we consider the set of problems in which either the communicated or the estimated distributions describe different, but overlapping, states of interest that are subsets of a larger full global joint state. We exploit the conditional independence structure of such problems and provide a rigorous derivation for a family of exact and approximate heterogeneous conditionally factorized channel filter methods. We further extend existing methods for approximate conservative filtering and decentralized fusion in heterogeneous dynamic problems. Numerical examples show more than 99.5% potential communication reduction for heterogeneous channel filter fusion, and a multi-target tracking simulation shows that these methods provide consistent estimates.      
### 45.Autonomous Off-road Navigation over Extreme Terrains with Perceptually-challenging Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2101.11110.pdf)
>  We propose a framework for resilient autonomous navigation in perceptually challenging unknown environments with mobility-stressing elements such as uneven surfaces with rocks and boulders, steep slopes, negative obstacles like cliffs and holes, and narrow passages. Environments are GPS-denied and perceptually-degraded with variable lighting from dark to lit and obscurants (dust, fog, smoke). Lack of prior maps and degraded communication eliminates the possibility of prior or off-board computation or operator intervention. This necessitates real-time on-board computation using noisy sensor data. To address these challenges, we propose a resilient architecture that exploits redundancy and heterogeneity in sensing modalities. Further resilience is achieved by triggering recovery behaviors upon failure. We propose a fast settling algorithm to generate robust multi-fidelity traversability estimates in real-time. The proposed approach was deployed on multiple physical systems including skid-steer and tracked robots, a high-speed RC car and legged robots, as a part of Team CoSTAR's effort to the DARPA Subterranean Challenge, where the team won 2nd and 1st place in the Tunnel and Urban Circuits, respectively.      
### 46.Automatically Identifying Language Family from Acoustic Examples in Low Resource Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2012.00876.pdf)
>  Existing multilingual speech NLP works focus on a relatively small subset of languages, and thus current linguistic understanding of languages predominantly stems from classical approaches. In this work, we propose a method to analyze language similarity using deep learning. Namely, we train a model on the Wilderness dataset and investigate how its latent space compares with classical language family findings. Our approach provides a new direction for cross-lingual data augmentation in any speech-based NLP task.      
