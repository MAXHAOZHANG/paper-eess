# ArXiv eess --Thu, 21 Jan 2021
### 1.Operation comfort vs. the importance of system components  [ :arrow_down: ](https://arxiv.org/pdf/2101.08205.pdf)
>  The purpose of this report is to look at the measures of importance of components in systems in terms of reliability. In the first work of Birnbaum (1968) on this subject, many interesting studies were created and important indicators were constructed that allowed to organize the components of complex systems. They are helpful in analyzing the reliability of the designed systems, establishing the principles of operation and maintenance. The significance measures presented here are collected and discussed regarding the motivation behind their creation. They concern an approach in which both elements and systems are binary, and the possibility of generalization to multistate systems is only mentioned. Among the discussed is one new proposal using the methods of game theory, combining the sensitivity to the structure of the system and the operational effects on the system performance. The presented severity measures use a knowledge of the system structure as well as reliability and wear and tear, and whether the components can be repaired and maintained.      
### 2.Alignment &amp; joint recovery of multi-slice dynamic MRI using deep generative manifold model  [ :arrow_down: ](https://arxiv.org/pdf/2101.08196.pdf)
>  We introduce a novel unsupervised deep generative manifold model for the recovery of multi-slice free-breathing and ungated cardiac MRI from highly undersampled measurements. The proposed scheme represents the multi-slice volume at each time point as the output of a deep convolutional neural network (CNN) generator, which is driven by latent vectors that capture the cardiac and respiratory phase at the specific time point. The main difference between the proposed method and the traditional CNN approaches is that the proposed scheme learns the network parameters from only the highly undersampled data rather than the extensive fully-sampled training data. We also learn the latent codes from the undersampled data using the stochastic gradient descent. Regularizations on the network and the latent codes are introduced to encourage the learning of smooth image manifold and the latent codes for each slice have the same distribution. The main benefits of the proposed scheme are (a) the ability to align multi-slice data and capitalize on the redundancy between the slices; (b) the ability to estimate the gating information directly from the k-t space data; and (c) the unsupervised learning strategy that eliminates the need for extensive training data.      
### 3.Modelling and Optimal Control of Multi Strain Epidemics, with Application to COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2101.08137.pdf)
>  This work introduces a novel epidemiological model that simultaneously considers multiple viral strains, reinfections due to waning immunity response over time and an optimal control formulation. This enables us to derive optimal mitigation strategies over a prescribed time horizon under a more realistic framework that does not imply perennial immunity and a single strain, although these can also be derived as particular cases of our formulation. The model also allows estimation of the number of infections over time in the absence of mitigation strategies under any number of viral strains. We validate our approach in the light of the COVID-19 epidemic and present a number of experiments to shed light on the overall behaviour under one or two strains in the absence of sufficient mitigation measures. We also derive optimal control strategies for distinct mitigation costs and evaluate the effect of these costs on the optimal mitigation measures over a two-year horizon. The results show that relaxations in the mitigation measures cause a rapid increase in the number of cases, which then demand more restrictive measures in the future.      
### 4.High-throughput fast full-color digital pathology based on Fourier ptychographic microscopy via color transfer  [ :arrow_down: ](https://arxiv.org/pdf/2101.08136.pdf)
>  Full-color imaging is significant in digital pathology. Compared with a grayscale image or a pseudo-color image that only contains the contrast information, it can identify and detect the target object better with color texture information. Fourier ptychographic microscopy (FPM) is a high-throughput computational imaging technique that breaks the tradeoff between high resolution (HR) and large field-of-view (FOV), which eliminates the artifacts of scanning and stitching in digital pathology and improves its imaging efficiency. However, the conventional full-color digital pathology based on FPM is still time-consuming due to the repeated experiments with tri-wavelengths. A color transfer FPM approach, termed CFPM was reported. The color texture information of a low resolution (LR) full-color pathologic image is directly transferred to the HR grayscale FPM image captured by only a single wavelength. The color space of FPM based on the standard CIE-XYZ color model and display based on the standard RGB (sRGB) color space were established. Different FPM colorization schemes were analyzed and compared with thirty different biological samples. The average root-mean-square error (RMSE) of the conventional method and CFPM compared with the ground truth is 5.3% and 5.7%, respectively. Therefore, the acquisition time is significantly reduced by 2/3 with the sacrifice of precision of only 0.4%. And CFPM method is also compatible with advanced fast FPM approaches to reduce computation time further.      
### 5.Review of Machine Learning Applications in Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2101.08119.pdf)
>  This paper looks at various aspects of Machine Learning (ML) applications in wireless communication technologies, focusing mainly on fifth-generation (5G) and millimeter wave (mmWave) technologies. This paper includes the summaries of 3 papers on machine learning applications in wireless communication technology. The paper deals with the need for integration of machine learning in wireless communication, types of machine learning techniques used in wireless communication, advantages and potential of ML in wireless communication, and implementation parameters of ML in wireless communication, as well as a study on RSS-Based Classification of usage in indoor millimeter-wave wireless networks.      
### 6.NB-IoT Random Access for Non-Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.08079.pdf)
>  The satellite component is recognized as a promising solution to complement and extend the coverage of future Internet of things (IoT) terrestrial networks (TNs). In this context, a study item to integrate satellites into narrowband IoT (NB-IoT) systems has been approved within the 3rd Generation Partnership Project (3GPP) standardization body. However, as NB-IoT systems were initially conceived for TNs, their basic design principles and operation might require some key modifications when incorporating the satellite component. These changes in NB-IoT systems, therefore, need to be carefully implemented in order to guarantee a seamless integration of both TN and non-terrestrial network (NTN) for a global coverage. This paper addresses this adaptation for the random access (RA) step in NB-IoT systems, which is in fact the most challenging aspect in the NTN context, for it deals with multi-user time-frequency synchronization and timing advance for data scheduling. In particular, we propose an RA technique which is robust to typical satellite channel impairments, including long delays, significant Doppler effects, and wide beams, without requiring any modification to the current NBIoT RA waveform. Performance evaluations demonstrate the proposal's capability of addressing different NTN configurations recently defined by 3GPP for the 5G new radio system.      
### 7.Collision-Free Flocking with a Dynamic Squad of Fixed-Wing UAVs Using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.08074.pdf)
>  Developing the collision-free flocking behavior for a dynamic squad of fixed-wing UAVs is still a challenge due to kinematic complexity and environmental uncertainty. In this paper, we deal with the decentralized leader-follower flocking control problem through deep reinforcement learning (DRL). Specifically, we formulate a decentralized DRL-based decision making framework from the perspective of every follower, where a collision avoidance mechanism is integrated into the flocking controller. Then, we propose a novel reinforcement learning algorithm CACER-II for training a shared control policy for all the followers. Besides, we design a plug-n-play embedding module based on convolutional neural networks and the attention mechanism. As a result, the variable-length system state can be encoded into a fixed-length embedding vector, which makes the learned DRL policies independent with the number or the order of followers. Finally, numerical simulation results demonstrate the effectiveness of the proposed method, and the learned policies can be directly transferred to semiphysical simulation without any parameter finetuning.      
### 8.Voltage Inference for and Coordination of Distributed Voltage Controls in Extremely-High DER-Penetration Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.08054.pdf)
>  The unique problems and phenomena in the distributed voltage control of large-scale power distribution systems with extremely-high DER-penetration are targeted in this paper. First, a DER-explicit distribution network model and voltage sensitivity are derived. Based on that, a voltage inference method is implemented to fill the gap of measurement insufficiency in the grid-edge areas. Then, autonomous Q control being implemented in each local area, a $\overline{Q}$-coordinated P control is designed to coordinate the reactive and real power controls. All the algorithms have been tested in standard and synthetic systems, and have expected results. Moreover, an open-source software platform, which integrates the modeling of communication networks, DER controls, and power networks, is developed to enable the distributed control and optimization algorithms in the grid simulation of the large-scale distribution systems.      
### 9.Variational Autoencoders with a Structural Similarity Loss in Time of Flight MRAs  [ :arrow_down: ](https://arxiv.org/pdf/2101.08052.pdf)
>  Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) enable visualization and analysis of cerebral arteries. This analysis may indicate normal variation of the configuration of the cerebrovascular system or vessel abnormalities, such as aneurysms. A model would be useful to represent normal cerebrovascular structure and variabilities in a healthy population and to differentiate from abnormalities. Current anomaly detection using autoencoding convolutional neural networks usually use a voxelwise mean-error for optimization. We propose optimizing a variational-autoencoder (VAE) with structural similarity loss (SSIM) for TOF-MRA reconstruction. A patch-trained 2D fully-convolutional VAE was optimized for TOF-MRA reconstruction by comparing vessel segmentations of original and reconstructed MRAs. The method was trained and tested on two datasets: the IXI dataset, and a subset from the ADAM challenge. Both trained networks were tested on a dataset including subjects with aneurysms. We compared VAE optimization with L2-loss and SSIM-loss. Performance was evaluated between original and reconstructed MRAs using mean square error, mean-SSIM, peak-signal-to-noise-ratio and dice similarity index (DSI) of segmented vessels. The L2-optimized VAE outperforms SSIM, with improved reconstruction metrics and DSIs for both datasets. Optimization using SSIM performed best for visual image quality, but with discrepancy in quantitative reconstruction and vascular segmentation. The larger, more diverse IXI dataset had overall better performance. Reconstruction metrics, including SSIM, were lower for MRAs including aneurysms. A SSIM-optimized VAE improved the visual perceptive image quality of TOF-MRA reconstructions. A L2-optimized VAE performed best for TOF-MRA reconstruction, where the vascular segmentation is important. SSIM is a potential metric for anomaly detection of MRAs.      
### 10.Broadband Non-Geostationary Satellite Communication Systems: Research Challenges and Key Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2101.08051.pdf)
>  Besides conventional geostationary (GSO) satellite broadband communication services, non-geostationary (NGSO) satellites are envisioned to support various new communication use cases from countless industries. These new scenarios bring many unprecedented challenges that will be discussed in this paper alongside with several potential future research opportunities. NGSO systems are known for various advantages, including their important features of low cost, lower propagation delay, smaller size, and lower losses in comparison to GSO satellites. However, there are still many deployment challenges to be tackled to ensure seamless integration not only with GSO systems but also with terrestrial networks. In this paper, we discuss several key challenges including satellite constellation and architecture designs, coexistence with GSO systems in terms of spectrum access and regulatory issues, resource management algorithms, and NGSO networking requirements. Additionally, the latest progress in provisioning secure communication via NGSO systems is discussed. Finally, this paper identifies multiple important open issues and research directions to inspire further studies towards the next generation of satellite networks.      
### 11.Bayesian Optimization Assisted Meal Bolus Decision Based on Gaussian Processes Learning and Risk-Sensitive Control  [ :arrow_down: ](https://arxiv.org/pdf/2101.08044.pdf)
>  Effective postprandial glucose control is important to glucose management for subjects with diabetes mellitus. In this work, a data-driven meal bolus decision method is proposed without the need of subject-specific glucose management parameters. The postprandial glucose dynamics is learnt using Gaussian process regression. Considering the asymmetric risks of hyper- and hypoglycemia and the uncertainties in the predicted glucose trajectories, an asymmetric risk-sensitive cost function is designed. Bayesian optimization is utilized to solve the optimization problem, since the gradient of the cost function is unavailable. The proposed approach is evaluated using the 10-adult cohort of the FDA-accepted UVA/Padova T1DM simulator and compared with the standard insulin bolus calculator. For the case of announced meals, the proposed method achieves satisfactory and similar performance in terms of mean glucose and percentage time in [70, 180] mg/dL without increasing the risk of hypoglycemia. Similar results are observed for the case without the meal information (assuming that the patient follows a consistent diet) and the case of basal rate mismatches. In addition, advisory-mode analysis is performed based on clinical data, which indicates that the method can determine safe and reasonable meal boluses in real clinical settings. The results verify the effectiveness and robustness of the proposed method and indicate the feasibility of achieving improved postprandial glucose regulation through a data-driven optimal control method.      
### 12.Bridge the Vision Gap from Field to Command: A Deep Learning Network Enhancing Illumination and Details  [ :arrow_down: ](https://arxiv.org/pdf/2101.08039.pdf)
>  With the goal of tuning up the brightness, low-light image enhancement enjoys numerous applications, such as surveillance, remote sensing and computational photography. Images captured under low-light conditions often suffer from poor visibility and blur. Solely brightening the dark regions will inevitably amplify the blur, thus may lead to detail loss. In this paper, we propose a simple yet effective two-stream framework named NEID to tune up the brightness and enhance the details simultaneously without introducing many computational costs. Precisely, the proposed method consists of three parts: Light Enhancement (LE), Detail Refinement (DR) and Feature Fusing (FF) module, which can aggregate composite features oriented to multiple tasks based on channel attention mechanism. Extensive experiments conducted on several benchmark datasets demonstrate the efficacy of our method and its superiority over state-of-the-art methods.      
### 13.Data-Driven Distributionally Robust Optimization for Real-Time Economic Dispatch Considering Secondary Frequency Regulation Cost  [ :arrow_down: ](https://arxiv.org/pdf/2101.08027.pdf)
>  With the large-scale integration of renewable power generation, frequency regulation resources (FRRs) are required to have larger capacities and faster ramp rates, which increases the cost of the frequency regulation ancillary service. Therefore, it is necessary to consider the frequency regulation cost and constraint along with real-time economic dispatch (RTED). In this paper, a data-driven distributionally robust optimization (DRO) method for RTED considering automatic generation control (AGC) is proposed. First, a Copula-based AGC signal model is developed to reflect the correlations among the AGC signal, load power and renewable generation variations. Secondly, samples of the AGC signal are taken from its conditional probability distribution under the forecasted load power and renewable generation variations. Thirdly, a distributionally robust RTED model considering the frequency regulation cost and constraint is built and transformed into a linear programming problem by leveraging the Wasserstein metric-based DRO technique. Simulation results show that the proposed method can reduce the total cost of power generation and frequency regulation.      
### 14.Minimum Length Scheduling for Multi-cell Full Duplex Wireless Powered Communication Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.08002.pdf)
>  In this paper, we investigate a novel minimum length scheduling problem to determine the optimal power control, and scheduling for constant and continuous rate models, while considering concurrent transmission of users, energy causality, maximum transmit power and traffic demand constraints. The formulated optimization problems are shown to be non-convex and combinatorial in nature, thus, difficult to solve for the global optimum. As a solution strategy, first, we propose optimal polynomial time algorithms for the power control problem considering constant and continuous rate models based on the evaluation of Perron-Frobenius conditions and usage of bisection method, respectively. Then, the proposed optimal power control solutions are used to determine the optimal transmission time for a subset of users that will be scheduled by the scheduling algorithms. For the constant rate scheduling problem, we propose a heuristic algorithm that aims to maximize the number of concurrently transmitting users by maximizing the allowable interference on each user without violating the signal-to-noise-ratio (SNR) requirements. For the continuous rate scheduling problem, we define a penalty function representing the advantage of concurrent transmission over individual transmission of the users. Following the optimality analysis of the penalty metric and demonstration of the equivalence between schedule length minimization and minimization of the sum of penalties, we propose a heuristic algorithm based on the allocation of the concurrently transmitting users with the goal of minimizing the sum penalties over the schedule. Through extensive simulations, we demonstrate that the proposed algorithm outperforms the successive transmission and concurrent transmission of randomly selected users for different HAP transmit powers, network densities and network size.      
### 15.Data-driven sparse polynomial chaos expansion for models with dependent inputs  [ :arrow_down: ](https://arxiv.org/pdf/2101.07997.pdf)
>  Polynomial chaos expansions (PCEs) have been used in many real-world engineering applications to quantify how the uncertainty of an output is propagated from inputs. PCEs for models with independent inputs have been extensively explored in the literature. Recently, different approaches have been proposed for models with dependent inputs to expand the use of PCEs to more real-world applications. Typical approaches include building PCEs based on the Gram-Schmidt algorithm or transforming the dependent inputs into independent inputs. However, the two approaches have their limitations regarding computational efficiency and additional assumptions about the input distributions, respectively. In this paper, we propose a data-driven approach to build sparse PCEs for models with dependent inputs. The proposed algorithm recursively constructs orthonormal polynomials using a set of monomials based on their correlations with the output. The proposed algorithm on building sparse PCEs not only reduces the number of minimally required observations but also improves the numerical stability and computational efficiency. Four numerical examples are implemented to validate the proposed algorithm.      
### 16.Cell image segmentation by Feature Random Enhancement Module  [ :arrow_down: ](https://arxiv.org/pdf/2101.07983.pdf)
>  It is important to extract good features using an encoder to realize semantic segmentation with high accuracy. Although loss function is optimized in training deep neural network, far layers from the layers for computing loss function are difficult to train. Skip connection is effective for this problem but there are still far layers from the loss function. In this paper, we propose the Feature Random Enhancement Module which enhances the features randomly in only training. By emphasizing the features at far layers from loss function, we can train those layers well and the accuracy was improved. In experiments, we evaluated the proposed module on two kinds of cell image datasets, and our module improved the segmentation accuracy without increasing computational cost in test phase.      
### 17.Identifying First-order Lowpass Graph Signals using Perron Frobenius Theorem  [ :arrow_down: ](https://arxiv.org/pdf/2101.07938.pdf)
>  This paper is concerned with the blind identification of graph filters from graph signals. Our aim is to determine if the graph filter generating the graph signals is first-order lowpass without knowing the graph topology. Notice that lowpass graph filter is a common prerequisite for applying graph signal processing tools for sampling, denoising, and graph learning. Our method is inspired by the Perron Frobenius theorem, which observes that for first-order lowpass graph filter, the top eigenvector of output covariance would be the only eigenvector with elements of the same sign. Utilizing this observation, we develop a simple detector that answers if a given data set is produced by a first-order lowpass graph filter. We analyze the effects of finite-sample, graph size, observation noise, strength of lowpass filter, on the detector's performance. Numerical experiments on synthetic and real data support our findings.      
### 18.Quarter Laplacian Filter for Edge Aware Image Processing  [ :arrow_down: ](https://arxiv.org/pdf/2101.07933.pdf)
>  This paper presents a quarter Laplacian filter that can preserve corners and edges during image smoothing. Its support region is $2\times2$, which is smaller than the $3\times3$ support region of Laplacian filter. Thus, it is more local. Moreover, this filter can be implemented via the classical box filter, leading to high performance for real time applications. Finally, we show its edge preserving property in several image processing tasks, including image smoothing, texture enhancement, and low-light image enhancement. The proposed filter can be adopted in a wide range of image processing applications.      
### 19.Impacts of Earthquakes on Electrical Grid Resilience  [ :arrow_down: ](https://arxiv.org/pdf/2101.07928.pdf)
>  One of the most complex and devastating disaster scenarios that the U.S.~Pacific Northwest region and the state of Oregon faces is a large magnitude Cascadia Subduction Zone earthquake event. The region's electrical grid lacks in resilience against the destruction of a megathrust earthquake, a powerful tsunami, hundreds of aftershocks and increased volcanic activity, all of which are highly probable components of this hazard. This research seeks to catalyze further understanding and improvement of resilience. By systematizing power system related experiences of historical earthquakes, and collecting practical and innovative ideas from other regions on how to enhance network design, construction, and operation, important steps are being taken toward a more resilient, earthquake-resistant grid. This paper presents relevant findings in an effort to be an overview and a useful guideline for those who are also working towards greater electrical grid resilience.      
### 20.A Guide to Design Disturbance Observer-based Motion Control Systems in Discrete-time Domain  [ :arrow_down: ](https://arxiv.org/pdf/2101.07920.pdf)
>  This paper analyses and synthesises the Disturbance Observer (DOb) based motion control systems in the discrete-time domain. By employing Bode Integral Theorem, it is shown that continuous-time analysis methods fall-short in explaining the dynamic behaviours of the DOb-based robust motion controllers implemented by computers and microcontrollers. For example, continuous-time analysis methods cannot explain why the robust stability and performance of the digital motion controller deteriorate as the bandwidth of the DOb increases. Therefore, unexpected dynamic responses (e.g., poor stability and performance, and high-sensitivity to disturbances and noise) may be observed when the parameters of the digital robust motion controller are tuned by using continuous-time synthesis methods in practice. This paper also analytically derives the robust stability and performance constraints of the DOb-based motion controllers in the discrete-time domain. The proposed design constraints allow one to systematically synthesise a high-performance digital robust motion controller. The validity of the proposed analysis and synthesis methods are verified by simulations.      
### 21.Fine-Tuning and Training of DenseNet for Histopathology Image Representation Using TCGA Diagnostic Slides  [ :arrow_down: ](https://arxiv.org/pdf/2101.07903.pdf)
>  Feature vectors provided by pre-trained deep artificial neural networks have become a dominant source for image representation in recent literature. Their contribution to the performance of image analysis can be improved through finetuning. As an ultimate solution, one might even train a deep network from scratch with the domain-relevant images, a highly desirable option which is generally impeded in pathology by lack of labeled images and the computational expense. In this study, we propose a new network, namely KimiaNet, that employs the topology of the DenseNet with four dense blocks, fine-tuned and trained with histopathology images in different configurations. We used more than 240,000 image patches with 1000x1000 pixels acquired at 20x magnification through our proposed "highcellularity mosaic" approach to enable the usage of weak labels of 7,126 whole slide images of formalin-fixed paraffin-embedded human pathology samples publicly available through the The Cancer Genome Atlas (TCGA) repository. We tested KimiaNet using three public datasets, namely TCGA, endometrial cancer images, and colorectal cancer images by evaluating the performance of search and classification when corresponding features of different networks are used for image representation. As well, we designed and trained multiple convolutional batch-normalized ReLU (CBR) networks. The results show that KimiaNet provides superior results compared to the original DenseNet and smaller CBR networks when used as feature extractor to represent histopathology images.      
### 22.Classification of COVID-19 X-ray Images Using a Combination of Deep and Handcrafted Features  [ :arrow_down: ](https://arxiv.org/pdf/2101.07866.pdf)
>  Coronavirus Disease 2019 (COVID-19) demonstrated the need for accurate and fast diagnosis methods for emergent viral diseases. Soon after the emergence of COVID-19, medical practitioners used X-ray and computed tomography (CT) images of patients' lungs to detect COVID-19. Machine learning methods are capable of improving the identification accuracy of COVID-19 in X-ray and CT images, delivering near real-time results, while alleviating the burden on medical practitioners. In this work, we demonstrate the efficacy of a support vector machine (SVM) classifier, trained with a combination of deep convolutional and handcrafted features extracted from X-ray chest scans. We use this combination of features to discriminate between healthy, common pneumonia, and COVID-19 patients. The performance of the combined feature approach is compared with a standard convolutional neural network (CNN) and the SVM trained with handcrafted features. We find that combining the features in our novel framework improves the performance of the classification task compared to the independent application of convolutional and handcrafted features. Specifically, we achieve an accuracy of 0.988 in the classification task with our combined approach compared to 0.963 and 0.983 accuracy for the handcrafted features with SVM and CNN respectively.      
### 23.A Physics-Based Finite-State Abstraction for Traffic Congestion Control  [ :arrow_down: ](https://arxiv.org/pdf/2101.07865.pdf)
>  This paper offers a finite-state abstraction of traffic coordination and congestion in a network of interconnected roads (NOIR). By applying mass conservation, we model traffic coordination as a Markov process. Model Predictive Control (MPC) is applied to control traffic congestion through the boundary of the traffic network. The optimal boundary inflow is assigned as the solution of a constrained quadratic programming problem. Additionally, the movement phases commanded by traffic signals are determined using receding horizon optimization. In simulation, we show how traffic congestion can be successfully controlled through optimizing boundary inflow and movement phases at traffic network junctions.      
### 24.Safe and Efficient Model-free Adaptive Control via Bayesian Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2101.07825.pdf)
>  Adaptive control approaches yield high-performance controllers when a precise system model or suitable parametrizations of the controller are available. Existing data-driven approaches for adaptive control mostly augment standard model-based methods with additional information about uncertainties in the dynamics or about disturbances. In this work, we propose a purely data-driven, model-free approach for adaptive control. Tuning low-level controllers based solely on system data raises concerns on the underlying algorithm safety and computational performance. Thus, our approach builds on GoOSE, an algorithm for safe and sample-efficient Bayesian optimization. We introduce several computational and algorithmic modifications in GoOSE that enable its practical use on a rotational motion system. We numerically demonstrate for several types of disturbances that our approach is sample efficient, outperforms constrained Bayesian optimization in terms of safety, and achieves the performance optima computed by grid evaluation. We further demonstrate the proposed adaptive control approach experimentally on a rotational motion system.      
### 25.Internet of Predictable Things (IoPT) Framework to Increase Cyber-Physical System Resiliency  [ :arrow_down: ](https://arxiv.org/pdf/2101.07816.pdf)
>  During the last two decades, distributed energy systems, especially renewable energy sources (RES), have become more economically viable with increasing market share and penetration levels on power systems. In addition to decarbonization and decentralization of energy systems, digitalization has also become very important. The use of artificial intelligence (AI), advanced optimization algorithms, Industrial Internet of Things (IIoT), and other digitalization frameworks makes modern power system assets more intelligent, while vulnerable to cybersecurity risks. This paper proposes the concept of the Internet of Predictable Things (IoPT) that incorporates advanced data analytics and machine learning methods to increase the resiliency of cyber-physical systems against cybersecurity risks. The proposed concept is demonstrated using a cyber-physical system testbed under a variety of cyber attack scenarios as a proof of concept (PoC).      
### 26.Trajectory optimization for contact-rich motions using implicit differential dynamic programming  [ :arrow_down: ](https://arxiv.org/pdf/2101.08246.pdf)
>  This paper presents a novel approach using sensitivity analysis for generalizing Differential Dynamic Programming (DDP) to systems characterized by implicit dynamics, such as those modelled via inverse dynamics and variational or implicit integrators. It leads to a more general formulation of DDP, enabling for example the use of the faster recursive Newton-Euler inverse dynamics. We leverage the implicit formulation for precise and exact contact modelling in DDP, where we focus on two contributions: (1) Contact dynamics in acceleration level that enables high-order integration schemes; (2) Formulation using an invertible contact model in the forward pass and a closed form solution in the backward pass to improve the numerical resolution of contacts. The performance of the proposed framework is validated (1) by comparing implicit versus explicit DDP for the swing-up of a double pendulum, and (2) by planning motions for two tasks using a single leg model making multi-body contacts with the environment: standing up from ground, where a priori contact enumeration is challenging, and maintaining balance under an external perturbation.      
### 27.Probabilistic Solar Power Forecasting: Long Short-Term Memory Network vs Simpler Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2101.08236.pdf)
>  The high penetration of volatile renewable energy sources such as solar make methods for coping with the uncertainty associated with them of paramount importance. Probabilistic forecasts are an example of these methods, as they assist energy planners in their decision-making process by providing them with information about the uncertainty of future power generation. Currently, there is a trend towards the use of deep learning probabilistic forecasting methods. However, the point at which the more complex deep learning methods should be preferred over more simple approaches is not yet clear. Therefore, the current article presents a simple comparison between a long short-term memory neural network and other more simple approaches. The comparison consists of training and comparing models able to provide one-day-ahead probabilistic forecasts for a solar power system. Moreover, the current paper makes use of an open-source dataset provided during the Global Energy Forecasting Competition of 2014 (GEFCom14).      
### 28.Data Association Between Perception and V2V Communication Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2101.08228.pdf)
>  The connectivity between vehicles, infrastructure, and other traffic participants brings a new dimension to automotive safety applications. Soon all the newly produced cars will have Vehicle to Everything (V2X) communication modems alongside the existing Advanced Driver Assistant Systems (ADAS). It is essential to identify the different sensor measurements for the same targets (Data Association) to use connectivity reliably as a safety feature alongside the standard ADAS functionality. Considering the camera is the most common sensor available for ADAS systems, in this paper, we present an experimental implementation of a Mahalanobis distance-based data association algorithm between the camera and the Vehicle to Vehicle (V2V) communication sensors. The implemented algorithm has low computational complexity and the capability of running in real-time. One can use the presented algorithm for sensor fusion algorithms or higher-level decision-making applications in ADAS modules.      
### 29.Trimming the Fat from OFDM: Pilot- and CP-less Communication with End-to-end Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.08213.pdf)
>  Orthogonal frequency division multiplexing (OFDM) is one of the dominant waveforms in wireless communication systems due to its efficient implementation. However, it suffers from a loss of spectral efficiency as it requires a cyclic prefix (CP) to mitigate inter-symbol interference (ISI) and pilots to estimate the channel. We propose in this work to address these drawbacks by learning a neural network (NN)-based receiver jointly with a constellation geometry and bit labeling at the transmitter, that allows CP-less and pilotless communication on top of OFDM without a significant loss in bit error rate (BER). Our approach enables at least 18% throughput gains compared to a pilot and CP-based baseline, and at least 4% gains compared to a system that uses a neural receiver with pilots but no CP.      
### 30.Self-supervised pre-training enhances change detection in Sentinel-2 imagery  [ :arrow_down: ](https://arxiv.org/pdf/2101.08122.pdf)
>  While annotated images for change detection using satellite imagery are scarce and costly to obtain, there is a wealth of unlabeled images being generated every day. In order to leverage these data to learn an image representation more adequate for change detection, we explore methods that exploit the temporal consistency of Sentinel-2 times series to obtain a usable self-supervised learning signal. For this, we build and make publicly available (<a class="link-external link-https" href="https://zenodo.org/record/4280482" rel="external noopener nofollow">this https URL</a>) the Sentinel-2 Multitemporal Cities Pairs (S2MTCP) dataset, containing multitemporal image pairs from 1520 urban areas worldwide. We test the results of multiple self-supervised learning methods for pre-training models for change detection and apply it on a public change detection dataset made of Sentinel-2 image pairs (OSCD).      
### 31.Active Model Learning using Informative Trajectories for Improved Closed-Loop Control on Real Robots  [ :arrow_down: ](https://arxiv.org/pdf/2101.08100.pdf)
>  Model-based controllers on real robots require accurate knowledge of the system dynamics to perform optimally. For complex dynamics, first-principles modeling is not sufficiently precise, and data-driven approaches can be leveraged to learn a statistical model from real experiments. However, the efficient and effective data collection for such a data-driven system on real robots is still an open challenge. This paper introduces an optimization problem formulation to find an informative trajectory that allows for efficient data collection and model learning. We present a sampling-based method that computes an approximation of the trajectory that minimizes the prediction uncertainty of the dynamics model. This trajectory is then executed, collecting the data to update the learned model. In experiments we demonstrate the capabilities of our proposed framework when applied to a complex omnidirectional flying vehicle with tiltable rotors. Using our informative trajectories results in models which outperform models obtained from non-informative trajectory by 13.3\% with the same amount of training data. Furthermore, we show that the model learned from informative trajectories generalizes better than the one learned from non-informative trajectories, achieving better tracking performance on different tasks.      
### 32.Component Tree Loss Function: Definition and Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2101.08063.pdf)
>  In this article, we propose a method to design loss functions based on component trees which can be optimized by gradient descent algorithms and which are therefore usable in conjunction with recent machine learning approaches such as neural networks. We show how the altitudes associated to the nodes of such hierarchical image representations can be differentiated with respect to the image pixel values. This feature is used to design a generic loss function that can select or discard image maxima based on various attributes such as extinction values. The possibilities of the proposed method are demonstrated on simulated and real image filtering.      
### 33.On the Parametrization and Statistics of Propagation Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2101.08056.pdf)
>  Propagation graphs (PGs) serve as a frequency-selective, spatially consistent channel model suitable for fast channel simulations in a scattering environment. So far, however, the parametrization of the model, and its consequences, have received little attention. In this contribution, we propose a new parametrization for PGs that adheres to the doubly exponentially decaying cluster structure of the Saleh-Valenzuela (SV) model. We show how to compute the newly proposed internal model parameters based on an approximation of the $K$-factor and the two decay rates from the SV model. Furthermore, via the singular values of multiple-input multiple-output (MIMO) channels, we compare the degrees of freedom (DoF) between our new and another frequently used parametrization. Specifically, we compare the DoF loss when the distance between antennas within the transmitter and receiver arrays or the average distance between scatterers decreases. Based on this comparison, it is shown that, in contrast to the typical parametrization, our newly proposed parametrization loses DoF in both scenarios, as one would expect from a spatially consistent channel model.      
### 34.Riemannian-based Discriminant Analysis for Feature Extraction and Classification  [ :arrow_down: ](https://arxiv.org/pdf/2101.08032.pdf)
>  Discriminant analysis, as a widely used approach in machine learning to extract low-dimensional features from the high-dimensional data, applies the Fisher discriminant criterion to find the orthogonal discriminant projection subspace. But most of the Euclidean-based algorithms for discriminant analysis are easily convergent to a spurious local minima and hardly obtain an unique solution. To address such problem, in this study we propose a novel method named Riemannian-based Discriminant Analysis (RDA), which transforms the traditional Euclidean-based methods to the Riemannian manifold space. In RDA, the second-order geometry of trust-region methods is utilized to learn the discriminant bases. To validate the efficiency and effectiveness of RDA, we conduct a variety of experiments on image classification tasks. The numerical results suggest that RDA can extract statistically significant features and robustly outperform state-of-the-art algorithms in classification tasks.      
### 35.Scalable Deep Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2101.08024.pdf)
>  Deep learning has been used to image compressive sensing (CS) for enhanced reconstruction performance. However, most existing deep learning methods train different models for different subsampling ratios, which brings additional hardware burden. In this paper, we develop a general framework named scalable deep compressive sensing (SDCS) for the scalable sampling and reconstruction (SSR) of all existing end-to-end-trained models. In the proposed way, images are measured and initialized linearly. Two sampling masks are introduced to flexibly control the subsampling ratios used in sampling and reconstruction, respectively. To make the reconstruction model adapt to any subsampling ratio, a training strategy dubbed scalable training is developed. In scalable training, the model is trained with the sampling matrix and the initialization matrix at various subsampling ratios by integrating different sampling matrix masks. Experimental results show that models with SDCS can achieve SSR without changing their structure while maintaining good performance, and SDCS outperforms other SSR methods.      
### 36.Deep Learning for Intelligent Demand Response and Smart Grids: A Comprehensive Survey  [ :arrow_down: ](https://arxiv.org/pdf/2101.08013.pdf)
>  Electricity is one of the mandatory commodities for mankind today. To address challenges and issues in the transmission of electricity through the traditional grid, the concepts of smart grids and demand response have been developed. In such systems, a large amount of data is generated daily from various sources such as power generation (e.g., wind turbines), transmission and distribution (microgrids and fault detectors), load management (smart meters and smart electric appliances). Thanks to recent advancements in big data and computing technologies, Deep Learning (DL) can be leveraged to learn the patterns from the generated data and predict the demand for electricity and peak hours. Motivated by the advantages of deep learning in smart grids, this paper sets to provide a comprehensive survey on the application of DL for intelligent smart grids and demand response. Firstly, we present the fundamental of DL, smart grids, demand response, and the motivation behind the use of DL. Secondly, we review the state-of-the-art applications of DL in smart grids and demand response, including electric load forecasting, state estimation, energy theft detection, energy sharing and trading. Furthermore, we illustrate the practicality of DL via various use cases and projects. Finally, we highlight the challenges presented in existing research works and highlight important issues and potential directions in the use of DL for smart grids and demand response.      
### 37.GEOMScope: Large Field-of-view 3D Lensless Microscopy with Low Computational Complexity  [ :arrow_down: ](https://arxiv.org/pdf/2101.07975.pdf)
>  Recent development of lensless imagers has enabled three-dimensional (3D) imaging through a thin piece of optics in close proximity to a camera sensor. A general challenge of wide-field lensless imaging is the high computational complexity and slow speed to reconstruct 3D objects through iterative optimization process. Here, we demonstrated GEOMScope, a lensless 3D microscope that forms image through a single layer of microlens array and reconstructs objects through a geometrical-optics-based pixel back projection algorithm and background suppressions. Compared to others, our method allows local reconstruction, which significantly reduces the required computation resource and increases the reconstruction speed by orders of magnitude. This enables near real-time object reconstructions across a large volume of 23x23x5 mm^3, with a lateral resolution of 40 um and axial resolution of 300 um. Our system opens new avenues for broad biomedical applications such as endoscopy, which requires both miniaturized device footprint and real-time high resolution visualization.      
### 38.Spinal Codes Optimization: Error Probability Analysis and Transmission Scheme Design  [ :arrow_down: ](https://arxiv.org/pdf/2101.07953.pdf)
>  Spinal codes are known to be capacity achieving over both the additive white Gaussian noise (AWGN) channel and the binary symmetric channel (BSC). Over wireless channels, Spinal encoding can also be regarded as an adaptive-coded-modulation (ACM) technique due to its rateless property, which fits it with mobile communications. Due to lack of tight analysis on error probability of Spinal codes, optimization of transmission scheme using Spinal codes has not been fully explored. In this work, we firstly derive new tight upper bounds of the frame error rate (FER) of Spinal codes for both the AWGN channel and the BSC in the finite block-length (FBL) regime. Based on the derived upper bounds, we then design the optimal transmission scheme. Specifically, we formulate a rate maximization problem as a nonlinear integer programming problem, and solve it by an iterative algorithm for its dual problem. As the optimal solution exhibits an incremental-tail-transmission pattern, we propose an improved transmission scheme for Spinal codes. Moreover, we develop a bubble decoding with memory (BD-M) algorithm to reduce the decoding time complexity without loss of rate performance. The improved transmission scheme at the transmitter and the BD-M algorithm at the receiver jointly constitute an "encoding-decoding" system of Spinal codes. Simulation results demonstrate that it can improve both the rate performance and the decoding throughput of Spinal codes.      
### 39.Noise Learning Based Denoising Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2101.07937.pdf)
>  This letter introduces a new denoiser that modifies the structure of denoising autoencoder (DAE), namely noise learning based DAE (nlDAE). The proposed nlDAE learns the noise instead of the original data. Then, the denoising is performed by subtracting the regenerated noise from the noisy input. Hence, nlDAE is more effective than DAE when the noise is simpler to regenerate than the original data. To validate the performance of nlDAE, we provide two case studies: symbol demodulation and precise localization. Numerical results suggest that nlDAE requires smaller latent space dimension and less training dataset compared to DAE.      
### 40.A Discrete Scheme for Computing Image's Weighted Gaussian Curvature  [ :arrow_down: ](https://arxiv.org/pdf/2101.07927.pdf)
>  Weighted Gaussian Curvature is an important measurement for images. However, its conventional computation scheme has low performance, low accuracy and requires that the input image must be second order differentiable. To tackle these three issues, we propose a novel discrete computation scheme for the weighted Gaussian curvature. Our scheme does not require the second order differentiability. Moreover, our scheme is more accurate, has smaller support region and computationally more efficient than the conventional schemes. Therefore, our scheme holds promise for a large range of applications where the weighted Gaussian curvature is needed, for example, image smoothing, cartoon texture decomposition, optical flow estimation, etc.      
### 41.Scalable Optimization for Wind Farm Control using Coordination Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2101.07844.pdf)
>  Wind farms are a crucial driver toward the generation of ecological and renewable energy. Due to their rapid increase in capacity, contemporary wind farms need to adhere to strict constraints on power output to ensure stability of the electricity grid. Specifically, a wind farm controller is required to match the farm's power production with a power demand imposed by the grid operator. This is a non-trivial optimization problem, as complex dependencies exist between the wind turbines. State-of-the-art wind farm control typically relies on physics-based heuristics that fail to capture the full load spectrum that defines a turbine's health status. When this is not taken into account, the long-term viability of the farm's turbines is put at risk. Given the complex dependencies that determine a turbine's lifetime, learning a flexible and optimal control strategy requires a data-driven approach. However, as wind farms are large-scale multi-agent systems, optimizing control strategies over the full joint action space is intractable. We propose a new learning method for wind farm control that leverages the sparse wind farm structure to factorize the optimization problem. Using a Bayesian approach, based on multi-agent Thompson sampling, we explore the factored joint action space for configurations that match the demand, while considering the lifetime of turbines. We apply our method to a grid-like wind farm layout, and evaluate configurations using a state-of-the-art wind flow simulator. Our results are competitive with a physics-based heuristic approach in terms of demand error, while, contrary to the heuristic, our method prolongs the lifetime of high-risk turbines.      
### 42.An Analytical Expression for the Effective Area of the Step-Index Single-Mode Optical Fiber  [ :arrow_down: ](https://arxiv.org/pdf/2101.07842.pdf)
>  The nonlinear coefficient $\gamma$ of the step-index single-mode fiber (SMF) is inversely proportional to the effective area of that fiber. An analytical expression for the effective area of the hybrid HE11-mode of the SMF is derived for the first time, using its exact electromagnetic field expressions. A simpler expression is also deduced under the weakly-guided fiber approximation. The expressions are found in terms of the generalized hypergeometric function.      
### 43.Implicit Bias of Linear RNNs  [ :arrow_down: ](https://arxiv.org/pdf/2101.07833.pdf)
>  Contemporary wisdom based on empirical studies suggests that standard recurrent neural networks (RNNs) do not perform well on tasks requiring long-term memory. However, precise reasoning for this behavior is still unknown. This paper provides a rigorous explanation of this property in the special case of linear RNNs. Although this work is limited to linear RNNs, even these systems have traditionally been difficult to analyze due to their non-linear parameterization. Using recently-developed kernel regime analysis, our main result shows that linear RNNs learned from random initializations are functionally equivalent to a certain weighted 1D-convolutional network. Importantly, the weightings in the equivalent model cause an implicit bias to elements with smaller time lags in the convolution and hence, shorter memory. The degree of this bias depends on the variance of the transition kernel matrix at initialization and is related to the classic exploding and vanishing gradients problem. The theory is validated in both synthetic and real data experiments.      
