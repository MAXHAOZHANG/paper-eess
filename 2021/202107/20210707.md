# ArXiv eess --Wed, 7 Jul 2021
### 1.Lexical Access Model for Italian -- Modeling human speech processing: identification of words in running speech toward lexical access based on the detection of landmarks and other acoustic cues to features  [ :arrow_down: ](https://arxiv.org/pdf/2107.02720.pdf)
>  Modelling the process that a listener actuates in deriving the words intended by a speaker requires setting a hypothesis on how lexical items are stored in memory. This work aims at developing a system that imitates humans when identifying words in running speech and, in this way, provide a framework to better understand human speech processing. We build a speech recognizer for Italian based on the principles of Stevens' model of Lexical Access in which words are stored as hierarchical arrangements of distinctive features (Stevens, K. N. (2002). "Toward a model for lexical access based on acoustic landmarks and distinctive features," J. Acoust. Soc. Am., 111(4):1872-1891). Over the past few decades, the Speech Communication Group at the Massachusetts Institute of Technology (MIT) developed a speech recognition system for English based on this approach. Italian will be the first language beyond English to be explored; the extension to another language provides the opportunity to test the hypothesis that words are represented in memory as a set of hierarchically-arranged distinctive features, and reveal which of the underlying mechanisms may have a language-independent nature. This paper also introduces a new Lexical Access corpus, the LaMIT database, created and labeled specifically for this work, that will be provided freely to the speech research community. Future developments will test the hypothesis that specific acoustic discontinuities - called landmarks - that serve as cues to features, are language independent, while other cues may be language-dependent, with powerful implications for understanding how the human brain recognizes speech.      
### 2.Unsupervised learning of MRI tissue properties using MRI physics models  [ :arrow_down: ](https://arxiv.org/pdf/2107.02704.pdf)
>  In neuroimaging, MRI tissue properties characterize underlying neurobiology, provide quantitative biomarkers for neurological disease detection and analysis, and can be used to synthesize arbitrary MRI contrasts. Estimating tissue properties from a single scan session using a protocol available on all clinical scanners promises to reduce scan time and cost, enable quantitative analysis in routine clinical scans and provide scan-independent biomarkers of disease. However, existing tissue properties estimation methods - most often $\mathbf{T_1}$ relaxation, $\mathbf{T_2^*}$ relaxation, and proton density ($\mathbf{PD}$) - require data from multiple scan sessions and cannot estimate all properties from a single clinically available MRI protocol such as the multiecho MRI scan. In addition, the widespread use of non-standard acquisition parameters across clinical imaging sites require estimation methods that can generalize across varying scanner parameters. However, existing learning methods are acquisition protocol specific and cannot estimate from heterogenous clinical data from different imaging sites. In this work we propose an unsupervised deep-learning strategy that employs MRI physics to estimate all three tissue properties from a single multiecho MRI scan session, and generalizes across varying acquisition parameters. The proposed strategy optimizes accurate synthesis of new MRI contrasts from estimated latent tissue properties, enabling unsupervised training, we also employ random acquisition parameters during training to achieve acquisition generalization. We provide the first demonstration of estimating all tissue properties from a single multiecho scan session. We demonstrate improved accuracy and generalizability for tissue property estimation and MRI synthesis.      
### 3.On the Impact of Oscillator Phase Noise in an IRS-assisted MISO TDD System  [ :arrow_down: ](https://arxiv.org/pdf/2107.02698.pdf)
>  In this work, we study the impact of the multiplicative phase noise in an IRS-assisted system. We consider an IRS-assisted system with multiplicative phase noise both at the BS and user. A novel channel estimation algorithm is proposed considering the phase noise. By utilizing the proposed channel estimates we investigate the system performance in the downlink, more specifically, we derive the ergodic capacity in closed form. Simulation results verify the correctness of the closed-form expression. We observe that the system becomes more robust against the phase noise as the number of reflective elements increases. Moreover, the influence of the additive channel noise in uplink vanishes as the number of reflecting elements grows asymptotically large.      
### 4.COVID-19 Pneumonia Severity Prediction using Hybrid Convolution-Attention Neural Architectures  [ :arrow_down: ](https://arxiv.org/pdf/2107.02672.pdf)
>  This study proposed a novel framework for COVID-19 severity prediction, which is a combination of data-centric and model-centric approaches. First, we propose a data-centric pre-training for extremely scare data scenarios of the investigating dataset. Second, we propose two hybrid convolution-attention neural architectures that leverage the self-attention from Transformer and Hopfield networks. Our proposed approach achieves significant improvement from the conventional baseline approach. The best model from our proposed approach achieves $R^2 = 0.85 \pm 0.05$ and Pearson correlation coefficient $\rho = 0.92 \pm 0.02$ in geographic extend and $R^2 = 0.72 \pm 0.09, \rho = 0.85\pm 0.06$ in opacity prediction.      
### 5.Exploiting Single-Channel Speech For Multi-channel End-to-end Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2107.02670.pdf)
>  Recently, the end-to-end training approach for neural beamformer-supported multi-channel ASR has shown its effectiveness in multi-channel speech recognition. However, the integration of multiple modules makes it more difficult to perform end-to-end training, particularly given that the multi-channel speech corpus recorded in real environments with a sizeable data scale is relatively limited. This paper explores the usage of single-channel data to improve the multi-channel end-to-end speech recognition system. Specifically, we design three schemes to exploit the single-channel data, namely pre-training, data scheduling, and data simulation. Extensive experiments on CHiME4 and AISHELL-4 datasets demonstrate that all three methods improve the multi-channel end-to-end training stability and speech recognition performance, while the data scheduling approach keeps a much simpler pipeline (vs. pre-training) and less computation cost (vs. data simulation). Moreover, we give a thorough analysis of our systems, including how the performance is affected by the choice of front-end, the data augmentation, training strategy, and single-channel data size.      
### 6.Detecting Hypo-plastic Left Heart Syndrome in Fetal Ultrasound via Disease-specific Atlas Maps  [ :arrow_down: ](https://arxiv.org/pdf/2107.02643.pdf)
>  Fetal ultrasound screening during pregnancy plays a vital role in the early detection of fetal malformations which have potential long-term health impacts. The level of skill required to diagnose such malformations from live ultrasound during examination is high and resources for screening are often limited. We present an interpretable, atlas-learning segmentation method for automatic diagnosis of Hypo-plastic Left Heart Syndrome (HLHS) from a single `4 Chamber Heart' view image. We propose to extend the recently introduced Image-and-Spatial Transformer Networks (Atlas-ISTN) into a framework that enables sensitising atlas generation to disease. In this framework we can jointly learn image segmentation, registration, atlas construction and disease prediction while providing a maximum level of clinical interpretability compared to direct image classification methods. As a result our segmentation allows diagnoses competitive with expert-derived manual diagnosis and yields an AUC-ROC of 0.978 (1043 cases for training, 260 for validation and 325 for testing).      
### 7.Differentially private federated deep learning for multi-site medical image segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.02586.pdf)
>  Collaborative machine learning techniques such as federated learning (FL) enable the training of models on effectively larger datasets without data transfer. Recent initiatives have demonstrated that segmentation models trained with FL can achieve performance similar to locally trained models. However, FL is not a fully privacy-preserving technique and privacy-centred attacks can disclose confidential patient data. Thus, supplementing FL with privacy-enhancing technologies (PTs) such as differential privacy (DP) is a requirement for clinical applications in a multi-institutional setting. The application of PTs to FL in medical imaging and the trade-offs between privacy guarantees and model utility, the ramifications on training performance and the susceptibility of the final models to attacks have not yet been conclusively investigated. Here we demonstrate the first application of differentially private gradient descent-based FL on the task of semantic segmentation in computed tomography. We find that high segmentation performance is possible under strong privacy guarantees with an acceptable training time penalty. We furthermore demonstrate the first successful gradient-based model inversion attack on a semantic segmentation model and show that the application of DP prevents it from divulging sensitive image features.      
### 8.Unsupervised Knowledge-Transfer for Learned Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2107.02572.pdf)
>  Deep learning-based image reconstruction approaches have demonstrated impressive empirical performance in many imaging modalities. These approaches generally require a large amount of high-quality training data, which is often not available. To circumvent this issue, we develop a novel unsupervised knowledge-transfer paradigm for learned iterative reconstruction within a Bayesian framework. The proposed approach learns an iterative reconstruction network in two phases. The first phase trains a reconstruction network with a set of ordered pairs comprising of ground truth images and measurement data. The second phase fine-tunes the pretrained network to the measurement data without supervision. Furthermore, the framework delivers uncertainty information over the reconstructed image. We present extensive experimental results on low-dose and sparse-view computed tomography, showing that the proposed framework significantly improves reconstruction quality not only visually, but also quantitatively in terms of PSNR and SSIM, and is competitive with several state-of-the-art supervised and unsupervised reconstruction techniques.      
### 9.A Theory of the Distortion-Perception Tradeoff in Wasserstein Space  [ :arrow_down: ](https://arxiv.org/pdf/2107.02555.pdf)
>  The lower the distortion of an estimator, the more the distribution of its outputs generally deviates from the distribution of the signals it attempts to estimate. This phenomenon, known as the perception-distortion tradeoff, has captured significant attention in image restoration, where it implies that fidelity to ground truth images comes at the expense of perceptual quality (deviation from statistics of natural images). However, despite the increasing popularity of performing comparisons on the perception-distortion plane, there remains an important open question: what is the minimal distortion that can be achieved under a given perception constraint? In this paper, we derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. We prove that the DP function is always quadratic, regardless of the underlying distribution. This stems from the fact that estimators on the DP curve form a geodesic in Wasserstein space. In the Gaussian setting, we further provide a closed form expression for such estimators. For general distributions, we show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint. The latter can be obtained as a stochastic transformation of the former.      
### 10.Location, Location: Enhancing the Evaluation of Text-to-Speech Synthesis Using the Rapid Prosody Transcription Paradigm  [ :arrow_down: ](https://arxiv.org/pdf/2107.02527.pdf)
>  Text-to-Speech synthesis systems are generally evaluated using Mean Opinion Score (MOS) tests, where listeners score samples of synthetic speech on a Likert scale. A major drawback of MOS tests is that they only offer a general measure of overall quality-i.e., the naturalness of an utterance-and so cannot tell us where exactly synthesis errors occur. This can make evaluation of the appropriateness of prosodic variation within utterances inconclusive. To address this, we propose a novel evaluation method based on the Rapid Prosody Transcription paradigm. This allows listeners to mark the locations of errors in an utterance in real-time, providing a probabilistic representation of the perceptual errors that occur in the synthetic signal. We conduct experiments that confirm that the fine-grained evaluation can be mapped to system rankings of standard MOS tests, but the error marking gives a much more comprehensive assessment of synthesized prosody. In particular, for standard audiobook test set samples, we see that error marks consistently cluster around words at major prosodic boundaries indicated by punctuation. However, for question-answer based stimuli, where we control information structure, we see differences emerge in the ability of neural TTS systems to generate context-appropriate prosodic prominence.      
### 11.Deep Learning Methods for Joint Optimization of Beamforming and Fronthaul Quantization in Cloud Radio Access Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.02520.pdf)
>  Cooperative beamforming across access points (APs) and fronthaul quantization strategies are essential for cloud radio access network (C-RAN) systems. The nonconvexity of the C-RAN optimization problems, which is stemmed from per-AP power and fronthaul capacity constraints, requires high computational complexity for executing iterative algorithms. To resolve this issue, we investigate a deep learning approach where the optimization module is replaced with a well-trained deep neural network (DNN). An efficient learning solution is proposed which constructs a DNN to produce a low-dimensional representation of optimal beamforming and quantization strategies. Numerical results validate the advantages of the proposed learning solution.      
### 12.A new smart-cropping pipeline for prostate segmentation using deep learning networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.02476.pdf)
>  Prostate segmentation from magnetic resonance imaging (MRI) is a challenging task. In recent years, several network architectures have been proposed to automate this process and alleviate the burden of manual annotation. Although the performance of these models has achieved promising results, there is still room for improvement before these models can be used safely and effectively in clinical practice. One of the major challenges in prostate MR image segmentation is the presence of class imbalance in the image labels where the background pixels dominate over the prostate. In the present work we propose a DL-based pipeline for cropping the region around the prostate from MRI images to produce a more balanced distribution of the foreground pixels (prostate) and the background pixels and improve segmentation accuracy. The effect of DL-cropping for improving the segmentation performance compared to standard center-cropping is assessed using five popular DL networks for prostate segmentation, namely U-net, U-net+, Res Unet++, Bridge U-net and Dense U-net. The proposed smart-cropping outperformed the standard center cropping in terms of segmentation accuracy for all the evaluated prostate segmentation networks. In terms of Dice score, the highest improvement was achieved for the U-net+ and ResU-net++ architectures corresponding to 8.9% and 8%, respectively.      
### 13.Separation Guided Speaker Diarization in Realistic Mismatched Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2107.02357.pdf)
>  We propose a separation guided speaker diarization (SGSD) approach by fully utilizing a complementarity of speech separation and speaker clustering. Since the conventional clustering-based speaker diarization (CSD) approach cannot well handle overlapping speech segments, we investigate, in this study, separation-based speaker diarization (SSD) which inherently has the potential to handle the speaker overlap regions. Our preliminary analysis shows that the state-of-the-art Conv-TasNet based speech separation, which works quite well on the simulation data, is unstable in realistic conversational speech due to the high mismatch speaking styles in simulated training speech and read speech. In doing so, separation-based processing can assist CSD in handling the overlapping speech segments under the realistic mismatched conditions. Specifically, several strategies are designed to select between the results of SSD and CSD systems based on an analysis of the instability of the SSD system performances. Experiments on the conversational telephone speech (CTS) data from DIHARD-III Challenge show that the proposed SGSD system can significantly improve the performance of state-of-the-art CSD systems, yielding relative diarization error rate reductions of 20.2% and 20.8% on the development set and evaluation set, respectively.      
### 14.Total Nitrogen Estimation in Agricultural Soils via Aerial Multispectral Imaging and LIBS  [ :arrow_down: ](https://arxiv.org/pdf/2107.02355.pdf)
>  Measuring soil health indicators is an important and challenging task that affects farmers' decisions on timing, placement, and quantity of fertilizers applied in the farms. Most existing methods to measure soil health indicators (SHIs) are in-lab wet chemistry or spectroscopy-based methods, which require significant human input and effort, time-consuming, costly, and are low-throughput in nature. To address this challenge, we develop an artificial intelligence (AI)-driven near real-time unmanned aerial vehicle (UAV)-based multispectral sensing (UMS) solution to estimate total nitrogen (TN) of the soil, an important macro-nutrient or SHI that directly affects the crop health. Accurate prediction of soil TN can significantly increase crop yield through informed decision making on the timing of seed planting, and fertilizer quantity and timing. We train two machine learning models including multi-layer perceptron and support vector machine to predict the soil nitrogen using a suite of data classes including multispectral characteristics of the soil and crops in red, near-infrared, and green spectral bands, computed vegetation indices, and environmental variables including air temperature and relative humidity. To generate the ground-truth data or the training data for the machine learning models, we measure the total nitrogen of the soil samples (collected from a farm) using laser-induced breakdown spectroscopy (LIBS).      
### 15.Domain Adaptation via CycleGAN for Retina Segmentation in Optical Coherence Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2107.02345.pdf)
>  With the FDA approval of Artificial Intelligence (AI) for point-of-care clinical diagnoses, model generalizability is of the utmost importance as clinical decision-making must be domain-agnostic. A method of tackling the problem is to increase the dataset to include images from a multitude of domains; while this technique is ideal, the security requirements of medical data is a major limitation. Additionally, researchers with developed tools benefit from the addition of open-sourced data, but are limited by the difference in domains. Herewith, we investigated the implementation of a Cycle-Consistent Generative Adversarial Networks (CycleGAN) for the domain adaptation of Optical Coherence Tomography (OCT) volumes. This study was done in collaboration with the Biomedical Optics Research Group and Functional &amp; Anatomical Imaging &amp; Shape Analysis Lab at Simon Fraser University. In this study, we investigated a learning-based approach of adapting the domain of a publicly available dataset, UK Biobank dataset (UKB). To evaluate the performance of domain adaptation, we utilized pre-existing retinal layer segmentation tools developed on a different set of RETOUCH OCT data. This study provides insight on state-of-the-art tools for domain adaptation compared to traditional processing techniques as well as a pipeline for adapting publicly available retinal data to the domains previously used by our collaborators.      
### 16.Impact of deep learning-based image super-resolution on binary signal detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.02338.pdf)
>  Deep learning-based image super-resolution (DL-SR) has shown great promise in medical imaging applications. To date, most of the proposed methods for DL-SR have only been assessed by use of traditional measures of image quality (IQ) that are commonly employed in the field of computer vision. However, the impact of these methods on objective measures of image quality that are relevant to medical imaging tasks remains largely unexplored. In this study, we investigate the impact of DL-SR methods on binary signal detection performance. Two popular DL-SR methods, the super-resolution convolutional neural network (SRCNN) and the super-resolution generative adversarial network (SRGAN), were trained by use of simulated medical image data. Binary signal-known-exactly with background-known-statistically (SKE/BKS) and signal-known-statistically with background-known-statistically (SKS/BKS) detection tasks were formulated. Numerical observers, which included a neural network-approximated ideal observer and common linear numerical observers, were employed to assess the impact of DL-SR on task performance. The impact of the complexity of the DL-SR network architectures on task-performance was quantified. In addition, the utility of DL-SR for improving the task-performance of sub-optimal observers was investigated. Our numerical experiments confirmed that, as expected, DL-SR could improve traditional measures of IQ. However, for many of the study designs considered, the DL-SR methods provided little or no improvement in task performance and could even degrade it. It was observed that DL-SR could improve the task-performance of sub-optimal observers under certain conditions. The presented study highlights the urgent need for the objective assessment of DL-SR methods and suggests avenues for improving their efficacy in medical imaging applications.      
### 17.Integrated Satellite-HAP-Terrestrial Networks for Dual-Band Connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2107.02336.pdf)
>  The recent development of high-altitude platforms (HAPs) has attracted increasing attention since they can serve as a promising communication method to assist satellite-terrestrial networks. In this paper, we consider an integrated three-layer satellite-HAP-terrestrial network where the HAP support dual-band connectivity. Specifically, the HAP can not only communicate with terrestrial users over C-band directly, but also provide backhaul services to terrestrial user terminals over Ka-band. We formulate a sum-rate maximization problem and then propose a fractional programming based algorithm to solve the problem by optimizing the bandwidth and power allocation iteratively. The closed-form optimal solutions for bandwidth allocation and power allocation in each iteration are also derived. Simulation results show the capacity enhancement brought by the dual-band connectivity of the HAP. The influence of the power of the HAP and the power of the satellite is also discussed.      
### 18.Exploring Deep Learning Methods for Real-Time Surgical Instrument Segmentation in Laparoscopy  [ :arrow_down: ](https://arxiv.org/pdf/2107.02319.pdf)
>  Minimally invasive surgery is a surgical intervention used to examine the organs inside the abdomen and has been widely used due to its effectiveness over open surgery. Due to the hardware improvements such as high definition cameras, this procedure has significantly improved and new software methods have demonstrated potential for computer-assisted procedures. However, there exists challenges and requirements to improve detection and tracking of the position of the instruments during these surgical procedures. To this end, we evaluate and compare some popular deep learning methods that can be explored for the automated segmentation of surgical instruments in laparoscopy, an important step towards tool tracking. Our experimental results exhibit that the Dual decoder attention network (DDANet) produces a superior result compared to other recent deep learning methods. DDANet yields a Dice coefficient of 0.8739 and mean intersection-over-union of 0.8183 for the Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of 101.36 frames-per-second that is critical for such procedures.      
### 19.Histogram of Cell Types: Deep Learning for Automated Bone Marrow Cytology  [ :arrow_down: ](https://arxiv.org/pdf/2107.02293.pdf)
>  Bone marrow cytology is required to make a hematological diagnosis, influencing critical clinical decision points in hematology. However, bone marrow cytology is tedious, limited to experienced reference centers and associated with high inter-observer variability. This may lead to a delayed or incorrect diagnosis, leaving an unmet need for innovative supporting technologies. We have developed the first ever end-to-end deep learning-based technology for automated bone marrow cytology. Starting with a bone marrow aspirate digital whole slide image, our technology rapidly and automatically detects suitable regions for cytology, and subsequently identifies and classifies all bone marrow cells in each region. This collective cytomorphological information is captured in a novel representation called Histogram of Cell Types (HCT) quantifying bone marrow cell class probability distribution and acting as a cytological "patient fingerprint". The approach achieves high accuracy in region detection (0.97 accuracy and 0.99 ROC AUC), and cell detection and cell classification (0.75 mAP, 0.78 F1-score, Log-average miss rate of 0.31). HCT has potential to revolutionize hematopathology diagnostic workflows, leading to more cost-effective, accurate diagnosis and opening the door to precision medicine.      
### 20.Empowering cyberphysical systems of systems with intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2107.02264.pdf)
>  Cyber Physical Systems have been going into a transition phase from individual systems to a collecttives of systems that collaborate in order to achieve a highly complex cause, realizing a system of systems approach. The automotive domain has been making a transition to the system of system approach aiming to provide a series of emergent functionality like traffic management, collaborative car fleet management or large-scale automotive adaptation to physical environment thus providing significant environmental benefits (e.g air pollution reduction) and achieving significant societal impact. Similarly, large infrastructure domains, are evolving into global, highly integrated cyber-physical systems of systems covering all parts of the value chain. In practice, there are significant challenges in CPSoS applicability and usability to be addressed, i.e. even a small CPSoS such as a car consists several subsystems Decentralization of CPSoS appoints tasks to individual CPSs within the System of Systems. CPSoSs are heterogenous systems. They comprise of various, autonomous, CPSs, each one of them having unique performance capabilities, criticality level, priorities and pursued goals. all CPSs must also harmonically pursue system-based achievements and collaborate in order to make system-of-system based decisions and implement the CPSoS functionality. This survey will provide a comprehensive review on current best practices in connected cyberphysical systems. The basis of our investigation is a dual layer architecture encompassing a perception layer and a behavioral layer. Perception algorithms with respect to scene understanding (object detection and tracking, pose estimation), localization mapping and path planning are thoroughly investigated. Behavioural part focuses on decision making and human in the loop control.      
### 21.A Unified Theory of Adaptive Subspace Detection. Part I: Detector Designs  [ :arrow_down: ](https://arxiv.org/pdf/2107.02235.pdf)
>  This paper addresses the problem of detecting multidimensional subspace signals, which model range-spread targets, in noise of unknown covariance. It is assumed that a primary channel of measurements, possibly consisting of signal plus noise, is augmented with a secondary channel of measurements containing only noise. The noises in these two channels share a common covariance matrix, up to a scale, which may be known or unknown. The signal model is a subspace model with variations: the subspace may be known or known only by its dimension; consecutive visits to the subspace may be unconstrained or they may be constrained by a prior distribution. As a consequence, there are four general classes of detectors and, within each class, there is a detector for the case where the scale between the primary and secondary channels is known, and for the case where this scale is unknown. The generalized likelihood ratio (GLR) based detectors derived in this paper, when organized with previously published GLR detectors, comprise a unified theory of adaptive subspace detection from primary and secondary channels of measurements.      
### 22.Automated age-related macular degeneration area estimation -- first results  [ :arrow_down: ](https://arxiv.org/pdf/2107.02211.pdf)
>  This work aims to research an automatic method for detecting Age-related Macular Degeneration (AMD) lesions in RGB eye fundus images. For this, we align invasively obtained eye fundus contrast images (the "golden standard" diagnostic) to the RGB ones and use them to hand-annotate the lesions. This is done using our custom-made tool. Using the data, we train and test five different convolutional neural networks: a custom one to classify healthy and AMD-affected eye fundi, and four well-known networks: ResNet50, ResNet101, MobileNetV3, and UNet to segment (localize) the AMD lesions in the affected eye fundus images. We achieve 93.55% accuracy or 69.71% Dice index as the preliminary best results in segmentation with MobileNetV3.      
### 23.Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of Absolute Difference  [ :arrow_down: ](https://arxiv.org/pdf/2107.02762.pdf)
>  Euclids algorithm is widely used in calculating of GCD (Greatest Common Divisor) of two positive numbers. There are various fields where this division is used such as channel coding, cryptography, and error correction codes. This makes the GCD a fundamental algorithm in number theory, so a number of methods have been discovered to efficiently compute it. The main contribution of this paper is to investigate a method that computes the GCD of two 32-bit numbers based on Euclidean algorithm which targets six different Xilinx chips. The complexity of this method that we call Optimized_GCDSAD is achieved by utilizing Sum of Absolute Difference (SAD) block which is based on a fast carry-out generation function. The efficiency of the proposed architecture is evaluated based on criteria such as time (latency), area delay product (ADP) and space (slice number) complexity. The VHDL codes of these architectures have been implemented and synthesized through ISE 14.7. A detailed comparative analysis indicates that the proposed Optimized_GCDSAD method based on SAD block outperforms previously known results.      
### 24.Frequency Shift Algorithm: Design of a Baseband Phase Locked Loop for Frequency-Domain Multiplexing Readout of X-ray Transition-Edge Sensor Microcalorimeters  [ :arrow_down: ](https://arxiv.org/pdf/2107.02754.pdf)
>  The Transition-Edge Sensor (TES) is an extremely sensitive device which is used to measure the energy of individual X-ray photons. For astronomical spectrometry applications, SRON develops a Frequency Domain Multiplexing (FDM) read-out system for kilopixel arrays of such TESs. Each TES is voltage biased at a specific frequency in the range 1 to 5 MHz. Isolation between the individual pixels is obtained through very narrow-band (high-Q) lithographic LC resonators. To prevent energy resolution degradation due to intermodulation line noise, the bias frequencies are distributed on a regular grid. The requirements on the accuracy of the LC resonance frequency are very high. The deviation of the resonance frequencies due to production tolerances is significant with respect to the bandwidth, and a controller is necessary to compensate for the LC series impedance. We present two such controllers: a simple orthogonal proportional-integrating (PI) controller and a more complex impedance estimator. Both controllers operate in baseband and try to make the TES current in-phase with the bias voltage, effectively operating as phase-locked loops (PLL). They allow off-LC-resonance operation of the TES pixels, while preserving TES thermal response and energy resolution. Extensive experimental results -- published in a companion paper recently -- with the proposed methods, show that these controllers allow the preservation of single pixel energy resolution in multiplexed operation.      
### 25.Saturation-Aware Model Predictive Energy Management for Droop-Controlled Islanded Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2107.02719.pdf)
>  In this paper, we propose a minimax model predictive control (MPC)-based energy management system that is robust with respect to uncertainties in renewable infeed and load. The MPC formulation includes a model of low-level droop control with saturation at the power and energy limits of the units. Robust MPC-based energy management systems tend to under-utilize the renewable energy sources to guarantee safe operation. In order to mitigate this effect, we further consider droop control of renewable energy sources. For a microgrid with droop-controlled units, we show that enhancing droop feedback with saturation enlarges the space of feasible control actions. However, the resulting controller requires to solve a mixed-integer problem with additional variables and equations representing saturation. We derive a computationally tractable formulation for this problem. Furthermore, we investigate the performance gained by using droop with saturation, renewable droop and combination of both in a case study.      
### 26.Evaluating subgroup disparity using epistemic uncertainty in mammography  [ :arrow_down: ](https://arxiv.org/pdf/2107.02716.pdf)
>  As machine learning (ML) continue to be integrated into healthcare systems that affect clinical decision making, new strategies will need to be incorporated in order to effectively detect and evaluate subgroup disparities to ensure accountability and generalizability in clinical workflows. In this paper, we explore how epistemic uncertainty can be used to evaluate disparity in patient demographics (race) and data acquisition (scanner) subgroups for breast density assessment on a dataset of 108,190 mammograms collected from 33 clinical sites. Our results show that even if aggregate performance is comparable, the choice of uncertainty quantification metric can significantly the subgroup level. We hope this analysis can promote further work on how uncertainty can be leveraged to increase transparency of machine learning applications for clinical deployment.      
### 27.HybrUR: A Hybrid Physical-Neural Solution for Unsupervised Underwater Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2107.02660.pdf)
>  Robust vision restoration for an underwater image remains a challenging problem. For the lack of aligned underwater-terrestrial image pairs, the unsupervised method is more suited to this task. However, the pure data-driven unsupervised method usually has difficulty in achieving realistic color correction for lack of optical constraint. In this paper, we propose a data- and physics-driven unsupervised architecture that learns underwater vision restoration from unpaired underwater-terrestrial images. For sufficient domain transformation and detail preservation, the underwater degeneration needs to be explicitly constructed based on the optically unambiguous physics law. Thus, we employ the Jaffe-McGlamery degradation theory to design the generation models, and use neural networks to describe the process of underwater degradation. Furthermore, to overcome the problem of invalid gradient when optimizing the hybrid physical-neural model, we fully investigate the intrinsic correlation between the scene depth and the degradation factors for the backscattering estimation, to improve the restoration performance through physical constraints. Our experimental results show that the proposed method is able to perform high-quality restoration for unconstrained underwater images without any supervision. On multiple benchmarks, we outperform several state-of-the-art supervised and unsupervised approaches. We also demonstrate that our methods yield encouraging results on real-world applications.      
### 28.Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.02655.pdf)
>  Due to a high heterogeneity in pose and size and to a limited number of available data, segmentation of pediatric images is challenging for deep learning methods. In this work, we propose a new CNN architecture that is pose and scale invariant thanks to the use of Spatial Transformer Network (STN). Our architecture is composed of three sequential modules that are estimated together during training: (i) a regression module to estimate a similarity matrix to normalize the input image to a reference one; (ii) a differentiable module to find the region of interest to segment; (iii) a segmentation module, based on the popular UNet architecture, to delineate the object. Unlike the original UNet, which strives to learn a complex mapping, including pose and scale variations, from a finite training dataset, our segmentation module learns a simpler mapping focusing on images with normalized pose and size. Furthermore, the use of an automatic bounding box detection through STN allows saving time and especially memory, while keeping similar performance. We test the proposed method in kidney and renal tumor segmentation on abdominal pediatric CT scanners. Results indicate that the estimated STN homogenization of size and pose accelerates the segmentation (25h), compared to standard data-augmentation (33h), while obtaining a similar quality for the kidney (88.01\% of Dice score) and improving the renal tumor delineation (from 85.52\% to 87.12\%).      
### 29.Hyperspectral Pansharpening Based on Improved Deep Image Prior and Residual Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2107.02630.pdf)
>  Hyperspectral pansharpening aims to synthesize a low-resolution hyperspectral image (LR-HSI) with a registered panchromatic image (PAN) to generate an enhanced HSI with high spectral and spatial resolution. Recently proposed HS pansharpening methods have obtained remarkable results using deep convolutional networks (ConvNets), which typically consist of three steps: (1) up-sampling the LR-HSI, (2) predicting the residual image via a ConvNet, and (3) obtaining the final fused HSI by adding the outputs from first and second steps. Recent methods have leveraged Deep Image Prior (DIP) to up-sample the LR-HSI due to its excellent ability to preserve both spatial and spectral information, without learning from large data sets. However, we observed that the quality of up-sampled HSIs can be further improved by introducing an additional spatial-domain constraint to the conventional spectral-domain energy function. We define our spatial-domain constraint as the $L_1$ distance between the predicted PAN image and the actual PAN image. To estimate the PAN image of the up-sampled HSI, we also propose a learnable spectral response function (SRF). Moreover, we noticed that the residual image between the up-sampled HSI and the reference HSI mainly consists of edge information and very fine structures. In order to accurately estimate fine information, we propose a novel over-complete network, called HyperKite, which focuses on learning high-level features by constraining the receptive from increasing in the deep layers. We perform experiments on three HSI datasets to demonstrate the superiority of our DIP-HyperKite over the state-of-the-art pansharpening methods. The deployment codes, pre-trained models, and final fusion outputs of our DIP-HyperKite and the methods used for the comparisons will be publicly made available at <a class="link-external link-https" href="https://github.com/wgcban/DIP-HyperKite.git" rel="external noopener nofollow">this https URL</a>.      
### 30.A Multi-Objective Approach for Sustainable Generative Audio Models  [ :arrow_down: ](https://arxiv.org/pdf/2107.02621.pdf)
>  In recent years, the deep learning community has largely focused on the accuracy of deep generative models, resulting in impressive improvements in several research fields. However, this scientific race for quality comes at a tremendous computational cost, which incurs vast energy consumption and greenhouse gas emissions. If the current exponential growth of computational consumption persists, Artificial Intelligence (AI) will sadly become a considerable contributor to global warming. <br>At the heart of this problem are the measures that we use as a scientific community to evaluate our work. Currently, researchers in the field of AI judge scientific works mostly based on the improvement in accuracy, log-likelihood, reconstruction or opinion scores, all of which entirely obliterates the actual computational cost of generative models. <br>In this paper, we introduce the idea of relying on a multi-objective measure based on Pareto optimality, which simultaneously integrates the models accuracy, as well as the environmental impact of their training. By applying this measure on the current state-of-the-art in generative audio models, we show that this measure drastically changes the perceived significance of the results in the field, encouraging optimal training techniques and resource allocation. We hope that this type of measure will be widely adopted, in order to help the community to better evaluate the significance of their work, while bringing computational cost -- and in fine carbon emissions -- in the spotlight of AI research.      
### 31.Characterization of drivers heterogeneity and its integration within traffic simulation  [ :arrow_down: ](https://arxiv.org/pdf/2107.02618.pdf)
>  Drivers heterogeneity and the broad range of vehicle characteristics are considered primarily responsible for the stochasticity observed in road traffic dynamics. Assessing the differences in driving style and incorporating individual driving behaviour in microsimulation has attracted significant attention lately. The first topic is studied extensively in the literature. The second one, on the contrary, remains an open issue. The present study proposes a methodology to characterise driving style in the free-flow regime and to incorporate drivers heterogeneity within a microsimulation framework. The methodology uses explicit and simplified modelling of the vehicle powertrain to separate the drivers behavior from the vehicle characteristics. Results show that inter and intra-driver heterogeneity can be captured by log-normal distributions of well-designed metric.Drivers are classified into three different groups (dynamic, ordinary and timid drivers).      
### 32.Self-training with noisy student model and semi-supervised loss function for dcase 2021 challenge task 4  [ :arrow_down: ](https://arxiv.org/pdf/2107.02569.pdf)
>  This report proposes a polyphonic sound event detection (SED) method for the DCASE 2021 Challenge Task 4. The proposed SED model consists of two stages: a mean-teacher model for providing target labels regarding weakly labeled or unlabeled data and a self-training-based noisy student model for predicting strong labels for sound events. The mean-teacher model, which is based on the residual convolutional recurrent neural network (RCRNN) for the teacher and student model, is first trained using all the training data from a weakly labeled dataset, an unlabeled dataset, and a strongly labeled synthetic dataset. Then, the trained mean-teacher model predicts the strong label to each of the weakly labeled and unlabeled datasets, which is brought to the noisy student model in the second stage of the proposed SED model. Here, the structure of the noisy student model is identical to the RCRNN-based student model of the mean-teacher model in the first stage. Then, it is self-trained by adding feature noises, such as time-frequency shift, mixup, SpecAugment, and dropout-based model noise. In addition, a semi-supervised loss function is applied to train the noisy student model, which acts as label noise injection. The performance of the proposed SED model is evaluated on the validation set of the DCASE 2021 Challenge Task 4, and then, several ensemble models that combine five-fold validation models with different hyperparameters of the semi-supervised loss function are finally selected as our final models.      
### 33.Automated Malware Design for Cyber Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.02538.pdf)
>  The design of attacks for cyber physical systems is critical to assess CPS resilience at design time and run-time, and to generate rich datasets from testbeds for research. Attacks against cyber physical systems distinguish themselves from IT attacks in that the main objective is to harm the physical system. Therefore, both cyber and physical system knowledge are needed to design such attacks. The current practice to generate attacks either focuses on the cyber part of the system using IT cyber security existing body of knowledge, or uses heuristics to inject attacks that could potentially harm the physical process. In this paper, we present a systematic approach to automatically generate integrity attacks from the CPS safety and control specifications, without knowledge of the physical system or its dynamics. The generated attacks violate the system operational and safety requirements, hence present a genuine test for system resilience. We present an algorithm to automate the malware payload development. Several examples are given throughout the paper to illustrate the proposed approach.      
### 34.AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style  [ :arrow_down: ](https://arxiv.org/pdf/2107.02530.pdf)
>  While recent text to speech (TTS) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses (um and uh) and diverse rhythms in spontaneous speech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained reading-style TTS model for spontaneous-style speech. Specifically, 1) to insert filled pauses (FP) in the text sequence appropriately, we introduce an FP predictor to the TTS model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts (MoE), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, we mine a spontaneous speech dataset to support our research this work and facilitate future research on spontaneous TTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and rhythms in spontaneous styles, and achieves much better MOS and SMOS scores than previous adaptive TTS systems.      
### 35.Semantic Segmentation Alternative Technique: Segmentation Domain Generation  [ :arrow_down: ](https://arxiv.org/pdf/2107.02525.pdf)
>  Detecting objects of interest in images was always a compelling task to automate. In recent years this task was more and more explored using deep learning techniques, mostly using region-based convolutional networks. In this project we propose an alternative semantic segmentation technique making use of Generative Adversarial Networks. We consider semantic segmentation to be a domain transfer problem. Thus, we train a feed forward network (FFNN) to receive as input a seed real image and generate as output its segmentation mask.      
### 36.Convolutional LSTM models to estimate network traffic  [ :arrow_down: ](https://arxiv.org/pdf/2107.02496.pdf)
>  Network utilisation efficiency can, at least in principle, often be improved by dynamically re-configuring routing policies to better distribute on-going large data transfers. Unfortunately, the information necessary to decide on an appropriate reconfiguration - details of on-going and upcoming data transfers such as their source and destination and, most importantly, their volume and duration - is usually lacking. Fortunately, the increased use of scheduled transfer services, such as FTS, makes it possible to collect the necessary information. However, the mere detection and characterisation of larger transfers is not sufficient to predict with confidence the likelihood a network link will become overloaded. In this paper we present the use of LSTM-based models (CNN-LSTM and Conv-LSTM) to effectively estimate future network traffic and so provide a solid basis for formulating a sensible network configuration plan.      
### 37.Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2107.02494.pdf)
>  The main challenges of image-to-image (I2I) translation are to make the translated image realistic and retain as much information from the source domain as possible. To address this issue, we propose a novel architecture, termed as IEGAN, which removes the encoder of each network and introduces an encoder that is independent of other networks. Compared with previous models, it embodies three advantages of our model: Firstly, it is more directly and comprehensively to grasp image information since the encoder no longer receives loss from generator and discriminator. Secondly, the independent encoder allows each network to focus more on its own goal which makes the translated image more realistic. Thirdly, the reduction in the number of encoders performs more unified image representation. However, when the independent encoder applies two down-sampling blocks, it's hard to extract semantic information. To tackle this problem, we propose deep and shallow information space containing characteristic and semantic information, which can guide the model to translate high-quality images under the task with significant shape or texture change. We compare IEGAN with other previous models, and conduct researches on semantic information consistency and component ablation at the same time. These experiments show the superiority and effectiveness of our architecture. Our code is published on: <a class="link-external link-https" href="https://github.com/Elvinky/IEGAN" rel="external noopener nofollow">this https URL</a>.      
### 38.Turbo Coded Single User Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2107.02437.pdf)
>  This work deals with turbo coded single user massive multiple input multiple output (SU-MMIMO) systems, with and without precoding. SU-MMIMO has a much higher spectral efficiency compared to multi-user massive MIMO (MU-MMIMO) since independent signals are transmitted from each of the antenna elements (spatial multiplexing). MU-MMIMO that uses beamforming has a much lower spectral efficiency, since the same signal (with a delay) is transmitted from each of the antenna elements. In this work, expressions for the upper bound on the average signal-to-noise ratio (SNR) per bit and spectral efficiency are derived for SU-MMIMO with and without precoding. We propose a performance index $f(N_t)$, which is a function of the number of transmit antennas $N_t$. Here $f(N_t)$ is the sum of the upper bound on the average SNR per bit and the spectral efficiency. We demonstrate that when the total number of antennas ($N_{\mathrm{tot}}$) in the transmitter and receiver is fixed, there exists a minimum value of $f(N_t)$, which has to be avoided. Computer simulations show that the bit-error-rate (BER) is nearly insensitive to a wide range of the number of transmit antennas and re-transmissions, when $N_{\mathrm{tot}}$ is large and kept constant. Thus, the spectral efficiency can be made as large as possible, for a given BER and $N_{\mathrm{tot}}$.      
### 39.Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration  [ :arrow_down: ](https://arxiv.org/pdf/2107.02433.pdf)
>  In order to tackle the difficulty associated with the ill-posed nature of the image registration problem, researchers use regularization to constrain the solution space. For most learning-based registration approaches, the regularization usually has a fixed weight and only constrains the spatial transformation. Such convention has two limitations: (1) The regularization strength of a specific image pair should be associated with the content of the images, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially regularizing the transformation (but overlooking the temporal consistency of different estimations) may not be the best strategy to cope with the ill-posedness. In this study, we propose a mean-teacher based registration framework. This framework incorporates an additional \textit{temporal regularization} term by encouraging the teacher model's temporal ensemble prediction to be consistent with that of the student model. At each training step, it also automatically adjusts the weights of the \textit{spatial regularization} and the \textit{temporal regularization} by taking account of the transformation uncertainty and appearance uncertainty derived from the perturbed teacher model. We perform experiments on multi- and uni-modal registration tasks, and the results show that our strategy outperforms the traditional and learning-based benchmark methods.      
### 40.Bayesian Nonparametric Modelling for Model-Free Reinforcement Learning in LTE-LAA and Wi-Fi Coexistence  [ :arrow_down: ](https://arxiv.org/pdf/2107.02431.pdf)
>  With the arrival of next generation wireless communication, a growing number of new applications like internet of things, autonomous driving systems, and drone are crowding the unlicensed spectrum. Licensed network such as the long-term evolution (LTE) also comes to the unlicensed spectrum for better providing high-capacity contents with low cost. However, LTE was not designed to share resources with others. Previous solutions usually work on fixed scenarios. This work features a Nonparametric Bayesian reinforcement learning algorithm to cope with the coexistence between Wi-Fi and LTE licensed assisted access (LTE-LAA) agents in 5 GHz unlicensed spectrum. The coexistence problem is modeled as a decentralized partially-observable Markov decision process (Dec-POMDP) and Bayesian inference is adopted for policy learning with nonparametric prior to accommodate the uncertainty of policy for different agents. A fairness measure is introduced in the reward function to encourage fair sharing between agents. Variational inference for posterior model approximation is considered to make the algorithm computationally efficient. Simulation results demonstrate that this algorithm can reach high value with compact policy representations in few learning iterations.      
### 41.Dynamical System Parameter Identification using Deep Recurrent Cell Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.02427.pdf)
>  In this paper, we investigate the parameter identification problem in dynamical systems through a deep learning approach. Focusing mainly on second-order, linear time-invariant dynamical systems, the topic of damping factor identification is studied. By utilizing a six-layer deep neural network with different recurrent cells, namely GRUs, LSTMs or BiLSTMs; and by feeding input-output sequence pairs captured from a dynamical system simulator, we search for an effective deep recurrent architecture in order to resolve damping factor identification problem. Our study results show that, although previously not utilized for this task in the literature, bidirectional gated recurrent cells (BiLSTMs) provide better parameter identification results when compared to unidirectional gated recurrent memory cells such as GRUs and LSTM. Thus, indicating that an input-output sequence pair of finite length, collected from a dynamical system and when observed anachronistically, may carry information in both time directions for prediction of a dynamical systems parameter.      
### 42.DL-AMP and DBTO: An Automatic Merge Planning and Trajectory Optimization and Its Application in Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2107.02413.pdf)
>  This paper presents an automatic merging algorithm for autonomous driving vehicles, which decouples the specific motion planning problem into a Dual-Layer Automatic Merge Planning (DL_AMP) and a Descent-Based Trajectory Optimization (DBTO). This work leads to great improvements in finding the best merge opportunity, lateral and longitudinal merge planning and control, trajectory postprocessing and driving comfort.      
### 43.GBLinks: GNN-based beam selection and link activation for ultra-dense D2D mmWave networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.02412.pdf)
>  Millimeter wave (mmWave) communication is regarded as a key enabled technology for the future wireless communication to satisfy the requirement of Gbps transmission rate and address the problem of spectrum shortage. Directional transmission used to combat the large pathloss of mmWave communications helps to realize the device-to-device (D2D) communication in ultra-dense networks. In this paper, we consider the problem of joint beam selection and link activation across a set of communication pairs in ultra-dense D2D mmWave networks. The resulting optimization problem is formulated as an integer programming problem that is nonconvex and NP-hard problem. Consequently, the global optimal solution, even the local optimal solution, cannot be generally obtained. To overcome this challenge, we resort to design a deep learning architecture based on graphic neural network to finish the joint beam selection and link activation, called as GBLinks model, with taking into account the network topology information. We further present an unsupervised Lagrangian dual learning framework to train the parameters of GBLinks. Numerical results show that the proposed GBLinks model can converges to a stable point with the number of iterations increases, in terms of the average sum rate. It also shows that GBLinks can reach near-optimal solution through comparing with the exhaustively search in small-scale D2D mmWave networks and outperforms selfish beam selection strategy with activating all links.      
### 44.Machine learning assisted quantum super-resolution microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2107.02401.pdf)
>  One of the main characteristics of optical imaging systems is the spatial resolution, which is restricted by the diffraction limit to approximately half the wavelength of the incident light. Along with the recently developed classical super-resolution techniques, which aim at breaking the diffraction limit in classical systems, there is a class of quantum super-resolution techniques which leverage the non-classical nature of the optical signals radiated by quantum emitters, the so-called antibunching super-resolution microscopy. This approach can ensure a factor of $\sqrt{n}$ improvement in the spatial resolution by measuring the n-th order autocorrelation function. The main bottleneck of the antibunching super-resolution microscopy is the time-consuming acquisition of multi-photon event histograms. We present a machine learning-assisted approach for the realization of rapid antibunching super-resolution imaging and demonstrate 12 times speed-up compared to conventional, fitting-based autocorrelation measurements. The developed framework paves the way to the practical realization of scalable quantum super-resolution imaging devices that can be compatible with various types of quantum emitters.      
### 45.Learning Semantic Segmentation of Large-Scale Point Clouds with Random Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2107.02389.pdf)
>  We study the problem of efficient semantic segmentation of large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Comparative experiments show that our RandLA-Net can process 1 million points in a single pass up to 200x faster than existing approaches. Moreover, extensive experiments on five large-scale point cloud datasets, including Semantic3D, SemanticKITTI, Toronto3D, NPM3D and S3DIS, demonstrate the state-of-the-art semantic segmentation performance of our RandLA-Net.      
### 46.Multi-Modal Motion Planning Using Composite Pose Graph Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2107.02384.pdf)
>  In this paper, we present a motion planning framework for multi-modal vehicle dynamics. Our proposed algorithm employs transcription of the optimization objective function, vehicle dynamics, and state and control constraints into sparse factor graphs, which -- combined with mode transition constraints -- constitute a composite pose graph. By formulating the multi-modal motion planning problem in composite pose graph form, we enable utilization of efficient techniques for optimization on sparse graphs, such as those widely applied in dual estimation problems, e.g., simultaneous localization and mapping (SLAM). The resulting motion planning algorithm optimizes the multi-modal trajectory, including the location of mode transitions, and is guided by the pose graph optimization process to eliminate unnecessary transitions, enabling efficient discovery of optimized mode sequences from rough initial guesses. We demonstrate multi-modal trajectory optimization in both simulation and real-world experiments for vehicles with various dynamics models, such as an airplane with taxi and flight modes, and a vertical take-off and landing (VTOL) fixed-wing aircraft that transitions between hover and horizontal flight modes.      
### 47.Chordal and factor-width decompositions for scalable semidefinite and polynomial optimization  [ :arrow_down: ](https://arxiv.org/pdf/2107.02379.pdf)
>  Chordal and factor-width decomposition methods for semidefinite programming and polynomial optimization have recently enabled the analysis and control of large-scale linear systems and medium-scale nonlinear systems. Chordal decomposition exploits the sparsity of semidefinite matrices in a semidefinite program (SDP), in order to formulate an equivalent SDP with smaller semidefinite constraints that can be solved more efficiently. Factor-width decompositions, instead, relax or strengthen SDPs with dense semidefinite matrices into more tractable problems, trading feasibility or optimality for lower computational complexity. This article reviews recent advances in large-scale semidefinite and polynomial optimization enabled by these two types of decomposition, highlighting connections and differences between them. We also demonstrate that chordal and factor-width decompositions allow for significant computational savings on a range of classical problems from control theory, and on more recent problems from machine learning. Finally, we outline possible directions for future research that have the potential to facilitate the efficient optimization-based study of increasingly complex large-scale dynamical systems.      
### 48.SplitAVG: A heterogeneity-aware federated deep learning method for medical imaging  [ :arrow_down: ](https://arxiv.org/pdf/2107.02375.pdf)
>  Federated learning is an emerging research paradigm for enabling collaboratively training deep learning models without sharing patient data. However, the data from different institutions are usually heterogeneous across institutions, which may reduce the performance of models trained using federated learning. In this study, we propose a novel heterogeneity-aware federated learning method, SplitAVG, to overcome the performance drops from data heterogeneity in federated learning. Unlike previous federated methods that require complex heuristic training or hyper parameter tuning, our SplitAVG leverages the simple network split and feature map concatenation strategies to encourage the federated model training an unbiased estimator of the target data distribution. We compare SplitAVG with seven state-of-the-art federated learning methods, using centrally hosted training data as the baseline on a suite of both synthetic and real-world federated datasets. We find that the performance of models trained using all the comparison federated learning methods degraded significantly with the increasing degrees of data heterogeneity. In contrast, SplitAVG method achieves comparable results to the baseline method under all heterogeneous settings, that it achieves 96.2% of the accuracy and 110.4% of the mean absolute error obtained by the baseline in a diabetic retinopathy binary classification dataset and a bone age prediction dataset, respectively, on highly heterogeneous data partitions. We conclude that SplitAVG method can effectively overcome the performance drops from variability in data distributions across institutions. Experimental results also show that SplitAVG can be adapted to different base networks and generalized to various types of medical imaging tasks.      
### 49.Physical Interaction as Communication: Learning Robot Objectives Online from Human Corrections  [ :arrow_down: ](https://arxiv.org/pdf/2107.02349.pdf)
>  When a robot performs a task next to a human, physical interaction is inevitable: the human might push, pull, twist, or guide the robot. The state-of-the-art treats these interactions as disturbances that the robot should reject or avoid. At best, these robots respond safely while the human interacts; but after the human lets go, these robots simply return to their original behavior. We recognize that physical human-robot interaction (pHRI) is often intentional -- the human intervenes on purpose because the robot is not doing the task correctly. In this paper, we argue that when pHRI is intentional it is also informative: the robot can leverage interactions to learn how it should complete the rest of its current task even after the person lets go. We formalize pHRI as a dynamical system, where the human has in mind an objective function they want the robot to optimize, but the robot does not get direct access to the parameters of this objective -- they are internal to the human. Within our proposed framework human interactions become observations about the true objective. We introduce approximations to learn from and respond to pHRI in real-time. We recognize that not all human corrections are perfect: often users interact with the robot noisily, and so we improve the efficiency of robot learning from pHRI by reducing unintended learning. Finally, we conduct simulations and user studies on a robotic manipulator to compare our proposed approach to the state-of-the-art. Our results indicate that learning from pHRI leads to better task performance and improved human satisfaction.      
### 50.Connecting Spatially Coupled LDPC Code Chains for Bit-Interleaved Coded Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2107.02327.pdf)
>  This paper investigates the design of spatially coupled low-density parity-check (SC-LDPC) codes constructed from connected-chain ensembles for bit-interleaved coded modulation (BICM) schemes. For short coupling lengths, connecting multiple SC-LDPC chains can improve decoding performance over single-chains and impose structured unequal error protection (UEP). A joint design of connected-chain ensembles and bit mapping to further exploit the UEP from codes and high-order modulations is proposed. Numerical results demonstrate the superiority of the proposed design over existing connected-chain ensembles and over single-chain ensembles with existing bit mapping design.      
### 51.New SST Optical Sensor of Pampilhosa da Serra: studies on image processing algorithms and multi-filter characterization of Space Debris  [ :arrow_down: ](https://arxiv.org/pdf/2107.02315.pdf)
>  As part of the Portuguese Space Surveillance and Tracking (SST) System, two new Wide Field of View (2.3deg x 2.3deg) small aperture (30cm) telescopes will be deployed in 2021, at the Pampilhosa da Serra Space Observatory (PASO), located in the center of the continental Portuguese territory, in the heart of a certified Dark Sky area. These optical systems will provide added value capabilities to the Portuguese SST network, complementing the optical telescopes currently in commissioning in Madeira and Azores. These telescopes are optimized for GEO and MEO survey operations and besides the required SST operational capability, they will also provide an important development component to the Portuguese SST network. The telescopes will be equipped with filter wheels, being able to perform observations in several optical bands including white light, BVRI bands and narrow band filters such as H(alpha) and O[III] to study potential different objects' albedos. This configuration enables us to conduct a study on space debris classification$/$characterization using combinations of different colors aiming the production of improved color index schemes to be incorporated in the automatic pipelines for classification of space debris. This optical sensor will also be used to conduct studies on image processing algorithms, including source extraction and classification solutions through the application of machine learning techniques. Since SST dedicated telescopes produce a large quantity of data per observation night, fast, efficient and automatic image processing techniques are mandatory. A platform like this one, dedicated to the development of Space Surveillance studies, will add a critical capability to keep the Portuguese SST network updated, and as a consequence it may provide useful developments to the European SST network as well.      
### 52.Design of pulsed waveforms for space debris detection with ATLAS  [ :arrow_down: ](https://arxiv.org/pdf/2107.02311.pdf)
>  ATLAS is the first Portuguese radar system that aims to detect space debris. The article introduces the system and provides a brief description of its capabilities. The system is capable of synthesizing arbitrary amplitude modulated pulse shapes with a resolution of 10 ns. Given that degree of freedom we decided to test an amplitude modulated chirp signal developed by us and a nested barker code. These waveforms are explained as well as their advantages and drawbacks for space debris detection. An experimental setup was developed to test the system receiver and waveforms are processed by digital matched filtering. The experiments test the system using different waveform shapes and noise levels. Experimental results are in agreement with simulation and show that the chirp signal is more resilient to Doppler shifts, has higher range resolution and lower peak-to-sidelobe ratio in comparison with the nested barker code. Future work in order to increase detection capabilities is discussed at the end.      
### 53.LightFuse: Lightweight CNN based Dual-exposure Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2107.02299.pdf)
>  Deep convolutional neural networks (DCNN) aided high dynamic range (HDR) imaging recently received a lot of attention. The quality of DCNN generated HDR images have overperformed the traditional counterparts. However, DCNN is prone to be computationally intensive and power-hungry. To address the challenge, we propose LightFuse, a light-weight CNN-based algorithm for extreme dual-exposure image fusion, which can be implemented on various embedded computing platforms with limited power and hardware resources. Two sub-networks are utilized: a GlobalNet (G) and a DetailNet (D). The goal of G is to learn the global illumination information on the spatial dimension, whereas D aims to enhance local details on the channel dimension. Both G and D are based solely on depthwise convolution (D Conv) and pointwise convolution (P Conv) to reduce required parameters and computations. Experimental results display that the proposed technique could generate HDR images with plausible details in extremely exposed regions. Our PSNR score exceeds the other state-of-the-art approaches by 1.2 to 1.6 times and achieves 1.4 to 20 times FLOP and parameter reduction compared with others.      
### 54.DeepCEL0 for 2D Single Molecule Localization in Fluorescence Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2107.02281.pdf)
>  In fluorescence microscopy, Single Molecule Localization Microscopy (SMLM) techniques aim at localizing with high precision high density fluorescent molecules by stochastically activating and imaging small subsets of blinking emitters. Super Resolution (SR) plays an important role in this field since it allows to go beyond the intrinsic light diffraction limit. In this work, we propose a deep learning-based algorithm for precise molecule localization of high density frames acquired by SMLM techniques whose $\ell_{2}$-based loss function is regularized by positivity and $\ell_{0}$-based constraints. The $\ell_{0}$ is relaxed through its Continuous Exact $\ell_{0}$ (CEL0) counterpart. The arising approach, named DeepCEL0, is parameter-free, more flexible, faster and provides more precise molecule localization maps if compared to the other state-of-the-art methods. We validate our approach on both simulated and real fluorescence microscopy data.      
### 55.Physics-Informed Graph Learning for Robust Fault Location in Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.02275.pdf)
>  The rapid growth of distributed energy resources potentially increases power grid instability. One promising strategy is to employ data in power grids to efficiently respond to abnormal events (e.g., faults) by detection and location. Unfortunately, most existing works lack physical interpretation and are vulnerable to the practical challenges: sparse observation, insufficient labeled datasets, and stochastic environment. We propose a physics-informed graph learning framework of two stages to handle these challenges when locating faults. Stage- I focuses on informing a graph neural network (GNN) with the geometrical structure of power grids; stage-II employs the physical similarity of labeled and unlabeled data samples to improve the location accuracy. We provide a random walk-based the underpinning of designing our GNNs to address the challenge of sparse observation and augment the correct prediction probability. We compare our approach with three baselines in the IEEE 123-node benchmark system, showing that the proposed method outperforms the others by significant margins, especially when label rates are low. Also, we validate the robustness of our algorithms to out-of-distribution-data (ODD) due to topology changes and load variations. Additionally, we adapt our graph learning framework to the IEEE 37-node test feeder and show high location performance with the proposed training strategy.      
### 56.High-Speed CMOS-Free Purely Spintronic Asynchronous Recurrent Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2107.02238.pdf)
>  Neuromorphic computing systems overcome the limitations of traditional von Neumann computing architectures. These computing systems can be further improved upon by using emerging technologies that are more efficient than CMOS for neural computation. Recent research has demonstrated memristors and spintronic devices in various neural network designs boost efficiency and speed. This paper presents a biologically inspired fully spintronic neuron used in a fully spintronic Hopfield RNN. The network is used to solve tasks, and the results are compared against those of current Hopfield neuromorphic architectures which use emerging technologies.      
