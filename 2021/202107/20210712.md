# ArXiv eess --Mon, 12 Jul 2021
### 1.Analytical Inverter-Based Distributed Generator Model for Power Flow Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2107.04576.pdf)
>  Quantifying the impact of inverter-based distributed generation (DG) sources on power-flow distribution system cases is arduous. Existing distribution system tools predominately model distributed generation sources as either negative PQ loads or as a PV generator and then employed a PV-PQ switching algorithm to mimic Volt/VAR support. These models neglect the unique characteristics of inverter-based distributed generation sources, have scalability and convergence issues, and are ill-suited for increasing solar penetration scenarios. This work proposes an inverter-based DG model accounting for the inverter's topology, sensing position, and control strategies. The model extends recently introduced analytical positive sequence generator models for three-phase studies. The use of circuit-simulation based heuristics help achieve robust convergence. Simulation of the PG&amp;E prototypical feeders using a prototype solver demonstrate the model's accuracy and efficacy.      
### 2.Scopeformer: n-CNN-ViT Hybrid Model for Intracranial Hemorrhage Classification  [ :arrow_down: ](https://arxiv.org/pdf/2107.04575.pdf)
>  We propose a feature generator backbone composed of an ensemble of convolutional neuralnetworks (CNNs) to improve the recently emerging Vision Transformer (ViT) models. We tackled the RSNA intracranial hemorrhage classification problem, i.e., identifying various hemorrhage types from computed tomography (CT) slices. We show that by gradually stacking several feature maps extracted using multiple Xception CNNs, we can develop a feature-rich input for the ViT model. Our approach allowed the ViT model to pay attention to relevant features at multiple levels. Moreover, pretraining the n CNNs using various paradigms leads to a diverse feature set and further improves the performance of the proposed n-CNN-ViT. We achieved a test accuracy of 98.04% with a weighted logarithmic loss value of 0.0708. The proposed architecture is modular and scalable in both the number of CNNs used for feature extraction and the size of the ViT.      
### 3.Modality specific U-Net variants for biomedical image segmentation: A survey  [ :arrow_down: ](https://arxiv.org/pdf/2107.04537.pdf)
>  With the advent of advancements in deep learning approaches, such as deep convolution neural network, residual neural network, adversarial network; U-Net architectures are most widely utilized in biomedical image segmentation to address the automation in identification and detection of the target regions or sub-regions. In recent studies, U-Net based approaches have illustrated state-of-the-art performance in different applications for the development of computer-aided diagnosis systems for early diagnosis and treatment of diseases such as brain tumor, lung cancer, alzheimer, breast cancer, etc. This article contributes to present the success of these approaches by describing the U-Net framework, followed by the comprehensive analysis of the U-Net variants for different medical imaging or modalities such as magnetic resonance imaging, X-ray, computerized tomography/computerized axial tomography, ultrasound, positron emission tomography, etc. Besides, this article also highlights the contribution of U-Net based frameworks in the on-going pandemic, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.      
### 4.Extremum-Seeking Adaptive-Droop for Model-free and Localized Volt-VAR Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2107.04515.pdf)
>  In an active power distribution system, Volt-VAR optimization (VVO) methods are employed to achieve network-level objectives such as minimization of network power losses. The commonly used model-based centralized and distributed VVO algorithms perform poorly in the absence of a communication system and with model and measurement uncertainties. In this paper, we proposed a model-free local Volt-VAR control approach for network-level optimization that does not require communication with other decision-making agents. The proposed algorithm is based on extremum-seeking approach that uses only local measurements to minimize the network power losses. To prove that the proposed extremum-seeking controller converges to the optimum solution, we also derive mathematical conditions for which the loss minimization problem is convex with respect to the control variables. Local controllers pose stability concerns during highly variable scenarios. Thus, the proposed extremum-seeking controller is integrated with an adaptive-droop control module to provide a stable local control response. The proposed approach is validated using IEEE 4-bus and IEEE 123-bus systems and achieves the loss minimization objective while maintaining the voltage within the pre-specific limits even during highly variable DER generation scenarios.      
### 5.Learning-Based Nonlinear $H^\infty$ Control via Game-Theoretic Differential Dynamic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2107.04507.pdf)
>  In this work, we present a learning-based nonlinear $H^\infty$ control algorithm that guarantee system performance under learned dynamics and disturbance estimate. The Gaussian Process (GP) regression is utilized to update the nominal dynamics of the system and provide disturbance estimate based on data gathered through interaction with the system. A soft-constrained differential game associated with the disturbance attenuation problem in nonlinear $H^\infty$ control is then formulated to obtain the nonlinear $H^\infty$ controller. The differential game is solved through the min-max Game-Theoretic Differential Dynamic Programming (GT-DDP) algorithm in continuous time. Simulation results on a quadcopter system demonstrate the efficiency of the learning-based control algorithm in handling external disturbances.      
### 6.Bayesian Error-in-Variables Models for the Identification of Power Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.04480.pdf)
>  The increasing integration of intermittent renewable generation, especially at the distribution level,necessitates advanced planning and optimisation methodologies contingent on the knowledge of thegrid, specifically the admittance matrix capturing the topology and line parameters of an electricnetwork. However, a reliable estimate of the admittance matrix may either be missing or quicklybecome obsolete for temporally varying grids. In this work, we propose a data-driven identificationmethod utilising voltage and current measurements collected from micro-PMUs. More precisely,we first present a maximum likelihood approach and then move towards a Bayesian framework,leveraging the principles of maximum a posteriori estimation. In contrast with most existing con-tributions, our approach not only factors in measurement noise on both voltage and current data,but is also capable of exploiting available a priori information such as sparsity patterns and knownline parameters. Simulations conducted on benchmark cases demonstrate that, compared to otheralgorithms, our method can achieve significantly greater accuracy.      
### 7.Representation Learning to Classify and Detect Adversarial Attacks against Speaker and Speech Recognition Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.04448.pdf)
>  Adversarial attacks have become a major threat for machine learning applications. There is a growing interest in studying these attacks in the audio domain, e.g, speech and speaker recognition; and find defenses against them. In this work, we focus on using representation learning to classify/detect attacks w.r.t. the attack algorithm, threat model or signal-to-adversarial-noise ratio. We found that common attacks in the literature can be classified with accuracies as high as 90%. Also, representations trained to classify attacks against speaker identification can be used also to classify attacks against speaker verification and speech recognition. We also tested an attack verification task, where we need to decide whether two speech utterances contain the same attack. We observed that our models did not generalize well to attack algorithms not included in the attack representation model training. Motivated by this, we evaluated an unknown attack detection task. We were able to detect unknown attacks with equal error rates of about 19%, which is promising.      
### 8.A Deep Discontinuity-Preserving Image Registration Network  [ :arrow_down: ](https://arxiv.org/pdf/2107.04440.pdf)
>  Image registration aims to establish spatial correspondence across pairs, or groups of images, and is a cornerstone of medical image computing and computer-assisted-interventions. Currently, most deep learning-based registration methods assume that the desired deformation fields are globally smooth and continuous, which is not always valid for real-world scenarios, especially in medical image registration (e.g. cardiac imaging and abdominal imaging). Such a global constraint can lead to artefacts and increased errors at discontinuous tissue interfaces. To tackle this issue, we propose a weakly-supervised Deep Discontinuity-preserving Image Registration network (DDIR), to obtain better registration performance and realistic deformation fields. We demonstrate that our method achieves significant improvements in registration accuracy and predicts more realistic deformations, in registration experiments on cardiac magnetic resonance (MR) images from UK Biobank Imaging Study (UKBB), than state-of-the-art approaches.      
### 9.Measurement of In-Band Optical Noise Spectral Density  [ :arrow_down: ](https://arxiv.org/pdf/2107.04426.pdf)
>  We present a method to measure the spectral density of in-band optical transmission impairments without coherent electrical reception and digital signal processing at the receiver. We determine the method's accuracy by numerical simulations and show experimentally its feasibility, including the measure of in-band nonlinear distortions power densities.      
### 10.A Differential Private Method for Distributed Optimization in Directed Networks via State Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2107.04370.pdf)
>  In this paper, we study the problem of consensus-based distributed optimization where a network of agents, abstracted as a directed graph, aims to minimize the sum of all agents' cost functions collaboratively. In existing distributed optimization approaches (Push-Pull/AB) for directed graphs, all agents exchange their states with neighbors to achieve the optimal solution with a constant stepsize, which may lead to the disclosure of sensitive and private information. For privacy preservation, we propose a novel state-decomposition based gradient tracking approach (SD-Push-Pull) for distributed optimzation over directed networks that preserves differential privacy, which is a strong notion that protects agents' privacy against an adversary with arbitrary auxiliary information. The main idea of the proposed approach is to decompose the gradient state of each agent into two sub-states. Only one substate is exchanged by the agent with its neighbours over time, and the other one is kept private. That is to say, only one substate is visible to an adversary, protecting the privacy from being leaked. It is proved that under certain decomposition principles, a bound for the sub-optimality of the proposed algorithm can be derived and the differential privacy is achieved simultaneously. Moreover, the trade-off between differential privacy and the optimization accuracy is also characterized. Finally, a numerical simulation is provided to illustrate the effectiveness of the proposed approach.      
### 11.Hepatocellular Carcinoma Segmentation fromDigital Subtraction Angiography Videos usingLearnable Temporal Difference  [ :arrow_down: ](https://arxiv.org/pdf/2107.04306.pdf)
>  Automatic segmentation of hepatocellular carcinoma (HCC)in Digital Subtraction Angiography (DSA) videos can assist radiologistsin efficient diagnosis of HCC and accurate evaluation of tumors in clinical practice. Few studies have investigated HCC segmentation from DSAvideos. It shows great challenging due to motion artifacts in filming, ambiguous boundaries of tumor regions and high similarity in imaging toother anatomical tissues. In this paper, we raise the problem of HCCsegmentation in DSA videos, and build our own DSA dataset. We alsopropose a novel segmentation network called DSA-LTDNet, including asegmentation sub-network, a temporal difference learning (TDL) moduleand a liver region segmentation (LRS) sub-network for providing additional guidance. DSA-LTDNet is preferable for learning the latent motioninformation from DSA videos proactively and boosting segmentation performance. All of experiments are conducted on our self-collected dataset.Experimental results show that DSA-LTDNet increases the DICE scoreby nearly 4% compared to the U-Net baseline.      
### 12.Loss Prediction: End-to-End Active Learning Approach For Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2107.04289.pdf)
>  End-to-end speech recognition systems usually require huge amounts of labeling resource, while annotating the speech data is complicated and expensive. Active learning is the solution by selecting the most valuable samples for annotation. In this paper, we proposed to use a predicted loss that estimates the uncertainty of the sample. The CTC (Connectionist Temporal Classification) and attention loss are informative for speech recognition since they are computed based on all decoding paths and alignments. We defined an end-to-end active learning pipeline, training an ASR/LP (Automatic Speech Recognition/Loss Prediction) joint model. The proposed approach was validated on an English and a Chinese speech recognition task. The experiments show that our approach achieves competitive results, outperforming random selection, least confidence, and estimated loss method.      
### 13.Retinal OCT Denoising with Pseudo-Multimodal Fusion Network  [ :arrow_down: ](https://arxiv.org/pdf/2107.04288.pdf)
>  Optical coherence tomography (OCT) is a prevalent imaging technique for retina. However, it is affected by multiplicative speckle noise that can degrade the visibility of essential anatomical structures, including blood vessels and tissue layers. Although averaging repeated B-scan frames can significantly improve the signal-to-noise-ratio (SNR), this requires longer acquisition time, which can introduce motion artifacts and cause discomfort to patients. In this study, we propose a learning-based method that exploits information from the single-frame noisy B-scan and a pseudo-modality that is created with the aid of the self-fusion method. The pseudo-modality provides good SNR for layers that are barely perceptible in the noisy B-scan but can over-smooth fine features such as small vessels. By using a fusion network, desired features from each modality can be combined, and the weight of their contribution is adjustable. Evaluated by intensity-based and structural metrics, the result shows that our method can effectively suppress the speckle noise and enhance the contrast between retina layers while the overall structure and small blood vessels are preserved. Compared to the single modality network, our method improves the structural similarity with low noise B-scan from 0.559 +\- 0.033 to 0.576 +\- 0.031.      
### 14.LIFE: A Generalizable Autodidactic Pipeline for 3D OCT-A Vessel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04282.pdf)
>  Optical coherence tomography (OCT) is a non-invasive imaging technique widely used for ophthalmology. It can be extended to OCT angiography (OCT-A), which reveals the retinal vasculature with improved contrast. Recent deep learning algorithms produced promising vascular segmentation results; however, 3D retinal vessel segmentation remains difficult due to the lack of manually annotated training data. We propose a learning-based method that is only supervised by a self-synthesized modality named local intensity fusion (LIF). LIF is a capillary-enhanced volume computed directly from the input OCT-A. We then construct the local intensity fusion encoder (LIFE) to map a given OCT-A volume and its LIF counterpart to a shared latent space. The latent space of LIFE has the same dimensions as the input data and it contains features common to both modalities. By binarizing this latent space, we obtain a volumetric vessel segmentation. Our method is evaluated in a human fovea OCT-A and three zebrafish OCT-A volumes with manual labels. It yields a Dice score of 0.7736 on human data and 0.8594 +/- 0.0275 on zebrafish data, a dramatic improvement over existing unsupervised algorithms.      
### 15.Secure Consensus via Objective Coding: Robustness Analysis to Channel Tampering  [ :arrow_down: ](https://arxiv.org/pdf/2107.04276.pdf)
>  This work mainly addresses continuous-time multi-agent consensus networks where an adverse attacker affects the convergence performances of said protocol. In particular, we develop a novel secure-by-design approach in which the presence of a network manager monitors the system and broadcasts encrypted tasks (i.e. hidden edge weight assignments) to the agents involved. Each agent is then expected to decode the received codeword containing data on the task through appropriate decoding functions by leveraging advanced security principles, such as objective coding and information localization. Within this framework, a stability analysis is conducted for showing the robustness to channel tampering in the scenario where part of the codeword corresponding to a single link in the system is corrupted. A trade-off between objective coding capabilityand network robustness is also pointed out. To support these novelties, an application example on decentralized estimation is provided. Moreover, such an investigation on robust stability is as well extended in the discrete-time domain. Further numerical simulations are given to validate the theoretical results in both the time domains.      
### 16.Defense against DoS and load altering attacks via model-free control: A proposal for a new cybersecurity setting  [ :arrow_down: ](https://arxiv.org/pdf/2107.04238.pdf)
>  Defense against cyberattacks is an emerging topic related to fault-tolerant control. In order to avoid difficult mathematical modeling, model-free control (MFC) is suggested as an alternative to classical control. For illustration purpose a Load Frequency Control of multi-areas power network is considered. In the simulations, load altering attacks and Denial of Service (DoS) in the communication network are applied to the system. Our aim is to compare the impact of cyberattacks on control loops closed via respectively a classical controller in such situations and a model-free one. Computer experiments show impressive results with MFC.      
### 17.Training a Deep Neural Network via Policy Gradients for Blind Source Separation in Polyphonic Music Recordings  [ :arrow_down: ](https://arxiv.org/pdf/2107.04235.pdf)
>  We propose a method for the blind separation of sounds of musical instruments in audio signals. We describe the individual tones via a parametric model, training a dictionary to capture the relative amplitudes of the harmonics. The model parameters are predicted via a U-Net, which is a type of deep neural network. The network is trained without ground truth information, based on the difference between the model prediction and the individual STFT time frames. Since some of the model parameters do not yield a useful backpropagation gradient, we model them stochastically and employ the policy gradient instead. To provide phase information and account for inaccuracies in the dictionary-based representation, we also let the network output a direct prediction, which we then use to resynthesize the audio signals for the individual instruments. Due to the flexibility of the neural network, inharmonicity can be incorporated seamlessly and no preprocessing of the input spectra is required. Our algorithm yields high-quality separation results with particularly low interference on a variety of different audio samples, both acoustic and synthetic, provided that the sample contains enough data for the training and that the spectral characteristics of the musical instruments are sufficiently stable to be approximated by the dictionary.      
### 18.Incorporating Multi-Target in Multi-Stage Speech Enhancement Model for Better Generalization  [ :arrow_down: ](https://arxiv.org/pdf/2107.04232.pdf)
>  Recent single-channel speech enhancement methods based on deep neural networks (DNNs) have achieved remarkable results, but there are still generalization problems in real scenes. Like other data-driven methods, DNN-based speech enhancement models produce significant performance degradation on untrained data. In this study, we make full use of the contribution of multi-target joint learning to the model generalization capability, and propose a lightweight and low-computing dilated convolutional network (DCN) model for a more robust speech denoising task. Our goal is to integrate the masking target, the mapping target, and the parameters of the traditional speech enhancement estimator into a DCN model to maximize their complementary advantages. To do this, we build a multi-stage learning framework to deal with multiple targets in stages to achieve their joint learning, namely `MT-in-MS'. Our experimental results show that compared with the state-of-the-art time domain and time-frequency domain models, this proposed low-cost DCN model can achieve better generalization performance in speaker, noise, and channel mismatch cases.      
### 19.Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04227.pdf)
>  Predicting the altered acoustic frames is an effective way of self-supervised learning for speech representation. However, it is challenging to prevent the pretrained model from overfitting. In this paper, we proposed to introduce two dropout regularization methods into the pretraining of transformer encoder: (1) attention dropout, (2) layer dropout. Both of the two dropout methods encourage the model to utilize global speech information, and avoid just copying local spectrum features when reconstructing the masked frames. We evaluated the proposed methods on phoneme classification and speaker recognition tasks. The experiments demonstrate that our dropout approaches achieve competitive results, and improve the performance of classification accuracy on downstream tasks.      
### 20.Deep Learning models for benign and malign Ocular Tumor Growth Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04220.pdf)
>  Relatively abundant availability of medical imaging data has provided significant support in the development and testing of Neural Network based image processing methods. Clinicians often face issues in selecting suitable image processing algorithm for medical imaging data. A strategy for the selection of a proper model is presented here. The training data set comprises optical coherence tomography (OCT) and angiography (OCT-A) images of 50 mice eyes with more than 100 days follow-up. The data contains images from treated and untreated mouse eyes. Four deep learning variants are tested for automatic (a) differentiation of tumor region with healthy retinal layer and (b) segmentation of 3D ocular tumor volumes. Exhaustive sensitivity analysis of deep learning models is performed with respect to the number of training and testing images using 8 eight performance indices to study accuracy, reliability/reproducibility, and speed. U-net with UVgg16 is best for malign tumor data set with treatment (having considerable variation) and U-net with Inception backbone for benign tumor data (with minor variation). Loss value and root mean square error (R.M.S.E.) are found most and least sensitive performance indices, respectively. The performance (via indices) is found to be exponentially improving regarding a number of training images. The segmented OCT-Angiography data shows that neovascularization drives the tumor volume. Image analysis shows that photodynamic imaging-assisted tumor treatment protocol is transforming an aggressively growing tumor into a cyst. An empirical expression is obtained to help medical professionals to choose a particular model given the number of images and types of characteristics. We recommend that the presented exercise should be taken as standard practice before employing a particular deep learning model for biomedical image analysis.      
### 21.On lattice-free boosted MMI training of HMM and CTC-based full-context ASR models  [ :arrow_down: ](https://arxiv.org/pdf/2107.04154.pdf)
>  Hybrid automatic speech recognition (ASR) models are typically sequentially trained with CTC or LF-MMI criteria. However, they have vastly different legacies and are usually implemented in different frameworks. In this paper, by decoupling the concepts of modeling units and label topologies and building proper numerator/denominator graphs accordingly, we establish a generalized framework for hybrid acoustic modeling (AM). In this framework, we show that LF-MMI is a powerful training criterion applicable to both limited-context and full-context models, for wordpiece/mono-char/bi-char/chenone units, with both HMM/CTC topologies. From this framework, we propose three novel training schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with different advantages in training performance, decoding efficiency and decoding time-stamp accuracy. The advantages of different training schemes are evaluated comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated on two real world ASR tasks to show their effectiveness. Besides, we also show bi-char(bc) HMM-MMI models can serve as better alignment models than traditional non-neural GMM-HMMs.      
### 22.Inertia Pricing in Stochastic Electricity Markets  [ :arrow_down: ](https://arxiv.org/pdf/2107.04101.pdf)
>  Renewable-dominant power systems explore options to procure virtual inertia services from non-synchronous resources (e.g., batteries, wind turbines) in addition to inertia traditionally provided by synchronous resources (e.g., thermal generators). This paper designs a stochastic electricity market that produces co-optimized and efficient prices for energy, reserve and inertia. We formulate a convex chance-constrained stochastic unit commitment model with inertia requirements and obtain equilibrium energy, reserve and inertia prices using convex duality. Numerical experiments on an illustrative system and a modified IEEE 118-bus systems show the performance of the proposed pricing mechanism.      
### 23.CASPIANET++: A Multidimensional Channel-Spatial Asymmetric Attention Network with Noisy Student Curriculum Learning Paradigm for Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04099.pdf)
>  Convolutional neural networks (CNNs) have been used quite successfully for semantic segmentation of brain tumors. However, current CNNs and attention mechanisms are stochastic in nature and neglect the morphological indicators used by radiologists to manually annotate regions of interest. In this paper, we introduce a channel and spatial wise asymmetric attention (CASPIAN) by leveraging the inherent structure of tumors to detect regions of saliency. To demonstrate the efficacy of our proposed layer, we integrate this into a well-established convolutional neural network (CNN) architecture to achieve higher Dice scores, with less GPU resources. Also, we investigate the inclusion of auxiliary multiscale and multiplanar attention branches to increase the spatial context crucial in semantic segmentation tasks. The resulting architecture is the new CASPIANET++, which achieves Dice Scores of 91.19% whole tumor, 87.6% for tumor core and 81.03% for enhancing tumor. Furthermore, driven by the scarcity of brain tumor data, we investigate the Noisy Student method for segmentation tasks. Our new Noisy Student Curriculum Learning paradigm, which infuses noise incrementally to increase the complexity of the training images exposed to the network, further boosts the enhancing tumor region to 81.53%. Additional validation performed on the BraTS2020 data shows that the Noisy Student Curriculum Learning method works well without any additional training or finetuning.      
### 24.Robust Control Barrier Functions under High Relative Degree and Input Constraints for Satellite Trajectories  [ :arrow_down: ](https://arxiv.org/pdf/2107.04094.pdf)
>  This paper presents methodologies for constructing Control Barrier Functions (CBFs) for nonlinear, control-affine systems, in the presence of input constraints and bounded disturbances. More specifically, given a constraint function with high-relative-degree with respect to the system dynamics, the paper considers three methodologies, two for relative-degree 2 and one for higher relative-degrees, for converting the constraint function into a CBF. Three special forms of Robust CBFs (RCBFs) are developed as functions of the input constraints, system dynamics, and disturbance bounds, and are applied to provably safe satellite control for asteroid exploration missions. The resultant RCBF conditions on the control input are enforced in a switched fashion, which expands the set of allowable trajectories and simplifies control input computation in the presence of multiple constraints. The proposed methods are verified in simulations demonstrating the developed RCBFs in an asteroid flyby scenario for a satellite with low-thrust actuators, and in asteroid proximity operations for a satellite with high-thrust actuators.      
### 25.Distributed Coverage Control of Multi-Agent Networks with Guaranteed Collision Avoidance in Cluttered Environments  [ :arrow_down: ](https://arxiv.org/pdf/2107.04078.pdf)
>  We propose a distributed control algorithm for a multi-agent network whose agents deploy over a cluttered region in accordance with a time-varying coverage density function while avoiding collisions with all obstacles they encounter. Our algorithm is built on a two-level characterization of the network. The first level treats the multi-agent network as a whole based on the distribution of the locations of its agents over the spatial domain. In the second level, the network is described in terms of the individual positions of its agents. The aim of the multi-agent network is to attain a spatial distribution that resembles that of a reference coverage density function (high-level problem) by means of local (microscopic) interactions of its agents (low-level problem). In addition, as the agents deploy, they must avoid collisions with all the obstacles in the region at all times. Our approach utilizes a modified version of Voronoi tessellations which are comprised of what we refer to as Obstacle-Aware Voronoi Cells (OAVC) in order to enable coverage control while ensuring obstacle avoidance. We consider two control problems. The first problem which we refer to as the high-level coverage control problem corresponds to an interpolation problem in the class of Gaussian mixtures (no collision avoidance requirement), which we solve analytically. The second problem which we refer to as the low-level coverage control problem corresponds to a distributed control problem (collision avoidance requirement is now enforced at all times) which is solved by utilizing Lloyd's algorithm together with the modified Voronoi tessellation (OAVC) and a time-varying coverage density function which corresponds to the solution of the high-level coverage control problem. Finally, simulation results for coverage in a cluttered environment are provided to demonstrate the efficacy of the proposed approach.      
### 26.Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images  [ :arrow_down: ](https://arxiv.org/pdf/2107.04062.pdf)
>  A two-step concept for 3D segmentation on 5 abdominal organs inside volumetric CT images is presented. First each relevant organ's volume of interest is extracted as bounding box. The extracted volume acts as input for a second stage, wherein two compared U-Nets with different architectural dimensions re-construct an organ segmentation as label mask. In this work, we focus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results indicate Dice improvements of about 6\% at maximum. In this study to our surprise, liver and kidneys for instance were tackled significantly better using the faster and GPU-memory saving 2D U-Nets. For other abdominal key organs, there were no significant differences, but we observe highly significant advantages for the 2D U-Net in terms of GPU computational efforts for all organs under study.      
### 27.3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image  [ :arrow_down: ](https://arxiv.org/pdf/2107.04055.pdf)
>  In this paper, a 3D-RegNet-based neural network is proposed for diagnosing the physical condition of patients with coronavirus (Covid-19) infection. In the application of clinical medicine, lung CT images are utilized by practitioners to determine whether a patient is infected with coronavirus. However, there are some laybacks can be considered regarding to this diagnostic method, such as time consuming and low accuracy. As a relatively large organ of human body, important spatial features would be lost if the lungs were diagnosed utilizing two dimensional slice image. Therefore, in this paper, a deep learning model with 3D image was designed. The 3D image as input data was comprised of two-dimensional pulmonary image sequence and from which relevant coronavirus infection 3D features were extracted and classified. The results show that the test set of the 3D model, the result: f1 score of 0.8379 and AUC value of 0.8807 have been achieved.      
### 28.ViTGAN: Training GANs with Vision Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2107.04589.pdf)
>  Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such observation can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). We observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce novel regularization techniques for training GANs with ViTs. Empirically, our approach, named ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets.      
### 29.Multi-level Stress Assessment from ECG in a Virtual Reality Environment using Multimodal Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2107.04566.pdf)
>  ECG is an attractive option to assess stress in serious Virtual Reality (VR) applications due to its non-invasive nature. However, the existing Machine Learning (ML) models perform poorly. Moreover, existing studies only perform a binary stress assessment, while to develop a more engaging biofeedback-based application, multi-level assessment is necessary. Existing studies annotate and classify a single experience (e.g. watching a VR video) to a single stress level, which again prevents design of dynamic experiences where real-time in-game stress assessment can be utilized. In this paper, we report our findings on a new study on VR stress assessment, where three stress levels are assessed. ECG data was collected from 9 users experiencing a VR roller coaster. The VR experience was then manually labeled in 10-seconds segments to three stress levels by three raters. We then propose a novel multimodal deep fusion model utilizing spectrogram and 1D ECG that can provide a stress prediction from just a 1-second window. Experimental results demonstrate that the proposed model outperforms the classical HRV-based ML models (9% increase in accuracy) and baseline deep learning models (2.5% increase in accuracy). We also report results on the benchmark WESAD dataset to show the supremacy of the model.      
### 30.A novel clock and timing approach for achieving 200+ km ALMA baselines  [ :arrow_down: ](https://arxiv.org/pdf/2107.04564.pdf)
>  Radio telescope arrays are interferometers and thus require coherent capture and processing of the signal from the astronomical source being observed. In ALMA this is accomplished by using a clock at each antenna for down-conversion and digitization, sent there from a central location via a round-trip phase-corrected technique, using specialized analogue photonic equipment and methods. This is challenging but works well at ALMA frequencies approaching 1 THz and over ~15 km of thermally and mechanically stabilized buried fiber. For future ALMA upgrades, which may involve much longer baselines and therefore fiber reaches, such an approach may not be feasible. This paper delves into an alternative and novel method of "incoherent clocking" (IC) wherein each ALMA antenna performs operations (down-conversion and digitization) using its own free-running local oscillator, its temporally-varying frequency is measured using all digital methods relative to a common clock domain, and subsequently the digitized data is corrected accordingly before further cross-correlation and beamforming processing. This method purports to allow for increasing ALMA baselines to 200 km or more using aerial fiber and COTS digital fiber optic transceivers.      
### 31.Objective task-based evaluation of artificial intelligence-based medical imaging methods: Framework, strategies and role of the physician  [ :arrow_down: ](https://arxiv.org/pdf/2107.04540.pdf)
>  Artificial intelligence (AI)-based methods are showing promise in multiple medical-imaging applications. Thus, there is substantial interest in clinical translation of these methods, requiring in turn, that they be evaluated rigorously. In this paper, our goal is to lay out a framework for objective task-based evaluation of AI methods. We will also provide a list of tools available in the literature to conduct this evaluation. Further, we outline the important role of physicians in conducting these evaluation studies. The examples in this paper will be proposed in the context of PET with a focus on neural-network-based methods. However, the framework is also applicable to evaluate other medical-imaging modalities and other types of AI methods.      
### 32.A Dual-Connection based Handover Scheme for Ultra-Dense Millimeter-Wave Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.04526.pdf)
>  Mobile users in an ultra-dense millimeter-wave cellular network experience handover events more frequently than in conventional networks, which results in increased service interruption time and performance degradation due to blockages. Multi-connectivity has been proposed to resolve this, and it also extends the coverage of millimeter-wave communications. In this paper, we propose a dual-connection based handover scheme for mobile UEs in an environment where they are connected simultaneously with two millimeter-wave cells to overcome frequent handover problems. This scheme allows a mobile UE to choose its serving link between the two mmWave connections according to the measured SINRs and then the corresponding base stations may forward duplicate packets to the UE. We compare our dual-connection based scheme with a conventional single-connection based scheme through ns-3 simulation. The simulation results show that the proposed scheme significantly reduces handover rate and delay. Therefore, we argue that the dual-connection based scheme helps mobile users achieve performance goals they require in ultra-dense cellular environments.      
### 33.ARC: Adversarially Robust Control Policies for Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2107.04487.pdf)
>  Deep neural networks have demonstrated their capability to learn control policies for a variety of tasks. However, these neural network-based policies have been shown to be susceptible to exploitation by adversarial agents. Therefore, there is a need to develop techniques to learn control policies that are robust against adversaries. We introduce Adversarially Robust Control (ARC), which trains the protagonist policy and the adversarial policy end-to-end on the same loss. The aim of the protagonist is to maximise this loss, whilst the adversary is attempting to minimise it. We demonstrate the proposed ARC training in a highway driving scenario, where the protagonist controls the follower vehicle whilst the adversary controls the lead vehicle. By training the protagonist against an ensemble of adversaries, it learns a significantly more robust control policy, which generalises to a variety of adversarial strategies. The approach is shown to reduce the amount of collisions against new adversaries by up to 90.25%, compared to the original policy. Moreover, by utilising an auxiliary distillation loss, we show that the fine-tuned control policy shows no drop in performance across its original training distribution.      
### 34.Adversarial Mixture Density Networks: Learning to Drive Safely from Collision Data  [ :arrow_down: ](https://arxiv.org/pdf/2107.04485.pdf)
>  Imitation learning has been widely used to learn control policies for autonomous driving based on pre-recorded data. However, imitation learning based policies have been shown to be susceptible to compounding errors when encountering states outside of the training distribution. Further, these agents have been demonstrated to be easily exploitable by adversarial road users aiming to create collisions. To overcome these shortcomings, we introduce Adversarial Mixture Density Networks (AMDN), which learns two distributions from separate datasets. The first is a distribution of safe actions learned from a dataset of naturalistic human driving. The second is a distribution representing unsafe actions likely to lead to collision, learned from a dataset of collisions. During training, we leverage these two distributions to provide an additional loss based on the similarity of the two distributions. By penalising the safe action distribution based on its similarity to the unsafe action distribution when training on the collision dataset, a more robust and safe control policy is obtained. We demonstrate the proposed AMDN approach in a vehicle following use-case, and evaluate under naturalistic and adversarial testing environments. We show that despite its simplicity, AMDN provides significant benefits for the safety of the learned control policy, when compared to pure imitation learning or standard mixture density network approaches.      
### 35.An Orchestration Platform that Puts Radiologists in the Driver's Seat of AI Innovation: A Methodological Approach  [ :arrow_down: ](https://arxiv.org/pdf/2107.04409.pdf)
>  Current AI-driven research in radiology requires resources and expertise that are often inaccessible to small and resource-limited labs. The clinicians who are able to participate in AI research are frequently well-funded, well-staffed, and either have significant experience with AI and computing, or have access to colleagues or facilities that do. Current imaging data is clinician-oriented and is not easily amenable to machine learning initiatives, resulting in inefficient, time consuming, and costly efforts that rely upon a crew of data engineers and machine learning scientists, and all too often preclude radiologists from driving AI research and innovation. We present the system and methodology we have developed to address infrastructure and platform needs, while reducing the staffing and resource barriers to entry. We emphasize a data-first and modular approach that streamlines the AI development and deployment process while providing efficient and familiar interfaces for radiologists, such that they can be the drivers of new AI innovations.      
### 36.Uncovering contributing factors to interruptions in the power grid: An Arctic case  [ :arrow_down: ](https://arxiv.org/pdf/2107.04406.pdf)
>  Electric failures are a problem for customers and grid operators. Identifying causes and localizing the source of failures in the grid is critical. Here, we focus on a specific power grid in the Arctic region of North Norway. First, we collect data pertaining to the grid topology, the topography of the area, the historical meteorological data, and the historical energy consumption/production data. Then, we exploit statistical and machine-learning techniques to predict the occurrence of failures. <br>We interpret the variables that mostly explain the classification results to be the main driving factors of power interruption. We are able to predict 57% (F1-score 0.53) of all failures reported over a period of 1 year with a weighted support-vector machine model. Wind speed and local industry activity are found to be the main controlling parameters where the location of exposed power lines is a likely trigger. In summary, we discuss causing factors for failures in the power grid and enable the distribution system operators to implement strategies to prevent and mitigate incoming failures.      
### 37.Block Alternating Bregman Majorization Minimization with Extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04395.pdf)
>  In this paper, we consider a class of nonsmooth nonconvex optimization problems whose objective is the sum of a block relative smooth function and a proper and lower semicontinuous block separable function. Although the analysis of block proximal gradient (BPG) methods for the class of block $L$-smooth functions have been successfully extended to Bregman BPG methods that deal with the class of block relative smooth functions, accelerated Bregman BPG methods are scarce and challenging to design. Taking our inspiration from Nesterov-type acceleration and the majorization-minimization scheme, we propose a block alternating Bregman Majorization-Minimization framework with Extrapolation (BMME). We prove subsequential convergence of BMME to a first-order stationary point under mild assumptions, and study its global convergence under stronger conditions. We illustrate the effectiveness of BMME on the penalized orthogonal nonnegative matrix factorization problem.      
### 38.Semantic Segmentation on Multiple Visual Domains  [ :arrow_down: ](https://arxiv.org/pdf/2107.04326.pdf)
>  Semantic segmentation models only perform well on the domain they are trained on and datasets for training are scarce and often have a small label-spaces, because the pixel level annotations required are expensive to make. Thus training models on multiple existing domains is desired to increase the output label-space. Current research shows that there is potential to improve accuracy across datasets by using multi-domain training, but this has not yet been successfully extended to datasets of three different non-overlapping domains without manual labelling. In this paper a method for this is proposed for the datasets Cityscapes, SUIM and SUN RGB-D, by creating a label-space that spans all classes of the datasets. Duplicate classes are merged and discrepant granularity is solved by keeping classes separate. Results show that accuracy of the multi-domain model has higher accuracy than all baseline models together, if hardware performance is equalized, as resources are not limitless, showing that models benefit from additional data even from domains that have nothing in common.      
### 39.Partial Gradient Optimal Thresholding Algorithms for a Class of Sparse Optimization Problems  [ :arrow_down: ](https://arxiv.org/pdf/2107.04319.pdf)
>  The optimization problems with a sparsity constraint is a class of important global optimization problems. A typical type of thresholding algorithms for solving such a problem adopts the traditional full steepest descent direction or Newton-like direction as a search direction to generate an iterate on which a certain thresholding is performed. Traditional hard thresholding discards a large part of a vector when the vector is dense. Thus a large part of important information contained in a dense vector has been lost in such a thresholding process. Recent study [Zhao, SIAM J Optim, 30(1), pp. 31-55, 2020] shows that the hard thresholding should be applied to a compressible vector instead of a dense vector to avoid a big loss of information. On the other hand, the optimal $k$-thresholding as a novel thresholding technique may overcome the intrinsic drawback of hard thresholding, and performs thresholding and objective function minimization simultaneously. This motivates us to propose the so-called partial gradient optimal thresholding method in this paper, which is an integration of the partial gradient and the optimal $k$-thresholding technique. The solution error bound and convergence for the proposed algorithms have been established in this paper under suitable conditions. Application of our results to the sparse optimization problems arising from signal recovery is also discussed. Experiment results from synthetic data indicate that the proposed algorithm called PGROTP is efficient and comparable to several existing algorithms.      
### 40.Massive MIMO Communication with Intelligent Reflecting Surface  [ :arrow_down: ](https://arxiv.org/pdf/2107.04255.pdf)
>  This paper studies the feasibility of deploying intelligent reflecting surfaces (IRSs) in massive MIMO (multiple-input multiple-output) systems to improve the performance of users in the service dead zone. To reduce the channel training overhead, we advocate a novel protocol for the uplink communication in the IRS-assisted massive MIMO systems. Under this protocol, the IRS reflection coefficients are optimized based on the channel covariance matrices, which are generally fixed for many coherence blocks, to boost the long-term performance. Then, given the IRS reflecting coefficients, the BS beamforming vectors are designed in each coherence block based on the effective channel of each user, which is the superposition of its direct and reflected user-IRS-BS channels, to improve the instantaneous performance. Since merely the user effective channels are estimated in each coherence block, the training overhead of this protocol is the same as that in the legacy wireless systems without IRSs. Moreover, in the asymptotic regime that the numbers of IRS elements and BS antennas both go to infinity with a fixed ratio, we manage to first characterize the minimum mean-squared error (MMSE) estimators of the user effective channels and then quantify the closed-form user achievable rates as functions of channel covariance matrices with channel training overhead and estimation error taken into account. Interestingly, it is shown that the properties of channel hardening and favorable propagation still hold for the user effective channels, and satisfactory user rates are thus achievable even if simple BS beamforming solutions, e.g., maximal-ratio combining, are employed. Finally, thanks to the rate characterization, we design a low-complexity algorithm to optimize the IRS reflection coefficients based on channel covariance matrices.      
### 41.Structured Hammerstein-Wiener Model Learning for Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2107.04247.pdf)
>  This paper aims to improve the reliability of optimal control using models constructed by machine learning methods. Optimal control problems based on such models are generally non-convex and difficult to solve online. In this paper, we propose a model that combines the Hammerstein-Wiener model with input convex neural networks, which have recently been proposed in the field of machine learning. An important feature of the proposed model is that resulting optimal control problems are effectively solvable exploiting their convexity and partial linearity while retaining flexible modeling ability. The practical usefulness of the method is examined through its application to the modeling and control of an engine airpath system.      
### 42.Improved Breath Phase and Continuous Adventitious Sound Detection in Lung and Tracheal Sound Using Mixed Set Training and Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04229.pdf)
>  Previously, we established a lung sound database, HF_Lung_V2 and proposed convolutional bidirectional gated recurrent unit (CNN-BiGRU) models with adequate ability for inhalation, exhalation, continuous adventitious sound (CAS), and discontinuous adventitious sound detection in the lung sound. In this study, we proceeded to build a tracheal sound database, HF_Tracheal_V1, containing 11107 of 15-second tracheal sound recordings, 23087 inhalation labels, 16728 exhalation labels, and 6874 CAS labels. The tracheal sound in HF_Tracheal_V1 and the lung sound in HF_Lung_V2 were either combined or used alone to train the CNN-BiGRU models for respective lung and tracheal sound analysis. Different training strategies were investigated and compared: (1) using full training (training from scratch) to train the lung sound models using lung sound alone and train the tracheal sound models using tracheal sound alone, (2) using a mixed set that contains both the lung and tracheal sound to train the models, and (3) using domain adaptation that finetuned the pre-trained lung sound models with the tracheal sound data and vice versa. Results showed that the models trained only by lung sound performed poorly in the tracheal sound analysis and vice versa. However, the mixed set training and domain adaptation can improve the performance of exhalation and CAS detection in the lung sound, and inhalation, exhalation, and CAS detection in the tracheal sound compared to positive controls (lung models trained only by lung sound and vice versa). Especially, a model derived from the mixed set training prevails in the situation of killing two birds with one stone.      
### 43.Multi-path Convolutional Neural Networks Efficiently Improve Feature Extraction in Continuous Adventitious Lung Sound Detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.04226.pdf)
>  We previously established a large lung sound database, HF_Lung_V2 (Lung_V2). We trained convolutional-bidirectional gated recurrent unit (CNN-BiGRU) networks for detecting inhalation, exhalation, continuous adventitious sound (CAS) and discontinuous adventitious sound at the recording level on the basis of Lung_V2. However, the performance of CAS detection was poor due to many reasons, one of which is the highly diversified CAS patterns. To make the original CNN-BiGRU model learn the CAS patterns more effectively and not cause too much computing burden, three strategies involving minimal modifications of the network architecture of the CNN layers were investigated: (1) making the CNN layers a bit deeper by using the residual blocks, (2) making the CNN layers a bit wider by increasing the number of CNN kernels, and (3) separating the feature input into multiple paths (the model was denoted by Multi-path CNN-BiGRU). The performance of CAS segment and event detection were evaluated. Results showed that improvement in CAS detection was observed among all the proposed architecture-modified models. The F1 score for CAS event detection of the proposed models increased from 0.445 to 0.491-0.530, which was deemed significant. However, the Multi-path CNN-BiGRU model outperformed the other models in terms of the number of winning titles (five) in total nine evaluation metrics. In addition, the Multi-path CNN-BiGRU model did not cause extra computing burden (0.97-fold inference time) compared to the original CNN-BiGRU model. Conclusively, the Multi-path CNN layers can efficiently improve the effectiveness of feature extraction and subsequently result in better CAS detection.      
### 44.Neural-Network-Optimized Degree-Specific Weights for LDPC MinSum Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2107.04221.pdf)
>  Neural Normalized MinSum (N-NMS) decoding delivers better frame error rate (FER) performance on linear block codes than conventional normalized MinSum (NMS) by assigning dynamic multiplicative weights to each check-to-variable message in each iteration. Previous N-NMS efforts have primarily investigated short-length block codes (N &lt; 1000), because the number of N-NMS parameters to be trained is proportional to the number of edges in the parity check matrix and the number of iterations, which imposes am impractical memory requirement when Pytorch or Tensorflow is used for training. This paper provides efficient approaches to training parameters of N-NMS that support N-NMS for longer block lengths. Specifically, this paper introduces a family of neural 2-dimensional normalized (N-2D-NMS) decoders with with various reduced parameter sets and shows how performance varies with the parameter set selected. The N-2D-NMS decoders share weights with respect to check node and/or variable node degree. Simulation results justify this approach, showing that the trained weights of N-NMS have a strong correlation to the check node degree, variable node degree, and iteration number. Further simulation results on the (3096,1032) protograph-based raptor-like (PBRL) code show that N-2D-NMS decoder can achieve the same FER as N-NMS with significantly fewer parameters required. The N-2D-NMS decoder for a (16200,7200) DVBS-2 standard LDPC code shows a lower error floor than belief propagation. Finally, a hybrid decoding structure combining a feedforward structure with a recurrent structure is proposed in this paper. The hybrid structure shows similar decoding performance to full feedforward structure, but requires significantly fewer parameters.      
### 45.Personalized Federated Learning over non-IID Data for Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2107.04189.pdf)
>  Localization and tracking of objects using data-driven methods is a popular topic due to the complexity in characterizing the physics of wireless channel propagation models. In these modeling approaches, data needs to be gathered to accurately train models, at the same time that user's privacy is maintained. An appealing scheme to cooperatively achieve these goals is known as Federated Learning (FL). A challenge in FL schemes is the presence of non-independent and identically distributed (non-IID) data, caused by unevenly exploration of different areas. In this paper, we consider the use of recent FL schemes to train a set of personalized models that are then optimally fused through Bayesian rules, which makes it appropriate in the context of indoor localization.      
### 46.EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments  [ :arrow_down: ](https://arxiv.org/pdf/2107.04174.pdf)
>  Augmented Reality (AR) as a platform has the potential to facilitate the reduction of the cocktail party effect. Future AR headsets could potentially leverage information from an array of sensors spanning many different modalities. Training and testing signal processing and machine learning algorithms on tasks such as beam-forming and speech enhancement require high quality representative data. To the best of the author's knowledge, as of publication there are no available datasets that contain synchronized egocentric multi-channel audio and video with dynamic movement and conversations in a noisy environment. In this work, we describe, evaluate and release a dataset that contains over 5 hours of multi-modal data useful for training and testing algorithms for the application of improving conversations for an AR glasses wearer. We provide speech intelligibility, quality and signal-to-noise ratio improvement results for a baseline method and show improvements across all tested metrics. The dataset we are releasing contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head bounding boxes, target of speech and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.      
### 47.Intelligent Link Adaptation for Grant-Free Access Cellular Networks: A Distributed Deep Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2107.04145.pdf)
>  With the continuous growth of machine-type devices (MTDs), it is expected that massive machine-type communication (mMTC) will be the dominant form of traffic in future wireless networks. Applications based on this technology, have fundamentally different traffic characteristics from human-to-human (H2H) communication, which involves a relatively small number of devices transmitting large packets consistently. Conversely, in mMTC applications, a very large number of MTDs transmit small packets sporadically. Therefore, conventional grant-based access schemes commonly adopted for H2H service, are not suitable for mMTC, as they incur in a large overhead associated with the channel request procedure. We propose three grant-free distributed optimization architectures that are able to significantly minimize the average power consumption of the network. The problem of physical layer (PHY) and medium access control (MAC) optimization in grant-free random access transmission is is modeled as a partially observable stochastic game (POSG) aimed at minimizing the average transmit power under a per-device delay constraint. The results show that the proposed architectures are able to achieve significantly less average latency than a baseline, while spending less power. Moreover, the proposed architectures are more robust than the baseline, as they present less variance in the performance for different system realizations.      
### 48.Distributed formation control for manipulator end-effectors  [ :arrow_down: ](https://arxiv.org/pdf/2107.04141.pdf)
>  We present three classes of distributed formation controllers for achieving and maintaining the 2D/3D formation shape of manipulator end-effectors to cope with different scenarios due to availability of modeling parameters. We firstly present a distributed formation controller for manipulators whose system parameters are perfectly known. The formation control objective is achieved by assigning virtual springs between end-effectors and by adding damping terms at joints, which provides a clear physical interpretation of the proposed solution. Subsequently, we extend it to the case where manipulator kinematic and system parameters are not exactly known. An extra integrator and an adaptive estimator are introduced for gravitational compensation and stabilization, respectively. Simulation results with planar manipulators and with seven degree-of-freedom humanoid manipulator arms are presented to illustrate the effectiveness of the proposed approach.      
### 49.The Simons Observatory: HoloSim-ML: machine learning applied to the efficient analysis of radio holography measurements of complex optical systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.04138.pdf)
>  Near-field radio holography is a common method for measuring and aligning mirror surfaces for millimeter and sub-millimeter telescopes. In instruments with more than a single mirror, degeneracies arise in the holography measurement, requiring multiple measurements and new fitting methods. We present HoloSim-ML, a Python code for beam simulation and analysis of radio holography data from complex optical systems. This code uses machine learning to efficiently determine the position of hundreds of mirror adjusters on multiple mirrors with few micron accuracy. We apply this approach to the example of the Simons Observatory 6m telescope.      
### 50.Improved Language Identification Through Cross-Lingual Self-Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.04082.pdf)
>  Language identification greatly impacts the success of downstream tasks such as automatic speech recognition. Recently, self-supervised speech representations learned by wav2vec 2.0 have been shown to be very effective for a range of speech tasks. We extend previous self-supervised work on language identification by experimenting with pre-trained models which were learned on real-world unconstrained speech in multiple languages and not just on English. We show that models pre-trained on many languages perform better and enable language identification systems that require very little labeled data to perform well. Results on a 25 languages setup show that with only 10 minutes of labeled data per language, a cross-lingually pre-trained model can achieve over 93% accuracy.      
### 51.Machine Learning for Stuttering Identification: Review, Challenges &amp; Future Directions  [ :arrow_down: ](https://arxiv.org/pdf/2107.04057.pdf)
>  Stuttering is a speech disorder during which the flow of speech is interrupted by involuntary pauses and repetition of sounds. Stuttering identification is an interesting interdisciplinary domain research problem which involves pathology, psychology, acoustics, and signal processing that makes it hard and complicated to detect. Recent developments in machine and deep learning have dramatically revolutionized speech domain, however minimal attention has been given to stuttering identification. This work fills the gap by trying to bring researchers together from interdisciplinary fields. In this paper, we review comprehensively acoustic features, statistical and deep learning based stuttering/disfluency classification methods. We also present several challenges and possible future directions.      
### 52.Optimal Output Consensus of Second-Order Uncertain Nonlinear Systems on Weight-Unbalanced Directed Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.04056.pdf)
>  This paper investigates the distributed optimal output consensus problem of second-order uncertain nonlinear multi-agent systems over weight-unbalanced directed networks. Under the standard assumption that local cost functions are strongly convex with globally Lipschitz gradients, a novel distributed dynamic state feedback controller is developed such that the outputs of all the agents reach the optimal solution to minimize the global cost function which is the sum of all the local cost functions. The controller design is based on a two-layer strategy, where a distributed optimal coordinator and a reference-tracking controller are proposed to address the challenges arising from unbalanced directed networks and uncertain nonlinear functions respectively. A key feature of the proposed controller is that the nonlinear functions containing the uncertainties and disturbances are not required to be globally Lipschitz. Furthermore, by exploiting adaptive control technique, no prior knowledge of the uncertainties or disturbances is required either. Two simulation examples are finally provided to illustrate the effectiveness of the proposed control scheme.      
