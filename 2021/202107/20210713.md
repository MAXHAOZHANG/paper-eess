# ArXiv eess --Tue, 13 Jul 2021
### 1.Implementing a LoRa Software-Defined Radio on a General-Purpose ULP Microcontroller  [ :arrow_down: ](https://arxiv.org/pdf/2107.05584.pdf)
>  Emerging Internet-of-Things sensing applications rely on ultra low-power (ULP) microcontroller units (MCUs) that wirelessly transmit data to the cloud. Typical MCUs nowadays consist of generic blocks, except for the protocol-specific radios implemented in hardware. Hardware radios however slow down the evolution of wireless protocols due to retrocompatiblity concerns. In this work, we explore a software-defined radio architecture by demonstrating a LoRa transceiver running on custom ULP MCU codenamed SleepRider with an ARM Cortex-M4 CPU. In SleepRider MCU, we offload the generic baseband operations (e.g., low-pass filtering) to a reconfigurable digital front-end block and use the Cortex-M4 CPU to perform the protocol-specific computations. Our software implementation of the LoRa physical layer only uses the native SIMD instructions of the Cortex-M4 to achieve real-time transmission and reception of LoRa packets. SleepRider MCU has been fabricated in a 28nm FDSOI CMOS technology and is used in a testbed to experimentally validate the software implementation. Experimental results show that the proposed software-defined radio requires only a CPU frequency of 20 MHz to correctly receive a LoRa packet, with an ultra-low power consumption of 0.42 mW on average.      
### 2.Extending Text-to-Speech Synthesis with Articulatory Movement Prediction using Ultrasound Tongue Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2107.05550.pdf)
>  In this paper, we present our first experiments in text-to-articulation prediction, using ultrasound tongue image targets. We extend a traditional (vocoder-based) DNN-TTS framework with predicting PCA-compressed ultrasound images, of which the continuous tongue motion can be reconstructed in synchrony with synthesized speech. We use the data of eight speakers, train fully connected and recurrent neural networks, and show that FC-DNNs are more suitable for the prediction of sequential data than LSTMs, in case of limited training data. Objective experiments and visualized predictions show that the proposed solution is feasible and the generated ultrasound videos are close to natural tongue movement. Articulatory movement prediction from text input can be useful for audiovisual speech synthesis or computer-assisted pronunciation training.      
### 3.MoDIR: Motion-Compensated Training for Deep Image Reconstruction without Ground Truth  [ :arrow_down: ](https://arxiv.org/pdf/2107.05533.pdf)
>  Deep neural networks for medical image reconstruction are traditionally trained using high-quality ground-truth images as training targets. Recent work onNoise2Noise (N2N) has shown the potential of using multiple noisy measurements of the same object as an alternative to having a ground truth. However, existing N2N-based methods cannot exploit information from various motion states, limiting their ability to learn on moving objects. This paper addresses this issue by proposing a novel motion-compensated deep image reconstruction (MoDIR) method that can use information from several unregistered and noisy measurements for training. MoDIR deals with object motion by including a deep registration module jointly trained with the deep reconstruction network without any ground-truth supervision. We validate MoDIR on both simulated and experimentally collected magnetic resonance imaging (MRI) data and show that it significantly improves imaging quality.      
### 4.Synthesizing Multi-Tracer PET Images for Alzheimer's Disease Patients using a 3D Unified Anatomy-aware Cyclic Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2107.05491.pdf)
>  Positron Emission Tomography (PET) is an important tool for studying Alzheimer's disease (AD). PET scans can be used as diagnostics tools, and to provide molecular characterization of patients with cognitive disorders. However, multiple tracers are needed to measure glucose metabolism (18F-FDG), synaptic vesicle protein (11C-UCB-J), and $\beta$-amyloid (11C-PiB). Administering multiple tracers to patient will lead to high radiation dose and cost. In addition, access to PET scans using new or less-available tracers with sophisticated production methods and short half-life isotopes may be very limited. Thus, it is desirable to develop an efficient multi-tracer PET synthesis model that can generate multi-tracer PET from single-tracer PET. Previous works on medical image synthesis focus on one-to-one fixed domain translations, and cannot simultaneously learn the feature from multi-tracer domains. Given 3 or more tracers, relying on previous methods will also create a heavy burden on the number of models to be trained. To tackle these issues, we propose a 3D unified anatomy-aware cyclic adversarial network (UCAN) for translating multi-tracer PET volumes with one unified generative model, where MR with anatomical information is incorporated. Evaluations on a multi-tracer PET dataset demonstrate the feasibility that our UCAN can generate high-quality multi-tracer PET volumes, with NMSE less than 15% for all PET tracers.      
### 5.The Power of Proxy Data and Proxy Networks for Hyper-Parameter Optimization in Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.05471.pdf)
>  Deep learning models for medical image segmentation are primarily data-driven. Models trained with more data lead to improved performance and generalizability. However, training is a computationally expensive process because multiple hyper-parameters need to be tested to find the optimal setting for best performance. In this work, we focus on accelerating the estimation of hyper-parameters by proposing two novel methodologies: proxy data and proxy networks. Both can be useful for estimating hyper-parameters more efficiently. We test the proposed techniques on CT and MR imaging modalities using well-known public datasets. In both cases using one dataset for building proxy data and another data source for external evaluation. For CT, the approach is tested on spleen segmentation with two datasets. The first dataset is from the medical segmentation decathlon (MSD), where the proxy data is constructed, the secondary dataset is utilized as an external validation dataset. Similarly, for MR, the approach is evaluated on prostate segmentation where the first dataset is from MSD and the second dataset is PROSTATEx. First, we show higher correlation to using full data for training when testing on the external validation set using smaller proxy data than a random selection of the proxy data. Second, we show that a high correlation exists for proxy networks when compared with the full network on validation Dice score. Third, we show that the proposed approach of utilizing a proxy network can speed up an AutoML framework for hyper-parameter search by 3.3x, and by 4.4x if proxy data and proxy network are utilized together.      
### 6.A Multi-Lead Fusion Method for the Accurate Delineation of QRS Complex Location in 12 Lead ECG Signal  [ :arrow_down: ](https://arxiv.org/pdf/2107.05469.pdf)
>  This paper presents a multi-lead fusion method for the accurate and automated detection of the QRS complex location in 12 lead ECG (Electrocardiogram) signals. The proposed multi-lead fusion method accurately delineates the QRS complex by the fusion of detected QRS complexes of the individual 12 leads. The proposed algorithm consists of two major stages. Firstly, the QRS complex location of each lead is detected by the single lead QRS detection algorithm. Secondly, the multi-lead fusion algorithm combines the information of the QRS complex locations obtained in each of the 12 leads. The performance of the proposed algorithm is improved in terms of Sensitivity and Positive Predictivity by discarding the false positives. The proposed method is validated on the ECG signals with various artifacts, inter and intra subject variations. The performance of the proposed method is validated on the long duration recorded ECG signals of St. Petersburg INCART database with Sensitivity of 99.87% and Positive Predictivity of 99.96% and on the short duration recorded ECG signals of CSE (Common Standards for Electrocardiography) multi-lead database with 100% Sensitivity and 99.13% Positive Predictivity.      
### 7.Sound Event Detection: A Tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2107.05463.pdf)
>  The goal of automatic sound event detection (SED) methods is to recognize what is happening in an audio signal and when it is happening. In practice, the goal is to recognize at what temporal instances different sounds are active within an audio signal. This paper gives a tutorial presentation of sound event detection, including its definition, signal processing and machine learning approaches, evaluation, and future perspectives.      
### 8.Sentinel-1 Additive Noise Removal from Cross-Polarization Extra-Wide TOPSAR with Dynamic Least-Squares  [ :arrow_down: ](https://arxiv.org/pdf/2107.05437.pdf)
>  Sentinel-1 is a synthetic aperture radar (SAR) platform with an operational mode called extra wide (EW) that offers large regions of ocean areas to be observed. A major issue with EW images is that the cross-polarized HV and VH channels have prominent additive noise patterns relative to low backscatter intensity, which disrupts tasks that require manual or automated interpretation. The European Space Agency (ESA) provides a method for removing the additive noise pattern by means of lookup tables, but applying them directly produces unsatisfactory results because characteristics of the noise still remain. Furthermore, evidence suggests that the magnitude of the additive noise dynamically depends on factors that are not considered by the ESA estimated noise field. <br>To address these issues we propose a quadratic objective function to model the mis-scale of the provided noise field on an image. We consider a linear denoising model that re-scales the noise field for each subswath, whose parameters are found from a least-squares solution over the objective function. This method greatly reduces the presence of additive noise while not requiring a set of training images, is robust to heterogeneity in images, dynamically estimates parameters for each image, and finds parameters using a closed-form solution. <br>Two experiments were performed to validate the proposed method. The first experiment simulated noise removal on a set of RADARSAT-2 images with noise fields artificially imposed on them. The second experiment conducted noise removal on a set of Sentinel-1 images taken over the five oceans. Afterwards, quality of the noise removal was evaluated based on the appearance of open-water. The two experiments indicate that the proposed method marks an improvement both visually and through numerical measures.      
### 9.Computer-Aided Diagnosis of Low Grade Endometrial Stromal Sarcoma (LGESS)  [ :arrow_down: ](https://arxiv.org/pdf/2107.05426.pdf)
>  Low grade endometrial stromal sarcoma (LGESS) is rare form of cancer, accounting for about 0.2% of all uterine cancer cases. Approximately 75% of LGESS patients are initially misdiagnosed with leiomyoma, which is a type of benign tumor, also known as fibroids. In this research, uterine tissue biopsy images of potential LGESS patients are preprocessed using segmentation and staining normalization algorithms. A variety of classic machine learning and leading deep learning models are then applied to classify tissue images as either benign or cancerous. For the classic techniques considered, the highest classification accuracy we attain is about 0.85, while our best deep learning model achieves an accuracy of approximately 0.87. These results indicate that properly trained learning algorithms can play a useful role in the diagnosis of LGESS.      
### 10.Autofocusing Optimal Search Algorithm for a Telescope System  [ :arrow_down: ](https://arxiv.org/pdf/2107.05398.pdf)
>  Focus accuracy affects the quality of the astronomical observations. Auto-focusing is necessary for imaging systems designed for astronomical observations. The automatic focus system searches for the best focus position by using a proposed search algorithm. The search algorithm uses the image's focus levels as its objective function in the search process. This paper aims to study the performance of several search algorithms to select a suitable one. The proper search algorithm will be used to develop an automatic focus system for Kottamia Astronomical Observatory (KAO). The optimal search algorithm is selected by applying several search algorithms into five sequences of star-clusters observations. Then, their performance is evaluated based on two criteria, which are accuracy and number of steps. The experimental results show that the Binary search is the optimal search algorithm.      
### 11.EndoUDA: A modality independent segmentation approach for endoscopy imaging  [ :arrow_down: ](https://arxiv.org/pdf/2107.05342.pdf)
>  Gastrointestinal (GI) cancer precursors require frequent monitoring for risk stratification of patients. Automated segmentation methods can help to assess risk areas more accurately, and assist in therapeutic procedures or even removal. In clinical practice, addition to the conventional white-light imaging (WLI), complimentary modalities such as narrow-band imaging (NBI) and fluorescence imaging are used. While, today most segmentation approaches are supervised and only concentrated on a single modality dataset, this work exploits to use a target-independent unsupervised domain adaptation (UDA) technique that is capable to generalize to an unseen target modality. In this context, we propose a novel UDA-based segmentation method that couples the variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and uses a joint loss for latent-space optimization for target samples. We show that our model can generalize to unseen target NBI (target) modality when trained using only WLI (source) modality. Our experiments on both upper and lower GI endoscopy data show the effectiveness of our approach compared to naive supervised approach and state-of-the-art UDA segmentation methods.      
### 12.Visual Transformer with Statistical Test for COVID-19 Classification  [ :arrow_down: ](https://arxiv.org/pdf/2107.05334.pdf)
>  With the massive damage in the world caused by Coronavirus Disease 2019 SARS-CoV-2 (COVID-19), many related research topics have been proposed in the past two years. The Chest Computed Tomography (CT) scans are the most valuable materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19 classification of Chest CT scan is based on a single-slice level, implying that the most critical CT slice should be selected from the original CT scan volume manually. We simultaneously propose 2-D and 3-D models to predict the COVID-19 of CT scan to tickle this issue. In our 2-D model, we introduce the Deep Wilcoxon signed-rank test (DWCC) to determine the importance of each slice of a CT scan to overcome the issue mentioned previously. Furthermore, a Convolutional CT scan-Aware Transformer (CCAT) is proposed to discover the context of the slices fully. The frame-level feature is extracted from each CT slice based on any backbone network and followed by feeding the features to our within-slice-Transformer (WST) to discover the context information in the pixel dimension. The proposed Between-Slice-Transformer (BST) is used to aggregate the extracted spatial-context features of every CT slice. A simple classifier is then used to judge whether the Spatio-temporal features are COVID-19 or non-COVID-19. The extensive experiments demonstrated that the proposed CCAT and DWCC significantly outperform the state-of-the-art methods.      
### 13.R3L: Connecting Deep Reinforcement Learning to Recurrent Neural Networks for Image Denoising via Residual Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2107.05318.pdf)
>  State-of-the-art image denoisers exploit various types of deep neural networks via deterministic training. Alternatively, very recent works utilize deep reinforcement learning for restoring images with diverse or unknown corruptions. Though deep reinforcement learning can generate effective policy networks for operator selection or architecture search in image restoration, how it is connected to the classic deterministic training in solving inverse problems remains unclear. In this work, we propose a novel image denoising scheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show that R3L is equivalent to a deep recurrent neural network that is trained using a stochastic reward, in contrast to many popular denoisers using supervised learning with deterministic losses. To benchmark the effectiveness of reinforcement learning in R3L, we train a recurrent neural network with the same architecture for residual recovery using the deterministic loss, thus to analyze how the two different training strategies affect the denoising performance. With such a unified benchmarking system, we demonstrate that the proposed R3L has better generalizability and robustness in image denoising when the estimated noise level varies, comparing to its counterparts using deterministic training, as well as various state-of-the-art image denoising algorithms.      
### 14.A Survey on Non-Geostationary Satellite Systems: The Communication Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2107.05312.pdf)
>  Non-geostationary (NGSO) satellites are envisioned to support various new communication applications from countless industries. NGSO systems are known for a number of key features such as lower propagation delay, smaller size, and lower signal losses in comparison to the conventional geostationary (GSO) satellites, which will enable latency-critical applications to be provided through satellites. NGSO promises a dramatic boost in communication speed and energy efficiency, and thus, tackling the main inhibiting factors of commercializing GSO satellites for broader utilizations. However, there are still many NGSO deployment challenges to be addressed to ensure seamless integration not only with GSO systems but also with terrestrial networks. These unprecedented challenges are discussed in this paper, including coexistence with GSO systems in terms of spectrum access and regulatory issues, satellite constellation and architecture designs, resource management problems, and user equipment requirements. Beyond this, the promised improvements of NGSO systems have motivated this survey to provide the state-of-the-art NGSO research focusing on the communication prospects, including physical layer and radio access technologies along with the networking aspects and the overall system features and architectures. We also outline a set of innovative research directions and new opportunities for future NGSO research.      
### 15.TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.05274.pdf)
>  With the development of deep encoder-decoder architectures and large-scale annotated medical datasets, great progress has been achieved in the development of automatic medical image segmentation. Due to the stacking of convolution layers and the consecutive sampling operations, existing standard models inevitably encounter the information recession problem of feature representations, which fails to fully model the global contextual feature dependencies. To overcome the above challenges, this paper proposes a novel Transformer based medical image semantic segmentation framework called TransAttUnet, in which the multi-level guided attention and multi-scale skip connection are jointly designed to effectively enhance the functionality and flexibility of traditional U-shaped architecture. Inspired by Transformer, a novel self-aware attention (SAA) module with both Transformer Self Attention (TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to effectively learn the non-local interactions between encoder features. In particular, we also establish additional multi-scale skip connections between decoder blocks to aggregate the different semantic-scale upsampling features. In this way, the representation ability of multi-scale context information is strengthened to generate discriminative features. Benefitting from these complementary components, the proposed TransAttUnet can effectively alleviate the loss of fine details caused by the information recession problem, improving the diagnostic sensitivity and segmentation quality of medical image analysis. Extensive experiments on multiple medical image segmentation datasets of different imaging demonstrate that our method consistently outperforms the state-of-the-art baselines.      
### 16.Asymptotic analysis of V-BLAST MIMO for coherent optical wireless communications in Gamma-Gamma turbulence  [ :arrow_down: ](https://arxiv.org/pdf/2107.05254.pdf)
>  This paper investigates the asymptotic BER performance of coherent optical wireless communication systems in Gamma-Gamma turbulence when applying the V-BLAST MIMO scheme. A new method is proposed to quantify the performance of the system and mathematical solutions for asymptotic BER performance are derived. Counterintuitive results are shown since the diversity gain of the V-BLAST MIMO system is equal to the number of the receivers. As a consequence, it is shown that when applying the V-BLAST MIMO scheme, the symbol rate per transmission can be equal to the number of transmitters with some cost to diversity gain. This means that we can simultaneously exploit the spatial multiplexing and diversity properties of the MIMO system to achieve a higher data rate than existing schemes in a channel that displays severe turbulence and moderate attenuation.      
### 17.Joint Activity Detection, Channel Estimation, and Data Decoding for Grant-free Massive Random Access  [ :arrow_down: ](https://arxiv.org/pdf/2107.05246.pdf)
>  In the massive machine-type communication (mMTC) scenario, a large number of devices with sporadic traffic need to access the network on limited radio resources. While grant-free random access has emerged as a promising mechanism for massive access, its potential has not been fully unleashed. In particular, the common sparsity pattern in the received pilot and data signal has been ignored in most existing studies, and auxiliary information of channel decoding has not been utilized for user activity detection. This paper endeavors to develop advanced receivers in a holistic manner for joint activity detection, channel estimation, and data decoding. In particular, a turbo receiver based on the bilinear generalized approximate message passing (BiG-AMP) algorithm is developed. In this receiver, all the received symbols will be utilized to jointly estimate the channel state, user activity, and soft data symbols, which effectively exploits the common sparsity pattern. Meanwhile, the extrinsic information from the channel decoder will assist the joint channel estimation and data detection. To reduce the complexity, a low-cost side information-aided receiver is also proposed, where the channel decoder provides side information to update the estimates on whether a user is active or not. Simulation results show that the turbo receiver is able to reduce the activity detection, channel estimation, and data decoding errors effectively, while the side information-aided receiver notably outperforms the conventional method with a relatively low complexity.      
### 18.UniSpeech at scale: An Empirical Study of Pre-training Method on Large-Scale Speech Recognition Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2107.05233.pdf)
>  Recently, there has been a vast interest in self-supervised learning (SSL) where the model is pre-trained on large scale unlabeled data and then fine-tuned on a small labeled dataset. The common wisdom is that SSL helps resource-limited tasks in which only a limited amount of labeled data is available. The benefit of SSL keeps diminishing when the labeled training data amount increases. To our best knowledge, at most a few thousand hours of labeled data was used in the study of SSL. In contrast, the industry usually uses tens of thousands of hours of labeled data to build high-accuracy speech recognition (ASR) systems for resource-rich languages. In this study, we take the challenge to investigate whether and how SSL can improve the ASR accuracy of a state-of-the-art production-scale Transformer-Transducer model, which was built with 65 thousand hours of anonymized labeled EN-US data.      
### 19.Perceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.05222.pdf)
>  In this paper we investigate speech denoising as a defense against adversarial attacks on automatic speech recognition (ASR) systems. Adversarial attacks attempt to force misclassification by adding small perturbations to the original speech signal. We propose to counteract this by employing a neural-network based denoiser as a pre-processor in the ASR pipeline. The denoiser is independent of the downstream ASR model, and thus can be rapidly deployed in existing systems. We found that training the denoisier using a perceptually motivated loss function resulted in increased adversarial robustness without compromising ASR performance on benign samples. Our defense was evaluated (as a part of the DARPA GARD program) on the 'Kenansville' attack strategy across a range of attack strengths and speech samples. An average improvement in Word Error Rate (WER) of about 7.7% was observed over the undefended model at 20 dB signal-to-noise-ratio (SNR) attack strength.      
### 20.Equivariant filter (EqF): A general filter design for systems on homogeneous spaces  [ :arrow_down: ](https://arxiv.org/pdf/2107.05193.pdf)
>  The kinematics of many mechanical systems encountered in robotics and other fields, such as single-bearing attitude estimation and SLAM, are naturally posed on homogeneous spaces: That is, their state lies in a smooth manifold equipped with a transitive Lie-group symmetry. This paper shows that any system posed in a homogeneous space can be extended to a larger system that is equivariant under a symmetry action. The equivariant structure of the system is exploited to propose a novel new filter, the Equivariant Filter (EqF), based on linearisation of global error dynamics derived from the symmetry action. The EqF is applied to an example of estimating the positions of stationary landmarks relative to a moving monocular camera that is intractable for previously proposed symmetry based filter design methodologies.      
### 21.The Impact of Three-phase Impedances on the Stability of DER systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.05191.pdf)
>  In this work we explore impedance-based interactions that arise when inverter-connected distributed energy resources (DERs) inject real and reactive power to regulate voltage and power flows on three-phase unbalanced distribution grids. We consider two inverter control frameworks that compute power setpoints: a mix of volt/var and volt/watt control, and phasor-based control. On a two-bus network we isolate how line length, R/X ratio, and mutual impedances each affects the stability of the DER system. We validate our analysis through simulation of the two-bus network, and validate that the effects found extend to the IEEE 123-node feeder. Furthermore, we find that the impedance properties make it more challenging to design a stable DER system for certain placements of DER on this feeder.      
### 22.Deep-learning-based Hyperspectral imaging through a RGB camera  [ :arrow_down: ](https://arxiv.org/pdf/2107.05190.pdf)
>  Hyperspectral image (HSI) contains both spatial pattern and spectral information which has been widely used in food safety, remote sensing, and medical detection. However, the acquisition of hyperspectral images is usually costly due to the complicated apparatus for the acquisition of optical spectrum. Recently, it has been reported that HSI can be reconstructed from single RGB image using convolution neural network (CNN) algorithms. Compared with the traditional hyperspectral cameras, the method based on CNN algorithms is simple, portable and low cost. In this study, we focused on the influence of the RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated with a monochromator were used as the standard light source to calibrate the CSS. And the experimental results show that the CSS plays a significant role in the reconstruction accuracy of an HSI. In addition, we proposed a new HSI reconstruction network where the dimensional structure of the original hyperspectral datacube was modified by 3D matrix transpose to improve the reconstruction accuracy.      
### 23.Board-level Code-Modulated Embedded Test and Calibration of an X-band Phased-Array Transceiver  [ :arrow_down: ](https://arxiv.org/pdf/2107.05162.pdf)
>  We present methods for built-in test and calibration of phased arrays using code-modulated embedded test (CoMET). Our approach employs Cartesian modulation of test signals within each element using existing phase shifters, combining of these signals into an aggregate code-multiplexed response, downconversion and creation of code-modulated element-to-element "interference products" using a built-in power detector, demodulation of correlations from the digitized interference response, and extraction of amplitude and phase per element using an equation solver. Rotated-axis methodology is discussed for accurate extraction of phase near the original 0/90/180/270 degree axes. Our techniques are demonstrated at board level for both receive and transmit modes using an eight-element 8-16 GHz phased array constructed using ADAR1000 chips from ADI. At 6 GHz, CoMET-extracted gain and phase are accurate to within 0.2 dB and 3 degree compared to network-analyzer measurements. We then employ CoMET in a calibration loop to determinate optimum control settings at 6 GHz, outside the 8-16 GHz band for which the array was designed. We achieve seven-bit phase resolution with equalized gain. The root-mean squared gain and phase errors are improved from 0.8 dB and 8 degree before calibration to 0.1 dB and 1.7 degree after calibration.      
### 24.Modelling and Characterisation of Flexibility\\from Distributed Energy Resources  [ :arrow_down: ](https://arxiv.org/pdf/2107.05144.pdf)
>  Harnessing flexibility from distributed energy resources (DER) to participate in various markets while accounting for relevant technical and commercial constraints is essential for the development of low-carbon grids. However, there is no clear definition or even description of the salient features of aggregated DER flexibility, including its steady-state and dynamic features and how these are impacted by network constraints and market requirements. This paper proposes a comprehensive DER flexibility modelling and characterisation framework that is based on the concept of nodal operating envelope (NOE). In particular, capacity, ramp, duration and cost are identified as key flexibility metrics and associated with different but consistent NOEs describing capability, feasibility, ramp, duration, economic, technical and commercial flexibility features. These NOEs, which conceptually arise from a Venn diagram, can be built via optimal power flow (OPF) analysis, visualised in the active-reactive power space, and used by different stakeholders. Results from a canonical test system and a real distribution system illustrate the value and applicability of the proposed framework to model and characterise provision of flexibility and market services from DER for different use cases.      
### 25.Generative adversarial network based single pixel imaging  [ :arrow_down: ](https://arxiv.org/pdf/2107.05135.pdf)
>  Single pixel imaging can reconstruct two-dimensional images of a scene with only a single-pixel detector. It has been widely used for imaging in non-visible bandwidth (e.g., near-infrared and X-ray) where focal-plane array sensors are challenging to be manufactured. In this paper, we propose a generative adversarial network based reconstruction algorithm for single pixel imaging, which demonstrates efficient reconstruction in 10ms and higher quality. We verify the proposed method with both synthetic and real-world experiments, and demonstrate a good quality of reconstruction of a real-world plaster using a 0.05 sampling rate.      
### 26.eGHWT: The extended Generalized Haar-Walsh Transform  [ :arrow_down: ](https://arxiv.org/pdf/2107.05121.pdf)
>  Extending computational harmonic analysis tools from the classical setting of regular lattices to the more general setting of graphs and networks is very important and much research has been done recently. The Generalized Haar-Walsh Transform (GHWT) developed by Irion and Saito (2014) is a multiscale transform for signals on graphs, which is a generalization of the classical Haar and Walsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh Transform (eGHWT), which is a generalization of the adapted time-frequency tilings of Thiele and Villemoes (1996). The eGHWT examines not only the efficiency of graph-domain partitions but also that of "sequency-domain" partitions simultaneously. Consequently, the eGHWT and its associated best-basis selection algorithm for graph signals significantly improve the performance of the previous GHWT with the similar computational cost, $O(N \log N)$, where $N$ is the number of nodes of an input graph. While the GHWT best-basis algorithm seeks the most suitable orthonormal basis for a given task among more than $(1.5)^N$ possible orthonormal bases in $\mathbb{R}^N$, the eGHWT best-basis algorithm can find a better one by searching through more than $0.618\cdot(1.84)^N$ possible orthonormal bases in $\mathbb{R}^N$. This article describes the details of the eGHWT best-basis algorithm and demonstrates its superiority using several examples including genuine graph signals as well as conventional digital images viewed as graph signals. Furthermore, we also show how the eGHWT can be extended to 2D signals and matrix-form data by viewing them as a tensor product of graphs generated from their columns and rows and demonstrate its effectiveness on applications such as image approximation.      
### 27.Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2107.05115.pdf)
>  In spite of the improvements achieved by the several denoising algorithms over the years, many of them still fail at preserving the fine details of the image after denoising. This is as a result of the smooth-out effect they have on the images. Most neural network-based algorithms have achieved better quantitative performance than the classical denoising algorithms. However, they also suffer from qualitative (visual) performance as a result of the smooth-out effect. In this paper, we propose an algorithm to address this shortcoming. We propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image denoising. This algorithm performs collaborative denoising of image patches in the sparse domain using a set of optimized neural network models. This results in a fast algorithm that is able to excellently obtain a trade-off between noise removal and details preservation. Extensive experiments show that the DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and qualitatively (visually) better than many of the state-of-the-art denoising algorithms.      
### 28.Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.05085.pdf)
>  Recent studies have shown that lung cancer screening using annual low-dose computed tomography (CT) reduces lung cancer mortality by 20% compared to traditional chest radiography. Therefore, CT lung screening has started to be used widely all across the world. However, analyzing these images is a serious burden for radiologists. The number of slices in a CT scan can be up to 600. Therefore, computer-aided-detection (CAD) systems are very important for faster and more accurate assessment of the data. In this study, we proposed a framework that analyzes CT lung screenings using convolutional neural networks (CNNs) to reduce false positives. We trained our model with different volume sizes and showed that volume size plays a critical role in the performance of the system. We also used different fusions in order to show their power and effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D convolutional operations applied to 3D data could result in information loss. The proposed framework has been tested on the dataset provided by the LUNA16 Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.      
### 29.T-s3ra: traffic-aware scheduling for secure slicing and resource allocation in sdn/nfv enabled 5g networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.05056.pdf)
>  Network slicing and resource allocation play pivotal roles in software-defined network (SDN)/network function virtualization (NFV)-assisted 5G networks. In 5G communications, the traffic rate is high, necessitating high data rates and low latency. Deep learning is a potential solution for overcoming these constraints. Secure slicing avoids resource wastage; however, DDoS attackers can exploit the sliced network. Therefore, we focused on secure slicing with resource allocation under massive network traffic. Traffic-aware scheduling is proposed for secure slicing and resource allocation over SDN/NFV-enabled 5G networks. In this approach (T-S3RA), user devices are authenticated using Boolean logic with a password-based key derivation function. The traffic is scheduled in 5G access points, and secure network slicing and resource allocation are implemented using deep learning models such as SliceNet and HopFieldNet, respectively. To predict DDoS attackers, we computed the Renyi entropy for packet classification. Experiments were conducted using a network simulator with 250 nodes in the network topology. Performance was evaluated using metrics such as throughput, latency, packet transmission ratio, packet loss ratio, slice capacity, bandwidth consumption, and slice acceptance ratio. T-S3RA was implemented in three 5G use cases with different requirements including massive machine-type communication, ultrareliable low-latency communication, and enhanced mobile broadband.      
### 30.NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.05023.pdf)
>  Automatic polyp segmentation has proven to be immensely helpful for endoscopy procedures, reducing the missing rate of adenoma detection for endoscopists while increasing efficiency. However, classifying a polyp as being neoplasm or not and segmenting it at the pixel level is still a challenging task for doctors to perform in a limited time. In this work, we propose a fine-grained formulation for the polyp segmentation problem. Our formulation aims to not only segment polyp regions, but also identify those at high risk of malignancy with high accuracy. In addition, we present a UNet-based neural network architecture called NeoUNet, along with a hybrid loss function to solve this problem. Experiments show highly competitive results for NeoUNet on our benchmark dataset compared to existing polyp segmentation models.      
### 31.Designing a Robust Frequency Offset Estimation Scheme Satisfying Target Decoder Performance in an OFDM System  [ :arrow_down: ](https://arxiv.org/pdf/2107.05004.pdf)
>  In a target communication system, a delicately designed frequency offset estimation scheme is required to meet certain decoding performance. In this paper, we proposed at wo-step estimation scheme, coarse and residual, with different value of an time interval parameter. A result of RF conduction test shows that the proposed method has an 1dB gain of SNR compared to coarse-only estimator. A result of the commercial test also indicates the proposed method outperforms coarse-only estimator especially in low SNR condition.      
### 32.A Deep-Bayesian Framework for Adaptive Speech Duration Modification  [ :arrow_down: ](https://arxiv.org/pdf/2107.04973.pdf)
>  We propose the first method to adaptively modify the duration of a given speech signal. Our approach uses a Bayesian framework to define a latent attention map that links frames of the input and target utterances. We train a masked convolutional encoder-decoder network to produce this attention map via a stochastic version of the mean absolute error loss function; our model also predicts the length of the target speech signal using the encoder embeddings. The predicted length determines the number of steps for the decoder operation. During inference, we generate the attention map as a proxy for the similarity matrix between the given input speech and an unknown target speech signal. Using this similarity matrix, we compute a warping path of alignment between the two signals. Our experiments demonstrate that this adaptive framework produces similar results to dynamic time warping, which relies on a known target signal, on both voice conversion and emotion conversion tasks. We also show that our technique results in a high quality of generated speech that is on par with state-of-the-art vocoders.      
### 33.Deep Geometric Distillation Network for Compressive Sensing MRI  [ :arrow_down: ](https://arxiv.org/pdf/2107.04943.pdf)
>  Compressed sensing (CS) is an efficient method to reconstruct MR image from small sampled data in $k$-space and accelerate the acquisition of MRI. In this work, we propose a novel deep geometric distillation network which combines the merits of model-based and deep learning-based CS-MRI methods, it can be theoretically guaranteed to improve geometric texture details of a linear reconstruction. Firstly, we unfold the model-based CS-MRI optimization problem into two sub-problems that consist of image linear approximation and image geometric compensation. Secondly, geometric compensation sub-problem for distilling lost texture details in approximation stage can be expanded by Taylor expansion to design a geometric distillation module fusing features of different geometric characteristic domains. Additionally, we use a learnable version with adaptive initialization of the step-length parameter, which allows model more flexibility that can lead to convergent smoothly. Numerical experiments verify its superiority over other state-of-the-art CS-MRI reconstruction approaches. The source code will be available at \url{<a class="link-external link-https" href="https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI" rel="external noopener nofollow">this https URL</a>}      
### 34.TeliNet, a simple and shallow Convolution Neural Network (CNN) to Classify CT Scans of COVID-19 patients  [ :arrow_down: ](https://arxiv.org/pdf/2107.04930.pdf)
>  Hundreds of millions of cases and millions of deaths have occurred worldwide due to COVID-19. The fight against this pandemic is on-going on multiple fronts. While vaccinations are picking up speed, there are still billions of unvaccinated people. In this fight diagnosis of the disease and isolation of the patients to prevent any spreads play a huge role. Machine Learning approaches have assisted the diagnosis of COVID-19 cases by analyzing chest X-ray and CT-scan images of patients. In this research we present a simple and shallow Convolutional Neural Network based approach, TeliNet, to classify CT-scan images of COVID-19 patients. Our results outperform the F1 score of VGGNet and the benchmark approaches. Our proposed solution is also more lightweight in comparison to the other methods.      
### 35.Hierarchical Learning Framework for UAV Detection and Identification  [ :arrow_down: ](https://arxiv.org/pdf/2107.04908.pdf)
>  The ubiquity of unmanned aerial vehicles (UAVs) or drones is posing both security and safety risks to the public as UAVs are now used for cybercrimes. To mitigate these risks, it is important to have a system that can detect or identify the presence of an intruding UAV in a restricted environment. In this work, we propose a radio frequency (RF) based UAV detection and identification system by exploiting signals emanating from both the UAV and its flight controller, respectively. While several RF devices (i.e., Bluetooth and WiFi devices) operate in the same frequency band as UAVs, the proposed framework utilizes a semi-supervised learning approach for the detection of UAV or UAV's control signals in the presence of other wireless signals such as Bluetooth and WiFi. The semi-supervised learning approach uses stacked denoising autoencoder and local outlier factor algorithms. After the detection of UAV or UAV's control signals, the signal is decomposed by using Hilbert-Huang transform and wavelet packet transform to extract features from the time-frequency-energy domain of the signal. The extracted feature sets are used to train a three-level hierarchical classifier for identifying the type of signals (i.e., UAV or UAV control signal), UAV models, and flight mode of UAV. To demonstrate the feasibility of the proposed framework, we carried out an outdoor experiment for data collection using six UAVs, five Bluetooth devices, and two WiFi devices. The acquired data is called Cardinal RF (CardRF) dataset, and it is available for public use to foster UAV detection and identification research.      
### 36.Collaborative Filtering-Based Method for Low-Resolution and Details Preserving Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2107.04865.pdf)
>  Over the years, progressive improvements in denoising performance have been achieved by several image denoising algorithms that have been proposed. Despite this, many of these state-of-the-art algorithms tend to smooth out the denoised image resulting in the loss of some image details after denoising. Many also distort images of lower resolution resulting in a partial or complete structural loss. In this paper, we address these shortcomings by proposing a collaborative filtering-based (CoFiB) denoising algorithm. Our proposed algorithm performs weighted sparse domain collaborative denoising by taking advantage of the fact that similar patches tend to have similar sparse representations in the sparse domain. This gives our algorithm the intelligence to strike a balance between image detail preservation and noise removal. Our extensive experiments showed that our proposed CoFiB algorithm does not only preserve the image details but also perform excellently for images of any given resolution where many denoising algorithms tend to struggle, specifically at low resolutions.      
### 37.Dense-Sparse Deep CNN Training for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2107.04857.pdf)
>  Recently, deep learning (DL) methods such as convolutional neural networks (CNNs) have gained prominence in the area of image denoising. This is owing to their proven ability to surpass state-of-the-art classical image denoising algorithms such as BM3D. Deep denoising CNNs (DnCNNs) use many feedforward convolution layers with added regularization methods of batch normalization and residual learning to improve denoising performance significantly. However, this comes at the expense of a huge number of trainable parameters. In this paper, we address this issue by reducing the number of parameters while achieving a comparable level of performance. We derive motivation from the improved performance obtained by training networks using the dense-sparse-dense (DSD) training approach. We extend this training approach to a reduced DnCNN (RDnCNN) network resulting in a faster denoising network with significantly reduced parameters and comparable performance to the DnCNN.      
### 38.Graphene-based Distributed 3D Sensing Electrodes for Mapping Spatiotemporal Auricular Physiological Signals  [ :arrow_down: ](https://arxiv.org/pdf/2107.04856.pdf)
>  Underneath the ear skin there are richly branching vascular and neural networks that ultimately connecting to our heart and brain. Hence, the three-dimensional (3D) mapping of auricular electrophysiological signals could provide a new perspective for biomedical studies such as diagnosis of cardiovascular diseases and neurological disorders. However, it is still extremely challenging for current sensing techniques to cover the entire ultra-curved auricle. Here, we report a graphene-based ear-conformable sensing device with embedded and distributed 3D electrodes which enable full-auricle physiological monitoring. The sensing device, which incorporates programable 3D electrode thread array and personalized auricular mold, has 3D-conformable sensing interfaces with curved auricular skin, and was developed using one-step multi-material 3D-printing process. As a proof-of-concept, spatiotemporal auricular electrical skin resistance (AESR) mapping was demonstrated. For the first time, 3D AESR contours were generated and human subject-specific AESR distributions among a population were observed. From the data of 17 volunteers, the auricular region-specific AESR changes after cycling exercise were observed in 98% of the tests and were validated via machine learning techniques. Correlations of AESR with heart rate and blood pressure were also studied using statistical analysis. This 3D electronic platform and AESR-based new biometrical findings show promising biomedical applications.      
### 39.Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method for Organs-at-risk Segmentation in Head and Neck CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2107.04847.pdf)
>  In radiotherapy planning, manual contouring is labor-intensive and time-consuming. Accurate and robust automated segmentation models improve the efficiency and treatment outcome. We aim to develop a novel hybrid deep learning approach, combining convolutional neural networks (CNNs) and the self-attention mechanism, for rapid and accurate multi-organ segmentation on head and neck computed tomography (CT) images. Head and neck CT images with manual contours of 115 patients were retrospectively collected and used. We set the training/validation/testing ratio to 81/9/25 and used the 10-fold cross-validation strategy to select the best model parameters. The proposed hybrid model segmented ten organs-at-risk (OARs) altogether for each case. The performance of the model was evaluated by three metrics, i.e., the Dice Similarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface distance (MSD). We also tested the performance of the model on the Head and Neck 2015 challenge dataset and compared it against several state-of-the-art automated segmentation algorithms. The proposed method generated contours that closely resemble the ground truth for ten OARs. Our results of the new Weaving Attention U-net demonstrate superior or similar performance on the segmentation of head and neck CT images.      
### 40.BSDA-Net: A Boundary Shape and Distance Aware Joint Learning Framework for Segmenting and Classifying OCTA Images  [ :arrow_down: ](https://arxiv.org/pdf/2107.04823.pdf)
>  Optical coherence tomography angiography (OCTA) is a novel non-invasive imaging technique that allows visualizations of vasculature and foveal avascular zone (FAZ) across retinal layers. Clinical researches suggest that the morphology and contour irregularity of FAZ are important biomarkers of various ocular pathologies. Therefore, precise segmentation of FAZ has great clinical interest. Also, there is no existing research reporting that FAZ features can improve the performance of deep diagnostic classification networks. In this paper, we propose a novel multi-level boundary shape and distance aware joint learning framework, named BSDA-Net, for FAZ segmentation and diagnostic classification from OCTA images. Two auxiliary branches, namely boundary heatmap regression and signed distance map reconstruction branches, are constructed in addition to the segmentation branch to improve the segmentation performance, resulting in more accurate FAZ contours and fewer outliers. Moreover, both low-level and high-level features from the aforementioned three branches, including shape, size, boundary, and signed directional distance map of FAZ, are fused hierarchically with features from the diagnostic classifier. Through extensive experiments, the proposed BSDA-Net is found to yield state-of-the-art segmentation and classification results on the OCTA-500, OCTAGON, and FAZID datasets.      
### 41.COVID Detection in Chest CTs: Improving the Baseline on COV19-CT-DB  [ :arrow_down: ](https://arxiv.org/pdf/2107.04808.pdf)
>  The paper presents a comparative analysis of three distinct approaches based on deep learning for COVID-19 detection in chest CTs. The first approach is a volumetric one, involving 3D convolutions, while the other two approaches perform at first slice-wise classification and then aggregate the results at the volume level. The experiments are carried on the COV19-CT-DB dataset, with the aim of addressing the challenge raised by the MIA-COV19D Competition within ICCV 2021. Our best results on the validation subset reach a macro-F1 score of 0.92, which improves considerably the baseline score of 0.70 set by the organizers.      
### 42.Using Optimization Algorithms for Control of Multiple Output DC-DC Converters  [ :arrow_down: ](https://arxiv.org/pdf/2107.04778.pdf)
>  The weighted voltage mode control represents a method for control of multiple outputs DC-DC converters. Accordingly, the weighted control redistributes the error among the outputs of these converters, and the regulation error can be reduced by adjusting the weighting factors. But the problem is that most designs are performed on the trial-and-error basis, and the results were rather inconsistent. Also, in conventional mathematical approaches, this factor is designed for converters by given parameters. In this paper, three optimization algorithms namely Imperialist Competitive Algorithm (ICA), Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are proposed for a quick and accurate estimation of the optimal weighting factors and improve the amount of regulation on outputs of multiple outputs forward DC-DC converters. Furthermore, Fuzzy Logic Controller (FLC) is utilized to minimize the total steady-state error and improve transient characteristics by controlling the duty cycle of the PWM controller. Simulations have been performed in several cases and results show that the proposed method improves the outputs cross regulations in multiple outputs forward DC-DC converters significantly. ICA based weighting factor estimator has higher speed and accuracy in comparison with two other presented algorithms (ACO and PSO) and so is more effective in comparison with them.      
### 43.Secure Dual-Functional Radar-Communication Transmission: Exploiting Interference for Resilience Against Target Eavesdropping  [ :arrow_down: ](https://arxiv.org/pdf/2107.04747.pdf)
>  We study security solutions for dual-functional radar communication (DFRC) systems, which detect the radar target and communicate with downlink cellular users in millimeter-wave (mmWave) wireless networks simultaneously. Uniquely for such scenarios, the radar target is regarded as a potential eavesdropper which might surveil the information sent from the base station (BS) to communication users (CUs), that is carried by the radar probing signal. Transmit waveform and receive beamforming are jointly designed to maximize the signal-to-interference-plus-noise ratio (SINR) of the radar under the security and power budget constraints. We apply a Directional Modulation (DM) approach to exploit constructive interference (CI), where the known multiuser interference (MUI) can be exploited as a source of useful signal. Moreover, to further deteriorate the eavesdropping signal at the radar target, we utilize destructive interference (DI) by pushing the received symbols at the target towards the destructive region of the signal constellation. Our numerical results verify the effectiveness of the proposed design showing a secure transmission with enhanced performance against benchmark DFRC techniques.      
### 44.U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina  [ :arrow_down: ](https://arxiv.org/pdf/2107.04721.pdf)
>  Fundus photography has routinely been used to document the presence and severity of retinal degenerative diseases such as age-related macular degeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical practice, for which the fovea and optic disc (OD) are important retinal landmarks. However, the occurrence of lesions, drusen, and other retinal abnormalities during retinal degeneration severely complicates automatic landmark detection and segmentation. Here we propose HBA-U-Net: a U-Net backbone enriched with hierarchical bottleneck attention. The network consists of a novel bottleneck attention block that combines and refines self-attention, channel attention, and relative-position attention to highlight retinal abnormalities that may be important for fovea and OD segmentation in the degenerated retina. HBA-U-Net achieved state-of-the-art results on fovea detection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of 25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for AMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD: ED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for landmark detection in the presence of a variety of retinal degenerative diseases.      
### 45.Global sensitivity analysis of (a)symmetric energy harvesters  [ :arrow_down: ](https://arxiv.org/pdf/2107.04647.pdf)
>  Parametric variability is inevitable in actual energy harvesters and can define crucial aspects of the system performance, especially in susceptible systems to small perturbations. In this way, this work aims to identify the most critical parameters in the dynamics of (a)symmetric bistable energy harvesters with nonlinear piezoelectric coupling, considering the variability of their physical and excitation parameters. For this purpose, a global sensitivity analysis based on the Sobol' indices is performed by an orthogonal decomposition in terms of conditional variances to access the dependence of the recovered power concerning the harvester parameters. This technique quantifies the variance concerning each parameter individually and jointly regarding the total variation of the model. The results indicate that the frequency and amplitude of excitation, asymmetric bias angle, and piezoelectric coupling at the electrical domain are the most influential parameters that affect the mean power harvested. It has also been shown that the order of importance of the parameters can change from stable conditions. In possession of this, a better understanding of the system under analysis is obtained, identifying vital parameters that rule the change of dynamic behavior and constituting a powerful tool in the robust design and prediction of nonlinear harvesters.      
### 46.UNIPOL: Unimodular sequence design via a separable iterative quartic polynomial optimization for active sensing systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.04610.pdf)
>  Sequences having better autocorrelation properties play a crucial role in enhancing the performance of active sensing systems. Hence, sequences with good autocorrelation properties are very much in demand. In this paper, we addressed the problem of designing a unimodular sequence having better side-lobe levels. We formulated it as a constrained optimization problem comprising the Integrated Side-lobe Level (ISL) metric and then proposed an effective algorithm (named UNIPOL - UNImodular sequence design via a separable iterative POLynomial optimization) where we perform the polynomial optimization at every iteration. The novelty of the paper comes from deriving a quartic majorization function that is separable in the sequence variables and that can be minimized parallelly. To evaluate the performance of our proposed algorithm we perform the numerical experiments for different sequence lengths and confirm that our proposed algorithm is the fastest algorithm to attain an actual optimum minimizer of the ISL metric. Our proposed algorithm is also computationally efficient due to its ease of implementation using the FFT, IFFT operations in a parallel fashion.      
### 47.UAV Control Optimization via Decentralized Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2107.04593.pdf)
>  Unmanned aerial vehicle (UAV) swarm control has applications including target tracking, surveillance, terrain mapping, and precision agriculture. Decentralized control methods are particularly useful when the swarm is large, as centralized methods (a single command center controlling the UAVs) suffer from exponential computational complexity, i.e., the computing time to obtain the optimal control for the UAVs grow exponentially with the number of UAVs in the swarm in centralized approaches. Although many centralized control methods exist, literature lacks decentralized control frameworks with broad applicability. To address this knowledge gap, we present a novel decentralized UAV swarm control strategy using a decision-theoretic framework called decentralized Markov decision process (Dec-MDP). We build these control strategies in the context of two case studies: a) swarm formation control problem; b) swarm control for multitarget tracking. As most decision theoretic formulations suffer from the curse of dimensionality, we adapt an approximate dynamic programming method called nominal belief-state optimization (NBO) to solve the decentralized control problems approximately in both the case studies. In the formation control case study, the objective is to drive the swarm from a geographical region to another geographical region where the swarm must form a certain geometrical shape (e.g., selected location on the surface of a sphere). The motivation for studying such problems comes from data fusion applications with UAV swarms where the fusion performance depends on the strategic relative separation of the UAVs from each other. In the target tracking case study, the objective is the control the motion of the UAVs in a decentralized manner while maximizing the overall target tracking performance. Motivation for this case study comes from the surveillance applications using UAV swarms.      
### 48.Hierarchical Neural Dynamic Policies  [ :arrow_down: ](https://arxiv.org/pdf/2107.05627.pdf)
>  We tackle the problem of generalization to unseen configurations for dynamic tasks in the real world while learning from high-dimensional image input. The family of nonlinear dynamical system-based methods have successfully demonstrated dynamic robot behaviors but have difficulty in generalizing to unseen configurations as well as learning from image inputs. Recent works approach this issue by using deep network policies and reparameterize actions to embed the structure of dynamical systems but still struggle in domains with diverse configurations of image goals, and hence, find it difficult to generalize. In this paper, we address this dichotomy by leveraging embedding the structure of dynamical systems in a hierarchical deep policy learning framework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of fitting deep dynamical systems to diverse data directly, H-NDPs form a curriculum by learning local dynamical system-based policies on small regions in state-space and then distill them into a global dynamical system-based policy that operates only from high-dimensional images. H-NDPs additionally provide smooth trajectories, a strong safety benefit in the real world. We perform extensive experiments on dynamic tasks both in the real world (digit writing, scooping, and pouring) and simulation (catching, throwing, picking). We show that H-NDPs are easily integrated with both imitation as well as reinforcement learning setups and achieve state-of-the-art results. Video results are at <a class="link-external link-https" href="https://shikharbahl.github.io/hierarchical-ndps/" rel="external noopener nofollow">this https URL</a>      
### 49.Linear Contact-Implicit Model-Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2107.05616.pdf)
>  We present a general approach for controlling robotic systems that make and break contact with their environments: linear contact-implicit model-predictive control (LCI-MPC). Our use of differentiable contact dynamics provides a natural extension of linear model-predictive control to contact-rich settings. The policy leverages precomputed linearizations about a reference state or trajectory while contact modes, encoded via complementarity constraints, are explicitly retained, resulting in policies that can be efficiently evaluated while maintaining robustness to changes in contact timings. In many cases, the algorithm is even capable of generating entirely new contact sequences. To enable real-time performance, we devise a custom structure-exploiting linear solver for the contact dynamics. We demonstrate that the policy can respond to disturbances by discovering and exploiting new contact modes and is robust to model mismatch and unmodeled environments for a collection of simulated robotic systems, including: pushbot, hopper, quadruped, and biped.      
### 50.Direct speech-to-speech translation with discrete units  [ :arrow_down: ](https://arxiv.org/pdf/2107.05604.pdf)
>  We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. Previous work addresses the problem by training an attention-based sequence-to-sequence model that maps source speech spectrograms into target spectrograms. To tackle the challenge of modeling continuous spectrogram features of the target speech, we propose to predict the self-supervised discrete representations learned from an unlabeled speech corpus instead. When target text transcripts are available, we design a multitask learning framework with joint speech and text training that enables the model to generate dual mode output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that predicting discrete units and joint speech and text training improve model performance by 11 BLEU compared with a baseline that predicts spectrograms and bridges 83% of the performance gap towards a cascaded system. When trained without any text transcripts, our model achieves similar performance as a baseline that predicts spectrograms and is trained with text data.      
### 51.Impact of Scene-Specific Enhancement Spectra on Matched Filter Greenhouse Gas Retrievals from Imaging Spectroscopy  [ :arrow_down: ](https://arxiv.org/pdf/2107.05578.pdf)
>  Matched filter (MF) techniques have been widely used for retrieval of greenhouse gas enhancements (enh.) from imaging spectroscopy datasets. While multiple algorithmic techniques and refinements have been proposed, the greenhouse gas target spectrum used for concentration enh. estimation has remained largely unaltered since the introduction of quantitative MF retrievals. The magnitude of retrieved methane and carbon dioxide enh., and thereby integrated mass enh. (IME) and estimated flux of point-source emitters, is heavily dependent on this target spectrum. Current standard use of molecular absorption coefficients to create unit enh. target spectra does not account for absorption by background concentrations of greenhouse gases, solar and sensor geometry, or atmospheric water vapor absorption. We introduce geometric and atmospheric parameters into the generation of scene-specific (SS) unit enh. spectra to provide target spectra that are compatible with all greenhouse gas retrieval MF techniques. For methane plumes, IME resulting from use of standard, generic enh. spectra varied from -22 to +28.7% compared to SS enh. spectra. Due to differences in spectral shape between the generic and SS enh. spectra, differences in methane plume IME were linked to surface spectral characteristics in addition to geometric and atmospheric parameters. IME differences for carbon dioxide plumes, with generic enh. spectra producing integrated mass enh. -76.1 to -48.1% compared to SS enh. spectra. Fluxes calculated from these integrated enh. would vary by the same %s, assuming equivalent wind conditions. Methane and carbon dioxide IME were most sensitive to changes in solar zenith angle and ground elevation. SS target spectra can improve confidence in greenhouse gas retrievals and flux estimates across collections of scenes with diverse geometric and atmospheric conditions.      
### 52.LATTE: LSTM Self-Attention based Anomaly Detection in Embedded Automotive Platforms  [ :arrow_down: ](https://arxiv.org/pdf/2107.05561.pdf)
>  Modern vehicles can be thought of as complex distributed embedded systems that run a variety of automotive applications with real-time constraints. Recent advances in the automotive industry towards greater autonomy are driving vehicles to be increasingly connected with various external systems (e.g., roadside beacons, other vehicles), which makes emerging vehicles highly vulnerable to cyber-attacks. Additionally, the increased complexity of automotive applications and the in-vehicle networks results in poor attack visibility, which makes detecting such attacks particularly challenging in automotive systems. In this work, we present a novel anomaly detection framework called LATTE to detect cyber-attacks in Controller Area Network (CAN) based networks within automotive platforms. Our proposed LATTE framework uses a stacked Long Short Term Memory (LSTM) predictor network with novel attention mechanisms to learn the normal operating behavior at design time. Subsequently, a novel detection scheme (also trained at design time) is used to detect various cyber-attacks (as anomalies) at runtime. We evaluate our proposed LATTE framework under different automotive attack scenarios and present a detailed comparison with the best-known prior works in this area, to demonstrate the potential of our approach.      
### 53.Multi-modality Deep Restoration of Extremely Compressed Face Videos  [ :arrow_down: ](https://arxiv.org/pdf/2107.05548.pdf)
>  Arguably the most common and salient object in daily video communications is the talking head, as encountered in social media, virtual classrooms, teleconferences, news broadcasting, talk shows, etc. When communication bandwidth is limited by network congestions or cost effectiveness, compression artifacts in talking head videos are inevitable. The resulting video quality degradation is highly visible and objectionable due to high acuity of human visual system to faces. To solve this problem, we develop a multi-modality deep convolutional neural network method for restoring face videos that are aggressively compressed. The main innovation is a new DCNN architecture that incorporates known priors of multiple modalities: the video-synchronized speech signal and semantic elements of the compression code stream, including motion vectors, code partition map and quantization parameters. These priors strongly correlate with the latent video and hence they are able to enhance the capability of deep learning to remove compression artifacts. Ample empirical evidences are presented to validate the superior performance of the proposed DCNN method on face videos over the existing state-of-the-art methods.      
### 54.Calliope -- A Polyphonic Music Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2107.05546.pdf)
>  The polyphonic nature of music makes the application of deep learning to music modelling a challenging task. On the other hand, the Transformer architecture seems to be a good fit for this kind of data. In this work, we present Calliope, a novel autoencoder model based on Transformers for the efficient modelling of multi-track sequences of polyphonic music. The experiments show that our model is able to improve the state of the art on musical sequence reconstruction and generation, with remarkably good results especially on long sequences.      
### 55.Reconfigurable Intelligent Surface-Aided MISO Systems with Statistical CSI: Channel Estimation, Analysis and Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2107.05512.pdf)
>  This paper investigates the reconfigurable reflecting surface (RIS)-aided multiple-input-single-output (MISO) systems with imperfect channel state information (CSI), where RIS-related channels are modeled by Rician fading. Considering the overhead and complexity in practical systems, we employ the low-complexity maximum ratio combining (MRC) beamforming at the base station (BS), and configure the phase shifts of the RIS based on long-term statistical CSI. Specifically, we first estimate the overall channel matrix based on the linear minimum mean square error (LMMSE) estimator, and evaluate the performance of MSE and normalized MSE (NMSE). Then, with the estimated channel, we derive the closed-form expressions of the ergodic rate. The derived expressions show that with Rician RIS-related channels, the rate can maintain at a non-zero value when the transmit power is scaled down proportionally to $1/M$ or $1/N^2$, where $M$ and $N$ are the number of antennas and reflecting elements, respectively. However, if all the RIS-related channels are fully Rayleigh, the transmit power of each user can only be scaled down proportionally to $1/\sqrt{M}$ or $1/N$. Finally, numerical results verify the promising benefits from the RIS to traditional MISO systems.      
### 56.Time Series Analysis of Computer Network Traffic in a Dedicated Link Aggregation  [ :arrow_down: ](https://arxiv.org/pdf/2107.05484.pdf)
>  Fractal behavior and long-range dependence are widely observed in measurements and characterization of traffic flow in high-speed computer networks of different technologies and coverage levels. This paper presents the results obtained when applying fractal analysis techniques on a time series obtained from traffic captures coming from an application server connected to the Internet through a high-speed link. The results obtained show that traffic flow in the dedicated high-speed network link have fractal behavior when the Hurst exponent is in the range of 0.5, 1, the fractal dimension between 1, 1.5, and the correlation coefficient between -0.5, 0. Based on these results, it is ideal to characterize both the singularities of the traffic and its impulsiveness during a fractal analysis of temporal scales. Finally, based on the results of the time series analyses, the fact that the traffic flows of current computer networks exhibit fractal behavior with a long-range dependency is reaffirmed.      
### 57.Anatomy-Constrained Contrastive Learning for Synthetic Segmentation without Ground-truth  [ :arrow_down: ](https://arxiv.org/pdf/2107.05482.pdf)
>  A large amount of manual segmentation is typically required to train a robust segmentation network so that it can segment objects of interest in a new imaging modality. The manual efforts can be alleviated if the manual segmentation in one imaging modality (e.g., CT) can be utilized to train a segmentation network in another imaging modality (e.g., CBCT/MRI/PET). In this work, we developed an anatomy-constrained contrastive synthetic segmentation network (AccSeg-Net) to train a segmentation network for a target imaging modality without using its ground truth. Specifically, we proposed to use anatomy-constraint and patch contrastive learning to ensure the anatomy fidelity during the unsupervised adaptation, such that the segmentation network can be trained on the adapted image with correct anatomical structure/content. The training data for our AccSeg-Net consists of 1) imaging data paired with segmentation ground-truth in source modality, and 2) unpaired source and target modality imaging data. We demonstrated successful applications on CBCT, MRI, and PET imaging data, and showed superior segmentation performances as compared to previous methods.      
### 58.GPTPU: Accelerating Applications using Edge Tensor Processing Units  [ :arrow_down: ](https://arxiv.org/pdf/2107.05473.pdf)
>  Neural network (NN) accelerators have been integrated into a wide-spectrum of computer systems to accommodate the rapidly growing demands for artificial intelligence (AI) and machine learning (ML) applications. NN accelerators share the idea of providing native hardware support for operations on multidimensional tensor data. Therefore, NN accelerators are theoretically tensor processors that can improve system performance for any problem that uses tensors as inputs/outputs. Unfortunately, commercially available NN accelerators only expose computation capabilities through AI/ML-specific interfaces. Furthermore, NN accelerators reveal very few hardware design details, so applications cannot easily leverage the tensor operations NN accelerators provide. <br>This paper introduces General-Purpose Computing on Edge Tensor Processing Units (GPTPU), an open-source, open-architecture framework that allows the developer and research communities to discover opportunities that NN accelerators enable for applications. GPTPU includes a powerful programming interface with efficient runtime system-level support -- similar to that of CUDA/OpenCL in GPGPU computing -- to bridge the gap between application demands and mismatched hardware/software interfaces. <br>We built GPTPU machine uses Edge Tensor Processing Units (Edge TPUs), which are widely available and representative of many commercial NN accelerators. We identified several novel use cases and revisited the algorithms. By leveraging the underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our results reveal that GPTPU can achieve a 2.46x speedup over high-end CPUs and reduce energy consumption by 40%.      
### 59.Learning and Adaptation in Millimeter-Wave: a Dual Timescale Variational Framework  [ :arrow_down: ](https://arxiv.org/pdf/2107.05466.pdf)
>  Millimeter-wave vehicular networks incur enormous beam-training overhead to enable narrow-beam communications. This paper proposes a learning and adaptation framework in which the dynamics of the communication beams are learned and then exploited to design adaptive beam-training with low overhead: on a long-timescale, a deep recurrent variational autoencoder (DR-VAE) uses noisy beam-training observations to learn a probabilistic model of beam dynamics; on a short-timescale, an adaptive beam-training procedure is formulated as a partially observable (PO-) Markov decision process (MDP) and optimized via point-based value iteration (PBVI) by leveraging beam-training feedback and a probabilistic prediction of the strongest beam pair provided by the DR-VAE. In turn, beam-training observations are used to refine the DR-VAE via stochastic gradient ascent in a continuous process of learning and adaptation. The proposed DR-VAE mobility learning framework learns accurate beam dynamics: it reduces the Kullback-Leibler divergence between the ground truth and the learned beam dynamics model by 86% over the Baum-Welch algorithm and by 92\% over a naive mobility learning approach that neglects feedback errors. The proposed dual-timescale approach yields a negligible loss of spectral efficiency compared to a genie-aided scheme operating under error-free feedback and foreknown mobility model. Finally, a low-complexity policy is proposed by reducing the POMDP to an error-robust MDP. It is shown that the PBVI- and error-robust MDP-based policies improve the spectral efficiency by 85% and 67%, respectively, over a policy that scans exhaustively over the dominant beam pairs, and by 16% and 7%, respectively, over a state-of-the-art POMDP policy.      
### 60.IGrow: A Smart Agriculture Solution to Autonomous Greenhouse Control  [ :arrow_down: ](https://arxiv.org/pdf/2107.05464.pdf)
>  Agriculture is the foundation of human civilization. However, the rapid increase and aging of the global population pose challenges on this cornerstone by demanding more healthy and fresh food. Internet of Things (IoT) technology makes modern autonomous greenhouse a viable and reliable engine of food production. However, the educated and skilled labor capable of overseeing high-tech greenhouses is scarce. Artificial intelligence (AI) and cloud computing technologies are promising solutions for precision control and high-efficiency production in such controlled environments. In this paper, we propose a smart agriculture solution, namely iGrow: (1) we use IoT and cloud computing technologies to measure, collect, and manage growing data, to support iteration of our decision-making AI module, which consists of an incremental model and an optimization algorithm; (2) we propose a three-stage incremental model based on accumulating data, enabling growers/central computers to schedule control strategies conveniently and at low cost; (3) we propose a model-based iterative optimization algorithm, which can dynamically optimize the greenhouse control strategy in real-time production. In the simulated experiment, evaluation results show the accuracy of our incremental model is comparable to an advanced tomato simulator, while our optimization algorithms can beat the champion of the 2nd Autonomous Greenhouse Challenge. Compelling results from the A/B test in real greenhouses demonstrate that our solution significantly increases production (commercially sellable fruits) (+ 10.15%) and net profit (+ 87.07%) with statistical significance compared to planting experts.      
### 61.Reconfigurable Intelligent Surfaces: Potentials, Applications, and Challenges for 6G Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.05460.pdf)
>  Reconfigurable intelligent surfaces (RISs), with the potential to realize a smart radio environment, have emerged as an energy-efficient and a cost-effective technology to support the services and demands foreseen for coming decades. By leveraging a large number of low-cost passive reflecting elements, RISs introduce a phase-shift in the impinging signal to create a favorable propagation channel between the transmitter and the receiver.~\textcolor{black}{In this article, we provide a tutorial overview of RISs for sixth-generation (6G) wireless networks. Specifically, we present a comprehensive discussion on performance gains that can be achieved by integrating RISs with emerging communication technologies. We address the practical implementation of RIS-assisted networks and expose the crucial challenges, including the RIS reconfiguration, deployment and size optimization, and channel estimation. Furthermore, we explore the integration of RIS and non-orthogonal multiple access (NOMA) under imperfect channel state information (CSI). Our numerical results illustrate the importance of better channel estimation in RIS-assisted networks and indicate the various factors that impact the size of RIS. Finally, we present promising future research directions for realizing RIS-assisted networks in 6G communication.      
### 62.DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2107.05429.pdf)
>  The dual-path RNN (DPRNN) was proposed to more effectively model extremely long sequences for speech separation in the time domain. By splitting long sequences to smaller chunks and applying intra-chunk and inter-chunk RNNs, the DPRNN reached promising performance in speech separation with a limited model size. In this paper, we combine the DPRNN module with Convolution Recurrent Network (CRN) and design a model called Dual-Path Convolution Recurrent Network (DPCRN) for speech enhancement in the time-frequency domain. We replace the RNNs in the CRN with DPRNN modules, where the intra-chunk RNNs are used to model the spectrum pattern in a single frame and the inter-chunk RNNs are used to model the dependence between consecutive frames. With only 0.8M parameters, the submitted DPCRN model achieves an overall mean opinion score (MOS) of 3.57 in the wide band scenario track of the Interspeech 2021 Deep Noise Suppression (DNS) challenge. Evaluations on some other test sets also show the efficacy of our model.      
### 63.End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.05382.pdf)
>  We propose a semi-supervised learning method for building end-to-end rich transcription-style automatic speech recognition (RT-ASR) systems from small-scale rich transcription-style and large-scale common transcription-style datasets. In spontaneous speech tasks, various speech phenomena such as fillers, word fragments, laughter and coughs, etc. are often included. While common transcriptions do not give special awareness to these phenomena, rich transcriptions explicitly convert them into special phenomenon tokens as well as textual tokens. In previous studies, the textual and phenomenon tokens were simultaneously estimated in an end-to-end manner. However, it is difficult to build accurate RT-ASR systems because large-scale rich transcription-style datasets are often unavailable. To solve this problem, our training method uses a limited rich transcription-style dataset and common transcription-style dataset simultaneously. The Key process in our semi-supervised learning is to convert the common transcription-style dataset into a pseudo-rich transcription-style dataset. To this end, we introduce style tokens which control phenomenon tokens are generated or not into transformer-based autoregressive modeling. We use this modeling for generating the pseudo-rich transcription-style datasets and for building RT-ASR system from the pseudo and original datasets. Our experiments on spontaneous ASR tasks showed the effectiveness of the proposed method.      
### 64.Measuring scientific output of researchers by t-index and Data Envelopment Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2107.05376.pdf)
>  There is a growing need for ranking universities, departments, research groups, and individual scholars. Usually, the scientific community measures the scientific merits of the researchers by using a variety of indicators that take into account both the productivity of scholars and the impact of their publications. We propose the t-index, the new indicator to measure the scientific merits of the individual researchers. The proposed t-index takes into account the number of citations, number of coauthors on every published paper, and career duration. The t-index makes the possible comparison of researchers at various stages of their careers. We also use in this paper the Data Envelopment Analysis (DEA) to measure the scientific merits of the individual researchers within the observed group of researchers. We chose 15 scholars in the scientific area of transportation engineering and measured their t-index values, as well as DEA scores.      
### 65.Real-Time Super-Resolution System of 4K-Video Based on Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.05307.pdf)
>  Video super-resolution (VSR) technology excels in reconstructing low-quality video, avoiding unpleasant blur effect caused by interpolation-based algorithms. However, vast computation complexity and memory occupation hampers the edge of deplorability and the runtime inference in real-life applications, especially for large-scale VSR task. This paper explores the possibility of real-time VSR system and designs an efficient and generic VSR network, termed EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for temporal coherence. In order to pursue faster VSR processing ability up to 4K resolution, this paper tries to choose lightweight network structure and efficient upsampling method to reduce the computation required by EGVSR network under the guarantee of high visual quality. Besides, we implement the batch normalization computation fusion, convolutional acceleration algorithm and other neural network acceleration techniques on the actual hardware platform to optimize the inference process of EGVSR network. Finally, our EGVSR achieves the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the most advanced VSR network at present, we achieve 85.04% reduction of computation density and 7.92x performance speedups. In terms of visual quality, the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP, etc.) on the public test dataset Vid4 and surpasses other state-of-the-art methods in overall performance score. The source code of this project can be found on <a class="link-external link-https" href="https://github.com/Thmen/EGVSR" rel="external noopener nofollow">this https URL</a>.      
### 66.Dihedral multi-reference alignment  [ :arrow_down: ](https://arxiv.org/pdf/2107.05262.pdf)
>  We study the dihedral multi-reference alignment problem of estimating the orbit of a signal from multiple noisy observations of the signal, translated by random elements of the dihedral group. We show that if the group elements are drawn from a generic distribution, the orbit of a generic signal is uniquely determined from the second moment of the observations. This implies that the optimal estimation rate in the high noise regime is proportional to the square of the variance of the noise. This is the first result of this type for multi-reference alignment over a non-abelian group with a non-uniform distribution of group elements. Based on tools from invariant theory and algebraic geometry, we also delineate conditions for unique orbit recovery for multi-reference alignment models over finite groups (namely, when the dihedral group is replaced by a general finite group) when the group elements are drawn from a generic distribution. Finally, we design and study numerically three computational frameworks for estimating the signal based on group synchronization, expectation-maximization, and the method of moments.      
### 67.AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes  [ :arrow_down: ](https://arxiv.org/pdf/2107.05255.pdf)
>  During pregnancy, ultrasound examination in the second trimester can assess fetal size according to standardized charts. To achieve a reproducible and accurate measurement, a sonographer needs to identify three standard 2D planes of the fetal anatomy (head, abdomen, femur) and manually mark the key anatomical landmarks on the image for accurate biometry and fetal weight estimation. This can be a time-consuming operator-dependent task, especially for a trainee sonographer. Computer-assisted techniques can help in automating the fetal biometry computation process. In this paper, we present a unified automated framework for estimating all measurements needed for the fetal weight assessment. The proposed framework semantically segments the key fetal anatomies using state-of-the-art segmentation models, followed by region fitting and scale recovery for the biometry estimation. We present an ablation study of segmentation algorithms to show their robustness through 4-fold cross-validation on a dataset of 349 ultrasound standard plane images from 42 pregnancies. Moreover, we show that the network with the best segmentation performance tends to be more accurate for biometry estimation. Furthermore, we demonstrate that the error between clinically measured and predicted fetal biometry is lower than the permissible error during routine clinical measurements.      
### 68.MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2107.05223.pdf)
>  This paper presents an attempt to employ the mask language modeling approach of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of polyphonic piano MIDI files for tackling a number of symbolic-domain discriminative music understanding tasks. These include two note-level classification tasks, i.e., melody extraction and velocity prediction, as well as two sequence-level classification tasks, i.e., composer classification and emotion classification. We find that, given a pre-trained Transformer, our models outperform recurrent neural network based baselines with less than 10 epochs of fine-tuning. Ablation studies show that the pre-training remains effective even if none of the MIDI data of the downstream tasks are seen at the pre-training stage, and that freezing the self-attention layers of the Transformer at the fine-tuning stage slightly degrades performance. All the five datasets employed in this work are publicly available, as well as checkpoints of our pre-trained and fine-tuned models. As such, our research can be taken as a benchmark for symbolic-domain music understanding.      
### 69.TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.05188.pdf)
>  In recent years, computer-aided diagnosis has become an increasingly popular topic. Methods based on convolutional neural networks have achieved good performance in medical image segmentation and classification. Due to the limitations of the convolution operation, the long-term spatial features are often not accurately obtained. Hence, we propose a TransClaw U-Net network structure, which combines the convolution operation with the transformer operation in the encoding part. The convolution part is applied for extracting the shallow spatial features to facilitate the recovery of the image resolution after upsampling. The transformer part is used to encode the patches, and the self-attention mechanism is used to obtain global information between sequences. The decoding part retains the bottom upsampling structure for better detail segmentation performance. The experimental results on Synapse Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net is better than other network structures. The ablation experiments also prove the generalization performance of TransClaw U-Net.      
### 70.AoI-minimizing Scheduling in UAV-relayed IoT Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.05181.pdf)
>  Due to flexibility, autonomy and low operational cost, unmanned aerial vehicles (UAVs), as fixed aerial base stations, are increasingly being used as \textit{relays} to collect time-sensitive information (i.e., status updates) from IoT devices and deliver it to the nearby terrestrial base station (TBS), where the information gets processed. In order to ensure timely delivery of information to the TBS (from all IoT devices), optimal scheduling of time-sensitive information over two hop UAV-relayed IoT networks (i.e., IoT device to the UAV [hop 1], and UAV to the TBS [hop 2]) becomes a critical challenge. To address this, we propose scheduling policies for Age of Information (AoI) minimization in such two-hop UAV-relayed IoT networks. To this end, we present a low-complexity MAF-MAD scheduler, that employs Maximum AoI First (MAF) policy for sampling of IoT devices at UAV (hop 1) and Maximum AoI Difference (MAD) policy for updating sampled packets from UAV to the TBS (hop 2). We show that MAF-MAD is the optimal scheduler under ideal conditions, i.e., error-free channels and generate-at-will traffic generation at IoT devices. On the contrary, for realistic conditions, we propose a Deep-Q-Networks (DQN) based scheduler. Our simulation results show that DQN-based scheduler outperforms MAF-MAD scheduler and three other baseline schedulers, i.e., Maximal AoI First (MAF), Round Robin (RR) and Random, employed at both hops under general conditions when the network is small (with 10's of IoT devices). However, it does not scale well with network size whereas MAF-MAD outperforms all other schedulers under all considered scenarios for larger networks.      
### 71.An Exact Sequential Linear Programming Algorithm for the Optimal Power Flow Problem  [ :arrow_down: ](https://arxiv.org/pdf/2107.05164.pdf)
>  Despite major advancements in nonlinear programming (NLP) and convex relaxations, most system operators around the world still predominantly use some form of linear programming (LP) approximation of the AC power flow equations. This is largely due to LP technology's superior reliability and computational efficiency, especially in real-time market applications, security-constrained applications, and extensions involving integer variables, in addition to its ability to readily generate locational marginal prices (LMP) for market applications. In the aim of leveraging the advantages of LP while retaining the accuracy of NLP interior-point methods (IPMs), this paper proposes a sequential linear programming (SLP) approach consisting of a sequence of carefully constructed supporting hyperplanes and halfspaces. The algorithm is numerically demonstrated to converge on 138 test cases with up the 3375 buses to feasible high-quality solutions (i) without AC feasibility restoration (i.e., using LP solvers exclusively), (ii) in computation times generally within the same order of magnitude as those from a state-of-the-art NLP solver, and (iii) with robustness against the choice of starting point. In particular, the (relative) optimality gaps and the mean constraint violations are on average around 1e-3% and 1e-7, respectively, under a single parameter setting for all the 138 test cases. To the best of our knowledge, the proposed SLP approach is the first to use LP exclusively to reach feasible and high-quality solutions to the nonconvex AC OPF in a reliable way, which paves the way for system and market operators to keep using their LP solvers but now with the ability to accurately capture transmission losses, price reactive power (Q-LMP), and obtain more accurate LMP.      
### 72.Spatial and Temporal Networks for Facial Expression Recognition in the Wild Videos  [ :arrow_down: ](https://arxiv.org/pdf/2107.05160.pdf)
>  The paper describes our proposed methodology for the seven basic expression classification track of Affective Behavior Analysis in-the-wild (ABAW) Competition 2021. In this task, facial expression recognition (FER) methods aim to classify the correct expression category from a diverse background, but there are several challenges. First, to adapt the model to in-the-wild scenarios, we use the knowledge from pre-trained large-scale face recognition data. Second, we propose an ensemble model with a convolution neural network (CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer (CNN-Transformer), to incorporate both spatial and temporal information. Our ensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as 0.4821 on the validation set.      
### 73.Open-Loop Equilibrium Strategies for Dynamic Influence Maximization Game Over Social Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.05138.pdf)
>  We consider the problem of budget allocation for competitive influence maximization over social networks. In this problem, multiple competing parties (players) want to distribute their limited advertising resources over a set of social individuals to maximize their long-run cumulative payoffs. It is assumed that the individuals are connected via a social network and update their opinions based on the classical DeGroot model. The players must decide the budget distribution among the individuals at a finite number of campaign times to maximize their overall payoff given as a function of individuals' opinions. We show that i) the optimal investment strategy for the case of a single-player can be found in polynomial time by solving a concave program, and ii) the open-loop equilibrium strategies for the multiplayer dynamic game can be computed efficiently by following natural regret minimization dynamics. Our results extend the earlier work on the static version of the problem to a dynamic multistage game.      
### 74.BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2107.05097.pdf)
>  Interpretable brain network models for disease prediction are of great value for the advancement of neuroscience. GNNs are promising to model complicated network data, but they are prone to overfitting and suffer from poor interpretability, which prevents their usage in decision-critical scenarios like healthcare. To bridge this gap, we propose BrainNNExplainer, an interpretable GNN framework for brain network analysis. It is mainly composed of two jointly learned modules: a backbone prediction model that is specifically designed for brain networks and an explanation generator that highlights disease-specific prominent brain network connections. Extensive experimental results with visualizations on two challenging disease prediction datasets demonstrate the unique interpretability and outstanding performance of BrainNNExplainer.      
### 75.Ambrosia: Reduction in Data Transfer from Sensor to Server for Increased Lifetime of IoT Sensor Nodes  [ :arrow_down: ](https://arxiv.org/pdf/2107.05090.pdf)
>  Data transmission accounts for significant energy consumption in wireless sensor networks where streaming data is generatedby the sensors. This impedes their use in many settings, including livestock monitoring over large pastures (which formsour target application). We present Ambrosia, a lightweight protocol that utilizes a window-based timeseries forecastingmechanism for data reduction. Ambrosia employs a configurable error threshold to ensure that the accuracy of end applicationsis unaffected by the data transfer reduction. Experimental evaluations using LoRa and BLE on a real livestock monitoringdeployment demonstrate 60% reduction in data transmission and a 2X increase in battery lifetime.      
### 76.Remote Blood Oxygen Estimation From Videos Using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.05087.pdf)
>  Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory functionality and is receiving increasing attention during the COVID-19 pandemic. Clinical findings show that it is possible for COVID-19 patients to have significantly low SpO$_2$ before any obvious symptoms. The prevalence of cameras has motivated researchers to investigate methods for monitoring SpO$_2$ using videos. Most prior schemes involving smartphones are contact-based: They require a fingertip to cover the phone's camera and the nearby light source to capture re-emitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO$_2$ estimation scheme using smartphone cameras. The scheme analyzes the videos of a participant's hand for physiological sensing, which is convenient and comfortable, and can protect their privacy and allow for keeping face masks on. We design our neural network architectures inspired by the optophysiological models for SpO$_2$ measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO$_2$ measurement, showing the potential of our proposed method to contribute to public health. We also analyze the impact of skin type and the side of a hand on SpO$_2$ estimation performance.      
### 77.A Cloud-Edge-Terminal Collaborative System for Temperature Measurement in COVID-19 Prevention  [ :arrow_down: ](https://arxiv.org/pdf/2107.05078.pdf)
>  To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary temperature measurement and mask detection in public areas are conducted. However, the existing temperature measurement methods face the problems of safety and deployment. In this paper, to realize safe and accurate temperature measurement even when a person's face is partially obscured, we propose a cloud-edge-terminal collaborative system with a lightweight infrared temperature measurement model. A binocular camera with an RGB lens and a thermal lens is utilized to simultaneously capture image pairs. Then, a mobile detection model based on a multi-task cascaded convolutional network (MTCNN) is proposed to realize face alignment and mask detection on the RGB images. For accurate temperature measurement, we transform the facial landmarks on the RGB images to the thermal images by an affine transformation and select a more accurate temperature measurement area on the forehead. The collected information is uploaded to the cloud in real time for COVID-19 prevention. Experiments show that the detection model is only 6.1M and the average detection speed is 257ms. At a distance of 1m, the error of indoor temperature measurement is about 3%. That is, the proposed system can realize real-time temperature measurement in public areas.      
### 78.Neural Waveshaping Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2107.05050.pdf)
>  We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.      
### 79.Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2107.05038.pdf)
>  The use of phonological features (PFs) potentially allows language-specific phones to remain linked in training, which is highly desirable for information sharing for multilingual and crosslingual speech recognition methods for low-resourced languages. A drawback suffered by previous methods in using phonological features is that the acoustic-to-PF extraction in a bottom-up way is itself difficult. In this paper, we propose to join phonology driven phone embedding (top-down) and deep neural network (DNN) based acoustic feature extraction (bottom-up) to calculate phone probabilities. The new method is called JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion from acoustics to phonological features is required for speech recognition. For each phone in the IPA (International Phonetic Alphabet) table, we encode its phonological features to a phonological-vector, and then apply linear or nonlinear transformation of the phonological-vector to obtain the phone embedding. A series of multilingual and crosslingual (both zero-shot and few-shot) speech recognition experiments are conducted on the CommonVoice dataset (German, French, Spanish and Italian) and the AISHLL-1 dataset (Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone embeddings over both JoinAP with linear phone embeddings and the traditional method with flat phone embeddings.      
### 80.Dual Optimization for Kolmogorov Model Learning Using Enhanced Gradient Descent  [ :arrow_down: ](https://arxiv.org/pdf/2107.05011.pdf)
>  Data representation techniques have made a substantial contribution to advancing data processing and machine learning (ML). Improving predictive power was the focus of previous representation techniques, which unfortunately perform rather poorly on the interpretability in terms of extracting underlying insights of the data. Recently, Kolmogorov model (KM) was studied, which is an interpretable and predictable representation approach to learning the underlying probabilistic structure of a set of random variables. The existing KM learning algorithms using semi-definite relaxation with randomization (SDRwR) or discrete monotonic optimization (DMO) have, however, limited utility to big data applications because they do not scale well computationally. In this paper, we propose a computationally scalable KM learning algorithm, based on the regularized dual optimization combined with enhanced gradient descent (GD) method. To make our method more scalable to large-dimensional problems, we propose two acceleration schemes, namely, eigenvalue decomposition (EVD) elimination strategy and proximal EVD algorithm. Furthermore, a thresholding technique by exploiting the approximation error analysis and leveraging the normalized Minkowski $\ell_1$-norm and its bounds, is provided for the selection of the number of iterations of the proximal EVD algorithm. When applied to big data applications, it is demonstrated that the proposed method can achieve compatible training/prediction performance with significantly reduced computational complexity; roughly two orders of magnitude improvement in terms of the time overhead, compared to the existing KM learning algorithms. Furthermore, it is shown that the accuracy of logical relation mining for interpretability by using the proposed KM learning algorithm exceeds $80\%$.      
### 81.PocketVAE: A Two-step Model for Groove Generation and Control  [ :arrow_down: ](https://arxiv.org/pdf/2107.05009.pdf)
>  Creating a good drum track to imitate a skilled performer in digital audio workstations (DAWs) can be a time-consuming process, especially for those unfamiliar with drums. In this work, we introduce PocketVAE, a groove generation system that applies grooves to users' rudimentary MIDI tracks, i.e, templates. Grooves can be either transferred from a reference track, generated randomly or with conditions, such as genres. Our system, consisting of different modules for each groove component, takes a two-step approach that is analogous to a music creation process. First, the note module updates the user template through addition and deletion of notes; Second, the velocity and microtiming modules add details to this generated note score. In order to model the drum notes, we apply a discrete latent representation method via Vector Quantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete property, unlike velocity and microtiming values. We show that our two-step approach and the usage of a discrete encoding space improves the learning of the original data distribution. Additionally, we discuss the benefit of incorporating control elements - genre, velocity and microtiming patterns - into the model.      
### 82.Stabilizing Neural Control Using Self-Learned Almost Lyapunov Critics  [ :arrow_down: ](https://arxiv.org/pdf/2107.04989.pdf)
>  The lack of stability guarantee restricts the practical use of learning-based methods in core control problems in robotics. We develop new methods for learning neural control policies and neural Lyapunov critic functions in the model-free reinforcement learning (RL) setting. We use sample-based approaches and the Almost Lyapunov function conditions to estimate the region of attraction and invariance properties through the learned Lyapunov critic functions. The methods enhance stability of neural controllers for various nonlinear systems including automobile and quadrotor control.      
### 83.Theoretical Limit of Radar Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04986.pdf)
>  In the field of radar parameter estimation, Cramer-Rao bound (CRB) is a commonly used theoretical limit. However, CRB is only achievable under high signal-to-noise (SNR) and does not adequately characterize performance in low and medium SNRs. In this paper, we employ the thoughts and methodologies of Shannon's information theory to study the theoretical limit of radar parameter estimation. Based on the posteriori probability density function of targets' parameters, joint range-scattering information and entropy error (EE) are defined to evaluate the performance. The closed-form approximation of EE is derived, which indicates that EE degenerates to the CRB in the high SNR region. For radar ranging, it is proved that the range information and the entropy error can be achieved by the sampling a posterior probability estimator, whose performance is entirely determined by the theoretical posteriori probability density function of the radar parameter estimation system. The range information and the entropy error are simulated with sampling a posterior probability estimator, where they are shown to outperform the CRB as they can be achieved under all SNR conditions      
### 84.ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data  [ :arrow_down: ](https://arxiv.org/pdf/2107.04954.pdf)
>  Most of the current supervised automatic music transcription (AMT) models lack the ability to generalize. This means that they have trouble transcribing real-world music recordings from diverse musical genres that are not presented in the labelled training data. In this paper, we propose a semi-supervised framework, ReconVAT, which solves this issue by leveraging the huge amount of available unlabelled music recordings. The proposed ReconVAT uses reconstruction loss and virtual adversarial training. When combined with existing U-net models for AMT, ReconVAT achieves competitive results on common benchmark datasets such as MAPS and MusicNet. For example, in the few-shot setting for the string part version of MusicNet, ReconVAT achieves F1-scores of 61.0% and 41.6% for the note-wise and note-with-offset-wise metrics respectively, which translates into an improvement of 22.2% and 62.5% compared to the supervised baseline model. Our proposed framework also demonstrates the potential of continual learning on new data, which could be useful in real-world applications whereby new data is constantly available.      
### 85.Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.04914.pdf)
>  Domain Adaptation (DA) methods are widely used in medical image segmentation tasks to tackle the problem of differently distributed train (source) and test (target) data. We consider the supervised DA task with a limited number of annotated samples from the target domain. It corresponds to one of the most relevant clinical setups: building a sufficiently accurate model on the minimum possible amount of annotated data. Existing methods mostly fine-tune specific layers of the pretrained Convolutional Neural Network (CNN). However, there is no consensus on which layers are better to fine-tune, e.g. the first layers for images with low-level domain shift or the deeper layers for images with high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture that automatically chooses the layers which should be optimally fine-tuned. More specifically, on the target domain, our method additionally learns the policy that indicates whether a specific layer should be fine-tuned or reused from the pretrained network. We show that our method performs at the same level as the best of the nonflexible fine-tuning methods even under the extreme scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a layer-wise visualization of the domain shift impact on the network, which could be further used to develop robust domain generalization methods. In order to extensively evaluate SpotTUnet performance, we use a publicly available dataset of brain MR images (CC359), characterized by explicit domain shift. We release a reproducible experimental pipeline.      
### 86.Learning-to-Dispatch: Reinforcement Learning Based Flight Planning under Emergency  [ :arrow_down: ](https://arxiv.org/pdf/2107.04897.pdf)
>  The effectiveness of resource allocation under emergencies especially hurricane disasters is crucial. However, most researchers focus on emergency resource allocation in a ground transportation system. In this paper, we propose Learning-to-Dispatch (L2D), a reinforcement learning (RL) based air route dispatching system, that aims to add additional flights for hurricane evacuation while minimizing the airspace's complexity and air traffic controller's workload. Given a bipartite graph with weights that are learned from the historical flight data using RL in consideration of short- and long-term gains, we formulate the flight dispatch as an online maximum weight matching problem. Different from the conventional order dispatch problem, there is no actual or estimated index that can evaluate how the additional evacuation flights influence the air traffic complexity. Then we propose a multivariate reward function in the learning phase and compare it with other univariate reward designs to show its superior performance. The experiments using the real-world dataset for Hurricane Irma demonstrate the efficacy and efficiency of our proposed schema.      
### 87.Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2107.04882.pdf)
>  Deep learning models have become a popular choice for medical image analysis. However, the poor generalization performance of deep learning models limits them from being deployed in the real world as robustness is critical for medical applications. For instance, the state-of-the-art Convolutional Neural Networks (CNNs) fail to detect adversarial samples or samples drawn statistically far away from the training distribution. In this work, we experimentally evaluate the robustness of a Mahalanobis distance-based confidence score, a simple yet effective method for detecting abnormal input samples, in classifying malaria parasitized cells and uninfected cells. Results indicated that the Mahalanobis confidence score detector exhibits improved performance and robustness of deep learning models, and achieves stateof-the-art performance on both out-of-distribution (OOD) and adversarial samples.      
### 88.Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution  [ :arrow_down: ](https://arxiv.org/pdf/2107.04878.pdf)
>  It is easier to hear birds than see them, however, they still play an essential role in nature and they are excellent indicators of deteriorating environmental quality and pollution. Recent advances in Machine Learning and Convolutional Neural Networks allow us to detect and classify bird sounds, by doing this, we can assist researchers in monitoring the status and trends of bird populations and biodiversity in ecosystems. We propose a sound detection and classification pipeline for analyzing complex soundscape recordings and identify birdcalls in the background. Our pipeline learns from weak labels, classifies fine-grained bird vocalizations in the wild, and is robust against background sounds (e.g., airplanes, rain, etc). Our solution achieved 10th place of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.      
### 89.Multi-Material Topology Optimization with Continuous Magnetization Direction for Permanent Magnet Synchronous Reluctance Motors  [ :arrow_down: ](https://arxiv.org/pdf/2107.04825.pdf)
>  Permanent magnet-assisted synchronous reluctance motors (PMSynRM) have a significantly higher average torque than synchronous reluctance motors. Thus, determining an optimal design results in a multi-material topology optimization problem, where one seeks to distribute ferromagnetic material, air and permanent magnets within the rotor in an optimal manner. This study proposed a novel density-based distribution scheme, which allows for continuous magnetization direction instead of a finite set of angles. Thus, an interpolation scheme is established between properties pertaining to magnets and non-linear materials. This allows for new designs to emerge without introducing complex geometric parameterization or relying on the user's biases and intuitions. Toward reducing computation time, Nitsche-type mortaring is applied, allowing for free rotation of the rotor mesh relative to the stator mesh. The average torque is approximated using only four-point static positions. This study investigates several interpolation schemes and presents a new one inspired by the topological derivative. We propose to filter the final design for the magnetization angle using K-mean clustering accounting for technical feasibility constraints of magnets. Finally, the design of the electrical motor is proposed to maximize torque value.      
### 90.Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique  [ :arrow_down: ](https://arxiv.org/pdf/2107.04813.pdf)
>  Plant leaf diseases pose a significant danger to food security and they cause depletion in quality and volume of production. Therefore accurate and timely detection of leaf disease is very important to check the loss of the crops and meet the growing food demand of the people. Conventional techniques depend on lab investigation and human skills which are generally costly and inaccessible. Recently, Deep Neural Networks have been exceptionally fruitful in image classification. In this research paper, plant leaf disease detection employing transfer learning is explored in the JPEG compressed domain. Here, the JPEG compressed stream consisting of DCT coefficients is, directly fed into the Neural Network to improve the efficiency of classification. The experimental results on JPEG compressed leaf dataset demonstrate the efficacy of the proposed model.      
### 91.Variational Information Bottleneck for Effective Low-resource Audio Classification  [ :arrow_down: ](https://arxiv.org/pdf/2107.04803.pdf)
>  Large-scale deep neural networks (DNNs) such as convolutional neural networks (CNNs) have achieved impressive performance in audio classification for their powerful capacity and strong generalization ability. However, when training a DNN model on low-resource tasks, it is usually prone to overfitting the small data and learning too much redundant information. To address this issue, we propose to use variational information bottleneck (VIB) to mitigate overfitting and suppress irrelevant information. In this work, we conduct experiments ona 4-layer CNN. However, the VIB framework is ready-to-use and could be easily utilized with many other state-of-the-art network architectures. Evaluation on a few audio datasets shows that our approach significantly outperforms baseline methods, yielding more than 5.0% improvement in terms of classification accuracy in some low-source settings.      
### 92.Stable Recovery of Weighted Sparse Signals from Phaseless Measurements via Weighted l1 Minimization  [ :arrow_down: ](https://arxiv.org/pdf/2107.04788.pdf)
>  The goal of phaseless compressed sensing is to recover an unknown sparse or approximately sparse signal from the magnitude of its measurements. However, it does not take advantage of any support information of the original signal. Therefore, our main contribution in this paper is to extend the theoretical framework for phaseless compressed sensing to incorporate with prior knowledge of the support structure of the signal. Specifically, we investigate two conditions that guarantee stable recovery of a weighted $k$-sparse signal via weighted l1 minimization without any phase information. We first prove that the weighted null space property (WNSP) is a sufficient and necessary condition for the success of weighted l1 minimization for weighted k-sparse phase retrievable. Moreover, we show that if a measurement matrix satisfies the strong weighted restricted isometry property (SWRIP), then the original signal can be stably recovered from the phaseless measurements.      
### 93.Layer-wise Analysis of a Self-supervised Speech Representation Model  [ :arrow_down: ](https://arxiv.org/pdf/2107.04734.pdf)
>  Recently proposed self-supervised learning approaches have been successful for pre-training speech representation models. The utility of these learned representations has been observed empirically, but not much has been studied about the type or extent of information encoded in the pre-trained representations themselves. Developing such insights can help understand the capabilities and limits of these models and enable the research community to more efficiently develop their usage for downstream applications. In this work, we begin to fill this gap by examining one recent and successful pre-trained model (wav2vec 2.0), via its intermediate representation vectors, using a suite of analysis tools. We use the metrics of canonical correlation, mutual information, and performance on simple downstream tasks with non-parametric probes, in order to (i) query for acoustic and linguistic information content, (ii) characterize the evolution of information across model layers, and (iii) understand how fine-tuning the model for automatic speech recognition (ASR) affects these observations. Our findings motivate modifying the fine-tuning protocol for ASR, which produces improved word error rates in a low-resource setting.      
### 94.Attitude Reconstruction from Inertial Measurement: Mitigating Runge Effect for Dynamic Applications  [ :arrow_down: ](https://arxiv.org/pdf/2107.04722.pdf)
>  Time-equispaced inertial measurements are practically used as inputs for motion determination. Polynomial interpolation is a common technique of recovering the gyroscope signal but is subject to a fundamentally numerical stability problem due to the Runge effect on equispaced samples. This paper reviews the theoretical results of Runge phenomenon in related areas and proposes a straightforward borrowing-and-cutting (BAC) strategy to depress it. It employs the neighboring samples for higher-order polynomial interpolation but only uses the middle polynomial segment in the actual time interval. The BAC strategy has been incorporated into attitude computation by functional iteration, leading to accuracy benefit of several orders of magnitude under the classical coning motion. It would potentially bring significant benefits to the inertial navigation computation under sustained dynamic motions.      
### 95.Antenna Array Thinning Through Quantum Computing  [ :arrow_down: ](https://arxiv.org/pdf/2107.04684.pdf)
>  Thinning antenna arrays through quantum Fourier transform (QFT) is proposed. Given the lattice of the candidate locations for the array elements, the problem of selecting which antenna location has to be either occupied or not by an array element is formulated in the quantum computing (QC) framework and then addressed with an ad-hoc design method based on a suitable implementation of the QFT algorithm. Representative numerical results are presented and discussed to point out the features and the advantages of the proposed QC-based thinning technique.      
### 96.Stiffness Mitigation in Stochastic Particle Flow Filters  [ :arrow_down: ](https://arxiv.org/pdf/2107.04672.pdf)
>  The linear convex log-homotopy has been used in the derivation of particle flow filters. One natural question is whether it is beneficial to consider other forms of homotopy. We revisit this question by considering a general linear form of log-homotopy for which we derive particle flow filters, validate the distribution of flows, and obtain conditions for the stability of particle flows. We then formulate the problem of stiffness mitigation as an optimal control problem by minimizing the condition number of the Hessian matrix of the posterior density function. The optimal homotopy can be efficiently obtained by solving a one-dimensional second order two-point boundary value problem. Compared with traditional matrix analysis based approaches to condition number improvements such as scaling, this novel approach explicitly exploits the special structure of the stochastic differential equations in particle flow filters. The effectiveness of the proposed approach is demonstrated by a numerical example.      
### 97.Toward Decentralized Interior Point Methods for Control  [ :arrow_down: ](https://arxiv.org/pdf/2107.04664.pdf)
>  Distributed and decentralized optimization are key for the control of network systems -- for example in distributed model predictive control, and in distributed sensing or estimation. Non-linear systems, however, lead to problems with non-convex constraints for which classical decentralized optimization algorithms lack convergence guarantees. Moreover, classical decentralized algorithms usually exhibit only linear convergence. This paper presents a decentralized optimization algorithm based on primal-dual interior point methods, which is based on neighbor-to-neighbor communication. We prove local convergence for non-convex problems at a superlinear rate. We show that the method works reliably on a medium-scale numerical example from power systems. Our results indicate that the proposed method outperforms ADMM in terms of computational complexity.      
