# ArXiv eess --Fri, 9 Jul 2021
### 1.Atlas-Based Segmentation of Intracochlear Anatomy in Metal Artifact Affected CT Images of the Ear with Co-trained Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.03987.pdf)
>  We propose an atlas-based method to segment the intracochlear anatomy (ICA) in the post-implantation CT (Post-CT) images of cochlear implant (CI) recipients that preserves the point-to-point correspondence between the meshes in the atlas and the segmented volumes. To solve this problem, which is challenging because of the strong artifacts produced by the implant, we use a pair of co-trained deep networks that generate dense deformation fields (DDFs) in opposite directions. One network is tasked with registering an atlas image to the Post-CT images and the other network is tasked with registering the Post-CT images to the atlas image. The networks are trained using loss functions based on voxel-wise labels, image content, fiducial registration error, and cycle-consistency constraint. The segmentation of the ICA in the Post-CT images is subsequently obtained by transferring the predefined segmentation meshes of the ICA in the atlas image to the Post-CT images using the corresponding DDFs generated by the trained registration networks. Our model can learn the underlying geometric features of the ICA even though they are obscured by the metal artifacts. We show that our end-to-end network produces results that are comparable to the current state of the art (SOTA) that relies on a two-steps approach that first uses conditional generative adversarial networks to synthesize artifact-free images from the Post-CT images and then uses an active shape model-based method to segment the ICA in the synthetic images. Our method requires a fraction of the time needed by the SOTA, which is important for end-user acceptance.      
### 2.Comparing Supervised Models And Learned Speech Representations For Classifying Intelligibility Of Disordered Speech On Selected Phrases  [ :arrow_down: ](https://arxiv.org/pdf/2107.03985.pdf)
>  Automatic classification of disordered speech can provide an objective tool for identifying the presence and severity of speech impairment. Classification approaches can also help identify hard-to-recognize speech samples to teach ASR systems about the variable manifestations of impaired speech. Here, we develop and compare different deep learning techniques to classify the intelligibility of disordered speech on selected phrases. We collected samples from a diverse set of 661 speakers with a variety of self-reported disorders speaking 29 words or phrases, which were rated by speech-language pathologists for their overall intelligibility using a five-point Likert scale. We then evaluated classifiers developed using 3 approaches: (1) a convolutional neural network (CNN) trained for the task, (2) classifiers trained on non-semantic speech representations from CNNs that used an unsupervised objective [1], and (3) classifiers trained on the acoustic (encoder) embeddings from an ASR system trained on typical speech [2]. We found that the ASR encoder's embeddings considerably outperform the other two on detecting and classifying disordered speech. Further analysis shows that the ASR embeddings cluster speech by the spoken phrase, while the non-semantic embeddings cluster speech by speaker. Also, longer phrases are more indicative of intelligibility deficits than single words.      
### 3.Cascaded Complementary Filter Architecture for Sensor Fusion in Attitude Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03970.pdf)
>  Attitude estimation is the process of computing the orientation angles of an object with respect to a fixed frame of reference. Gyroscope, accelerometer, and magnetometer are some of the fundamental sensors used in attitude estimation. The orientation angles computed from these sensors are combined using the sensor fusion methodologies to obtain accurate estimates. The complementary filter is one of the widely adopted techniques whose performance is highly dependent on the appropriate selection of its gain parameters. This paper presents a novel cascaded architecture of the complementary filter that employs a nonlinear and linear version of the complementary filter within one framework. The nonlinear version is used to correct the gyroscope bias, while the linear version estimates the attitude angle. The significant advantage of the proposed architecture is its independence of the filter parameters, thereby avoiding tuning the filters gain parameters. The proposed architecture does not require any mathematical modeling of the system and is computationally inexpensive. The proposed methodology is applied to the real-world datasets, and the estimation results were found to be promising compared to the other state-of-the-art algorithms.      
### 4.Cyber Insurance Against Cyberattacks on Electric Vehicle Charging Stations  [ :arrow_down: ](https://arxiv.org/pdf/2107.03954.pdf)
>  Even with state-of-the-art defense mechanisms, cyberattacks in the electric power distribution sector are commonplace. Particularly alarming are load-altering (demand-side) cyberattacks launched through high-wattage assets, which are not continuously monitored by electric power utilities. Electric Vehicle Charging Stations (EVCSs) are among such high-wattage assets and, therefore, cyber insurance can be an effective mechanism to protect EVCSs from economic losses caused by cyberattacks. This paper presents a data-driven cyber insurance design model for public EVCSs. Under some mildly restrictive assumptions, we derive an optimal cyber insurance premium. Then, we robustify this optimal premium against uncertainty in data and investigate the risk of insuring the EVCSs using Conditional Value-at-Risk. A case study with data from EVCSs in Manhattan, New York illustrates our results.      
### 5.A hybrid deep learning framework for Covid-19 detection via 3D Chest CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2107.03904.pdf)
>  In this paper, we present a hybrid deep learning framework named CTNet which combines convolutional neural network and transformer together for the detection of COVID-19 via 3D chest CT images. It consists of a CNN feature extractor module with SE attention to extract sufficient features from CT scans, together with a transformer model to model the discriminative features of the 3D CT scans. Compared to previous works, CTNet provides an effective and efficient method to perform COVID-19 diagnosis via 3D CT scans with data resampling strategy. Advanced results on a large and public benchmarks, COV19-CT-DB database was achieved by the proposed CTNet, over the state-of-the-art baseline approachproposed together with the dataset.      
### 6.Federated Learning for Multi-Center Imaging Diagnostics: A Study in Cardiovascular Disease  [ :arrow_down: ](https://arxiv.org/pdf/2107.03901.pdf)
>  Deep learning models can enable accurate and efficient disease diagnosis, but have thus far been hampered by the data scarcity present in the medical world. Automated diagnosis studies have been constrained by underpowered single-center datasets, and although some results have shown promise, their generalizability to other institutions remains questionable as the data heterogeneity between institutions is not taken into account. By allowing models to be trained in a distributed manner that preserves patients' privacy, federated learning promises to alleviate these issues, by enabling diligent multi-center studies. We present the first federated learning study on the modality of cardiovascular magnetic resonance (CMR) and use four centers derived from subsets of the M\&amp;M and ACDC datasets, focusing on the diagnosis of hypertrophic cardiomyopathy (HCM). We adapt a 3D-CNN network pretrained on action recognition and explore two different ways of incorporating shape prior information to the model, and four different data augmentation set-ups, systematically analyzing their impact on the different collaborative learning choices. We show that despite the small size of data (180 subjects derived from four centers), the privacy preserving federated learning achieves promising results that are competitive with traditional centralized learning. We further find that federatively trained models exhibit increased robustness and are more sensitive to domain shift effects.      
### 7.Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03887.pdf)
>  In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution segmentation of the heart is essential for detailed description of its anatomical structures. However, due to the limit of acquisition duration and respiratory/cardiac motion, stacks of multi-slice 2D images are acquired in clinical routine. The segmentation of these images provides a low-resolution representation of cardiac anatomy, which may contain artefacts caused by motion. Here we propose a novel latent optimisation framework that jointly performs motion correction and super resolution for cardiac image segmentations. Given a low-resolution segmentation as input, the framework accounts for inter-slice motion in cardiac MR imaging and super-resolves the input into a high-resolution segmentation consistent with input. A multi-view loss is incorporated to leverage information from both short-axis view and long-axis view of cardiac imaging. To solve the inverse problem, iterative optimisation is performed in a latent space, which ensures the anatomical plausibility. This alleviates the need of paired low-resolution and high-resolution images for supervised learning. Experiments on two cardiac MR datasets show that the proposed framework achieves high performance, comparable to state-of-the-art super-resolution approaches and with better cross-domain generalisability and anatomical plausibility.      
### 8.Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03846.pdf)
>  Deep neural networks have increased the accuracy of automatic segmentation, however, their accuracy depends on the availability of a large number of fully segmented images. Methods to train deep neural networks using images for which some, but not all, regions of interest are segmented are necessary to make better use of partially annotated datasets. In this paper, we propose the first axiomatic definition of label-set loss functions that are the loss functions that can handle partially segmented images. We prove that there is one and only one method to convert a classical loss function for fully segmented images into a proper label-set loss function. Our theory also allows us to define the leaf-Dice loss, a label-set generalization of the Dice loss particularly suited for partial supervision with only missing labels. Using the leaf-Dice loss, we set a new state of the art in partially supervised learning for fetal brain 3D MRI segmentation. We achieve a deep neural network able to segment white matter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter, deep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of anatomically normal fetuses or with open spina bifida. Our implementation of the proposed label-set loss functions is available at <a class="link-external link-https" href="https://github.com/LucasFidon/label-set-loss-functions" rel="external noopener nofollow">this https URL</a>      
### 9.Automated Gain Control Through Deep Reinforcement Learning for Downstream Radar Object Detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.03792.pdf)
>  Cognitive radars are systems that rely on learning through interactions of the radar with the surrounding environment. To realize this, radar transmit parameters can be adapted such that they facilitate some downstream task. This paper proposes the use of deep reinforcement learning (RL) to learn policies for gain control under the object detection task. The YOLOv3 single-shot object detector is used for the downstream task and will be concurrently used alongside the RL agent. Furthermore, a synthetic dataset is introduced which models the radar environment with use of the Grand Theft Auto V game engine. This approach allows for simulation of vast amounts of data with flexible assignment of the radar parameters to aid in the active learning process.      
### 10.Single Pole-To-Earth Fault Detection and Location on the Tehran Railway System Using ICA and PSO Trained Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2107.03791.pdf)
>  In a railroad feeding system, detecting a location of pole to earth faults is important for safe operation of the system. The goal of this paper is to use a combination of the evolutionary algorithm and neural networks to increase the accuracy of single pole-to-earth fault detection and location on Tehran railroad power supply system. Accordingly, Imperialist Competitive Algorithm (ICA) and Particle Swarm Optimization (PSO) are used to train the neural network for enhancing learning process accuracy and the convergence. Owing to the nonlinearity of system, the fault detection is an ideal application for the proposed method where 600 Hz harmonic ripple method is used in this paper for fault detection. The substations were simulated by considering various situations in feeding the circuit, the transformer and the silicon rectifier has been developed by typical Tehran metro parameters. Required data for the network learning the process have been gathered from simulation results. 600Hz components value will change with the change of the location of single pole to earth fault. Therefore, 600Hz components are used as inputs of the neural network when fault location is the output of the network system. The simulation results show that the fault location can be accurately predicted in proposed methods.      
### 11.Derivative Based Proportionate Approach for Sparse Impulse Response Identification  [ :arrow_down: ](https://arxiv.org/pdf/2107.03777.pdf)
>  Proportionate type algorithms were developed and excessively used in the echo cancellation problems due to sparse characteristics of the echo channels. In the past, most of the attention was paid to a particular type of proportionate approach, which assigns step-sizes to filter coefficients proportional to the magnitude of the corresponding coefficient. In this letter, we propose a new proportionate type algorithm, which takes dynamic behavior of the estimated filter coefficient into account while assigning individual step-sizes to each coefficient. Proposed algorithm introduces an effective way to assign individual step-sizes using the time derivatives of the filter coefficients. Computational complexity of the proposed algorithm is similar to those of previously proposed algorithms. Simulation results have shown the improvements in the convergence rate achieved by the proposed algorithm.      
### 12.A Comparison of Data-Driven Techniques for Power Grid Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03762.pdf)
>  Power grid parameter estimation involves the estimation of unknown parameters, such as inertia and damping coefficients, using observed dynamics. In this work, we present a comparison of data-driven algorithms for the power grid parameter estimation problem. First, we propose a new algorithm to solve the parameter estimation problem based on the Sparse Identification of Nonlinear Dynamics (SINDy) approach, which uses linear regression to infer the parameters that best describe the observed data. We then compare its performance against two benchmark algorithms, namely, the unscented Kalman filter (UKF) approach and the physics-informed neural networks (PINN) approach. We perform extensive simulations on IEEE bus systems to examine the performance of the aforementioned algorithms. Our results show that the SINDy algorithm outperforms the PINN and UKF algorithms in being able to accurately estimate the power grid parameters over a wide range of system parameters (including high and low inertia systems). Moreover, it is extremely efficient computationally and so takes significantly less time than the PINN algorithm, thus making it suitable for real-time parameter estimation.      
### 13.Expressive Voice Conversion: A Joint Framework for Speaker Identity and Emotional Style Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2107.03748.pdf)
>  Traditional voice conversion(VC) has been focused on speaker identity conversion for speech with a neutral expression. We note that emotional expression plays an essential role in daily communication, and the emotional style of speech can be speaker-dependent. In this paper, we study the technique to jointly convert the speaker identity and speaker-dependent emotional style, that is called expressive voice conversion. We propose a StarGAN-based framework to learn a many-to-many mapping across different speakers, that takes into account speaker-dependent emotional style without the need for parallel data. To achieve this, we condition the generator on emotional style encoding derived from a pre-trained speech emotion recognition(SER) model. The experiments validate the effectiveness of our proposed framework in both objective and subjective evaluations. To our best knowledge, this is the first study on expressive voice conversion.      
### 14.Diffraction Tomography with Helmholtz Equation: Efficient and Robust Multigrid-Based Solver  [ :arrow_down: ](https://arxiv.org/pdf/2107.03679.pdf)
>  Diffraction tomography is a noninvasive technique that estimates the refractive indices of unknown objects and involves an inverse-scattering problem governed by the wave equation. Recent works have shown the benefit of nonlinear models of wave propagation that account for multiple scattering and reflections. In particular, the Lippmann-Schwinger~(LiS) model defines an inverse problem to simulate the wave propagation. Although accurate, this model is hard to solve when the samples are highly contrasted or have a large physical size. In this work, we introduce instead a Helmholtz-based nonlinear model for inverse scattering. To solve the corresponding inverse problem, we propose a robust and efficient multigrid-based solver. Moreover, we show that our method is a suitable alternative to the LiS model, especially for strongly scattering objects. Numerical experiments on simulated and real data demonstrate the effectiveness of the Helmholtz model, as well as the efficiency of the proposed multigrid method.      
### 15.Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?  [ :arrow_down: ](https://arxiv.org/pdf/2107.03651.pdf)
>  To explore the clinical validity of elastic deformation of optical coherence tomography (OCT) images for data augmentation in the development of deep-learning model for detection of diabetic macular edema (DME).      
### 16.Heavily Augmented Sound Event Detection utilizing Weak Predictions  [ :arrow_down: ](https://arxiv.org/pdf/2107.03649.pdf)
>  The performances of Sound Event Detection (SED) systems are greatly limited by the difficulty in generating large strongly labeled dataset. In this work, we used two main approaches to overcome the lack of strongly labeled data. First, we applied heavy data augmentation on input features. Data augmentation methods used include not only conventional methods used in speech/audio domains but also our proposed method named FilterAugment. Second, we propose two methods to utilize weak predictions to enhance weakly supervised SED performance. As a result, we obtained the best PSDS1 of 0.4336 and best PSDS2 of 0.8161 on the DESED real validation dataset. This work is submitted to DCASE 2021 Task4 and is ranked on the 3rd place.      
### 17.Deep Learning Based Image Retrieval in the JPEG Compressed Domain  [ :arrow_down: ](https://arxiv.org/pdf/2107.03648.pdf)
>  Content-based image retrieval (CBIR) systems on pixel domain use low-level features, such as colour, texture and shape, to retrieve images. In this context, two types of image representations i.e. local and global image features have been studied in the literature. Extracting these features from pixel images and comparing them with images from the database is very time-consuming. Therefore, in recent years, there has been some effort to accomplish image analysis directly in the compressed domain with lesser computations. Furthermore, most of the images in our daily transactions are stored in the JPEG compressed format. Therefore, it would be ideal if we could retrieve features directly from the partially decoded or compressed data and use them for retrieval. Here, we propose a unified model for image retrieval which takes DCT coefficients as input and efficiently extracts global and local features directly in the JPEG compressed domain for accurate image retrieval. The experimental findings indicate that our proposed model performed similarly to the current DELG model which takes RGB features as an input with reference to mean average precision while having a faster training and retrieval speed.      
### 18.A hybrid virtual sensing approach for approximating non-linear dynamic system behavior using LSTM networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.03645.pdf)
>  Modern Internet of Things solutions are used in a variety of different areas, ranging from connected vehicles and healthcare to industrial applications. They rely on a large amount of interconnected sensors, which can lead to both technical and economical challenges. Virtual sensing techniques aim to reduce the number of physical sensors in a system by using data from available measurements to estimate additional unknown quantities of interest. Successful model-based solutions include Kalman filters or the combination of finite element models and modal analysis, while many data-driven methods rely on machine learning algorithms. The presented hybrid virtual sensing approach combines Long Short-Term Memory networks with frequency response function models in order to estimate the behavior of non-linear dynamic systems with multiple input and output channels. Network training and prediction make use of short signal subsequences, which are later recombined by applying a windowing technique. The frequency response function model acts as a baseline estimate which perfectly captures linear dynamic systems and is augmented by the non-linear Long Short-Term Memory network following two different hybrid modeling strategies. The approach is tested using a non-linear experimental dataset, which results from measurements of a three-component servo-hydraulic fatigue test bench. A variety of metrics in time and frequency domains, as well as fatigue strength under variable amplitudes are used to evaluate the approximation quality of the proposed method. In addition to virtual sensing, the algorithm is also applied to a forward prediction task. Synthetic data are used in a separate study to estimate the prediction quality on datasets of different size.      
### 19.Regional Differential Information Entropy for Super-Resolution Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2107.03642.pdf)
>  PSNR and SSIM are the most widely used metrics in super-resolution problems, because they are easy to use and can evaluate the similarities between generated images and reference images. However, single image super-resolution is an ill-posed problem, there are multiple corresponding high-resolution images for the same low-resolution image. The similarities can't totally reflect the restoration effect. The perceptual quality of generated images is also important, but PSNR and SSIM do not reflect perceptual quality well. To solve the problem, we proposed a method called regional differential information entropy to measure both of the similarities and perceptual quality. To overcome the problem that traditional image information entropy can't reflect the structure information, we proposed to measure every region's information entropy with sliding window. Considering that the human visual system is more sensitive to the brightness difference at low brightness, we take $\gamma$ quantization rather than linear quantization. To accelerate the method, we reorganized the calculation procedure of information entropy with a neural network. Through experiments on our IQA dataset and PIPAL, this paper proves that RDIE can better quantify perceptual quality of images especially GAN-based images.      
### 20.Identification and Adaptation with Binary-Valued Observations under Non-Persistent Excitation Condition  [ :arrow_down: ](https://arxiv.org/pdf/2107.03588.pdf)
>  Dynamical systems with binary-valued observations are widely used in information industry, technology of biological pharmacy and other fields. Though there have been much efforts devoted to the identification of such systems, most of the previous investigations are based on first-order gradient algorithm which usually has much slower convergence rate than the Quasi-Newton algorithm. Moreover, persistence of excitation(PE) conditions are usually required to guarantee consistent parameter estimates in the existing literature, which are hard to be verified or guaranteed for feedback control systems. In this paper, we propose an online projected Quasi-Newton type algorithm for parameter estimation of stochastic regression models with binary-valued observations and varying thresholds. By using both the stochastic Lyapunov function and martingale estimation methods, we establish the strong consistency of the estimation algorithm and provide the convergence rate, under a signal condition which is considerably weaker than the traditional PE condition and coincides with the weakest possible excitation known for the classical least square algorithm of stochastic regression models. Convergence of adaptive predictors and their applications in adaptive control are also discussed.      
### 21.High Force Density Multi-Stage Electrohydrodynamic Jets Using Folded Laser Microfabricated Electrodes  [ :arrow_down: ](https://arxiv.org/pdf/2107.03567.pdf)
>  The electrohydrodynamic (EHD) force produced by ions ejected from a corona plasma is a solid state, silent mechanism for accelerating air, useful for applications ranging from electronics cooling to flying microrobots. This paper presents the theoretical motivation and the first implementation of a multi-stage, highly miniaturized EHD device, which can provide both improved absolute power output and power density as compared to single-stage devices. A laser microfabricated, folded electrode design reduces component count and assembly time. Data from one, two, and three-stage devices demonstrates a near linear scaling of output force with stage count, indicating inter-stage ducting successfully reduces losses. Device lifetime is assessed to validate the use of stainless-steel emission electrodes. Areal thrust, force density, and volumetric power density for the three-stage device are among the highest ever measured from an EHD actuator.      
### 22.Generation of Synthetic Multi-Resolution Time Series Load Data  [ :arrow_down: ](https://arxiv.org/pdf/2107.03547.pdf)
>  The availability of large datasets is crucial for the development of new power system applications and tools; unfortunately, very few are publicly and freely available. We designed an end-to-end generative framework for the creation of synthetic bus-level time-series load data for transmission networks. The model is trained on a real dataset of over 70 Terabytes of synchrophasor measurements spanning multiple years. Leveraging a combination of principal component analysis and conditional generative adversarial network models, the scheme we developed allows for the generation of data at varying sampling rates (up to a maximum of 30 samples per second) and ranging in length from seconds to years. The generative models are tested extensively to verify that they correctly capture the diverse characteristics of real loads. Finally, we develop an open-source tool called LoadGAN which gives researchers access to the fully trained generative models via a graphical interface.      
### 23.Synthetic Time-Series Load Data via Conditional Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.03545.pdf)
>  A framework for the generation of synthetic time-series transmission-level load data is presented. Conditional generative adversarial networks are used to learn the patterns of a real dataset of hourly-sampled week-long load profiles and generate unique synthetic profiles on demand, based on the season and type of load required. Extensive testing of the generative model is performed to verify that the synthetic data fully captures the characteristics of real loads and that it can be used for downstream power system and/or machine learning applications.      
### 24.Energy Efficient Federated Learning in Integrated Fog-Cloud Computing Enabled Internet-of-Things Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.03520.pdf)
>  We investigate resource allocation scheme to reduce the energy consumption of federated learning (FL) in the integrated fog-cloud computing enabled Internet-of-things (IoT) networks. In the envisioned system, IoT devices are connected with the centralized cloud server (CS) via multiple fog access points (F-APs). We consider two different scenarios for training the local models. In the first scenario, local models are trained at the IoT devices and the F-APs upload the local model parameters to the CS. In the second scenario, local models are trained at the F-APs based on the collected data from the IoT devices and the F-APs collaborate with the CS for updating the model parameters. Our objective is to minimize the overall energy-consumption of both scenarios subject to FL time constraint. Towards this goal, we devise a joint optimization of scheduling of IoT devices with the F-APs, transmit power allocation, computation frequency allocation at the devices and F-APs and decouple it into two subproblems. In the first subproblem, we optimize the IoT device scheduling and power allocation, while in the second subproblem, we optimize the computation frequency allocation. For each scenario, we develop a conflict graph based solution to iteratively solve the two subproblems. Simulation results show that the proposed two schemes achieve a considerable performance gain in terms of the energy consumption minimization. The presented simulation results interestingly reveal that for a large number of IoT devices and large data sizes, it is more energy efficient to train the local models at the IoT devices instead of the F-APs.      
### 25.New Hybrid Maximum Power Point Tracking Methods for Fuel Cell using Artificial Intelligent  [ :arrow_down: ](https://arxiv.org/pdf/2107.03519.pdf)
>  In this paper, two maximum power point tracking (MPPT) methods for Fuel Cell (FC) systems based on Adaptive Neuro-Fuzzy Inference Systems (ANFIS) and Imperialist Competitive Algorithm trained Neural Network (ICANN) are presented. The first operation voltage of the fuel cell corresponding to maximum power point is determined based on the data, and then the duty cycle of a DC/DC converter is adjusted using fuzzy logic controller to force the system that operates in conditions which match up with its maximum power point, in order to minimize the fuel consumption. The proposed systems and conventional fuzzy controller system are simulated in the MATLAB environment and results show acceptable operation under fast variation of conditions as well as normal conditions in minimum time.      
### 26.The Influence of Frequency Containment Reserve Flexibilization on the Economics of Electric Vehicle Fleet Operation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03489.pdf)
>  Simultaneously with the transformation in the energy system, the spot and ancillary service markets for electricity have become increasingly flexible with shorter service periods and lower minimum powers. This flexibility has made the fastest form of frequency regulation - the frequency containment reserve (FCR) - particularly attractive for large-scale battery storage systems (BSSs) and led to a market growth of these systems. However, this growth resulted in high competition and consequently falling FCR prices, making the FCR market increasingly unattractive to large-scale BSSs. In the context of multi-use concepts, this market may be interesting especially for a pool of electric vehicles (EVs), which can generate additional revenue during their idle times. In this paper, multi-year measurement data of 22 commercial EVs are used for the development of a simulation model for marketing FCR. In addition, logbooks of more than 460 vehicles of different economic sectors are evaluated. Based on the simulations, the effects of flexibilization on the marketing of a pool of EVs are analyzed for the example of the German FCR market design, which is valid for many countries in Europe. It is shown that depending on the sector, especially the recently made changes of service periods from one week to one day and from one day to four hours generate the largest increase in available pool power. Further reductions in service periods, on the other hand, offer only a small advantage, as the idle times are often longer than the short service periods. In principle, increasing flexibility overcompensates for falling FCR prices and leads to higher revenues, even if this does not apply across all sectors examined. A pool of 1,000 EVs could theoretically generate revenues of about 5,000 EUR - 8,000 EUR per week on the German FCR market in 2020.      
### 27.A Highway Toll Lane Framework that Unites Autonomous Vehicles and High-occupancy Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2107.03477.pdf)
>  We consider the scenario where human-driven/autonomous vehicles with low/high occupancy are sharing a segment of highway and autonomous vehicles are capable of increasing the traffic throughput by preserving a shorter headway than human-driven vehicles. We propose a toll lane framework where a lane on the highway is reserved freely for autonomous vehicles with high occupancy, which have the greatest capability to increase social mobility, and the other three classes of vehicles can choose to use the toll lane with a toll or use the other regular lanes freely. All vehicles are assumed to be only interested in minimizing their own travel costs. We explore the resulting lane choice equilibria under the framework and establish desirable properties of the equilibria, which implicitly compare high-occupancy vehicles with autonomous vehicles in terms of their capabilities to increase social mobility. We further use numerical examples in the optimal toll design, the occupancy threshold design, and the policy design problems to clarify the various potential applications of this toll lane framework that unites high-occupancy vehicles and autonomous vehicles. To our best knowledge, this is the first work that systematically studies a toll lane framework that unites autonomous vehicles and high-occupancy vehicles on the roads.      
### 28.Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03442.pdf)
>  In large studies involving multi protocol Magnetic Resonance Imaging (MRI), it can occur to miss one or more sub-modalities for a given patient owing to poor quality (e.g. imaging artifacts), failed acquisitions, or hallway interrupted imaging examinations. In some cases, certain protocols are unavailable due to limited scan time or to retrospectively harmonise the imaging protocols of two independent studies. Missing image modalities pose a challenge to segmentation frameworks as complementary information contributed by the missing scans is then lost. In this paper, we propose a novel model, Multi-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute one or more missing sub-modalities for a patient scan. MGP-VAE can leverage the Gaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the subjects/patients and sub-modalities correlations. Instead of designing one network for each possible subset of present sub-modalities or using frameworks to mix feature maps, missing data can be generated from a single model based on all the available samples. We show the applicability of MGP-VAE on brain tumor segmentation where either, two, or three of four sub-modalities may be missing. Our experiments against competitive segmentation baselines with missing sub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE model for segmentation tasks.      
### 29.The Risk of Hidden Failures to the United States Electrical Grid and Potential for Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03439.pdf)
>  Hidden failures present a noticeable impact to the reliability of the United States electrical grid. These hazards are responsible for protective device misoperations and can cause multiple-element contingencies across nearby components, greatly increasing the likelihood of cascading events. This paper provides an in-depth overview of the causes and risks of hidden failures and discusses methods for identifying critical locations where hidden failures could pose a risk of cascading failure, with the ultimate goal being to identify efficient mitigation methods that can prevent their occurrence in protective relays.      
### 30.User equilibrium traffic assignment: k paths subtracting-adding algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2107.04018.pdf)
>  The traffic assignment problem is one of the most important transportation planning problems. The task faced by transportation planners, traffic engineers, and computer scientists is to generate high quality, approximate solutions of users equilibrium, that enable traffic scenario comparisons in a reasonable CPU time. We introduce the k Paths Subtracting-Adding (k-PSA) algorithm to approximate the user equilibrium of the traffic assignment problem. The k-PSA algorithm consists of two alternating phases: (1) enlargement of the set of attractive paths; (2) subtracting-adding trips between generated attractive paths for each origin-destination pair of nodes. The proposed algorithm performs the two phases iteratively until the number of paths for each origin-destination pair is k. We tested the proposed algorithm on four benchmark transportation networks from the literature. The performed numerical tests show that the proposed approach generates, in short, computation times, solutions that are, on average, very close to the user equilibrium.      
### 31.The influence of various optimization algorithms on nuclear power plant steam turbine exergy efficiency and destruction  [ :arrow_down: ](https://arxiv.org/pdf/2107.03897.pdf)
>  This paper presents an exergy analysis of the whole turbine, turbine cylinders and cylinder parts in four different operating regimes. Analyzed turbine operates in nuclear power plant while three of four operating regimes are obtained by using optimization algorithms - SA (Simplex Algorithm), GA (Genetic Algorithm) and IGSA (Improved Genetic-Simplex Algorithm). IGSA operating regime gives the highest developed mechanical power of the whole turbine equal to 1022.48 MW, followed by GA (1020.06 MW) and SA (1017.16 MW), while in Original operating regime whole turbine develop mechanical power equal to 996.29 MW. In addition, IGSA causes the highest increase in developed mechanical power of almost all cylinders and cylinder parts in comparison to the Original operating regime. All observed optimization algorithms increases the exergy destruction of the whole turbine in comparison to Original operating regime - the lowest increase causes IGSA, followed by GA and finally SA. The highest exergy efficiency of the whole turbine, equal to 85.92% is obtained by IGSA, followed by GA (85.89%) and SA (85.82%), while the lowest exergy efficiency is obtained in Original operating regime (85.70%). Analyzed turbine, which operates by using wet steam is low influenced by the ambient temperature change. IGSA, which shows dominant performance in exergy analysis parameters of the analyzed turbine, in certain situations is overpowered by GA. Therefore, in optimization of steam turbine performance, IGSA and GA can be recommended.      
### 32.Optimizing Data Processing in Space for Object Detection in Satellite Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2107.03774.pdf)
>  There is a proliferation in the number of satellites launched each year, resulting in downlinking of terabytes of data each day. The data received by ground stations is often unprocessed, making this an expensive process considering the large data sizes and that not all of the data is useful. This, coupled with the increasing demand for real-time data processing, has led to a growing need for on-orbit processing solutions. In this work, we investigate the performance of CNN-based object detectors on constrained devices by applying different image compression techniques to satellite data. We examine the capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier; low-power, high-performance computers, with integrated GPUs, small enough to fit on-board a nanosatellite. We take a closer look at object detection networks, including the Single Shot MultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a Large Scale Dataset for Object Detection in Aerial Images. The performance is measured in terms of execution time, memory consumption, and accuracy, and are compared against a baseline containing a server with two powerful GPUs. The results show that by applying image compression techniques, we are able to improve the execution time and memory consumption, achieving a fully runnable dataset. A lossless compression technique achieves roughly a 10% reduction in execution time and about a 3% reduction in memory consumption, with no impact on the accuracy. While a lossy compression technique improves the execution time by up to 144% and the memory consumption is reduced by as much as 97%. However, it has a significant impact on accuracy, varying depending on the compression ratio. Thus the application and ratio of these compression techniques may differ depending on the required level of accuracy for a particular task.      
### 33.Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil  [ :arrow_down: ](https://arxiv.org/pdf/2107.03675.pdf)
>  Speech evaluation is an essential component in computer-assisted language learning (CALL). While speech evaluation on English has been popular, automatic speech scoring on low resource languages remains challenging. Work in this area has focused on monolingual specific designs and handcrafted features stemming from resource-rich languages like English. Such approaches are often difficult to generalize to other languages, especially if we also want to consider suprasegmental qualities such as rhythm. In this work, we examine three different languages that possess distinct rhythm patterns: English (stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit robust feature representations inspired by music processing and vector representation learning. Empirical validations show consistent gains for all three languages when predicting pronunciation, rhythm and intonation performance.      
### 34.PNC Enabled IIoT: A General Framework for Channel-Coded Asymmetric Physical-Layer Network Coding  [ :arrow_down: ](https://arxiv.org/pdf/2107.03605.pdf)
>  This paper investigates the application of physical-layer network coding (PNC) to Industrial Internet-of-Things (IIoT) where a controller and a robot are out of each other's transmission range, and they exchange messages with the assistance of a relay. We particularly focus on a scenario where the controller has more transmitted information, and the channel of the controller is stronger than that of the robot. To reduce the communication latency, we propose an asymmetric transmission scheme where the controller and robot transmit different amount of information in the uplink of PNC simultaneously. To achieve this, the controller chooses a higher order modulation. In addition, the both users apply channel codes to guarantee the reliability. A problem is a superimposed symbol at the relay contains different amount of source information from the two end users. It is thus hard for the relay to deduce meaningful network-coded messages by applying the current PNC decoding techniques which require the end users to transmit the same amount of information. To solve this problem, we propose a lattice-based scheme where the two users encode-and-modulate their information in lattices with different lattice construction levels. Our design is versatile on that the two end users can freely choose their modulation orders based on their channel power, and the design is applicable for arbitrary channel codes.      
### 35.Reinforcement Learning based Negotiation-aware Motion Planning of Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2107.03600.pdf)
>  For autonomous vehicles integrating onto roadways with human traffic participants, it requires understanding and adapting to the participants' intention and driving styles by responding in predictable ways without explicit communication. This paper proposes a reinforcement learning based negotiation-aware motion planning framework, which adopts RL to adjust the driving style of the planner by dynamically modifying the prediction horizon length of the motion planner in real time adaptively w.r.t the event of a change in environment, typically triggered by traffic participants' switch of intents with different driving styles. The framework models the interaction between the autonomous vehicle and other traffic participants as a Markov Decision Process. A temporal sequence of occupancy grid maps are taken as inputs for RL module to embed an implicit intention reasoning. Curriculum learning is employed to enhance the training efficiency and the robustness of the algorithm. We applied our method to narrow lane navigation in both simulation and real world to demonstrate that the proposed method outperforms the common alternative due to its advantage in alleviating the social dilemma problem with proper negotiation skills.      
### 36.Analysis and Comparison of Time Series of Power Consumption of Sistan and Tehran distribution networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.03515.pdf)
>  Data presented in the form of time series as its analysis and applications recently have become increasingly important in different areas and domains. Prediction and classification of time-series data play a vital role in multiple fields. In this paper, the time series analysis related to power consumption at 12 o'clock every day in the period of 2012 to 2014 has been compared for two distribution networks of Sistan and one of the four networks of Tehran. By analyzing the power consumption of these two networks, a comparison can be made between these two regions in terms of development and climate difference and the impact of social, industrial and environmental phenomena. The reason for choosing these two networks was to compare a deprived area with an area in the capital. CRP tool software and toolkits have been used to analyze and compare time series, and various tools have been used to compare two time series.      
### 37.Federated Learning with Downlink Device Selection  [ :arrow_down: ](https://arxiv.org/pdf/2107.03510.pdf)
>  We study federated edge learning, where a global model is trained collaboratively using privacy-sensitive data at the edge of a wireless network. A parameter server (PS) keeps track of the global model and shares it with the wireless edge devices for training using their private local data. The devices then transmit their local model updates, which are used to update the global model, to the PS. The algorithm, which involves transmission over PS-to-device and device-to-PS links, continues until the convergence of the global model or lack of any participating devices. In this study, we consider device selection based on downlink channels over which the PS shares the global model with the devices. Performing digital downlink transmission, we design a partial device participation framework where a subset of the devices is selected for training at each iteration. Therefore, the participating devices can have a better estimate of the global model compared to the full device participation case which is due to the shared nature of the broadcast channel with the price of updating the global model with respect to a smaller set of data. At each iteration, the PS broadcasts different quantized global model updates to different participating devices based on the last global model estimates available at the devices. We investigate the best number of participating devices through experimental results for image classification using the MNIST dataset with biased distribution.      
### 38.BumbleBee: A Transformer for Music  [ :arrow_down: ](https://arxiv.org/pdf/2107.03443.pdf)
>  We will introduce BumbleBee, a transformer model that will generate MIDI music data . We will tackle the issue of transformers applied to long sequences by implementing a longformer generative model that uses dilating sliding windows to compute the attention layers. We will compare our results to that of the music transformer and Long-Short term memory (LSTM) to benchmark our results. This analysis will be performed using piano MIDI files, in particular , the JSB Chorales dataset that has already been used for other research works (Huang et al., 2018)      
### 39.Reconfigurable Intelligent Surface-Assisted Massive MIMO: Favorable Propagation, Channel Hardening, and Rank Deficiency  [ :arrow_down: ](https://arxiv.org/pdf/2107.03434.pdf)
>  Massive multiple-input multiple-output (MIMO) and reconfigurable intelligent surface (RIS) are two promising technologies for 5G-and-beyond wireless networks, capable of providing large array gain and multiuser spatial multiplexing. Without requiring additional frequency bands, those technologies offer significant improvements in both spectral and energy efficiency by simultaneously serving many users. The performance analysis of an RIS-assisted Massive MIMO system as a function of the channel statistics relies heavily on fundamental properties including favorable propagation, channel hardening, and rank deficiency. The coexistence of both direct and indirect links results in aggregated channels, whose properties are the main concerns of this lecture note. For practical systems with a finite number of antennas and scattering elements of the RIS, we evaluate the corresponding deterministic metrics with Rayleigh fading channels as a typical example.      
### 40.IowaRain: A Statewide Rain Event Dataset Based on Weather Radars and Quantitative Precipitation Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.03432.pdf)
>  Effective environmental planning and management to address climate change could be achieved through extensive environmental modeling with machine learning and conventional physical models. In order to develop and improve these models, practitioners and researchers need comprehensive benchmark datasets that are prepared and processed with environmental expertise that they can rely on. This study presents an extensive dataset of rainfall events for the state of Iowa (2016-2019) acquired from the National Weather Service Next Generation Weather Radar (NEXRAD) system and processed by a quantitative precipitation estimation system. The dataset presented in this study could be used for better disaster monitoring, response and recovery by paving the way for both predictive and prescriptive modeling.      
### 41.Sleep syndromes onset detection based on automatic sleep staging algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2107.03387.pdf)
>  In this paper, we propose a novel method and a practical approach to predicting early onsets of sleep syndromes, including restless leg syndrome, insomnia, based on an algorithm that is comprised of two modules. A Fast Fourier Transform is applied to 30 seconds long epochs of EEG recordings to provide localized time-frequency information, and a deep convolutional LSTM neural network is trained for sleep stage classification. Automating sleep stages detection from EEG data offers great potential to tackling sleep irregularities on a daily basis. Thereby, a novel approach for sleep stage classification is proposed which combines the best of signal processing and statistics. In this study, we used the PhysioNet Sleep European Data Format (EDF) Database. The code evaluation showed impressive results, reaching an accuracy of 86.43, precision of 77.76, recall of 93,32, F1-score of 89.12 with the final mean false error loss of 0.09.      
