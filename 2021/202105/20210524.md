# ArXiv eess --Mon, 24 May 2021
### 1.Going Deeper through the Gleason Scoring Scale: An Automatic end-to-end System for Histology Prostate Grading and Cribriform Pattern Detection  [ :arrow_down: ](https://arxiv.org/pdf/2105.10490.pdf)
>  The Gleason scoring system is the primary diagnostic and prognostic tool for prostate cancer. In recent years, with the development of digitisation devices, the use of computer vision techniques for the analysis of biopsies has increased. However, to the best of the authors' knowledge, the development of algorithms to automatically detect individual cribriform patterns belonging to Gleason grade 4 has not yet been studied in the literature. The objective of the work presented in this paper is to develop a deep-learning-based system able to support pathologists in the daily analysis of prostate biopsies. The methodological core of this work is a patch-wise predictive model based on convolutional neural networks able to determine the presence of cancerous patterns. In particular, we train from scratch a simple self-design architecture. The cribriform pattern is detected by retraining the set of filters of the last convolutional layer in the network. From the reconstructed prediction map, we compute the percentage of each Gleason grade in the tissue to feed a multi-layer perceptron which provides a biopsy-level score.mIn our SICAPv2 database, composed of 182 annotated whole slide images, we obtained a Cohen's quadratic kappa of 0.77 in the test set for the patch-level Gleason grading with the proposed architecture trained from scratch. Our results outperform previous ones reported in the literature. Furthermore, this model reaches the level of fine-tuned state-of-the-art architectures in a patient-based four groups cross validation. In the cribriform pattern detection task, we obtained an area under ROC curve of 0.82. Regarding the biopsy Gleason scoring, we achieved a quadratic Cohen's Kappa of 0.81 in the test subset. Shallow CNN architectures trained from scratch outperform current state-of-the-art methods for Gleason grades classification.      
### 2.WeGleNet: A Weakly-Supervised Convolutional Neural Network for the Semantic Segmentation of Gleason Grades in Prostate Histology Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.10445.pdf)
>  Prostate cancer is one of the main diseases affecting men worldwide. The Gleason scoring system is the primary diagnostic tool for prostate cancer. This is obtained via the visual analysis of cancerous patterns in prostate biopsies performed by expert pathologists, and the aggregation of the main Gleason grades in a combined score. Computer-aided diagnosis systems allow to reduce the workload of pathologists and increase the objectivity. Recently, efforts have been made in the literature to develop algorithms aiming the direct estimation of the global Gleason score at biopsy/core level with global labels. However, these algorithms do not cover the accurate localization of the Gleason patterns into the tissue. In this work, we propose a deep-learning-based system able to detect local cancerous patterns in the prostate tissue using only the global-level Gleason score during training. The methodological core of this work is the proposed weakly-supervised-trained convolutional neural network, WeGleNet, based on a multi-class segmentation layer after the feature extraction module, a global-aggregation, and the slicing of the background class for the model loss estimation during training. We obtained a Cohen's quadratic kappa (k) of 0.67 for the pixel-level prediction of cancerous patterns in the validation cohort. We compared the model performance for semantic segmentation of Gleason grades with supervised state-of-the-art architectures in the test cohort. We obtained a pixel-level k of 0.61 and a macro-averaged f1-score of 0.58, at the same level as fully-supervised methods. Regarding the estimation of the core-level Gleason score, we obtained a k of 0.76 and 0.67 between the model and two different pathologists. WeGleNet is capable of performing the semantic segmentation of Gleason grades similarly to fully-supervised methods without requiring pixel-level annotations.      
### 3.Covariance-Free Sparse Bayesian Learning  [ :arrow_down: ](https://arxiv.org/pdf/2105.10439.pdf)
>  Sparse Bayesian learning (SBL) is a powerful framework for tackling the sparse coding problem while also providing uncertainty quantification. However, the most popular inference algorithms for SBL become too expensive for high-dimensional problems due to the need to maintain a large covariance matrix. To resolve this issue, we introduce a new SBL inference algorithm that avoids explicit computation of the covariance matrix, thereby saving significant time and space. Instead of performing costly matrix inversions, our covariance-free method solves multiple linear systems to obtain provably unbiased estimates of the posterior statistics needed by SBL. These systems can be solved in parallel, enabling further acceleration of the algorithm via graphics processing units. In practice, our method can be up to thousands of times faster than existing baselines, reducing hours of computation time to seconds. We showcase how our new algorithm enables SBL to tractably tackle high-dimensional signal recovery problems, such as deconvolution of calcium imaging data and multi-contrast reconstruction of magnetic resonance images. Finally, we open-source a toolbox containing all of our implementations to drive future research in SBL.      
### 4.Self-learning for weakly supervised Gleason grading of local patterns  [ :arrow_down: ](https://arxiv.org/pdf/2105.10420.pdf)
>  Prostate cancer is one of the main diseases affecting men worldwide. The gold standard for diagnosis and prognosis is the Gleason grading system. In this process, pathologists manually analyze prostate histology slides under microscope, in a high time-consuming and subjective task. In the last years, computer-aided-diagnosis (CAD) systems have emerged as a promising tool that could support pathologists in the daily clinical practice. Nevertheless, these systems are usually trained using tedious and prone-to-error pixel-level annotations of Gleason grades in the tissue. To alleviate the need of manual pixel-wise labeling, just a handful of works have been presented in the literature. Motivated by this, we propose a novel weakly-supervised deep-learning model, based on self-learning CNNs, that leverages only the global Gleason score of gigapixel whole slide images during training to accurately perform both, grading of patch-level patterns and biopsy-level scoring. To evaluate the performance of the proposed method, we perform extensive experiments on three different external datasets for the patch-level Gleason grading, and on two different test sets for global Grade Group prediction. We empirically demonstrate that our approach outperforms its supervised counterpart on patch-level Gleason grading by a large margin, as well as state-of-the-art methods on global biopsy-level scoring. Particularly, the proposed model brings an average improvement on the Cohen's quadratic kappa (k) score of nearly 18% compared to full-supervision for the patch-level Gleason grading task.      
### 5.Photonic single perceptron at Giga-OP/s speeds with Kerr microcombs for scalable optical neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.10407.pdf)
>  Optical artificial neural networks (ONNs) have significant potential for ultra-high computing speed and energy efficiency. We report a novel approach to ONNs that uses integrated Kerr optical microcombs. This approach is programmable and scalable and is capable of reaching ultrahigh speeds. We demonstrate the basic building block ONNs, a single neuron perceptron, by mapping synapses onto 49 wavelengths to achieve an operating speed of 11.9 x 109 operations per second, or GigaOPS, at 8 bits per operation, which equates to 95.2 gigabits/s (Gbps). We test the perceptron on handwritten digit recognition and cancer cell detection, achieving over 90% and 85% accuracy, respectively. By scaling the perceptron to a deep learning network using off the shelf telecom technology we can achieve high throughput operation for matrix multiplication for real-time massive data processing.      
### 6.Dyadic aggregated autoregressive (DASAR) model for time-frequency representation of biomedical signals  [ :arrow_down: ](https://arxiv.org/pdf/2105.10406.pdf)
>  This paper introduces a new time-frequency representation method for biomedical signals: the dyadic aggregated autoregressive (DASAR) model. Signals, such as electroencephalograms (EEGs) and functional near-infrared spectroscopy (fNIRS), exhibit physiological information through time-evolving spectrum components at specific frequency intervals: 0-50 Hz (EEG) or 0-150 mHz (fNIRS). Spectrotemporal features in signals are conventionally estimated using short-time Fourier transform (STFT) and wavelet transform (WT). However, both methods may not offer the most robust or compact representation despite their widespread use in biomedical contexts. The presented method, DASAR, improves precise frequency identification and tracking of interpretable frequency components with a parsimonious set of parameters. DASAR achieves these characteristics by assuming that the biomedical time-varying spectrum comprises several independent stochastic oscillators with (piecewise) time-varying frequencies. Local stationarity can be assumed within dyadic subdivisions of the recordings, while the stochastic oscillators can be modeled with an aggregation of second-order autoregressive models (ASAR). DASAR can provide a more accurate representation of the (highly contrasted) EEG and fNIRS frequency ranges by increasing the estimation accuracy in user-defined spectrum region of interest (SROI). A mental arithmetic experiment on a hybrid EEG-fNIRS was conducted to assess the efficiency of the method. Our proposed technique, STFT, and WT were applied on both biomedical signals to discover potential oscillators that improve the discrimination between the task condition and its baseline. The results show that DASAR provided the highest spectrum differentiation and it was the only method that could identify Mayer waves as narrow-band artifacts at 97.4-97.5 mHz.      
### 7.Controllable Transmission Networks under Demand Uncertainty with Modular FACTS  [ :arrow_down: ](https://arxiv.org/pdf/2105.10402.pdf)
>  The transmission system operators (TSOs) are responsible to provide secure and efficient access to the transmission system for all stakeholders. This task is gradually getting challenging due to the demand growth, demand uncertainty, rapid changes in generation mix, and market policies. Traditionally, the TSOs try to maximize the technical performance of the transmission network via building new overhead lines or physical hardening. However, obtaining public acceptance for building new lines is not an easy step in this procedure. For this reason, the TSOs try to capture the capabilities of existing assets. This paper investigates the use of modular FACTS devices (to alter the line characteristics) for improving the capability of transmission network in serving the uncertain demand without the need for building new overhead lines. The proposed method considers the uncertainty of demands and controls the utilization of existing transmission assets. The mathematical results obtained are validated with a complete non-linear simulation model of three transmission networks.      
### 8.Semi-supervised Learning for Identifying the Likelihood of Agitation in People with Dementia  [ :arrow_down: ](https://arxiv.org/pdf/2105.10398.pdf)
>  Interpreting the environmental, behavioural and psychological data from in-home sensory observations and measurements can provide valuable insights into the health and well-being of individuals. Presents of neuropsychiatric and psychological symptoms in people with dementia have a significant impact on their well-being and disease prognosis. Agitation in people with dementia can be due to many reasons such as pain or discomfort, medical reasons such as side effects of a medicine, communication problems and environment. This paper discusses a model for analysing the risk of agitation in people with dementia and how in-home monitoring data can support them. We proposed a semi-supervised model which combines a self-supervised learning model and a Bayesian ensemble classification. We train and test the proposed model on a dataset from a clinical study. The dataset was collected from sensors deployed in 96 homes of patients with dementia. The proposed model outperforms the state-of-the-art models in recall and f1-score values by 20%. The model also indicates better generalisability compared to the baseline models.      
### 9.Combined Transmission and Distribution State-Estimation for Future Electric Grids  [ :arrow_down: ](https://arxiv.org/pdf/2105.10395.pdf)
>  Proliferation of grid resources on the distribution network along with the inability to forecast them accurately will render the existing methodology of grid operation and control untenable in the future. Instead, a more distributed yet coordinated approach for grid operation and control will emerge that models and analyzes the grid with a larger footprint and deeper hierarchy to unify control of disparate T&amp;D grid resources under a common framework. Such approach will require AC state-estimation (ACSE) of joint T&amp;D networks. Today, no practical method for realizing combined T&amp;D ACSE exists. This paper addresses that gap from circuit-theoretic perspective through realizing a combined T&amp;D ACSE solution methodology that is fast, convex and robust against bad-data. To address daunting challenges of problem size (million+ variables) and data-privacy, the approach is distributed both in memory and computing resources. To ensure timely convergence, the approach constructs a distributed circuit model for combined T&amp;D networks and utilizes node-tearing techniques for efficient parallelism. To demonstrate the efficacy of the approach, combined T&amp;D ACSE algorithm is run on large test networks that comprise of multiple T&amp;D feeders. The results reflect the accuracy of the estimates in terms of root mean-square error and algorithm scalability in terms of wall-clock time.      
### 10.High Accuracy and Low Complexity Frequency Offset Estimation Based on All-Phase FFT for M-QAM Coherent Optical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.10394.pdf)
>  A low complexity frequency offset estimation algorithm based on all-phase FFT for M-QAM is proposed. Compared with two-stage algorithms such as FFT+CZT and FFT+ZoomFFT, our algorithm can lower computational complexity by 73% and 30% respectively, without loss of the estimation accuracy.      
### 11.ReLUSyn: Synthesizing Stealthy Attacks for Deep Neural Network Based Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.10393.pdf)
>  Cyber Physical Systems (cps) are deployed in many mission-critical settings, such as medical devices, autonomous vehicular systems and aircraft control management systems. As more and more CPS adopt Deep Neural Networks (Deep Neural Network (dnns), these systems can be vulnerable to attacks. . Prior work has demonstrated the susceptibility of CPS to False Data Injection Attacks (False Data Injection Attacks (fdias), which can cause significant damage. We identify a new category of attacks on these systems. In this paper, we demonstrate that DNN based CPS are also subject to these attacks. These attacks, which we call Ripple False Data Injection Attacks (rfdia), use minimal input perturbations to stealthily change the dnn output. The input perturbations propagate as ripples through multiple dnn layers to affect the output in a targeted manner. We develop an automated technique to synthesize rfdias against DNN-based CPS. Our technique models the attack as an optimization problem using Mixed Integer Linear Programming (Mixed Integer Linear Program (milp)). We define an abstraction for dnnbased cps that allows us to automatically: 1) identify the critical inputs, and 2) find the smallest perturbations that produce output changes. We demonstrate our technique on three practical cps with two mission-critical applications: an (Artifical Pancreas System (aps)) and two aircraft control management systems (Horizontal Collision Avoidance System (hcas) and Collision Avoidance System-Xu (acas-xu)).      
### 12.Automated Detection of Abnormal EEGs in Epilepsy With a Compact and Efficient CNN Model  [ :arrow_down: ](https://arxiv.org/pdf/2105.10358.pdf)
>  Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but it requires expertise and experience to identify abnormalities. It is thus crucial to develop automated models for the detection of abnormal EEGs related to epilepsy. This paper describes the development of a novel class of compact and efficient convolutional neural networks (CNNs) for detecting abnormal time intervals and electrodes in EEGs for epilepsy. The designed model is inspired by a CNN developed for brain-computer interfacing called multichannel EEGNet (mEEGNet). Unlike the EEGNet, the proposed model, mEEGNet, has the same number of electrode inputs and outputs to detect abnormalities. The mEEGNet was evaluated with a clinical dataset consisting of 29 cases of juvenile and childhood absence epilepsy labeled by a clinical expert. The labels were given to paroxysmal discharges visually observed in both ictal (seizure) and interictal (nonseizure) intervals. Results showed that the mEEGNet detected abnormal EEGs with the area under the curve, F1-values, and sensitivity equivalent to or higher than those of existing CNNs. Moreover, the number of parameters is much smaller than other CNN models. To our knowledge, the dataset of absence epilepsy validated with machine learning through this research is the largest in the literature.      
### 13.Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion  [ :arrow_down: ](https://arxiv.org/pdf/2105.10341.pdf)
>  In the race to bring Artificial Intelligence (AI) to the edge, collaborative intelligence has emerged as a promising way to lighten the computation load on edge devices that run applications based on Deep Neural Networks (DNNs). Typically, a deep model is split at a certain layer into edge and cloud sub-models. The deep feature tensor produced by the edge sub-model is transmitted to the cloud, where the remaining computationally intensive workload is performed by the cloud sub-model. The communication channel between the edge and cloud is imperfect, which will result in missing data in the deep feature tensor received at the cloud side. In this study, we examine the effectiveness of four low-rank tensor completion methods in recovering missing data in the deep feature tensor. We consider both sparse tensors, such as those produced by the VGG16 model, as well as non-sparse tensors, such as those produced by ResNet34 model. We study tensor completion effectiveness in both conplexity-constrained and unconstrained scenario.      
### 14.Optimal Estimator Design and Properties Analysis for Interconnected Systems with Asymmetric Information Structure  [ :arrow_down: ](https://arxiv.org/pdf/2105.10299.pdf)
>  This paper studies the optimal state estimation problem for interconnected systems. Each subsystem can obtain its own measurement in real time, while, the measurements transmitted between the subsystems suffer from random delay. The optimal estimator is analytically designed for minimizing the conditional error covariance. Due to the random delay, the error covariance of the estimation is random. The boundedness of the expected error covariance (EEC) is analyzed. In particular, a new condition that is easy to verify is established for the boundedness of EEC. Further, the properties about EEC with respect to the delay probability is studied. We found that there exists a critical probability such that the EEC is bounded if the delay probability is below the critical probability. Also, a lower and upper bound of the critical probability is effectively computed. Finally, the proposed results are applied to a power system, and the effectiveness of the designed methods is illustrated by simulations.      
### 15.Near-Optimal Design of Safe Output Feedback Controllers from Noisy Data  [ :arrow_down: ](https://arxiv.org/pdf/2105.10280.pdf)
>  Recent work in data-driven control has revived behavioral theory to perform a variety of complex control tasks, by directly plugging libraries of past input-output trajectories into optimal control problems. Despite recent advances, two key aspects remain unclear when the data are corrupted by noise: how can safety be guaranteed, and to what extent is the control performance affected? In this work, we provide a quantitative answer to these questions. In particular, we formulate a robustly safe version of the recently introduced Behavioral Input-Output Parametrization (BIOP) for the optimal predictive control of unknown constrained systems. The proposed framework has three main advantages: 1) it allows one to safely operate the system while explicitly quantifying, as a function of the noise level corrupting the data, how much the performance degrades, 2) it can be used in combination with state-of-the-art impulse response estimators, and finally, being a data-driven approach, 3) the state-space parameters and the initial state need not be specified for controller synthesis. We corroborate our results through numerical experiments.      
### 16.Predictive control barrier functions: Enhanced safety mechanisms for learning-based control  [ :arrow_down: ](https://arxiv.org/pdf/2105.10241.pdf)
>  While learning-based control techniques often outperform classical controller designs, safety requirements limit the acceptance of such methods in many applications. Recent developments address this issue through so-called predictive safety filters, which assess if a proposed learning-based control input can lead to constraint violations and modifies it if necessary to ensure safety for all future time steps. The theoretical guarantees of such predictive safety filters rely on the model assumptions and minor deviations can lead to failure of the filter putting the system at risk. This paper introduces an auxiliary soft-constrained predictive control problem that is always feasible at each time step and asymptotically stabilizes the feasible set of the original safety filter, thereby providing a recovery mechanism in safety-critical situations. This is achieved by a simple constraint tightening in combination with a terminal control barrier function. By extending discrete-time control barrier function theory, we establish that the proposed auxiliary problem provides a `predictive' control barrier function. The resulting algorithm is demonstrated using numerical examples.      
### 17.AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.10239.pdf)
>  Covid-19 global pandemic continues to devastate health care systems across the world. In many countries, the 2nd wave is very severe. Economical and rapid testing, as well as diagnosis, is urgently needed to control the pandemic. At present, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR) testing can be the fastest, scalable, and non-invasive method. The existing methods suffer due to the limited CXR samples available from Covid-19. Thus, inspired by the limitations of the open-source work in this field, we propose attention guided contrastive CNN architecture (AC-CovidNet) for Covid-19 detection in CXR images. The proposed method learns the robust and discriminative features with the help of contrastive loss. Moreover, the proposed method gives more importance to the infected regions as guided by the attention mechanism. We compute the sensitivity of the proposed method over the publicly available Covid-19 dataset. It is observed that the proposed AC-CovidNet exhibits very promising performance as compared to the existing methods even with limited training data. It can tackle the bottleneck of CXR Covid-19 datasets being faced by the researchers. The code used in this paper is released publicly at \url{<a class="link-external link-https" href="https://github.com/shivram1987/AC-CovidNet/" rel="external noopener nofollow">this https URL</a>}.      
### 18.An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma  [ :arrow_down: ](https://arxiv.org/pdf/2105.10238.pdf)
>  Pelvic ring disruptions result from blunt injury mechanisms and are often found in patients with multi-system trauma. To grade pelvic fracture severity in trauma victims based on whole-body CT, the Tile AO/OTA classification is frequently used. Due to the high volume of whole-body trauma CTs generated in busy trauma centers, an automated approach to Tile classification would provide substantial value, e.,g., to prioritize the reading queue of the attending trauma radiologist. In such scenario, an automated method should perform grading based on a transparent process and based on interpretable features to enable interaction with human readers and lower their workload by offering insights from a first automated read of the scan. This paper introduces an automated yet interpretable pelvic trauma decision support system to assist radiologists in fracture detection and Tile grade classification. The method operates similarly to human interpretation of CT scans and first detects distinct pelvic fractures on CT with high specificity using a Faster-RCNN model that are then interpreted using a structural causal model based on clinical best practices to infer an initial Tile grade. The Bayesian causal model and finally, the object detector are then queried for likely co-occurring fractures that may have been rejected initially due to the highly specific operating point of the detector, resulting in an updated list of detected fractures and corresponding final Tile grade. Our method is transparent in that it provides finding location and type using the object detector, as well as information on important counterfactuals that would invalidate the system's recommendation and achieves an AUC of 83.3%/85.1% for translational/rotational instability. Despite being designed for human-machine teaming, our approach does not compromise on performance compared to previous black-box approaches.      
### 19.Helsinki Deblur Challenge 2021: description of photographic data  [ :arrow_down: ](https://arxiv.org/pdf/2105.10233.pdf)
>  The photographic dataset collected for the Helsinki Deblur Challenge 2021 (HDC2021) contains pairs of images taken by two identical cameras of the same target but with different conditions. One camera is always in focus and produces sharp and low-noise images the other camera produces blurred and noisy images as it is gradually more and more out of focus and has a higher ISO setting. Even though the dataset was designed and captured with the HDC2021 in mind it can be used for any testing and benchmarking of image deblurring algorithms. The data is available here: <a class="link-external link-https" href="https://doi.org/10.5281/zenodo.477228" rel="external noopener nofollow">this https URL</a>      
### 20.Modeling the Operation of an Autonomous Profilograph as a Dynamic System When Measuring Oceanological Fields  [ :arrow_down: ](https://arxiv.org/pdf/2105.10224.pdf)
>  On the basis of the proposed model of a diving buoy with adjustable buoyancy, a model of a double-loop adaptive control system for the speed modes of an autonomous profiler has been developed, depending on the gradients of the measured parameters. The results of modeling are presented.      
### 21.Anomaly Detection By Autoencoder Based On Weighted Frequency Domain Loss  [ :arrow_down: ](https://arxiv.org/pdf/2105.10214.pdf)
>  In image anomaly detection, Autoencoders are the popular methods that reconstruct the input image that might contain anomalies and output a clean image with no abnormalities. These Autoencoder-based methods usually calculate the anomaly score from the reconstruction error, the difference between the input image and the reconstructed image. On the other hand, the accuracy of the reconstruction is insufficient in many of these methods, so it leads to degraded accuracy of anomaly detection. To improve the accuracy of the reconstruction, we consider defining loss function in the frequency domain. In general, we know that natural images contain many low-frequency components and few high-frequency components. Hence, to improve the accuracy of the reconstruction of high-frequency components, we introduce a new loss function named weighted frequency domain loss(WFDL). WFDL provides a sharper reconstructed image, which contributes to improving the accuracy of anomaly detection. In this paper, we show our method's superiority over the conventional Autoencoder methods by comparing it with AUROC on the MVTec AD dataset.      
### 22.Rational Dynamic Price Model for Demand Response Programs in Modern Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.10205.pdf)
>  Demand response (DR) refers to change in electricity consumption pattern of customers during on-peak hours in lieu of financial gains to reduce stress on distribution systems. Existing dynamic price models have not provided adequate success to price-based demand response (PBDR) programs. It happened as these models have raised typical socio-economic problems pertaining to cross-subsidy, free-riders, social inequity, assured profit of utilities, financial gains and comfort of customers, etc. This paper presents a new dynamic price model for PBDR in distribution systems which aims to overcome some of the above mentioned problems of the existing price models. The main aim of the developed price model is to overcome the problems of cross-subsidy and free-riders of the existing price models for widespread acceptance, deployment and efficient utilization of PBDR programs in contemporary distribution systems. Proposed price model generates demand-linked price signal that imposes different price signals to different customers during on-peak hours and remains static otherwise. This makes proposed model a class apart from other existing models. The novelty of the proposed model lies in the fact that the financial benefits and penalties pertaining to DR are self-adjusted among customers while preserving social equity and profit of the utility. Such an ideology has not been yet addressed in the literature. Detailed investigation of application results on a standard test bench reveals that the proposed model equally cares regarding the interests of both customers and utility. For economic assessment, a comparison of the proposed price model with the existing pricing models is also performed.      
### 23.Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2105.10194.pdf)
>  Over the past decades, enormous efforts have been made to improve the performance of linear or nonlinear mixing models for hyperspectral unmixing, yet their ability to simultaneously generalize various spectral variabilities and extract physically meaningful endmembers still remains limited due to the poor ability in data fitting and reconstruction and the sensitivity to various spectral variabilities. Inspired by the powerful learning ability of deep learning, we attempt to develop a general deep learning approach for hyperspectral unmixing, by fully considering the properties of endmembers extracted from the hyperspectral imagery, called endmember-guided unmixing network (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a two-stream Siamese deep network, which learns an additional network from the pure or nearly-pure endmembers to correct the weights of another unmixing network by sharing network parameters and adding spectrally meaningful constraints (e.g., non-negativity and sum-to-one) towards a more accurate and interpretable unmixing solution. Furthermore, the resulting general framework is not only limited to pixel-wise spectral unmixing but also applicable to spatial information modeling with convolutional operators for spatial-spectral unmixing. Experimental results conducted on three different datasets with the ground-truth of abundance maps corresponding to each material demonstrate the effectiveness and superiority of the EGU-Net over state-of-the-art unmixing algorithms. The codes will be available from the website: <a class="link-external link-https" href="https://github.com/danfenghong/IEEE_TNNLS_EGU-Net" rel="external noopener nofollow">this https URL</a>.      
### 24.Deep Learning-based Implicit CSI Feedback in Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2105.10100.pdf)
>  Massive multiple-input multiple-output can obtain more performance gain by exploiting the downlink channel state information (CSI) at the base station (BS). Therefore, studying CSI feedback with limited communication resources in frequency-division duplexing systems is of great importance. Recently, deep learning (DL)-based CSI feedback has shown considerable potential. However, the existing DL-based explicit feedback schemes are difficult to deploy because current fifth-generation mobile communication protocols and systems are designed based on an implicit feedback mechanism. In this paper, we propose a DL-based implicit feedback architecture to inherit the low-overhead characteristic, which uses neural networks (NNs) to replace the precoding matrix indicator (PMI) encoding and decoding modules. By using environment information, the NNs can achieve a more refined mapping between the precoding matrix and the PMI compared with codebooks. The correlation between subbands is also used to further improve the feedback performance. Simulation results show that, for a single resource block (RB), the proposed architecture can save 25.0% and 40.0% of overhead compared with Type I codebook under two antenna configurations, respectively. For a wideband system with 52 RBs, overhead can be saved by 30.7% and 48.0% compared with Type II codebook when ignoring and considering extracting subband correlation, respectively.      
### 25.Latent-space scalability for multi-task collaborative intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2105.10089.pdf)
>  We investigate latent-space scalability for multi-task collaborative intelligence, where one of the tasks is object detection and the other is input reconstruction. In our proposed approach, part of the latent space can be selectively decoded to support object detection while the remainder can be decoded when input reconstruction is needed. Such an approach allows reduced computational resources when only object detection is required, and this can be achieved without reconstructing input pixels. By varying the scaling factors of various terms in the training loss function, the system can be trained to achieve various trade-offs between object detection accuracy and input reconstruction quality. Experiments are conducted to demonstrate the adjustable system performance on the two tasks compared to the relevant benchmarks.      
### 26.Robust heartbeat detection using multimodal recordings and ECG quality assessment with signal amplitudes dispersion  [ :arrow_down: ](https://arxiv.org/pdf/2105.10046.pdf)
>  Method: In this study, a new method is introduced for distinguishing noise-free segments of ECG from noisy segments that use sample amplitude dispersion with an adoptive threshold for variance of samples amplitude and a method which uses compatibility of detected beats in ECG and some of the other signals which are related to the heart activity such as BP, arterial pressure (ART) and pulmonary artery pressure (PAP). A prioritization is applied in other pulsatile signals based on the amplitude and clarity of the peaks on them, and a fusion strategy is employed for segments on which ECG is noisy and other available signals in the data, which contain peaks corresponding to R peak of the ECG, are scored in three steps scoring function.      
### 27.Convolutional Block Design for Learned Fractional Downsampling  [ :arrow_down: ](https://arxiv.org/pdf/2105.09999.pdf)
>  The layers of convolutional neural networks (CNNs) can be used to alter the resolution of their inputs, but the scaling factors are limited to integer values. However, in many image and video processing applications, the ability to resize by a fractional factor would be advantageous. One example is conversion between resolutions standardized for video compression, such as from 1080p to 720p. To solve this problem, we propose an alternative building block, formulated as a conventional convolutional layer followed by a differentiable resizer. More concretely, the convolutional layer preserves the resolution of the input, while the resizing operation is fully handled by the resizer. In this way, any CNN architecture can be adapted for non-integer resizing. As an application, we replace the resizing convolutional layer of a modern deep downsampling model by the proposed building block, and apply it to an adaptive bitrate video streaming scenario. Our experimental results show that an improvement in coding efficiency over the conventional Lanczos algorithm is attained, in terms of PSNR, SSIM, and VMAF on test videos.      
### 28.Dense Reconstruction of Transparent Objects by Altering Incident Light Paths Through Refraction  [ :arrow_down: ](https://arxiv.org/pdf/2105.09993.pdf)
>  This paper addresses the problem of reconstructing the surface shape of transparent objects. The difficulty of this problem originates from the viewpoint dependent appearance of a transparent object, which quickly makes reconstruction methods tailored for diffuse surfaces fail disgracefully. In this paper, we introduce a fixed viewpoint approach to dense surface reconstruction of transparent objects based on refraction of light. We present a simple setup that allows us to alter the incident light paths before light rays enter the object by immersing the object partially in a liquid, and develop a method for recovering the object surface through reconstructing and triangulating such incident light paths. Our proposed approach does not need to model the complex interactions of light as it travels through the object, neither does it assume any parametric form for the object shape nor the exact number of refractions and reflections taken place along the light paths. It can therefore handle transparent objects with a relatively complex shape and structure, with unknown and inhomogeneous refractive index. We also show that for thin transparent objects, our proposed acquisition setup can be further simplified by adopting a single refraction approximation. Experimental results on both synthetic and real data demonstrate the feasibility and accuracy of our proposed approach.      
### 29.Energy-Efficient mm-Wave Backhauling via Frame Aggregation in Wide Area Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.09979.pdf)
>  Wide area networks for surveying applications, such as seismic acquisition, have been witnessing a significant increase in node density and area, where large amounts of data have to be transferred in real-time. While cables can meet these requirements, they account for a majority of the equipment weight, maintenance, and labor costs. A novel wireless network architecture, compliant with the IEEE 802.11ad standard, is proposed for establishing scalable, energy-efficient, and gigabit-rate backhaul across very large areas. Statistical path-loss and line-of-sight models are derived using real-world topographic data in well-known seismic regions. Additionally, a cross-layer analytical model is derived for 802.11 systems that can characterize the overall latency and power consumption under the impact of co-channel interference. On the basis of these models, a Frame Aggregation Power-Saving Backhaul (FA-PSB) scheme is proposed for near-optimal power conservation under a latency constraint, through a duty-cycled approach. A performance evaluation with respect to the survey size and data generation rate reveals that the proposed architecture and the FA-PSB scheme can support real-time acquisition in large-scale high-density scenarios while operating with minimal power consumption, thereby enhancing the lifetime of wireless seismic surveys. The FA-PSB scheme can be applied to cellular backhaul and sensor networks as well.      
### 30.Bringing A Robot Simulator to the SCAMP Vision System  [ :arrow_down: ](https://arxiv.org/pdf/2105.10479.pdf)
>  This work develops and demonstrates the integration of the SCAMP-5d vision system into the CoppeliaSim robot simulator, creating a semi-simulated environment. By configuring a camera in the simulator and setting up communication with the SCAMP python host through remote API, sensor images from the simulator can be transferred to the SCAMP vision sensor, where on-sensor image processing such as CNN inference can be performed. SCAMP output is then fed back into CoppeliaSim. This proposed platform integration enables rapid prototyping validations of SCAMP algorithms for robotic systems. We demonstrate a car localisation and tracking task using this proposed semi-simulated platform, with a CNN inference on SCAMP to command the motion of a robot. We made this platform available online.      
### 31.Towards Realization of Augmented Intelligence in Dermatology: Advances and Future Directions  [ :arrow_down: ](https://arxiv.org/pdf/2105.10477.pdf)
>  Artificial intelligence (AI) algorithms using deep learning have advanced the classification of skin disease images; however these algorithms have been mostly applied "in silico" and not validated clinically. Most dermatology AI algorithms perform binary classification tasks (e.g. malignancy versus benign lesions), but this task is not representative of dermatologists' diagnostic range. The American Academy of Dermatology Task Force on Augmented Intelligence published a position statement emphasizing the importance of clinical validation to create human-computer synergy, termed augmented intelligence (AuI). Liu et al's recent paper, "A deep learning system for differential diagnosis of skin diseases" represents a significant advancement of AI in dermatology, bringing it closer to clinical impact. However, significant issues must be addressed before this algorithm can be integrated into clinical workflow. These issues include accurate and equitable model development, defining and assessing appropriate clinical outcomes, and real-world integration.      
### 32.Sparse Layered MIMO with Iterative Detection  [ :arrow_down: ](https://arxiv.org/pdf/2105.10476.pdf)
>  In this paper, we propose a novel transmission scheme, called sparse layered MIMO (SL-MIMO), that combines non-orthogonal transmission and singular value decomposition (SVD) precoding. Nonorthogonality in SL-MIMO allows re-using of the eigen-channels which improves the spectral efficiency and error rate performance of the system through enhancing the coding gain and diversity gain. We also present a low-complexity message-passing (MP) detector for the proposed SL-MIMO system which performs quite close to maximum likelihood (ML). The joint moment generating function (MGF) of the ordered eigenvalues is calculated and used to derive a closed-form upper bound on the average word error probability (AWEP) of the SL-MIMO system, and this derived expression is then used to analyze the diversity gain of the system. We use our analytical results to design sub-optimal codebooks to minimize the error rate of the SL-MIMO system. Simulation results in 4x4 and 6x6 multiple-input multiple-output (MIMO) systems with 4-ary, 16-ary, and 64-ary constellations show that our proposed SL-MIMO scheme outperforms competing approaches such as X- and Y-codes in terms of system error rate performance. SL-MIMO has 5.6 dB advantage compared to X-codes and 4.7 dB advantage compared to Y-codes in 6x6 MIMO system with a 64-ary constellation.      
### 33.Research on Regional Urban Economic Development by Nightlight-time Remote Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2105.10459.pdf)
>  In order to study the phenomenon of regional economic development and urban expansion from the perspective of night-light remote sensing images, researchers use NOAA-provided night-light remote sensing image data (data from 1992 to 2013) along with ArcGIS software to process image information, obtain the basic pixel information data of specific areas of the image, and analyze these data from the space-time domain for presentation of the trend of regional economic development in China in recent years, and tries to explore the urbanization effect brought by the rapid development of China's economy. Through the analysis and study of the data, the results show that the urbanization development speed in China is still at its peak, and has great development potential and space. But at the same time, people also need to pay attention to the imbalance of regional development.      
### 34.Extraction of physically meaningful endmembers from STEM spectrum-images combining geometrical and statistical approaches  [ :arrow_down: ](https://arxiv.org/pdf/2105.10416.pdf)
>  This article addresses extraction of physically meaningful information from STEM EELS and EDX spectrum-images using methods of Multivariate Statistical Analysis. The problem is interpreted in terms of data distribution in a multi-dimensional factor space, which allows for a straightforward and intuitively clear comparison of various approaches. A new computationally efficient and robust method for finding physically meaningful endmembers in spectrum-image datasets is presented. The method combines the geometrical approach of Vertex Component Analysis with the statistical approach of Bayesian inference. The algorithm is described in detail at an example of EELS spectrum-imaging of a multi-compound CMOS transistor.      
### 35.Real-time estimation of phase and amplitude with application to neural data  [ :arrow_down: ](https://arxiv.org/pdf/2105.10404.pdf)
>  Computation of the instantaneous phase and amplitude via the Hilbert Transform is a powerful tool of data analysis. This approach finds many applications in various science and engineering branches but is not proper for causal estimation because it requires knowledge of the signal's past and future. However, several problems require real-time estimation of phase and amplitude; an illustrative example is phase-locked or amplitude-dependent stimulation in neuroscience. In this paper, we discuss and compare three causal algorithms that do not rely on the Hilbert Transform but exploit well-known physical phenomena, the synchronization and the resonance. After testing the algorithms on a synthetic data set, we illustrate their performance computing phase and amplitude for the accelerometer tremor measurements and a Parkinsonian patient's beta-band brain activity.      
### 36.High Fidelity Fingerprint Generation: Quality, Uniqueness, and Privacy  [ :arrow_down: ](https://arxiv.org/pdf/2105.10403.pdf)
>  In this work, we utilize progressive growth-based Generative Adversarial Networks (GANs) to develop the Clarkson Fingerprint Generator (CFG). We demonstrate that the CFG is capable of generating realistic, high fidelity, $512\times512$ pixels, full, plain impression fingerprints. Our results suggest that the fingerprints generated by the CFG are unique, diverse, and resemble the training dataset in terms of minutiae configuration and quality, while not revealing the underlying identities of the training data. We make the pre-trained CFG model and the synthetically generated dataset publicly available at <a class="link-external link-https" href="https://github.com/keivanB/Clarkson_Finger_Gen" rel="external noopener nofollow">this https URL</a>      
### 37.An Algorithm for Grant-Free Random Access in Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2105.10378.pdf)
>  Massive access is one of the main use cases of beyond 5G (B5G) wireless networks and massive MIMO is a key technology for supporting it. Prior works studied massive access in the co-located massive MIMO framework. In this paper, we investigate the activity detection in grant-free random access for massive machine type communications (mMTC) in cell-free massive MIMO network. Each active device transmits a pre-assigned non-orthogonal pilot sequence to the APs and the APs send the received signals to a central processing unit (CPU) for joint activity detection. We formulate the maximum likelihood device activity detection problem and provide an algorithm based on coordinate descent method having affordable complexity. We show that the cell-free massive MIMO network can support low-powered mMTC devices and provide a broad coverage.      
### 38.LoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2105.10371.pdf)
>  Loops, seamlessly repeatable musical segments, are a cornerstone of modern music production. Contemporary artists often mix and match various sampled or pre-recorded loops based on musical criteria such as rhythm, harmony and timbral texture to create compositions. Taking such criteria into account, we present LoopNet, a feed-forward generative model for creating loops conditioned on intuitive parameters. We leverage Music Information Retrieval (MIR) models as well as a large collection of public loop samples in our study and use the Wave-U-Net architecture to map control parameters to audio. We also evaluate the quality of the generated audio and propose intuitive controls for composers to map the ideas in their minds to an audio loop.      
### 39.Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.10369.pdf)
>  Deep learning has achieved promising segmentation performance on 3D left atrium MR images. However, annotations for segmentation tasks are expensive, costly and difficult to obtain. In this paper, we introduce a novel hierarchical consistency regularized mean teacher framework for 3D left atrium segmentation. In each iteration, the student model is optimized by multi-scale deep supervision and hierarchical consistency regularization, concurrently. Extensive experiments have shown that our method achieves competitive performance as compared with full annotation, outperforming other stateof-the-art semi-supervised segmentation methods.      
### 40.Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification  [ :arrow_down: ](https://arxiv.org/pdf/2105.10340.pdf)
>  It is well known that the mismatch between training (source) and test (target) data distribution will significantly decrease the performance of acoustic scene classification (ASC) systems. To address this issue, domain adaptation (DA) is one solution and many unsupervised DA methods have been proposed. These methods focus on a scenario of single source domain to single target domain. However, we will face such problem that test data comes from multiple target domains. This problem can be addressed by producing one model per target domain, but this solution is too costly. In this paper, we propose a novel unsupervised multi-target domain adaption (MTDA) method for ASC, which can adapt to multiple target domains simultaneously and make use of the underlying relation among multiple domains. Specifically, our approach combines traditional adversarial adaptation with two novel discriminator tasks that learns a common subspace shared by all domains. Furthermore, we propose to divide the target domain into the easy-to-adapt and hard-to-adapt domain, which enables the system to pay more attention to hard-to-adapt domain in training. The experimental results on the DCASE 2020 Task 1-A dataset and the DCASE 2019 Task 1-B dataset show that our proposed method significantly outperforms the previous unsupervised DA methods.      
### 41.Contention-Aware GPU Partitioning and Task-to-Partition Allocation for Real-Time Workloads  [ :arrow_down: ](https://arxiv.org/pdf/2105.10312.pdf)
>  In order to satisfy timing constraints, modern real-time applications require massively parallel accelerators such as General Purpose Graphic Processing Units (GPGPUs). Generation after generation, the number of computing clusters made available in novel GPU architectures is steadily increasing, hence, investigating suitable scheduling approaches is now mandatory. Such scheduling approaches are related to mapping different and concurrent compute kernels within the GPU computing clusters, hence grouping GPU computing clusters into schedulable partitions. In this paper we propose novel techniques to define GPU partitions; this allows us to define suitable task-to-partition allocation mechanisms in which tasks are GPU compute kernels featuring different timing requirements. Such mechanisms will take into account the interference that GPU kernels experience when running in overlapping time windows. Hence, an effective and simple way to quantify the magnitude of such interference is also presented. We demonstrate the efficiency of the proposed approaches against the classical techniques that considered the GPU as a single, non-partitionable resource.      
### 42.Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices  [ :arrow_down: ](https://arxiv.org/pdf/2105.10288.pdf)
>  Single-Image Super Resolution (SISR) is a classical computer vision problem and it has been studied for over decades. With the recent success of deep learning methods, recent work on SISR focuses solutions with deep learning methodologies and achieves state-of-the-art results. However most of the state-of-the-art SISR methods contain millions of parameters and layers, which limits their practical applications. In this paper, we propose a hardware (Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization robust real-time super resolution network (XLSR). The proposed model's building block is inspired from root modules for Image classification. We successfully applied root modules to SISR problem, further more to make the model uint8 quantization robust we used Clipped ReLU at the last layer of the network and achieved great balance between reconstruction quality and runtime. Furthermore, although the proposed network contains 30x fewer parameters than VDSR its performance surpasses it on Div2K validation set. The network proved itself by winning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.      
### 43.A Lightweight Deep Network for Efficient CSI Feedback in Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.10283.pdf)
>  To fully exploit the advantages of massive multiple-input multiple-output (m-MIMO), accurate channel state information (CSI) is required at the transmitter. However, excessive CSI feedback for large antenna arrays is inefficient and thus undesirable in practical applications. By exploiting the inherent correlation characteristics of complex-valued channel responses in the angular-delay domain, we propose a novel neural network (NN) architecture, namely ENet, for CSI compression and feedback in m-MIMO. Even if the ENet processes the real and imaginary parts of the CSI values separately, its special structure enables the network trained for the real part only to be reused for the imaginary part. The proposed ENet shows enhanced performance with the network size reduced by nearly an order of magnitude compared to the existing NN-based solutions. Experimental results verify the effectiveness of the proposed ENet.      
### 44.AI-Based and Mobility-Aware Energy Efficient Resource Allocation and Trajectory Design for NFV Enabled Aerial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.10282.pdf)
>  In this paper, we propose a novel joint intelligent trajectory design and resource allocation algorithm based on user's mobility and their requested services for unmanned aerial vehicles (UAVs) assisted networks, where UAVs act as nodes of a network function virtualization (NFV) enabled network. Our objective is to maximize energy efficiency and minimize the average delay on all services by allocating the limited radio and NFV resources. In addition, due to the traffic conditions and mobility of users, we let some Virtual Network Functions (VNFs) to migrate from their current locations to other locations to satisfy the Quality of Service requirements. We formulate our problem to find near-optimal locations of UAVs, transmit power, subcarrier assignment, placement, and scheduling the requested service's functions over the UAVs and perform suitable VNF migration. Then we propose a novel Hierarchical Hybrid Continuous and Discrete Action (HHCDA) deep reinforcement learning method to solve our problem. Finally, the convergence and computational complexity of the proposed algorithm and its performance analyzed for different parameters. Simulation results show that our proposed HHCDA method decreases the request reject rate and average delay by 31.5% and 20% and increases the energy efficiency by 40% compared to DDPG method.      
### 45.Measure Concentration on the OFDM-based Random Access Channel  [ :arrow_down: ](https://arxiv.org/pdf/2105.10270.pdf)
>  It is well known that CS can boost massive random access protocols. Usually, the protocols operate in some overloaded regime where the sparsity can be exploited. In this paper, we consider a different approach by taking an orthogonal FFT base, subdivide its image into appropriate sub-channels and let each subchannel take only a fraction of the load. To show that this approach can actually achieve the full capacity we provide i) new concentration inequalities, and ii) devise a sparsity capture effect, i.e where the sub-division can be driven such that the activity in each each sub-channel is sparse by design. We show by simulations that the system is scalable resulting in a coarsely 30-fold capacity increase.      
### 46.Energy Minimized Federated Fog Computing over Passive Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.10242.pdf)
>  The rapid growth of time-sensitive applications and services has driven enhancements to computing infrastructures. The main challenge that needs addressing for these applications is the optimal placement of the end-users demands to reduce the total power consumption and delay. One of the widely adopted paradigms to address such a challenge is fog computing. Placing fog units close to end-users at the edge of the network can help mitigate some of the latency and energy efficiency issues. Compared to the traditional hyperscale cloud data centres, fog computing units are constrained by computational power, hence, the capacity of fog units plays a critical role in meeting the stringent demands of the end-users due to intensive processing workloads. In this paper, we aim to optimize the placement of virtual machines (VMs) demands originating from end-users in a fog computing setting by formulating a Mixed Integer Linear Programming (MILP) model to minimize the total power consumption through the use of a federated architecture made up of multiple distributed fog cells. The obtained results show an increase in processing capacity in the fog layer and a reduction in the power consumption by up to 26% compared to the Non-Federated fogs network.      
### 47.A scalable distributed dynamical systems approach to compute the strongly connected components and diameter of networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.10229.pdf)
>  Finding strongly connected components (SCCs) and the diameter of a directed network play a key role in a variety of discrete optimization problems, and subsequently, machine learning and control theory problems. On the one hand, SCCs are used in solving the 2-satisfiability problem, which has applications in clustering, scheduling, and visualization. On the other hand, the diameter has applications in network learning and discovery problems enabling efficient internet routing and searches, as well as identifying faults in the power grid. <br>In this paper, we leverage consensus-based principles to find the SCCs in a scalable and distributed fashion with a computational complexity of $\mathcal{O}\left(Dd_{\text{in-degree}}^{\max}\right)$, where $D$ is the (finite) diameter of the network and $d_{\text{in-degree}}^{\max}$ is the maximum in-degree of the network. Additionally, we prove that our algorithm terminates in $D+1$ iterations, which allows us to retrieve the diameter of the network. We illustrate the performance of our algorithm on several random networks, including Erdős-Rényi, Barabási-Albert, and \mbox{Watts-Strogatz} networks.      
### 48.Rotation invariant CNN using scattering transform for image classification  [ :arrow_down: ](https://arxiv.org/pdf/2105.10175.pdf)
>  Deep convolutional neural networks accuracy is heavily impacted by rotations of the input data. In this paper, we propose a convolutional predictor that is invariant to rotations in the input. This architecture is capable of predicting the angular orientation without angle-annotated data. Furthermore, the predictor maps continuously the random rotation of the input to a circular space of the prediction. For this purpose, we use the roto-translation properties existing in the Scattering Transform Networks with a series of 3D Convolutions. We validate the results by training with upright and randomly rotated samples. This allows further applications of this work on fields like automatic re-orientation of randomly oriented datasets.      
### 49.Anomaly Detection of Test-Time Evasion Attacks using Class-conditional Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.10101.pdf)
>  Deep Neural Networks (DNNs) have been shown vulnerable to adversarial (Test-Time Evasion (TTE)) attacks which, by making small changes to the input, alter the DNN's decision. We propose an attack detector based on class-conditional Generative Adversarial Networks (GANs). We model the distribution of clean data conditioned on the predicted class label by an Auxiliary Classifier GAN (ACGAN). Given a test sample and its predicted class, three detection statistics are calculated using the ACGAN Generator and Discriminator. Experiments on image classification datasets under different TTE attack methods show that our method outperforms state-of-the-art detection methods. We also investigate the effectiveness of anomaly detection using different DNN layers (input features or internal-layer features) and demonstrate that anomalies are harder to detect using features closer to the DNN's output layer.      
### 50.Direct Simultaneous Multi-Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2105.10087.pdf)
>  This paper presents a novel algorithm that registers a collection of mono-modal 3D images in a simultaneous fashion, named as Direct Simultaneous Registration (DSR). The algorithm optimizes global poses of local frames directly based on the intensities of images (without extracting features from the images). To obtain the optimal result, we start with formulating a Direct Bundle Adjustment (DBA) problem which jointly optimizes pose parameters of local frames and intensities of panoramic image. By proving the independence of the pose from panoramic image in the iterative process, DSR is proposed and proved to be able to generate the same optimal poses as DBA, but without optimizing the intensities of the panoramic image. The proposed DSR method is particularly suitable in mono-modal registration and in the scenarios where distinct features are not available, such as Transesophageal Echocardiography (TEE) images. The proposed method is validated via simulated and in-vivo 3D TEE images. It is shown that the proposed method outperforms conventional sequential registration method in terms of accuracy and the obtained results can produce good alignment in in-vivo images.      
### 51.Single-shot Compressed 3D Imaging by Exploiting Random Scattering and Astigmatism  [ :arrow_down: ](https://arxiv.org/pdf/2105.10073.pdf)
>  Based on point spread function (PSF) engineering and astigmatism due to a pair of cylindrical lenses, a novel compressed imaging mechanism is proposed to achieve single-shot incoherent 3D imaging. The speckle-like PSF of the imaging system is sensitive to axial shift, which makes it feasible to reconstruct a 3D image by solving an optimization problem with sparsity constraint. With the experimentally calibrated PSFs, the proposed method is demonstrated by a synthetic 3D point object and real 3D object, and the images in different axial slices can be reconstructed faithfully. Moreover, 3D multispectral compressed imaging is explored with the same system, and the result is rather satisfactory with a synthetic point object. Because of the inherent compatibility between the compression in spectral and axial dimensions, the proposed mechanism has the potential to be a unified framework for multi-dimensional compressed imaging.      
### 52.Distributionally Robust Surrogate Optimal Control for High-Dimensional Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.10070.pdf)
>  This paper presents a novel methodology for tractably solving optimal control problems for high-dimensional systems. This work is motivated by the ongoing challenge of computational complexity and optimality in high-dimensional non-convex optimal control. We address these key challenges with the following approach. First, we identify a surrogate modeling methodology which takes as input the initial state and a time series of control inputs, and outputs an approximation of the objective function and constraint functions. Importantly this approach entirely absorbs the individual state transition dynamics. Additionally, the dependence on the initial state means we can apply dimensionality reduction to compress this variable while retaining most of its information. Uncertainty in the surrogate objective may affect the result optimality. Critically, however, uncertainty in the surrogate constraint functions could lead to infeasibility, i.e. unsafe actions. Therefore, we apply Wasserstein ambiguity sets to "robustify" our surrogate modeling approach subject to worst-case out-of-sample modeling error based on the distribution of test data residuals. We demonstrate the efficacy of this combined approach through a case study of safe optimal fast charging of a high-dimensional lithium-ion battery model at low temperatures.      
### 53.Energy-Efficient Distributed Machine Learning in Cloud Fog Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.10048.pdf)
>  Massive amounts of data are expected to be generated by the billions of objects that form the Internet of Things (IoT). A variety of automated services such as monitoring will largely depend on the use of different Machine Learning (ML) algorithms. Traditionally, ML models are processed by centralized cloud data centers, where IoT readings are offloaded to the cloud via multiple networking hops in the access, metro, and core layers. This approach will inevitably lead to excessive networking power consumptions as well as Quality-of-Service (QoS) degradation such as increased latency. Instead, in this paper, we propose a distributed ML approach where the processing can take place in intermediary devices such as IoT nodes and fog servers in addition to the cloud. We abstract the ML models into Virtual Service Requests (VSRs) to represent multiple interconnected layers of a Deep Neural Network (DNN). Using Mixed Integer Linear Programming (MILP), we design an optimization model that allocates the layers of a DNN in a Cloud/Fog Network (CFN) in an energy efficient way. We evaluate the impact of DNN input distribution on the performance of the CFN and compare the energy efficiency of this approach to the baseline where all layers of DNNs are processed in the centralized Cloud Data Center (CDC).      
### 54.A Streaming End-to-End Framework For Spoken Language Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2105.10042.pdf)
>  End-to-end spoken language understanding (SLU) has recently attracted increasing interest. Compared to the conventional tandem-based approach that combines speech recognition and language understanding as separate modules, the new approach extracts users' intentions directly from the speech signals, resulting in joint optimization and low latency. Such an approach, however, is typically designed to process one intention at a time, which leads users to take multiple rounds to fulfill their requirements while interacting with a dialogue system. In this paper, we propose a streaming end-to-end framework that can process multiple intentions in an online and incremental way. The backbone of our framework is a unidirectional RNN trained with the connectionist temporal classification (CTC) criterion. By this design, an intention can be identified when sufficient evidence has been accumulated, and multiple intentions can be identified sequentially. We evaluate our solution on the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is about 97 % on all multi-intent settings. This result is comparable to the performance of the state-of-the-art non-streaming models, but is achieved in an online and incremental way. We also employ our model to a keyword spotting task using the Google Speech Commands dataset and the results are also highly promising.      
### 55.Model and Load Predictive Control for Design and Energy Management of Shipboard Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.10038.pdf)
>  In current Medium Voltage DC (MVDC) Shipboard Power Systems (SPSs), multiple sources exist to supply power to a common dc bus. Conventionally, the power management of such systems is performed by controlling Power Generation Modules (PGMs) which include fuel operated generators and underlying converters. Moreover, energy management is performed by the emerging single or hybrid Energy Storage Systems (ESSs). This paper presents a model and load predictive control framework for power and energy management of SPSs. Here, MPC with load prediction is used for three main objectives: (1) to request power and energy from generators and energy storage elements according to their individual State of Power (SOP) and ramp-rate limitations, (2) to consider and integrate the generator cost and degradation, and (3) to reach a specific parking (final) State of Charge (SOC) for the ESSs at the end of the prediction horizon. The solution of the optimization problem is demonstrated using MATLAB and the functionality of the control framework is validated in real-time simulation environment.      
### 56.XGBoost energy consumption prediction based on multi-system data HVAC  [ :arrow_down: ](https://arxiv.org/pdf/2105.09945.pdf)
>  The energy consumption of the HVAC system accounts for a significant portion of the energy consumption of the public building system, and using an efficient energy consumption prediction model can assist it in carrying out effective energy-saving transformation. Unlike the traditional energy consumption prediction model, this paper extracts features from large data sets using XGBoost, trains them separately to obtain multiple models, then fuses them with LightGBM's independent prediction results using MAE, infers energy consumption related variables, and successfully applies this model to the self-developed Internet of Things platform.      
