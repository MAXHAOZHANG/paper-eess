# ArXiv eess --Wed, 19 May 2021
### 1.A Millimeter-Wave Self-Mixing Array with Large Gain and Wide Angular Receiving Range  [ :arrow_down: ](https://arxiv.org/pdf/2105.08685.pdf)
>  The concept of self-mixing antenna arrays is presented and analyzed with respect to its beneficial behavior of large gain over a wide angular range. The large gain is attained by an antenna array with large element spacing, where all array element signals are combined approximately coherently over the entire angular receiving range. This functionality is achieved by the self-mixing principle, where an exact description via an intermediate frequency (IF) array factor is derived. For verification purposes, a 4 x 2 self-mixing array is fabricated and measured in the frequency range from 34 GHz to 39 GHz. A multiple-resonances millimeter-wave microstrip patch antenna has been especially developed to achieve large bandwidth and a wide angular receiving range. The broad beamwidth is achieved by two parasitic patches and suitable radiation characteristics of the resonant modes. The self-mixing of the receive signal is realized at each antenna element by a Schottky diode with an optimized operating point. The down-converted array element signals are then combined and measured at the IF. The receive power is increased significantly over a large angular range as compared to conventional array feeding techniques. The simulation results are verified by measurements, which show very good agreement.      
### 2.Evolutionary Algorithms for Multi-Objective Optimization of Drone Controller Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2105.08650.pdf)
>  Drones are effective for reducing human activity and interactions by performing tasks such as exploring and inspecting new environments, monitoring resources and delivering packages. Drones need a controller to maintain stability and to reach their goal. The most well-known drone controllers are proportional-integral-derivative (PID) and proportional-derivative (PD) controllers. However, the controller parameters need to be tuned and optimized. In this paper, we introduce the use of two evolutionary algorithms, biogeography-based optimization~(BBO) and particle swarm optimization (PSO), for multi-objective optimization (MOO) to tune the parameters of the PD controller of a drone. The combination of MOO, BBO, and PSO results in various methods for optimization: vector evaluated BBO and PSO, denoted as VEBBO and VEPSO; and non-dominated sorting BBO and PSO, denoted as NSBBO and NSPSO. The multi-objective cost function is based on tracking errors for the four states of the system. Two criteria for evaluating the Pareto fronts of the optimization methods, normalized hypervolume and relative coverage, are used to compare performance. Results show that NSBBO generally performs better than the other methods.      
### 3.Fast and Accurate Single-Image Depth Estimation on Mobile Devices, Mobile AI 2021 Challenge: Report  [ :arrow_down: ](https://arxiv.org/pdf/2105.08630.pdf)
>  Depth estimation is an important computer vision problem with many practical applications to mobile devices. While many solutions have been proposed for this task, they are usually very computationally expensive and thus are not applicable for on-device inference. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based depth estimation solutions that can demonstrate a nearly real-time performance on smartphones and IoT platforms. For this, the participants were provided with a new large-scale dataset containing RGB-depth image pairs obtained with a dedicated stereo ZED camera producing high-resolution depth maps for objects located at up to 50 meters. The runtime of all models was evaluated on the popular Raspberry Pi 4 platform with a mobile ARM-based Broadcom chipset. The proposed solutions can generate VGA resolution depth maps at up to 10 FPS on the Raspberry Pi 4 while achieving high fidelity results, and are compatible with any Android or Linux-based mobile devices. A detailed description of all models developed in the challenge is provided in this paper.      
### 4.Fast Camera Image Denoising on Mobile GPUs with Deep Learning, Mobile AI 2021 Challenge: Report  [ :arrow_down: ](https://arxiv.org/pdf/2105.08629.pdf)
>  Image denoising is one of the most critical problems in mobile photo processing. While many solutions have been proposed for this task, they are usually working with synthetic data and are too computationally expensive to run on mobile devices. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image denoising solution that can demonstrate high efficiency on smartphone GPUs. For this, the participants were provided with a novel large-scale dataset consisting of noisy-clean image pairs captured in the wild. The runtime of all models was evaluated on the Samsung Exynos 2100 chipset with a powerful Mali GPU capable of accelerating floating-point and quantized neural networks. The proposed solutions are fully compatible with any mobile GPU and are capable of processing 480p resolution images under 40-80 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper.      
### 5.Embedded Model Predictive Controller on Low-Cost Low-End Microcontroller for Electrical Drives  [ :arrow_down: ](https://arxiv.org/pdf/2105.08623.pdf)
>  It is very well-known that the implementation of Model Predictive Controller (MPC) on embedded platforms is challenging due to the computational complexities associated while solving an optimization problem. Although, there are many efficient embedded implementations existing by now, but for faster, more dynamic and non-linear control applications, there is no cost-effective and memory efficient embedded solutions. In this paper, we show the implementation of embedded explicit MPC for a motor speed control application on a lowcost 8 bit PIC 18 series microcontroller which costs only $5. The offset-free explicit MPC is designed for reference tracking, constraints handling, and disturbance rejection. The developed control law is exported to low-level C code and utilized in HIL co-simulations. We present the results of memory demand and control performance under various operating scenarios. The presented results show that the developed embedded MPC utilize about 40% of RAM and 92% of ROM for prediction horizon up to 3 samples. The performance of developed MPC is compared with the conventional PI controller. Overall results show that the presented approach is cost-effective, portable, and gives better performance than the PI controller.      
### 6.UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion with Ensemble Monte Carlo Dropout for COVID-19 Detection  [ :arrow_down: ](https://arxiv.org/pdf/2105.08590.pdf)
>  The COVID-19 (Coronavirus disease 2019) has infected more than 151 million people and caused approximately 3.17 million deaths around the world up to the present. The rapid spread of COVID-19 is continuing to threaten human's life and health. Therefore, the development of computer-aided detection (CAD) systems based on machine and deep learning methods which are able to accurately differentiate COVID-19 from other diseases using chest computed tomography (CT) and X-Ray datasets is essential and of immediate priority. Different from most of the previous studies which used either one of CT or X-ray images, we employed both data types with sufficient samples in implementation. On the other hand, due to the extreme sensitivity of this pervasive virus, model uncertainty should be considered, while most previous studies have overlooked it. Therefore, we propose a novel powerful fusion model named $UncertaintyFuseNet$ that consists of an uncertainty module: Ensemble Monte Carlo (EMC) dropout. The obtained results prove the effectiveness of our proposed fusion for COVID-19 detection using CT scan and X-Ray datasets. Also, our proposed $UncertaintyFuseNet$ model is significantly robust to noise and performs well with the previously unseen data. The source codes and models of this study are available at: <a class="link-external link-https" href="https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification" rel="external noopener nofollow">this https URL</a>.      
### 7.PNLSS Toolbox 1.0  [ :arrow_down: ](https://arxiv.org/pdf/2105.08534.pdf)
>  This is a demonstration of the PNLSS Toolbox 1.0. The toolbox is designed to identify polynomial nonlinear state-space models from data. Nonlinear state-space models can describe a wide range of nonlinear systems. An illustration is provided on experimental data of an electrical system mimicking the forced Duffing oscillator, and on numerical data of a nonlinear fluid dynamics problem.      
### 8.Handling Structural Mismatches in Real-time Opera Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2105.08531.pdf)
>  Algorithms for reliable real-time score following in live opera promise a lot of useful applications such as automatic subtitles display, or real-time video cutting in live streaming. Until now, such systems were based on the strong assumption that an opera performance follows the structure of the score linearly. However, this is rarely the case in practice, because of different opera versions and directors' cutting choices. In this paper, we propose a two-level solution to this problem. We introduce a real-time-capable, high-resolution (HR) tracker that can handle jumps or repetitions at specific locations provided to it. We then combine this with an additional low-resolution (LR) tracker that can handle all sorts of mismatches that can occur at any time, with some imprecision, and can re-direct the HR tracker if the latter is `lost' in the score. We show that the combination of the two improves tracking robustness in the presence of strong structural mismatches.      
### 9.Imaging through fog using quadrature lock-in discrimination  [ :arrow_down: ](https://arxiv.org/pdf/2105.08524.pdf)
>  We report experiments conducted in the field in the presence of fog, that were aimed at imaging under poor visibility. By means of intensity modulation at the source and two-dimensional quadrature lock-in detection by software at the receiver, a significant enhancement of the contrast-to-noise ratio was achieved in the imaging of beacons over hectometric distances. Further by illuminating the field of view with a modulated source, the technique helped reveal objects that were earlier obscured due to multiple scattering of light. This method, thus, holds promise of aiding in various forms of navigation under poor visibility due to fog.      
### 10.Decoupling multivariate functions using a non-parametric Filtered CPD approach  [ :arrow_down: ](https://arxiv.org/pdf/2105.08518.pdf)
>  Black-box model structures are dominated by large multivariate functions. Usually a generic basis function expansion is used, e.g. a polynomial basis, and the parameters of the function are tuned given the data. This is a pragmatic and often necessary step considering the black-box nature of the problem. However, having identified a suitable function, there is no need to stick to the original basis. So-called decoupling techniques aim at translating multivariate functions into an alternative basis, thereby both reducing the number of parameters and retrieving underlying structure. In this work a filtered canonical polyadic decomposition (CPD) is introduced. It is a non-parametric method which is able to retrieve decoupled functions even when facing non-unique decompositions. Tackling this obstacle paves the way for a large number of modelling applications.      
### 11.COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2105.08506.pdf)
>  Detecting COVID-19 in computed tomography (CT) or radiography images has been proposed as a supplement to the definitive RT-PCR test. We present a deep learning ensemble for detecting COVID-19 infection, combining slice-based (2D) and volume-based (3D) approaches. The 2D system detects the infection on each CT slice independently, combining them to obtain the patient-level decision via different methods (averaging and long-short term memory networks). The 3D system takes the whole CT volume to arrive to the patient-level decision in one step. A new high resolution chest CT scan dataset, called the IST-C dataset, is also collected in this work. The proposed ensemble, called IST-CovNet, obtains 90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting COVID-19 among normal controls and other types of lung pathologies; and 93.69% accuracy and 0.99 AUC score on the publicly available MosMed dataset that consists of COVID-19 scans and normal controls only. The system is deployed at Istanbul University Cerrahpasa School of Medicine.      
### 12.Multi-view Contrastive Coding of Remote Sensing Images at Pixel-level  [ :arrow_down: ](https://arxiv.org/pdf/2105.08501.pdf)
>  Our planet is viewed by satellites through multiple sensors (e.g., multi-spectral, Lidar and SAR) and at different times. Multi-view observations bring us complementary information than the single one. Alternatively, there are common features shared between different views, such as geometry and semantics. Recently, contrastive learning methods have been proposed for the alignment of multi-view remote sensing images and improving the feature representation of single sensor images by modeling view-invariant factors. However, these methods are based on the pretraining of the predefined tasks or just focus on image-level classification. Moreover, these methods lack research on uncertainty estimation. In this work, a pixel-wise contrastive approach based on an unlabeled multi-view setting is proposed to overcome this limitation. This is achieved by the use of contrastive loss in the feature alignment and uniformity between multi-view images. In this approach, a pseudo-Siamese ResUnet is trained to learn a representation that aims to align features from the shifted positive pairs and uniform the induced distribution of the features on the hypersphere. The learned features of multi-view remote sensing images are evaluated on a liner protocol evaluation and an unsupervised change detection task. We analyze key properties of the approach that make it work, finding that the requirement of shift equivariance ensured the success of the proposed approach and the uncertainty estimation of representations leads to performance improvements. Moreover, the performance of multi-view contrastive learning is affected by the choice of different sensors. Results demonstrate both improvements in efficiency and accuracy over the state-of-the-art multi-view contrastive methods.      
### 13.Deep Correlation Analysis for Audio-EEG Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2105.08492.pdf)
>  The electroencephalography (EEG), which is one of the easiest modes of recording brain activations in a non-invasive manner, is often distorted due to recording artifacts which adversely impacts the stimulus-response analysis. The most prominent techniques thus far attempt to improve the stimulus-response correlations using linear methods. In this paper, we propose a neural network based correlation analysis framework that significantly improves over the linear methods for auditory stimuli. A deep model is proposed for intra-subject audio-EEG analysis based on directly optimizing the correlation loss. Further, a neural network model with a shared encoder architecture is proposed for improving the inter-subject stimulus response correlations. These models attempt to suppress the EEG artifacts while preserving the components related to the stimulus. Several experiments are performed using EEG recordings from subjects listening to speech and music stimuli. In these experiments, we show that the deep models improve the Pearson correlation significantly over the linear methods (average absolute improvements of 7.4% in speech tasks and 29.3% in music tasks). We also analyze the impact of several model parameters on the stimulus-response correlation.      
### 14.Transfer learning approach to Classify the X-ray image that corresponds to corona disease Using ResNet50 pretrained by ChexNet  [ :arrow_down: ](https://arxiv.org/pdf/2105.08382.pdf)
>  Coronavirus adversely has affected people worldwide. There are common symptoms between the Covid19 virus disease and other respiratory diseases like pneumonia or Influenza. Therefore, diagnosing it fast is crucial not only to save patients but also to prevent it from spreading. One of the most reliant methods of diagnosis is through X-ray images of a lung. With the help of deep learning approaches, we can teach the deep model to learn the condition of an affected lung. Therefore, it can classify the new sample as if it is a Covid19 infected patient or not. In this project, we train a deep model based on ResNet50 pretrained by ImageNet dataset and CheXNet dataset. Based on the imbalanced CoronaHack Chest X-Ray dataset introducing by Kaggle we applied both binary and multi-class classification. Also, we compare the results when using Focal loss and Cross entropy loss.      
### 15.Inplace knowledge distillation with teacher assistant for improved training of flexible deep neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.08369.pdf)
>  Deep neural networks (DNNs) have achieved great success in various machine learning tasks. However, most existing powerful DNN models are computationally expensive and memory demanding, hindering their deployment in devices with low memory and computational resources or in applications with strict latency requirements. Thus, several resource-adaptable or flexible approaches were recently proposed that train at the same time a big model and several resource-specific sub-models. Inplace knowledge distillation (IPKD) became a popular method to train those models and consists in distilling the knowledge from a larger model (teacher) to all other sub-models (students). In this work a novel generic training method called IPKD with teacher assistant (IPKD-TA) is introduced, where sub-models themselves become teacher assistants teaching smaller sub-models. We evaluated the proposed IPKD-TA training method using two state-of-the-art flexible models (MSDNet and Slimmable MobileNet-V1) with two popular image classification benchmarks (CIFAR-10 and CIFAR-100). Our results demonstrate that the IPKD-TA is on par with the existing state of the art while improving it in most cases.      
### 16.Peer-to-Peer Energy Cooperation in Building Community over A Lossy Network  [ :arrow_down: ](https://arxiv.org/pdf/2105.08280.pdf)
>  Energy management of buildings is of vital importance for the urban low-carbon transition. This paper proposes a sustainable energy cooperation framework for the building community by communication-efficient peer-to-peer transaction. Firstly, the energy cooperation of buildings is formulated as a social welfare maximization problem, in which buildings may directly trade energy with neighbors. In addition, considering privacy concerns and communication losses arisen in peer-to-peer energy trading, a communication-failure-robust distributed algorithm is developed to achieve the optimal energy dispatch solutions. Finally, simulation results show that the proposed framework substantially reduces the total cost of the building community and the algorithm is robust to communication losses in the network when only part of links (even one link) are active during iterations.      
### 17.EchoCP: An Echocardiography Dataset in Contrast Transthoracic Echocardiography for Patent Foramen Ovale Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2105.08267.pdf)
>  Patent foramen ovale (PFO) is a potential separation between the septum, primum and septum secundum located in the anterosuperior portion of the atrial septum. PFO is one of the main factors causing cryptogenic stroke which is the fifth leading cause of death in the United States. For PFO diagnosis, contrast transthoracic echocardiography (cTTE) is preferred as being a more robust method compared with others. However, the current PFO diagnosis through cTTE is extremely slow as it is proceeded manually by sonographers on echocardiography videos. Currently there is no publicly available dataset for this important topic in the community. In this paper, we present EchoCP, as the first echocardiography dataset in cTTE targeting PFO diagnosis. <br>EchoCP consists of 30 patients with both rest and Valsalva maneuver videos which covers various PFO grades. We further establish an automated baseline method for PFO diagnosis based on the state-of-the-art cardiac chamber segmentation technique, which achieves 0.89 average mean Dice score, but only 0.70/0.67 mean accuracies for PFO diagnosis, leaving large room for improvement. We hope that the challenging EchoCP dataset can stimulate further research and lead to innovative and generic solutions that would have an impact in multiple domains. Our dataset is released.      
### 18.A time-domain nearfield frequency-invariant beamforming method  [ :arrow_down: ](https://arxiv.org/pdf/2105.08219.pdf)
>  Most existing beamforming methods are frequency-domain methods, and are designed for enhancing a farfield target source over a narrow frequency band. They have found diverse applications and are still under active development. However, they struggle to achieve desired performance if the target source is in the nearfield with a broadband output. This paper proposes a time-domain nearfield frequency-invariant beamforming method. The time-domain implementation makes the beamformer output suitable for further use by real-time applications, the nearfield focusing enables the beamforming method to suppress an interference even if it is in the same direction as the target source, and the frequency-invariant beampattern makes the beamforming method suitable for enhancing the target source over a broad frequency band. These three features together make the beamforming method suitable for real-time broadband nearfield source enhancement, such as speech enhancement in room environments. The beamformer design process is separated from the sound field measurement process, and such that a designed beamformer applies to sensor arrays with various structures. The beamformer design process is further simplified by decomposing it into several independent parts. Simulation results confirm the performance of the proposed beamforming method.      
### 19.Randomly Initialized Convolutional Neural Network for the Recognition of COVID-19 using X-ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.08199.pdf)
>  By the start of 2020, the novel coronavirus disease (COVID-19) has been declared a worldwide pandemic. Because of the severity of this infectious disease, several kinds of research have focused on combatting its ongoing spread. One potential solution to detect COVID-19 is by analyzing the chest X-ray images using Deep Learning (DL) models. In this context, Convolutional Neural Networks (CNNs) are presented as efficient techniques for early diagnosis. In this study, we propose a novel randomly initialized CNN architecture for the recognition of COVID-19. This network consists of a set of different-sized hidden layers created from scratch. The performance of this network is evaluated through two public datasets, which are the COVIDx and the enhanced COVID-19 datasets. Both of these datasets consist of 3 different classes of images: COVID19, pneumonia, and normal chest X-ray images. The proposed CNN model yields encouraging results with 94% and 99% of accuracy for COVIDx and enhanced COVID-19 dataset, respectively.      
### 20.Transfer Learning Enhanced Generative Adversarial Networks for Multi-Channel MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2105.08175.pdf)
>  Deep learning based generative adversarial networks (GAN) can effectively perform image reconstruction with under-sampled MR data. In general, a large number of training samples are required to improve the reconstruction performance of a certain model. However, in real clinical applications, it is difficult to obtain tens of thousands of raw patient data to train the model since saving k-space data is not in the routine clinical flow. Therefore, enhancing the generalizability of a network based on small samples is urgently needed. In this study, three novel applications were explored based on parallel imaging combined with the GAN model (PI-GAN) and transfer learning. The model was pre-trained with public Calgary brain images and then fine-tuned for use in (1) patients with tumors in our center; (2) different anatomies, including knee and liver; (3) different k-space sampling masks with acceleration factors (AFs) of 2 and 6. As for the brain tumor dataset, the transfer learning results could remove the artifacts found in PI-GAN and yield smoother brain edges. The transfer learning results for the knee and liver were superior to those of the PI-GAN model trained with its own dataset using a smaller number of training cases. However, the learning procedure converged more slowly in the knee datasets compared to the learning in the brain tumor datasets. The reconstruction performance was improved by transfer learning both in the models with AFs of 2 and 6. Of these two models, the one with AF=2 showed better results. The results also showed that transfer learning with the pre-trained model could solve the problem of inconsistency between the training and test datasets and facilitate generalization to unseen data.      
### 21.Accelerating 3D MULTIPLEX MRI Reconstruction with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2105.08163.pdf)
>  Multi-contrast MRI images provide complementary contrast information about the characteristics of anatomical structures and are commonly used in clinical practice. Recently, a multi-flip-angle (FA) and multi-echo GRE method (MULTIPLEX MRI) has been developed to simultaneously acquire multiple parametric images with just one single scan. However, it poses two challenges for MULTIPLEX to be used in the 3D high-resolution setting: a relatively long scan time and the huge amount of 3D multi-contrast data for reconstruction. Currently, no DL based method has been proposed for 3D MULTIPLEX data reconstruction. We propose a deep learning framework for undersampled 3D MRI data reconstruction and apply it to MULTIPLEX MRI. The proposed deep learning method shows good performance in image quality and reconstruction time.      
### 22.A mm-Wave Patch Antenna with Broad Bandwidth and a Wide Angular Range  [ :arrow_down: ](https://arxiv.org/pdf/2105.08162.pdf)
>  A novel mm-wave microstrip-fed patch antenna with broad bandwidth and wide angular coverage suitable for integration in planar arrays is designed, analyzed and verified by measurements. The antenna provides a bandwidth of 13.1% between 34.1 GHz and 38.9 GHz, which is achieved by a slotted multiple resonances microstrip patch and a matching circuit in microstrip technology. The antenna is built on RO3003 substrate with top and ground layers, which is low cost compared to other techniques. For simple integration with microstrip and frontend circuits, the feeding happens in the top layer with a microstrip coupling gap feed. The wide half power beamwidth is achieved by suitably designed parasitic patches for the first resonant mode. The second resonant mode has a wide half power beamwidth by default. The half power beamwidth is between 100° and 125° within the matched bandwidth, which is a very good value for a microstrip patch antenna radiating over a ground plane. The measured input impedance and radiation characteristic show very good agreement with simulation results.      
### 23.Cardiac Functional Analysis with Cine MRI via Deep Learning Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2105.08157.pdf)
>  Retrospectively gated cine (retro-cine) MRI is the clinical standard for cardiac functional analysis. Deep learning (DL) based methods have been proposed for the reconstruction of highly undersampled MRI data and show superior image quality and magnitude faster reconstruction time than CS-based methods. Nevertheless, it remains unclear whether DL reconstruction is suitable for cardiac function analysis. To address this question, in this study we evaluate and compare the cardiac functional values (EDV, ESV and EF for LV and RV, respectively) obtained from highly accelerated MRI acquisition using DL based reconstruction algorithm (DL-cine) with values from CS-cine and conventional retro-cine. To the best of our knowledge, this is the first work to evaluate the cine MRI with deep learning reconstruction for cardiac function analysis and compare it with other conventional methods. The cardiac functional values obtained from cine MRI with deep learning reconstruction are consistent with values from clinical standard retro-cine MRI.      
### 24.COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs  [ :arrow_down: ](https://arxiv.org/pdf/2105.08147.pdf)
>  Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently obtained to determine the extent of lung disease and are a valuable source of data for creating artificial intelligence models. Most work to date assessing disease severity on chest imaging has focused on segmenting computed tomography (CT) images; however, given that CTs are performed much less frequently than chest X-rays for COVID-19 patients, automated lung lesion segmentation on chest X-rays could be clinically valuable. There currently exists a universal shortage of chest X-rays with ground truth COVID-19 lung lesion annotations, and manually contouring lung opacities is a tedious, labor-intensive task. To accelerate severity detection and augment the amount of publicly available chest X-ray training data for supervised deep learning (DL) models, we leverage existing annotated CT images to generate frontal projection "chest X-ray" images for training COVID-19 chest X-ray models. In this paper, we propose an automated pipeline for segmentation of COVID-19 lung lesions on chest X-rays comprised of a Mask R-CNN trained on a mixed dataset of open-source chest X-rays and coronal X-ray projections computed from annotated volumetric CTs. On a test set containing 40 chest X-rays of COVID-19 positive patients, our model achieved IoU scores of 0.81 $\pm$ 0.03 and 0.79 $\pm$ 0.03 when trained on a dataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50 projections from CTs, respectively. Our model far outperforms current baselines with limited supervised training and may assist in automated COVID-19 severity quantification on chest X-rays.      
### 25.On exploration requirements for learning safety constraints  [ :arrow_down: ](https://arxiv.org/pdf/2105.08143.pdf)
>  Enforcing safety for dynamical systems is challenging, since it requires constraint satisfaction along trajectory predictions. Equivalent control constraints can be computed in the form of sets that enforce positive invariance, and can thus guarantee safety in feedback controllers without predictions. However, these constraints are cumbersome to compute from models, and it is not yet well established how to infer constraints from data. In this paper, we shed light on the key objects involved in learning control constraints from data in a model-free setting. In particular, we discuss the family of constraints that enforce safety in the context of a nominal control policy, and expose that these constraints do not need to be accurate everywhere. They only need to correctly exclude a subset of the state-actions that would cause failure, which we call the critical set.      
### 26.A Data-Efficient Approach to Behind-the-Meter Solar Generation Disaggregation  [ :arrow_down: ](https://arxiv.org/pdf/2105.08122.pdf)
>  With the emergence of cost effective battery storage and the decline in the solar photovoltaic (PV) levelized cost of energy (LCOE), the number of behind-the-meter solar PV systems is expected to increase steadily. The ability to estimate solar generation from these latent systems is crucial for a range of applications, including distribution system planning and operation, demand response, and non-intrusive load monitoring (NILM). This paper investigates the problem of disaggregating solar generation from smart meter data when historical disaggregated data from the target home is unavailable, and deployment characteristics of the PV system are unknown. The proposed approach entails inferring the physical characteristics from smart meter data and disaggregating solar generation using an iterative algorithm. This algorithm takes advantage of solar generation data (aka proxy measurements) from a few sites that are located in the same area as the target home, and solar generation data synthesized using a physical PV model. We evaluate our methods with 4 different proxy settings on around 160 homes in the United States and Australia, and show that the solar disaggregation accuracy is improved by 32.31% and 15.66% over two state-of-the-art methods using only one real proxy along with three synthetic proxies. Furthermore, we demonstrate that using the disaggregated home load rather than the net load data could improve the overall accuracy of three popular NILM methods by at least 22%.      
### 27.Robust Doppler Filter Design for Joint RadCom Systems Based on Spatial Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2105.08060.pdf)
>  This paper analyses the effect of beampattern variation on clutter modulation for joint radar and communication (RadCom) systems based on spatial modulation. Besides, this paper considers the design of robust doppler filter for joint RadCom system when the receive temporal steering vector is mismatched with the desired temporal steering vector. This paper aims to maximize the worst Signal-to-Interference-plus-Noise-Ratio (SINR) of the doppler filter in the doppler interval under the similarity constraint and constant white noise gain constraint. In order to solve the nonconvex optimization problem, we relax the original optimization problem to a semidefinite program (SDP), which is convex and can be solved using CVX toolbox. Lots of numerical results are presented to show the robustness of the designed doppler filter.      
### 28.Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2105.08059.pdf)
>  Supervised deep learning has swiftly become a workhorse for accelerated MRI in recent years, offering state-of-the-art performance in image reconstruction from undersampled acquisitions. Training deep supervised models requires large datasets of undersampled and fully-sampled acquisitions typically from a matching set of subjects. Given scarce access to large medical datasets, this limitation has sparked interest in unsupervised methods that reduce reliance on fully-sampled ground-truth data. A common framework is based on the deep image prior, where network-driven regularization is enforced directly during inference on undersampled acquisitions. Yet, canonical convolutional architectures are suboptimal in capturing long-range relationships, and randomly initialized networks may hamper convergence. To address these limitations, here we introduce a novel unsupervised MRI reconstruction method based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a deep adversarial network with cross-attention transformer blocks to map noise and latent variables onto MR images. This unconditional network learns a high-quality MRI prior in a self-supervised encoding task. A zero-shot reconstruction is performed on undersampled test data, where inference is performed by optimizing network parameters, latent and noise variables to ensure maximal consistency to multi-coil MRI data. Comprehensive experiments on brain MRI datasets clearly demonstrate the superior performance of SLATER against several state-of-the-art unsupervised methods.      
### 29.A parameter refinement method for Ptychography based on Deep Learning concepts  [ :arrow_down: ](https://arxiv.org/pdf/2105.08058.pdf)
>  X-ray Ptychography is an advanced computational microscopy technique which is delivering exceptionally detailed quantitative imaging of biological and nanotechnology specimens. However coarse parametrisation in propagation distance, position errors and partial coherence frequently menaces the experiment viability. In this work we formally introduced these actors, solving the whole reconstruction as an optimisation problem. A modern Deep Learning framework is used to correct autonomously the setup incoherences, thus improving the quality of a ptychography reconstruction. Automatic procedures are indeed crucial to reduce the time for a reliable analysis, which has a significant impact on all the fields that use this kind of microscopy. We implemented our algorithm in our software framework, SciComPty, releasing it as open-source. We tested our system on both synthetic datasets and also on real data acquired at the TwinMic beamline of the Elettra synchrotron facility.      
### 30.Timely Private Information Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2105.08713.pdf)
>  We introduce the problem of \emph{timely} private information retrieval (PIR) from $N$ non-colluding and replicated servers. In this problem, a user desires to retrieve a message out of $M$ messages from the servers, whose contents are continuously updating. The retrieval process should be executed in a timely manner such that no information is leaked about the identity of the message. To assess the timeliness, we use the \emph{age of information} (AoI) metric. Interestingly, the timely PIR problem reduces to an AoI minimization subject to PIR constraints under \emph{asymmetric traffic}. We explicitly characterize the optimal tradeoff between the PIR rate and the AoI metric (peak AoI or average AoI) for the case of $N=2$, $M=3$. Further, we provide some structural insights on the general problem with arbitrary $N$, $M$.      
### 31.A pragmatic approach for designing transparent WDM optical networks with multi-objectives  [ :arrow_down: ](https://arxiv.org/pdf/2105.08588.pdf)
>  In facing with the explosive Internet traffic growth, optical transport networks based on WDM technologies forming the core part of Internet infrastructure carrying multi-Tb/s has to be re-considered from both designing, planning, operation and management perspectives to attain greater efficiency. Thanks to the convergence of significant advances in optical transmission technologies, and photonic switching, transparent (all-optical) architecture has come into practice, paving the way for eliminating the over-utilization of costly optical-electrical-optical (O-E-O) interfaces and hence, yielding remarkable savings of cost and energy consumption compared to opaque architecture. Traditional designs for transparent optical networks based on single-objective optimization model aiming at optimizing solely a single performance metric appears to be insufficient to capture the nuances of practical designs while conventional multi-objective approach tends to reach (non-) optimal solutions. Different from existing works, we present a new framework for multi-objective WDM network designs capturing several goals on one hand and on the other hand, achieving optimal solutions. Moreover, our proposal exploits the characteristics of each constituent objectives to lay the foundation for setting up weight coefficient so that the order of optimization is guaranteed. Equally important, our proposal is pragmatic in the sense that the complexity of the optimization model remains the same as the single-objective model while the quality of solution has been greatly improved. We have extensively tested realistic optical core networks topologies, that is, COST239 and NSFNET, with various network traffic conditions and it turns out that our design brings about a saving of wavelength link usage up to roughly $28\%$ in the most favorable cases while $14\%$ is expected for the least favorable cases.      
### 32.Dependent Multi-Task Learning with Causal Intervention for Image Captioning  [ :arrow_down: ](https://arxiv.org/pdf/2105.08573.pdf)
>  Recent work for image captioning mainly followed an extract-then-generate paradigm, pre-extracting a sequence of object-based features and then formulating image captioning as a single sequence-to-sequence task. Although promising, we observed two problems in generated captions: 1) content inconsistency where models would generate contradicting facts; 2) not informative enough where models would miss parts of important information. From a causal perspective, the reason is that models have captured spurious statistical correlations between visual features and certain expressions (e.g., visual features of "long hair" and "woman"). In this paper, we propose a dependent multi-task learning framework with the causal intervention (DMTCI). Firstly, we involve an intermediate task, bag-of-categories generation, before the final task, image captioning. The intermediate task would help the model better understand the visual features and thus alleviate the content inconsistency problem. Secondly, we apply Pearl's do-calculus on the model, cutting off the link between the visual features and possible confounders and thus letting models focus on the causal visual features. Specifically, the high-frequency concept set is considered as the proxy confounders where the real confounders are inferred in the continuous space. Finally, we use a multi-agent reinforcement learning (MARL) strategy to enable end-to-end training and reduce the inter-task error accumulations. The extensive experiments show that our model outperforms the baseline models and achieves competitive performance with state-of-the-art models.      
### 33.Federated Learning With Highly Imbalanced Audio Data  [ :arrow_down: ](https://arxiv.org/pdf/2105.08550.pdf)
>  Federated learning (FL) is a privacy-preserving machine learning method that has been proposed to allow training of models using data from many different clients, without these clients having to transfer all their data to a central server. There has as yet been relatively little consideration of FL or other privacy-preserving methods in audio. In this paper, we investigate using FL for a sound event detection task using audio from the FSD50K dataset. Audio is split into clients based on uploader metadata. This results in highly imbalanced subsets of data between clients, noted as a key issue in FL scenarios. A series of models is trained using `high-volume' clients that contribute 100 audio clips or more, testing the effects of varying FL parameters, followed by an additional model trained using all clients with no minimum audio contribution. It is shown that FL models trained using the high-volume clients can perform similarly to a centrally-trained model, though there is much more noise in results than would typically be expected for a centrally-trained model. The FL model trained using all clients has a considerably reduced performance compared to the centrally-trained model.      
### 34.Decoupling P-NARX models using filtered CPD  [ :arrow_down: ](https://arxiv.org/pdf/2105.08530.pdf)
>  Nonlinear Auto-Regressive eXogenous input (NARX) models are a popular class of nonlinear dynamical models. Often a polynomial basis expansion is used to describe the internal multivariate nonlinear mapping (P-NARX). Resorting to fixed basis functions is convenient since it results in a closed form solution of the estimation problem. The drawback, however, is that the predefined basis does not necessarily lead to a sparse representation of the relationship, typically resulting in very large numbers of parameters. So-called decoupling techniques were specifically designed to reduce large multivariate functions. It was found that, often, a more efficient parameterisation can be retrieved by rotating towards a new basis. Characteristic to the decoupled structure is that, expressed in the new basis, the relationship is structured such that only single-input single-output nonlinear functions are required. Classical decoupling techniques are unfit to deal with the case of single-output NARX models. In this work, this limitation is overcome by adopting the filtered CPD decoupling method of Decuyper et al. (2021b). The approach is illustrated on data from the Sliverbox benchmark: measurement data from an electronic circuit implementation of a forced Duffing oscillator.      
### 35.Privacy-Preserving Constrained Domain Generalization for Medical Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2105.08511.pdf)
>  Deep neural networks (DNN) have demonstrated unprecedented success for medical imaging applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for patient privacy protection, the broad applications of medical imaging classification driven by DNN with large-scale training data have been largely hindered. For example, when training the DNN from one domain (e.g., with data only from one hospital), the generalization capability to another domain (e.g., data from another hospital) could be largely lacking. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, We propose to improve the information aggregation process on the centralized server-side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the "unseen" but related medical images. The rationale and effectiveness of our proposed method can be explained by connecting our proposed method with the Maximum Mean Discrepancy (MMD) which has been widely adopted as the distribution distance measurement. Experimental results on two challenging medical imaging classification tasks indicate that our method can achieve better cross-domain generalization capability compared to the state-of-the-art federated learning methods.      
### 36.IRS-Aided Wireless Relaying: Optimal Deployment and Capacity Scaling  [ :arrow_down: ](https://arxiv.org/pdf/2105.08495.pdf)
>  In this letter, we consider an intelligent reflecting surface (IRS)-aided wireless relaying system, where a decode-and-forward relay (R) is employed to forward data from a source (S) to a destination (D), aided by M passive reflecting elements. We consider two practical IRS deployment strategies, namely, single-IRS deployment where all reflecting elements are mounted on one single IRS that is deployed near S, R, or D, and multi-IRS deployment where the reflecting elements are allocated over three separate IRSs which are deployed near S, R, and D, respectively. Under the line-of-sight (LoS) channel model, we characterize the capacity scaling orders with respect to an increasing M for the IRS-aided relay system with different IRS deployment strategies. For single-IRS deployment, we show that deploying the IRS near R achieves the highest capacity as compared to that near S or D. While for multi-IRS deployment, we propose a practical cooperative IRS passive beamforming design which is analytically shown to achieve a larger capacity scaling order than the optimal single-IRS deployment (i.e., near R) when M is sufficiently large. Numerical examples are provided, which validate our theoretical results.      
### 37.Utility-Based Precoding Optimization Framework for Large Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2105.08477.pdf)
>  The spectral efficiency of wireless networks can be made nearly infinitely large by deploying many antennas, but the deployment of very many antennas requires new topologies beyond the compact and discrete antenna arrays used by conventional base stations. In this paper, we consider the large intelligent surface scenario where small antennas are deployed on a large and dense two-dimensional grid. Building on the heritage of MIMO, we first analyze the beamwidth and sidelobes when transmitting from large intelligent surfaces. We compare different precoding schemes and determine how to optimize the transmit power with respect to different utility functions.      
### 38.Predictor-Based Output Feedback Stabilization of an Input Delayed Parabolic PDE with Boundary Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2105.08431.pdf)
>  This paper is concerned with the output feedback boundary stabilization of general 1-D reaction diffusion PDEs in the presence of an arbitrarily large input delay. We consider the cases of Dirichlet/Neumann/Robin boundary conditions for the both boundary control and boundary condition. The boundary measurement takes the form of a either Dirichlet or Neumann trace. The adopted control strategy is composed of a finite-dimensional observer estimating the first modes of the PDE coupled with a predictor to compensate the input delay. In this context, we show for any arbitrary value of the input delay that the control strategy achieves the exponential stabilization of the closed-loop system, for system trajectories evaluated in $H^1$ norm (also in $L^2$ norm in the case of a Dirichlet boundary measurement), provided the dimension of the observer is selected large enough. The reported proof of this result requires to perform both control design and stability analysis using simultaneously the (non-homogeneous) original version of the PDE and one of its equivalent homogeneous representations.      
### 39.A sparse ADMM-based solver for linear MPC subject to terminal quadratic constraint  [ :arrow_down: ](https://arxiv.org/pdf/2105.08419.pdf)
>  This paper presents a sparse solver based on the alternating direction method of multipliers algorithm for a linear model predictive control (MPC) formulation in which the terminal state is constrained to a given ellipsoid. The motivation behind this solver is to substitute the typical polyhedral invariant set used as a terminal constraint in many nominal and robust linear MPC formulations with an invariant set in the form of an ellipsoid, which is (typically) much easier to compute and results in an optimization problem with significantly fewer constraints, even for average-sized systems. However, this optimization problem is no longer the quadratic programming problem found in most linear MPC approaches, thus meriting the development of a tailored solver. The proposed solver is suitable for its use in embedded systems, since it is sparse, has a small memory footprint and requires no external libraries. We show the results of its implementation in an embedded system to control a simulated multivariable plant, comparing it against other alternatives.      
### 40.Nonlinear Boundary Output Feedback Stabilization of Reaction Diffusion PDEs  [ :arrow_down: ](https://arxiv.org/pdf/2105.08418.pdf)
>  This paper studies the design of a finite-dimensional output feedback controller for the stabilization of a reaction-diffusion equation in the presence of a sector nonlinearity in the boundary input. Due to the input nonlinearity, classical approaches relying on the transfer of the control from the boundary into the domain with explicit occurrence of the time-derivative of the control cannot be applied. In this context, we first demonstrate using Lyapunov direct method how a finite-dimensional observer-based controller can be designed, without using the time derivative of the boundary input as an auxiliary command, in order to achieve the boundary stabilization of general 1-D reaction-diffusion equations with Robin boundary conditions and a measurement selected as a Dirichlet trace. We extend this approach to the case of a control applying at the boundary through a sector nonlinearity. We show from the derived stability conditions the existence of a size of the sector (in which the nonlinearity is confined) so that the stability of the closed-loop system is achieved when selecting the dimension of the observer large enough.      
### 41.Data-driven distributionally robust MPC using the Wasserstein metric  [ :arrow_down: ](https://arxiv.org/pdf/2105.08414.pdf)
>  A data-driven MPC scheme is proposed to safely control constrained stochastic linear systems using distributionally robust optimization. Distributionally robust constraints based on the Wasserstein metric are imposed to bound the state constraint violations in the presence of process disturbance. A feedback control law is solved to guarantee that the predicted states comply with constraints. The stochastic constraints are satisfied with regard to the worst-case distribution within the Wasserstein ball centered at their discrete empirical probability distribution. The resulting distributionally robust MPC framework is computationally tractable and efficient, as well as recursively feasible. The innovation of this approach is that all the information about the uncertainty can be determined empirically from the data. The effectiveness of the proposed scheme is demonstrated through numerical case studies.      
### 42.Relative Positional Encoding for Transformers with Linear Complexity  [ :arrow_down: ](https://arxiv.org/pdf/2105.08399.pdf)
>  Recent advances in Transformer models allow for unprecedented sequence lengths, due to linear space and time complexity. In the meantime, relative positional encoding (RPE) was proposed as beneficial for classical Transformers and consists in exploiting lags instead of absolute positions for inference. Still, RPE is not available for the recent linear-variants of the Transformer, because it requires the explicit computation of the attention matrix, which is precisely what is avoided by such methods. In this paper, we bridge this gap and present Stochastic Positional Encoding as a way to generate PE that can be used as a replacement to the classical additive (sinusoidal) PE and provably behaves like RPE. The main theoretical contribution is to make a connection between positional encoding and cross-covariance structures of correlated Gaussian processes. We illustrate the performance of our approach on the Long-Range Arena benchmark and on music generation.      
### 43.Deep Learning-based Physical-Layer Secret Key Generation for FDD Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.08364.pdf)
>  Physical-layer key generation (PKG) establishes cryptographic keys from highly correlated measurements of wireless channels, which relies on reciprocal channel characteristics between uplink and downlink, is a promising wireless security technique for Internet of Things (IoT). However, it is challenging to extract common features in frequency division duplexing (FDD) systems as uplink and downlink transmissions operate at different frequency bands whose channel frequency responses are not reciprocal any more. Existing PKG methods for FDD systems have many limitations, i.e., high overhead and security problems. This paper proposes a novel PKG scheme that uses the feature mapping function between different frequency bands obtained by deep learning to make two users generate highly similar channel features in FDD systems. In particular, this is the first time to apply deep learning for PKG in FDD systems. We first prove the existence of the band feature mapping function for a given environment and a feedforward network with a single hidden layer can approximate the mapping function. Then a Key Generation neural Network (KGNet) is proposed for reciprocal channel feature construction, and a key generation scheme based on the KGNet is also proposed. Numerical results verify the excellent performance of the KGNet-based key generation scheme in terms of randomness, key generation ratio, and key error rate, which proves that it is feasible to generate keys for FDD systems with lower overhead and more secure functions compared to existing methods.      
### 44.Generic Reversible Visible Watermarking Via Regularized Graph Fourier Transform Coding  [ :arrow_down: ](https://arxiv.org/pdf/2105.08350.pdf)
>  Reversible visible watermarking (RVW) is an active copyright protection mechanism. It not only transparently superimposes copyright patterns on specific positions of digital images or video frames to declare the copyright ownership information, but also completely erases the visible watermark image and thus enables restoring the original host image without any distortion. However, existing RVW algorithms mostly construct the reversible mapping mechanism for a specific visible watermarking scheme, which is not general. Hence, we propose a generic RVW framework to accommodate various visible watermarking schemes, which is based on Regularized Graph Fourier Transform (GFT) coding. In particular, we obtain a reconstruction data packet -- the compressed difference image between the watermarked image and the original host image, which is embedded into the watermarked image via any conventional reversible data hiding method to facilitate the blind recovery of the host image. The key is to achieve compact compression of the difference image for efficient embedding of the reconstruction data packet. To this end, we propose regularized GFT coding, where the difference image is smoothed via the graph Laplacian regularizer for more efficient compression and then encoded by multi-resolution GFTs in an approximately optimal manner. Experimental results show that the proposed method achieves the state-of-the-art performance with high data compression efficiency, which is applicable to both gray-scale and color images. In addition, the proposed generic framework accommodates various visible watermarking algorithms, which demonstrates strong versatility.      
### 45.Adaptive Video Encoding For Different Video Codecs  [ :arrow_down: ](https://arxiv.org/pdf/2105.08191.pdf)
>  By 2022, we expect video traffic to reach 82% of the total internet traffic. Undoubtedly, the abundance of video-driven applications will likely lead internet video traffic percentage to a further increase in the near future, enabled by associate advances in video devices' capabilities. In response to this ever-growing demand, the Alliance for Open Media (AOM) and the Joint Video Experts Team (JVET) have demonstrated strong and renewed interest in developing new video codecs. In the fast-changing video codecs' landscape, there is thus, a genuine need to develop adaptive methods that can be universally applied to different codecs. In this study, we formulate video encoding as a multi-objective optimization process where video quality (as a function of VMAF and PSNR), bitrate demands, and encoding rate (in encoded frames per second) are jointly optimized, going beyond the standard video encoding approaches that focus on rate control targeting specific bandwidths. More specifically, we create a dense video encoding space (offline) and then employ regression to generate forward prediction models for each one of the afore-described optimization objectives, using only Pareto-optimal points. We demonstrate our adaptive video encoding approach that leverages the generated forward prediction models that qualify for real-time adaptation using different codecs (e.g., SVT-AV1 and x265) for a variety of video datasets and resolutions. To motivate our approach and establish the promise for future fast VVC encoders, we also perform a comparative performance evaluation using both subjective and objective metrics and report on bitrate savings among all possible pairs between VVC, SVT-AV1, x265, and VP9 codecs.      
### 46.Point-based Acoustic Scattering for Interactive Sound Propagation via Surface Encoding  [ :arrow_down: ](https://arxiv.org/pdf/2105.08177.pdf)
>  We present a novel geometric deep learning method to compute the acoustic scattering properties of geometric objects. Our learning algorithm uses a point cloud representation of objects to compute the scattering properties and integrates them with ray tracing for interactive sound propagation in dynamic scenes. We use discrete Laplacian-based surface encoders and approximate the neighborhood of each point using a shared multi-layer perceptron. We show that our formulation is permutation invariant and present a neural network that computes the scattering function using spherical harmonics. Our approach can handle objects with arbitrary topologies and deforming models, and takes less than 1ms per object on a commodity GPU. We have analyzed the accuracy and perform validation on thousands of unseen 3D objects and highlight the benefits over other point-based geometric deep learning methods. To the best of our knowledge, this is the first real-time learning algorithm that can approximate the acoustic scattering properties of arbitrary objects with high accuracy.      
### 47.Zero Dynamics, Pendulum Models, and Angular Momentum in Feedback Control of Bipedal Locomotion  [ :arrow_down: ](https://arxiv.org/pdf/2105.08170.pdf)
>  Low-dimensional models are ubiquitous in the bipedal robotics literature. On the one hand, are the simplified pendulum models selected to capture the center of mass dynamics. On the other hand, is the passive low-dimensional model induced by virtual constraints. In the first case, the low-dimensional model is valued for its physical insight and analytical tractability. In the second case, the low-dimensional model is integral to a rigorous analysis of the stability of walking gaits in the full-dimensional model of the robot. This paper brings these two approaches together, clarifying their commonalities and differences. In the process of doing so, we argue that angular momentum about the contact point is a better indicator of robot state than linear velocity. Concretely, we show that an approximate (pendulum and zero dynamics) model parameterized by angular momentum is more accurate on a physical robot (e.g., legs with mass) than is a related approximate model parameterized in terms of linear velocity. We implement an associated angular-momentum-based controller on Cassie, a 3D robot, and demonstrate high agility and robustness in experiments.      
### 48.Safe Occlusion-aware Autonomous Driving via Game-Theoretic Active Perception  [ :arrow_down: ](https://arxiv.org/pdf/2105.08169.pdf)
>  Autonomous vehicles interacting with other traffic participants heavily rely on the perception and prediction of other agents' behaviors to plan safe trajectories. However, as occlusions limit the vehicle's perception ability, reasoning about potential hazards beyond the field-of-view is one of the most challenging issues in developing autonomous driving systems. This paper introduces a novel analytical approach that poses the problem of safe trajectory planning under occlusions as a hybrid zero-sum dynamic game between the autonomous vehicle (evader), and an initially hidden traffic participant (pursuer). Due to occlusions, the pursuer's state is initially unknown to the evader and may later be discovered by the vehicle's sensors. The analysis yields optimal strategies for both players as well as the set of initial conditions from which the autonomous vehicle is guaranteed to avoid collisions. We leverage this theoretical result to develop a novel trajectory planning framework for autonomous driving that provides worst-case safety guarantees while minimizing conservativeness by accounting for the vehicle's ability to actively avoid other road users as soon as they are detected in future observations. Our framework is agnostic to the driving environment and suitable for various motion planners. We demonstrate our algorithm on challenging urban and highway driving scenarios using the open-source CARLA simulator. The experimental results can be found in <a class="link-external link-https" href="https://youtu.be/Cdm1T6Iv7GI" rel="external noopener nofollow">this https URL</a>.      
### 49.Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2105.08164.pdf)
>  This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.      
### 50.The Confluence of Networks, Games and Learning  [ :arrow_down: ](https://arxiv.org/pdf/2105.08158.pdf)
>  Recent years have witnessed significant advances in technologies and services in modern network applications, including smart grid management, wireless communication, cybersecurity as well as multi-agent autonomous systems. Considering the heterogeneous nature of networked entities, emerging network applications call for game-theoretic models and learning-based approaches in order to create distributed network intelligence that responds to uncertainties and disruptions in a dynamic or an adversarial environment. This paper articulates the confluence of networks, games and learning, which establishes a theoretical underpinning for understanding multi-agent decision-making over networks. We provide an selective overview of game-theoretic learning algorithms within the framework of stochastic approximation theory, and associated applications in some representative contexts of modern network systems, such as the next generation wireless communication networks, the smart grid and distributed machine learning. In addition to existing research works on game-theoretic learning over networks, we highlight several new angles and research endeavors on learning in games that are related to recent developments in artificial intelligence. Some of the new angles extrapolate from our own research interests. The overall objective of the paper is to provide the reader a clear picture of the strengths and challenges of adopting game-theoretic learning methods within the context of network systems, and further to identify fruitful future research directions on both theoretical and applied studies.      
### 51.MUSER: MUltimodal Stress Detection using Emotion Recognition as an Auxiliary Task  [ :arrow_down: ](https://arxiv.org/pdf/2105.08146.pdf)
>  The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER -- a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-of-the-art results.      
### 52.Weakly Private Information Retrieval Under Rényi Divergence  [ :arrow_down: ](https://arxiv.org/pdf/2105.08114.pdf)
>  Private information retrieval (PIR) is a protocol that guarantees the privacy of a user who is in communication with databases. The user wants to download one of the messages stored in the databases while hiding the identity of the desired message. Recently, the benefits that can be obtained by weakening the privacy requirement have been studied, but the definition of weak privacy needs to be elaborated upon. In this paper, we attempt to quantify the weak privacy (i.e., information leakage) in PIR problems by using the Rényi divergence that generalizes the Kullback-Leibler divergence. By introducing Rényi divergence into the existing PIR problem, the tradeoff relationship between privacy (information leakage) and PIR performance (download cost) is characterized via convex optimization. Furthermore, we propose an alternative PIR scheme with smaller message sizes than the Tian-Sun-Chen (TSC) scheme. The proposed scheme cannot achieve the PIR capacity of perfect privacy since the message size of the TSC scheme is the minimum to achieve the PIR capacity. However, we show that the proposed scheme can be better than the TSC scheme in the weakly PIR setting, especially under a low download cost regime.      
### 53.An Integrated Deep Learning and Dynamic Programming Method for Predicting Tumor Suppressor Genes, Oncogenes, and Fusion from PDB Structures  [ :arrow_down: ](https://arxiv.org/pdf/2105.08100.pdf)
>  Mutations in proto-oncogenes (ONGO) and the loss of regulatory function of tumor suppression genes (TSG) are the common underlying mechanism for uncontrolled tumor growth. While cancer is a heterogeneous complex of distinct diseases, finding the potentiality of the genes related functionality to ONGO or TSG through computational studies can help develop drugs that target the disease. This paper proposes a classification method that starts with a preprocessing stage to extract the feature map sets from the input 3D protein structural information. The next stage is a deep convolutional neural network stage (DCNN) that outputs the probability of functional classification of genes. We explored and tested two approaches: in Approach 1, all filtered and cleaned 3D-protein-structures (PDB) are pooled together, whereas in Approach 2, the primary structures and their corresponding PDBs are separated according to the genes' primary structural information. Following the DCNN stage, a dynamic programming-based method is used to determine the final prediction of the primary structures' functionality. We validated our proposed method using the COSMIC online database. For the ONGO vs TSG classification problem, the AUROC of the DCNN stage for Approach 1 and Approach 2 DCNN are 0.978 and 0.765, respectively. The AUROCs of the final genes' primary structure functionality classification for Approach 1 and Approach 2 are 0.989, and 0.879, respectively. For comparison, the current state-of-the-art reported AUROC is 0.924.      
### 54.Learning to Automatically Catch Potholes in Worldwide Road Scene Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.07986.pdf)
>  Among several road hazards that are present in any paved way in the world, potholes are one of the most annoying and also involving higher maintenance costs. There exists an increasing interest on the automated detection of these hazards enabled by technological and research progress. Our research work tackled the challenge of pothole detection from images of real world road scenes. The main novelty resides on the application of the latest progress in AI to learn the visual appearance of potholes. We built a large dataset of images with pothole annotations. They contained road scenes from different cities in the world, taken with different cameras, vehicles and viewpoints under varied environmental conditions. Then, we fine-tuned four different object detection models based on Faster R-CNN and SSD deep neural networks. We achieved high average precision and the pothole detector was tested on the Nvidia DrivePX2 platform with GPGPU capability, which can be embedded on vehicles. Moreover, it was deployed on a real vehicle to notify the detected potholes to a given IoT platform as part of AUTOPILOT H2020 project.      
### 55.Digital Resistance during COVID-19: A Workflow Management System of Contactless Purchasing and Its Empirical Study of Customer Acceptance  [ :arrow_down: ](https://arxiv.org/pdf/2105.07838.pdf)
>  The COVID-19 pandemic has stimulated the shift of work and life from the physical to a more digital format. To survive and thrive, companies have integrated more digital-enabled elements into their businesses to facilitate resilience, by avoiding potential close physical contact. Following Design Science Research Methodology (DSRM), this paper builds a workflow management system for contactless digital resilience when customers are purchasing in a store. Customer behavior, in coping with digital resilience against COVID-19, is illustrated and empirically tested, using a derivative model in which the constructs are from classical theories. Data was collected from individual customers via the Internet, and 247 completed questionnaires were examined.      
