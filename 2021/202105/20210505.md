# ArXiv eess --Wed, 5 May 2021
### 1.Market Potential for CO$_2$ Removal and Sequestration from Renewable Natural Gas Production in California  [ :arrow_down: ](https://arxiv.org/pdf/2105.01644.pdf)
>  Bioenergy with Carbon Capture and Sequestration (BECCS) is critical for stringent climate change mitigation, but is commercially and technologically immature and resource-intensive. In California, state and federal fuel and climate policies can drive first-markets for BECCS. We develop a spatially explicit optimization model to assess niche markets for renewable natural gas (RNG) production with carbon capture and sequestration (CCS) from waste biomass in California. Existing biomass residues produce biogas and RNG and enable low-cost CCS through the upgrading process and CO$_2$ truck transport. Under current state and federal policy incentives, we could capture and sequester 2.9 million MT CO$_2$/year (0.7% of California's 2018 CO$_2$ emissions) and produce 93 PJ RNG/year (4% of California's 2018 natural gas demand) with a profit maximizing objective. Existing federal and state policies produce profits of \$11/GJ. Distributed RNG production with CCS potentially catalyzes markets and technologies for CO$_2$ capture, transport, and storage in California.      
### 2.Efficient Data Optimisation for Harmonic Inpainting with Finite Elements  [ :arrow_down: ](https://arxiv.org/pdf/2105.01586.pdf)
>  Harmonic inpainting with optimised data is very popular for inpainting-based image compression. We improve this approach in three important aspects. Firstly, we replace the standard finite differences discretisation by a finite element method with triangle elements. This does not only speed up inpainting and data selection, but even improves the reconstruction quality. Secondly, we propose highly efficient algorithms for spatial and tonal data optimisation that are several orders of magnitude faster than state-of-the-art methods. Last but not least, we show that our algorithms also allow working with very large images. This has previously been impractical due to the memory and runtime requirements of prior algorithms.      
### 3.Exploring Disentanglement with Multilingual and Monolingual VQ-VAE  [ :arrow_down: ](https://arxiv.org/pdf/2105.01573.pdf)
>  This work examines the content and usefulness of disentangled phone and speaker representations from two separately trained VQ-VAE systems: one trained on multilingual data and another trained on monolingual data. We explore the multi- and monolingual models using four small proof-of-concept tasks: copy-synthesis, voice transformation, linguistic code-switching, and content-based privacy masking. From these tasks, we reflect on how disentangled phone and speaker representations can be used to manipulate speech in a meaningful way. Our experiments demonstrate that the VQ representations are suitable for these tasks, including creating new voices by mixing speaker representations together. We also present our novel technique to conceal the content of targeted words within an utterance by manipulating phone VQ codes, while retaining speaker identity and intelligibility of surrounding words. Finally, we discuss recommendations for further increasing the viability of disentangled representations.      
### 4.ReBeatICG: Real-time Low-Complexity Beat-to-beat Impedance Cardiogram Delineation Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2105.01525.pdf)
>  This work presents ReBeatICG, a real-time, low-complexity beat-to-beat impedance cardiography (ICG) delineation algorithm that allows hemodynamic parameters monitoring. The proposed procedure relies only on the ICG signal compared to most algorithms found in the literature that rely on synchronous electrocardiogram signal (ECG) recordings. ReBeatICG was designed with implementation on an ultra-low-power microcontroller (MCU) in mind. The detection accuracy of the developed algorithm is tested against points manually labeled by cardiologists. It achieves a detection Gmean accuracy of 94.9%, 98.6%, 90.3%, and 84.3% for the B, C, X, and O points, respectively. Furthermore, several hemodynamic parameters were calculated based on annotated characteristic points and compared with values generated from the cardiologists' annotations. ReBeatICG achieved mean error rates of 0.11ms, 9.72ms, 8.32ms, and 3.97% for HR, LVET, IVRT, and relative C-point amplitude, respectively.      
### 5.Learning-based feedforward augmentation for steady state rejection of residual dynamics on a nanometer-accurate planar actuator system  [ :arrow_down: ](https://arxiv.org/pdf/2105.01458.pdf)
>  Growing demands in the semiconductor industry result in the need for enhanced performance of lithographic equipment. However, position tracking accuracy of high precision mechatronics is often limited by the presence of disturbance sources, which originate from unmodelled or unforeseen deterministic environmental effects. To negate the effects of these disturbances, a learning based feedforward controller is employed, where the underlying control policy is estimated from experimental data based on Gaussian Process regression. The proposed approach exploits the property of including prior knowledge on the expected steady state behaviour of residual dynamics in terms of kernel selection. Corresponding hyper-parameters are optimized using the maximization of the marginalized likelihood. Consequently, the learned function is employed as augmentation of the currently employed rigid body feedforward controller. The effectiveness of the augmentation is experimentally validated on a magnetically levitated planar motor stage. The results of this paper demonstrate the benefits and possibilities of machine-learning based approaches for compensation of static effects, which originate from residual dynamics, such that position tracking performance for moving-magnet planar motor actuators is improved.      
### 6.Securing the Inter-Spacecraft Links: Physical Layer Key Generation from Doppler Frequency Shift  [ :arrow_down: ](https://arxiv.org/pdf/2105.01448.pdf)
>  In this work, we propose a secret key generation procedure specifically designed for the inter-spacecraft communication links. As a novel secrecy source, the spacecrafts utilize Doppler frequency shift based measurements. In this way, the mobilities of the communication devices are exploited to generate secret keys, where this resource can be utilized in the environments that the channel fading based key generation methods are not available. The mobility of a spacecraft is modeled as the superposition of a pre-determined component and a dynamic component. We derive the maximum achievable secret key generation rate from the Doppler frequency shift. The proposed secret key generation procedure extracts the Doppler frequency shift in the form of nominal power spectral density samples (NPSDS). We propose a maximum-likelihood (ML) estimation for the NPSDS at the spacecrafts, then a uniform quantizer is utilized to obtain secret key bits. The key disagreement rate (KDR) is analytically obtained for the proposed key generation procedure. Through numerical studies, the tightness of the provided approximations is shown. Both the theoretical and numerical results demonstrate the validity and the practicality of the presented physical layer key generation procedure considering the security of the communication links of spacecrafts.      
### 7.A Two-Stage Coordinative Zonal Volt/VAR Control Scheme for Distribution Systems with High Inverter-based Resources  [ :arrow_down: ](https://arxiv.org/pdf/2105.01405.pdf)
>  This paper presents a two-stage zonal Volt/VAR control scheme for coordinating inverter-based resources (IBR) with utility-owned voltage regulators (VR) to regulate voltage in unbalanced 3-phase distribution systems. First, correlations between nodal voltages are derived from nodal voltage sensitivity studies. Then, the feeder is partitioned into non-overlapping, weakly-coupled voltage control zones based on nodal voltage correlations. IBR are used in the first stage to regulate voltage changes continuously and VR are used in the second stage to regulate large voltage deviations. An online VR voltage setpoint tuning strategy is developed to reduce excessive tap changes and avoid large voltage fluctuations without retrofitting existing VR controllers. In addition, the proposed algorithm uses real-time voltage measurements only from the critical nodes (typically less than 4% of total nodes) to reduce the sensing and communication needs. Actual distribution feeder topologies and load and PV time-series data are used to verify the performance of the algorithm. Because the method is a rule-based approach, it runs extremely fast, requires fewer measurements, and requires no retrofit to the existing VR control mechanisms. Simulation results show that the performance of the proposed method in terms of voltage control results and average numbers of VR tap changes are satisfactory.      
### 8.Performance Evaluation of Deep Convolutional Maxout Neural Network in Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2105.01399.pdf)
>  In this paper, various structures and methods of Deep Artificial Neural Networks (DNN) will be evaluated and compared for the purpose of continuous Persian speech recognition. One of the first models of neural networks used in speech recognition applications were fully connected Neural Networks (FCNNs) and, consequently, Deep Neural Networks (DNNs). Although these models have better performance compared to GMM / HMM models, they do not have the proper structure to model local speech information. Convolutional Neural Network (CNN) is a good option for modeling the local structure of biological signals, including speech signals. Another issue that Deep Artificial Neural Networks face, is the convergence of networks on training data. The main inhibitor of convergence is the presence of local minima in the process of training. Deep Neural Network Pre-training methods, despite a large amount of computing, are powerful tools for crossing the local minima. But the use of appropriate neuronal models in the network structure seems to be a better solution to this problem. The Rectified Linear Unit neuronal model and the Maxout model are the most suitable neuronal models presented to this date. Several experiments were carried out to evaluate the performance of the methods and structures mentioned. After verifying the proper functioning of these methods, a combination of all models was implemented on FARSDAT speech database for continuous speech recognition. The results obtained from the experiments show that the combined model (CMDNN) improves the performance of ANNs in speech recognition versus the pre-trained fully connected NNs with sigmoid neurons by about 3%.      
### 9.Digital Twin-Assisted Cooperative Driving at Non-Signalized Intersections  [ :arrow_down: ](https://arxiv.org/pdf/2105.01357.pdf)
>  Digital Twin, as an emerging technology related to Cyber-Physical Systems (CPS) and Internet of Things (IoT), has attracted increasing attentions during the past decade. Conceptually, a Digital Twin is a digital replica of a physical entity in the real world, and this technology is leveraged in this study to design a cooperative driving system at non-signalized intersections, allowing connected vehicles to cooperate with each other to cross intersections without any full stops. Within the proposed Digital Twin framework, we developed an enhanced first-in-first-out (FIFO) slot reservation algorithm to schedule the sequence of crossing vehicles, a consensus motion control algorithm to calculate vehicles' referenced longitudinal motion, and a model-based motion estimation algorithm to tackle communication delay and packet loss. Additionally, an augmented reality (AR) human-machine-interface (HMI) is designed to provide the guidance to drivers to cooperate with other connected vehicles. Agent-based modeling and simulation of the proposed system is conducted in Unity game engine based on a real-world map in San Francisco, and the human-in-the-loop (HITL) simulation results prove the benefits of the proposed algorithms with 20% reduction in travel time and 23.7% reduction in energy consumption, respectively, when compared with traditional signalized intersections.      
### 10.Some open questions on morphological operators and representations in the deep learning era  [ :arrow_down: ](https://arxiv.org/pdf/2105.01339.pdf)
>  During recent years, the renaissance of neural networks as the major machine learning paradigm and more specifically, the confirmation that deep learning techniques provide state-of-the-art results for most of computer vision tasks has been shaking up traditional research in image processing. The same can be said for research in communities working on applied harmonic analysis, information geometry, variational methods, etc. For many researchers, this is viewed as an existential threat. On the one hand, research funding agencies privilege mainstream approaches especially when these are unquestionably suitable for solving real problems and for making progress on artificial intelligence. On the other hand, successful publishing of research in our communities is becoming almost exclusively based on a quantitative improvement of the accuracy of any benchmark task. As most of my colleagues sharing this research field, I am confronted with the dilemma of continuing to invest my time and intellectual effort on mathematical morphology as my driving force for research, or simply focussing on how to use deep learning and contributing to it. The solution is not obvious to any of us since our research is not fundamental, it is just oriented to solve challenging problems, which can be more or less theoretical. Certainly, it would be foolish for anyone to claim that deep learning is insignificant or to think that one's favourite image processing domain is productive enough to ignore the state-of-the-art. I fully understand that the labs and leading people in image processing communities have been shifting their research to almost exclusively focus on deep learning techniques. My own position is different: I do think there is room for progress on mathematically grounded image processing branches, under the condition that these are rethought in a broader sense from the deep learning paradigm. Indeed, I firmly believe that the convergence between mathematical morphology and the computation methods which gravitate around deep learning (fully connected networks, convolutional neural networks, residual neural networks, recurrent neural networks, etc.) is worthwhile. The goal of this talk is to discuss my personal vision regarding these potential interactions. Without any pretension of being exhaustive, I want to address it with a series of open questions, covering a wide range of specificities of morphological operators and representations, which could be tackled and revisited under the paradigm of deep learning. An expected benefit of such convergence between morphology and deep learning is a cross-fertilization of concepts and techniques between both fields. In addition, I think the future answer to some of these questions can provide some insight on understanding, interpreting and simplifying deep learning networks.      
### 11.Scale Equivariant Neural Networks with Morphological Scale-Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2105.01335.pdf)
>  The translation equivariance of convolutions can make convolutional neural networks translation equivariant or invariant. Equivariance to other transformations (e.g. rotations, affine transformations, scalings) may also be desirable as soon as we know a priori that transformed versions of the same objects appear in the data. The semigroup cross-correlation, which is a linear operator equivariant to semigroup actions, was recently proposed and applied in conjunction with the Gaussian scale-space to create architectures which are equivariant to discrete scalings. In this paper, a generalization using a broad class of liftings, including morphological scale-spaces, is proposed. The architectures obtained from different scale-spaces are tested and compared in supervised classification and semantic segmentation tasks where objects in test images appear at different scales compared to training images. In both classification and segmentation tasks, the scale-equivariant architectures improve dramatically the generalization to unseen scales compared to a convolutional baseline. Besides, in our experiments morphological scale-spaces outperformed the Gaussian scale-space in geometrical tasks.      
### 12.Speech Decomposition Based on a Hybrid Speech Model and Optimal Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.01302.pdf)
>  In a hybrid speech model, both voiced and unvoiced components can coexist in a segment. Often, the voiced speech is regarded as the deterministic component, and the unvoiced speech and additive noise are the stochastic components. Typically, the speech signal is considered stationary within fixed segments of 20-40 ms, but the degree of stationarity varies over time. For decomposing noisy speech into its voiced and unvoiced components, a fixed segmentation may be too crude, and we here propose to adapt the segment length according to the signal local characteristics. The segmentation relies on parameter estimates of a hybrid speech model and the maximum a posteriori (MAP) and log-likelihood criteria as rules for model selection among the possible segment lengths, for voiced and unvoiced speech, respectively. Given the optimal segmentation markers and the estimated statistics, both components are estimated using linear filtering. A codebook-based approach differentiates between unvoiced speech and noise. A better extraction of the components is possible by taking into account the adaptive segmentation, compared to a fixed one. Also, a lower distortion for voiced speech and higher segSNR for both components is possible, as compared to other decomposition methods.      
### 13.COVID-Net CT-S: 3D Convolutional Neural Network Architectures for COVID-19 Severity Assessment using Chest CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.01284.pdf)
>  The health and socioeconomic difficulties caused by the COVID-19 pandemic continues to cause enormous tensions around the world. In particular, this extraordinary surge in the number of cases has put considerable strain on health care systems around the world. A critical step in the treatment and management of COVID-19 positive patients is severity assessment, which is challenging even for expert radiologists given the subtleties at different stages of lung disease severity. Motivated by this challenge, we introduce COVID-Net CT-S, a suite of deep convolutional neural networks for predicting lung disease severity due to COVID-19 infection. More specifically, a 3D residual architecture design is leveraged to learn volumetric visual indicators characterizing the degree of COVID-19 lung disease severity. Experimental results using the patient cohort collected by the China National Center for Bioinformation (CNCB) showed that the proposed COVID-Net CT-S networks, by leveraging volumetric features, can achieve significantly improved severity assessment performance when compared to traditional severity assessment networks that learn and leverage 2D visual features to characterize COVID-19 severity.      
### 14.End-to-End Learning for Uplink MU-SIMO Joint Transmitter and Non-Coherent Receiver Design in Fading Channels  [ :arrow_down: ](https://arxiv.org/pdf/2105.01260.pdf)
>  In this paper, a novel end-to-end learning approach, namely JTRD-Net, is proposed for uplink multiuser single-input multiple-output (MU-SIMO) joint transmitter and non-coherent receiver design (JTRD) in fading channels. The basic idea lies in the use of artificial neural networks (ANNs) to replace traditional communication modules at both transmitter and receiver sides. More specifically, the transmitter side is modeled as a group of parallel linear layers, which are responsible for multiuser waveform design; and the non-coherent receiver is formed by a deep feed-forward neural network (DFNN) so as to provide multiuser detection (MUD) capabilities. The entire JTRD-Net can be trained from end to end to adapt to channel statistics through deep learning. After training, JTRD-Net can work efficiently in a non-coherent manner without requiring any levels of channel state information (CSI). In addition to the network architecture, a novel weight-initialization method, namely symmetrical-interval initialization, is proposed for JTRD-Net. It is shown that the symmetrical-interval initialization outperforms the conventional method (e.g. Xavier initialization) in terms of well-balanced convergence-rate among users. Simulation results show that the proposed JTRD-Net approach takes significant advantages in terms of reliability and scalability over baseline schemes on both i.i.d. complex Gaussian channels and spatially-correlated channels.      
### 15.Weakly-Supervised Universal Lesion Segmentation with Regional Level Set Loss  [ :arrow_down: ](https://arxiv.org/pdf/2105.01218.pdf)
>  Accurately segmenting a variety of clinically significant lesions from whole body computed tomography (CT) scans is a critical task on precision oncology imaging, denoted as universal lesion segmentation (ULS). Manual annotation is the current clinical practice, being highly time-consuming and inconsistent on tumor's longitudinal assessment. Effectively training an automatic segmentation model is desirable but relies heavily on a large number of pixel-wise labelled data. Existing weakly-supervised segmentation approaches often struggle with regions nearby the lesion boundaries. In this paper, we present a novel weakly-supervised universal lesion segmentation method by building an attention enhanced model based on the High-Resolution Network (HRNet), named AHRNet, and propose a regional level set (RLS) loss for optimizing lesion boundary delineation. AHRNet provides advanced high-resolution deep image features by involving a decoder, dual-attention and scale attention mechanisms, which are crucial to performing accurate lesion segmentation. RLS can optimize the model reliably and effectively in a weakly-supervised fashion, forcing the segmentation close to lesion boundary. Extensive experimental results demonstrate that our method achieves the best performance on the publicly large-scale DeepLesion dataset and a hold-out test set.      
### 16.Event Camera Simulator Design for Modeling Attention-based Inference Architectures  [ :arrow_down: ](https://arxiv.org/pdf/2105.01203.pdf)
>  In recent years, there has been a growing interest in realizing methodologies to integrate more and more computation at the level of the image sensor. The rising trend has seen an increased research interest in developing novel event cameras that can facilitate CNN computation directly in the sensor. However, event-based cameras are not generally available in the market, limiting performance exploration on high-level models and algorithms. This paper presents an event camera simulator that can be a potent tool for hardware design prototyping, parameter optimization, attention-based innovative algorithm development, and benchmarking. The proposed simulator implements a distributed computation model to identify relevant regions in an image frame. Our simulator's relevance computation model is realized as a collection of modules and performs computations in parallel. The distributed computation model is configurable, making it highly useful for design space exploration. The Rendering engine of the simulator samples frame-regions only when there is a new event. The simulator closely emulates an image processing pipeline similar to that of physical cameras. Our experimental results show that the simulator can effectively emulate event vision with low overheads.      
### 17.Automated Estimation of Total Lung Volume using Chest Radiographs and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2105.01181.pdf)
>  Total lung volume is an important quantitative biomarker and is used for the assessment of restrictive lung diseases. In this study, we investigate the performance of several deep-learning approaches for automated measurement of total lung volume from chest radiographs. 7621 posteroanterior and lateral view chest radiographs (CXR) were collected from patients with chest CT available. Similarly, 928 CXR studies were chosen from patients with pulmonary function test (PFT) results. The reference total lung volume was calculated from lung segmentation on CT or PFT data, respectively. This dataset was used to train deep-learning architectures to predict total lung volume from chest radiographs. The experiments were constructed in a step-wise fashion with increasing complexity to demonstrate the effect of training with CT-derived labels only and the sources of error. The optimal models were tested on 291 CXR studies with reference lung volume obtained from PFT. The optimal deep-learning regression model showed an MAE of 408 ml and a MAPE of 8.1\% and Pearson's r = 0.92 using both frontal and lateral chest radiographs as input. CT-derived labels were useful for pre-training but the optimal performance was obtained by fine-tuning the network with PFT-derived labels. We demonstrate, for the first time, that state-of-the-art deep learning solutions can accurately measure total lung volume from plain chest radiographs. The proposed model can be used to obtain total lung volume from routinely acquired chest radiographs at no additional cost and could be a useful tool to identify trends over time in patients referred regularly for chest x-rays.      
### 18.Quantifying and Maximizing the Benefits of Back-End Noise Adaption on Attention-Based Speech Recognition Models  [ :arrow_down: ](https://arxiv.org/pdf/2105.01134.pdf)
>  This work analyzes how attention-based Bidirectional Long Short-Term Memory (BLSTM) models adapt to noise-augmented speech. We identify crucial components for noise adaptation in BLSTM models by freezing model components during fine-tuning. We first freeze larger model subnetworks and then pursue a fine-grained freezing approach in the encoder after identifying its importance for noise adaptation. The first encoder layer is shown to be crucial for noise adaptation, and the weights are shown to be more important than the other layers. Appreciable accuracy benefits are identified when fine-tuning on a target noisy environment from a model pretrained with noisy speech relative to fine-tuning from a model pretrained with only clean speech when tested on the target noisy environment. For this analysis, we produce our own dataset augmentation tool and it is open-sourced to encourage future efforts in exploring noise adaptation in ASR.      
### 19.Riemannian Geometry with differentiable ambient space and metric operator  [ :arrow_down: ](https://arxiv.org/pdf/2105.01583.pdf)
>  We show Riemannian geometry could be studied by identifying the tangent bundle of a Riemannian manifold $\mathcal{M}$ with a subbundle of the trivial bundle $\mathcal{M} \times \mathcal{E}$, obtained by embedding $\mathcal{M}$ differentiably in a Euclidean space $\mathcal{E}$. Given such an embedding, we can extend the metric tensor on $\mathcal{M}$ to a (positive-definite) operator-valued function acting on $\mathcal{E}$, giving us an embedded ambient structure. The formulas for the Christoffel symbols and Riemannian curvature in local coordinates have simple generalizations to this setup. For a Riemannian submersion $\mathfrak{q}:\mathcal{M}\to \mathcal{B}$ from an embedded manifold $\mathcal{M}\subset \mathcal{E}$, we define a submersed ambient structure and obtain similar formulas, with the O'Neil tensor expressed in terms of the projection to the horizontal bundle $\mathcal{H}\mathcal{M}$. Using this framework, we provide the embedded and submersed ambient structures for the double tangent bundle $\mathcal{T}\mathcal{T}\mathcal{M}$ and the tangent of the horizontal bundle $\mathcal{T}\mathcal{H}\mathcal{M}$, describe the fibration of a horizontal bundle over the tangent bundle of the base manifold and extend the notion of a canonical flip to the submersion case. We obtain a formula for horizontal lifts of Jacobi fields, and a new closed-form formula for Jacobi fields of naturally reductive homogeneous spaces. We construct natural metrics on these double tangent bundles, in particular, extending Sasaki and other natural metrics to the submersion case. We illustrate by providing explicit calculations for several manifolds.      
### 20.Simple and Cheap Setup for Timing Tapping Responses Synchronized to Auditory Stimuli  [ :arrow_down: ](https://arxiv.org/pdf/2105.01570.pdf)
>  Measuring human capabilities to synchronize in time, adapt to perturbations to timing sequences or reproduce time intervals often require experimental setups that allow recording response times with millisecond precision. Most setups present auditory stimuli using either MIDI devices or specialized hardware such as Arduino and are often expensive or require calibration and advanced programming skills. Here, we present in detail an experimental setup that only requires an external sound card and minor electronic skills, works on a conventional PC, is cheaper than alternatives and requires almost no programming skills. It is intended for presenting any auditory stimuli and recording tapping response times with within 2 milliseconds precision (up to -2ms lag). This paper shows why desired accuracy in recording response times against auditory stimuli is difficult to achieve in conventional computer setups, presents an experimental setup to overcome this and explains in detail how to set it up and use the provided code. Finally, the code for analyzing the recorded tapping responses was evaluated, showing that no spurious or missing events were found in 94% of the analyzed recordings.      
### 21.Abstraction-Guided Truncations for Stationary Distributions of Markov Population Models  [ :arrow_down: ](https://arxiv.org/pdf/2105.01536.pdf)
>  To understand the long-run behavior of Markov population models, the computation of the stationary distribution is often a crucial part. We propose a truncation-based approximation that employs a state-space lumping scheme, aggregating states in a grid structure. The resulting approximate stationary distribution is used to iteratively refine relevant and truncate irrelevant parts of the state-space. This way, the algorithm learns a well-justified finite-state projection tailored to the stationary behavior. We demonstrate the method's applicability to a wide range of non-linear problems with complex stationary behaviors.      
### 22.Holographic MIMO Communications  [ :arrow_down: ](https://arxiv.org/pdf/2105.01535.pdf)
>  Imagine a MIMO communication system that fully exploits the propagation characteristics offered by an electromagnetic channel and ultimately approaches the limits imposed by wireless communications. This is the concept of Holographic MIMO communications. Accurate and tractable channel modeling is critical to understanding its full potential. Classical stochastic models used by communications theorists are derived under the electromagnetic far-field assumption. However, such assumption breaks down when large (compared to the wavelength) antenna arrays are considered - as envisioned in future wireless communications. In this paper, we start from the first principles of wave propagation and provide a Fourier plane-wave series expansion of the channel response, which fully captures the essence of electromagnetic propagation in arbitrary scattering and is also valid in the (radiative) near-field. The expansion is based on the Fourier spectral representation and has an intuitive physical interpretation, as it statistically describes the angular coupling between source and receiver. When discretized, it leads to a low-rank semi-unitarily equivalent approximation of the spatial electromagnetic channel in the angular domain. The developed channel model is used to compute the ergodic capacity of a Holographic MIMO system with different degrees of channel state information.      
### 23.VQCPC-GAN: Variable-length Adversarial Audio Synthesis using Vector-Quantized Contrastive Predictive Coding  [ :arrow_down: ](https://arxiv.org/pdf/2105.01531.pdf)
>  Influenced by the field of Computer Vision, Generative Adversarial Networks (GANs) are often adopted for the audio domain using fixed-size two-dimensional spectrogram representations as the "image data". However, in the (musical) audio domain, it is often desired to generate output of variable duration. This paper presents VQCPC-GAN, an adversarial framework for synthesizing variable-length audio by exploiting Vector-Quantized Contrastive Predictive Coding (VQCPC). A sequence of VQCPC tokens extracted from real audio data serves as conditional input to a GAN architecture, providing step-wise time-dependent features of the generated content. The input noise z (characteristic in adversarial architectures) remains fixed over time, ensuring temporal consistency of global features. We evaluate the proposed model by comparing a diverse set of metrics against various strong baselines. Results show that, even though the baselines score best, VQCPC-GAN achieves comparable performance even when generating variable-length audio. Numerous sound examples are provided in the accompanying website, and we release the code for reproducibility.      
### 24.Radio Communication Scenarios in 5G-Railways  [ :arrow_down: ](https://arxiv.org/pdf/2105.01511.pdf)
>  With the rapid development of railways, especially high-speed railways, there is an increasingly urgent demand for new wireless communication system for railways. Taking the mature 5G technology as an opportunity, 5G-railways (5G-R) have been widely regarded as a solution to meet the diversified demands of railway wireless communications. For the design, deployment and improvement of 5G-R networks, radio communication scenario classification plays an important role, affecting channel modeling and system performance evaluation. In this paper, a standardized radio communication scenario classification, including 18 scenarios, is proposed for 5G-R. This paper analyzes the differences of 5G-R scenarios compared with the traditional cellular networks and GSM-railways, according to 5G-R requirements and the unique physical environment and propagation characteristics. The proposed standardized scenario classification helps deepen the research of 5G-R and promote the development and application of the existing advanced technologies in railways.      
### 25.Consensus Dynamics and Opinion Formation on Hypergraphs  [ :arrow_down: ](https://arxiv.org/pdf/2105.01369.pdf)
>  In this chapter, we derive and analyse models for consensus dynamics on hypergraphs. As we discuss, unless there are nonlinear node interaction functions, it is always possible to rewrite the system in terms of a new network of effective pairwise node interactions, regardless of the initially underlying multi-way interaction structure. We thus focus on dynamics based on a certain class of non-linear interaction functions, which can model different sociological phenomena such as peer pressure and stubbornness. Unlike for linear consensus dynamics on networks, we show how our nonlinear model dynamics can cause shifts away from the average system state. We examine how these shifts are influenced by the distribution of the initial states, the underlying hypergraph structure and different forms of non-linear scaling of the node interaction function.      
### 26.Simplified Klinokinesis using Spiking Neural Networks for Resource-Constrained Navigation on the Neuromorphic Processor Loihi  [ :arrow_down: ](https://arxiv.org/pdf/2105.01358.pdf)
>  C. elegans shows chemotaxis using klinokinesis where the worm senses the concentration based on a single concentration sensor to compute the concentration gradient to perform foraging through gradient ascent/descent towards the target concentration followed by contour tracking. The biomimetic implementation requires complex neurons with multiple ion channel dynamics as well as interneurons for control. While this is a key capability of autonomous robots, its implementation on energy-efficient neuromorphic hardware like Intel's Loihi requires adaptation of the network to hardware-specific constraints, which has not been achieved. In this paper, we demonstrate the adaptation of chemotaxis based on klinokinesis to Loihi by implementing necessary neuronal dynamics with only LIF neurons as well as a complete spike-based implementation of all functions e.g. Heaviside function and subtractions. Our results show that Loihi implementation is equivalent to the software counterpart on Python in terms of performance - both during foraging and contour tracking. The Loihi results are also resilient in noisy environments. Thus, we demonstrate a successful adaptation of chemotaxis on Loihi - which can now be combined with the rich array of SNN blocks for SNN based complex robotic control.      
### 27.Effects of Quantization on the Multiple-Round Secret-Key Capacity  [ :arrow_down: ](https://arxiv.org/pdf/2105.01350.pdf)
>  We consider the strong secret key (SK) agreement problem for the satellite communication setting, where a remote source (a satellite) chooses a common binary phase shift keying (BPSK) modulated input for three statistically independent additive white Gaussian channels (AWGN) whose outputs are observed by, respectively, two legitimate receivers (Alice and Bob) and an eavesdropper (Eve). Legitimate receivers have access to an authenticated, noiseless, two-way, and public communication link, so they can exchange multiple rounds of public messages to agree on a SK hidden from Eve. Without loss of generality, the noise variances for Alice's and Bob's measurement channels are both fixed to a value $Q&gt;1$, whereas the noise over Eve's measurement channel has a unit variance, so $Q$ represents a channel quality ratio. The significant and not necessarily expected effect of quantizations at all receivers on the scaling of the SK capacity with respect to a sufficiently large and finite channel quality ratio $Q$ is illustrated by showing 1) the achievability of a constant SK for any finite BPSK modulated satellite output by proposing a thresholding algorithm as an advantage distillation protocol for AWGN channels and 2) the converse (i.e., unachievability) bound for the case when all receivers apply a one-bit uniform quantizer to noisy BPSK modulated observations before SK agreement, for which the SK capacity is shown to decrease quadratically in $Q$. Our results prove that soft information not only increases the reliability and the achieved SK rate but also increases the scaling of the SK capacity at least quadratically in $Q$ as compared to hard information.      
### 28.Collaborative Multi-Resource Allocation in Terrestrial-Satellite Network Towards 6G  [ :arrow_down: ](https://arxiv.org/pdf/2105.01259.pdf)
>  Terrestrial-satellite networks are envisioned to play a significant role in the sixth-generation (6G) wireless networks. In such networks, hot air balloons are useful as they can relay the signals between satellites and ground stations. Most existing works assume that the hot air balloons are deployed at the same height with the same minimum elevation angle to the satellites, which may not be practical due to possible route conflict with airplanes and other flight equipment. In this paper, we consider a TSN containing hot air balloons at different heights and with different minimum elevation angles, which creates the challenge of non-uniform available serving time for the communication between the hot air balloons and the satellites. Jointly considering the caching, computing, and communication (3C) resource management for both the ground-balloon-satellite links and inter-satellite laser links, our objective is to maximize the network energy efficiency. Firstly, by proposing a tapped water-filling algorithm, we schedule the traffic to relay among satellites according to the available serving time of satellites. Then, we generate a series of configuration matrices, based on which we formulate the relationship of relay time and the power consumption involved in the relay among satellites. Finally, the integrated system model of TSN is built and solved by geometric programming with Taylor series approximation. Simulation results demonstrate the effectiveness of our proposed scheme.      
### 29.Streaming end-to-end speech recognition with jointly trained neural feature enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2105.01254.pdf)
>  In this paper, we present a streaming end-to-end speech recognition model based on Monotonic Chunkwise Attention (MoCha) jointly trained with enhancement layers. Even though the MoCha attention enables streaming speech recognition with recognition accuracy comparable to a full attention-based approach, training this model is sensitive to various factors such as the difficulty of training examples, hyper-parameters, and so on. Because of these issues, speech recognition accuracy of a MoCha-based model for clean speech drops significantly when a multi-style training approach is applied. Inspired by Curriculum Learning [1], we introduce two training strategies: Gradual Application of Enhanced Features (GAEF) and Gradual Reduction of Enhanced Loss (GREL). With GAEF, the model is initially trained using clean features. Subsequently, the portion of outputs from the enhancement layers gradually increases. With GREL, the portion of the Mean Squared Error (MSE) loss for the enhanced output gradually reduces as training proceeds. In experimental results on the LibriSpeech corpus and noisy far-field test sets, the proposed model with GAEF-GREL training strategies shows significantly better results than the conventional multi-style training approach.      
### 30.Regret-Optimal Full-Information Control  [ :arrow_down: ](https://arxiv.org/pdf/2105.01244.pdf)
>  We consider the infinite-horizon, discrete-time full-information control problem. Motivated by learning theory, as a criterion for controller design we focus on regret, defined as the difference between the LQR cost of a causal controller (that has only access to past and current disturbances) and the LQR cost of a clairvoyant one (that has also access to future disturbances). In the full-information setting, there is a unique optimal non-causal controller that in terms of LQR cost dominates all other controllers. Since the regret itself is a function of the disturbances, we consider the worst-case regret over all possible bounded energy disturbances, and propose to find a causal controller that minimizes this worst-case regret. The resulting controller has the interpretation of guaranteeing the smallest possible regret compared to the best non-causal controller, no matter what the future disturbances are. We show that the regret-optimal control problem can be reduced to a Nehari problem, i.e., to approximate an anticausal operator with a causal one in the operator norm. In the state-space setting, explicit formulas for the optimal regret and for the regret-optimal controller (in both the causal and the strictly causal settings) are derived. The regret-optimal controller is the sum of the classical $H_2$ state-feedback law and a finite-dimensional controller obtained from the Nehari problem. The controller construction simply requires the solution to the standard LQR Riccati equation, in addition to two Lyapunov equations. Simulations over a range of plants demonstrates that the regret-optimal controller interpolates nicely between the $H_2$ and the $H_\infty$ optimal controllers, and generally has $H_2$ and $H_\infty$ costs that are simultaneously close to their optimal values. The regret-optimal controller thus presents itself as a viable option for control system design.      
### 31.Polynomial-Time Algorithms for Multi-Agent Minimal-Capacity Planning  [ :arrow_down: ](https://arxiv.org/pdf/2105.01225.pdf)
>  We study the problem of minimizing the resource capacity of autonomous agents cooperating to achieve a shared task. More specifically, we consider high-level planning for a team of homogeneous agents that operate under resource constraints in stochastic environments and share a common goal: given a set of target locations, ensure that each location will be visited infinitely often by some agent almost surely. We formalize the dynamics of agents by consumption Markov decision processes. In a consumption Markov decision process, the agent has a resource of limited capacity. Each action of the agent may consume some amount of the resource. To avoid exhaustion, the agent can replenish its resource to full capacity in designated reload states. The resource capacity restricts the capabilities of the agent. The objective is to assign target locations to agents, and each agent is only responsible for visiting the assigned subset of target locations repeatedly. Moreover, the assignment must ensure that the agents can carry out their tasks with minimal resource capacity. We reduce the problem of finding target assignments for a team of agents with the lowest possible capacity to an equivalent graph-theoretical problem. We develop an algorithm that solves this graph problem in time that is \emph{polynomial} in the number of agents, target locations, and size of the consumption Markov decision process. We demonstrate the applicability and scalability of the algorithm in a scenario where hundreds of unmanned underwater vehicles monitor hundreds of locations in environments with stochastic ocean currents.      
### 32.Safe Navigation in Human Occupied Environments Using Sampling and Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2105.01204.pdf)
>  Sampling-based methods such as Rapidly-exploring Random Trees (RRTs) have been widely used for generating motion paths for autonomous mobile systems. In this work, we extend time-based RRTs with Control Barrier Functions (CBFs) to generate, safe motion plans in dynamic environments with many pedestrians. Our framework is based upon a human motion prediction model which is well suited for indoor narrow environments. We demonstrate our approach on a high-fidelity model of the Toyota Human Support Robot navigating in narrow corridors. We show in three scenarios that our proposed online method can navigate safely in the presence of moving agents with unknown dynamics.      
### 33.Fusing multimodal neuroimaging data with a variational autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2105.01128.pdf)
>  Neuroimaging studies often involve the collection of multiple data modalities. These modalities contain both shared and mutually exclusive information about the brain. This work aims at finding a scalable and interpretable method to fuse the information of multiple neuroimaging modalities using a variational autoencoder (VAE). To provide an initial assessment, this work evaluates the representations that are learned using a schizophrenia classification task. A support vector machine trained on the representations achieves an area under the curve for the classifier's receiver operating characteristic (ROC-AUC) of 0.8610.      
