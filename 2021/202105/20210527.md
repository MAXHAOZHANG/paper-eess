# ArXiv eess --Thu, 27 May 2021
### 1.Towards Transparent Application of Machine Learning in Video Processing  [ :arrow_down: ](https://arxiv.org/pdf/2105.12700.pdf)
>  Machine learning techniques for more efficient video compression and video enhancement have been developed thanks to breakthroughs in deep learning. The new techniques, considered as an advanced form of Artificial Intelligence (AI), bring previously unforeseen capabilities. However, they typically come in the form of resource-hungry black-boxes (overly complex with little transparency regarding the inner workings). Their application can therefore be unpredictable and generally unreliable for large-scale use (e.g. in live broadcast). The aim of this work is to understand and optimise learned models in video processing applications so systems that incorporate them can be used in a more trustworthy manner. In this context, the presented work introduces principles for simplification of learned models targeting improved transparency in implementing machine learning for video production and distribution applications. These principles are demonstrated on video compression examples, showing how bitrate savings and reduced complexity can be achieved by simplifying relevant deep learning models.      
### 2.Reconfigurable Architecture for Spatial Sensing in Wideband Radio Front-End  [ :arrow_down: ](https://arxiv.org/pdf/2105.12693.pdf)
>  The deployment of cellular spectrum in licensed, shared and unlicensed spectrum demands wideband sensing over non-contiguous sub-6 GHz spectrum. To improve the spectrum and energy efficiency, beamforming and massive multi-antenna systems are being explored which demand spatial sensing i.e. blind identification of vacant frequency bands and direction-of-arrival (DoA) of the occupied bands. We propose a reconfigurable architecture to perform spatial sensing of multi-band spectrum digitized via wideband radio front-end comprising of the sparse antenna array (SAA) and Sub-Nyquist Sampling (SNS). The proposed architecture comprises SAA pre-processing and algorithms to perform spatial sensing directly on SNS samples. The proposed architecture is realized on Zynq System on Chip (SoC), consisting of the ARM processor and FPGA, via hardware-software co-design (HSCD). Using the dynamic partial reconfiguration (DPR), on-the-fly switching between algorithms depending on the number of active signals in the sensed spectrum is enabled. The functionality, resource utilization, and execution time of the proposed architecture are analyzed for various HSCD configurations, word-length, number of digitized samples, signal-to-noise ratio (SNR), and antenna array (sparse/non-sparse).      
### 3.Full-Duplex meets Reconfigurable Surfaces: RIS-assisted SIC for Full-Duplex Radios  [ :arrow_down: ](https://arxiv.org/pdf/2105.12646.pdf)
>  Reconfigurable intelligent surfaces (RIS) are a key enabler of various new applications in 6G smart radio environments. By utilizing an RIS prototype system, this paper aims to enhance self-interference (SI) cancellation for in-band full-duplex(FD) communication systems. In FD communication, SI of a node severely limits the performance of the node by shadowing the received signal from a distant node with its own transmit signal. To this end, we propose to assist SI cancellation by exploiting a RIS to form a suitable cancellation signal, thus canceling the leaked SI in the analog domain. Building upon a 64 element RIS prototype we present results of RIS-assisted SI cancellation from a real testbed. Given a suitable amount of initial analog isolation, we are able to cancel the leaked signal by as much as -85 dB. The presented case study shows promising performance to build an FD communication system on this foundation.      
### 4.Inferring power system dynamics from synchrophasor data using Gaussian processes  [ :arrow_down: ](https://arxiv.org/pdf/2105.12608.pdf)
>  Synchrophasor data provide unprecedented opportunities for inferring power system dynamics, such as estimating voltage angles, frequencies, and accelerations along with power injection at all buses. Aligned to this goal, this work puts forth a novel framework for learning dynamics after small-signal disturbances by leveraging Gaussian processes (GPs). We extend results on learning of a linear time-invariant system using GPs to the multi-input multi-output setup. This is accomplished by decomposing power system swing dynamics into a set of single-input single-output linear systems with narrow frequency pass bands. The proposed learning technique captures time derivatives in continuous time, accommodates data streams sampled at different rates, and can cope with missing data and heterogeneous levels of accuracy. While Kalman filter-based approaches require knowing all system inputs, the proposed framework handles readings of system inputs, outputs, their derivatives, and combinations thereof collected from an arbitrary subset of buses. Relying on minimal system information, it further provides uncertainty quantification in addition to point estimates of system dynamics. Numerical tests verify that this technique can infer dynamics at non-metered buses, impute and predict synchrophasors, and locate faults under linear and non-linear system models under ambient and fault disturbances.      
### 5.AI for Calcium Scoring  [ :arrow_down: ](https://arxiv.org/pdf/2105.12558.pdf)
>  Calcium scoring, a process in which arterial calcifications are detected and quantified in CT, is valuable in estimating the risk of cardiovascular disease events. Especially when used to quantify the extent of calcification in the coronary arteries, it is a strong and independent predictor of coronary heart disease events. Advances in artificial intelligence (AI)-based image analysis have produced a multitude of automatic calcium scoring methods. While most early methods closely follow standard calcium scoring accepted in clinic, recent approaches extend this procedure to enable faster or more reproducible calcium scoring. This chapter provides an introduction to AI for calcium scoring, and an overview of the developed methods and their applications. We conclude with a discussion on AI methods in calcium scoring and propose potential directions for future research.      
### 6.Exploiting Temporal Dependencies for Cross-Modal Music Piece Identification  [ :arrow_down: ](https://arxiv.org/pdf/2105.12536.pdf)
>  This paper addresses the problem of cross-modal musical piece identification and retrieval: finding the appropriate recording(s) from a database given a sheet music query, and vice versa, working directly with audio and scanned sheet music images. The fundamental approach to this is to learn a cross-modal embedding space with a suitable similarity structure for audio and sheet image snippets, using a deep neural network, and identifying candidate pieces by cross-modal near neighbour search in this space. However, this method is oblivious of temporal aspects of music. In this paper, we introduce two strategies that address this shortcoming. First, we present a strategy that aligns sequences of embeddings learned from sheet music scans and audio snippets. A series of experiments on whole piece and fragment-level retrieval on 24 hours worth of classical piano recordings demonstrates significant improvement. Second, we show that the retrieval can be further improved by introducing an attention mechanism to the embedding learning model that reduces the effects of tempo variations in music. To conclude, we assess the scalability of our method and discuss potential measures to make it suitable for truly large-scale applications.      
### 7.MMV-Net: A Multiple Measurement Vector Network for Multi-frequency Electrical Impedance Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2105.12474.pdf)
>  Multi-frequency Electrical Impedance Tomography (mfEIT) is an emerging biomedical imaging modality to reveal frequency-dependent conductivity distributions in biomedical applications. Conventional model-based image reconstruction methods suffer from low spatial resolution, unconstrained frequency correlation and high computational cost. Deep learning has been extensively applied in solving the EIT inverse problem in biomedical and industrial process imaging. However, most existing learning-based approaches deal with the single-frequency setup, which is inefficient and ineffective when extended to address the multi-frequency setup. In this paper, we present a Multiple Measurement Vector (MMV) model based learning algorithm named MMV-Net to solve the mfEIT image reconstruction problem. MMV-Net takes into account the correlations between mfEIT images and unfolds the update steps of the Alternating Direction Method of Multipliers (ADMM) for the MMV problem. The non-linear shrinkage operator associated with the weighted l2_1 regularization term is generalized with a cascade of a Spatial Self-Attention module and a Convolutional Long Short-Term Memory (ConvLSTM) module to capture intra- and inter-frequency dependencies. The proposed MMVNet was validated on our Edinburgh mfEIT Dataset and a series of comprehensive experiments. All reconstructed results show superior image quality, convergence performance and noise robustness against the state of the art.      
### 8.Weighing Features of Lung and Heart Regions for Thoracic Disease Classification  [ :arrow_down: ](https://arxiv.org/pdf/2105.12430.pdf)
>  Chest X-rays are the most commonly available and affordable radiological examination for screening thoracic diseases. According to the domain knowledge of screening chest X-rays, the pathological information usually lay on the lung and heart regions. However, it is costly to acquire region-level annotation in practice, and model training mainly relies on image-level class labels in a weakly supervised manner, which is highly challenging for computer-aided chest X-ray screening. To address this issue, some methods have been proposed recently to identify local regions containing pathological information, which is vital for thoracic disease classification. Inspired by this, we propose a novel deep learning framework to explore discriminative information from lung and heart regions. We design a feature extractor equipped with a multi-scale attention module to learn global attention maps from global images. To exploit disease-specific cues effectively, we locate lung and heart regions containing pathological information by a well-trained pixel-wise segmentation model to generate binarization masks. By introducing element-wise logical AND operator on the learned global attention maps and the binarization masks, we obtain local attention maps in which pixels are $1$ for lung and heart region and $0$ for other regions. By zeroing features of non-lung and heart regions in attention maps, we can effectively exploit their disease-specific cues in lung and heart regions. Compared to existing methods fusing global and local features, we adopt feature weighting to avoid weakening visual cues unique to lung and heart regions. Evaluated by the benchmark split on the publicly available chest X-ray14 dataset, the comprehensive experiments show that our method achieves superior performance compared to the state-of-the-art methods.      
### 9.Permutation invariance and uncertainty in multitemporal image super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2105.12409.pdf)
>  Recent advances have shown how deep neural networks can be extremely effective at super-resolving remote sensing imagery, starting from a multitemporal collection of low-resolution images. However, existing models have neglected the issue of temporal permutation, whereby the temporal ordering of the input images does not carry any relevant information for the super-resolution task and causes such models to be inefficient with the, often scarce, ground truth data that available for training. Thus, models ought not to learn feature extractors that rely on temporal ordering. In this paper, we show how building a model that is fully invariant to temporal permutation significantly improves performance and data efficiency. Moreover, we study how to quantify the uncertainty of the super-resolved image so that the final user is informed on the local quality of the product. We show how uncertainty correlates with temporal variation in the series, and how quantifying it further improves model performance. Experiments on the Proba-V challenge dataset show significant improvements over the state of the art without the need for self-ensembling, as well as improved data efficiency, reaching the performance of the challenge winner with just 25% of the training data.      
### 10.Experimental Exploration of Unlicensed Sub-GHz Massive MIMO for Massive Internet-of-Things  [ :arrow_down: ](https://arxiv.org/pdf/2105.12402.pdf)
>  Due to the increase of Internet-of-Things (IoT) devices, IoT networks are getting overcrowded. Networks can be extended with more gateways, increasing the number of supported devices. However, as investigated in this work, massive MIMO has the potential to increase the number of simultaneous connections, while also lowering the energy expenditure of these devices. We present a study of the channel characteristics of massive MIMO in the unlicensed sub-GHz band. The goal is to support IoT applications with strict requirements in terms of number of devices, power consumption, and reliability. The assessment is based on experimental measurements using both a uniform linear and a rectangular array. Our study demonstrates and validates the advantages of deploying massive MIMO gateways to serve IoT nodes. While the results are general, here we specifically focus on static nodes. The array gain and channel hardening effect yield opportunities to lower the transmit-power of IoT nodes while also increasing reliability. The exploration confirms that exploiting large arrays brings great opportunities to connect a massive number of IoT devices by separating the nodes in the spatial domain. In addition, we give an outlook on how static IoT nodes could be scheduled based on partial channel state information.      
### 11.CBANet: Towards Complexity and Bitrate Adaptive Deep Image Compression using a Single Network  [ :arrow_down: ](https://arxiv.org/pdf/2105.12386.pdf)
>  In this paper, we propose a new deep image compression framework called Complexity and Bitrate Adaptive Network (CBANet), which aims to learn one single network to support variable bitrate coding under different computational complexity constraints. In contrast to the existing state-of-the-art learning based image compression frameworks that only consider the rate-distortion trade-off without introducing any constraint related to the computational complexity, our CBANet considers the trade-off between the rate and distortion under dynamic computational complexity constraints. Specifically, to decode the images with one single decoder under various computational complexity constraints, we propose a new multi-branch complexity adaptive module, in which each branch only takes a small portion of the computational budget of the decoder. The reconstructed images with different visual qualities can be readily generated by using different numbers of branches. Furthermore, to achieve variable bitrate decoding with one single decoder, we propose a bitrate adaptive module to project the representation from a base bitrate to the expected representation at a target bitrate for transmission. Then it will project the transmitted representation at the target bitrate back to that at the base bitrate for the decoding process. The proposed bit adaptive module can significantly reduce the storage requirement for deployment platforms. As a result, our CBANet enables one single codec to support multiple bitrate decoding under various computational complexity constraints. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of our CBANet for deep image compression.      
### 12.Training Speech Enhancement Systems with Noisy Speech Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2105.12315.pdf)
>  Recently, deep neural network (DNN)-based speech enhancement (SE) systems have been used with great success. During training, such systems require clean speech data - ideally, in large quantity with a variety of acoustic conditions, many different speaker characteristics and for a given sampling rate (e.g., 48kHz for fullband SE). However, obtaining such clean speech data is not straightforward - especially, if only considering publicly available datasets. At the same time, a lot of material for automatic speech recognition (ASR) with the desired acoustic/speaker/sampling rate characteristics is publicly available except being clean, i.e., it also contains background noise as this is even often desired in order to have ASR systems that are noise-robust. Hence, using such data to train SE systems is not straightforward. In this paper, we propose two improvements to train SE systems on noisy speech data. First, we propose several modifications of the loss functions, which make them robust against noisy speech targets. In particular, computing the median over the sample axis before averaging over time-frequency bins allows to use such data. Furthermore, we propose a noise augmentation scheme for mixture-invariant training (MixIT), which allows using it also in such scenarios. For our experiments, we use the Mozilla Common Voice dataset and we show that using our robust loss function improves PESQ by up to 0.19 compared to a system trained in the traditional way. Similarly, for MixIT we can see an improvement of up to 0.27 in PESQ when using our proposed noise augmentation.      
### 13.An Improved Random Matrix Prediction Model for Manoeuvring Extended Targets  [ :arrow_down: ](https://arxiv.org/pdf/2105.12299.pdf)
>  This paper proposes an improved prediction update for extended target tracking with the random matrix model. A key innovation is to employ a generalised non-central inverse Wishart distribution to model the state transition density of the target extent; resulting in a prediction update that accounts for kinematic state dependent transformations. Moreover, the proposed prediction update offers an additional tuning parameter c.f. previous works, requires only a single Kullback-Leibler divergence minimisation, and improves overall target tracking performance when compared to state-of-the-art alternatives.      
### 14.Photoacoustic-monitored laser treatment for tattoo removal: a feasibility study  [ :arrow_down: ](https://arxiv.org/pdf/2105.12288.pdf)
>  Skin blemishes and diseases have attracted increasing research interest in recent decades, due to their growing frequency of occurrence and the severity of related diseases. Various laser treatment approaches have been introduced for the alleviation and removal of skin pigmentation. The treatments' effects highly depend on the experience and prognosis of the relevant operators. But, the operation process lacks real-time feedback, which may directly reflect the extent of the treatment. In this manuscript, we report a photoacoustic-guided laser treatment method with a feasibility study, specifically for laser treatment targeting the tattoo's removal. The results well validated the feasibility of the proposed method through the experiments on phantoms and ex vivo pig skin samples.      
### 15.Finite sample guarantees for quantile estimation: An application to detector threshold tuning  [ :arrow_down: ](https://arxiv.org/pdf/2105.12239.pdf)
>  In threshold-based anomaly detection, we want to tune the threshold of a detector to achieve an acceptable false alarm rate. However, tuning the threshold is often a non-trivial task due to unknown detector output distributions. A detector threshold that guarantees an acceptable false alarm rate is equivalent to a specific quantile of the detector output distribution. Therefore, we use quantile estimators based on order statistics to estimate the detector threshold. The estimation of quantiles from sample data has a more than a century long tradition and we provide three different distribution-free finite sample guarantees for a class of quantile estimators. The first guarantee is based on the Dworetzky-Kiefer-Wolfowitz inequality, the second guarantee utilizes Chebyshev's inequality, and the third guarantee is based on exact confidence intervals for a binomial distribution. These guarantees are then compared and used in the detector threshold tuning problem. We use both simulated data as well as data obtained from a Hardware-in-the-Loop simulation with the Temperature Control Lab to verify the guarantees provided.      
### 16.Large-Scale Scenarios of Electric Vehicle Charging with a Data-Driven Model of Control  [ :arrow_down: ](https://arxiv.org/pdf/2105.12234.pdf)
>  Planning to support widespread transportation electrification depends on detailed estimates for the electricity demand from electric vehicles in both uncontrolled and controlled or smart charging scenarios. We present a modeling approach to rapidly generate charging estimates that include control for large-scale scenarios with millions of individual drivers. We model uncontrolled charging demand using statistical representations of real charging sessions. We model the effect of load modulation control on aggregate charging profiles with a novel machine learning approach that replaces traditional optimization approaches. We demonstrate its performance modeling workplace charging control with multiple electricity rate schedules, achieving small errors (2.5% to 4.5%), while accelerating computations by more than 4000 times. We illustrate the methodology by generating scenarios for California's 2030 charging demand including multiple charging segments and controls, with scenarios run locally in under 50 seconds, and for assisting rate design modeling the large-scale impact of a new workplace charging rate.      
### 17.Gated Fusion Network for SAO Filter and Inter Frame Prediction in Versatile Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2105.12229.pdf)
>  To achieve higher coding efficiency, Versatile Video Coding (VVC) includes several novel components, but at the expense of increasing decoder computational complexity. These technologies at a low bit rate often create contouring and ringing effects on the reconstructed frames and introduce various blocking artifacts at block boundaries. To suppress those visual artifacts, the VVC framework supports four post-processing filter operations. The interoperation of these filters introduces extra signaling bits and eventually becomes overhead at higher resolution video processing. In this paper, a novel deep learning-based model is proposed for sample adaptive offset (SAO) nonlinear filtering operation and substantiated the merits of intra-inter frame quality enhancement. We introduced a variable filter size multi-scale CNN (MSCNN) to improve the denoising operation and incorporated strided deconvolution for further computation improvement. We demonstrated that our deconvolution model can effectively be trained by leveraging the high-frequency edge features learned in a parallel fashion using feature fusion and residual learning. The simulation results demonstrate that the proposed method outperforms the baseline VVC method in BD-BR, BD-PSNR measurements and achieves an average of 3.762 % bit rate saving on the standard video test sequences.      
### 18.Safe Value Functions  [ :arrow_down: ](https://arxiv.org/pdf/2105.12204.pdf)
>  The relationship between safety and optimality in control is not well understood, and they are often seen as important yet conflicting objectives. There is a pressing need to formalize this relationship, especially given the growing prominence of learning-based methods. Indeed, it is common practice in reinforcement learning to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine this relationship, and formalize the requirements for safe value functions: value functions that are both optimal for a given task, and enforce safety. We reveal the structure of this relationship through a proof of strong duality, showing that there always exists a finite penalty that induces a safe value function. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.      
### 19.Imaging in random media by two-point coherent interferometry  [ :arrow_down: ](https://arxiv.org/pdf/2105.12174.pdf)
>  This paper considers wave-based imaging through a heterogeneous (random) scattering medium. The goal is to estimate the support of the reflectivity function of a remote scene from measurements of the backscattered wave field. The proposed imaging methodology is based on the coherent interferometric (CINT) approach that exploits the local empirical cross correlations of the measurements of the wave field. The standard CINT images are known to be robust (statistically stable) with respect to the random medium, but the stability comes at the expense of a loss of resolution. This paper shows that a two-point CINT function contains the information needed to obtain statistically stable and high-resolution images. Different methods to build such images are presented, theoretically analyzed and compared with the standard imaging approaches using numerical simulations. The first method involves a phase-retrieval step to extract the reflectivity function from the modulus of its Fourier transform. The second method involves the evaluation of the leading eigenvector of the two-point CINT imaging function seen as the kernel of a linear operator. The third method uses an optimization step to extract the reflectivity function from some cross products of its Fourier transform. The presentation is for the synthetic aperture radar data acquisition setup, where a moving sensor probes the scene with signals emitted periodically and records the resulting backscattered wave. The generalization to other imaging setups, with passive or active arrays of sensors, is discussed briefly.      
### 20.Magnetic Particle Spectroscopy (MPS) with One-stage Lock-in Implementation for Magnetic Bioassays with Improved Sensitivities  [ :arrow_down: ](https://arxiv.org/pdf/2105.12718.pdf)
>  In recent years, magnetic particle spectroscopy (MPS) has become a highly sensitive and versatile sensing technique for quantitative bioassays. It relies on the dynamic magnetic responses of magnetic nanoparticles (MNPs) for the detection of target analytes in liquid phase. There are many research studies reporting the application of MPS for detecting a variety of analytes including viruses, toxins, and nucleic acids, etc. Herein, we report a modified version of MPS platform with the addition of a one-stage lock-in design to remove the feedthrough signals induced by external driving magnetic fields, thus capturing only MNP responses for improved system sensitivity. This one-stage lock-in MPS system is able to detect as low as 781 ng multi-core Nanomag50 iron oxide MNPs (micromod Partikeltechnologie GmbH) and 78 ng single-core SHB30 iron oxide MNPs (Ocean NanoTech). In addition, using a streptavidin-biotin binding system as a proof-of-concept, we show that these single-core SHB30 MNPs can be used for Brownian relaxation-based bioassays while the multi-core Nanomag50 cannot be used. The effects of MNP amount on the concentration dependent response profiles for detecting streptavidin was also investigated. Results show that by using lower concentration/amount of MNPs, concentration-response curves shift to lower concentration/amount of target analytes. This lower concentrationresponse indicates the possibility of improved bioassay sensitivities by using lower amounts of MNPs.      
### 21.Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2105.12708.pdf)
>  Loanwords, such as Anglicisms, are a challenge in German speech recognition. Due to their irregular pronunciation compared to native German words, automatically generated pronunciation dictionaries often include faulty phoneme sequences for Anglicisms. In this work, we propose a multitask sequence-to-sequence approach for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms. We extended a grapheme-to-phoneme model with a classifier to distinguish Anglicisms from native German words. With this approach, the model learns to generate pronunciations differently depending on the classification result. We used our model to create supplementary Anglicism pronunciation dictionaries that are added to an existing German speech recognition model. Tested on a dedicated Anglicism evaluation set, we improved the recognition of Anglicisms compared to a baseline model, reducing the word error rate by 1 % and the Anglicism error rate by 3 %. We show that multitask learning can help solving the challenge of loanwords in German speech recognition.      
### 22.Topological Pilot Assignment in Large-Scale Distributed MIMO Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.12645.pdf)
>  We consider the pilot assignment problem in large-scale distributed multi-input multi-output (MIMO) networks, where a large number of remote radio head (RRH) antennas are randomly distributed in a wide area, and jointly serve a relatively smaller number of users (UE) coherently. By artificially imposing topological structures on the UE-RRH connectivity, we model the network by a partially-connected interference network, so that the pilot assignment problem can be cast as a topological interference management problem with multiple groupcast messages. Building upon such connection, we formulate the topological pilot assignment (TPA) problem in two different ways with respect to whether or not the to-be-estimated channel connectivity pattern is known a priori. When it is known, we formulate the TPA problem as a low-rank matrix completion problem that can be solved by a simple alternating projection algorithm. Otherwise, we formulate it as a sequential maximum weight induced matching problem that can be solved by either a mixed integer linear program or a simple yet efficient greedy algorithm. With respect to two different formulations of the TPA problem, we evaluate the efficiency of the proposed algorithms under the cell-free massive MIMO setting.      
### 23.Perspective -- On the thermodynamics of perfect unconditional security  [ :arrow_down: ](https://arxiv.org/pdf/2105.12592.pdf)
>  A secure key distribution (exchange) scheme is unconditionally secure if it is unbreakable against arbitrary technological improvements of computing power and/or any development of new algorithms. There are only two families of experimentally realized and tested unconditionally secure key distribution technologies: Quantum Key Distribution (QKD), the base of quantum cryptography, which utilizes quantum physical photonic features; and the Kirchhoff-Law-Johnson-Noise (KLJN) system that is based on classical statistical physics (fluctuation-dissipation theorem). The focus topic of this paper is the thermodynamical situation of the KLJN system. In all the original works, the proposed KLJN schemes required thermal equilibrium between the devices of the communicating parties to achieve perfect security. However, Vadai, et al, in (Nature) Science Reports 5 (2015) 13653 shows a modified scheme, where there is a non-zero thermal noise energy flow between the parties, yet the system seems to resist all the known attack types. We introduce a new attack type against their system. The new attack utilizes coincidence events between the line current and voltages. We show that there is non-zero information leak toward the Eavesdropper, even under idealized conditions. As soon as the thermal equilibrium is restored, the system becomes perfectly secure again. In conclusion, perfect unconditional security requires thermal equilibrium.      
### 24.Predicting invasive ductal carcinoma using a Reinforcement Sample Learning Strategy using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2105.12564.pdf)
>  Invasive ductal carcinoma is a prevalent, potentially deadly disease associated with a high rate of morbidity and mortality. Its malignancy is the second leading cause of death from cancer in women. The mammogram is an extremely useful resource for mass detection and invasive ductal carcinoma diagnosis. We are proposing a method for Invasive ductal carcinoma that will use convolutional neural networks (CNN) on mammograms to assist radiologists in diagnosing the disease. Due to the varying image clarity and structure of certain mammograms, it is difficult to observe major cancer characteristics such as microcalcification and mass, and it is often difficult to interpret and diagnose these attributes. The aim of this study is to establish a novel method for fully automated feature extraction and classification in invasive ductal carcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor classification algorithm that makes novel use of convolutional neural networks on breast mammogram images to increase feature extraction and training speed. The algorithm makes two contributions.      
### 25.On Robustness of Kernel-Based Regularized System Identification  [ :arrow_down: ](https://arxiv.org/pdf/2105.12516.pdf)
>  This paper presents a novel feature of the kernel-based system identification method. We prove that the regularized kernel-based approach for the estimation of a finite impulse response is equivalent to a robust least-squares problem with a particular uncertainty set defined in terms of the kernel matrix, and thus, it is called kernel-based uncertainty set. We provide a theoretical foundation for the robustness of the kernel-based approach to input disturbances. Based on robust and regularized least-squares methods, different formulations of system identification are considered, where the kernel-based uncertainty set is employed in some of them. We apply these methods to a case where the input measurements are subject to disturbances. Subsequently, we perform extensive numerical experiments and compare the results to examine the impact of utilizing kernel-based uncertainty sets in the identification procedure. The numerical experiments confirm that the robust least square identification approach with the kernel-based uncertainty set improves the robustness of the estimation to the input disturbances.      
### 26.Compensating class imbalance for acoustic chimpanzee detection with convolutional recurrent neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.12502.pdf)
>  Automatic detection systems are important in passive acoustic monitoring (PAM) systems, as these record large amounts of audio data which are infeasible for humans to evaluate manually. In this paper we evaluated methods for compensating class imbalance for deep-learning based automatic detection of acoustic chimpanzee calls. The prevalence of chimpanzee calls in natural habitats is very rare, i.e. databases feature a heavy imbalance between background and target calls. Such imbalances can have negative effects on classifier performances. We employed a state-of-the-art detection approach based on convolutional recurrent neural networks (CRNNs). We extended the detection pipeline through various stages for compensating class imbalance. These included (1) spectrogram denoising, (2) alternative loss functions, and (3) resampling. Our key findings are: (1) spectrogram denoising operations significantly improved performance for both target classes, (2) standard binary cross entropy reached the highest performance, and (3) manipulating relative class imbalance through resampling either decreased or maintained performance depending on the target class. Finally, we reached detection performances of 33% for drumming and 5% for vocalization, which is a &gt;7 fold increase compared to previously published results. We conclude that supporting the network to learn decoupling noise conditions from foreground classes is of primary importance for increasing performance.      
### 27.Designing ECG Monitoring Healthcare System with Federated Transfer Learning and Explainable AI  [ :arrow_down: ](https://arxiv.org/pdf/2105.12497.pdf)
>  Deep learning play a vital role in classifying different arrhythmias using the electrocardiography (ECG) data. Nevertheless, training deep learning models normally requires a large amount of data and it can lead to privacy concerns. Unfortunately, a large amount of healthcare data cannot be easily collected from a single silo. Additionally, deep learning models are like black-box, with no explainability of the predicted results, which is often required in clinical healthcare. This limits the application of deep learning in real-world health systems. In this paper, we design a new explainable artificial intelligence (XAI) based deep learning framework in a federated setting for ECG-based healthcare applications. The federated setting is used to solve issues such as data availability and privacy concerns. Furthermore, the proposed framework setting effectively classifies arrhythmia's using an autoencoder and a classifier, both based on a convolutional neural network (CNN). Additionally, we propose an XAI-based module on top of the proposed classifier to explain the classification results, which help clinical practitioners make quick and reliable decisions. The proposed framework was trained and tested using the MIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98% for arrhythmia detection using noisy and clean data, respectively, with five-fold cross-validation.      
### 28.Dynamic Games of Social Distancing during an Epidemic: Analysis of Asymmetric Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2105.12431.pdf)
>  Individual behaviors play an essential role in the dynamics of transmission of infectious diseases, including COVID--19. This paper studies a dynamic game model that describes the social distancing behaviors during an epidemic, assuming a continuum of players and individual infection dynamics. The evolution of the players' infection states follows a variant of the well-known SIR dynamics. We assume that the players are not sure about their infection state, and thus they choose their actions based on their individually perceived probabilities of being susceptible, infected or removed. The cost of each player depends both on her infection state and on the contact with others. We prove the existence of a Nash equilibrium and characterize Nash equilibria using nonlinear complementarity problems. We then exploit some monotonicity properties of the optimal policies to obtain a reduced-order characterization for Nash equilibrium and reduce its computation to the solution of a low-dimensional optimization problem. It turns out that, even in the symmetric case, where all the players have the same parameters, players may have very different behaviors. We finally present some numerical studies that illustrate this interesting phenomenon and investigate the effects of several parameters, including the players' vulnerability, the time horizon, and the maximum allowed actions, on the optimal policies and the players' costs.      
### 29.Receptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.12395.pdf)
>  In this paper, we study the performance of variants of well-known Convolutional Neural Network (CNN) architectures on different audio tasks. We show that tuning the Receptive Field (RF) of CNNs is crucial to their generalization. An insufficient RF limits the CNN's ability to fit the training data. In contrast, CNNs with an excessive RF tend to over-fit the training data and fail to generalize to unseen testing data. As state-of-the-art CNN architectures-in computer vision and other domains-tend to go deeper in terms of number of layers, their RF size increases and therefore they degrade in performance in several audio classification and tagging tasks. We study well-known CNN architectures and how their building blocks affect their receptive field. We propose several systematic approaches to control the RF of CNNs and systematically test the resulting architectures on different audio classification and tagging tasks and datasets. The experiments show that regularizing the RF of CNNs using our proposed approaches can drastically improve the generalization of models, out-performing complex architectures and pre-trained models on larger datasets. The proposed CNNs achieve state-of-the-art results in multiple tasks, from acoustic scene classification to emotion and theme detection in music to instrument recognition, as demonstrated by top ranks in several pertinent challenges (DCASE, MediaEval).      
### 30.User Grouping and Reflective Beamforming for IRS-Aided URLLC  [ :arrow_down: ](https://arxiv.org/pdf/2105.12360.pdf)
>  This paper studies an intelligent reflecting surface (IRS)-aided downlink ultra-reliable and low-latency communication (URLLC) system, in which an IRS is dedicatedly deployed to assist a base station (BS) to send individual short-packet messages to multiple users. To enhance the URLLC performance, the users are divided into different groups and the messages for users in each group are encoded into a single codeword. By considering the time division multiple access (TDMA) protocol among different groups, our objective is to minimize the total latency for all users subject to their individual reliability requirements, via jointly optimizing the user grouping and block-length allocation at the BS together with the reflective beamforming at the IRS. We solve the latency minimization problem via the alternating optimization, in which the blocklengths and the reflective beamforming are optimized by using the techniques of successive convex approximation (SCA) and semi-definite relaxation (SDR), while the user grouping is updated by K-means and greedy-based methods. Numerical results show that the proposed designs can significantly reduce the communication latency, as compared to various benchmark schemes, which unveil the importance of user grouping and reflective beamforming optimization for exploiting the joint encoding gain and enhancing the worst-case user performance.      
### 31.Certainty Equivalent Quadratic Control for Markov Jump Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.12358.pdf)
>  Real-world control applications often involve complex dynamics subject to abrupt changes or variations. Markov jump linear systems (MJS) provide a rich framework for modeling such dynamics. Despite an extensive history, theoretical understanding of parameter sensitivities of MJS control is somewhat lacking. Motivated by this, we investigate robustness aspects of certainty equivalent model-based optimal control for MJS with quadratic cost function. Given the uncertainty in the system matrices and in the Markov transition matrix is bounded by $\epsilon$ and $\eta$ respectively, robustness results are established for (i) the solution to coupled Riccati equations and (ii) the optimal cost, by providing explicit perturbation bounds which decay as $\mathcal{O}(\epsilon + \eta)$ and $\mathcal{O}((\epsilon + \eta)^2)$ respectively.      
### 32.A data-driven approach to beating SAA out-of-sample  [ :arrow_down: ](https://arxiv.org/pdf/2105.12342.pdf)
>  While solutions of Distributionally Robust Optimization (DRO) problems can sometimes have a higher out-of-sample expected reward than the Sample Average Approximation (SAA), there is no guarantee. In this paper, we introduce the class of Distributionally Optimistic Optimization (DOO) models, and show that it is always possible to "beat" SAA out-of-sample if we consider not just worst-case (DRO) models but also best-case (DOO) ones. We also show, however, that this comes at a cost: Optimistic solutions are more sensitive to model error than either worst-case or SAA optimizers, and hence are less robust.      
### 33.Comparison of Dynamic and Kinematic Model Driven Extended Kalman Filters (EKF) for the Localization of Autonomous Underwater Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2105.12309.pdf)
>  Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs) are used for a wide variety of missions related to exploration and scientific research. Successful navigation by these systems requires a good localization system. Kalman filter based localization techniques have been prevalent since the early 1960s and extensive research has been carried out using them, both in development and in design. It has been found that the use of a dynamic model (instead of a kinematic model) in the Kalman filter can lead to more accurate predictions, as the dynamic model takes the forces acting on the AUV into account. Presented in this paper is a motion-predictive extended Kalman filter (EKF) for AUVs using a simplified dynamic model. The dynamic model is derived first and then it was simplified for a RexROV, a type of submarine vehicle used in simple underwater exploration, inspection of subsea structures, pipelines and shipwrecks. The filter was implemented with a simulated vehicle in an open-source marine vehicle simulator called UUV Simulator and the results were compared with the ground truth. The results show good prediction accuracy for the dynamic filter, though improvements are needed before the EKF can be used on real-time. Some perspective and discussion on practical implementation is presented to show the next steps needed for this concept.      
### 34.Optimal Transport Based Refinement of Physics-Informed Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.12307.pdf)
>  In this paper, we propose a refinement strategy to the well-known Physics-Informed Neural Networks (PINNs) for solving partial differential equations (PDEs) based on the concept of Optimal Transport (OT). <br>Conventional black-box PINNs solvers have been found to suffer from a host of issues: spectral bias in fully-connected architectures, unstable gradient pathologies, as well as difficulties with convergence and accuracy. <br>Current network training strategies are agnostic to dimension sizes and rely on the availability of powerful computing resources to optimize through a large number of collocation points. <br>This is particularly challenging when studying stochastic dynamical systems with the Fokker-Planck-Kolmogorov Equation (FPKE), a second-order PDE which is typically solved in high-dimensional state space. <br>While we focus exclusively on the stationary form of the FPKE, positivity and normalization constraints on its solution make it all the more unfavorable to solve directly using standard PINNs approaches. <br>To mitigate the above challenges, we present a novel training strategy for solving the FPKE using OT-based sampling to supplement the existing PINNs framework. <br>It is an iterative approach that induces a network trained on a small dataset to add samples to its training dataset from regions where it nominally makes the most error. <br>The new samples are found by solving a linear programming problem at every iteration. <br>The paper is complemented by an experimental evaluation of the proposed method showing its applicability on a variety of stochastic systems with nonlinear dynamics.      
### 35.Collision Recovery Control of a Foldable Quadrotor  [ :arrow_down: ](https://arxiv.org/pdf/2105.12273.pdf)
>  Autonomous missions of small unmanned aerial vehicles (UAVs) are prone to collisions owing to environmental disturbances and localization errors. Consequently, a UAV that can endure collisions and perform recovery control in critical aerial missions is desirable to prevent loss of the vehicle and/or payload. We address this problem by proposing a novel foldable quadrotor system which can sustain collisions and recover safely. The quadrotor is designed with integrated mechanical compliance using a torsional spring such that the impact time is increased and the net impact force on the main body is decreased. The post-collision dynamics is analysed and a recovery controller is proposed which stabilizes the system to a hovering location without additional collisions. Flight test results on the proposed and a conventional quadrotor demonstrate that for the former, integrated spring-damper characteristics reduce the rebound velocity and lead to simple recovery control algorithms in the event of unintended collisions as compared to a rigid quadrotor of the same dimension.      
### 36.On Secrecy Performance of Mixed α-η-μ and Malaga RF-FSO Variable Gain Relaying Channel  [ :arrow_down: ](https://arxiv.org/pdf/2105.12265.pdf)
>  With the completion of standardization of fifth-generation (5G) networks, the researchers have begun visioning sixth-generation (6G) networks that are predicted to be human-centric. Hence, similar to 5G networks, besides high data rate, providing secrecy and privacy will be the center of attention by the wireless research community. To support the visions beyond 5G (B5G) and 6G, in this paper we propose a secure radio frequency (RF)-free-space optical (FSO) mixed framework under the attempt of wiretapping by an eavesdropper at the RF hop. We assume the RF links undergo alpha-eta-mu fading whereas the FSO link exhibits a unified Malaga turbulence model with pointing error. The secrecy performance is evaluated by deducing expressions for three secrecy metrics i.e. average secrecy capacity, secure outage probability, and probability of non-zero secrecy capacity in terms of univariate and bivariate Meijer's G and Fox's H functions. We further capitalize on these expressions to demonstrate the impacts of fading, atmospheric turbulence, and pointing errors and show a comparison between two detection techniques (i.e. heterodyne detection (HD) and intensity modulation with direct detection (IM/DD)) that clearly reveals better secrecy can be achieved with HD technique relative to the IM/DD method. The inclusion of generalized fading models at the RF and FSO hops offers unification of several classical scenarios as special cases thereby exhibiting a more generic nature relative to the existing literature. Finally, all the analytical results are corroborated via Monte-Carlo simulations.      
### 37.Bayesian Nonparametric Reinforcement Learning in LTE and Wi-Fi Coexistence  [ :arrow_down: ](https://arxiv.org/pdf/2105.12249.pdf)
>  With the formation of next generation wireless communication, a growing number of new applications like internet of things, autonomous car, and drone is crowding the unlicensed spectrum. Licensed network such as LTE also comes to the unlicensed spectrum for better providing high-capacity contents with low cost. However, LTE was not designed for sharing spectrum with others. A cooperation center for these networks is costly because they possess heterogeneous properties and everyone can enter and leave the spectrum unrestrictedly, so the design will be challenging. Since it is infeasible to incorporate potentially infinite scenarios with one unified design, an alternative solution is to let each network learn its own coexistence policy. Previous solutions only work on fixed scenarios. In this work a reinforcement learning algorithm is presented to cope with the coexistence between Wi-Fi and LTE-LAA agents in 5 GHz unlicensed spectrum. The coexistence problem was modeled as a Dec-POMDP and Bayesian approach was adopted for policy learning with nonparametric prior to accommodate the uncertainty of policy for different agents. A fairness measure was introduced in the reward function to encourage fair sharing between agents. The reinforcement learning was turned into an optimization problem by transforming the value function as likelihood and variational inference for posterior approximation. Simulation results demonstrate that this algorithm can reach high value with compact policy representations, and stay computationally efficient when applying to agent set.      
### 38.Learning a Model-Driven Variational Network for Deformable Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2105.12227.pdf)
>  Data-driven deep learning approaches to image registration can be less accurate than conventional iterative approaches, especially when training data is limited. To address this whilst retaining the fast inference speed of deep learning, we propose VR-Net, a novel cascaded variational network for unsupervised deformable image registration. Using the variable splitting optimization scheme, we first convert the image registration problem, established in a generic variational framework, into two sub-problems, one with a point-wise, closed-form solution while the other one is a denoising problem. We then propose two neural layers (i.e. warping layer and intensity consistency layer) to model the analytical solution and a residual U-Net to formulate the denoising problem (i.e. generalized denoising layer). Finally, we cascade the warping layer, intensity consistency layer, and generalized denoising layer to form the VR-Net. Extensive experiments on three (two 2D and one 3D) cardiac magnetic resonance imaging datasets show that VR-Net outperforms state-of-the-art deep learning methods on registration accuracy, while maintains the fast inference speed of deep learning and the data-efficiency of variational model.      
### 39.A risk analysis framework for real-time control systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.12225.pdf)
>  We present a Monte Carlo simulation framework for analysing the risk involved in deploying real-time control systems in safety-critical applications, as well as an algorithm design technique allowing one (in certain situations) to robustify a control algorithm. Both approaches are very general and agnostic to the initial control algorithm. We present examples showing that these techniques can be used to analyse the reliability of implementations of non-linear model predictive control algorithms.      
### 40.Robust Value Iteration for Continuous Control Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2105.12189.pdf)
>  When transferring a control policy from simulation to a physical system, the policy needs to be robust to variations in the dynamics to perform well. Commonly, the optimal policy overfits to the approximate model and the corresponding state-distribution, often resulting in failure to trasnfer underlying distributional shifts. In this paper, we present Robust Fitted Value Iteration, which uses dynamic programming to compute the optimal value function on the compact state domain and incorporates adversarial perturbations of the system dynamics. The adversarial perturbations encourage a optimal policy that is robust to changes in the dynamics. Utilizing the continuous-time perspective of reinforcement learning, we derive the optimal perturbations for the states, actions, observations and model parameters in closed-form. Notably, the resulting algorithm does not require discretization of states or actions. Therefore, the optimal adversarial perturbations can be efficiently incorporated in the min-max value function update. We apply the resulting algorithm to the physical Furuta pendulum and cartpole. By changing the masses of the systems we evaluate the quantitative and qualitative performance across different model parameters. We show that robust value iteration is more robust compared to deep reinforcement learning algorithm and the non-robust version of the algorithm. Videos of the experiments are shown at <a class="link-external link-https" href="https://sites.google.com/view/rfvi" rel="external noopener nofollow">this https URL</a>      
### 41.Degradation Aware Predictive Energy Management Strategy for Ship Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.12173.pdf)
>  Integration of modern defence weapons into ship power systems poses a challenge in terms of meeting the high ramp rate requirements of those loads. It might be demanding for the generators to meet the ramp rates of these loads. Failure to meet so, might lead to stability issues. This is addressed by conglomeration of generators and energy storage elements to handle the required power demand posed by loads. This paper proposes an energy management strategy based on model predictive control that incorporates the uncertainty in the load prediction. The proposed controller optimally coordinates the power split between the generators and energy storage elements to guarantee that the power demand is met taking into considerations the ramp rate limitations and the load uncertainty. A low bandwidth model consisting of a single generator and a single energy storage element is used to validate the results of the proposed energy management strategy. The results demonstrate the robustness of the controller under load prediction uncertainty and demonstrate the effect of load uncertainty on battery capacity loss.      
### 42.AutoReCon: Neural Architecture Search-based Reconstruction for Data-free Compression  [ :arrow_down: ](https://arxiv.org/pdf/2105.12151.pdf)
>  Data-free compression raises a new challenge because the original training dataset for a pre-trained model to be compressed is not available due to privacy or transmission issues. Thus, a common approach is to compute a reconstructed training dataset before compression. The current reconstruction methods compute the reconstructed training dataset with a generator by exploiting information from the pre-trained model. However, current reconstruction methods focus on extracting more information from the pre-trained model but do not leverage network engineering. This work is the first to consider network engineering as an approach to design the reconstruction method. Specifically, we propose the AutoReCon method, which is a neural architecture search-based reconstruction method. In the proposed AutoReCon method, the generator architecture is designed automatically given the pre-trained model for reconstruction. Experimental results show that using generators discovered by the AutoRecon method always improve the performance of data-free compression.      
