# ArXiv eess --Tue, 18 May 2021
### 1.Unsupervised Deep Learning Methods for Biological Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2105.08040.pdf)
>  Recently, deep learning approaches have become the main research frontier for biological image reconstruction problems thanks to their high performance, along with their ultra-fast reconstruction times. However, due to the difficulty of obtaining matched reference data for supervised learning, there has been increasing interest in unsupervised learning approaches that do not need paired reference data. In particular, self-supervised learning and generative models have been successfully used for various biological imaging applications. In this paper, we overview these approaches from a coherent perspective in the context of classical inverse problems, and discuss their applications to biological imaging.      
### 2.56 GBaud PAM-4 100 km Transmission System with Photonic Processing Schemes  [ :arrow_down: ](https://arxiv.org/pdf/2105.07990.pdf)
>  Analog photonic computing has been proposed and tested in recent years as an alternative approach for data recovery in fiber transmission systems. Photonic reservoir computing, performing nonlinear transformations of the transmitted signals and exhibiting internal fading memory, has been found advantageous for this kind of processing. In this work, we show that the effectiveness of the internal fading memory depends significantly on the properties of the signal to be processed. Specifically, we demonstrate two experimental photonic post-processing schemes for a 56 GBaud PAM-4 experimental transmission system, with 100 km uncompensated standard single-mode fiber and direct detection. We show that, for transmission systems with significant chromatic dispersion, the contribution of a photonic reservoir's fading memory to the computational performance is limited. In a comparison between the data recovery performances between a reservoir computing and an extreme learning machine fiber-based configuration, we find that both offer equivalent data recovery. The extreme learning machine approach eliminates the necessity of external recurrent connectivity, which simplifies the system and increases the computation speed. Error-free data recovery is experimentally demonstrated for an optical signal to noise ratio above 30 dB, outperforming an implementation of a Kramers-Kronig receiver in the digital domain.      
### 3.DFENet: A Novel Dimension Fusion Edge Guided Network for Brain MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07962.pdf)
>  The rapid increment of morbidity of brain stroke in the last few years have been a driving force towards fast and accurate segmentation of stroke lesions from brain MRI images. With the recent development of deep-learning, computer-aided and segmentation methods of ischemic stroke lesions have been useful for clinicians in early diagnosis and treatment planning. However, most of these methods suffer from inaccurate and unreliable segmentation results because of their inability to capture sufficient contextual features from the MRI volumes. To meet these requirements, 3D convolutional neural networks have been proposed, which, however, suffer from huge computational requirements. To mitigate these problems, we propose a novel Dimension Fusion Edge-guided network (DFENet) that can meet both of these requirements by fusing the features of 2D and 3D CNNs. Unlike other methods, our proposed network uses a parallel partial decoder (PPD) module for aggregating and upsampling selected features, rich in important contextual information. Additionally, we use an edge-guidance and enhanced mixing loss for constantly supervising and improvising the learning process of the network. The proposed method is evaluated on publicly available Anatomical Tracings of Lesions After Stroke (ATLAS) dataset, resulting in mean DSC, IoU, Precision and Recall values of 0.5457, 0.4015, 0.6371, and 0.4969 respectively. The results, when compared to other state-of-the-art methods, outperforms them by a significant margin. Therefore, the proposed model is robust, accurate, superior to the existing methods, and can be relied upon for biomedical applications.      
### 4.Joint Optimization of Hadamard Sensing and Reconstruction in Compressed Sensing Fluorescence Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2105.07961.pdf)
>  Compressed sensing fluorescence microscopy (CS-FM) proposes a scheme whereby less measurements are collected during sensing and reconstruction is performed to recover the image. Much work has gone into optimizing the sensing and reconstruction portions separately. We propose a method of jointly optimizing both sensing and reconstruction end-to-end under a total measurement constraint, enabling learning of the optimal sensing scheme concurrently with the parameters of a neural network-based reconstruction network. We train our model on a rich dataset of confocal, two-photon, and wide-field microscopy images comprising of a variety of biological samples. We show that our method outperforms several baseline sensing schemes and a regularized regression reconstruction algorithm.      
### 5.Distributionally Robust Chance-Constrained Flexibility Planning for Integrated Energy System  [ :arrow_down: ](https://arxiv.org/pdf/2105.07906.pdf)
>  Inflexible combined heat and power (CHP) plants and uncertain wind power production result in excess power in distribution networks, which leads to inverse power flow challenging grid operations. Power-to-X facilities such as electrolysers and electric boilers can offer extra flexibility to the integrated energy system. In this regard, we aim to jointly determine the optimal Power-to-X facility sizing and integrated energy system operations in this study. To account for wind power uncertainties, a distributionally robust chance-constrained model is developed to characterize wind power uncertainties using ambiguity sets. Linear decision rules are applied to analytically express real-time recourse actions when uncertainties are exposed, which allows the propagation of wind power uncertainties to gas and heat systems. Accordingly, the developed three-stage distributionally robust chance-constrained model is converted into a computationally tractable single-stage mixed-integer conic model. A case study validates the effectiveness of introducing the electrolyser and electric boiler into the integrated energy system, with respect to the decreased system cost, expanded CHP plant flexibility and reduced inverse power flow. The developed distributionally robust optimization model exhibits better effectiveness and robustness compared to a chance-constrained optimization model assuming wind forecast errors follow Gaussian distribution. Detailed profit analysis reveals that although the overall system cost is minimized, the profit is distributed unevenly across various stakeholders in the system.      
### 6.ADMM and Spectral Proximity Operators in Hyperspectral Broadband Phase Retrieval for Quantitative Phase Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2105.07891.pdf)
>  A novel formulation of the hyperspectral broadband phase retrieval is developed for the scenario where both object and modulation phase masks are spectrally varying. The proposed algorithm is based on a complex domain version of the alternating direction method of multipliers (ADMM) and Spectral Proximity Operators (SPO) derived for Gaussian and Poissonian observations. Computations for these operators are reduced to the solution of sets of cubic (for Gaussian) and quadratic (for Poissonian) algebraic equations. These proximity operators resolve two problems. Firstly, the complex domain spectral components of signals are extracted from the total intensity observations calculated as sums of the signal spectral intensities. In this way, the spectral analysis of the total intensities is achieved. Secondly, the noisy observations are filtered, compromising noisy intensity observations and their predicted counterparts. The ability to resolve the hyperspectral broadband phase retrieval problem and to find the spectrum varying object are essentially defined by the spectral properties of object and image formation operators. The simulation tests demonstrate that the phase retrieval in this formulation can be successfully resolved.      
### 7.Fast and Accurate Camera Scene Detection on Smartphones  [ :arrow_down: ](https://arxiv.org/pdf/2105.07869.pdf)
>  AI-powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone, though the problem of accurate scene prediction has not yet been addressed by the research community. This paper for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD) containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper are available on the project website.      
### 8.Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI 2021 Challenge: Report  [ :arrow_down: ](https://arxiv.org/pdf/2105.07825.pdf)
>  Image super-resolution is one of the most popular computer vision problems with many important applications to mobile devices. While many solutions have been proposed for this task, they are usually not optimized even for common smartphone AI hardware, not to mention more constrained smart TV platforms that are often supporting INT8 inference only. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image super-resolution solutions that can demonstrate a real-time performance on mobile or edge NPUs. For this, the participants were provided with the DIV2K dataset and trained quantized models to do an efficient 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated NPU capable of accelerating quantized neural networks. The proposed solutions are fully compatible with all major mobile AI accelerators and are capable of reconstructing Full HD images under 40-60 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper.      
### 9.Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report  [ :arrow_down: ](https://arxiv.org/pdf/2105.07809.pdf)
>  As the quality of mobile cameras starts to play a crucial role in modern smartphones, more and more attention is now being paid to ISP algorithms used to improve various perceptual aspects of mobile photos. In this Mobile AI challenge, the target was to develop an end-to-end deep learning-based image signal processing (ISP) pipeline that can replace classical hand-crafted ISPs and achieve nearly real-time performance on smartphone NPUs. For this, the participants were provided with a novel learned ISP dataset consisting of RAW-RGB image pairs captured with the Sony IMX586 Quad Bayer mobile sensor and a professional 102-megapixel medium format camera. The runtime of all models was evaluated on the MediaTek Dimensity 1000+ platform with a dedicated AI processing unit capable of accelerating both floating-point and quantized neural networks. The proposed solutions are fully compatible with the above NPU and are capable of processing Full HD photos under 60-100 milliseconds while achieving high fidelity results. A detailed description of all models developed in this challenge is provided in this paper.      
### 10.Deep regression for uncertainty-aware and interpretable analysis of large-scale body MRI  [ :arrow_down: ](https://arxiv.org/pdf/2105.07797.pdf)
>  Large-scale medical studies such as the UK Biobank examine thousands of volunteer participants with medical imaging techniques. Combined with the vast amount of collected metadata, anatomical information from these images has the potential for medical analyses at unprecedented scale. However, their evaluation often requires manual input and long processing times, limiting the amount of reference values for biomarkers and other measurements available for research. Recent approaches with convolutional neural networks for regression can perform these evaluations automatically. On magnetic resonance imaging (MRI) data of more than 40,000 UK Biobank subjects, these systems can estimate human age, body composition and more. This style of analysis is almost entirely data-driven and no manual intervention or guidance with manually segmented ground truth images is required. The networks often closely emulate the reference method that provided their training data and can reach levels of agreement comparable to the expected variability between established medical gold standard techniques. The risk of silent failure can be individually quantified by predictive uncertainty obtained from a mean-variance criterion and ensembling. Saliency analysis furthermore enables an interpretation of the underlying relevant image features and showed that the networks learned to correctly target specific organs, limbs, and regions of interest.      
### 11.Complex Frequency  [ :arrow_down: ](https://arxiv.org/pdf/2105.07769.pdf)
>  The paper introduces the concept of complex frequency. The imaginary part of the complex frequency is the variation with respect of a synchronous reference of the local bus frequency as commonly defined in power system studies. The real part is defined based on the variation of the voltage magnitude. The latter term is crucial for the correct interpretation and analysis of the variation of the frequency at each bus of the network. The paper also develops a set of differential equations that describe the link between complex powers and complex frequencies at network buses in transient conditions. No simplifications are assumed except for constant elements of the network admittance matrix. A variety of analytical and numerical examples show the applications and potentials of the proposed concept.      
### 12.Efficient Off-Policy Q-Learning for Data-Based Discrete-Time LQR Problems  [ :arrow_down: ](https://arxiv.org/pdf/2105.07761.pdf)
>  This paper introduces and analyzes an improved Q-learning algorithm for discrete-time linear time-invariant systems. The proposed method does not require any knowledge of the system dynamics, and it enjoys significant efficiency advantages over other data-based optimal control methods in the literature. This algorithm can be fully executed off-line, as it does not require to apply the current estimate of the optimal input to the system as in on-policy algorithms. It is shown that a persistently exciting input, defined from an easily tested matrix rank condition, guarantees the convergence of the algorithm. Moreover, the method avoids the use of linear matrix inequalities (LMIs) for control design, decreasing the corresponding computational complexity. A data-based method is proposed to design the initial stabilizing feedback gain that the algorithm requires. Robustness of the algorithm in the presence of noisy measurements is analyzed. Both theoretical and simulation comparisons are performed to show the advantages of this algorithm against other model-free control design methods.      
### 13.Low-Input Accurate Periodic Motion of an Underactuated Mechanism: Mass Distribution and Nonlinear Spring Shaping  [ :arrow_down: ](https://arxiv.org/pdf/2105.07745.pdf)
>  This work presents a control-oriented structural design approach for a 2-DOF underactuated mechanical system, with the purpose of generating an optimal oscillatory behavior of the end-effector. To achieve the desired periodic motion, we propose to adjust the dynamic response of the mechanism by selecting its mass distribution and the characteristic of a nonlinear spring. In particular, we introduce a two-step optimization strategy to shape the system's zero dynamics, obtained via input-output linearization. The first part of the procedure aims to minimize the root-mean-square value of the input torque by optimizing the mechanism's mass distribution. In this context, we show that a perfect matching with the desired trajectory can be reached by assuming the ability to design an arbitrary shape of the system's elastic properties. Then, in order to favor a simpler physical implementation of the structure, we dedicate the second optimization step to the piecewise linear approximation of the previously defined stiffness characteristic. The proposed procedure is finally tested in detailed numerical simulations, confirming its effectiveness in generating a complex and efficient periodic motion.      
### 14.Probabilistic robust linear quadratic regulators with Gaussian processes  [ :arrow_down: ](https://arxiv.org/pdf/2105.07668.pdf)
>  Probabilistic models such as Gaussian processes (GPs) are powerful tools to learn unknown dynamical systems from data for subsequent use in control design. While learning-based control has the potential to yield superior performance in demanding applications, robustness to uncertainty remains an important challenge. Since Bayesian methods quantify uncertainty of the learning results, it is natural to incorporate these uncertainties into a robust design. In contrast to most state-of-the-art approaches that consider worst-case estimates, we leverage the learning method's posterior distribution in the controller synthesis. The result is a more informed and, thus, more efficient trade-off between performance and robustness. We present a novel controller synthesis for linearized GP dynamics that yields robust controllers with respect to a probabilistic stability margin. The formulation is based on a recently proposed algorithm for linear quadratic control synthesis, which we extend by giving probabilistic robustness guarantees in the form of credibility bounds for the system's stability.Comparisons to existing methods based on worst-case and certainty-equivalence designs reveal superior performance and robustness properties of the proposed method.      
### 15.Optimal Spatial Signal Design for mmWave Positioning under Imperfect Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2105.07664.pdf)
>  We consider the problem of spatial signal design for multipath-assisted mmWave positioning under synchronization errors and location uncertainties. First, the optimal precoder is shown to lie in a low-dimensional subspace spanned by directional and derivative beams pointing towards the existing paths. Under imperfect knowledge of path parameters, we propose an optimal robust design and a codebook-based heuristic design with optimized beam power allocation by exploiting the low-dimensional structure. Through numerical results, we characterize three different position error bound (PEB) regimes with respect to clock bias uncertainty and show that the proposed low-complexity codebook-based designs outperform the conventional directional beam codebook and achieve near-optimal PEB performance.      
### 16.Dual-Stage Low-Complexity Reconfigurable Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2105.07632.pdf)
>  This paper proposes a dual-stage, low complexity, and reconfigurable technique to enhance the speech contaminated by various types of noise sources. Driven by input data and audio contents, the proposed dual-stage speech enhancement approach performs a coarse and fine processing in the first-stage and second-stage, respectively. In this paper, we demonstrate that the proposed speech enhancement solution significantly enhances the metrics of 3-fold QUality Evaluation of Speech in Telecommunication (3QUEST) consisting of speech mean-opinion-score (SMOS) and noise MOS (NMOS) for near-field and far-field applications. Moreover, the proposed speech enhancement approach greatly improves both the signal-to-noise ratio (SNR) and subjective listening experience. For comparisons, the traditional speech enhancement methods reduce the SMOS although they increase NMOS and SNR. In addition, the proposed speech enhancement scheme can be easily adopted in both capture path and speech render path for speech communication and conferencing systems, and voice-trigger applications.      
### 17.Dissipation of Oscillation Energy and Distribution of Damping Power in a Multimachine Power System: A Small-signal Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2105.07618.pdf)
>  This paper revisits the concept of damping torque in a multimachine power system and its relation to the dissipation of oscillation energy in synchronous machine windings. As a multimachine extension of an existing result on a single-machine-infinite-bus (SMIB) system, we show that the total damping power for a mode stemming from the interaction of electromagnetic torques and rotor speeds is equal to the sum of average power dissipations in the generator windings corresponding to the modal oscillation. Further, counter-intuitive to the SMIB result, we demonstrate that, although the equality holds on an aggregate, such is not the case for individual machines in an interconnected system. To that end, distribution factors are derived for expressing the average damping power of each generator as a linear combination of average powers of modal energy dissipation in the windings of all machines in the system. These factors represent the distribution of damping power in a multimachine system. The results are validated on IEEE 4-machine and 16-machine test systems.      
### 18.Dermoscopic Image Classification with Neural Style Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2105.07592.pdf)
>  Skin cancer, the most commonly found human malignancy, is primarily diagnosed visually via dermoscopic analysis, biopsy, and histopathological examination. However, unlike other types of cancer, automated image classification of skin lesions is deemed more challenging due to the irregularity and variability in the lesions' appearances. In this work, we propose an adaptation of the Neural Style Transfer (NST) as a novel image pre-processing step for skin lesion classification problems. We represent each dermoscopic image as the style image and transfer the style of the lesion onto a homogeneous content image. This transfers the main variability of each lesion onto the same localized region, which allows us to integrate the generated images together and extract latent, low-rank style features via tensor decomposition. We train and cross-validate our model on a dermoscopic data set collected and preprocessed from the International Skin Imaging Collaboration (ISIC) database. We show that the classification performance based on the extracted tensor features using the style-transferred images significantly outperforms that of the raw images by more than 10%, and is also competitive with well-studied, pre-trained CNN models through transfer learning. Additionally, the tensor decomposition further identifies latent style clusters, which may provide clinical interpretation and insights.      
### 19.Deep learning for detecting pulmonary tuberculosis via chest radiography: an international study across 10 countries  [ :arrow_down: ](https://arxiv.org/pdf/2105.07540.pdf)
>  Tuberculosis (TB) is a top-10 cause of death worldwide. Though the WHO recommends chest radiographs (CXRs) for TB screening, the limited availability of CXR interpretation is a barrier. We trained a deep learning system (DLS) to detect active pulmonary TB using CXRs from 9 countries across Africa, Asia, and Europe, and utilized large-scale CXR pretraining, attention pooling, and noisy student semi-supervised learning. Evaluation was on (1) a combined test set spanning China, India, US, and Zambia, and (2) an independent mining population in South Africa. Given WHO targets of 90% sensitivity and 70% specificity, the DLS's operating point was prespecified to favor sensitivity over specificity. On the combined test set, the DLS's ROC curve was above all 9 India-based radiologists, with an AUC of 0.90 (95%CI 0.87-0.92). The DLS's sensitivity (88%) was higher than the India-based radiologists (75% mean sensitivity), p&lt;0.001 for superiority; and its specificity (79%) was non-inferior to the radiologists (84% mean specificity), p=0.004. Similar trends were observed within HIV positive and sputum smear positive sub-groups, and in the South Africa test set. We found that 5 US-based radiologists (where TB isn't endemic) were more sensitive and less specific than the India-based radiologists (where TB is endemic). The DLS also remained non-inferior to the US-based radiologists. In simulations, using the DLS as a prioritization tool for confirmatory testing reduced the cost per positive case detected by 40-80% compared to using confirmatory testing alone. To conclude, our DLS generalized to 5 countries, and merits prospective evaluation to assist cost-effective screening efforts in radiologist-limited settings. Operating point flexibility may permit customization of the DLS to account for site-specific factors such as TB prevalence, demographics, clinical resources, and customary practice patterns.      
### 20.Distinguishing Aerial Intruders from Trajectory Data: A Model-Based Hypothesis-Testing Approach  [ :arrow_down: ](https://arxiv.org/pdf/2105.07505.pdf)
>  Motivated by security needs in unmanned aerial system (UAS) operations, an algorithm for identifying airspace intruders (e.g., birds vs. drones) is developed. The algorithm is structured to use sensed intruder velocity data from Internet-of-Things platforms together with limited knowledge of physical models. The identification problem is posed as a statistical hypothesis testing or detection problem, wherein inertial feedback-controlled objects subject to stochastic actuation must be distinguished by speed data. The maximum a posteriori probability detector is obtained, and then is simplified to an explicit computation based on two points in the sample autocorrelation of the data. The simplified form allows computationally-friendly implementation of the algorithm, and simplified learning from archived data. Also, the total probability of error of the detector is computed and characterized. Simulations based on synthesized data are presented to illustrate and supplement the formal analyses.      
### 21.Advances in Artificial Intelligence to Reduce Polyp Miss Rates during Colonoscopy  [ :arrow_down: ](https://arxiv.org/pdf/2105.07467.pdf)
>  BACKGROUND AND CONTEXT: Artificial intelligence has the potential to aid gastroenterologists by reducing polyp miss detection rates during colonoscopy screening for colorectal cancer. <br>NEW FINDINGS: We introduce a new deep neural network architecture, the Focus U-Net, which achieves state-of-the-art performance for polyp segmentation across five public datasets containing images of polyps obtained during colonoscopy. <br>LIMITATIONS: The model has been validated on images taken during colonoscopy but requires validation on live video data to ensure generalisability. <br>IMPACT: Once validated on live video data, our polyp segmentation algorithm could be integrated into colonoscopy practice and assist gastroenterologists by reducing the number of polyps missed      
### 22.MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07451.pdf)
>  Methods based on convolutional neural networks have improved the performance of biomedical image segmentation. However, most of these methods cannot efficiently segment objects of variable sizes and train on small and biased datasets, which are common in biomedical use cases. While methods exist that incorporate multi-scale fusion approaches to address the challenges arising with variable sizes, they usually use complex models that are more suitable for general semantic segmentation computer vision problems. In this paper, we propose a novel architecture called MSRF-Net, which is specially designed for medical image segmentation tasks. The proposed MSRF-Net is able to exchange multi-scale features of varying receptive fields using a dual-scale dense fusion block (DSDF). Our DSDF block can exchange information rigorously across two different resolution scales, and our MSRF sub-network uses multiple DSDF blocks in sequence to perform multi-scale fusion. This allows the preservation of resolution, improved information flow, and propagation of both high- and low-level features to obtain accurate segmentation maps. The proposed MSRF-Net allows to capture object variabilities and provides improved results on different biomedical datasets. Extensive experiments on MSRF-Net demonstrate that the proposed method outperforms most of the cutting-edge medical image segmentation state-of-the-art methods. MSRF-Net advances the performance on four publicly available datasets, and also, MSRF-Net is more generalizable as compared to state-of-the-art methods.      
### 23.Theoretical Concept Study of Cooperative Abnormality Detection and Localization in Fluidic-Medium Molecular Communication  [ :arrow_down: ](https://arxiv.org/pdf/2105.07438.pdf)
>  In this paper, we propose a theoretical framework for cooperative abnormality detection and localization systems by exploiting molecular communication setup. The system consists of mobile sensors in a fluidic medium, which are injected into the medium to search the environment for abnormality. Some fusion centers (FC) are placed at specific locations in the medium, which absorb all sensors arrived at their locations, and by observing its state, each FC decides on the abnormality existence and/or its location. To reduce the effects of sensor imperfection, we propose a scheme where the sensors release some molecules (i.e., markers) into the medium after they sense an abnormality. If the goal is abnormality detection, the released molecules are used to cooperatively activate other sensors. If the goal is abnormality localization, the released molecules are used by the FCs to determine the location. In our model, both sensors' imperfection and markers background noise are taken into account. For the detection phase, we consider two sensor types based on their activation strategy by markers. To make the analysis tractable, we assume some ideal assumptions for the sensors' model. We investigate the related binary hypothesis testing problem and obtain the probabilities of false alarm and miss-detection. It is shown that using sensors with the ability of cooperatively activating each other can significantly improve the detection performance in terms of probability of error. For the localization phase, we consider two types of FCs based on their capability in reading sensors' storage levels. We study their performance and obtain the optimal and sub-optimal decision schemes and also the probability of localization error for both perfect and imperfect sensing regimes.      
### 24.Unsupervised MMRegNet based on Spatially Encoded Gradient Information  [ :arrow_down: ](https://arxiv.org/pdf/2105.07392.pdf)
>  Multi-modality medical images can provide relevant and complementary anatomical information for a target (organ, tumor or tissue). Registering the multi-modality images to a common space can fuse these comprehensive information, and bring convenience for clinical application. Recently, neural networks have been widely investigated to boost registration methods. However, it is still challenging to develop a multi-modality registration network due to the lack of robust criteria for network training. Besides, most existing registration networks mainly focus on pairwise registration, and can hardly be applicable for multiple image scenarios. In this work, we propose a multi-modality registration network (MMRegNet), which can jointly register multiple images with different modalities to a target image. Meanwhile, we present spatially encoded gradient information to train the MMRegNet in an unsupervised manner. The proposed network was evaluated on two datasets, i.e, MM-WHS 2017 and CHAOS 2019. The results show that the proposed network can achieve promising performance for cardiac left ventricle and liver registration tasks. Source code is released publicly on github.      
### 25.An accelerated expectation-maximization for multi-reference alignment  [ :arrow_down: ](https://arxiv.org/pdf/2105.07372.pdf)
>  The multi-reference alignment (MRA) problem entails estimating an image from multiple noisy and rotated copies of itself. If the noise level is low, one can reconstruct the image by estimating the missing rotations, aligning the images, and averaging out the noise. While accurate rotation estimation is impossible if the noise level is high, the rotations can still be approximated, and thus can provide indispensable information. In particular, learning the approximation error can be harnessed for efficient image estimation. In this paper, we propose a new computational framework, called Synch-EM, that consists of angular synchronization followed by expectation-maximization (EM). The synchronization step results in a concentrated distribution of rotations; this distribution is learned and then incorporated into the EM as a Bayesian prior. The learned distribution also dramatically reduces the search space, and thus the computational load, of the EM iterations. We show by extensive numerical experiments that the proposed framework can significantly accelerate EM for MRA in high noise levels, occasionally by a few orders of magnitude, without degrading the reconstruction quality.      
### 26.X-Vectors with Multi-Scale Aggregation for Speaker Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2105.07367.pdf)
>  Speaker diarization is the process of labeling different speakers in a speech signal. Deep speaker embeddings are generally extracted from short speech segments and clustered to determine the segments belong to same speaker identity. The x-vector, which embeds segment-level speaker characteristics by statistically pooling frame-level representations, is one of the most widely used deep speaker embeddings in speaker diarization. Multi-scale aggregation, which employs multi-scale representations from different layers, has recently successfully been used in short duration speaker verification. In this paper, we investigate a multi-scale aggregation approach in an x-vector embedding framework for speaker diarization by exploiting multiple statistics pooling layers from different frame-level layers. Thus, it is expected that x-vectors with multi-scale aggregation have the potential to capture meaningful speaker characteristics from short segments, effectively taking advantage of different information at multiple layers. Experimental evaluation on the CALLHOME dataset showed that our approach provides substantial improvement over the baseline x-vectors.      
### 27.Cooperative 3D Beamforming for Small-Cell and Cell-Free 6G Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.07359.pdf)
>  Three dimensional (3D) resource reuse is an important design requirement for the prospective 6G wireless communication systems. Hence, we propose a cooperative 3D beamformer for use in 3D space. Explicitly, we harness multiple base station antennas for joint zero forcing transmit pre-coding for beaming the transmit signals in specific 3D directions. The technique advocated is judiciously configured for use in both cell-based and cell-free wireless architectures. We evaluated the performance of the proposed scheme using the novel metric of Volumetric Spectral Efficiency (VSE). We also characterized the performance of the scheme in terms of its spectral efficiency (SE) and Bit Error Rate (BER) through extensive simulation studies.      
### 28.Leveraging Classification Metrics for Quantitative System-Level Analysis with Temporal Logic Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2105.07343.pdf)
>  In many autonomy applications, performance of perception algorithms is important for effective planning and control. In this paper, we introduce a framework for computing the probability of satisfaction of formal system specifications given a confusion matrix, a statistical average performance measure for multi-class classification. We define the probability of satisfaction of a linear temporal logic formula given a specific initial state of the agent and true state of the environment. Then, we present an algorithm to construct a Markov chain that represents the system behavior under the composition of the perception and control components such that the probability of the temporal logic formula computed over the Markov chain is consistent with the probability that the temporal logic formula is satisfied by our system. We illustrate this approach on a simple example of a car with pedestrian on the sidewalk environment, and compute the probability of satisfaction of safety requirements for varying parameters of the vehicle. We also illustrate how satisfaction probability changes with varied precision and recall derived from the confusion matrix. Based on our results, we identify several opportunities for future work in developing quantitative system-level analysis that incorporates perception models.      
### 29.Clutter Mitigation for Joint RadCom Systems Based On Spatial Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07328.pdf)
>  Joint radar and communication (RadCom) systems have been proposed to integrate radar and communication into one platform and achieve spectrum sharing in recent years. However, the joint RadCom systems cause the clutter modulation and the performance degradation of radar. Therefore, it's very essential to improve the performance of radar when designing joint RadCom systems. This paper deals with the clutter mitigation for joint RadCom systems based on spatial modulation. The communication information embedding introduces variation of transmit beampatterns in a coherent processing interval (CPI) and causes the clutter modulation and spreading of the clutter spectrum. This paper propose a reduced dimension spatial temporal adaptive processing (RD-STAP) method, i.e., we firstly perform time domain filtering and then perform spatial domain filtering on received data. As for time domain filtering, this paper mitigates the extended clutter by subspace projection (SP) and proposes a more effective eigen-decomposition algorithm based on Power method than singular value decomposition (SVD) to obtain clutter subspace basis vectors. And the matched filter is performed on the data after clutter mitigation to display the moving target. And receive beamforming is utilized for spatial domain filtering. Simulation results highlight the effectiveness of the proposed RD-STAP method and the eigen-decomposition algorithm.      
### 30.Data-Driven Reachability Analysis from Noisy Data  [ :arrow_down: ](https://arxiv.org/pdf/2105.07229.pdf)
>  We consider the problem of computing reachable sets directly from noisy data without a given system model. Several reachability algorithms are presented, and their accuracy is shown to depend on the underlying system generating the data. First, an algorithm for computing over-approximated reachable sets based on matrix zonotopes is proposed for linear systems. Constrained matrix zonotopes are introduced to provide less conservative reachable sets at the cost of increased computational expenses and utilized to incorporate prior knowledge about the unknown system model. Then we extend the approach to polynomial systems and under the assumption of Lipschitz continuity to nonlinear systems. Theoretical guarantees are given for these algorithms in that they give a proper over-approximative reachable set containing the true reachable set. Multiple numerical examples show the applicability of the introduced algorithms, and accuracy comparisons are made between algorithms.      
### 31.Multi-scale super-resolution generation of low-resolution scanned pathological images  [ :arrow_down: ](https://arxiv.org/pdf/2105.07200.pdf)
>  Digital pathology slide is easy to store and manage, convenient to browse and transmit. However, because of the high-resolution scan for example 40 times magnification(40X) during the digitization, the file size of each whole slide image exceeds 1Gigabyte, which eventually leads to huge storage capacity and very slow network transmission. We design a strategy to scan slides with low resolution (5X) and a super-resolution method is proposed to restore the image details when in diagnosis. The method is based on a multi-scale generative adversarial network, which sequentially generate three high-resolution images such as 10X, 20X and 40X. The perceived loss, generator loss of the generated images and real images are compared on three image resolutions, and a discriminator is used to evaluate the difference of highest-resolution generated image and real image. A dataset consisting of 100,000 pathological images from 10 types of human tissues is performed for training and testing the network. The generated images have high peak-signal-to-noise-ratio (PSNR) and structural-similarity-index (SSIM). The PSNR of 10X to 40X image are 24.16, 22.27 and 20.44, and the SSIM are 0.845, 0.680 and 0.512, which are better than other super-resolution networks such as DBPN, ESPCN, RDN, EDSR and MDSR. Moreover, visual inspections show that the generated high-resolution images by our network have enough details for diagnosis, good color reproduction and close to real images, while other five networks are severely blurred, local deformation or miss important details. Moreover, no significant differences can be found on pathological diagnosis based on the generated and real images. The proposed multi-scale network can generate good high-resolution pathological images, and will provide a low-cost storage (about 15MB/image on 5X), faster image sharing method for digital pathology.      
### 32.Robust Data-Enabled Predictive Control: Tractable Formulations and Performance Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2105.07199.pdf)
>  We introduce a general framework for robust data-enabled predictive control (DeePC) for linear time-invariant (LTI) systems. The proposed framework enables us to obtain model-free optimal control for LTI systems based on noisy input/output data. More specifically, robust DeePC solves a min-max optimization problem to compute the optimal control sequence that is resilient to all possible realizations of the uncertainties in the input/output data within a prescribed uncertainty set. We present computationally tractable reformulations of the min-max problem with various uncertainty sets. Furthermore, we show that even though an accurate prediction of the future behavior is unattainable in practice due to inaccessibility of the perfect input/output data, the obtained robust optimal control sequence provides performance guarantees for the actually realized input/output cost. We further show that the robust DeePC generalizes and robustifies the regularized DeePC (with quadratic regularization or 1-norm regularization) proposed in the literature. Finally, we demonstrate the performance of the proposed robust DeePC algorithm on high-fidelity, nonlinear, and noisy simulations of a grid-connected power converter system.      
### 33.RadioNet: Transformer based Radio Map Prediction Model For Dense Urban Environments  [ :arrow_down: ](https://arxiv.org/pdf/2105.07158.pdf)
>  Radio Map Prediction (RMP), aiming at estimating coverage of radio wave, has been widely recognized as an enabling technology for improving radio spectrum efficiency. However, fast and reliable radio map prediction can be very challenging due to the complicated interaction between radio waves and the environment. In this paper, a novel Transformer based deep learning model termed as RadioNet is proposed for radio map prediction in urban scenarios. In addition, a novel Grid Embedding technique is proposed to substitute the original Position Embedding in Transformer to better anchor the relative position of the radiation source, destination and environment. The effectiveness of proposed method is verified on an urban radio wave propagation dataset. Compared with the SOTA model on RMP task, RadioNet reduces the validation loss by 27.3\%, improves the prediction reliability from 90.9\% to 98.9\%. The prediction speed is increased by 4 orders of magnitude, when compared with ray-tracing based method. We believe that the proposed method will be beneficial to high-efficiency wireless communication, real-time radio visualization, and even high-speed image rendering.      
### 34.Window-Level is a Strong Denoising Surrogate  [ :arrow_down: ](https://arxiv.org/pdf/2105.07153.pdf)
>  CT image quality is heavily reliant on radiation dose, which causes a trade-off between radiation dose and image quality that affects the subsequent image-based diagnostic performance. However, high radiation can be harmful to both patients and operators. Several (deep learning-based) approaches have been attempted to denoise low dose images. However, those approaches require access to large training sets, specifically the full dose CT images for reference, which can often be difficult to obtain. Self-supervised learning is an emerging alternative for lowering the reference data requirement facilitating unsupervised learning. Currently available self-supervised CT denoising works are either dependent on foreign domain or pretexts are not very task-relevant. To tackle the aforementioned challenges, we propose a novel self-supervised learning approach, namely Self-Supervised Window-Leveling for Image DeNoising (SSWL-IDN), leveraging an innovative, task-relevant, simple, yet effective surrogate -- prediction of the window-leveled equivalent. SSWL-IDN leverages residual learning and a hybrid loss combining perceptual loss and MSE, all incorporated in a VAE framework. Our extensive (in- and cross-domain) experimentation demonstrates the effectiveness of SSWL-IDN in aggressive denoising of CT (abdomen and chest) images acquired at 5\% dose level only.      
### 35.RIDnet: Radiologist-Inspired Deep Neural Network for Low-dose CT Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2105.07146.pdf)
>  Being low-level radiation exposure and less harmful to health, low-dose computed tomography (LDCT) has been widely adopted in the early screening of lung cancer and COVID-19. LDCT images inevitably suffer from the degradation problem caused by complex noises. It was reported that, compared with commercial iterative reconstruction methods, deep learning (DL)-based LDCT denoising methods using convolutional neural network (CNN) achieved competitive performance. Most existing DL-based methods focus on the local information extracted by CNN, while ignoring both explicit non-local and context information (which are leveraged by radiologists). To address this issue, we propose a novel deep learning model named radiologist-inspired deep denoising network (RIDnet) to imitate the workflow of a radiologist reading LDCT images. Concretely, the proposed model explicitly integrates all the local, non-local and context information rather than local information only. Our radiologist-inspired model is potentially favoured by radiologists as a familiar workflow. A double-blind reader study on a public clinical dataset shows that, compared with state-of-the-art methods, our proposed model achieves the most impressive performance in terms of the structural fidelity, the noise suppression and the overall score. As a physicians-inspired model, RIDnet gives a new research roadmap that takes into account the behavior of physicians when designing decision support tools for assisting clinical diagnosis. Models and code are available at <a class="link-external link-https" href="https://github.com/tonyckc/RIDnet_demo" rel="external noopener nofollow">this https URL</a>.      
### 36.Image Super-Resolution Quality Assessment: Structural Fidelity Versus Statistical Naturalness  [ :arrow_down: ](https://arxiv.org/pdf/2105.07139.pdf)
>  Single image super-resolution (SISR) algorithms reconstruct high-resolution (HR) images with their low-resolution (LR) counterparts. It is desirable to develop image quality assessment (IQA) methods that can not only evaluate and compare SISR algorithms, but also guide their future development. In this paper, we assess the quality of SISR generated images in a two-dimensional (2D) space of structural fidelity versus statistical naturalness. This allows us to observe the behaviors of different SISR algorithms as a tradeoff in the 2D space. Specifically, SISR methods are traditionally designed to achieve high structural fidelity but often sacrifice statistical naturalness, while recent generative adversarial network (GAN) based algorithms tend to create more natural-looking results but lose significantly on structural fidelity. Furthermore, such a 2D evaluation can be easily fused to a scalar quality prediction. Interestingly, we find that a simple linear combination of a straightforward local structural fidelity and a global statistical naturalness measures produce surprisingly accurate predictions of SISR image quality when tested using public subject-rated SISR image datasets. Code of the proposed SFSN model is publicly available at \url{<a class="link-external link-https" href="https://github.com/weizhou-geek/SFSN" rel="external noopener nofollow">this https URL</a>}.      
### 37.Impacts of Time-of-Use Rate Changes on the Electricity Bills of Commercial Consumers  [ :arrow_down: ](https://arxiv.org/pdf/2105.07106.pdf)
>  Changes in the profile of prices in wholesale electricity markets prompt utilities to redesign their tariffs and adjust their time-of-use periods to ensure a more adequate cost recovery. However, changing the rate structures could adversely affect commercial consumers by increasing their electricity bills and hindering their ability to reduce costs using techniques like net energy metering. As time-of-use periods are adjusted, consumers will need to rely on the flexibility of distributed energy resources to achieve cost reductions. This paper explores the effect that Pacific Gas and Electric Company's redesigned rates have on the electricity bills of consumers with different demand profiles. Sensitivity analyses are conducted to examine the effect of asset sizing on reducing costs under each tariff.      
### 38.Iterative Algorithms for Assessing Network Resilience Against Structured Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2105.07080.pdf)
>  This paper studies network resilience against structured additive perturbations to its topology. We consider dynamic networks modeled as linear time-invariant systems subject to perturbations of bounded energy satisfying specific sparsity and entry-wise constraints. Given an energy level, the structured pseudospectral abscissa captures the worst-possible perturbation an adversary could employ to de-stabilize the network, and the structured stability radius is the maximum energy in the structured perturbation that the network can withstand without becoming unstable. Building on a novel characterization of the worst-case structured perturbation, we propose iterative algorithms that efficiently compute the structured pseudospectral abscissa and structured stability radius. We provide theoretical guarantees of the local convergence of the algorithms and illustrate their efficacy and accuracy on several network examples.      
### 39.Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End  [ :arrow_down: ](https://arxiv.org/pdf/2105.07071.pdf)
>  Comprehending the overall intent of an utterance helps a listener recognize the individual words spoken. Inspired by this fact, we perform a novel study of the impact of explicitly incorporating intent representations as additional information to improve a recurrent neural network-transducer (RNN-T) based automatic speech recognition (ASR) system. An audio-to-intent (A2I) model encodes the intent of the utterance in the form of embeddings or posteriors, and these are used as auxiliary inputs for RNN-T training and inference. Experimenting with a 50k-hour far-field English speech corpus, this study shows that when running the system in non-streaming mode, where intent representation is extracted from the entire utterance and then used to bias streaming RNN-T search from the start, it provides a 5.56% relative word error rate reduction (WERR). On the other hand, a streaming system using per-frame intent posteriors as extra inputs for the RNN-T ASR system yields a 3.33% relative WERR. A further detailed analysis of the streaming system indicates that our proposed method brings especially good gain on media-playing related intents (e.g. 9.12% relative WERR on PlayMusicIntent).      
### 40.On Performance of Energy Harvested Cooperative NOMA Under Imperfect CSI and Imperfect SIC  [ :arrow_down: ](https://arxiv.org/pdf/2105.07047.pdf)
>  With the advent of 5G and the need for energy-efficient massively connected wireless networks, in this work, we consider an energy harvesting (EH) based multi-relay downlink cooperative non-orthogonal multiple access (NOMA) system with practical constraints. The base station serves NOMA users with the help of decode-and-forward based multiple EH relays, where relays harvest the energy from the base station's radio frequency. A relay is selected from the multiple K-relay by using a partial relay selection protocol. The system is considered to operate in half-duplex mode over a generalized independent and identical Nakagami$-m$ fading channel. The closed-form expression of outage probability and ergodic rate are derived for users, under the assumption of imperfect channel state information (CSI) and imperfect successive interference cancellation (SIC) at the receiver node. Expression of outage probability and ergodic rate for both users under the assumption of perfect CSI and perfect SIC are also presented. Further, the asymptotic expression for the outage probability is also shown. The derived analytical expressions are verified through Monte-Carlo simulations.      
### 41.SA-GAN: Structure-Aware Generative Adversarial Network for Shape-Preserving Synthetic CT Generation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07044.pdf)
>  In medical image synthesis, model training could be challenging due to the inconsistencies between images of different modalities even with the same patient, typically caused by internal status/tissue changes as different modalities are usually obtained at a different time. This paper proposes a novel deep learning method, Structure-aware Generative Adversarial Network (SA-GAN), that preserves the shapes and locations of in-consistent structures when generating medical images. SA-GAN is employed to generate synthetic computed tomography (synCT) images from magnetic resonance imaging (MRI) with two parallel streams: the global stream translates the input from the MRI to the CT domain while the local stream automatically segments the inconsistent organs, maintains their locations and shapes in MRI, and translates the organ intensities to CT. Through extensive experiments on a pelvic dataset, we demonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs and organ segmentation and supports MR-only treatment planning in disease sites with internal organ status changes.      
### 42.The Boombox: Visual Reconstruction from Acoustic Vibrations  [ :arrow_down: ](https://arxiv.org/pdf/2105.08052.pdf)
>  We introduce The Boombox, a container that uses acoustic vibrations to reconstruct an image of its inside contents. When an object interacts with the container, they produce small acoustic vibrations. The exact vibration characteristics depend on the physical properties of the box and the object. We demonstrate how to use this incidental signal in order to predict visual structure. After learning, our approach remains effective even when a camera cannot view inside the box. Although we use low-cost and low-power contact microphones to detect the vibrations, our results show that learning from multi-modal data enables us to transform cheap acoustic sensors into rich visual sensors. Due to the ubiquity of containers, we believe integrating perception capabilities into them will enable new applications in human-computer interaction and robotics. Our project website is at: <a class="link-external link-http" href="http://boombox.cs.columbia.edu" rel="external noopener nofollow">this http URL</a>      
### 43.Koopman NMPC: Koopman-based Learning and Nonlinear Model Predictive Control of Control-affine Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.08036.pdf)
>  Koopman-based learning methods can potentially be practical and powerful tools for dynamical robotic systems. However, common methods to construct Koopman representations seek to learn lifted linear models that cannot capture nonlinear actuation effects inherent in many robotic systems. This paper presents a learning and control methodology that is a first step towards overcoming this limitation. Using the Koopman canonical transform, control-affine dynamics can be expressed by a lifted bilinear model. The learned model is used for nonlinear model predictive control (NMPC) design where the bilinear structure can be exploited to improve computational efficiency. The benefits for control-affine dynamics compared to existing Koopman-based methods are highlighted through an example of a simulated planar quadrotor. Prediction error is greatly reduced and closed loop performance similar to NMPC with full model knowledge is achieved.      
### 44.The Theory of Functional Connections: A journey from theory to application  [ :arrow_down: ](https://arxiv.org/pdf/2105.08034.pdf)
>  The Theory of Functional Connections (TFC) is a general methodology for functional interpolation that can embed a set of user-specified linear constraints. The functionals derived from this method, called \emph{constrained expressions}, analytically satisfy the imposed constraints and can be leveraged to transform constrained optimization problems to unconstrained ones. By simplifying the optimization problem, this technique has been shown to produce a numerical scheme that is faster, more accurate, and robust to poor initialization. The content of this dissertation details the complete development of the Theory of Functional Connections. First, the seminal paper on the Theory of Functional Connections is discussed and motivates the discovery of a more general formulation of the constrained expressions. Leveraging this formulation, a rigorous structure of the constrained expression is produced with associated mathematical definitions, claims, and proofs. Furthermore, the second part of this dissertation explains how this technique can be used to solve ordinary differential equations providing a wide variety of examples compared to the state-of-the-art. The final part of this work focuses on unitizing the techniques and algorithms produced in the prior sections to explore the feasibility of using the Theory of Functional Connections to solve real-time optimal control problems, namely optimal landing problems.      
### 45.Large depth of range Maxwellian-viewing SMV near-eye display based on a Pancharatnam-Berry optical element  [ :arrow_down: ](https://arxiv.org/pdf/2105.07931.pdf)
>  In order to overcome the accommodation and convergence (A-C) conflict that commonly causes visual fatigue in AR display, we propose a Maxwellian-viewing-super-multi-view (MV-SMV) near-eye display system based on a Pancharatnam-Berry optical element (PBOE). The PBOE, which is constituted with an array of high-efficiency polarization gratings, is implemented to direct different views to different directions simultaneously, constructing the 3D light field. Meanwhile, each view is like a Maxwellian view display that possesses a small viewpoint and a large depth of field (DOF). Hence, the MV-SMV display can display virtual images with correct accommodation depth cue within a large DOF. We implement a proof-of-concept MV-SMV display prototype with 3 x 1 and 3 x 2 viewpoints using a 1D PBOE and a 2D PBOE, respectively, and achieve a DOF of 4.37 diopters experimentally.      
### 46.CNN-based Approaches For Cross-Subject Classification in Motor Imagery: From The State-of-The-Art to DynamicNet  [ :arrow_down: ](https://arxiv.org/pdf/2105.07917.pdf)
>  Motor imagery (MI)-based brain-computer interface (BCI) systems are being increasingly employed to provide alternative means of communication and control for people suffering from neuro-motor impairments, with a special effort to bring these systems out of the controlled lab environments. Hence, accurately classifying MI from brain signals, e.g., from electroencephalography (EEG), is essential to obtain reliable BCI systems. However, MI classification is still a challenging task, because the signals are characterized by poor SNR, high intra-subject and cross-subject variability. Deep learning approaches have started to emerge as valid alternatives to standard machine learning techniques, e.g., filter bank common spatial pattern (FBCSP), to extract subject-independent features and to increase the cross-subject classification performance of MI BCI systems. In this paper, we first present a review of the most recent studies using deep learning for MI classification, with particular attention to their cross-subject performance. Second, we propose DynamicNet, a Python-based tool for quick and flexible implementations of deep learning models based on convolutional neural networks. We show-case the potentiality of DynamicNet by implementing EEGNet, a well-established architecture for effective EEG classification. Finally, we compare its performance with FBCSP in a 4-class MI classification over public datasets. To explore its cross-subject classification ability, we applied three different cross-validation schemes. From our results, we demonstrate that DynamicNet-implemented EEGNet outperforms FBCSP by about 25%, with a statistically significant difference when cross-subject validation schemes are applied.      
### 47.Adaptive Finite-time and Fixed-time Control Design using Output Stability Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2105.07893.pdf)
>  The present paper provides a sufficient condition to ensure output finite-time and fixed-time stability. Comparing with analogous researches the proposed result is less restrictive and obtained for a wider class of systems. The presented output stability condition is used for adaptive control design, where the state vector of a plant is extended by adjustable control parameters.      
### 48.A Geometrical Interpretation of Frequency  [ :arrow_down: ](https://arxiv.org/pdf/2105.07762.pdf)
>  The letter provides a geometrical interpretation of frequency in electric circuits. According to this interpretation, the frequency is defined as a multivector with symmetric and antisymmetric components. The conventional definition of frequency is shown to be a special case of the proposed theoretical framework. Several examples serve to show the features, generality as well as practical aspects of the proposed approach.      
### 49.Stochastic Control through Approximate Bayesian Input Inference  [ :arrow_down: ](https://arxiv.org/pdf/2105.07693.pdf)
>  Optimal control under uncertainty is a prevailing challenge in control, due to the difficulty in producing tractable solutions for the stochastic optimization problem. By framing the control problem as one of input estimation, advanced approximate inference techniques can be used to handle the statistical approximations in a principled and practical manner. Analyzing the Gaussian setting, we present a solver capable of several stochastic control methods, and was found to be superior to popular baselines on nonlinear simulated tasks. We draw connections that relate this inference formulation to previous approaches for stochastic optimal control, and outline several advantages that this inference view brings due to its statistical nature.      
### 50.Sound Event Detection with Adaptive Frequency Selection  [ :arrow_down: ](https://arxiv.org/pdf/2105.07596.pdf)
>  In this work, we present HIDACT, a novel network architecture for adaptive computation for efficiently recognizing acoustic events. We evaluate the model on a sound event detection task where we train it to adaptively process frequency bands. The model learns to adapt to the input without requesting all frequency sub-bands provided. It can make confident predictions within fewer processing steps, hence reducing the amount of computation. Experimental results show that HIDACT has comparable performance to baseline models with more parameters and higher computational complexity. Furthermore, the model can adjust the amount of computation based on the data and computational budget.      
### 51.It$\hat{\text{o}}$TTS and It$\hat{\text{o}}$Wave: Linear Stochastic Differential Equation Is All You Need For Audio Generation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07583.pdf)
>  In this paper, we propose to unify the two aspects of voice synthesis, namely text-to-speech (TTS) and vocoder, into one framework based on a pair of forward and reverse-time linear stochastic differential equations (SDE). The solutions of this SDE pair are two stochastic processes, one of which turns the distribution of mel spectrogram (or wave), that we want to generate, into a simple and tractable distribution. The other is the generation procedure that turns this tractable simple signal into the target mel spectrogram (or wave). The model that generates mel spectrogram is called It$\hat{\text{o}}$TTS, and the model that generates wave is called It$\hat{\text{o}}$Wave. It$\hat{\text{o}}$TTS and It$\hat{\text{o}}$Wave use the Wiener process as a driver to gradually subtract the excess signal from the noise signal to generate realistic corresponding meaningful mel spectrogram and audio respectively, under the conditional inputs of original text or mel spectrogram. The results of the experiment show that the mean opinion scores (MOS) of It$\hat{\text{o}}$TTS and It$\hat{\text{o}}$Wave can exceed the current state-of-the-art methods, reached 3.925$\pm$ 0.160 and 4.35$\pm$ 0.115 respectively.      
### 52.Monitoring electrical systems data-network equipment by means ofFuzzy and Paraconsistent Annotated Logic  [ :arrow_down: ](https://arxiv.org/pdf/2105.07579.pdf)
>  The constant increase in the amount and complexity of information obtained from IT data networkelements, for its correct monitoring and management, is a reality. The same happens to data net-works in electrical systems that provide effective supervision and control of substations and hydro-electric plants. Contributing to this fact is the growing number of installations and new environmentsmonitored by such data networks and the constant evolution of the technologies involved. This sit-uation potentially leads to incomplete and/or contradictory data, issues that must be addressed inorder to maintain a good level of monitoring and, consequently, management of these systems. Inthis paper, a prototype of an expert system is developed to monitor the status of equipment of datanetworks in electrical systems, which deals with inconsistencies without trivialising the inferences.This is accomplished in the context of the remote control of hydroelectric plants and substationsby a Regional Operation Centre (ROC). The expert system is developed with algorithms definedupon a combination of Fuzzy logic and Paraconsistent Annotated Logic with Annotation of TwoValues (PAL2v) in order to analyse uncertain signals and generate the operating conditions (faulty,normal, unstable or inconsistent / indeterminate) of the equipment that are identified as importantfor the remote control of hydroelectric plants and substations. A prototype of this expert systemwas installed on a virtualised server with CLP500 software (from the EFACEC manufacturer) thatwas applied to investigate scenarios consisting of a Regional (Brazilian) Operation Centre, with aGeneric Substation and a Generic Hydroelectric Plant, representing a remote control environment.      
### 53.Exploring Self-Supervised Representation Ensembles for COVID-19 Cough Classification  [ :arrow_down: ](https://arxiv.org/pdf/2105.07566.pdf)
>  The usage of smartphone-collected respiratory sound, trained with deep learning models, for detecting and classifying COVID-19 becomes popular recently. It removes the need for in-person testing procedures especially for rural regions where related medical supplies, experienced workers, and equipment are limited. However, existing sound-based diagnostic approaches are trained in a fully supervised manner, which requires large scale well-labelled data. It is critical to discover new methods to leverage unlabelled respiratory data, which can be obtained more easily. In this paper, we propose a novel self-supervised learning enabled framework for COVID-19 cough classification. A contrastive pre-training phase is introduced to train a Transformer-based feature encoder with unlabelled data. Specifically, we design a random masking mechanism to learn robust representations of respiratory sounds. The pre-trained feature encoder is then fine-tuned in the downstream phase to perform cough classification. In addition, different ensembles with varied random masking rates are also explored in the downstream phase. Through extensive evaluations, we demonstrate that the proposed contrastive pre-training, the random masking mechanism, and the ensemble architecture contribute to improving cough classification performance.      
### 54.Power-grid stability prediction using transferable machine learnings  [ :arrow_down: ](https://arxiv.org/pdf/2105.07562.pdf)
>  Complex network analyses have provided clues to improve power-grid stability with the help of numerical models. The high computational cost of numerical simulations, however, has inhibited the approach especially when it deals with the dynamic properties of power grids such as frequency synchronization. In this study, we investigate machine learning techniques to estimate the stability of power grid synchronization. We test three different machine learning algorithms -- random forest, support vector machine, and artificial neural network -- training them with two different types of synthetic power grids consisting of homogeneous and heterogeneous input-power distribution, respectively. We find that the three machine learning models better predict the synchronization stability of power-grid nodes when they are trained with the heterogeneous input-power distribution than the homogeneous one. With the real-world power grids of Great Britain, Spain, France, and Germany, we also demonstrate that the machine learning algorithms trained on synthetic power grids are transferable to the stability prediction of the real-world power grids, which implies the prospective applicability of machine learning techniques on power-grid studies.      
### 55.Sparse system identification by low-rank approximation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07522.pdf)
>  In this document, some general results from approximation theory and matrix analysis with applications to the approximate sparse identification of time series models and nonlinear discrete-time dynamical systems are presented. The aforementioned theoretical methods are translated into predictive algorithms that can be used to approximately simulate the behavior of a given dynamical system, based on some structured data measured from the system. The approximation of the state-transition operators determined primarily by matrices of parameters to be identified based on data measured from a given system, is approached by proving the existence of low-rank approximations of submatrices of the trajectory matrices corresponding to the measured data, that can be used to compute approximate sparse representations of the matrices of parameters. Prototypical algorithms and numerical implementations of the aforementioned techniques to the approximate identification and predictive simulation of time series models with symmetries and nonlinear structured dynamical systems in theoretical physics, fluid dynamics and weather forecasting are presented.      
### 56.Optimal control of robust team stochastic games  [ :arrow_down: ](https://arxiv.org/pdf/2105.07405.pdf)
>  In stochastic dynamic environments, team stochastic games have emerged as a versatile paradigm for studying sequential decision-making problems of fully cooperative multi-agent systems. However, the optimality of the derived policies is usually sensitive to the model parameters, which are typically unknown and required to be estimated from noisy data in practice. To mitigate the sensitivity of the optimal policy to these uncertain parameters, in this paper, we propose a model of "robust" team stochastic games, where players utilize a robust optimization approach to make decisions. This model extends team stochastic games to the scenario of incomplete information and meanwhile provides an alternative solution concept of robust team optimality. To seek such a solution, we develop a learning algorithm in the form of a Gauss-Seidel modified policy iteration and prove its convergence. This algorithm, compared with robust dynamic programming, not only possesses a faster convergence rate, but also allows for using approximation calculations to alleviate the curse of dimensionality. Moreover, some numerical simulations are presented to demonstrate the effectiveness of the algorithm by generalizing the game model of social dilemmas to sequential robust scenarios.      
### 57.Dissipativity and Integral Quadratic Constraints, Tailored computational robustness tests for complex interconnections  [ :arrow_down: ](https://arxiv.org/pdf/2105.07401.pdf)
>  A central notion in systems theory is dissipativity, which has been introduced by Jan Willems with the explicit goal of arriving at a fundamental understanding of the stability properties of feedback interconnections. In robust control, the framework of integral quadratic constraints (IQCs) builds on the seminal contributions of Yakubovich and Zames in the 1960's. It provides a technique for analyzing the stability of an interconnection of some linear system in feedback with a whole class of systems, also refereed to as uncertainty. <br>In this paper we survey the key ideas of exploiting dissipativity and integral quadratic constraints for the computational analysis of robust stability and performance properties of uncertain interconnections in terms of linear matrix inequalities. In particular for dynamic supply rates, the paper revolves around the notion of finite-horizon integral quadratic constraints with a terminal cost. We reveal that this provides a seamless link between the general IQC theorem and dissipativity theory that has been established only rather recently.      
### 58.BDANet: Multiscale Convolutional Neural Network with Cross-directional Attention for Building Damage Assessment from Satellite Images  [ :arrow_down: ](https://arxiv.org/pdf/2105.07364.pdf)
>  Fast and effective responses are required when a natural disaster (e.g., earthquake, hurricane, etc.) strikes. Building damage assessment from satellite imagery is critical before relief effort is deployed. With a pair of pre- and post-disaster satellite images, building damage assessment aims at predicting the extent of damage to buildings. With the powerful ability of feature representation, deep neural networks have been successfully applied to building damage assessment. Most existing works simply concatenate pre- and post-disaster images as input of a deep neural network without considering their correlations. In this paper, we propose a novel two-stage convolutional neural network for Building Damage Assessment, called BDANet. In the first stage, a U-Net is used to extract the locations of buildings. Then the network weights from the first stage are shared in the second stage for building damage assessment. In the second stage, a two-branch multi-scale U-Net is employed as backbone, where pre- and post-disaster images are fed into the network separately. A cross-directional attention module is proposed to explore the correlations between pre- and post-disaster images. Moreover, CutMix data augmentation is exploited to tackle the challenge of difficult classes. The proposed method achieves state-of-the-art performance on a large-scale dataset -- xBD. The code is available at <a class="link-external link-https" href="https://github.com/ShaneShen/BDANet-Building-Damage-Assessment" rel="external noopener nofollow">this https URL</a>.      
### 59.Rate-Splitting Multiple Access for Downlink Multiuser MIMO: Precoder Optimization and PHY-Layer Design  [ :arrow_down: ](https://arxiv.org/pdf/2105.07362.pdf)
>  Rate-Splitting Multiple Access (RSMA) has recently appeared as a powerful and robust multiple access and interference management strategy for downlink Multi-user (MU) multi-antenna communications. In this work, we study the precoder design problem for RSMA scheme in downlink MU systems with both perfect and imperfect Channel State Information at the Transmitter (CSIT) and assess the role and benefits of transmitting multiple common streams. Unlike existing works which have considered single-antenna receivers (Multiple-Input Single-Output--MISO), we propose and extend the RSMA framework for multi-antenna receivers (Multiple-Input Multiple-Output--MIMO) and formulate the precoder optimization problem with the aim of maximizing the Weighted Ergodic Sum-Rate (WESR). Precoder optimization is solved using Sample Average Approximation (SAA) together with the proposed vectorization and Weighted Minimum Mean Square Error (WMMSE) based approach. Achievable sum-Degree of Freedom (DoF) of RSMA is derived for the proposed framework as an increasing function of the number of transmitted common and private streams, which is further validated by the Ergodic Sum Rate (ESR) performance using Monte Carlo simulations. Conventional MU-MIMO based on linear precoders and Non-Orthogonal Multiple Access (NOMA) schemes are considered as baselines. Numerical results show that with imperfect CSIT, the sum-DoF and ESR performance of RSMA is superior than that of the two baselines, and is increasing with the number of transmitted common streams. Moreover, by better managing the interference, RSMA not only has significant ESR gains over baseline schemes but is more robust to CSIT inaccuracies, network loads and user deployments.      
### 60.Model-Based Offline Planning with Trajectory Pruning  [ :arrow_down: ](https://arxiv.org/pdf/2105.07351.pdf)
>  Offline reinforcement learning (RL) enables learning policies using pre-collected datasets without environment interaction, which provides a promising direction to make RL useable in real-world systems. Although recent offline RL studies have achieved much progress, existing methods still face many practical challenges in real-world system control tasks, such as computational restriction during agent training and the requirement of extra control flexibility. Model-based planning framework provides an attractive solution for such tasks. However, most model-based planning algorithms are not designed for offline settings. Simply combining the ingredients of offline RL with existing methods either provides over-restrictive planning or leads to inferior performance. We propose a new light-weighted model-based offline planning framework, namely MOPP, which tackles the dilemma between the restrictions of offline learning and high-performance planning. MOPP encourages more aggressive trajectory rollout guided by the behavior policy learned from data, and prunes out problematic trajectories to avoid potential out-of-distribution samples. Experimental results show that MOPP provides competitive performance compared with existing model-based offline planning and RL approaches, and allows easy adaptation to varying objectives and extra constraints.      
### 61.Unsupervised Super-Resolution of Satellite Imagery for High Fidelity Material Label Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2105.07322.pdf)
>  Urban material recognition in remote sensing imagery is a highly relevant, yet extremely challenging problem due to the difficulty of obtaining human annotations, especially on low resolution satellite images. To this end, we propose an unsupervised domain adaptation based approach using adversarial learning. We aim to harvest information from smaller quantities of high resolution data (source domain) and utilize the same to super-resolve low resolution imagery (target domain). This can potentially aid in semantic as well as material label transfer from a richly annotated source to a target domain.      
### 62.The Volctrans Neural Speech Translation System for IWSLT 2021  [ :arrow_down: ](https://arxiv.org/pdf/2105.07319.pdf)
>  This paper describes the systems submitted to IWSLT 2021 by the Volctrans team. We participate in the offline speech translation and text-to-text simultaneous translation tracks. For offline speech translation, our best end-to-end model achieves 8.1 BLEU improvements over the benchmark on the MuST-C test set and is even approaching the results of a strong cascade solution. For text-to-text simultaneous translation, we explore the best practice to optimize the wait-k model. As a result, our final submitted systems exceed the benchmark at around 7 BLEU on the same latency regime. We will publish our code and model to facilitate both future research works and industrial applications.      
### 63.Regret Analysis of Distributed Online LQR Control for Unknown LTI Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.07310.pdf)
>  Online learning has recently opened avenues for rethinking classical optimal control beyond time-invariant cost metrics, and online controllers are designed when the performance criteria changes adversarially over time. Inspired by this line of research, we study the distributed online linear quadratic regulator (LQR) problem for linear time-invariant (LTI) systems with unknown dynamics. Consider a multi-agent network where each agent is modeled as a LTI system. The LTI systems are associated with time-varying quadratic costs that are revealed sequentially. The goal of the network is to collectively (i) estimate the unknown dynamics and (ii) compute local control sequences competitive to that of the best centralized policy in hindsight that minimizes the sum of costs for all agents. This problem is formulated as a {\it regret} minimization. We propose a distributed variant of the online LQR algorithm where each agent computes its system estimate during an exploration stage. The agent then applies distributed online gradient descent on a semi-definite programming (SDP) whose feasible set is based on the agent's system estimate. We prove that the regret bound of our proposed algorithm scales $\tilde{O}(T^{2/3})$, implying the consensus of the network over time. We also provide simulation results verifying our theoretical guarantee.      
### 64.1D CNN Architectures for Music Genre Classification  [ :arrow_down: ](https://arxiv.org/pdf/2105.07302.pdf)
>  This paper proposes a 1D residual convolutional neural network (CNN) architecture for music genre classification and compares it with other recent 1D CNN architectures. The 1D CNNs learn a representation and a discriminant directly from the raw audio signal. Several convolutional layers capture the time-frequency characteristics of the audio signal and learn various filters relevant to the music genre recognition task. The proposed approach splits the audio signal into overlapped segments using a sliding window to comply with the fixed-length input constraint of the 1D CNNs. As a result, music genre classification can be carried out on a single audio segment or on the aggregation of the predictions on several audio segments, which improves the final accuracy. The performance of the proposed 1D residual CNN is assessed on a public dataset of 1,000 audio clips. The experimental results have shown that it achieves 80.93% of mean accuracy in classifying music genres and outperforms other 1D CNN architectures.      
### 65.Aerial-PASS: Panoramic Annular Scene Segmentation in Drone Videos  [ :arrow_down: ](https://arxiv.org/pdf/2105.07209.pdf)
>  Aerial pixel-wise scene perception of the surrounding environment is an important task for UAVs (Unmanned Aerial Vehicles). Previous research works mainly adopt conventional pinhole cameras or fisheye cameras as the imaging device. However, these imaging systems cannot achieve large Field of View (FoV), small size, and lightweight at the same time. To this end, we design a UAV system with a Panoramic Annular Lens (PAL), which has the characteristics of small size, low weight, and a 360-degree annular FoV. A lightweight panoramic annular semantic segmentation neural network model is designed to achieve high-accuracy and real-time scene parsing. In addition, we present the first drone-perspective panoramic scene segmentation dataset Aerial-PASS, with annotated labels of track, field, and others. A comprehensive variety of experiments shows that the designed system performs satisfactorily in aerial panoramic scene parsing. In particular, our proposed model strikes an excellent trade-off between segmentation performance and inference speed suitable, validated on both public street-scene and our established aerial-scene datasets.      
### 66.Joint estimation of multiple Granger causal networks: Inference of group-level brain connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2105.07196.pdf)
>  This paper considers joint learning of multiple sparse Granger graphical models to discover underlying common and differential Granger causality (GC) structures across multiple time series. This can be applied to drawing group-level brain connectivity inferences from a homogeneous group of subjects or discovering network differences among groups of signals collected under heterogeneous conditions. By recognizing that the GC of a single multivariate time series can be characterized by common zeros of vector autoregressive (VAR) lag coefficients, a group sparse prior is included in joint regularized least-squares estimations of multiple VAR models. Group-norm regularizations based on group- and fused-lasso penalties encourage a decomposition of multiple networks into a common GC structure, with other remaining parts defined in individual-specific networks. Prior information about sparseness and sparsity patterns of desired GC networks are incorporated as relative weights, while a non-convex group norm in the penalty is proposed to enhance the accuracy of network estimation in low-sample settings. Extensive numerical results on simulations illustrated our method's improvements over existing sparse estimation approaches on GC network sparsity recovery. Our methods were also applied to available resting-state fMRI time series from the ADHD-200 data sets to learn the differences of causality mechanisms, called effective brain connectivity, between adolescents with ADHD and typically developing children. Our analysis revealed that parts of the causality differences between the two groups often resided in the orbitofrontal region and areas associated with the limbic system, which agreed with clinical findings and data-driven results in previous studies.      
### 67.Delay Robustness of Consensus Algorithms: Beyond The Uniform Connectivity (Extended Version)  [ :arrow_down: ](https://arxiv.org/pdf/2105.07183.pdf)
>  Consensus of autonomous agents is a benchmark problem in multi-agent control. In this paper, we consider continuous-time averaging consensus policies (or Laplacian flows) and their discrete-time counterparts over time-varying graphs in presence of unknown but bounded communication delays. It is known that consensus is established (no matter how large the delays are) if the graph is periodically, or uniformly quasi-strongly connected (UQSC). The UQSC condition is often believed to be the weakest sufficient condition under which consensus can be proved. We show that the UQSC condition can actually be substantially relaxed and replaced by a condition that we call aperiodic quasi-strong connectivity (AQSC), which, in some sense, proves to be very close to the necessary condition of integral connectivity. Furthermore, in some special situations such as undirected or type-symmetric graph, we find a necessary and sufficient condition for consensus in presence of bounded delay; the relevant results have been previously proved only in the undelayed case. The consensus criteria established in this paper generalize a number of results known in the literature.      
### 68.Move2Hear: Active Audio-Visual Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07142.pdf)
>  We introduce the active audio-visual source separation problem, where an agent must move intelligently in order to better isolate the sounds coming from an object of interest in its environment. The agent hears multiple audio sources simultaneously (e.g., a person speaking down the hall in a noisy household) and must use its eyes and ears to automatically separate out the sounds originating from the target object within a limited time budget. Towards this goal, we introduce a reinforcement learning approach that trains movement policies controlling the agent's camera and microphone placement over time, guided by the improvement in predicted audio separation quality. We demonstrate our approach in scenarios motivated by both augmented reality (system is already co-located with the target object) and mobile robotics (agent begins arbitrarily far from the target object). Using state-of-the-art realistic audio-visual simulations in 3D environments, we demonstrate our model's ability to find minimal movement sequences with maximal payoff for audio source separation. Project: <a class="link-external link-http" href="http://vision.cs.utexas.edu/projects/move2hear" rel="external noopener nofollow">this http URL</a>.      
### 69.Analyzing Images for Music Recommendation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07135.pdf)
>  Experiencing images with suitable music can greatly enrich the overall user experience. The proposed image analysis method treats an artwork image differently from a photograph image. Automatic image classification is performed using deep-learning based models. An illustrative analysis showcasing the ability of our deep-models to inherently learn and utilize perceptually relevant features when classifying artworks is also presented. The Mean Opinion Score (MOS) obtained from subjective assessments of the respective image and recommended music pairs supports the effectiveness of our approach.      
### 70.Hardware Synthesis of State-Space Equations; Application to FPGA Implementation of Shallow and Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.07131.pdf)
>  Nowadays, shallow and deep Neural Networks (NNs) have vast applications including biomedical engineering, image processing, computer vision, and speech recognition. Many researchers have developed hardware accelerators including field-programmable gate arrays (FPGAs) for implementing high-performance and energy efficient NNs. Apparently, the hardware architecture design process is specific and time-consuming for each NN. Therefore, a systematic way to design, implement and optimize NNs is highly demanded. The paper presents a systematic approach to implement state-space models in register transfer level (RTL), with special interest for NN implementation. The proposed design flow is based on the iterative nature of state-space models and the analogy between state-space formulations and finite-state machines. The method can be used in linear/nonlinear and time-varying/time-invariant systems. It can also be used to implement either intrinsically iterative systems (widely used in various domains such as signal processing, numerical analysis, computer arithmetic, and control engineering), or systems that could be rewritten in equivalent iterative forms. The implementation of recurrent NNs such as long short-term memory (LSTM) NNs, which have intrinsic state-space forms, are another major applications for this framework. As a case study, it is shown that state-space systems can be used for the systematic implementation and optimization of NNs (as nonlinear and time-varying dynamic systems). An RTL code generating software is also provided online, which simplifies the automatic generation of NNs of arbitrary size.      
### 71.Lightweight Compression of Intermediate Neural Network Features for Collaborative Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2105.07102.pdf)
>  In collaborative intelligence applications, part of a deep neural network (DNN) is deployed on a lightweight device such as a mobile phone or edge device, and the remaining portion of the DNN is processed where more computing resources are available, such as in the cloud. This paper presents a novel lightweight compression technique designed specifically to quantize and compress the features output by the intermediate layer of a split DNN, without requiring any retraining of the network weights. Mathematical models for estimating the clipping and quantization error of ReLU and leaky-ReLU activations at this intermediate layer are developed and used to compute optimal clipping ranges for coarse quantization. We also present a modified entropy-constrained design algorithm for quantizing clipped activations. When applied to popular object-detection and classification DNNs, we were able to compress the 32-bit floating point intermediate activations down to 0.6 to 0.8 bits, while keeping the loss in accuracy to less than 1%. When compared to HEVC, we found that the lightweight codec consistently provided better inference accuracy, by up to 1.3%. The performance and simplicity of this lightweight compression technique makes it an attractive option for coding an intermediate layer of a split neural network for edge/cloud applications.      
### 72.Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.07059.pdf)
>  Automated segmentation in medical image analysis is a challenging task that requires a large amount of manually labeled data. However, manually annotating medical data is often laborious, and most existing learning-based approaches fail to accurately delineate object boundaries without effective geometric constraints. Contrastive learning, a sub-area of self-supervised learning, has recently been noted as a promising direction in multiple application fields. In this work, we present a novel Contrastive Voxel-wise Representation Learning (CVRL) method with geometric constraints to learn global-local visual representations for volumetric medical image segmentation with limited annotations. Our framework can effectively learn global and local features by capturing 3D spatial context and rich anatomical information. Specifically, we introduce a voxel-to-volume contrastive algorithm to learn global information from 3D images, and propose to perform local voxel-to-voxel contrast to explicitly make use of local cues in the embedding space. Moreover, we integrate an elastic interaction-based active contour model as a geometric regularization term to enable fast and reliable object delineations in an end-to-end learning manner. Results on the Atrial Segmentation Challenge dataset demonstrate superiority of our proposed scheme, especially in a setting with a very limited number of annotated data.      
### 73.Slicing-Based AI Service Provisioning on Network Edge  [ :arrow_down: ](https://arxiv.org/pdf/2105.07052.pdf)
>  Edge intelligence leverages computing resources on network edge to provide artificial intelligence (AI) services close to network users. As it enables fast inference and distributed learning, edge intelligence is envisioned to be an important component of 6G networks. In this article, we investigate AI service provisioning for supporting edge intelligence. First, we present the features and requirements of AI services. Then, we introduce AI service data management, and customize network slicing for AI services. Specifically, we propose a novel resource pooling method to jointly manage service data and network resources for AI services. A trace-driven case study demonstrates the effectiveness of the proposed resource pooling method. Through this study, we illustrate the necessity, challenge, and potential of AI service provisioning on network edge.      
### 74.Information Theoretic Key Agreement Protocol based on ECG signals  [ :arrow_down: ](https://arxiv.org/pdf/2105.07037.pdf)
>  Wireless body area networks (WBANs) are becoming increasingly popular as they allow individuals to continuously monitor their vitals and physiological parameters remotely from the hospital. With the spread of the SARS-CoV-2 pandemic, the availability of portable pulse-oximeters and wearable heart rate detectors has boomed in the market. At the same time, in 2020 we assisted to an unprecedented increase of healthcare breaches, revealing the extreme vulnerability of the current generation of WBANs. Therefore, the development of new security protocols to ensure data protection, authentication, integrity and privacy within WBANs are highly needed. Here, we targeted a WBAN collecting ECG signals from different sensor nodes on the individual's body, we extracted the inter-pulse interval (i.e., R-R interval) sequence from each of them, and we developed a new information theoretic key agreement protocol that exploits the inherent randomness of ECG to ensure authentication between sensor pairs within the WBAN. After proper pre-processing, we provide an analytical solution that ensures robust authentication; we provide a unique information reconciliation matrix, which gives good performance for all ECG sensor pairs; and we can show that a relationship between information reconciliation and privacy amplification matrices can be found. Finally, we show the trade-off between the level of security, in terms of key generation rate, and the complexity of the error correction scheme implemented in the system.      
### 75.The Benefit Of Temporally-Strong Labels In Audio Event Classification  [ :arrow_down: ](https://arxiv.org/pdf/2105.07031.pdf)
>  To reveal the importance of temporal precision in ground truth audio event labels, we collected precise (~0.1 sec resolution) "strong" labels for a portion of the AudioSet dataset. We devised a temporally strong evaluation set (including explicit negatives of varying difficulty) and a small strong-labeled training subset of 67k clips (compared to the original dataset's 1.8M clips labeled at 10 sec resolution). We show that fine-tuning with a mix of weak and strongly labeled data can substantially improve classifier performance, even when evaluated using only the original weak labels. For a ResNet50 architecture, d' on the strong evaluation data including explicit negatives improves from 1.13 to 1.41. The new labels are available as an update to AudioSet.      
### 76.Chord Recognition- Music and Audio Information Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2105.07019.pdf)
>  Music Information Retrieval (MIR) is a collaborative scientific study that help to build innovative information research themes, novel frameworks, and developing connected delivery mechanisms in addition to making the world's massive collection of music open for everyone. Modern rock music proved to be difficult to estimate tempo and chord recognition did not work. All of the findings indicate that modern rock and metal music can be analysed, despite its complexity, but that further research is needed in this area to make it useful. Using a neural network has been one of the simplest ways of dealing with it. The pitch class profile vector is used in the neural network method. Because the vector only contains 12 elements of semi-tone values, it is enough for chord recognition. Of course, there are other ways of achieving this work, most of them depend on pitch class profiling to transform the chord into a type that can be recognised, but the recognition process is time-consuming centred on extremely complicated and memory-intensive methods.      
