# ArXiv eess --Mon, 3 May 2021
### 1.A Computationally Efficient 2D MUSIC Approach for 5G and 6G Sensing Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.15132.pdf)
>  Future cellular networks are intended to have the ability to sense the environment by utilizing reflections of transmitted signals. Multi-dimensional sensing brings along the crucial advantage of being able to resort to multiple domains to resolve targets, enhancing detection capabilities compared to 1D estimation. However, estimating parameters jointly in 5G New Radio (NR) systems poses the challenge of limiting the computational complexity while preserving a high resolution. To that end, wepropose a channel state information (CSI) decimation technique for MUltiple SIgnal Classification (MUSIC)-based joint rangeangle of arrival (AoA) estimation. We further introduce multi-peak search routines to achieve additional detection capability improvements. Simulation results with orthogonal frequency-division multiplexing (OFDM) signals show that we attain higher detection probabilities for closely spaced targets than with 1D range-only estimation. Moreover, we demonstrate that for our considered 5G setup, we are able to significantly reduce the required number of computations due to CSI decimation.      
### 2.Detection and Inference of Randomness-based Behavior for Resilient Multi-vehicle Coordinated Operations  [ :arrow_down: ](https://arxiv.org/pdf/2104.15101.pdf)
>  A resilient multi-vehicle system cooperatively performs tasks by exchanging information, detecting and removing cyber-attacks that have the intent of hijacking or diminishing performance of the entire system. In this paper, we propose a framework to: i) detect and isolate misbehaving vehicles in the network, and ii) securely encrypt information among the network to alert and attract nearby vehicles toward points of interest in the environment without explicitly broadcasting safety-critical information. To accomplish these goals, we leverage a decentralized virtual spring-damper mesh physics model for formation control on each vehicle. To discover inconsistent behavior of any vehicle in the network, we consider an approach that monitors the changes in residual sign between the expected and the predicted states of neighbor vehicles, raising an alarm any time the signed behavior does not match with an expectation. Similarly, to disguise important information and trigger vehicles to switch to different behaviors, we leverage side-channel information on the state of the vehicles and characterize a different spring-damper signature model detectable by neighbor vehicles for consistency checking. Our framework is demonstrated in simulation and through experiments on formations of unmanned ground vehicles (UGVs) in the presence of malicious man-in-the-middle (MITM) communication attacks.      
### 3.Detection of Hidden Attacks on Cyber-Physical Systems from Serial Magnitude and Sign Randomness Inconsistencies  [ :arrow_down: ](https://arxiv.org/pdf/2104.15097.pdf)
>  Stealthy false data injection attacks on cyber-physical systems (CPSs) introduce erroneous measurement information to on-board sensors with the purpose to degrade system performance. An intelligent attacker is able to leverage knowledge of the system model and noise characteristics to alter sensor measurements while remaining undetected. To achieve this objective, the stealthy attack sequence is designed such that the detector performs similarly in the attacked and attack-free cases. Consequently, an attacker that wants to remain hidden will leave behind traces of inconsistent behavior, contradicting the system model. To deal with this problem, we propose a runtime monitor to find these inconsistencies in sensor measurements by monitoring for serial inconsistencies of the detection test measure. Specifically, we employ the chi-square fault detection procedure to monitor the magnitude and signed sequence of its chi-square test measure. We validate our approach with simulations on an unmanned ground vehicle (UGV) under stealthy attacks and compare the detection performance with various state-of-the-art anomaly detectors.      
### 4.Mean Field MARL Based Bandwidth Negotiation Method for Massive Devices Spectrum Sharing  [ :arrow_down: ](https://arxiv.org/pdf/2104.15085.pdf)
>  In this paper, a novel bandwidth negotiation mechanism is proposed for massive devices wireless spectrum sharing, in which individual device locally negotiates bandwidth usage with neighbor devices and globally optimal spectrum utilization is achieved through distributed decision-making. Since only sparse feedback is needed, the proposed mechanism can greatly reduce the signaling overhead. In order to solve the distributed optimization problem when massive devices coexist, mean field multi-agent reinforcement learning (MF-MARL) based bandwidth decision algorithm is proposed, which allow device make globally optimal decision leveraging only neighborhood observation. In simulation, distributed bandwidth negotiation between 1000 devices is demonstrated and the spectrum utilization rate is above 95%. The proposed method is beneficial to reduce spectrum conflicts, increase spectrum utilization for massive devices spectrum sharing.      
### 5.Impact on power system transient stability of AC-line-emulation controllers of VSC-HVDC links  [ :arrow_down: ](https://arxiv.org/pdf/2104.15039.pdf)
>  High voltage direct current links based on voltage source converters (VSC-HVDC) embedded in alternating current (AC) systems are receiving a great deal of attention recently because they can contribute positively to the flexibilisation of modern power systems. Among several possibilities, AC-line-emulation control has been highlighted as an simple-but-useful alternative for these type of systems. With this strategy, the power flow through the link is controlled proportionally to the angle difference between its two AC terminals and this provides self-adaptation of the power flow in case of contingencies in the parallel AC lines, naturally. Although this controller is mainly concerned with steady state, it can also have an impact on the dynamic behaviour of the system, which has not been sufficiently analysed. Along this line, this paper analyses the impact of AC-line-emulation controllers of VSC-HVDC on power system transient stability. Nonlinear time-domain simulations were carried out by using PSS/E on a small test system with an embedded point-to-point VSC-HVDC link. The critical clearing time (CCT) of a test fault has been used to assess transient-stability margins of the whole system. The paper provides recommendations for the design of AC-line-emulation controllers in order to ensure that transient stability is not jeopardised.      
### 6.A direct approach to stochastic multi-period AC security constrained optimal power flow  [ :arrow_down: ](https://arxiv.org/pdf/2104.15038.pdf)
>  The accelerated penetration rate of renewable energy sources (RES) brings environmental benefits at the expense of increasing operation cost and undermining the satisfaction of the N-1 security criterion. To address the latter issue, this paper extends the state of the art, i.e. deterministic and single time period AC security-constrained optimal power flow (SCOPF), to capture two new dimensions: RES stochasticity and multiple time periods modelling inter-temporal constraints of emerging sources of flexibility such as flexible loads (FL) and energy storage systems (ESS). Accordingly, the paper proposes and solves for the first time a new problem formulation in the form of stochastic multi-period AC SCOPF (S-MP-SCOPF). The S-MP-SCOPF is formulated as a non-linear programming (NLP) problem. It computes optimal setpoints of flexibility resources and other conventional control means for congestion management and voltage control in day-ahead operation. Another salient feature of this paper is the comprehensive and accurate modelling of AC power flow model for both pre-contingency and post-contingency states, inter-temporal constraints for resources such as FL and ESS in a 24-hours time horizon, and RES uncertainties. The importance and performances of the proposed model are illustrated on two test systems of 5 nodes and 60 nodes, respectively.      
### 7.ModelGuard: Runtime Validation of Lipschitz-continuous Models  [ :arrow_down: ](https://arxiv.org/pdf/2104.15006.pdf)
>  This paper presents ModelGuard, a sampling-based approach to runtime model validation for Lipschitz-continuous models. Although techniques exist for the validation of many classes of models the majority of these methods cannot be applied to the whole of Lipschitz-continuous models, which includes neural network models. Additionally, existing techniques generally consider only white-box models. By taking a sampling-based approach, we can address black-box models, represented only by an input-output relationship and a Lipschitz constant. We show that by randomly sampling from a parameter space and evaluating the model, it is possible to guarantee the correctness of traces labeled consistent and provide a confidence on the correctness of traces labeled inconsistent. We evaluate the applicability and scalability of ModelGuard in three case studies, including a physical platform.      
### 8.Grid Interfaces to Electric Vehicle Chargers Using Statistically-Structured Power Conversion for Second-Use Batteries as Energy Buffering  [ :arrow_down: ](https://arxiv.org/pdf/2104.14976.pdf)
>  The rapid growth of electric vehicles (EVs) will include electric grid stress from EV chargers and produce a large number of diminished EV batteries. EV batteries are expected to retain about 80 % of their original capacity at the end of vehicle life. Employing these in second-use battery energy storage systems (2-BESS) as energy buffers for EV chargers further reduces the environmental impact of battery manufacturing and recycling. One of the obstacles that limits performance and cost to 2-BESS is the heterogeneity of second-use batteries. In this paper, we show that a structure for power processing within a 2-BESS with hierarchical partial power processing can be optimally designed for stochastic variation in EV demand, dynamic grid constraints, and statistical variation in battery capacity. Statistically-structured hierarchical partial power processing shows better battery energy utilization, lower derating, and higher captured value in comparison to conventional partial power processing and full power processing for similar power conversion cost.      
### 9.Intelligent Decision Method for Main Control Parameters of Tunnel Boring Machine based on Multi-Objective Optimization of Excavation Efficiency and Cost  [ :arrow_down: ](https://arxiv.org/pdf/2104.14975.pdf)
>  Timely and reasonable matching of the control parameters and geological conditions of the rock mass in tunnel excavation is crucial for hard rock tunnel boring machines (TBMs). Therefore, this paper proposes an intelligent decision method for the main control parameters of the TBM based on the multi-objective optimization of excavation efficiency and cost. The main objectives of this method are to obtain the most important parameters of the rock mass and machine, determine the optimization objective, and establish the objective function. In this study, muck information was included as an important parameter in the traditional rock mass and machine parameter database. The rock-machine interaction model was established through an improved neural network algorithm. Using 250 sets of data collected in the field, the validity of the rock-machine interaction relationship model was verified. Then, taking the cost as the optimization objective, the cost calculation model related to tunneling and the cutter was obtained. Subsequently, combined with rock-machine interaction model, the objective function of control parameter optimization based on cost was established. Finally, a tunneling test was carried out at the engineering site, and the main TBM control parameters (thrust and torque) after the optimization decision were used to excavate the test section. Compared with the values in the section where the TBM operators relied on experience, the average penetration rate of the TBM increased by 11.10%, and the average cutter life increased by 15.62%. The results indicate that this method can play an effective role in TBM tunneling in the test section.      
### 10.Disentangling the frequency content in optoacoustics  [ :arrow_down: ](https://arxiv.org/pdf/2104.14966.pdf)
>  Signals acquired by optoacoustic tomography systems have broadband frequency content that encodes information about structures on different physical scales. Concurrent processing and rendering of such broadband signals may result in images with poor contrast and fidelity due to a bias towards low frequency contributions from larger structures. This problem cannot be addressed by filtering different frequency bands and reconstructing them individually, as this procedure leads to artefacts due to its incompatibility with the entangled frequency content of signals generated by structures of different sizes. Here we introduce frequency-band model-based (fbMB) reconstruction to separate frequency-band-specific optoacoustic image components during image formation, thereby enabling structures of all sizes to be rendered with high fidelity. In order to disentangle the overlapping frequency content of image components, fbMB uses soft priors to achieve an optimal trade-off between localization of the components in frequency bands and their structural integrity. We demonstrate that fbMB produces optoacoustic images with improved contrast and fidelity, which reveal anatomical structures in in vivo images of mice in unprecedented detail. These enhancements further improve the accuracy of spectral unmixing in small vasculature. By offering a precise treatment of the frequency components of optoacoustic signals, fbMB improves the quality, accuracy, and quantification of optoacoustic images and provides a method of choice for optoacoustic reconstructions.      
### 11.A Review on Bio-Cyber Interfaces for Intrabody Molecular Communications Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.14944.pdf)
>  The recent advancements in bio-engineering and wireless communications systems have motivated researchers to propose novel applications for telemedicine, therapeutics and human health monitoring. For instance, through wireless medical telemetry a healthcare worker can remotely measure biological signals and control certain processes in the organism required for the maintenance of the patient's health state. This technology can be further extended to use Bio-Nano devices to promote a real-time monitoring of the human health and storage of the gathered data in the cloud. This brings new challenges and opportunities for the development of biosensing network, which will depend on the extension of the current intrabody devices functionalities. In this paper we will cover the recent progress made on implantable micro-scale devices and introduce the perspective of improve them to foster the development of new theranostics based on data collected at the nanoscale level.      
### 12.Crackle Detection In Lung Sounds Using Transfer Learning And Multi-Input Convolitional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.14921.pdf)
>  Large annotated lung sound databases are publicly available and might be used to train algorithms for diagnosis systems. However, it might be a challenge to develop a well-performing algorithm for small non-public data, which have only a few subjects and show differences in recording devices and setup. In this paper, we use transfer learning to tackle the mismatch of the recording setup. This allows us to transfer knowledge from one dataset to another dataset for crackle detection in lung sounds. In particular, a single input convolutional neural network (CNN) model is pre-trained on a source domain using ICBHI 2017, the largest publicly available database of lung sounds. We use log-mel spectrogram features of respiratory cycles of lung sounds. The pre-trained network is used to build a multi-input CNN model, which shares the same network architecture for respiratory cycles and their corresponding respiratory phases. The multi-input model is then fine-tuned on the target domain of our self-collected lung sound database for classifying crackles and normal lung sounds. Our experimental results show significant performance improvements of 9.84% (absolute) in F-score on the target domain using the multi-input CNN model based on transfer learning for crackle detection in adventitious lung sound classification task.      
### 13.Privacy-Preserving Co-synthesis Against Sensor-Actuator Eavesdropping Intruder  [ :arrow_down: ](https://arxiv.org/pdf/2104.14919.pdf)
>  In this work, we investigate the problem of privacy-preserving supervisory control against an external passive intruder via co-synthesis of dynamic mask, edit function, and supervisor for opacity enforcement and requirement satisfaction. We attempt to achieve the following goals: 1) the system secret cannot be inferred by the intruder, i.e., opacity of secrets against the intruder, and the existence of the dynamic mask and the edit function should not be discovered by the intruder, i.e., covertness of dynamic mask and edit function against the intruder; 2) the closed-loop system behaviors should satisfy some safety and nonblockingness requirement. We assume the intruder can eavesdrop both the sensing information generated by the sensors and the control commands issued to the actuators, and we refer to such an intruder as a sensor-actuator eavesdropping intruder. Our approach is to model the co-synthesis problem as a distributed supervisor synthesis problem in the Ramadge-Wonham supervisory control framework, and we propose an incremental synthesis heuristic to incrementally synthesize a dynamic mask, an edit function, and a supervisor, which consists of three steps: 1) we first synthesize an ensemble ME of dynamic mask and edit function to ensure the opacity and the covertness against a sensor eavesdropping but command non-eavesdropping intruder, and marker-reachability; 2) we then decompose ME into a dynamic mask and an edit function by using a constraint-based approach, with the help of a Boolean satisfiability (SAT) solver; 3) finally, we synthesize a supervisor such that opacity and covertness can be ensured against the sensor-actuator eavesdropping intruder, and at the same time safety and nonblockingness requirement can be ensured. The effectiveness of our approach is illustrated on an example about the enforcement of location privacy for an autonomous vehicle.      
### 14.On the Undesired Equilibria Induced by Control Barrier Function Based Quadratic Programs  [ :arrow_down: ](https://arxiv.org/pdf/2104.14895.pdf)
>  In this paper, we propose a new control barrier function based quadratic program for general nonlinear control-affine systems, which, without any assumptions other than those taken in the original program, simultaneously guarantees forward invariance of the safety set, complete elimination of undesired equilibrium points inside it, and local asymptotic stability of the origin. To better appreciate this result, we first characterize the equilibrium points of the closed-loop system with the original quadratic program formulation. We then provide analytical results on how a certain parameter in the original quadratic program should be chosen to remove the undesired equilibrium points or to confine them in a small neighborhood of the origin. The new formulation then follows from these analytical results. Numerical examples are given alongside the theoretical discussions.      
### 15.Robust joint registration of multiple stains and MRI for multimodal 3D histology reconstruction: Application to the Allen human brain atlas  [ :arrow_down: ](https://arxiv.org/pdf/2104.14873.pdf)
>  Joint registration of a stack of 2D histological sections to recover 3D structure (3D histology reconstruction) finds application in areas such as atlas building and validation of in vivo imaging. Straighforward pairwise registration of neighbouring sections yields smooth reconstructions but has well-known problems such as banana effect (straightening of curved structures) and z-shift (drift). While these problems can be alleviated with an external, linearly aligned reference (e.g., Magnetic Resonance images), registration is often inaccurate due to contrast differences and the strong nonlinear distortion of the tissue, including artefacts such as folds and tears. In this paper, we present a probabilistic model of spatial deformation that yields reconstructions for multiple histological stains that that are jointly smooth, robust to outliers, and follow the reference shape. The model relies on a spanning tree of latent transforms connecting all the sections and slices, and assumes that the registration between any pair of images can be see as a noisy version of the composition of (possibly inverted) latent transforms connecting the two images. Bayesian inference is used to compute the most likely latent transforms given a set of pairwise registrations between image pairs within and across modalities. Results on synthetic deformations on multiple MR modalities, show that our method can accurately and robustly register multiple contrasts even in the presence of outliers. The 3D histology reconstruction of two stains (Nissl and parvalbumin) from the Allen human brain atlas, show its benefits on real data with severe distortions. We also provide the correspondence to MNI space, bridging the gap between two of the most used atlases in histology and MRI. Data is available at <a class="link-external link-https" href="https://openneuro.org/datasets/ds003590" rel="external noopener nofollow">this https URL</a> and code at <a class="link-external link-https" href="https://github.com/acasamitjana/3dhirest" rel="external noopener nofollow">this https URL</a>.      
### 16.On the Computation of PSNR for a Set of Images or Video  [ :arrow_down: ](https://arxiv.org/pdf/2104.14868.pdf)
>  When comparing learned image/video restoration and compression methods, it is common to report peak-signal to noise ratio (PSNR) results. However, there does not exist a generally agreed upon practice to compute PSNR for sets of images or video. Some authors report average of individual image/frame PSNR, which is equivalent to computing a single PSNR from the geometric mean of individual image/frame mean-square error (MSE). Others compute a single PSNR from the arithmetic mean of frame MSEs for each video. Furthermore, some compute the MSE/PSNR of Y-channel only, while others compute MSE/PSNR for RGB channels. This paper investigates different approaches to computing PSNR for sets of images, single video, and sets of video and the relation between them. We show the difference between computing the PSNR based on arithmetic vs. geometric mean of MSE depends on the distribution of MSE over the set of images or video, and that this distribution is task-dependent. In particular, these two methods yield larger differences in restoration problems, where the MSE is exponentially distributed and smaller differences in compression problems, where the MSE distribution is narrower. We hope this paper will motivate the community to clearly describe how they compute reported PSNR values to enable consistent comparison.      
### 17.A Thermodynamic based and Data Driven Hybrid Network for Gas Turbine Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2104.14842.pdf)
>  The on-wing engine performance is difficult to track for thermodynamic models because of its inaccurate component maps, and also difficult for data driven methods for their over-fitting to measurement errors. So, we propose a thermodynamic based and data driven hybrid network for gas turbine modeling. Different from thermodynamic models, our network reconstructs the component characteristics in a data-driven way to take component degeneration and individual difference into consideration. Moreover, different from data driven methods, in the training phase, physical based equations and the analytical mathematical description are used to ensure that the optimization converges to the gas turbine's dynamics. A huge number of relaxed quasi steady state flight data to 26970 is used to train and test our hybrid network. The result shows that the accuracy of our hybrid network can reach about 7% measured by max T6 relative error, 5% better than map fitting based thermodynamic model and 8% better than pure data driven method with similar model volume.      
### 18.A Practical Approach for Rate-Distortion-Perception Analysis in Learned Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2104.14836.pdf)
>  Rate-distortion optimization (RDO) of codecs, where distortion is quantified by the mean-square error, has been a standard practice in image/video compression over the years. RDO serves well for optimization of codec performance for evaluation of the results in terms of PSNR. However, it is well known that the PSNR does not correlate well with perceptual evaluation of images; hence, RDO is not well suited for perceptual optimization of codecs. Recently, rate-distortion-perception trade-off has been formalized by taking the Kullback-Leibner (KL) divergence between the distributions of the original and reconstructed images as a perception measure. Learned image compression methods that simultaneously optimize rate, mean-square loss, VGG loss, and an adversarial loss were proposed. Yet, there exists no easy approach to fix the rate, distortion or perception at a desired level in a practical learned image compression solution to perform an analysis of the trade-off between rate, distortion and perception measures. In this paper, we propose a practical approach to fix the rate to carry out perception-distortion analysis at a fixed rate in order to perform perceptual evaluation of image compression results in a principled manner. Experimental results provide several insights for practical rate-distortion-perception analysis in learned image compression.      
### 19.Cramér-Rao Bounds for Near-Field Localization  [ :arrow_down: ](https://arxiv.org/pdf/2104.14825.pdf)
>  Multiple antenna arrays play a key role in wireless networks for communications but also localization and sensing. The use of large antenna arrays pushes towards the near-field propagation regime in which the wavefront is no longer plane but spherical. This allows to infer the position and orientation of an arbitrary source from the received signal without the need of using multiple anchor nodes. To understand the fundamental limits of large antenna arrays for localization, this paper fusions wave propagation theory with estimation theory, and computes the Cramér-Rao Bound (CRB) for the estimation of the three Cartesian coordinates of the source on the basis of the electromagnetic vector field, observed over a rectangular surface area. To simplify the analysis, we assume that the source is a dipole, whose center is located on the line perpendicular to the surface center, with an orientation a priori known. Numerical and asymptotic results are given to quantify the CRBs, and to gain insights into the effect of various system parameters on the ultimate estimation accuracy. It turns out that surfaces of practical size may guarantee a centimeter-level accuracy in the mmWave bands.      
### 20.Deformable TDNN with adaptive receptive fields for speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2104.14791.pdf)
>  Time Delay Neural Networks (TDNNs) are widely used in both DNN-HMM based hybrid speech recognition systems and recent end-to-end systems. Nevertheless, the receptive fields of TDNNs are limited and fixed, which is not desirable for tasks like speech recognition, where the temporal dynamics of speech are varied and affected by many factors. This paper proposes to use deformable TDNNs for adaptive temporal dynamics modeling in end-to-end speech recognition. Inspired by deformable ConvNets, deformable TDNNs augment the temporal sampling locations with additional offsets and learn the offsets automatically based on the ASR criterion, without additional supervision. Experiments show that deformable TDNNs obtain state-of-the-art results on WSJ benchmarks (1.42\%/3.45\% WER on WSJ eval92/dev93 respectively), outperforming standard TDNNs significantly. Furthermore, we propose the latency control mechanism for deformable TDNNs, which enables deformable TDNNs to do streaming ASR without accuracy degradation.      
### 21.Analytical models and estimations for a meander line turn protecting against an ultrawide band pulse  [ :arrow_down: ](https://arxiv.org/pdf/2104.14784.pdf)
>  The paper presents analytical models for calculating the response to a pulse excitation in a turn of a meander line. One of the models was used to derive a condition for equalizing the voltage amplitudes for three pulses at the end of the turn which has different per-unit-length delays of even and odd modes. The obtained models and equalization condition were validated by quasistatic simulation. The results will allow for accelerated design and optimization of this type of structures for protection against ultrawide band pulses without costly multivariate analysis and calculation of response      
### 22.A Compact Model for SiC Power MOSFETs for Large Current and High Voltage Operation  [ :arrow_down: ](https://arxiv.org/pdf/2104.14733.pdf)
>  This work presents a physics based compact model for SiC power MOSFETs that accurately describes the I-V characteristics up to large voltages and currents. Charge-based formulations accounting for the different physics of SiC power MOSFETs are presented. The formulations account for the effect of the large SiC/SiO2 interface traps density characteristic of SiC MOSFETs and its dependence with temperature. The modeling of interface charge density is found to be necessary to describe the electrostatics of SiC power MOSFETs when operating at simultaneous high current and high voltage regions. The proposed compact model accurately fits the measurement data extracted of a 160 milli ohms, 1200V SiC power MOSFET in the complete IV plane from drain-voltage $V_d$ = 5mV up to 800 V and current ranges from few mA to 30 A.      
### 23.Simultaneous Denoising and Localization Network for Photoacoustic Target Localization  [ :arrow_down: ](https://arxiv.org/pdf/2104.14713.pdf)
>  A significant research problem of recent interest is the localization of targets like vessels, surgical needles, and tumors in photoacoustic (PA) images. To achieve accurate localization, a high photoacoustic signal-to-noise ratio (SNR) is required. However, this is not guaranteed for deep targets, as optical scattering causes an exponential decay in optical fluence with respect to tissue depth. To address this, we develop a novel deep learning method designed to explicitly exhibit robustness to noise present in photoacoustic radio-frequency (RF) data. More precisely, we describe and evaluate a deep neural network architecture consisting of a shared encoder and two parallel decoders. One decoder extracts the target coordinates from the input RF data while the other boosts the SNR and estimates clean RF data. The joint optimization of the shared encoder and dual decoders lends significant noise robustness to the features extracted by the encoder, which in turn enables the network to contain detailed information about deep targets that may be obscured by noise. Additional custom layers and newly proposed regularizers in the training loss function (designed based on observed RF data signal and noise behavior) serve to increase the SNR in the cleaned RF output and improve model performance. To account for depth-dependent strong optical scattering, our network was trained with simulated photoacoustic datasets of targets embedded at different depths inside tissue media of different scattering levels. The network trained on this novel dataset accurately locates targets in experimental PA data that is clinically relevant with respect to the localization of vessels, needles, or brachytherapy seeds. We verify the merits of the proposed architecture by outperforming the state of the art on both simulated and experimental datasets.      
### 24.Lung Cancer Diagnosis Using Deep Attention Based on Multiple Instance Learning and Radiomics  [ :arrow_down: ](https://arxiv.org/pdf/2104.14655.pdf)
>  Early diagnosis of lung cancer is a key intervention for the treatment of lung cancer computer aided diagnosis (CAD) can play a crucial role. However, most published CAD methods treat lung cancer diagnosis as a lung nodule classification problem, which does not reflect clinical practice, where clinicians diagnose a patient based on a set of images of nodules, instead of one specific nodule. Besides, the low interpretability of the output provided by these methods presents an important barrier for their adoption. In this article, we treat lung cancer diagnosis as a multiple instance learning (MIL) problem in order to better reflect the diagnosis process in the clinical setting and for the higher interpretability of the output. We chose radiomics as the source of input features and deep attention-based MIL as the classification algorithm.The attention mechanism provides higher interpretability by estimating the importance of each instance in the set for the final <a class="link-external link-http" href="http://diagnosis.In" rel="external noopener nofollow">this http URL</a> order to improve the model's performance in a small imbalanced dataset, we introduce a new bag simulation method for MIL.The results show that our method can achieve a mean accuracy of 0.807 with a standard error of the mean (SEM) of 0.069, a recall of 0.870 (SEM 0.061), a positive predictive value of 0.928 (SEM 0.078), a negative predictive value of 0.591 (SEM 0.155) and an area under the curve (AUC) of 0.842 (SEM 0.074), outperforming other MIL methods.Additional experiments show that the proposed oversampling strategy significantly improves the model's performance. In addition, our experiments show that our method provides an indication of the importance of each nodule in determining the diagnosis, which combined with the well-defined radiomic features, make the results more interpretable and acceptable for doctors and patients.      
### 25.Fast Multiscale Diffusion on Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2104.14652.pdf)
>  Diffusing a graph signal at multiple scales requires computing the action of the exponential of several multiples of the Laplacian matrix. We tighten a bound on the approximation error of truncated Chebyshev polynomial approximations of the exponential, hence significantly improving a priori estimates of the polynomial order for a prescribed error. We further exploit properties of these approximations to factorize the computation of the action of the diffusion operator over multiple scales, thus reducing drastically its computational cost.      
### 26.A Novel Real-Time Energy Management Strategy for Grid-Supporting Microgrid: Enabling Flexible Trading Power  [ :arrow_down: ](https://arxiv.org/pdf/2104.14635.pdf)
>  In recent years, there has been significant growth of distributed energy resources (DERs) penetration in the power grid. The stochastic and intermittent features of variable DERs such as roof top photovoltaic (PV) bring substantial uncertainties to the grid on the consumer end and weaken the grid reliability. In addition, the fact that numerous DERs are widespread in the grid makes it hard to monitor and manage DERs. To address this challenge, this paper proposes a novel real-time grid-supporting energy management (GSEM) strategy for grid-supporting microgrid (MG). This strategy can not only properly manage DERs in a MG but also enable DERs to provide grid services, which enables a MG to be grid-supporting via flexible trading power. The proposed GSEM strategy is based on a 2-step optimization which includes a routine economic dispatch (ED) step and an acceptable trading power range determination step. Numerical simulations demonstrate the performance of the proposed GSEM strategy which enables the grid operator to have a dispatch choice of trading power with MG and enhance the reliability and resilience of the main grid.      
### 27.Generative Models Improve Radiomics Reproducibility in Low Dose CTs: A Simulation Study  [ :arrow_down: ](https://arxiv.org/pdf/2104.15050.pdf)
>  Radiomics is an active area of research in medical image analysis, the low reproducibility of radiomics has limited its applicability to clinical practice. This issue is especially prominent when radiomic features are calculated from noisy images, such as low dose computed tomography (CT) scans. In this article, we investigate the possibility of improving the reproducibility of radiomic features calculated on noisy CTs by using generative models for denoising.One traditional denoising method - non-local means - and two generative models - encoder-decoder networks (EDN) and conditional generative adversarial networks (CGANs) - were selected as the test models. We added noise to the sinograms of full dose CTs to mimic low dose CTs with two different levels of noise: low-noise CT and high-noise CT. Models were trained on high-noise CTs and used to denoise low-noise CTs without re-training. We also test the performance of our model in real data, using dataset of same-day repeat low dose CTs to assess the reproducibility of radiomic features in denoised images. The EDN and the CGAN improved the concordance correlation coefficients (CCC) of radiomic features for low-noise images from 0.87 to 0.92 and for high-noise images from 0.68 to 0.92 respectively. Moreover, the EDN and the CGAN improved the test-retest reliability of radiomic features (mean CCC increased from 0.89 to 0.94) based on real low dose CTs. The results show that denoising using EDN and CGANs can improve the reproducibility of radiomic features calculated on noisy CTs. Moreover, images with different noise levels can be denoised to improve the reproducibility using these models without re-training, as long as the noise intensity is equal or lower than that in high-noise CTs. To the authors' knowledge, this is the first effort to improve the reproducibility of radiomic features calculated on low dose CT scans.      
### 28.Safety-Control of Mobile Robots Under Time-Delay Using Barrier Certificates and a Two-Layer Predictor  [ :arrow_down: ](https://arxiv.org/pdf/2104.15047.pdf)
>  Performing swift and agile maneuvers is essential for the safe operation of autonomous mobile robots. Moreover, the presence of time-delay restricts the response time of the system and hinders the safety performance. Thus, this paper proposes a modular and scalable safety-control design that utilizes the Smith predictor and barrier certificates to safely and consistently avoid obstacles with different footprints. The proposed solution includes a two-layer predictor to compensate for the time-delay in the servo-system and angle control loops. The proposed predictor configuration dramatically improves the transient performance and reduces response time. Barrier certificates are used to determine the safe range of the robot's heading angle to avoid collisions. The proposed obstacle avoidance technique conveniently integrates with various trajectory tracking algorithms, which enhances design flexibility. The angle condition is adaptively calculated and corrects the robot's heading angle and angular velocity. Also, the proposed method accommodates multiple obstacles and decouples the control structure from the obstacles' shape, count, and distribution. The control structure has only eight tunable parameters facilitating control calibration and tuning in large systems of mobile robots. Extensive experimental results verify the effectiveness of the proposed safety-control.      
### 29.Deep Image Destruction: A Comprehensive Study on Vulnerability of Deep Image-to-Image Models against Adversarial Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2104.15022.pdf)
>  Recently, the vulnerability of deep image classification models to adversarial attacks has been investigated. However, such an issue has not been thoroughly studied for image-to-image models that can have different characteristics in quantitative evaluation, consequences of attacks, and defense strategy. To tackle this, we present comprehensive investigations into the vulnerability of deep image-to-image models to adversarial attacks. For five popular image-to-image tasks, 16 deep models are analyzed from various standpoints such as output quality degradation due to attacks, transferability of adversarial examples across different tasks, and characteristics of perturbations. We show that unlike in image classification tasks, the performance degradation on image-to-image tasks can largely differ depending on various factors, e.g., attack methods and task objectives. In addition, we analyze the effectiveness of conventional defense methods used for classification models in improving the robustness of the image-to-image models.      
### 30.Impacts of shared autonomous vehicles: Tradeoff between parking demand reduction and congestion increase  [ :arrow_down: ](https://arxiv.org/pdf/2104.15019.pdf)
>  Shared autonomous vehicles (SAVs) can have significant impacts on the transport system and land use by replacing private vehicles. Sharing vehicles without drivers is expected to reduce parking demand, and as a side effect, increase congestion owing to the empty fleets made by SAVs picking up travelers and relocating. Although the impact may not be uniform over a region of interest owing to the heterogeneity of travel demand distribution and network configuration, few studies have debated such impact at a local scale, such as in transportation analysis zones (TAZs). To understand the impact in relation to geographical situations, this study aims to estimate the impacts of SAVs at the local scale by simulating their operation on a developed simulator. Using the mainland of Okinawa, Japan as a case study, it was found that parking demand was reduced the most in residence-dominant zones in terms of quantity and in office-dominant zones in terms of proportion. As a side effect of replacing private vehicles with SAVs, empty fleets increase congestion, particularly at the periphery of the city. Overall, the results show the heterogeneous impacts of the SAVs at the TAZ level on both land use and traffic, thus suggesting the importance of developing appropriate strategies for urban and transport planning when considering the characteristics of the zones.      
### 31.A Path to Smart Radio Environments: An Industrial Viewpoint on Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2104.14985.pdf)
>  With both the standardization and commercialization completed in an unforeseen pace for the 5th generation (5G) wireless network, researchers, engineers and executives from the academia and the industry have turned their sights on candidate technologies to support the next generation wireless networks. Reconfigurable intelligent surfaces (RIS), sometimes referred to as intelligent reflecting surfaces (IRS), have been identified to be potential components of the future wireless networks because they can reconfigure the propagation environment for wireless signals with low-cost passive devices. In doing so, the coverage of a cell can be expected to increase significantly as well as the overall throughput of the network. RIS has not only become an attractive research area but also triggered a couple of projects to develop appropriate solutions to enable the set-up of hardware demonstrations and prototypes. In parallel, technical discussions and activities towards standardization already took off in some regions. Promoting RIS to be integrated into future commercial networks and become a commercial success requires significant standardization work taken place both at regional level standards developing organizations (SDO) and international SDOs such as the 3rd Generation Partnership Project (3GPP). While many research papers study how RIS can be used and optimized, few effort is devoted to analyzing the challenges to commercialize RIS and how RIS can be standardized. This paper intends to shed some light on RIS from an industrial viewpoint and provide a clear roadmap to make RIS industrially feasible.      
### 32.RSSI-Based Location Classification Using a Particle Filter to Fuse Sensor Estimates  [ :arrow_down: ](https://arxiv.org/pdf/2104.14874.pdf)
>  For Cyper-Physical Production Systems (CPPS), localization is becoming increasingly important as wireless and mobile devices are considered an integral part. While localizing targets in a wireless communication system based on the Received Signal Strength Indicators (RSSIs) is a usual solution, it is limited by sensor quality. We consider the scenario of a car moving in and out of a chamber and propose to use a particle filter for sensor fusion, allowing us to incorporate non-idealities in our model and achieve a high-quality position estimate. Then, we use Machine Learning (ML) to classify the vehicle position. Our results show that the location output of the particle filter is a better input to the classifiers than the raw RSSI data, and we achieve improved accuracy while simultaneously reducing the number of features that the ML has to consider. We also compare the performance of multiple ML algorithms and show that SVMs provide the overall best performance for the given task.      
### 33.RSSI-Based Machine Learning with Pre- and Post-Processing for Cell-Localization in IWSNs  [ :arrow_down: ](https://arxiv.org/pdf/2104.14865.pdf)
>  Industrial wireless sensor networks are becoming crucial for modern manufacturing. If the sensors in those networks are mobile, the position information, besides the sensor data itself, can be of high relevance. E.g. this position information can increase the trustability of a wireless sensor measurement by assuring that the sensor is not physically removed, off track, or otherwise compromised. <br>In certain applications, localization information at cell-level, whether the sensor is inside or outside a room or cell, is sufficient. For this, localization using Received Signal Strength Indicator (RSSI) measurements is very popular since RSSI values are available in almost all existing technologies and no direct interaction with the mobile sensor node and its communication in the network is needed. For this scenario, we propose methods to improve the robustness and accuracy of common machine learning classifiers, by using features based on short-term moments and a second classification stage using Hidden Markov Models. With the data from an extensive measurement campaign, we show the applicability of our method and achieve a cell-level localization accuracy of 93.5\%.      
### 34.Unique Ergodicity in the Interconnections of Ensembles with Applications to Two-Sided Markets  [ :arrow_down: ](https://arxiv.org/pdf/2104.14858.pdf)
>  There has been much recent interest in two-sided markets and dynamics thereof. In a rather a general discrete-time feedback model, which we show conditions that assure that for each agent, there exists the limit of a long-run average allocation of a resource to the agent, which is independent of any initial conditions. We call this property the unique ergodicity. <br>Our model encompasses two-sided markets and more complicated interconnections of workers and customers, such as in a supply chain. It allows for non-linearity of the response functions of market participants. It allows for uncertainty in the response of market participants by considering a set of the possible responses to either price or other signals and a measure to sample from these. Finally, it allows for an arbitrary delay between the arrival of incoming data and the clearing of a market.      
### 35.Scaling End-to-End Models for Large-Scale Multilingual ASR  [ :arrow_down: ](https://arxiv.org/pdf/2104.14830.pdf)
>  Building ASR models across many language families is a challenging multi-task learning problem due to large language variations and heavily unbalanced data. Existing work has shown positive transfer from high resource to low resource languages. However, degradations on high resource languages are commonly observed due to interference from the heterogeneous multilingual data and reduction in per-language capacity. We conduct a capacity study on a 15-language task, with the amount of data per language varying from 7.7K to 54.7K hours. We adopt GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that (1) scaling the number of model parameters is an effective way to solve the capacity bottleneck - our 500M-param model is already better than monolingual baselines and scaling it to 1B and 10B brought further quality gains; (2) larger models are not only more data efficient, but also more efficient in terms of training cost as measured in TPU days - the 1B-param model reaches the same accuracy at 34% of training time as the 500M-param model; (3) given a fixed capacity budget, adding depth usually works better than width and large encoders tend to do better than large decoders.      
### 36.Dance Generation with Style Embedding: Learning and Transferring Latent Representations of Dance Styles  [ :arrow_down: ](https://arxiv.org/pdf/2104.14802.pdf)
>  Choreography refers to creation of dance steps and motions for dances according to the latent knowledge in human mind, where the created dance motions are in general style-specific and consistent. So far, such latent style-specific knowledge about dance styles cannot be represented explicitly in human language and has not yet been learned in previous works on music-to-dance generation tasks. In this paper, we propose a novel music-to-dance synthesis framework with controllable style embeddings. These embeddings are learned representations of style-consistent kinematic abstraction of reference dance clips, which act as controllable factors to impose style constraints on dance generation in a latent manner. Thus, the dance styles can be transferred to dance motions by merely modifying the style embeddings. To support this study, we build a large music-to-dance dataset. The qualitative and quantitative evaluations demonstrate the advantage of our proposed framework, as well as the ability of synthesizing diverse styles of dances from identical music via style embeddings.      
### 37.Massive-MIMO Iterative Channel Estimation and Decoding (MICED) in the Uplink  [ :arrow_down: ](https://arxiv.org/pdf/2104.14797.pdf)
>  Massive MIMO uses a large number of antennas to increase the spectral efficiency (SE) through spatial multiplexing of users, which requires accurate channel state information. It is often assumed that regular pilots (RP), where a fraction of the time-frequency resources is reserved for pilots, suffices to provide high SE. However, the SE is limited by the pilot overhead and pilot contamination. An alternative is superimposed pilots (SP) where all resources are used for pilots and data. This removes the pilot overhead and reduces pilot contamination by using longer pilots. However, SP suffers from data interference that reduces the SE gains. This paper proposes the Massive-MIMO Iterative Channel Estimation and Decoding (MICED) algorithm where partially decoded data is used as side-information to improve the channel estimation and increase SE. We show that users with precise data estimates can help users with poor data estimates to decode. Numerical results with QPSK modulation and LDPC codes show that the MICED algorithm increases the SE and reduces the block-error-rate with RP and SP compared to conventional methods. The MICED algorithm with SP delivers the highest SE and it is especially effective in scenarios with short coherence blocks like high mobility or high frequencies.      
### 38.Perceptual Image Quality Assessment with Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2104.14730.pdf)
>  In this paper, we propose an image quality transformer (IQT) that successfully applies a transformer architecture to a perceptual full-reference image quality assessment (IQA) task. Perceptual representation becomes more important in image quality assessment. In this context, we extract the perceptual feature representations from each of input images using a convolutional neural network (CNN) backbone. The extracted feature maps are fed into the transformer encoder and decoder in order to compare a reference and distorted images. Following an approach of the transformer-based vision models, we use extra learnable quality embedding and position embedding. The output of the transformer is passed to a prediction head in order to predict a final quality score. The experimental results show that our proposed model has an outstanding performance for the standard IQA datasets. For a large-scale IQA dataset containing output images of generative model, our model also shows the promising results. The proposed IQT was ranked first among 13 participants in the NTIRE 2021 perceptual image quality assessment challenge. Our work will be an opportunity to further expand the approach for the perceptual IQA task.      
### 39.Dynamic population games  [ :arrow_down: ](https://arxiv.org/pdf/2104.14662.pdf)
>  In this paper, we define a new class of dynamic games played in large populations of anonymous agents. The behavior of agents in these games depends on a time-homogeneous type and a time-varying state, which are private to each agent and characterize their available actions and motifs. We consider finite type, state, and action spaces. On the individual agent level, the state evolves in discrete-time as the agent participates in interactions, in which the state transitions are affected by the agent's individual action and the distribution of other agents' states and actions. On the societal level, we consider that the agents form a continuum of mass and that interactions occur either synchronously or asynchronously, and derive models for the evolution of the agents' state distribution. We characterize the stationary equilibrium as the solution concept in our games, which is a condition where all agents are playing their best response and the state distribution is stationary. At least one stationary equilibrium is guaranteed to exist in every dynamic population game. Our approach intersects with previous works on anonymous sequential games, mean-field games, and Markov decision evolutionary games, but it is novel in how we relate the dynamic setting to a classical, static population game setting. In particular, stationary equilibria can be reduced to standard Nash equilibria in classical population games. This simplifies the analysis of these games and inspires the formulation of an evolutionary model for the coupled dynamics of both the agents' actions and states.      
### 40.Text2Video: Text-driven Talking-head Video Synthesis with Phonetic Dictionary  [ :arrow_down: ](https://arxiv.org/pdf/2104.14631.pdf)
>  With the advance of deep learning technology, automatic video generation from audio or text has become an emerging and promising research topic. In this paper, we present a novel approach to synthesize video from the text. The method builds a phoneme-pose dictionary and trains a generative adversarial network (GAN) to generate video from interpolated phoneme poses. Compared to audio-driven video generation algorithms, our approach has a number of advantages: 1) It only needs a fraction of the training data used by an audio-driven approach; 2) It is more flexible and not subject to vulnerability due to speaker variation; 3) It significantly reduces the preprocessing, training and inference time. We perform extensive experiments to compare the proposed method with state-of-the-art talking face generation methods on a benchmark dataset and datasets of our own. The results demonstrate the effectiveness and superiority of our approach.      
### 41.User-centric Cell-free Massive MIMO Networks: A Survey of Opportunities, Challenges and Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2104.14589.pdf)
>  Densification of network base stations is indispensable to achieve the stringent Quality of Service (QoS) requirements of future mobile networks. However, with a dense deployment of transmitters, interference management becomes an arduous task. To solve this issue, exploring radically new network architectures with intelligent coordination and cooperation capabilities is crucial. This survey paper investigates the emerging user-centric cell-free massive multiple-input multiple-output (MIMO) network architecture that sets a foundation for future mobile networks. Such networks use a dense deployment of distributed units (DUs) to serve users; the crucial difference from the traditional cellular paradigm is that a specific serving cluster of DUs is defined for each user. This framework provides macro diversity, power efficiency, interference management, and robust connectivity. Most importantly, the user-centric approach eliminates cell edges, thus contributing to uniform coverage and performance for users across the network area. We present here a guide to the key challenges facing the deployment of this network scheme and contemplate the solutions being proposed for the main bottlenecks facing cell-free communications. Specifically, we survey the literature targeting the fronthaul, then we scan the details of the channel estimation required, resource allocation, delay, and scalability issues. Furthermore, we highlight some technologies that can provide a management platform for this scheme such as distributed software-defined network (SDN) and self-organizing network (SON). Our article serves as a check point that delineates the current status and indicates future directions for this area in a comprehensive manner.      
### 42.Crack Semantic Segmentation using the U-Net with Full Attention Strategy  [ :arrow_down: ](https://arxiv.org/pdf/2104.14586.pdf)
>  Structures suffer from the emergence of cracks, therefore, crack detection is always an issue with much concern in structural health monitoring. Along with the rapid progress of deep learning technology, image semantic segmentation, an active research field, offers another solution, which is more effective and intelligent, to crack detection Through numerous artificial neural networks have been developed to address the preceding issue, corresponding explorations are never stopped improving the quality of crack detection. This paper presents a novel artificial neural network architecture named Full Attention U-net for image semantic segmentation. The proposed architecture leverages the U-net as the backbone and adopts the Full Attention Strategy, which is a synthesis of the attention mechanism and the outputs from each encoding layer in skip connection. Subject to the hardware in training, the experiments are composed of verification and validation. In verification, 4 networks including U-net, Attention U-net, Advanced Attention U-net, and Full Attention U-net are tested through cell images for a competitive study. With respect to mean intersection-over-unions and clarity of edge identification, the Full Attention U-net performs best in verification, and is hence applied for crack semantic segmentation in validation to demonstrate its effectiveness.      
### 43.A Novel Look at LIDAR-aided Data-driven mmWave Beam Selection  [ :arrow_down: ](https://arxiv.org/pdf/2104.14579.pdf)
>  Efficient millimeter wave (mmWave) beam selection in vehicle-to-infrastructure (V2I) communication is a crucial yet challenging task due to the narrow mmWave beamwidth and high user mobility. To reduce the search overhead of iterative beam discovery procedures, contextual information from light detection and ranging (LIDAR) sensors mounted on vehicles has been leveraged by data-driven methods to produce useful side information. In this paper, we propose a lightweight neural network (NN) architecture along with the corresponding LIDAR preprocessing, which significantly outperforms previous works. Our solution comprises multiple novelties that improve both the convergence speed and the final accuracy of the model. In particular, we define a novel loss function inspired by the knowledge distillation idea, introduce a curriculum training approach exploiting line-of-sight (LOS)/non-line-of-sight (NLOS) information, and we propose a non-local attention module to improve the performance for the more challenging NLOS cases. Simulation results on benchmark datasets show that, utilizing solely LIDAR data and the receiver position, our NN-based beam selection scheme can achieve 79.9% throughput of an exhaustive beam sweeping approach without any beam search overhead and 95% by searching among as few as 6 beams.      
### 44.Stochastic Mirror Descent for Low-Rank Tensor Decomposition Under Non-Euclidean Losses  [ :arrow_down: ](https://arxiv.org/pdf/2104.14562.pdf)
>  This work considers low-rank canonical polyadic decomposition (CPD) under a class of non-Euclidean loss functions that frequently arise in statistical machine learning and signal processing. These loss functions are often used for certain types of tensor data, e.g., count and binary tensors, where the least squares loss is considered unnatural.Compared to the least squares loss, the non-Euclidean losses are generally more challenging to handle. Non-Euclidean CPD has attracted considerable interests and a number of prior works exist. However, pressing computational and theoretical challenges, such as scalability and convergence issues, still remain. This work offers a unified stochastic algorithmic framework for large-scale CPD decomposition under a variety of non-Euclidean loss functions. Our key contribution lies in a tensor fiber sampling strategy-based flexible stochastic mirror descent framework. Leveraging the sampling scheme and the multilinear algebraic structure of low-rank tensors, the proposed lightweight algorithm ensures global convergence to a stationary point under reasonable conditions. Numerical results show that our framework attains promising non-Euclidean CPD performance. The proposed framework also exhibits substantial computational savings compared to state-of-the-art methods.      
### 45.Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2104.14526.pdf)
>  Tensors, which provide a powerful and flexible model for representing multi-attribute data and multi-way interactions, play an indispensable role in modern data science across various fields in science and engineering. A fundamental task is to faithfully recover the tensor from highly incomplete measurements in a statistically and computationally efficient manner. Harnessing the low-rank structure of tensors in the Tucker decomposition, this paper develops a scaled gradient descent (ScaledGD) algorithm to directly recover the tensor factors with tailored spectral initializations, and shows that it provably converges at a linear rate independent of the condition number of the ground truth tensor for two canonical problems -- tensor completion and tensor regression -- as soon as the sample size is above the order of $n^{3/2}$ ignoring other dependencies, where $n$ is the dimension of the tensor. This leads to an extremely scalable approach to low-rank tensor estimation compared with prior art, which suffers from at least one of the following drawbacks: extreme sensitivity to ill-conditioning, high per-iteration costs in terms of memory and computation, or poor sample complexity guarantees. To the best of our knowledge, ScaledGD is the first algorithm that achieves near-optimal statistical and computational complexities simultaneously for low-rank tensor completion with the Tucker decomposition. Our algorithm highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the underlying symmetry in low-rank tensor factorization.      
