# ArXiv eess --Thu, 13 May 2021
### 1.20-fold Accelerated 7T fMRI Using Referenceless Self-Supervised Deep Learning Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2105.05827.pdf)
>  High spatial and temporal resolution across the whole brain is essential to accurately resolve neural activities in fMRI. Therefore, accelerated imaging techniques target improved coverage with high spatio-temporal resolution. Simultaneous multi-slice (SMS) imaging combined with in-plane acceleration are used in large studies that involve ultrahigh field fMRI, such as the Human Connectome Project. However, for even higher acceleration rates, these methods cannot be reliably utilized due to aliasing and noise artifacts. Deep learning (DL) reconstruction techniques have recently gained substantial interest for improving highly-accelerated MRI. Supervised learning of DL reconstructions generally requires fully-sampled training datasets, which is not available for high-resolution fMRI studies. To tackle this challenge, self-supervised learning has been proposed for training of DL reconstruction with only undersampled datasets, showing similar performance to supervised learning. In this study, we utilize a self-supervised physics-guided DL reconstruction on a 5-fold SMS and 4-fold in-plane accelerated 7T fMRI data. Our results show that our self-supervised DL reconstruction produce high-quality images at this 20-fold acceleration, substantially improving on existing methods, while showing similar functional precision and temporal effects in the subsequent analysis compared to a standard 10-fold accelerated acquisition.      
### 2.Deep Snapshot HDR Reconstruction Based on the Polarization Camera  [ :arrow_down: ](https://arxiv.org/pdf/2105.05824.pdf)
>  The recent development of the on-chip micro-polarizer technology has made it possible to acquire four spatially aligned and temporally synchronized polarization images with the same ease of operation as a conventional camera. In this paper, we investigate the use of this sensor technology in high-dynamic-range (HDR) imaging. Specifically, observing that natural light can be attenuated differently by varying the orientation of the polarization filter, we treat the multiple images captured by the polarization camera as a set captured under different exposure times. In our approach, we first study the relationship among polarizer orientation, degree and angle of polarization of light to the exposure time of a pixel in the polarization image. Subsequently, we propose a deep snapshot HDR reconstruction framework to recover an HDR image using the polarization images. A polarized HDR dataset is created to train and evaluate our approach. We demonstrate that our approach performs favorably against state-of-the-art HDR reconstruction algorithms.      
### 3.Non-sequential Division  [ :arrow_down: ](https://arxiv.org/pdf/2105.05747.pdf)
>  The division operation is important for many areas of data processing. Especially considering today's demand for hardware accelerators for machine learning algorithms, there is a high demand for an efficient calculation of the division function, e.g. for averaging operations or the online calculation of activation functions. For such algorithms, which are often iterative in nature, one would like to have a non-sequential way of calculating the division operation. The work presents such an approach. It is based on an efficient way of calculating the reciprocal operation, based on a low complexity approximation combined with a correction function. The described approach allows approximating the division operation (with errors that can be made arbitrarily low), within one clock cycle using only low hardware requirements (although one might reasonably use more clock cycles to increase the clock frequency). These hardware requirements are scale-able depending on the desired precision. We show results obtained by synthesis and hardware simulations demonstrating the low complexity and high clock speed achievable with the described method.      
### 4.Fault-Resilient PCIe Bus with Real-time Error Detection and Correction  [ :arrow_down: ](https://arxiv.org/pdf/2105.05739.pdf)
>  This paper presents a novel IP design for real-time fault/error detection and recovery on a peripheral component interconnect express (PCIe) which interfaces a host system (here a PC) to a slave design including processing system and memory transaction implemented on a Zynq Ultrascale Xilinx Kintex FPGA board (KCU105). The proposed IP design is capable of detection and correction of different types of PCIe errors on-the-fly      
### 5.Deep Multi-agent Reinforcement Learning for Highway On-Ramp Merging in Mixed Traffic  [ :arrow_down: ](https://arxiv.org/pdf/2105.05701.pdf)
>  On-ramp merging is a challenging task for autonomous vehicles (AVs), especially in mixed traffic where AVs coexist with human-driven vehicles (HDVs). In this paper, we formulate the mixed-traffic highway on-ramp merging problem as a multi-agent reinforcement learning (MARL) problem, where the AVs (on both merge lane and through lane) collaboratively learn a policy to adapt to HDVs to maximize the traffic throughput. We develop an efficient and scalable MARL framework that can be used in dynamic traffic where the communication topology could be time-varying. Parameter sharing and local rewards are exploited to foster inter-agent cooperation while achieving great scalability. An action masking scheme is employed to improve learning efficiency by filtering out invalid/unsafe actions at each step. In addition, a novel priority-based safety supervisor is developed to significantly reduce collision rate and greatly expedite the training process. A gym-like simulation environment is developed and open-sourced with three different levels of traffic densities. We exploit curriculum learning to efficiently learn harder tasks from trained models under simpler settings. Comprehensive experimental results show the proposed MARL framework consistently outperforms several state-of-the-art benchmarks.      
### 6.Situational Coverage and Rate Distribution Maps for 5G V2X Systems Using Ray-Tracing  [ :arrow_down: ](https://arxiv.org/pdf/2105.05689.pdf)
>  Millimeter wave (mmWave) is a practical solution to provide high data rate for vehicle-to-everything (V2X) communications. This enables the future autonomous vehicles to exchange big data with the base stations (BSs) such as the velocity, and the location to enhance the safety for the advanced driving assistance system (ADAS). To achieve this goal, we propose to develop a situational rate map to characterize the distribution of the rates achieved between the BSs and a uniform grid of vehicles. In this context, we consider a mmWave 5G cellular system with two physical structures which are the analog-only beamforming and hybrid precoding with limited feedback to investigate the rate distribution for single and multiuser scenarios. We will use the Ray-Tracing tool to construct the simulation environment and generate the channels between the BSs and the grid of vehicles. Finally, we will study the effects of the carrier frequency, the bandwidth, the BSs deployment, the blockage, the codebook, the physical architectures and the number of served users on the rate distribution maps. Moreover, we will present the rate statistics to evaluate the coverage of served users for certain services requirements and for various road shapes such as corners, intersections and straightways.      
### 7.A CNN-based Prediction-Aware Quality Enhancement Framework for VVC  [ :arrow_down: ](https://arxiv.org/pdf/2105.05658.pdf)
>  This paper presents a framework for Convolutional Neural Network (CNN)-based quality enhancement task, by taking advantage of coding information in the compressed video signal. The motivation is that normative decisions made by the encoder can significantly impact the type and strength of artifacts in the decoded images. In this paper, the main focus has been put on decisions defining the prediction signal in intra and inter frames. This information has been used in the training phase as well as input to help the process of learning artifacts that are specific to each coding type. Furthermore, to retain a low memory requirement for the proposed method, one model is used for all Quantization Parameters (QPs) with a QP-map, which is also shared between luma and chroma components. In addition to the Post Processing (PP) approach, the In-Loop Filtering (ILF) codec integration has also been considered, where the characteristics of the Group of Pictures (GoP) are taken into account to boost the performance. The proposed CNN-based Quality Enhancement(QE) framework has been implemented on top of the VVC Test Model (VTM-10). Experiments show that the prediction-aware aspect of the proposed method improves the coding efficiency gain of the default CNN-based QE method by 1.52%, in terms of BD-BR, at the same network complexity compared to the default CNN-based QE filter.      
### 8.Synergistic Benefits in IRS- and RS-enabled C-RAN with Energy-Efficient Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2105.05619.pdf)
>  The potential of intelligent reflecting surfaces (IRSs) is investigated as a promising technique for enhancing the energy efficiency of wireless networks. Specifically, the IRS enables passive beamsteering by employing many low-cost individually controllable reflect elements. The resulting change of the channel state, however, increases both, signal quality and interference at the users. To counteract this negative side effect, we employ rate splitting (RS), which inherently is able to mitigate the impact of interference. We facilitate practical implementation by considering a Cloud Radio Access Network (C-RAN) at the cost of finite fronthaul-link capacities, which necessitate the allocation of sensible user-centric clusters to ensure energy-efficient transmissions. Dynamic methods for RS and the user clustering are proposed to account for the interdependencies of the individual techniques. Numerical results show that the dynamic RS method establishes synergistic benefits between RS and the IRS. Additionally, the dynamic user clustering and the IRS cooperate synergistically, with a gain of up to 88% when compared to the static scheme. Interestingly, with an increasing fronthaul capacity, the gain of the dynamic user clustering decreases, while the gain of the dynamic RS method increases. Around the resulting intersection, both methods affect the system concurrently, improving the energy efficiency drastically.      
### 9.StutterNet: Stuttering Detection Using Time Delay Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2105.05599.pdf)
>  This paper introduce StutterNet, a novel deep learning based stuttering detection capable of detecting and identifying various types of disfluencies. Most of the existing work in this domain uses automatic speech recognition (ASR) combined with language models for stuttering detection. Compared to the existing work, which depends on the ASR module, our method relies solely on the acoustic signal. We use a time-delay neural network (TDNN) suitable for capturing contextual aspects of the disfluent utterances. We evaluate our system on the UCLASS stuttering dataset consisting of more than 100 speakers. Our method achieves promising results and outperforms the state-of-the-art residual neural network based method. The number of trainable parameters of the proposed method is also substantially less due to the parameter sharing scheme of TDNN.      
### 10.AVA: Adversarial Vignetting Attack against Visual Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2105.05558.pdf)
>  Vignetting is an inherited imaging phenomenon within almost all optical systems, showing as a radial intensity darkening toward the corners of an image. Since it is a common effect for photography and usually appears as a slight intensity variation, people usually regard it as a part of a photo and would not even want to post-process it. Due to this natural advantage, in this work, we study vignetting from a new viewpoint, i.e., adversarial vignetting attack (AVA), which aims to embed intentionally misleading information into vignetting and produce a natural adversarial example without noise patterns. This example can fool the state-of-the-art deep convolutional neural networks (CNNs) but is imperceptible to humans. To this end, we first propose the radial-isotropic adversarial vignetting attack (RI-AVA) based on the physical model of vignetting, where the physical parameters (e.g., illumination factor and focal length) are tuned through the guidance of target CNN models. To achieve higher transferability across different CNNs, we further propose radial-anisotropic adversarial vignetting attack (RA-AVA) by allowing the effective regions of vignetting to be radial-anisotropic and shape-free. Moreover, we propose the geometry-aware level-set optimization method to solve the adversarial vignetting regions and physical parameters jointly. We validate the proposed methods on three popular datasets, i.e., DEV, CIFAR10, and Tiny ImageNet, by attacking four CNNs, e.g., ResNet50, EfficientNet-B0, DenseNet121, and MobileNet-V2, demonstrating the advantages of our methods over baseline methods on both transferability and image quality.      
### 11.Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2105.05537.pdf)
>  In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. Especially, the deep neural networks based on U-shaped architecture and skip-connections have been widely applied in a variety of medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global and long-range semantic information interaction well due to the locality of the convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by 4x, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes and trained models will be publicly available at <a class="link-external link-https" href="https://github.com/HuCaoFighting/Swin-Unet" rel="external noopener nofollow">this https URL</a>.      
### 12.Formal Verification of Control Systems against Hyperproperties via Barrier Certificates  [ :arrow_down: ](https://arxiv.org/pdf/2105.05493.pdf)
>  Hyperproperties are system properties that require quantification over multiple execution traces of a system. Hyperproperties can express several specifications of interest for cyber-physical systems, such as opacity, robustness, and noninterference, which cannot be expressed using linear time properties. This paper presents a discretization-free approach for the formal verification of discrete-time control systems against hyperproperties. The proposed approach involves decomposition of complex hyperproperties into several verification conditions by exploiting the automata-based structure corresponding to the complement of the original specifications. These verification conditions are then discharged by synthesizing the augmented barrier certificates, which provide certain safety guarantees for the underlying system. For systems with polynomial dynamics, we present a sound procedure to synthesize polynomial-type augmented barrier certificates by reducing the problem to sum-of-squares optimizations. We demonstrate the effectiveness of the proposed approach on two physical case-studies concerning two important hyperproperties: initial-state opacity and initial-state robustness.      
### 13.Razumikhin-type Control Lyapunov and Barrier Functions for Time-Delay Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.05450.pdf)
>  This paper studies the stabilization and safety problems of nonlinear time-delay systems, where time delays exist in system state and affect the controller design. Following the Razumikhin approach, we propose a novel control Lyapunov-Razumikhin function to facilitate the controller design and to achieve the stabilization objective. To ensure the safety objective, we propose a Razumikhin-type control barrier function for time-delay systems for the first time. Furthermore, the proposed Razumikhin-type control Lyapunov and barrier functions are merged such that the stabilization and safety control design can be combined to address the stabilization and safety simultaneously, which further extends the control design from the delay-free case into the time-delay case. Finally, the proposed approach is illustrated via a numerical example from mechanic systems.      
### 14.A novel feed rate scheduling method based on Sigmoid function with chord error and kinematics constraints  [ :arrow_down: ](https://arxiv.org/pdf/2105.05434.pdf)
>  In high speed CNC (Compute Numerical Control) machining, the feed rate scheduling has played an important role to ensure machining quality and machining efficiency. In this paper, a novel feed rate scheduling method is proposed for generating smooth feed rate profile conveniently with the consideration of both geometric error and kinematic error. First, a relationship between feed rate value and chord error is applied to determine the feed rate curve. Then, breaking points, which can split whole curve into several blocks, can be found out using proposed two step screening method. For every block, a feed rate profile based on the Sigmoid function is generated. With the consideration of kinematic limitation and machining efficiency, a time-optimal feed rate adjustment algorithm is proposed to further adjust feed rate value at breaking points. After planning feed rate profile for each block, all blocks feed rate profile will be connected smoothly. The resulting feed rate profile is more concise compared with the polynomial profile and more efficient compared with the trigonometric profile. Finally, simulations with two free-form NURBS curves are conducted and comparison with the sine-curve method are carried out to verify the feasibility and applicability of the proposed method.      
### 15.Discrete-time Contraction-based Control of Nonlinear Systems with Parametric Uncertainties using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2105.05432.pdf)
>  Flexible manufacturing in the process industry requires control systems to achieve time-varying setpoints (e.g., product specifications) based on market demand. Contraction theory provides a useful framework for reference-independent system analysis and tracking control for nonlinear systems. However, determination of the control contraction metrics and control laws can be very difficult for general nonlinear systems. This work develops an approach to discrete-time contraction analysis and control using neural networks. The methodology involves training a neural network to learn a contraction metric and feedback gain. The resulting contraction-based controller embeds the trained neural network and is capable of achieving efficient tracking of time-varying references, with a full range of model uncertainty, without the need for controller structure redesign. This is a robust approach that can deal with bounded parametric uncertainties in the process model, which are commonly encountered in industrial (chemical) processes. Simulation examples are provided to illustrate the above approach.      
### 16.On the Fundamental Diagrams of Commercial Adaptive Cruise Control: Worldwide Experimental Evidence  [ :arrow_down: ](https://arxiv.org/pdf/2105.05380.pdf)
>  Recently, several experiments on commercial adaptive cruise control (ACC) vehicles have been conducted worldwide, providing an unprecedented opportunity to study the ACC behaviors. This paper has conducted a comprehensive empirical study on the ACC equilibrium behaviors using all available data to date and obtained the ACC fundamental diagrams. We find that ACC systems display a linear speed-spacing relationship, like human-driven vehicles, but the features of the speed-spacing relationships and the corresponding fundamental diagrams differ from human-driven traffic. ACC generally has very small spacing at minimum headway, resulting in much larger flow (and capacity) than human traffic, but the opposite occurs if maximum headway is used. Many ACC systems have extremely large wave speed, which may impose safety risk. Lastly, ACC jam spacing is much larger than human traffic, translating to much smaller jam density, which will reduce road storage capacity.      
### 17.Computational Simulation and Analysis of Major Control Parameters of Time-Dependent PV/T Collectors  [ :arrow_down: ](https://arxiv.org/pdf/2105.05358.pdf)
>  In order to improve performance of photovoltaic/thermal (or PV/T for simplicity) collectors, this paper firstly validated a previous computational thermal model and then introduced an improved computational thermal model to investigate the effects of the major control parameters on the thermal performance of PV/T collectors, including solar cell temperature, back surface temperature, and outlet water temperature. Besides, a computational electrical model of PV/T system was also introduced to elaborate the relationship of voltage, current and power of a PV module (MSX60 polycrystalline solar cell) used in an experiment in the literature. Simulation results agree with the experimental data very well. The effects of the time-steps from 1 hour to minute, which is closed to the real time, were also reported. At last, several suggestions to improve the efficiency of PV/T system were illustrated.      
### 18.A Unified Power-Setpoint Tracking Algorithm for Utility-Scale PV Systems with Power Reserves and Fast Frequency Response Capabilities  [ :arrow_down: ](https://arxiv.org/pdf/2105.05324.pdf)
>  This paper presents a fast power-setpoint tracking algorithm to enable utility-scale photovoltaic (PV) systems to provide high quality grid services such as power reserves and fast frequency response. The algorithm unites maximum power-point estimation (MPPE) with flexible power-point tracking (FPPT) control to improve the performance of both algorithms, achieving fast and accurate PV power-setpoint tracking even under rapid solar irradiance changes. The MPPE is developed using a real-time, nonlinear curve-fitting approach based on the Levenberg-Marquardt algorithm. A modified adaptive FPPT based on the Perturb and Observe technique is developed for the power-setpoint tracking. By using MPPE to decouple the impact of irradiance changes on the measured PV output power, we develop a fast convergence technique for tracking power-reference changes within three FPPT iterations. Furthermore, to limit the maximum output power ripple, a new design is introduced for the steady-state voltage step size of the adaptive FPPT. The proposed algorithm is implemented on a testbed consisting of a 500 kVA three-phase, single-stage, utility-scale PV system on the OPAL-RT eMEGASIM platform. Results show that the proposed method outperforms the state-of-the-art.      
### 19.GANs for Medical Image Synthesis: An Empirical Study  [ :arrow_down: ](https://arxiv.org/pdf/2105.05318.pdf)
>  Generative Adversarial Networks (GANs) have become increasingly powerful, generating mind-blowing photorealistic images that mimic the content of datasets they were trained to replicate. One recurrent theme in medical imaging is whether GANs can also be effective at generating workable medical data as they are for generating realistic RGB images. In this paper, we perform a multi-GAN and multi-application study to gauge the benefits of GANs in medical imaging. We tested various GAN architectures from basic DCGAN to more sophisticated style-based GANs on three medical imaging modalities and organs namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on well-known and widely utilized datasets from which their FID score were computed to measure the visual acuity of their generated images. We further tested their usefulness by measuring the segmentation accuracy of a U-Net trained on these generated images. <br>Results reveal that GANs are far from being equal as some are ill-suited for medical imaging applications while others are much better off. The top-performing GANs are capable of generating realistic-looking medical images by FID standards that can fool trained experts in a visual Turing test and comply to some metrics. However, segmentation results suggests that no GAN is capable of reproducing the full richness of a medical datasets.      
### 20.Unlimited Sampling from Theory to Practice: Fourier-Prony Recovery and Prototype ADC  [ :arrow_down: ](https://arxiv.org/pdf/2105.05818.pdf)
>  Following the Unlimited Sampling strategy to alleviate the omnipresent dynamic range barrier, we study the problem of recovering a bandlimited signal from point-wise modulo samples, aiming to connect theoretical guarantees with hardware implementation considerations. Our starting point is a class of non-idealities that we observe in prototyping an unlimited sampling based analog-to-digital converter. To address these non-idealities, we provide a new Fourier domain recovery algorithm. Our approach is validated both in theory and via extensive experiments on our prototype analog-to-digital converter, providing the first demonstration of unlimited sampling for data arising from real hardware, both for the current and previous approaches. Advantages of our algorithm include that it is agnostic to the modulo threshold and it can handle arbitrary folding times. We expect that the end-to-end realization studied in this paper will pave the path for exploring the unlimited sampling methodology in a number of real world applications.      
### 21.Symmetric Private Information Retrieval with User-Side Common Randomness  [ :arrow_down: ](https://arxiv.org/pdf/2105.05807.pdf)
>  We consider the problem of symmetric private information retrieval (SPIR) with user-side common randomness. In SPIR, a user retrieves a message out of $K$ messages from $N$ non-colluding and replicated databases in such a way that no single database knows the retrieved message index (user privacy), and the user gets to know nothing further than the retrieved message (database privacy). SPIR has a capacity smaller than the PIR capacity which requires only user privacy, is infeasible in the case of a single database, and requires shared common randomness among the databases. We introduce a new variant of SPIR where the user is provided with a random subset of the shared database common randomness, which is unknown to the databases. We determine the exact capacity region of the triple $(d, \rho_S, \rho_U)$, where $d$ is the download cost, $\rho_S$ is the amount of shared database (server) common randomness, and $\rho_U$ is the amount of available user-side common randomness. We show that with a suitable amount of $\rho_U$, this new SPIR achieves the capacity of conventional PIR. As a corollary, single-database SPIR becomes feasible. Further, the presence of user-side $\rho_U$ reduces the amount of required server-side $\rho_S$.      
### 22.Linking Physical Objects to Their Digital Twins via Fiducial Markers Designed for Invisibility to Humans  [ :arrow_down: ](https://arxiv.org/pdf/2105.05800.pdf)
>  The ability to label and track physical objects that are assets in digital representations of the world is foundational to many complex systems. Simple, yet powerful methods such as bar- and QR-codes have been highly successful, e.g. in the retail space, but the lack of security, limited information content and impossibility of seamless integration with the environment have prevented a large-scale linking of physical objects to their digital twins. This paper proposes to link digital assets created through BIM with their physical counterparts using fiducial markers with patterns defined by Cholesteric Spherical Reflectors (CSRs), selective retroreflectors produced using liquid crystal self-assembly. The markers leverage the ability of CSRs to encode information that is easily detected and read with computer vision while remaining practically invisible to the human eye. We analyze the potential of a CSR-based infrastructure from the perspective of BIM, critically reviewing the outstanding challenges in applying this new class of functional materials, and we discuss extended opportunities arising in assisting autonomous mobile robots to reliably navigate human-populated environments, as well as in augmented reality.      
### 23.Global Structure-Aware Drum Transcription Based on Self-Attention Mechanisms  [ :arrow_down: ](https://arxiv.org/pdf/2105.05791.pdf)
>  This paper describes an automatic drum transcription (ADT) method that directly estimates a tatum-level drum score from a music signal, in contrast to most conventional ADT methods that estimate the frame-level onset probabilities of drums. To estimate a tatum-level score, we propose a deep transcription model that consists of a frame-level encoder for extracting the latent features from a music signal and a tatum-level decoder for estimating a drum score from the latent features pooled at the tatum level. To capture the global repetitive structure of drum scores, which is difficult to learn with a recurrent neural network (RNN), we introduce a self-attention mechanism with tatum-synchronous positional encoding into the decoder. To mitigate the difficulty of training the self-attention-based model from an insufficient amount of paired data and improve the musical naturalness of the estimated scores, we propose a regularized training method that uses a global structure-aware masked language (score) model with a self-attention mechanism pretrained from an extensive collection of drum scores. Experimental results showed that the proposed regularized model outperformed the conventional RNN-based model in terms of the tatum-level error rate and the frame-level F-measure, even when only a limited amount of paired data was available so that the non-regularized model underperformed the RNN-based model.      
### 24.Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders  [ :arrow_down: ](https://arxiv.org/pdf/2105.05752.pdf)
>  Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De show that our method achieves the state-of-the-art performance of 18.3 and 25.2 BLEU points. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.      
### 25.Joint Routing and Charging Problem of Multiple Electric Vehicles: A Fast Optimization Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2105.05711.pdf)
>  Logistics has gained great attentions with the prosperous development of commerce, which is often seen as the classic optimal vehicle routing problem. Meanwhile, electric vehicle (EV) has been widely used in logistic fleet to curb the emission of green house gases in recent years. Solving the optimization problem of joint routing and charging of multiple EVs is in a urgent need, whose objective function includes charging time, charging cost, EVs travel time, usage fees of EV and revenue from serving customers. This joint problem is formulated as a mixed integer programming (MIP) problem, which, however, is NP-hard due to integer restrictions and bilinear terms from the coupling between routing and charging decisions. The main contribution of this paper lies at proposing an efficient two stage algorithm that can decompose the original MIP problem into two linear programming (LP) problems, by exploiting the exactness of LP relaxation and eliminating the coupled term. This algorithm can achieve a nearoptimal solution in polynomial time. In addition, another variant algorithm is proposed based on the two stage one, to further improve the quality of solution.      
### 26.Bregman algorithms for a class of Mixed-Integer Generalized Nash Equilibrium Problems  [ :arrow_down: ](https://arxiv.org/pdf/2105.05687.pdf)
>  We consider the problem of computing a mixed-strategy generalized Nash equilibrium (MS-GNE) for a class of games where each agent has both continuous and integer decision variables. Specifically, we propose a novel Bregman forward-reflected-backward splitting and design distributed algorithms that exploit the problem structure. Technically, we prove convergence to a variational MS-GNE under monotonicity and Lipschitz continuity assumptions, which are typical of continuous GNE problems. Finally, we show the performance of our algorithms via numerical experiments.      
### 27.Cross-Modal and Multimodal Data Analysis Based on Functional Mapping of Spectral Descriptors and Manifold Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2105.05631.pdf)
>  Multimodal manifold modeling methods extend the spectral geometry-aware data analysis to learning from several related and complementary modalities. Most of these methods work based on two major assumptions: 1) there are the same number of homogeneous data samples in each modality, and 2) at least partial correspondences between modalities are given in advance as prior knowledge. This work proposes two new multimodal modeling methods. The first method establishes a general analyzing framework to deal with the multimodal information problem for heterogeneous data without any specific prior knowledge. For this purpose, first, we identify the localities of each manifold by extracting local descriptors via spectral graph wavelet signatures (SGWS). Then, we propose a manifold regularization framework based on the functional mapping between SGWS descriptors (FMBSD) for finding the pointwise correspondences. The second method is a manifold regularized multimodal classification based on pointwise correspondences (M$^2$CPC) used for the problem of multiclass classification of multimodal heterogeneous, which the correspondences between modalities are determined based on the FMBSD method. The experimental results of evaluating the FMBSD method on three common cross-modal retrieval datasets and evaluating the (M$^2$CPC) method on three benchmark multimodal multiclass classification datasets indicate their effectiveness and superiority over state-of-the-art methods.      
### 28.Global Optimization for IRS-Assisted Wireless Communications: from Physics and Electromagnetic Perspectives  [ :arrow_down: ](https://arxiv.org/pdf/2105.05618.pdf)
>  Intelligent reflecting surfaces (IRSs) are envisioned to be a disruptive wireless communication technique that is capable of reconfiguring the wireless propagation environment. In this paper, we study a far-field IRS-assisted multiple-input single-output (MISO) communication system operating in free space. To maximize the received power of the receiver from the physics and electromagnetic nature point of view, an optimization, including beamforming of the transmitter, phase shifts of the IRS, orientation and position of the IRS is formulated and solved. After exploiting the property of line-of-sight (LoS), we derive closed-form solutions of beamforming and phase shifts. For the non-trivial IRS position optimization problem in arbitrary three-dimensional space, a dimensional-reducing theory is proved, which is useful to reduce the complexity of search method. The simulation results show that the proposed closed-form beamforming and phase shifts are near-optimal solutions. Besides, the IRS significantly enhances the performance of the communication system when it is deployed at the optimal position.      
### 29.Capacity Bounds and User Identification Costs in Rayleigh-Fading Many-Access Channel  [ :arrow_down: ](https://arxiv.org/pdf/2105.05603.pdf)
>  Many-access channel (MnAC) model allows the number of users in the system and the number of active users to scale as a function of the blocklength and as such is suited for dynamic communication systems with massive number of users such as the Internet of Things. Existing MnAC models assume a priori knowledge of channel gains which is impractical since acquiring Channel State Information (CSI) for massive number of users can overwhelm the available radio resources. This paper incorporates Rayleigh fading effects to the MnAC model and derives an upper bound on the symmetric message-length capacity of the Rayleigh-fading Gaussian MnAC. Furthermore, a lower bound on the minimum number of channel uses for discovering the active users is established. In addition, the performance of Noisy Combinatorial Orthogonal Matching Pursuit (N-COMP) based group testing (GT) is studied as a practical strategy for active device discovery. Simulations show that, for a given SNR, as the number of users increase, the required number of channel uses for N-COMP GT scales approximately the same way as the lower bound on minimum user identification cost. Moreover, in the low SNR regime, for sufficiently large population sizes, the number of channel uses required by N-COMP GT was observed to be within a factor of two of the lower bound when the expected number of active users scales sub-linearly with the total population size.      
### 30.A Resilient and Energy-Aware Task Allocation Framework for Heterogeneous Multi-Robot Systems  [ :arrow_down: ](https://arxiv.org/pdf/2105.05586.pdf)
>  In the context of heterogeneous multi-robot teams deployed for executing multiple tasks, this paper develops an energy-aware framework for allocating tasks to robots in an online fashion. With a primary focus on long-duration autonomy applications, we opt for a survivability-focused approach. Towards this end, the task prioritization and execution -- through which the allocation of tasks to robots is effectively realized -- are encoded as constraints within an optimization problem aimed at minimizing the energy consumed by the robots at each point in time. In this context, an allocation is interpreted as a prioritization of a task over all others by each of the robots. Furthermore, we present a novel framework to represent the heterogeneous capabilities of the robots, by distinguishing between the features available on the robots, and the capabilities enabled by these features. By embedding these descriptions within the optimization problem, we make the framework resilient to situations where environmental conditions make certain features unsuitable to support a capability and when component failures on the robots occur. We demonstrate the efficacy and resilience of the proposed approach in a variety of use-case scenarios, consisting of simulations and real robot experiments.      
### 31.Discrete representations in neural models of spoken language  [ :arrow_down: ](https://arxiv.org/pdf/2105.05582.pdf)
>  The distributed and continuous representations used by neural networks are at odds with representations employed in linguistics, which are typically symbolic. Vector quantization has been proposed as a way to induce discrete neural representations that are closer in nature to their linguistic counterparts. However, it is not clear which metrics are the best-suited to analyze such discrete representations. We compare the merits of four commonly used metrics in the context of weakly supervised models of spoken language. We perform a systematic analysis of the impact of (i) architectural choices, (ii) the learning objective and training dataset, and (iii) the evaluation metric. We find that the different evaluation metrics can give inconsistent results. In particular, we find that the use of minimal pairs of phoneme triples as stimuli during evaluation disadvantages larger embeddings, unlike metrics applied to complete utterances.      
### 32.A Novel Uncertainty-aware Collaborative Learning Method for Remote Sensing Image Classification Under Multi-Label Noise  [ :arrow_down: ](https://arxiv.org/pdf/2105.05496.pdf)
>  In remote sensing (RS), collecting a large number of reliable training images annotated by multiple land-cover class labels for multi-label classification (MLC) is time-consuming and costly. To address this problem, the publicly available thematic products are often used for annotating RS images with zero-labeling cost. However, in this case the training set can include noisy multi-labels that distort the learning process, resulting in inaccurate predictions. This paper proposes an architect-independent Consensual Collaborative Multi-Label Learning (CCML) method to train deep classifiers under input-dependent (heteroscedastic) multi-label noise in the MLC problems. The proposed CCML identifies, ranks, and corrects noisy multi-label images through four main modules: 1) group lasso module; 2) discrepancy module; 3) flipping module; and 4) swap module. The group lasso module detects the potentially noisy labels by estimating the label uncertainty based on the aggregation of two collaborative networks. The discrepancy module ensures that the two networks learn diverse features, while obtaining the same predictions. The flipping module corrects the identified noisy labels, and the swap module exchanges the ranking information between the two networks. The experiments conducted on the multi-label RS image archive IR-BigEarthNet confirm the robustness of the proposed CCML under extreme multi-label noise rates.      
### 33.Learning Graphs from Smooth Signals under Moment Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2105.05458.pdf)
>  We consider the problem of inferring the graph structure from a given set of smooth graph signals. The number of perceived graph signals is always finite and possibly noisy, thus the statistical properties of the data distribution is ambiguous. Traditional graph learning models do not take this distributional uncertainty into account, thus performance may be sensitive to different sets of data. In this paper, we propose a distributionally robust approach to graph learning, which incorporates the first and second moment uncertainty into the smooth graph learning model. Specifically, we cast our graph learning model as a minimax optimization problem, and further reformulate it as a nonconvex minimization problem with linear constraints. In our proposed formulation, we find a theoretical interpretation of the Laplacian regularizer, which is adopted in many existing works in an intuitive manner. Although the first moment uncertainty leads to an annoying square root term in the objective function, we prove that it enjoys the smoothness property with probability 1 over the entire constraint. We develop a efficient projected gradient descent (PGD) method and establish its global iterate convergence to a critical point. We conduct extensive experiments on both synthetic and real data to verify the effectiveness of our model and the efficiency of the PGD algorithm. Compared with the state-of-the-art smooth graph learning methods, our approach exhibits superior and more robust performance across different populations of signals in terms of various evaluation metrics.      
### 34.Indoor positioning systems: Smart fusion of a variety of sensor readings  [ :arrow_down: ](https://arxiv.org/pdf/2105.05438.pdf)
>  Robust and versatile localization techniques are key to the success of the next industrial revolution. Yet, it is uncertain which combination of sensors will be the most robust and valuable. Thus, we present a versatile and reproducible measurement system incorporating a manifold number of state-of-the art sensors to compare and fuse the raw input data. It is shown that some techniques show very good results on the same scenario and data-set, but fall apart on translating to a slightly different scenario. In general we show that the vanilla approach to fuse the raw data achieves reasonable results in the generalization domain, demonstrating that radiofrequency (RF) localization techniques in combination with an inertial measurement unit (IMU) could result in a very robust and promising candidate for solving this challenging task.      
### 35.On Parameter Optimization and Reach Enhancement for the Improved Soft-Aided Staircase Decoder  [ :arrow_down: ](https://arxiv.org/pdf/2105.05419.pdf)
>  The so-called improved soft-aided bit-marking algorithm was recently proposed for staircase codes (SCCs) in the context of fiber optical communications. This algorithm is known as iSABM-SCC. With the help of channel soft information, the iSABM-SCC decoder marks bits via thresholds to deal with both miscorrections and failures of hard-decision (HD) decoding. In this paper, we study iSABM-SCC focusing on the parameter optimization of the algorithm and its performance analysis, in terms of the gap to the achievable information rates (AIRs) of HD codes and the fiber reach enhancement. We show in this paper that the marking thresholds and the number of modified component decodings heavily affect the performance of iSABM-SCC, and thus, they need to be carefully optimized. By replacing standard decoding with the optimized iSABM-SCC decoding, the gap to the AIRs of HD codes can be reduced to 0.26-1.02 dB for code rates of 0.74-0.87 in the additive white Gaussian noise channel with 8-ary pulse amplitude modulation. The obtained reach increase is up to 22% for data rates between 401 Gbps and 468 Gbps in an optical fiber channel.      
### 36.A Statistical Model for Melody Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2105.05385.pdf)
>  A commonly-cited reason for the poor performance of automatic chord estimation (ACE) systems within music information retrieval (MIR) is that non-chord tones (i.e., notes outside the supporting harmony) contribute to error during the labeling process. Despite the prevalence of machine learning approaches in MIR, there are cases where alternative approaches provide a simpler alternative while allowing for insights into musicological practices. In this project, we present a statistical model for predicting chord tones based on music theory rules. Our model is currently focused on predicting chord tones in classical music, since composition in this style is highly constrained, theoretically making the placement of chord tones highly predictable. Indeed, music theorists have labeling systems for every variety of non-chord tone, primarily classified by the note's metric position and intervals of approach and departure. Using metric position, duration, and melodic intervals as predictors, we build a statistical model for predicting chord tones using the TAVERN dataset. While our probabilistic approach is similar to other efforts in the domain of automatic harmonic analysis, our focus is on melodic reduction rather than predicting harmony. However, we hope to pursue applications for ACE in the future. Finally, we implement our melody reduction model using an existing symbolic visualization tool, to assist with melody reduction and non-chord tone identification for computational musicology researchers and music theorists.      
### 37.On Compressed Sensing of Binary Signals for the Unsourced Random Access Channel  [ :arrow_down: ](https://arxiv.org/pdf/2105.05350.pdf)
>  Motivated by applications in unsourced random access, this paper develops a novel scheme for the problem of compressed sensing of binary signals. In this problem, the goal is to design a sensing matrix $A$ and a recovery algorithm, such that the sparse binary vector $\mathbf{x}$ can be recovered reliably from the measurements $\mathbf{y}=A\mathbf{x}+\sigma\mathbf{z}$, where $\mathbf{z}$ is additive white Gaussian noise. We propose to design $A$ as a parity check matrix of a low-density parity-check code (LDPC), and to recover $\mathbf{x}$ from the measurements $\mathbf{y}$ using a Markov chain Monte Carlo algorithm, which runs relatively fast due to the sparse structure of $A$. The performance of our scheme is comparable to state-of-the-art schemes, which use dense sensing matrices, while enjoying the advantages of using a sparse sensing matrix.      
### 38.Efficient Solution Strategy for Chance-Constrained Optimal Power Flow based on FAST and Data-driven Convexification  [ :arrow_down: ](https://arxiv.org/pdf/2105.05336.pdf)
>  The uncertainty of multiple power loads and re-newable energy generations in power systems increases the complexity of power flow analysis for decision-makers. The chance-constraint method can be applied to model the optimi-zation problems of power flow with uncertainty. This paper develops a novel solution approach for chance-constrained AC optimal power flow (CCACOPF) problem based on the da-ta-driven convexification of power flow and the fast algorithm for scenario technique (FAST). This method is computationally effective for mainly two reasons. First, the original nonconvex AC power flow constraints are approximated by a set of learn-ing-based quadratic convex ones. Second, FAST is a more ad-vanced distribution-free scenario-based solution method using far less scenarios than the conventional one, retaining a high confidence level. Eventually, the CCACOPF is converted into a computationally tractable convex optimization problem. The simulation results on IEEE test cases indicate that 1) the pro-posed solution method can excel the conventional one and ro-bust program in computational efficiency, 2) the data-driven convexification of power flow is effective in approximating original complex AC power flow.      
### 39.Sequential Fair Allocation: Achieving the Optimal Envy-Efficiency Tradeoff Curve  [ :arrow_down: ](https://arxiv.org/pdf/2105.05308.pdf)
>  We consider the problem of dividing limited resources to individuals arriving over $T$ rounds. Each round has a random number of individuals arrive, and individuals can be characterized by their type (i.e. preferences over the different resources). A standard notion of `fairness' in this setting is that an allocation simultaneously satisfy envy-freeness and efficiency. The former is an individual guarantee, requiring that each agent prefers her own allocation over the allocation of any other; in contrast, efficiency is a global property, requiring that the allocations clear the available resources. For divisible resources, when the number of individuals of each type are known upfront, the above desiderata are simultaneously achievable for a large class of utility functions. However, in an online setting when the number of individuals of each type are only revealed round by round, no policy can guarantee these desiderata simultaneously, and hence the best one can do is to try and allocate so as to approximately satisfy the two properties. <br>We show that in the online setting, the two desired properties (envy-freeness and efficiency) are in direct contention, in that any algorithm achieving additive envy-freeness up to a factor of $L_T$ necessarily suffers an efficiency loss of at least $1 / L_T$. We complement this uncertainty principle with a simple algorithm, HopeGuardrail, which allocates resources based on an adaptive threshold policy. We show that our algorithm is able to achieve any fairness-efficiency point on this frontier, and moreover, in simulation results, provides allocations close to the optimal fair solution in hindsight. This motivates its use in practical applications, as the algorithm is able to adapt to any desired fairness efficiency trade-off.      
### 40.Distribution of the Scaled Condition Number of Single-spiked Complex Wishart Matrices  [ :arrow_down: ](https://arxiv.org/pdf/2105.05307.pdf)
>  Let $\mathbf{X}\in\mathbb{C}^{m\times n}$ ($m\geq n$) be a random matrix with independent rows each distributed as complex multivariate Gaussian with zero mean and {\it single-spiked} covariance matrix $\mathbf{I}_n+ \eta \mathbf{u}\mathbf{u}^*$, where $\mathbf{I}_n$ is the $n\times n$ identity matrix, $\mathbf{u}\in\mathbb{C}^{n\times n}$ is an arbitrary vector with a unit Euclidean norm, $\eta\geq 0$ is a non-random parameter, and $(\cdot)^*$ represents conjugate-transpose. This paper investigates the distribution of the random quantity $\kappa_{\text{SC}}^2(\mathbf{X})=\sum_{k=1}^n \lambda_k/\lambda_1$, where $0&lt;\lambda_1&lt;\lambda_2&lt;\ldots&lt;\lambda_n&lt;\infty$ are the ordered eigenvalues of $\mathbf{X}^*\mathbf{X}$ (i.e., single-spiked Wishart matrix). This random quantity is intimately related to the so called {\it scaled condition number} or the Demmel condition number (i.e., $\kappa_{\text{SC}}(\mathbf{X})$) and the minimum eigenvalue of the fixed trace Wishart-Laguerre ensemble (i.e., $\kappa_{\text{SC}}^{-2}(\mathbf{X})$). In particular, we use an orthogonal polynomial approach to derive an exact expression for the probability density function of $\kappa_{\text{SC}}^2(\mathbf{X})$ which is amenable to asymptotic analysis as matrix dimensions grow large. Our asymptotic results reveal that, as $m,n\to\infty$ such that $m-n$ is fixed and when $\eta$ scales on the order of $1/n$, $\kappa_{\text{SC}}^2(\mathbf{X})$ scales on the order of $n^3$. In this respect we establish simple closed-form expressions for the limiting distributions.      
