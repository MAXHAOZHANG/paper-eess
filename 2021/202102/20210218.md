# ArXiv eess --Thu, 18 Feb 2021
### 1.A Mutual Reference Shape for Segmentation Fusion and Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2102.08939.pdf)
>  This paper proposes the estimation of a mutual shape from a set of different segmentation results using both active contours and information theory. The mutual shape is here defined as a consensus shape estimated from a set of different segmentations of the same object. In an original manner, such a shape is defined as the minimum of a criterion that benefits from both the mutual information and the joint entropy of the input segmentations. This energy criterion is justified using similarities between information theory quantities and area measures, and presented in a continuous variational framework. In order to solve this shape optimization problem, shape derivatives are computed for each term of the criterion and interpreted as an evolution equation of an active contour. A mutual shape is then estimated together with the sensitivity and specificity of each segmentation. Some synthetic examples allow us to cast the light on the difference between the mutual shape and an average shape. The applicability of our framework has also been tested for segmentation evaluation and fusion of different types of real images (natural color images, old manuscripts, medical images).      
### 2.Pathloss modeling of reconfigurable intelligent surface assisted THz wireless systems  [ :arrow_down: ](https://arxiv.org/pdf/2102.08757.pdf)
>  This paper presents an analytical pathloss model for reconfigurable intelligent surface (RIS) assisted terahertz (THz) wireless systems. Specifically, the model accommodates both the THz link and the RIS particularities. Finally, we derive a closed-form expression that returns the optimal phase shifting of each RIS reflection unit. The derived pathloss model is validated through extensive electromagnetic simulations and is expected to play a key role in the design of RIS-assisted THz wireless systems.      
### 3.A Sketching Framework for Reduced Data Transfer in Photon Counting Lidar  [ :arrow_down: ](https://arxiv.org/pdf/2102.08732.pdf)
>  Single-photon lidar has become a prominent tool for depth imaging in recent years. At the core of the technique, the depth of a target is measured by constructing a histogram of time delays between emitted light pulses and detected photon arrivals. A major data processing bottleneck arises on the device when either the number of photons per pixel is large or the resolution of the time stamp is fine, as both the space requirement and the complexity of the image reconstruction algorithms scale with these parameters. We solve this limiting bottleneck of existing lidar techniques by sampling the characteristic function of the time of flight (ToF) model to build a compressive statistic, a so-called sketch of the time delay distribution, which is sufficient to infer the spatial distance and intensity of the object. The size of the sketch scales with the degrees of freedom of the ToF model (number of objects) and not, fundamentally, with the number of photons or the time stamp resolution. Moreover, the sketch is highly amenable for on-chip online processing. We show theoretically that the loss of information for compression is controlled and the mean squared error of the inference quickly converges towards the optimal CramÃ©r-Rao bound (i.e. no loss of information) for modest sketch sizes. The proposed compressed single-photon lidar framework is tested and evaluated on real life datasets of complex scenes where it is shown that a compression rate of up-to 1/150 is achievable in practice without sacrificing the overall resolution of the reconstructed image.      
### 4.A Dataset and Benchmark for Malaria Life-Cycle Classification in Thin Blood Smear Images  [ :arrow_down: ](https://arxiv.org/pdf/2102.08708.pdf)
>  Malaria microscopy, microscopic examination of stained blood slides to detect parasite Plasmodium, is considered to be a gold-standard for detecting life-threatening disease malaria. Detecting the plasmodium parasite requires a skilled examiner and may take up to 10 to 15 minutes to completely go through the whole slide. Due to a lack of skilled medical professionals in the underdeveloped or resource deficient regions, many cases go misdiagnosed; resulting in unavoidable complications and/or undue medication. We propose to complement the medical professionals by creating a deep learning-based method to automatically detect (localize) the plasmodium parasites in the photograph of stained film. To handle the unbalanced nature of the dataset, we adopt a two-stage approach. Where the first stage is trained to detect blood cells and classify them into just healthy or infected. The second stage is trained to classify each detected cell further into the life-cycle stage. To facilitate the research in machine learning-based malaria microscopy, we introduce a new large scale microscopic image malaria dataset. Thirty-eight thousand cells are tagged from the 345 microscopic images of different Giemsa-stained slides of blood samples. Extensive experimentation is performed using different CNN backbones including VGG, DenseNet, and ResNet on this dataset. Our experiments and analysis reveal that the two-stage approach works better than the one-stage approach for malaria detection. To ensure the usability of our approach, we have also developed a mobile app that will be used by local hospitals for investigation and educational purposes. The dataset, its annotations, and implementation codes will be released upon publication of the paper.      
### 5.Variational Autoencoder for Speech Enhancement with a Noise-Aware Encoder  [ :arrow_down: ](https://arxiv.org/pdf/2102.08706.pdf)
>  Recently, a generative variational autoencoder (VAE) has been proposed for speech enhancement to model speech statistics. However, this approach only uses clean speech in the training phase, making the estimation particularly sensitive to noise presence, especially in low signal-to-noise ratios (SNRs). To increase the robustness of the VAE, we propose to include noise information in the training phase by using a noise-aware encoder trained on noisy-clean speech pairs. We evaluate our approach on real recordings of different noisy environments and acoustic conditions using two different noise datasets. We show that our proposed noise-aware VAE outperforms the standard VAE in terms of overall distortion without increasing the number of model parameters. At the same time, we demonstrate that our model is capable of generalizing to unseen noise conditions better than a supervised feedforward deep neural network (DNN). Furthermore, we demonstrate the robustness of the model performance to a reduction of the noisy-clean speech training data size.      
### 6.Performance Dependency of LSTM and NAR Beamformers With Respect to Sensor Array Properties in V2I Scenario  [ :arrow_down: ](https://arxiv.org/pdf/2102.08680.pdf)
>  Prediction and nullifying the interference is a challenging problem in vehicle to infrastructure scenarios . The implementation of practical V2I network is limited because of inevitability of interference due to random nature of the wireless channel. The interference introduces angle ambiguity between the road side units mounted base station and user equipment. This paper proposes an adaptive beamforming technique for mitigation of interference in V2I networks, especially in multiuser environment. In this work , Long short term based (LSTM) based deep learning and non linear auto regresive technique based regressor have been employed to predict the angles between the road side units and user equipment .Advance prediction of transmit and receive signals enables reliable vehicle to infrastructure communication. Instead of predicting the beamforming matrix directly, we predict the main features using LSTM for learning dependencies in the input time series ,where complex variables were taken as input states and final beamformed signal was the output. simulation results have confirmed that the proposed LSTM model achieves comparable performance in terms of system throughput when compared with the non linear auto regressive method implemented as an artificial neural network.      
### 7.CheXternal: Generalization of Deep Learning Models for Chest X-ray Interpretation to Photos of Chest X-rays and External Clinical Settings  [ :arrow_down: ](https://arxiv.org/pdf/2102.08660.pdf)
>  Recent advances in training deep learning models have demonstrated the potential to provide accurate chest X-ray interpretation and increase access to radiology expertise. However, poor generalization due to data distribution shifts in clinical settings is a key barrier to implementation. In this study, we measured the diagnostic performance for 8 different chest X-ray models when applied to (1) smartphone photos of chest X-rays and (2) external datasets without any finetuning. All models were developed by different groups and submitted to the CheXpert challenge, and re-applied to test datasets without further tuning. We found that (1) on photos of chest X-rays, all 8 models experienced a statistically significant drop in task performance, but only 3 performed significantly worse than radiologists on average, and (2) on the external set, none of the models performed statistically significantly worse than radiologists, and five models performed statistically significantly better than radiologists. Our results demonstrate that some chest X-ray models, under clinically relevant distribution shifts, were comparable to radiologists while other models were not. Future work should investigate aspects of model training procedures and dataset collection that influence generalization in the presence of data distribution shifts.      
### 8.A Polynomial Chaos Approach to Robust $\mathcal{H}_\infty$ Static Output-Feedback Control with Bounded Truncation Error  [ :arrow_down: ](https://arxiv.org/pdf/2102.08626.pdf)
>  This article considers the $\mathcal{H}_\infty$ static output-feedback control for linear time-invariant uncertain systems with polynomial dependence on probabilistic time-invariant parametric uncertainties. By applying polynomial chaos theory, the control synthesis problem is solved using a high-dimensional expanded system which characterizes stochastic state uncertainty propagation. A closed-loop polynomial chaos transformation is proposed to derive the closed-loop expanded system. The approach explicitly accounts for the closed-loop dynamics and preserves the $\mathcal{L}_2$-induced gain, which results in smaller transformation errors compared to existing polynomial chaos transformations. The effect of using finite-degree polynomial chaos expansions is first captured by a norm-bounded linear differential inclusion, and then addressed by formulating a robust polynomial chaos based control synthesis problem. This proposed approach avoids the use of high-degree polynomial chaos expansions to alleviate the destabilizing effect of truncation errors, which significantly reduces computational complexity. In addition, some analysis is given for the condition under which the robustly stabilized expanded system implies the robust stability of the original system. A numerical example illustrates the effectiveness of the proposed approach.      
### 9.Supporting More Active Users for Massive Access via Data-assisted Activity Detection  [ :arrow_down: ](https://arxiv.org/pdf/2102.08621.pdf)
>  Massive machine-type communication (mMTC) has been regarded as one of the most important use scenarios in the fifth generation (5G) and beyond wireless networks, which demands scalable access for a large number of devices. While grant-free random access has emerged as a promising mechanism for massive access, its potential has not been fully unleashed. Particularly, the two key tasks in massive access systems, namely, user activity detection and data detection, were handled separately in most existing studies, which ignored the common sparsity pattern in the received pilot and data signal. Moreover, error detection and correction in the payload data provide additional mechanisms for performance improvement. In this paper, we propose a data-assisted activity detection framework, which aims at supporting more active users by reducing the activity detection error, consisting of false alarm and missed detection errors. Specifically, after an initial activity detection step based on the pilot symbols, the false alarm users are filtered by applying energy detection for the data symbols; once data symbols of some active users have been successfully decoded, their effect in activity detection will be resolved via successive pilot interference cancellation, which reduces the missed detection error. Simulation results show that the proposed algorithm effectively increases the activity detection accuracy, and it is able to support $\sim 20\%$ more active users compared to a conventional method in some sample scenarios.      
### 10.Smoothed Least-Laxity-First Algorithm for EV Charging  [ :arrow_down: ](https://arxiv.org/pdf/2102.08610.pdf)
>  Adaptive charging can charge electric vehicles (EVs) at scale cost effectively, despite the uncertainty in EV arrivals. We formulate adaptive EV charging as a feasibility problem that meets all EVs' energy demands before their deadlines while satisfying constraints in charging rate and total charging power. We propose an online algorithm, smoothed least-laxity-first (sLLF), that decides the current charging rates without the knowledge of future arrivals and demands. We characterize the performance of the sLLF algorithm analytically and numerically. Numerical experiments with real-world data show that it has a significantly higher rate of feasible EV charging than several other existing EV charging algorithms. Resource augmentation framework is employed to assess the feasibility condition of the algorithm. The assessment shows that the sLLF algorithm achieves perfect feasibility with only a 0.07 increase in resources.      
### 11.Enhanced Modeling of Contingency Response in Security-constrained Optimal Power Flow  [ :arrow_down: ](https://arxiv.org/pdf/2102.08579.pdf)
>  This paper provides an enhanced modeling of the contingency response that collectively reflects high-fidelity physical and operational characteristics of power grids. Integrating active and reactive power contingency responses into the security-constrained optimal power flow (SCOPF) problem is challenging, due to the nonsmoothness and nonconvexity of feasible sets in consequence of piece-wise curves representing generator characteristics. We introduce a continuously-differentiable model using functions that closely resemble PV/PQ switching and the generator contingency response. These models enforce physical and operational limits by optimally allocating active power imbalances among available generators and deciding the bus type to switch from the PV type to the PQ type. The efficacy of this method is numerically validated on the IEEE 30-bus, 300-bus, and 118-bus systems with 12, 10, and 100 contingencies, respectively.      
### 12.Self-Triggered Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2102.08571.pdf)
>  In this paper, we study Markov Decision Processes (MDPs) with self-triggered strategies, where the idea of self-triggered control is extended to more generic MDP models. This extension broadens the application of self-triggering policies to a broader range of systems. We study the co-design problems of the control policy and the triggering policy to optimize two pre-specified cost criteria. The first cost criterion is introduced by incorporating a pre-specified update penalty into the traditional MDP cost criteria to reduce the use of communication resources. Under this criteria, a novel dynamic programming (DP) equation called DP equation with optimized lookahead to proposed to solve for the self-triggering policy under this criteria. The second self-triggering policy is to maximize the triggering time while still guaranteeing a pre-specified level of sub-optimality. Theoretical underpinnings are established for the computation and implementation of both policies. Through a gridworld numerical example, we illustrate the two policies' effectiveness in reducing sources consumption and demonstrate the trade-offs between resource consumption and system performance.      
### 13.Deep cross-modality (MR-CT) educed distillation learning for cone beam CT lung tumor segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2102.08556.pdf)
>  Despite the widespread availability of in-treatment room cone beam computed tomography (CBCT) imaging, due to the lack of reliable segmentation methods, CBCT is only used for gross set up corrections in lung radiotherapies. Accurate and reliable auto-segmentation tools could potentiate volumetric response assessment and geometry-guided adaptive radiation therapies. Therefore, we developed a new deep learning CBCT lung tumor segmentation method. Methods: The key idea of our approach called cross modality educed distillation (CMEDL) is to use magnetic resonance imaging (MRI) to guide a CBCT segmentation network training to extract more informative features during training. We accomplish this by training an end-to-end network comprised of unpaired domain adaptation (UDA) and cross-domain segmentation distillation networks (SDN) using unpaired CBCT and MRI datasets. Feature distillation regularizes the student network to extract CBCT features that match the statistical distribution of MRI features extracted by the teacher network and obtain better differentiation of tumor from background.} We also compared against an alternative framework that used UDA with MR segmentation network, whereby segmentation was done on the synthesized pseudo MRI representation. All networks were trained with 216 weekly CBCTs and 82 T2-weighted turbo spin echo MRI acquired from different patient cohorts. Validation was done on 20 weekly CBCTs from patients not used in training. Independent testing was done on 38 weekly CBCTs from patients not used in training or validation. Segmentation accuracy was measured using surface Dice similarity coefficient (SDSC) and Hausdroff distance at 95th percentile (HD95) metrics.      
### 14.A Novel Bayesian Approach for the Two-Dimensional Harmonic Retrieval Problem  [ :arrow_down: ](https://arxiv.org/pdf/2102.08515.pdf)
>  Sparse signal recovery algorithms like sparse Bayesian learning work well but the complexity quickly grows when tackling higher dimensional parametric dictionaries. In this work we propose a novel Bayesian strategy to address the two dimensional harmonic retrieval problem, through remodeling and reparameterization of the standard data model. This new model allows us to introduce a block sparsity structure in a manner that enables a natural pairing of the parameters in the two dimensions. The numerical simulations demonstrate that the inference algorithm developed (H-MSBL) does not suffer from source identifiability issues and is capable of estimating the harmonic components in challenging scenarios, while maintaining a low computational complexity.      
### 15.Learning deep multiresolution representations for pansharpening  [ :arrow_down: ](https://arxiv.org/pdf/2102.08423.pdf)
>  Retaining spatial characteristics of panchromatic image and spectral information of multispectral bands is a critical issue in pansharpening. This paper proposes a pyramid based deep fusion framework that preserves spectral and spatial characteristics at different scales. The spectral information is preserved by passing the corresponding low resolution multispectral image as residual component of the network at each scale. The spatial information is preserved by training the network at each scale with the high frequencies of panchromatic image alongside the corresponding low resolution multispectral image. The parameters of different networks are shared across the pyramid in order to add spatial details consistently across scales. The parameters are also shared across fusion layers within a network at a specific scale. Experiments suggest that the proposed architecture outperforms state of the art pansharpening models. The proposed model, code and dataset is publicly available at <a class="link-external link-https" href="https://github.com/sohaibali01/deep_pyramid_fusion" rel="external noopener nofollow">this https URL</a>.      
### 16.Toward Efficient Wide-Area Identification of Multiple Element Contingencies in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2102.08415.pdf)
>  Power system N-x contingency analysis has inherent challenges due to its combinatorial characteristic where outages grow exponentially with the increase of x and N. To address these challenges, this paper proposes a method that utilizes Line Outage Distribution Factors (LODFs) and group betweenness centrality to identify subsets of critical branches. The proposed LODF metrics are used to select the high-impact branches. Based on each selected branch, the approach constructs the subgraph with parameters of distance and search level, while using branches' LODF metrics as the weights. A key innovation of this work is the use of the distance and search level parameters, which allow the subgraph to identify the most coupled critical elements that may be far away from a selected branch. The proposed approach is validated using the 200- and 500-bus test cases, and results show that the proposed approach can identify multiple N-x contingencies that cause violations.      
### 17.Timely Broadcasting in Erasure Networks: Age-Rate Tradeoffs  [ :arrow_down: ](https://arxiv.org/pdf/2102.08926.pdf)
>  The interplay between timeliness and rate efficiency is investigated in packet erasure broadcast channels with feedback. A scheduling framework is proposed in which coding actions, as opposed to users, are scheduled to attain desired tradeoffs between rate and age of information (AoI). This tradeoff is formalized by an upper bound on AoI as a function of the target rate constraints and two lower bounds: one as a function of the communication rate and one as a function of the arrival rate. Simulation results show that (i) coding can be beneficial in reducing AoI in the regime of moderate arrival rates even without rate constraints and the benefit increases with the number of users, and (ii) AoI increases with both the target rate constraint and the arrival rate when either is kept fixed, but decreases with them when they are set to be equal.      
### 18.Maximizing Social Welfare Subject to Network Externalities: A Unifying Submodular Optimization Approach  [ :arrow_down: ](https://arxiv.org/pdf/2102.08915.pdf)
>  We consider the problem of maximizing social welfare by allocating indivisible items to a set of agents subject to network externalities. We first provide a general formulation that captures some of the known models as a special case. We then show that the social welfare maximization problem benefits some nice sub-or supermodular properties. That allows us to devise simple polynomial-time approximation algorithms using LovÃ¡sz extension and multilinear extension of the objective function. Our principled approach recovers or improves some of the existing algorithms and provides a simple unifying method for maximizing social welfare subject to network externalities.      
### 19.Numerical Evaluation of a muon tomography system for imaging defects in concrete structures  [ :arrow_down: ](https://arxiv.org/pdf/2102.08913.pdf)
>  Among its numerous applications, deployment in civil structures has caught attraction of many recently. The detection of defects based on inherent physical quantities such as density and atomic number by probing naturally available cosmic muons makes MST a novel idea suitable for inexpensive and non-destructive imaging. In this work, capability of MST to detect concrete defects has been tested and evaluated in terms of two-dimensional imaging and statistical calculations. The imaging has been done on unique and critical defects causing degradation in civil structures. The capability and limitation of MST in this avenue have also been studied.      
### 20.Dissipativity, reciprocity and passive network synthesis: from Jan Willems' seminal Dissipative Dynamical Systems papers to the present day  [ :arrow_down: ](https://arxiv.org/pdf/2102.08855.pdf)
>  The dissipativity concept sits at the intersection of physics, systems theory and control engineering, as a natural generalisation of passive systems that dissipate energy. It relates properties of the external behavior of systems to their internal state, and connects such wide-ranging subjects as optimal control, algebraic Riccati equations, linear matrix inequalities, complex functions, and spectral factorization. Its applications include the analysis and design of interconnected systems (such as cyber-physical systems), robustness and the absolute stability problem (the passivity, small-gain, circle and Popov theorems and the theory of integral quadratic constraints), and network synthesis (of electrical, mechanical and multi-physics systems). In this article, we detail recent developments in the treatment of dissipativity theory for systems that are not necessarily controllable and that need not lend themselves naturally to an input-state-output perspective, drawing inspiration from the behavioral theory of Jan Willems and collaborators. Such systems are prevalent among physical systems, and we will illustrate the considered concepts using simple electric circuit examples.      
### 21.DESED-FL and URBAN-FL: Federated Learning Datasets for Sound Event Detection  [ :arrow_down: ](https://arxiv.org/pdf/2102.08833.pdf)
>  Research on sound event detection (SED) in environmental settings has seen increased attention in recent years. Large amounts of (private) domestic or urban audio data raise significant logistical and privacy concerns. The inherently distributed nature of these tasks, make federated learning (FL) a promising approach to take advantage of large-scale data while mitigating privacy issues. While FL has also seen increased attention recently, to the best of our knowledge there is no research towards FL for SED. To address this gap and foster further research in this field, we create and publish novel FL datasets for SED in domestic and urban environments. Furthermore, we provide baseline results on the datasets in a FL context for three deep neural network architectures. The results indicate that FL is a promising approach for SED, but faces challenges with divergent data distributions inherent to distributed client edge devices.      
### 22.A Knowledge-based Approach for the Automatic Construction of Skill Graphs for Online Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2102.08827.pdf)
>  Automated vehicles need to be aware of the capabilities they currently possess. Skill graphs are directed acylic graphs in which a vehicle's capabilities and the dependencies between these capabilities are modeled. The skills a vehicle requires depend on the behaviors the vehicle has to perform and the operational design domain (ODD) of the vehicle. Skill graphs were originally proposed for online monitoring of the current capabilities of an automated vehicle. They have also been shown to be useful during other parts of the development process, e.g. system design, system verification. Skill graph construction is an iterative, expert-based, manual process with little to no guidelines. This process is, thus, prone to errors and inconsistencies especially regarding the propagation of changes in the vehicle's intended ODD into the skill graphs. In order to circumnavigate this problem, we propose to formalize expert knowledge regarding skill graph construction into a knowledge base and automate the construction process. Thus, all changes in the vehicle's ODD are reflected in the skill graphs automatically leading to a reduction in inconsistencies and errors in the constructed skill graphs.      
### 23.Semi-linear Poisson-mediated Flocking in a Cucker-Smale Model  [ :arrow_down: ](https://arxiv.org/pdf/2102.08772.pdf)
>  We propose a family of compactly supported parametric interaction functions in the general Cucker-Smale flocking dynamics such that the mean-field macroscopic system of mass and momentum balance equations with non-local damping terms can be converted from a system of partial integro-differential equations to an augmented system of partial differential equations in a compact set. We treat the interaction functions as Green's functions for an operator corresponding to a semi-linear Poisson equation and compute the density and momentum in a translating reference frame, i.e. one that is taken in reference to the flock's centroid. This allows us to consider the dynamics in a fixed, flock-centered compact set without loss of generality. We approach the computation of the non-local damping using the standard finite difference treatment of the chosen differential operator, resulting in a tridiagonal system which can be solved quickly.      
### 24.Vaccination and SARS-CoV-2 variants: how much containment is still needed? A quantitative assessment  [ :arrow_down: ](https://arxiv.org/pdf/2102.08704.pdf)
>  Despite the progress in medical care, combined population-wide interventions (such as physical distancing, testing and contact tracing) are still crucial to manage the SARS-CoV-2 pandemic, aggravated by the emergence of new highly transmissible variants. We combine the compartmental SIDARTHE model, predicting the course of COVID-19 infections, with a new data-based model that projects new cases onto casualties and healthcare system costs. Based on the Italian case study, we outline several scenarios: mass vaccination campaigns with different paces, different transmission rates due to new variants, and different enforced countermeasures, including the alternation of opening and closure phases. Our results demonstrate that non-pharmaceutical interventions (NPIs) have a higher impact on the epidemic evolution than vaccination, which advocates for the need to keep containment measures in place throughout the vaccination campaign. We also show that, if intermittent open-close strategies are adopted, deaths and healthcare system costs can be drastically reduced, without any aggravation of socioeconomic losses, as long as one has the foresight to start with a closing phase rather than an opening one.      
### 25.ACTA: A Mobile-Health Solution for Integrated Nudge-Neurofeedback Training for Senior Citizens  [ :arrow_down: ](https://arxiv.org/pdf/2102.08692.pdf)
>  As the worldwide population gets increasingly aged, in-home telemedicine and mobile-health solutions represent promising services to promote active and independent aging and to contribute to a paradigm shift towards patient-centric healthcare. In this work, we present ACTA (Advanced Cognitive Training for Aging), a prototype mobile-health solution to provide advanced cognitive training for senior citizens with mild cognitive impairments. We disclose here the conceptualization of ACTA as the integration of two promising rehabilitation strategies: the "Nudge theory", from the cognitive domain, and the neurofeedback, from the neuroscience domain. Moreover, in ACTA we exploit the most advanced machine learning techniques to deliver customized and fully adaptive support to the elderly, while training in an ecological environment. ACTA represents the next-step beyond SENIOR, an earlier mobile-health project for cognitive training based on Nudge theory, currently ongoing in Lombardy Region. Beyond SENIOR, ACTA represents a highly-usable, accessible, low-cost, new-generation mobile-health solution to promote independent aging and effective motor-cognitive training support, while empowering the elderly in their own aging.      
### 26.DICODerma: A practical approach for metadata management of images in dermatology  [ :arrow_down: ](https://arxiv.org/pdf/2102.08673.pdf)
>  Clinical images are vital for diagnosing and monitoring skin diseases, and their importance has increased with the growing popularity of machine learning. Lack of standards has stifled innovation in dermatological imaging, unlike other image-intensive specialties such as radiology. We investigate the meta-requirements for utilizing the popular DICOM standard for metadata management of images in dermatology. We propose practical design solutions and provide open-source tools to integrate dermatologists' workflow with enterprise imaging systems. Using the tool, dermatologists can tag, search, organize and convert clinical images to the DICOM format. We believe that our less disruptive approach will improve the adoption of standards in the specialty.      
### 27.Coupled Feature Learning for Multimodal Medical Image Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2102.08641.pdf)
>  Multimodal image fusion aims to combine relevant information from images acquired with different sensors. In medical imaging, fused images play an essential role in both standard and automated diagnosis. In this paper, we propose a novel multimodal image fusion method based on coupled dictionary learning. The proposed method is general and can be employed for different medical imaging modalities. Unlike many current medical fusion methods, the proposed approach does not suffer from intensity attenuation nor loss of critical information. Specifically, the images to be fused are decomposed into coupled and independent components estimated using sparse representations with identical supports and a Pearson correlation constraint, respectively. An alternating minimization algorithm is designed to solve the resulting optimization problem. The final fusion step uses the max-absolute-value rule. Experiments are conducted using various pairs of multimodal inputs, including real MR-CT and MR-PET images. The resulting performance and execution times show the competitiveness of the proposed method in comparison with state-of-the-art medical image fusion methods.      
### 28.Knowledge discovery from emergency ambulance dispatch during COVID-19: A case study of Nagoya City, Japan  [ :arrow_down: ](https://arxiv.org/pdf/2102.08628.pdf)
>  Accurate forecasting of medical service requirements is an important big data problem that is crucial for resource management in critical times such as natural disasters and pandemics. With the global spread of coronavirus disease 2019 (COVID-19), several concerns have been raised regarding the ability of medical systems to handle sudden changes in the daily routines of healthcare providers. One significant problem is the management of ambulance dispatch and control during a pandemic. To help address this problem, we first analyze ambulance dispatch data records from April 2014 to August 2020 for Nagoya City, Japan. Significant changes were observed in the data during the pandemic, including the state of emergency (SoE) declared across Japan. In this study, we propose a deep learning framework based on recurrent neural networks to estimate the number of emergency ambulance dispatches (EADs) during a SoE. The fusion of data includes environmental factors, the localization data of mobile phone users, and the past history of EADs, thereby providing a general framework for knowledge discovery and better resource management. The results indicate that the proposed blend of training data can be used efficiently in a real-world estimation of EAD requirements during periods of high uncertainties such as pandemics.      
### 29.Consistent Right-Invariant Fixed-Lag Smoother with Application to Visual Inertial SLAM  [ :arrow_down: ](https://arxiv.org/pdf/2102.08596.pdf)
>  State estimation problems that use relative observations routinely arise in navigation of unmanned aerial vehicles, autonomous ground vehicles, \etc whose proper operation relies on accurate state estimates and reliable covariances. These problems have immanent unobservable directions. Traditional causal estimators, however, usually gain spurious information on the unobservable directions, leading to over confident covariance inconsistent with the actual estimator errors. The consistency problem of fixed-lag smoothers (FLSs) has only been attacked by the first estimate Jacobian (FEJ) technique because of the complexity to analyze their observability property. But the FEJ has several drawbacks hampering its wide adoption. To ensure the consistency of a FLS, this paper introduces the right invariant error formulation into the FLS framework. To our knowledge, we are the first to analyze the observability of a FLS with the right invariant error. Our main contributions are twofold. As the first novelty, to bypass the complexity of analysis with the classic observability matrix, we show that observability analysis of FLSs can be done equivalently on the linearized system. Second, we prove that the inconsistency issue in the traditional FLS can be elegantly solved by the right invariant error formulation without artificially correcting Jacobians. By applying the proposed FLS to the monocular visual inertial simultaneous localization and mapping (SLAM) problem, we confirm that the method consistently estimates covariance similarly to a batch smoother in simulation and that our method achieved comparable accuracy as traditional FLSs on real data.      
### 30.End-to-end lyrics Recognition with Voice to Singing Style Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2102.08575.pdf)
>  Automatic transcription of monophonic/polyphonic music is a challenging task due to the lack of availability of large amounts of transcribed data. In this paper, we propose a data augmentation method that converts natural speech to singing voice based on vocoder based speech synthesizer. This approach, called voice to singing (V2S), performs the voice style conversion by modulating the F0 contour of the natural speech with that of a singing voice. The V2S model based style transfer can generate good quality singing voice thereby enabling the conversion of large corpora of natural speech to singing voice that is useful in building an E2E lyrics transcription system. In our experiments on monophonic singing voice data, the V2S style transfer provides a significant gain (relative improvements of 21%) for the E2E lyrics transcription system. We also discuss additional components like transfer learning and lyrics based language modeling to improve the performance of the lyrics transcription system.      
### 31.Ensemble Transfer Learning of Elastography and B-mode Breast Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2102.08567.pdf)
>  Computer-aided detection (CAD) of benign and malignant breast lesions becomes increasingly essential in breast ultrasound (US) imaging. The CAD systems rely on imaging features identified by the medical experts for their performance, whereas deep learning (DL) methods automatically extract features from the data. The challenge of the DL is the insufficiency of breast US images available to train the DL models. Here, we present an ensemble transfer learning model to classify benign and malignant breast tumors using B-mode breast US (B-US) and strain elastography breast US (SE-US) images. This model combines semantic features from AlexNet &amp; ResNet models to classify benign from malignant tumors. We use both B-US and SE-US images to train the model and classify the tumors. We retrospectively gathered 85 patients' data, with 42 benign and 43 malignant cases confirmed with the biopsy. Each patient had multiple B-US and their corresponding SE-US images, and the total dataset contained 261 B-US images and 261 SE-US images. Experimental results show that our ensemble model achieves a sensitivity of 88.89% and specificity of 91.10%. These diagnostic performances of the proposed method are equivalent to or better than manual identification. Thus, our proposed ensemble learning method would facilitate detecting early breast cancer, reliably improving patient care.      
### 32.Weighted Recursive Least Square Filter and Neural Network based Residual Echo Suppression for the AEC-Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2102.08551.pdf)
>  This paper presents a real-time Acoustic Echo Cancellation (AEC) algorithm submitted to the AEC-Challenge. The algorithm consists of three modules: Generalized Cross-Correlation with PHAse Transform (GCC-PHAT) based time delay compensation, weighted Recursive Least Square (wRLS) based linear adaptive filtering and neural network based residual echo suppression. The wRLS filter is derived from a novel semi-blind source separation perspective. The neural network model predicts a Phase-Sensitive Mask (PSM) based on the aligned reference and the linear filter output. The algorithm achieved a mean subjective score of 4.00 and ranked 2nd in the AEC-Challenge.      
### 33.NEAT: A Framework for Automated Exploration of Floating Point Approximations  [ :arrow_down: ](https://arxiv.org/pdf/2102.08547.pdf)
>  Much recent research is devoted to exploring tradeoffs between computational accuracy and energy efficiency at different levels of the system stack. Approximation at the floating point unit (FPU) allows saving energy by simply reducing the number of computed floating point bits in return for accuracy loss. Although, finding the most energy efficient approximation for various applications with minimal effort is the main challenge. To address this issue, we propose NEAT: a pin tool that helps users automatically explore the accuracy-energy tradeoff space induced by various floating point implementations. NEAT helps programmers explore the effects of simultaneously using multiple floating point implementations to achieve the lowest energy consumption for an accuracy constraint or vice versa. NEAT accepts one or more user-defined floating point implementations and programmable placement rules for where/when to apply them. NEAT then automatically replaces floating point operations with different implementations based on the user-specified rules during the runtime and explores the resulting tradeoff space to find the best use of approximate floating point implementations for the precision tuning throughout the program. We evaluate NEAT by enforcing combinations of 24/53 different floating point implementations with three sets of placement rules on a wide range of benchmarks. We find that heuristic precision tuning at the function level provides up to 22% and 48% energy savings at 1% and 10% accuracy loss comparing to applying a single implementation for the whole application. Also, NEAT is applicable to neural networks where it finds the optimal precision level for each layer considering an accuracy target for the model.      
### 34.Separated Proportional-Integral Lagrangian for Chance Constrained Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2102.08539.pdf)
>  Safety is essential for reinforcement learning (RL) applied in real-world tasks like autonomous driving. Chance constraints which guarantee the satisfaction of state constraints at a high probability are suitable to represent the requirements in real-world environment with uncertainty. Existing chance constrained RL methods like the penalty method and the Lagrangian method either exhibit periodic oscillations or cannot satisfy the constraints. In this paper, we address these shortcomings by proposing a separated proportional-integral Lagrangian (SPIL) algorithm. Taking a control perspective, we first interpret the penalty method and the Lagrangian method as proportional feedback and integral feedback control, respectively. Then, a proportional-integral Lagrangian method is proposed to steady learning process while improving safety. To prevent integral overshooting and reduce conservatism, we introduce the integral separation technique inspired by PID control. Finally, an analytical gradient of the chance constraint is utilized for model-based policy optimization. The effectiveness of SPIL is demonstrated by a narrow car-following task. Experiments indicate that compared with previous methods, SPIL improves the performance while guaranteeing safety, with a steady learning process.      
### 35.ATCSpeechNet: A multilingual end-to-end speech recognition framework for air traffic control systems  [ :arrow_down: ](https://arxiv.org/pdf/2102.08535.pdf)
>  In this paper, a multilingual end-to-end framework, called as ATCSpeechNet, is proposed to tackle the issue of translating communication speech into human-readable text in air traffic control (ATC) systems. In the proposed framework, we focus on integrating the multilingual automatic speech recognition (ASR) into one model, in which an end-to-end paradigm is developed to convert speech waveform into text directly, without any feature engineering or lexicon. In order to make up for the deficiency of the handcrafted feature engineering caused by ATC challenges, a speech representation learning (SRL) network is proposed to capture robust and discriminative speech representations from the raw wave. The self-supervised training strategy is adopted to optimize the SRL network from unlabeled data, and further to predict the speech features, i.e., wave-to-feature. An end-to-end architecture is improved to complete the ASR task, in which a grapheme-based modeling unit is applied to address the multilingual ASR issue. Facing the problem of small transcribed samples in the ATC domain, an unsupervised approach with mask prediction is applied to pre-train the backbone network of the ASR model on unlabeled data by a feature-to-feature process. Finally, by integrating the SRL with ASR, an end-to-end multilingual ASR framework is formulated in a supervised manner, which is able to translate the raw wave into text in one model, i.e., wave-to-text. Experimental results on the ATCSpeech corpus demonstrate that the proposed approach achieves a high performance with a very small labeled corpus and less resource consumption, only 4.20% label error rate on the 58-hour transcribed corpus. Compared to the baseline model, the proposed approach obtains over 100% relative performance improvement which can be further enhanced with the increasing of the size of the transcribed samples.      
### 36.Applications of nonlinear control to a whole-brain network of FitzHugh-Nagumo oscillators  [ :arrow_down: ](https://arxiv.org/pdf/2102.08524.pdf)
>  We apply the framework of optimal nonlinear control to steer the dynamics of a whole-brain network of FitzHugh-Nagumo oscillators. Its nodes correspond to the cortical areas of an atlas-based segmentation of the human cerebral cortex, and the inter-node coupling strengths are derived from Diffusion Tensor Imaging data of the connectome of the human brain. Nodes are coupled using an additive scheme without delays and are driven by background inputs with fixed mean and additive Gaussian noise. Optimal control inputs to nodes are determined by minimizing a cost functional that penalizes the deviations from a desired network dynamic, the control energy, and spatially non-sparse control inputs. Using the strength of the background input and the overall coupling strength as order parameters, the network's state-space decomposes into regions of low and high activity fixed points separated by a high amplitude limit cycle all of which qualitatively correspond to the states of an isolated network node. Along the borders, however, additional limit cycles, asynchronous states and multistability can be observed. Optimal control is applied to several state-switching and network synchronization tasks, and the results are compared to controllability measures from linear control theory for the same connectome. We find that intuitions from the latter about the roles of nodes in steering the network dynamics, which are solely based on connectome features, do not generally carry over to nonlinear systems, as had been previously implied. Instead, the role of nodes under optimal nonlinear control critically depends on the specified task and the system's location in state space. Our results shed new light on the controllability of brain network states and may serve as an inspiration for the design of new paradigms for non-invasive brain stimulation.      
### 37.Performance Analyses of MRT/MRC in Dual-Hop NOMA Full-Duplex AF Relay Networks with Residual Hardware Impairments  [ :arrow_down: ](https://arxiv.org/pdf/2102.08464.pdf)
>  This paper analyzes the performance of maximum-ratio transmission (MRT)/maximum-ratio combining (MRC) scheme in a dual-hop non-orthogonal multiple access (NOMA) full-duplex (FD) relay networks in the presence of residual hardware impairments (RHIs). The effects of channel estimation errors (CEEs) and imperfect successive interference cancellation are also considered for a realistic performance analysis. In the network, the base station and multiple users utilize MRT and MRC, respectively, while a dedicated relay consisting of two antennas, one for receiving and the other for broadcasting, operates in amplify-and-forward mode. For performance criterion, exact outage probability (OP) expression is derived for Nakagami-m fading channels. Furthermore, a tight lower bound and asymptotic expressions are also derived to provide more insights into the obtained OP in terms of diversity order and array gain. The obtained numerical results demonstrate the importance of loop-interference cancellation process at FD relay in order for the investigated system to perform better than half-duplex-NOMA counterpart. Also, a performance trade-off between the MRT and MRC schemes is observed in the presence of CEEs among users. Furthermore, it is shown that RHIs have a significant effect on the performance of users with lower power coefficients, however it does not change the diversity order. RHIs and CEEs have the most and least deterioration effects on the system performance, respectively.      
### 38.cuFINUFFT: a load-balanced GPU library for general-purpose nonuniform FFTs  [ :arrow_down: ](https://arxiv.org/pdf/2102.08463.pdf)
>  Nonuniform fast Fourier transforms dominate the computational cost in many applications including image reconstruction and signal processing. We thus present a general-purpose GPU-based CUDA library for type 1 (nonuniform to uniform) and type 2 (uniform to nonuniform) transforms in dimensions 2 and 3, in single or double precision. It achieves high performance for a given user-requested accuracy, regardless of the distribution of nonuniform points, via cache-aware point reordering, and load-balanced blocked spreading in shared memory. At low accuracies, this gives on-GPU throughputs around $10^9$ nonuniform points per second, and (even including host-device transfer) is typically 4-10$\times$ faster than the latest parallel CPU code FINUFFT (at 28 threads). It is competitive with two established GPU codes, being up to 90$\times$ faster at high accuracy and/or type 1 clustered point distributions. Finally we demonstrate a 6-18$\times$ speedup versus CPU in an X-ray diffraction 3D iterative reconstruction task at $10^{-12}$ accuracy, observing excellent multi-GPU weak scaling up to one rank per GPU.      
### 39.A Review of Testing Object-Based Environment Perception for Safe Automated Driving  [ :arrow_down: ](https://arxiv.org/pdf/2102.08460.pdf)
>  Safety assurance of automated driving systems must consider uncertain environment perception. This paper reviews literature addressing how perception testing is realized as part of safety assurance. We focus on testing for verification and validation purposes at the interface between perception and planning, and structure our analysis along the three axes 1) test criteria and metrics, 2) test scenarios, and 3) reference data. Furthermore, the analyzed literature includes related safety standards, safety-independent perception algorithm benchmarking, and sensor modeling. We find that the realization of safety-aware perception testing remains an open issue since challenges concerning the three testing axes and their interdependencies currently do not appear to be sufficiently solved.      
### 40.Selfie Periocular Verification using an Efficient Super-Resolution Approach  [ :arrow_down: ](https://arxiv.org/pdf/2102.08449.pdf)
>  Selfie-based biometrics has great potential for a wide range of applications from marketing to higher security environments like online banking. This is now especially relevant since e.g. periocular verification is contactless, and thereby safe to use in pandemics such as COVID-19. However, selfie-based biometrics faces some challenges since there is limited control over the data acquisition conditions. Therefore, super-resolution has to be used to increase the quality of the captured images. Most of the state of the art super-resolution methods use deep networks with large filters, thereby needing to train and store a correspondingly large number of parameters, and making their use difficult for mobile devices commonly used for selfie-based. <br>In order to achieve an efficient super-resolution method, we propose an Efficient Single Image Super-Resolution (ESISR) algorithm, which takes into account a trade-off between the efficiency of the deep neural network and the size of its filters. To that end, the method implements a novel loss function based on the Sharpness metric. This metric turns out to be more suitable for increasing the quality of the eye images. Our method drastically reduces the number of parameters when compared with Deep CNNs with Skip Connection and Network (DCSCN): from 2,170,142 to 28,654 parameters when the image size is increased by a factor of x3. Furthermore, the proposed method keeps the sharp quality of the images, which is highly relevant for biometric recognition purposes. The results on remote verification systems with raw images reached an Equal Error Rate (EER) of 8.7% for FaceNet and 10.05% for VGGFace. Where embedding vectors were used from periocular images the best results reached an EER of 8.9% (x3) for FaceNet and 9.90% (x4) for VGGFace.      
### 41.Multi-Stage Transmission Line Flow Control Using Centralized and Decentralized Reinforcement Learning Agents  [ :arrow_down: ](https://arxiv.org/pdf/2102.08430.pdf)
>  Planning future operational scenarios of bulk power systems that meet security and economic constraints typically requires intensive labor efforts in performing massive simulations. To automate this process and relieve engineers' burden, a novel multi-stage control approach is presented in this paper to train centralized and decentralized reinforcement learning agents that can automatically adjust grid controllers for regulating transmission line flows at normal condition and under contingencies. The power grid flow control problem is formulated as Markov Decision Process (MDP). At stage one, centralized soft actor-critic (SAC) agent is trained to control generator active power outputs in a wide area to control transmission line flows against specified security limits. If line overloading issues remain unresolved, stage two is used to train decentralized SAC agent via load throw-over at local substations. The effectiveness of the proposed approach is verified on a series of actual planning cases used for operating the power grid of SGCC Zhejiang Electric Power Company.      
### 42.Synthesis of Winning Attacks on Communication Protocols using Supervisory Control Theory  [ :arrow_down: ](https://arxiv.org/pdf/2102.06028.pdf)
>  There is an increasing need to study the vulnerability of communication protocols in distributed systems to malicious attacks that attempt to violate safety or liveness properties. In this paper, we propose a general methodology for formal synthesis of successful attacks against protocols where the attacker always eventually wins, called For-all attacks. This generalizes previous work on the synthesis of There-exists attacks, where the attacker can sometimes win. As we model protocols and system architectures by finite-state automata, our methodology employs the supervisory control theory of discrete event systems, which is well suited to pose and the synthesis of For-all attacks where the attacker has partial observability and controllability of the system events. We demonstrate our methodology using examples of man-in-the-middle attacks against the Alternating Bit Protocol and the Transmission Control Protocol.      
